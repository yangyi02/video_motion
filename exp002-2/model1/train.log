[INFO 2017-06-26 11:39:06,230 main.py:170] Namespace(batch_size=64, display=False, image_dir='/home/yi/Downloads/mpii-64-few', image_size=64, init_model_path='', input_video_path='video', learning_rate=0.001, method='unsupervised', motion_range=2, num_channel=3, num_inputs=3, output_flow_path='flow', output_flow_video_path='flow_video', save_dir='./model', test=False, test_epoch=100, test_interval=1000, test_video=False, train=True, train_epoch=10000)
[INFO 2017-06-26 11:39:08,605 main.py:50] epoch 0, training loss: 218350.06, average training loss: 218350.06, base loss: 27975.11
[INFO 2017-06-26 11:39:09,187 main.py:50] epoch 1, training loss: 180884.31, average training loss: 199617.19, base loss: 28010.21
[INFO 2017-06-26 11:39:09,797 main.py:50] epoch 2, training loss: 145437.14, average training loss: 181557.17, base loss: 27188.05
[INFO 2017-06-26 11:39:10,385 main.py:50] epoch 3, training loss: 120011.16, average training loss: 166170.67, base loss: 26573.47
[INFO 2017-06-26 11:39:10,969 main.py:50] epoch 4, training loss: 105887.92, average training loss: 154114.12, base loss: 26011.53
[INFO 2017-06-26 11:39:11,550 main.py:50] epoch 5, training loss: 94074.13, average training loss: 144107.46, base loss: 25672.60
[INFO 2017-06-26 11:39:12,131 main.py:50] epoch 6, training loss: 82593.48, average training loss: 135319.75, base loss: 25531.05
[INFO 2017-06-26 11:39:12,714 main.py:50] epoch 7, training loss: 73477.59, average training loss: 127589.48, base loss: 25314.11
[INFO 2017-06-26 11:39:13,301 main.py:50] epoch 8, training loss: 65707.32, average training loss: 120713.68, base loss: 25599.21
[INFO 2017-06-26 11:39:13,883 main.py:50] epoch 9, training loss: 56639.16, average training loss: 114306.23, base loss: 25426.55
[INFO 2017-06-26 11:39:14,464 main.py:50] epoch 10, training loss: 50483.19, average training loss: 108504.13, base loss: 25357.99
[INFO 2017-06-26 11:39:15,062 main.py:50] epoch 11, training loss: 46301.05, average training loss: 103320.54, base loss: 25420.34
[INFO 2017-06-26 11:39:15,640 main.py:50] epoch 12, training loss: 43528.07, average training loss: 98721.12, base loss: 25456.15
[INFO 2017-06-26 11:39:16,224 main.py:50] epoch 13, training loss: 38126.95, average training loss: 94392.97, base loss: 25407.41
[INFO 2017-06-26 11:39:16,808 main.py:50] epoch 14, training loss: 35577.45, average training loss: 90471.93, base loss: 25375.80
[INFO 2017-06-26 11:39:17,389 main.py:50] epoch 15, training loss: 35897.83, average training loss: 87061.05, base loss: 25511.20
[INFO 2017-06-26 11:39:17,967 main.py:50] epoch 16, training loss: 30826.71, average training loss: 83753.15, base loss: 25408.59
[INFO 2017-06-26 11:39:18,546 main.py:50] epoch 17, training loss: 30696.17, average training loss: 80805.54, base loss: 25390.35
[INFO 2017-06-26 11:39:19,127 main.py:50] epoch 18, training loss: 29917.30, average training loss: 78127.21, base loss: 25422.71
[INFO 2017-06-26 11:39:19,711 main.py:50] epoch 19, training loss: 28400.39, average training loss: 75640.87, base loss: 25415.24
[INFO 2017-06-26 11:39:20,287 main.py:50] epoch 20, training loss: 28514.81, average training loss: 73396.77, base loss: 25470.62
[INFO 2017-06-26 11:39:20,865 main.py:50] epoch 21, training loss: 26459.51, average training loss: 71263.26, base loss: 25401.04
[INFO 2017-06-26 11:39:21,456 main.py:50] epoch 22, training loss: 26170.13, average training loss: 69302.69, base loss: 25391.13
[INFO 2017-06-26 11:39:22,040 main.py:50] epoch 23, training loss: 25602.90, average training loss: 67481.86, base loss: 25364.94
[INFO 2017-06-26 11:39:22,623 main.py:50] epoch 24, training loss: 27141.28, average training loss: 65868.24, base loss: 25417.22
[INFO 2017-06-26 11:39:23,203 main.py:50] epoch 25, training loss: 24767.42, average training loss: 64287.44, base loss: 25388.23
[INFO 2017-06-26 11:39:23,784 main.py:50] epoch 26, training loss: 28163.73, average training loss: 62949.53, base loss: 25502.57
[INFO 2017-06-26 11:39:24,364 main.py:50] epoch 27, training loss: 22422.21, average training loss: 61502.12, base loss: 25397.89
[INFO 2017-06-26 11:39:24,946 main.py:50] epoch 28, training loss: 26348.45, average training loss: 60289.93, base loss: 25457.52
[INFO 2017-06-26 11:39:25,533 main.py:50] epoch 29, training loss: 25771.88, average training loss: 59139.32, base loss: 25501.62
[INFO 2017-06-26 11:39:26,112 main.py:50] epoch 30, training loss: 25220.75, average training loss: 58045.18, base loss: 25514.43
[INFO 2017-06-26 11:39:26,693 main.py:50] epoch 31, training loss: 22502.74, average training loss: 56934.47, base loss: 25437.82
[INFO 2017-06-26 11:39:27,276 main.py:50] epoch 32, training loss: 25508.25, average training loss: 55982.16, base loss: 25471.02
[INFO 2017-06-26 11:39:27,864 main.py:50] epoch 33, training loss: 26200.41, average training loss: 55106.23, base loss: 25526.38
[INFO 2017-06-26 11:39:28,449 main.py:50] epoch 34, training loss: 23796.36, average training loss: 54211.66, base loss: 25510.72
[INFO 2017-06-26 11:39:29,031 main.py:50] epoch 35, training loss: 23477.17, average training loss: 53357.93, base loss: 25473.91
[INFO 2017-06-26 11:39:29,612 main.py:50] epoch 36, training loss: 23304.09, average training loss: 52545.66, base loss: 25454.68
[INFO 2017-06-26 11:39:30,200 main.py:50] epoch 37, training loss: 28062.74, average training loss: 51901.37, base loss: 25561.07
[INFO 2017-06-26 11:39:30,781 main.py:50] epoch 38, training loss: 25693.01, average training loss: 51229.36, base loss: 25610.61
[INFO 2017-06-26 11:39:31,367 main.py:50] epoch 39, training loss: 25991.09, average training loss: 50598.41, base loss: 25661.39
[INFO 2017-06-26 11:39:31,949 main.py:50] epoch 40, training loss: 23959.90, average training loss: 49948.69, base loss: 25661.95
[INFO 2017-06-26 11:39:32,533 main.py:50] epoch 41, training loss: 22743.41, average training loss: 49300.94, base loss: 25627.96
[INFO 2017-06-26 11:39:33,117 main.py:50] epoch 42, training loss: 22919.14, average training loss: 48687.41, base loss: 25601.23
[INFO 2017-06-26 11:39:33,704 main.py:50] epoch 43, training loss: 23226.25, average training loss: 48108.75, base loss: 25585.45
[INFO 2017-06-26 11:39:34,288 main.py:50] epoch 44, training loss: 21902.27, average training loss: 47526.38, base loss: 25541.78
[INFO 2017-06-26 11:39:34,876 main.py:50] epoch 45, training loss: 25903.18, average training loss: 47056.31, base loss: 25586.07
[INFO 2017-06-26 11:39:35,459 main.py:50] epoch 46, training loss: 25666.34, average training loss: 46601.21, base loss: 25630.83
[INFO 2017-06-26 11:39:36,040 main.py:50] epoch 47, training loss: 23009.12, average training loss: 46109.71, base loss: 25615.69
[INFO 2017-06-26 11:39:36,624 main.py:50] epoch 48, training loss: 23721.72, average training loss: 45652.81, base loss: 25625.33
[INFO 2017-06-26 11:39:37,212 main.py:50] epoch 49, training loss: 21600.90, average training loss: 45171.77, base loss: 25576.95
[INFO 2017-06-26 11:39:37,799 main.py:50] epoch 50, training loss: 22573.40, average training loss: 44728.67, base loss: 25566.19
[INFO 2017-06-26 11:39:38,383 main.py:50] epoch 51, training loss: 21378.01, average training loss: 44279.61, base loss: 25514.49
[INFO 2017-06-26 11:39:38,965 main.py:50] epoch 52, training loss: 23515.14, average training loss: 43887.83, base loss: 25505.81
[INFO 2017-06-26 11:39:39,548 main.py:50] epoch 53, training loss: 21354.17, average training loss: 43470.54, base loss: 25464.22
[INFO 2017-06-26 11:39:40,137 main.py:50] epoch 54, training loss: 20308.12, average training loss: 43049.41, base loss: 25403.85
[INFO 2017-06-26 11:39:40,745 main.py:50] epoch 55, training loss: 22581.75, average training loss: 42683.91, base loss: 25403.59
[INFO 2017-06-26 11:39:41,327 main.py:50] epoch 56, training loss: 22119.62, average training loss: 42323.14, base loss: 25386.26
[INFO 2017-06-26 11:39:41,911 main.py:50] epoch 57, training loss: 22929.23, average training loss: 41988.76, base loss: 25395.58
[INFO 2017-06-26 11:39:42,493 main.py:50] epoch 58, training loss: 24985.16, average training loss: 41700.56, base loss: 25440.25
[INFO 2017-06-26 11:39:43,075 main.py:50] epoch 59, training loss: 23375.98, average training loss: 41395.15, base loss: 25462.90
[INFO 2017-06-26 11:39:43,661 main.py:50] epoch 60, training loss: 23242.76, average training loss: 41097.57, base loss: 25474.46
[INFO 2017-06-26 11:39:44,254 main.py:50] epoch 61, training loss: 21807.25, average training loss: 40786.44, base loss: 25458.29
[INFO 2017-06-26 11:39:44,845 main.py:50] epoch 62, training loss: 21103.69, average training loss: 40474.01, base loss: 25433.63
[INFO 2017-06-26 11:39:45,431 main.py:50] epoch 63, training loss: 21657.11, average training loss: 40180.00, base loss: 25425.60
[INFO 2017-06-26 11:39:46,018 main.py:50] epoch 64, training loss: 20201.80, average training loss: 39872.64, base loss: 25382.21
[INFO 2017-06-26 11:39:46,599 main.py:50] epoch 65, training loss: 22392.64, average training loss: 39607.79, base loss: 25388.40
[INFO 2017-06-26 11:39:47,185 main.py:50] epoch 66, training loss: 24046.64, average training loss: 39375.54, base loss: 25420.55
[INFO 2017-06-26 11:39:47,773 main.py:50] epoch 67, training loss: 20579.83, average training loss: 39099.13, base loss: 25394.27
[INFO 2017-06-26 11:39:48,362 main.py:50] epoch 68, training loss: 21803.33, average training loss: 38848.47, base loss: 25394.24
[INFO 2017-06-26 11:39:48,952 main.py:50] epoch 69, training loss: 20027.23, average training loss: 38579.59, base loss: 25356.89
[INFO 2017-06-26 11:39:49,540 main.py:50] epoch 70, training loss: 20289.08, average training loss: 38321.98, base loss: 25327.59
[INFO 2017-06-26 11:39:50,122 main.py:50] epoch 71, training loss: 21223.42, average training loss: 38084.50, base loss: 25325.09
[INFO 2017-06-26 11:39:50,704 main.py:50] epoch 72, training loss: 22703.29, average training loss: 37873.80, base loss: 25329.52
[INFO 2017-06-26 11:39:51,291 main.py:50] epoch 73, training loss: 20680.40, average training loss: 37641.45, base loss: 25317.88
[INFO 2017-06-26 11:39:51,878 main.py:50] epoch 74, training loss: 21055.17, average training loss: 37420.30, base loss: 25312.57
[INFO 2017-06-26 11:39:52,464 main.py:50] epoch 75, training loss: 20825.01, average training loss: 37201.94, base loss: 25300.54
[INFO 2017-06-26 11:39:53,056 main.py:50] epoch 76, training loss: 20609.38, average training loss: 36986.46, base loss: 25289.74
[INFO 2017-06-26 11:39:53,641 main.py:50] epoch 77, training loss: 21665.53, average training loss: 36790.03, base loss: 25300.79
[INFO 2017-06-26 11:39:54,228 main.py:50] epoch 78, training loss: 20313.92, average training loss: 36581.48, base loss: 25293.39
[INFO 2017-06-26 11:39:54,813 main.py:50] epoch 79, training loss: 19596.58, average training loss: 36369.16, base loss: 25267.25
[INFO 2017-06-26 11:39:55,399 main.py:50] epoch 80, training loss: 20378.27, average training loss: 36171.75, base loss: 25257.89
[INFO 2017-06-26 11:39:55,983 main.py:50] epoch 81, training loss: 20548.95, average training loss: 35981.22, base loss: 25252.95
[INFO 2017-06-26 11:39:56,576 main.py:50] epoch 82, training loss: 21124.68, average training loss: 35802.23, base loss: 25251.43
[INFO 2017-06-26 11:39:57,165 main.py:50] epoch 83, training loss: 19526.86, average training loss: 35608.48, base loss: 25228.98
[INFO 2017-06-26 11:39:57,751 main.py:50] epoch 84, training loss: 20694.73, average training loss: 35433.02, base loss: 25225.93
[INFO 2017-06-26 11:39:58,338 main.py:50] epoch 85, training loss: 19508.60, average training loss: 35247.85, base loss: 25203.97
[INFO 2017-06-26 11:39:58,927 main.py:50] epoch 86, training loss: 20246.10, average training loss: 35075.42, base loss: 25201.15
[INFO 2017-06-26 11:39:59,519 main.py:50] epoch 87, training loss: 20797.96, average training loss: 34913.17, base loss: 25203.93
[INFO 2017-06-26 11:40:00,112 main.py:50] epoch 88, training loss: 19704.71, average training loss: 34742.29, base loss: 25190.88
[INFO 2017-06-26 11:40:00,702 main.py:50] epoch 89, training loss: 21168.57, average training loss: 34591.47, base loss: 25199.78
[INFO 2017-06-26 11:40:01,291 main.py:50] epoch 90, training loss: 19793.30, average training loss: 34428.86, base loss: 25189.34
[INFO 2017-06-26 11:40:01,876 main.py:50] epoch 91, training loss: 22172.69, average training loss: 34295.64, base loss: 25222.04
[INFO 2017-06-26 11:40:02,463 main.py:50] epoch 92, training loss: 22817.90, average training loss: 34172.22, base loss: 25247.11
[INFO 2017-06-26 11:40:03,053 main.py:50] epoch 93, training loss: 22482.29, average training loss: 34047.86, base loss: 25274.63
[INFO 2017-06-26 11:40:03,644 main.py:50] epoch 94, training loss: 20187.57, average training loss: 33901.96, base loss: 25276.31
[INFO 2017-06-26 11:40:04,231 main.py:50] epoch 95, training loss: 23035.76, average training loss: 33788.77, base loss: 25304.80
[INFO 2017-06-26 11:40:04,823 main.py:50] epoch 96, training loss: 23035.08, average training loss: 33677.91, base loss: 25327.89
[INFO 2017-06-26 11:40:05,412 main.py:50] epoch 97, training loss: 19552.43, average training loss: 33533.77, base loss: 25315.63
[INFO 2017-06-26 11:40:06,002 main.py:50] epoch 98, training loss: 19331.56, average training loss: 33390.32, base loss: 25290.86
[INFO 2017-06-26 11:40:06,608 main.py:50] epoch 99, training loss: 19843.33, average training loss: 33254.85, base loss: 25290.98
[INFO 2017-06-26 11:40:07,193 main.py:50] epoch 100, training loss: 23009.27, average training loss: 33153.40, base loss: 25321.97
[INFO 2017-06-26 11:40:07,780 main.py:50] epoch 101, training loss: 22127.81, average training loss: 33045.31, base loss: 25338.96
[INFO 2017-06-26 11:40:08,367 main.py:50] epoch 102, training loss: 19327.80, average training loss: 32912.13, base loss: 25328.47
[INFO 2017-06-26 11:40:08,958 main.py:50] epoch 103, training loss: 18952.89, average training loss: 32777.91, base loss: 25312.91
[INFO 2017-06-26 11:40:09,548 main.py:50] epoch 104, training loss: 20923.94, average training loss: 32665.01, base loss: 25328.17
[INFO 2017-06-26 11:40:10,141 main.py:50] epoch 105, training loss: 21707.10, average training loss: 32561.64, base loss: 25341.57
[INFO 2017-06-26 11:40:10,728 main.py:50] epoch 106, training loss: 22702.51, average training loss: 32469.49, base loss: 25360.41
[INFO 2017-06-26 11:40:11,316 main.py:50] epoch 107, training loss: 19256.81, average training loss: 32347.15, base loss: 25352.35
[INFO 2017-06-26 11:40:11,904 main.py:50] epoch 108, training loss: 19236.56, average training loss: 32226.87, base loss: 25341.29
[INFO 2017-06-26 11:40:12,493 main.py:50] epoch 109, training loss: 23428.79, average training loss: 32146.89, base loss: 25379.76
[INFO 2017-06-26 11:40:13,080 main.py:50] epoch 110, training loss: 19709.83, average training loss: 32034.85, base loss: 25382.08
[INFO 2017-06-26 11:40:13,672 main.py:50] epoch 111, training loss: 19555.89, average training loss: 31923.43, base loss: 25379.43
[INFO 2017-06-26 11:40:14,268 main.py:50] epoch 112, training loss: 19391.27, average training loss: 31812.52, base loss: 25370.93
[INFO 2017-06-26 11:40:14,860 main.py:50] epoch 113, training loss: 22487.38, average training loss: 31730.72, base loss: 25392.47
[INFO 2017-06-26 11:40:15,450 main.py:50] epoch 114, training loss: 19615.09, average training loss: 31625.37, base loss: 25391.62
[INFO 2017-06-26 11:40:16,042 main.py:50] epoch 115, training loss: 21594.07, average training loss: 31538.89, base loss: 25406.99
[INFO 2017-06-26 11:40:16,628 main.py:50] epoch 116, training loss: 21278.84, average training loss: 31451.20, base loss: 25417.09
[INFO 2017-06-26 11:40:17,215 main.py:50] epoch 117, training loss: 19157.03, average training loss: 31347.01, base loss: 25413.04
[INFO 2017-06-26 11:40:17,808 main.py:50] epoch 118, training loss: 20193.11, average training loss: 31253.28, base loss: 25422.72
[INFO 2017-06-26 11:40:18,401 main.py:50] epoch 119, training loss: 18799.57, average training loss: 31149.50, base loss: 25409.27
[INFO 2017-06-26 11:40:18,996 main.py:50] epoch 120, training loss: 19296.23, average training loss: 31051.54, base loss: 25407.82
[INFO 2017-06-26 11:40:19,587 main.py:50] epoch 121, training loss: 21373.38, average training loss: 30972.21, base loss: 25434.23
[INFO 2017-06-26 11:40:20,176 main.py:50] epoch 122, training loss: 21172.04, average training loss: 30892.53, base loss: 25436.50
[INFO 2017-06-26 11:40:20,764 main.py:50] epoch 123, training loss: 19381.69, average training loss: 30799.71, base loss: 25441.24
[INFO 2017-06-26 11:40:21,353 main.py:50] epoch 124, training loss: 17765.47, average training loss: 30695.43, base loss: 25414.54
[INFO 2017-06-26 11:40:21,945 main.py:50] epoch 125, training loss: 20386.20, average training loss: 30613.61, base loss: 25426.14
[INFO 2017-06-26 11:40:22,537 main.py:50] epoch 126, training loss: 19278.17, average training loss: 30524.36, base loss: 25424.51
[INFO 2017-06-26 11:40:23,128 main.py:50] epoch 127, training loss: 18123.62, average training loss: 30427.48, base loss: 25402.55
[INFO 2017-06-26 11:40:23,722 main.py:50] epoch 128, training loss: 18548.05, average training loss: 30335.39, base loss: 25394.66
[INFO 2017-06-26 11:40:24,312 main.py:50] epoch 129, training loss: 17481.00, average training loss: 30236.51, base loss: 25370.48
[INFO 2017-06-26 11:40:24,904 main.py:50] epoch 130, training loss: 21065.81, average training loss: 30166.50, base loss: 25384.32
[INFO 2017-06-26 11:40:25,493 main.py:50] epoch 131, training loss: 21211.99, average training loss: 30098.66, base loss: 25394.22
[INFO 2017-06-26 11:40:26,081 main.py:50] epoch 132, training loss: 22268.10, average training loss: 30039.79, base loss: 25419.68
[INFO 2017-06-26 11:40:26,665 main.py:50] epoch 133, training loss: 17984.14, average training loss: 29949.82, base loss: 25408.30
[INFO 2017-06-26 11:40:27,254 main.py:50] epoch 134, training loss: 17237.99, average training loss: 29855.66, base loss: 25381.62
[INFO 2017-06-26 11:40:27,847 main.py:50] epoch 135, training loss: 21268.49, average training loss: 29792.52, base loss: 25394.39
[INFO 2017-06-26 11:40:28,441 main.py:50] epoch 136, training loss: 19544.78, average training loss: 29717.72, base loss: 25396.27
[INFO 2017-06-26 11:40:29,035 main.py:50] epoch 137, training loss: 21691.47, average training loss: 29659.56, base loss: 25417.24
[INFO 2017-06-26 11:40:29,627 main.py:50] epoch 138, training loss: 21097.11, average training loss: 29597.96, base loss: 25428.76
[INFO 2017-06-26 11:40:30,216 main.py:50] epoch 139, training loss: 17752.91, average training loss: 29513.35, base loss: 25413.23
[INFO 2017-06-26 11:40:30,804 main.py:50] epoch 140, training loss: 20945.01, average training loss: 29452.58, base loss: 25425.32
[INFO 2017-06-26 11:40:31,388 main.py:50] epoch 141, training loss: 18835.43, average training loss: 29377.81, base loss: 25420.80
[INFO 2017-06-26 11:40:31,975 main.py:50] epoch 142, training loss: 20549.44, average training loss: 29316.07, base loss: 25424.31
[INFO 2017-06-26 11:40:32,577 main.py:50] epoch 143, training loss: 18000.71, average training loss: 29237.50, base loss: 25417.51
[INFO 2017-06-26 11:40:33,166 main.py:50] epoch 144, training loss: 18173.66, average training loss: 29161.19, base loss: 25409.37
[INFO 2017-06-26 11:40:33,757 main.py:50] epoch 145, training loss: 18828.70, average training loss: 29090.42, base loss: 25410.17
[INFO 2017-06-26 11:40:34,346 main.py:50] epoch 146, training loss: 18543.65, average training loss: 29018.68, base loss: 25405.55
[INFO 2017-06-26 11:40:34,937 main.py:50] epoch 147, training loss: 21093.56, average training loss: 28965.13, base loss: 25422.20
[INFO 2017-06-26 11:40:35,528 main.py:50] epoch 148, training loss: 18325.07, average training loss: 28893.72, base loss: 25416.48
[INFO 2017-06-26 11:40:36,116 main.py:50] epoch 149, training loss: 20801.19, average training loss: 28839.77, base loss: 25429.40
[INFO 2017-06-26 11:40:36,704 main.py:50] epoch 150, training loss: 18229.06, average training loss: 28769.50, base loss: 25430.12
[INFO 2017-06-26 11:40:37,294 main.py:50] epoch 151, training loss: 19101.08, average training loss: 28705.89, base loss: 25437.59
[INFO 2017-06-26 11:40:37,882 main.py:50] epoch 152, training loss: 18518.90, average training loss: 28639.31, base loss: 25434.42
[INFO 2017-06-26 11:40:38,474 main.py:50] epoch 153, training loss: 18043.95, average training loss: 28570.51, base loss: 25423.96
[INFO 2017-06-26 11:40:39,066 main.py:50] epoch 154, training loss: 18395.26, average training loss: 28504.86, base loss: 25419.62
[INFO 2017-06-26 11:40:39,661 main.py:50] epoch 155, training loss: 18741.44, average training loss: 28442.27, base loss: 25423.72
[INFO 2017-06-26 11:40:40,250 main.py:50] epoch 156, training loss: 20840.79, average training loss: 28393.86, base loss: 25431.11
[INFO 2017-06-26 11:40:40,837 main.py:50] epoch 157, training loss: 17105.59, average training loss: 28322.41, base loss: 25414.75
[INFO 2017-06-26 11:40:41,422 main.py:50] epoch 158, training loss: 20574.73, average training loss: 28273.69, base loss: 25425.42
[INFO 2017-06-26 11:40:42,016 main.py:50] epoch 159, training loss: 18345.02, average training loss: 28211.63, base loss: 25419.86
[INFO 2017-06-26 11:40:42,611 main.py:50] epoch 160, training loss: 17707.71, average training loss: 28146.39, base loss: 25417.62
[INFO 2017-06-26 11:40:43,215 main.py:50] epoch 161, training loss: 18163.77, average training loss: 28084.77, base loss: 25413.62
[INFO 2017-06-26 11:40:43,827 main.py:50] epoch 162, training loss: 18031.90, average training loss: 28023.09, base loss: 25408.45
[INFO 2017-06-26 11:40:44,437 main.py:50] epoch 163, training loss: 16895.57, average training loss: 27955.24, base loss: 25389.19
[INFO 2017-06-26 11:40:45,046 main.py:50] epoch 164, training loss: 17657.29, average training loss: 27892.83, base loss: 25382.19
[INFO 2017-06-26 11:40:45,659 main.py:50] epoch 165, training loss: 20385.56, average training loss: 27847.61, base loss: 25392.01
[INFO 2017-06-26 11:40:46,269 main.py:50] epoch 166, training loss: 20524.79, average training loss: 27803.76, base loss: 25402.43
[INFO 2017-06-26 11:40:46,880 main.py:50] epoch 167, training loss: 18084.96, average training loss: 27745.91, base loss: 25405.62
[INFO 2017-06-26 11:40:47,488 main.py:50] epoch 168, training loss: 20245.98, average training loss: 27701.53, base loss: 25422.11
[INFO 2017-06-26 11:40:48,098 main.py:50] epoch 169, training loss: 17990.25, average training loss: 27644.40, base loss: 25415.05
[INFO 2017-06-26 11:40:48,706 main.py:50] epoch 170, training loss: 17526.85, average training loss: 27585.24, base loss: 25408.88
[INFO 2017-06-26 11:40:49,315 main.py:50] epoch 171, training loss: 17687.53, average training loss: 27527.69, base loss: 25407.57
[INFO 2017-06-26 11:40:49,927 main.py:50] epoch 172, training loss: 18314.53, average training loss: 27474.44, base loss: 25412.49
[INFO 2017-06-26 11:40:50,541 main.py:50] epoch 173, training loss: 16867.72, average training loss: 27413.48, base loss: 25399.25
[INFO 2017-06-26 11:40:51,153 main.py:50] epoch 174, training loss: 18468.77, average training loss: 27362.37, base loss: 25402.39
[INFO 2017-06-26 11:40:51,764 main.py:50] epoch 175, training loss: 18713.02, average training loss: 27313.22, base loss: 25406.97
[INFO 2017-06-26 11:40:52,374 main.py:50] epoch 176, training loss: 17128.35, average training loss: 27255.68, base loss: 25398.67
[INFO 2017-06-26 11:40:52,984 main.py:50] epoch 177, training loss: 18035.25, average training loss: 27203.88, base loss: 25395.23
[INFO 2017-06-26 11:40:53,593 main.py:50] epoch 178, training loss: 20201.10, average training loss: 27164.76, base loss: 25398.87
[INFO 2017-06-26 11:40:54,203 main.py:50] epoch 179, training loss: 20804.80, average training loss: 27129.43, base loss: 25408.60
[INFO 2017-06-26 11:40:54,812 main.py:50] epoch 180, training loss: 19724.35, average training loss: 27088.51, base loss: 25413.42
[INFO 2017-06-26 11:40:55,420 main.py:50] epoch 181, training loss: 19789.19, average training loss: 27048.41, base loss: 25415.75
[INFO 2017-06-26 11:40:56,029 main.py:50] epoch 182, training loss: 20244.41, average training loss: 27011.23, base loss: 25426.23
[INFO 2017-06-26 11:40:56,640 main.py:50] epoch 183, training loss: 19511.81, average training loss: 26970.47, base loss: 25430.41
[INFO 2017-06-26 11:40:57,256 main.py:50] epoch 184, training loss: 17304.13, average training loss: 26918.22, base loss: 25422.36
[INFO 2017-06-26 11:40:57,872 main.py:50] epoch 185, training loss: 17407.17, average training loss: 26867.08, base loss: 25417.18
[INFO 2017-06-26 11:40:58,488 main.py:50] epoch 186, training loss: 16938.65, average training loss: 26813.99, base loss: 25412.68
[INFO 2017-06-26 11:40:59,119 main.py:50] epoch 187, training loss: 21103.60, average training loss: 26783.62, base loss: 25430.63
[INFO 2017-06-26 11:40:59,734 main.py:50] epoch 188, training loss: 17830.55, average training loss: 26736.25, base loss: 25429.76
[INFO 2017-06-26 11:41:00,348 main.py:50] epoch 189, training loss: 21534.07, average training loss: 26708.87, base loss: 25456.39
[INFO 2017-06-26 11:41:00,963 main.py:50] epoch 190, training loss: 17857.99, average training loss: 26662.53, base loss: 25450.04
[INFO 2017-06-26 11:41:01,578 main.py:50] epoch 191, training loss: 20352.01, average training loss: 26629.66, base loss: 25462.52
[INFO 2017-06-26 11:41:02,196 main.py:50] epoch 192, training loss: 16523.62, average training loss: 26577.30, base loss: 25448.48
[INFO 2017-06-26 11:41:02,812 main.py:50] epoch 193, training loss: 15909.58, average training loss: 26522.31, base loss: 25429.06
[INFO 2017-06-26 11:41:03,429 main.py:50] epoch 194, training loss: 17666.22, average training loss: 26476.89, base loss: 25427.95
[INFO 2017-06-26 11:41:04,043 main.py:50] epoch 195, training loss: 20320.00, average training loss: 26445.48, base loss: 25436.99
[INFO 2017-06-26 11:41:04,656 main.py:50] epoch 196, training loss: 18238.12, average training loss: 26403.82, base loss: 25442.33
[INFO 2017-06-26 11:41:05,272 main.py:50] epoch 197, training loss: 17483.39, average training loss: 26358.77, base loss: 25439.45
[INFO 2017-06-26 11:41:05,886 main.py:50] epoch 198, training loss: 16934.25, average training loss: 26311.41, base loss: 25428.52
[INFO 2017-06-26 11:41:06,501 main.py:50] epoch 199, training loss: 17725.48, average training loss: 26268.48, base loss: 25430.33
[INFO 2017-06-26 11:41:07,116 main.py:50] epoch 200, training loss: 17322.22, average training loss: 26223.97, base loss: 25428.93
[INFO 2017-06-26 11:41:07,740 main.py:50] epoch 201, training loss: 15981.12, average training loss: 26173.26, base loss: 25413.21
[INFO 2017-06-26 11:41:08,360 main.py:50] epoch 202, training loss: 19809.31, average training loss: 26141.91, base loss: 25422.59
[INFO 2017-06-26 11:41:08,982 main.py:50] epoch 203, training loss: 15548.65, average training loss: 26089.98, base loss: 25403.65
[INFO 2017-06-26 11:41:09,604 main.py:50] epoch 204, training loss: 16605.30, average training loss: 26043.72, base loss: 25393.36
[INFO 2017-06-26 11:41:10,225 main.py:50] epoch 205, training loss: 17514.42, average training loss: 26002.31, base loss: 25391.75
[INFO 2017-06-26 11:41:10,846 main.py:50] epoch 206, training loss: 19629.69, average training loss: 25971.53, base loss: 25396.54
[INFO 2017-06-26 11:41:11,465 main.py:50] epoch 207, training loss: 16835.46, average training loss: 25927.60, base loss: 25392.31
[INFO 2017-06-26 11:41:12,092 main.py:50] epoch 208, training loss: 20490.26, average training loss: 25901.59, base loss: 25406.89
[INFO 2017-06-26 11:41:12,726 main.py:50] epoch 209, training loss: 18896.96, average training loss: 25868.23, base loss: 25418.27
[INFO 2017-06-26 11:41:13,361 main.py:50] epoch 210, training loss: 16992.22, average training loss: 25826.17, base loss: 25410.14
[INFO 2017-06-26 11:41:13,996 main.py:50] epoch 211, training loss: 19747.94, average training loss: 25797.49, base loss: 25418.63
[INFO 2017-06-26 11:41:14,631 main.py:50] epoch 212, training loss: 17254.29, average training loss: 25757.39, base loss: 25419.93
[INFO 2017-06-26 11:41:15,267 main.py:50] epoch 213, training loss: 17517.38, average training loss: 25718.88, base loss: 25422.58
[INFO 2017-06-26 11:41:15,903 main.py:50] epoch 214, training loss: 19806.49, average training loss: 25691.38, base loss: 25427.71
[INFO 2017-06-26 11:41:16,536 main.py:50] epoch 215, training loss: 19409.47, average training loss: 25662.30, base loss: 25429.13
[INFO 2017-06-26 11:41:17,171 main.py:50] epoch 216, training loss: 15908.39, average training loss: 25617.35, base loss: 25411.32
[INFO 2017-06-26 11:41:17,807 main.py:50] epoch 217, training loss: 20796.63, average training loss: 25595.24, base loss: 25428.39
[INFO 2017-06-26 11:41:18,449 main.py:50] epoch 218, training loss: 20272.11, average training loss: 25570.93, base loss: 25437.93
[INFO 2017-06-26 11:41:19,091 main.py:50] epoch 219, training loss: 15616.92, average training loss: 25525.68, base loss: 25418.19
[INFO 2017-06-26 11:41:19,733 main.py:50] epoch 220, training loss: 19720.61, average training loss: 25499.42, base loss: 25423.08
[INFO 2017-06-26 11:41:20,376 main.py:50] epoch 221, training loss: 19897.81, average training loss: 25474.18, base loss: 25433.05
[INFO 2017-06-26 11:41:21,021 main.py:50] epoch 222, training loss: 19899.40, average training loss: 25449.19, base loss: 25438.98
[INFO 2017-06-26 11:41:21,667 main.py:50] epoch 223, training loss: 17357.05, average training loss: 25413.06, base loss: 25436.69
[INFO 2017-06-26 11:41:22,311 main.py:50] epoch 224, training loss: 16896.60, average training loss: 25375.21, base loss: 25429.07
[INFO 2017-06-26 11:41:22,953 main.py:50] epoch 225, training loss: 17341.35, average training loss: 25339.66, base loss: 25429.62
[INFO 2017-06-26 11:41:23,593 main.py:50] epoch 226, training loss: 18684.27, average training loss: 25310.34, base loss: 25428.48
[INFO 2017-06-26 11:41:24,235 main.py:50] epoch 227, training loss: 16573.71, average training loss: 25272.02, base loss: 25420.21
[INFO 2017-06-26 11:41:24,881 main.py:50] epoch 228, training loss: 16911.65, average training loss: 25235.51, base loss: 25420.20
[INFO 2017-06-26 11:41:25,526 main.py:50] epoch 229, training loss: 20025.80, average training loss: 25212.86, base loss: 25431.85
[INFO 2017-06-26 11:41:26,170 main.py:50] epoch 230, training loss: 20181.47, average training loss: 25191.08, base loss: 25447.10
[INFO 2017-06-26 11:41:26,827 main.py:50] epoch 231, training loss: 19545.96, average training loss: 25166.75, base loss: 25454.85
[INFO 2017-06-26 11:41:27,470 main.py:50] epoch 232, training loss: 19743.79, average training loss: 25143.48, base loss: 25459.77
[INFO 2017-06-26 11:41:28,112 main.py:50] epoch 233, training loss: 17088.89, average training loss: 25109.05, base loss: 25455.81
[INFO 2017-06-26 11:41:28,755 main.py:50] epoch 234, training loss: 15661.18, average training loss: 25068.85, base loss: 25437.17
[INFO 2017-06-26 11:41:29,397 main.py:50] epoch 235, training loss: 16525.18, average training loss: 25032.65, base loss: 25430.71
[INFO 2017-06-26 11:41:30,040 main.py:50] epoch 236, training loss: 16956.82, average training loss: 24998.57, base loss: 25430.32
[INFO 2017-06-26 11:41:30,684 main.py:50] epoch 237, training loss: 20267.89, average training loss: 24978.70, base loss: 25444.37
[INFO 2017-06-26 11:41:31,328 main.py:50] epoch 238, training loss: 16195.60, average training loss: 24941.95, base loss: 25438.42
[INFO 2017-06-26 11:41:31,970 main.py:50] epoch 239, training loss: 16954.44, average training loss: 24908.67, base loss: 25435.22
[INFO 2017-06-26 11:41:32,613 main.py:50] epoch 240, training loss: 17050.07, average training loss: 24876.06, base loss: 25436.76
[INFO 2017-06-26 11:41:33,256 main.py:50] epoch 241, training loss: 19811.11, average training loss: 24855.13, base loss: 25446.16
[INFO 2017-06-26 11:41:33,898 main.py:50] epoch 242, training loss: 20491.34, average training loss: 24837.17, base loss: 25462.28
[INFO 2017-06-26 11:41:34,540 main.py:50] epoch 243, training loss: 16454.03, average training loss: 24802.81, base loss: 25458.09
[INFO 2017-06-26 11:41:35,182 main.py:50] epoch 244, training loss: 21057.74, average training loss: 24787.53, base loss: 25480.57
[INFO 2017-06-26 11:41:35,821 main.py:50] epoch 245, training loss: 19354.48, average training loss: 24765.44, base loss: 25485.01
[INFO 2017-06-26 11:41:36,463 main.py:50] epoch 246, training loss: 19625.88, average training loss: 24744.63, base loss: 25493.63
[INFO 2017-06-26 11:41:37,106 main.py:50] epoch 247, training loss: 16872.43, average training loss: 24712.89, base loss: 25493.31
[INFO 2017-06-26 11:41:37,747 main.py:50] epoch 248, training loss: 19300.13, average training loss: 24691.15, base loss: 25499.21
[INFO 2017-06-26 11:41:38,386 main.py:50] epoch 249, training loss: 19839.07, average training loss: 24671.74, base loss: 25505.92
[INFO 2017-06-26 11:41:39,027 main.py:50] epoch 250, training loss: 19287.70, average training loss: 24650.29, base loss: 25512.73
[INFO 2017-06-26 11:41:39,671 main.py:50] epoch 251, training loss: 16314.23, average training loss: 24617.21, base loss: 25504.92
[INFO 2017-06-26 11:41:40,314 main.py:50] epoch 252, training loss: 15959.26, average training loss: 24582.99, base loss: 25493.78
[INFO 2017-06-26 11:41:40,957 main.py:50] epoch 253, training loss: 15477.04, average training loss: 24547.14, base loss: 25480.13
[INFO 2017-06-26 11:41:41,598 main.py:50] epoch 254, training loss: 16779.49, average training loss: 24516.68, base loss: 25477.99
[INFO 2017-06-26 11:41:42,239 main.py:50] epoch 255, training loss: 16780.57, average training loss: 24486.46, base loss: 25476.04
[INFO 2017-06-26 11:41:42,882 main.py:50] epoch 256, training loss: 17022.67, average training loss: 24457.42, base loss: 25476.51
[INFO 2017-06-26 11:41:43,525 main.py:50] epoch 257, training loss: 15795.20, average training loss: 24423.85, base loss: 25468.49
[INFO 2017-06-26 11:41:44,168 main.py:50] epoch 258, training loss: 19479.47, average training loss: 24404.76, base loss: 25475.14
[INFO 2017-06-26 11:41:44,811 main.py:50] epoch 259, training loss: 15543.99, average training loss: 24370.68, base loss: 25467.26
[INFO 2017-06-26 11:41:45,452 main.py:50] epoch 260, training loss: 20216.28, average training loss: 24354.76, base loss: 25481.48
[INFO 2017-06-26 11:41:46,093 main.py:50] epoch 261, training loss: 18068.05, average training loss: 24330.76, base loss: 25488.37
[INFO 2017-06-26 11:41:46,737 main.py:50] epoch 262, training loss: 18899.79, average training loss: 24310.11, base loss: 25493.01
[INFO 2017-06-26 11:41:47,379 main.py:50] epoch 263, training loss: 17639.90, average training loss: 24284.85, base loss: 25500.22
[INFO 2017-06-26 11:41:48,021 main.py:50] epoch 264, training loss: 19756.09, average training loss: 24267.76, base loss: 25514.64
[INFO 2017-06-26 11:41:48,664 main.py:50] epoch 265, training loss: 20082.35, average training loss: 24252.02, base loss: 25531.09
[INFO 2017-06-26 11:41:49,306 main.py:50] epoch 266, training loss: 19134.36, average training loss: 24232.86, base loss: 25531.66
[INFO 2017-06-26 11:41:49,958 main.py:50] epoch 267, training loss: 17760.84, average training loss: 24208.71, base loss: 25539.58
[INFO 2017-06-26 11:41:50,612 main.py:50] epoch 268, training loss: 18981.97, average training loss: 24189.28, base loss: 25545.57
[INFO 2017-06-26 11:41:51,262 main.py:50] epoch 269, training loss: 16002.03, average training loss: 24158.95, base loss: 25535.35
[INFO 2017-06-26 11:41:51,915 main.py:50] epoch 270, training loss: 14784.35, average training loss: 24124.36, base loss: 25519.38
[INFO 2017-06-26 11:41:52,566 main.py:50] epoch 271, training loss: 19154.63, average training loss: 24106.09, base loss: 25525.55
[INFO 2017-06-26 11:41:53,216 main.py:50] epoch 272, training loss: 16428.50, average training loss: 24077.97, base loss: 25525.28
[INFO 2017-06-26 11:41:53,868 main.py:50] epoch 273, training loss: 15611.59, average training loss: 24047.07, base loss: 25516.90
[INFO 2017-06-26 11:41:54,521 main.py:50] epoch 274, training loss: 15512.90, average training loss: 24016.03, base loss: 25508.48
[INFO 2017-06-26 11:41:55,189 main.py:50] epoch 275, training loss: 19392.09, average training loss: 23999.28, base loss: 25520.68
[INFO 2017-06-26 11:41:55,843 main.py:50] epoch 276, training loss: 19084.85, average training loss: 23981.54, base loss: 25526.53
[INFO 2017-06-26 11:41:56,504 main.py:50] epoch 277, training loss: 14787.01, average training loss: 23948.47, base loss: 25507.81
[INFO 2017-06-26 11:41:57,164 main.py:50] epoch 278, training loss: 16663.32, average training loss: 23922.35, base loss: 25508.10
[INFO 2017-06-26 11:41:57,825 main.py:50] epoch 279, training loss: 18656.93, average training loss: 23903.55, base loss: 25514.01
[INFO 2017-06-26 11:41:58,486 main.py:50] epoch 280, training loss: 16597.29, average training loss: 23877.55, base loss: 25516.50
[INFO 2017-06-26 11:41:59,148 main.py:50] epoch 281, training loss: 16053.05, average training loss: 23849.80, base loss: 25514.03
[INFO 2017-06-26 11:41:59,808 main.py:50] epoch 282, training loss: 16454.66, average training loss: 23823.67, base loss: 25512.14
[INFO 2017-06-26 11:42:00,468 main.py:50] epoch 283, training loss: 18891.27, average training loss: 23806.30, base loss: 25514.86
[INFO 2017-06-26 11:42:01,129 main.py:50] epoch 284, training loss: 16873.31, average training loss: 23781.98, base loss: 25516.43
[INFO 2017-06-26 11:42:01,789 main.py:50] epoch 285, training loss: 16543.54, average training loss: 23756.67, base loss: 25517.13
[INFO 2017-06-26 11:42:02,450 main.py:50] epoch 286, training loss: 16352.74, average training loss: 23730.87, base loss: 25515.72
[INFO 2017-06-26 11:42:03,109 main.py:50] epoch 287, training loss: 18303.48, average training loss: 23712.02, base loss: 25517.67
[INFO 2017-06-26 11:42:03,770 main.py:50] epoch 288, training loss: 15596.76, average training loss: 23683.94, base loss: 25507.54
[INFO 2017-06-26 11:42:04,428 main.py:50] epoch 289, training loss: 16199.37, average training loss: 23658.14, base loss: 25505.83
[INFO 2017-06-26 11:42:05,089 main.py:50] epoch 290, training loss: 19383.63, average training loss: 23643.45, base loss: 25516.07
[INFO 2017-06-26 11:42:05,748 main.py:50] epoch 291, training loss: 16147.12, average training loss: 23617.77, base loss: 25513.72
[INFO 2017-06-26 11:42:06,408 main.py:50] epoch 292, training loss: 14946.78, average training loss: 23588.18, base loss: 25503.67
[INFO 2017-06-26 11:42:07,069 main.py:50] epoch 293, training loss: 17496.18, average training loss: 23567.46, base loss: 25510.83
[INFO 2017-06-26 11:42:07,729 main.py:50] epoch 294, training loss: 18726.56, average training loss: 23551.05, base loss: 25516.87
[INFO 2017-06-26 11:42:08,388 main.py:50] epoch 295, training loss: 19791.27, average training loss: 23538.35, base loss: 25525.81
[INFO 2017-06-26 11:42:09,051 main.py:50] epoch 296, training loss: 16234.77, average training loss: 23513.76, base loss: 25526.18
[INFO 2017-06-26 11:42:09,711 main.py:50] epoch 297, training loss: 15617.50, average training loss: 23487.26, base loss: 25520.19
[INFO 2017-06-26 11:42:10,370 main.py:50] epoch 298, training loss: 19886.73, average training loss: 23475.22, base loss: 25534.17
[INFO 2017-06-26 11:42:11,031 main.py:50] epoch 299, training loss: 15640.96, average training loss: 23449.10, base loss: 25527.83
[INFO 2017-06-26 11:42:11,691 main.py:50] epoch 300, training loss: 16303.33, average training loss: 23425.36, base loss: 25527.69
[INFO 2017-06-26 11:42:12,351 main.py:50] epoch 301, training loss: 15301.05, average training loss: 23398.46, base loss: 25520.70
[INFO 2017-06-26 11:42:13,011 main.py:50] epoch 302, training loss: 17896.11, average training loss: 23380.30, base loss: 25519.70
[INFO 2017-06-26 11:42:13,673 main.py:50] epoch 303, training loss: 18508.28, average training loss: 23364.27, base loss: 25525.14
[INFO 2017-06-26 11:42:14,333 main.py:50] epoch 304, training loss: 18889.17, average training loss: 23349.60, base loss: 25532.29
[INFO 2017-06-26 11:42:14,994 main.py:50] epoch 305, training loss: 15351.34, average training loss: 23323.46, base loss: 25524.99
[INFO 2017-06-26 11:42:15,655 main.py:50] epoch 306, training loss: 15258.67, average training loss: 23297.19, base loss: 25519.67
[INFO 2017-06-26 11:42:16,316 main.py:50] epoch 307, training loss: 15740.94, average training loss: 23272.66, base loss: 25515.22
[INFO 2017-06-26 11:42:16,976 main.py:50] epoch 308, training loss: 16614.55, average training loss: 23251.11, base loss: 25516.94
[INFO 2017-06-26 11:42:17,638 main.py:50] epoch 309, training loss: 16026.69, average training loss: 23227.81, base loss: 25517.32
[INFO 2017-06-26 11:42:18,298 main.py:50] epoch 310, training loss: 17894.55, average training loss: 23210.66, base loss: 25517.52
[INFO 2017-06-26 11:42:18,960 main.py:50] epoch 311, training loss: 18476.40, average training loss: 23195.49, base loss: 25521.50
[INFO 2017-06-26 11:42:19,622 main.py:50] epoch 312, training loss: 15544.94, average training loss: 23171.04, base loss: 25519.61
[INFO 2017-06-26 11:42:20,282 main.py:50] epoch 313, training loss: 19369.73, average training loss: 23158.94, base loss: 25528.72
[INFO 2017-06-26 11:42:20,943 main.py:50] epoch 314, training loss: 16411.02, average training loss: 23137.52, base loss: 25529.50
[INFO 2017-06-26 11:42:21,604 main.py:50] epoch 315, training loss: 16487.08, average training loss: 23116.47, base loss: 25534.29
[INFO 2017-06-26 11:42:22,266 main.py:50] epoch 316, training loss: 19040.69, average training loss: 23103.61, base loss: 25541.60
[INFO 2017-06-26 11:42:22,927 main.py:50] epoch 317, training loss: 15612.35, average training loss: 23080.06, base loss: 25540.70
[INFO 2017-06-26 11:42:23,588 main.py:50] epoch 318, training loss: 18691.21, average training loss: 23066.30, base loss: 25548.93
[INFO 2017-06-26 11:42:24,261 main.py:50] epoch 319, training loss: 15266.92, average training loss: 23041.92, base loss: 25543.67
[INFO 2017-06-26 11:42:24,923 main.py:50] epoch 320, training loss: 15181.23, average training loss: 23017.44, base loss: 25537.39
[INFO 2017-06-26 11:42:25,585 main.py:50] epoch 321, training loss: 18987.88, average training loss: 23004.92, base loss: 25544.20
[INFO 2017-06-26 11:42:26,243 main.py:50] epoch 322, training loss: 19234.42, average training loss: 22993.25, base loss: 25556.97
[INFO 2017-06-26 11:42:26,904 main.py:50] epoch 323, training loss: 16210.59, average training loss: 22972.31, base loss: 25558.25
[INFO 2017-06-26 11:42:27,564 main.py:50] epoch 324, training loss: 15133.08, average training loss: 22948.19, base loss: 25551.62
[INFO 2017-06-26 11:42:28,223 main.py:50] epoch 325, training loss: 19149.27, average training loss: 22936.54, base loss: 25561.15
[INFO 2017-06-26 11:42:28,884 main.py:50] epoch 326, training loss: 15214.67, average training loss: 22912.93, base loss: 25554.97
[INFO 2017-06-26 11:42:29,544 main.py:50] epoch 327, training loss: 16217.04, average training loss: 22892.51, base loss: 25556.65
[INFO 2017-06-26 11:42:30,202 main.py:50] epoch 328, training loss: 18111.53, average training loss: 22877.98, base loss: 25553.68
[INFO 2017-06-26 11:42:30,864 main.py:50] epoch 329, training loss: 17542.46, average training loss: 22861.81, base loss: 25550.81
[INFO 2017-06-26 11:42:31,523 main.py:50] epoch 330, training loss: 14792.52, average training loss: 22837.43, base loss: 25538.89
[INFO 2017-06-26 11:42:32,183 main.py:50] epoch 331, training loss: 15754.73, average training loss: 22816.10, base loss: 25535.95
[INFO 2017-06-26 11:42:32,845 main.py:50] epoch 332, training loss: 18536.06, average training loss: 22803.25, base loss: 25542.02
[INFO 2017-06-26 11:42:33,506 main.py:50] epoch 333, training loss: 17691.67, average training loss: 22787.94, base loss: 25541.55
[INFO 2017-06-26 11:42:34,167 main.py:50] epoch 334, training loss: 16222.92, average training loss: 22768.35, base loss: 25543.48
[INFO 2017-06-26 11:42:34,828 main.py:50] epoch 335, training loss: 14296.88, average training loss: 22743.13, base loss: 25532.37
[INFO 2017-06-26 11:42:35,489 main.py:50] epoch 336, training loss: 15374.18, average training loss: 22721.27, base loss: 25528.95
[INFO 2017-06-26 11:42:36,150 main.py:50] epoch 337, training loss: 14631.70, average training loss: 22697.33, base loss: 25518.03
[INFO 2017-06-26 11:42:36,813 main.py:50] epoch 338, training loss: 15606.15, average training loss: 22676.41, base loss: 25517.10
[INFO 2017-06-26 11:42:37,473 main.py:50] epoch 339, training loss: 15730.11, average training loss: 22655.98, base loss: 25513.44
[INFO 2017-06-26 11:42:38,137 main.py:50] epoch 340, training loss: 16551.63, average training loss: 22638.08, base loss: 25516.61
[INFO 2017-06-26 11:42:38,798 main.py:50] epoch 341, training loss: 15303.72, average training loss: 22616.64, base loss: 25512.65
[INFO 2017-06-26 11:42:39,461 main.py:50] epoch 342, training loss: 16191.05, average training loss: 22597.90, base loss: 25512.41
[INFO 2017-06-26 11:42:40,133 main.py:50] epoch 343, training loss: 15357.33, average training loss: 22576.86, base loss: 25508.44
[INFO 2017-06-26 11:42:40,804 main.py:50] epoch 344, training loss: 15440.35, average training loss: 22556.17, base loss: 25505.36
[INFO 2017-06-26 11:42:41,476 main.py:50] epoch 345, training loss: 16783.34, average training loss: 22539.49, base loss: 25509.85
[INFO 2017-06-26 11:42:42,146 main.py:50] epoch 346, training loss: 18806.67, average training loss: 22528.73, base loss: 25519.98
[INFO 2017-06-26 11:42:42,818 main.py:50] epoch 347, training loss: 18109.16, average training loss: 22516.03, base loss: 25521.25
[INFO 2017-06-26 11:42:43,508 main.py:50] epoch 348, training loss: 18579.97, average training loss: 22504.75, base loss: 25526.81
[INFO 2017-06-26 11:42:44,196 main.py:50] epoch 349, training loss: 18938.14, average training loss: 22494.56, base loss: 25537.49
[INFO 2017-06-26 11:42:44,883 main.py:50] epoch 350, training loss: 18497.10, average training loss: 22483.17, base loss: 25543.20
[INFO 2017-06-26 11:42:45,570 main.py:50] epoch 351, training loss: 19118.13, average training loss: 22473.61, base loss: 25553.50
[INFO 2017-06-26 11:42:46,258 main.py:50] epoch 352, training loss: 15401.83, average training loss: 22453.58, base loss: 25548.90
[INFO 2017-06-26 11:42:46,946 main.py:50] epoch 353, training loss: 19249.41, average training loss: 22444.53, base loss: 25560.25
[INFO 2017-06-26 11:42:47,634 main.py:50] epoch 354, training loss: 18292.31, average training loss: 22432.83, base loss: 25563.94
[INFO 2017-06-26 11:42:48,332 main.py:50] epoch 355, training loss: 14992.42, average training loss: 22411.93, base loss: 25558.13
[INFO 2017-06-26 11:42:49,031 main.py:50] epoch 356, training loss: 16464.63, average training loss: 22395.27, base loss: 25561.64
[INFO 2017-06-26 11:42:49,730 main.py:50] epoch 357, training loss: 15418.35, average training loss: 22375.78, base loss: 25558.17
[INFO 2017-06-26 11:42:50,428 main.py:50] epoch 358, training loss: 18661.98, average training loss: 22365.44, base loss: 25566.67
[INFO 2017-06-26 11:42:51,127 main.py:50] epoch 359, training loss: 18708.63, average training loss: 22355.28, base loss: 25575.25
[INFO 2017-06-26 11:42:51,824 main.py:50] epoch 360, training loss: 18357.88, average training loss: 22344.21, base loss: 25580.05
[INFO 2017-06-26 11:42:52,520 main.py:50] epoch 361, training loss: 15131.70, average training loss: 22324.28, base loss: 25578.60
[INFO 2017-06-26 11:42:53,218 main.py:50] epoch 362, training loss: 15569.18, average training loss: 22305.67, base loss: 25577.30
[INFO 2017-06-26 11:42:53,930 main.py:50] epoch 363, training loss: 15424.31, average training loss: 22286.77, base loss: 25578.64
[INFO 2017-06-26 11:42:54,627 main.py:50] epoch 364, training loss: 16202.29, average training loss: 22270.10, base loss: 25579.22
[INFO 2017-06-26 11:42:55,326 main.py:50] epoch 365, training loss: 15150.66, average training loss: 22250.65, base loss: 25573.46
[INFO 2017-06-26 11:42:56,024 main.py:50] epoch 366, training loss: 16194.13, average training loss: 22234.14, base loss: 25575.99
[INFO 2017-06-26 11:42:56,721 main.py:50] epoch 367, training loss: 18500.84, average training loss: 22224.00, base loss: 25581.68
[INFO 2017-06-26 11:42:57,420 main.py:50] epoch 368, training loss: 18723.91, average training loss: 22214.51, base loss: 25588.72
[INFO 2017-06-26 11:42:58,117 main.py:50] epoch 369, training loss: 15982.86, average training loss: 22197.67, base loss: 25588.05
[INFO 2017-06-26 11:42:58,814 main.py:50] epoch 370, training loss: 18075.68, average training loss: 22186.56, base loss: 25590.41
[INFO 2017-06-26 11:42:59,510 main.py:50] epoch 371, training loss: 18867.33, average training loss: 22177.64, base loss: 25598.79
[INFO 2017-06-26 11:43:00,209 main.py:50] epoch 372, training loss: 18235.51, average training loss: 22167.07, base loss: 25604.08
[INFO 2017-06-26 11:43:00,908 main.py:50] epoch 373, training loss: 18392.40, average training loss: 22156.98, base loss: 25609.14
[INFO 2017-06-26 11:43:01,605 main.py:50] epoch 374, training loss: 18185.16, average training loss: 22146.39, base loss: 25613.73
[INFO 2017-06-26 11:43:02,302 main.py:50] epoch 375, training loss: 15190.76, average training loss: 22127.89, base loss: 25610.44
[INFO 2017-06-26 11:43:02,998 main.py:50] epoch 376, training loss: 18969.42, average training loss: 22119.51, base loss: 25620.21
[INFO 2017-06-26 11:43:03,696 main.py:50] epoch 377, training loss: 15405.12, average training loss: 22101.75, base loss: 25618.97
[INFO 2017-06-26 11:43:04,395 main.py:50] epoch 378, training loss: 14823.03, average training loss: 22082.54, base loss: 25613.62
[INFO 2017-06-26 11:43:05,093 main.py:50] epoch 379, training loss: 14571.83, average training loss: 22062.78, base loss: 25606.84
[INFO 2017-06-26 11:43:05,792 main.py:50] epoch 380, training loss: 18371.11, average training loss: 22053.09, base loss: 25611.89
[INFO 2017-06-26 11:43:06,492 main.py:50] epoch 381, training loss: 18385.13, average training loss: 22043.48, base loss: 25619.62
[INFO 2017-06-26 11:43:07,210 main.py:50] epoch 382, training loss: 14648.56, average training loss: 22024.18, base loss: 25611.53
[INFO 2017-06-26 11:43:07,929 main.py:50] epoch 383, training loss: 18625.52, average training loss: 22015.33, base loss: 25619.78
[INFO 2017-06-26 11:43:08,645 main.py:50] epoch 384, training loss: 16435.64, average training loss: 22000.83, base loss: 25621.32
[INFO 2017-06-26 11:43:09,362 main.py:50] epoch 385, training loss: 15858.99, average training loss: 21984.92, base loss: 25624.58
[INFO 2017-06-26 11:43:10,080 main.py:50] epoch 386, training loss: 17649.87, average training loss: 21973.72, base loss: 25625.29
[INFO 2017-06-26 11:43:10,800 main.py:50] epoch 387, training loss: 15416.52, average training loss: 21956.82, base loss: 25624.17
[INFO 2017-06-26 11:43:11,529 main.py:50] epoch 388, training loss: 15134.24, average training loss: 21939.28, base loss: 25619.19
[INFO 2017-06-26 11:43:12,261 main.py:50] epoch 389, training loss: 14457.65, average training loss: 21920.10, base loss: 25612.44
[INFO 2017-06-26 11:43:12,990 main.py:50] epoch 390, training loss: 15073.37, average training loss: 21902.59, base loss: 25609.25
[INFO 2017-06-26 11:43:13,719 main.py:50] epoch 391, training loss: 15270.51, average training loss: 21885.67, base loss: 25605.87
[INFO 2017-06-26 11:43:14,451 main.py:50] epoch 392, training loss: 16073.65, average training loss: 21870.88, base loss: 25609.75
[INFO 2017-06-26 11:43:15,180 main.py:50] epoch 393, training loss: 18165.79, average training loss: 21861.48, base loss: 25613.32
[INFO 2017-06-26 11:43:15,909 main.py:50] epoch 394, training loss: 14831.70, average training loss: 21843.68, base loss: 25607.10
[INFO 2017-06-26 11:43:16,639 main.py:50] epoch 395, training loss: 15893.37, average training loss: 21828.65, base loss: 25608.15
[INFO 2017-06-26 11:43:17,370 main.py:50] epoch 396, training loss: 18193.98, average training loss: 21819.50, base loss: 25614.09
[INFO 2017-06-26 11:43:18,101 main.py:50] epoch 397, training loss: 18262.63, average training loss: 21810.56, base loss: 25617.07
[INFO 2017-06-26 11:43:18,832 main.py:50] epoch 398, training loss: 14816.65, average training loss: 21793.03, base loss: 25611.67
[INFO 2017-06-26 11:43:19,562 main.py:50] epoch 399, training loss: 14600.58, average training loss: 21775.05, base loss: 25603.42
[INFO 2017-06-26 11:43:20,295 main.py:50] epoch 400, training loss: 14470.03, average training loss: 21756.83, base loss: 25597.30
[INFO 2017-06-26 11:43:21,025 main.py:50] epoch 401, training loss: 17443.89, average training loss: 21746.11, base loss: 25597.23
[INFO 2017-06-26 11:43:21,756 main.py:50] epoch 402, training loss: 14651.81, average training loss: 21728.50, base loss: 25590.50
[INFO 2017-06-26 11:43:22,487 main.py:50] epoch 403, training loss: 14788.91, average training loss: 21711.32, base loss: 25587.03
[INFO 2017-06-26 11:43:23,217 main.py:50] epoch 404, training loss: 17983.17, average training loss: 21702.12, base loss: 25590.05
[INFO 2017-06-26 11:43:23,945 main.py:50] epoch 405, training loss: 18338.79, average training loss: 21693.83, base loss: 25596.81
[INFO 2017-06-26 11:43:24,674 main.py:50] epoch 406, training loss: 15041.69, average training loss: 21677.49, base loss: 25595.86
[INFO 2017-06-26 11:43:25,416 main.py:50] epoch 407, training loss: 17773.90, average training loss: 21667.92, base loss: 25598.24
[INFO 2017-06-26 11:43:26,145 main.py:50] epoch 408, training loss: 18746.19, average training loss: 21660.78, base loss: 25607.29
[INFO 2017-06-26 11:43:26,874 main.py:50] epoch 409, training loss: 17376.16, average training loss: 21650.33, base loss: 25607.49
[INFO 2017-06-26 11:43:27,602 main.py:50] epoch 410, training loss: 15757.89, average training loss: 21635.99, base loss: 25606.82
[INFO 2017-06-26 11:43:28,331 main.py:50] epoch 411, training loss: 16605.01, average training loss: 21623.78, base loss: 25613.39
[INFO 2017-06-26 11:43:29,060 main.py:50] epoch 412, training loss: 14884.58, average training loss: 21607.46, base loss: 25608.56
[INFO 2017-06-26 11:43:29,789 main.py:50] epoch 413, training loss: 14686.18, average training loss: 21590.75, base loss: 25602.18
[INFO 2017-06-26 11:43:30,518 main.py:50] epoch 414, training loss: 15800.58, average training loss: 21576.79, base loss: 25599.36
[INFO 2017-06-26 11:43:31,246 main.py:50] epoch 415, training loss: 15330.38, average training loss: 21561.78, base loss: 25595.69
[INFO 2017-06-26 11:43:31,976 main.py:50] epoch 416, training loss: 15741.01, average training loss: 21547.82, base loss: 25595.40
[INFO 2017-06-26 11:43:32,705 main.py:50] epoch 417, training loss: 15543.48, average training loss: 21533.45, base loss: 25592.96
[INFO 2017-06-26 11:43:33,434 main.py:50] epoch 418, training loss: 17295.64, average training loss: 21523.34, base loss: 25593.31
[INFO 2017-06-26 11:43:34,163 main.py:50] epoch 419, training loss: 17801.45, average training loss: 21514.48, base loss: 25594.94
[INFO 2017-06-26 11:43:34,892 main.py:50] epoch 420, training loss: 14910.83, average training loss: 21498.79, base loss: 25589.19
[INFO 2017-06-26 11:43:35,622 main.py:50] epoch 421, training loss: 15104.98, average training loss: 21483.64, base loss: 25586.35
[INFO 2017-06-26 11:43:36,351 main.py:50] epoch 422, training loss: 15567.88, average training loss: 21469.66, base loss: 25586.46
[INFO 2017-06-26 11:43:37,080 main.py:50] epoch 423, training loss: 17179.39, average training loss: 21459.54, base loss: 25588.16
[INFO 2017-06-26 11:43:37,813 main.py:50] epoch 424, training loss: 15259.95, average training loss: 21444.95, base loss: 25583.68
[INFO 2017-06-26 11:43:38,546 main.py:50] epoch 425, training loss: 14500.39, average training loss: 21428.65, base loss: 25577.56
[INFO 2017-06-26 11:43:39,278 main.py:50] epoch 426, training loss: 18178.12, average training loss: 21421.04, base loss: 25580.73
[INFO 2017-06-26 11:43:40,010 main.py:50] epoch 427, training loss: 15510.20, average training loss: 21407.23, base loss: 25578.39
[INFO 2017-06-26 11:43:40,743 main.py:50] epoch 428, training loss: 15524.63, average training loss: 21393.51, base loss: 25578.53
[INFO 2017-06-26 11:43:41,476 main.py:50] epoch 429, training loss: 16466.68, average training loss: 21382.06, base loss: 25582.06
[INFO 2017-06-26 11:43:42,208 main.py:50] epoch 430, training loss: 18587.51, average training loss: 21375.57, base loss: 25590.19
[INFO 2017-06-26 11:43:42,940 main.py:50] epoch 431, training loss: 17543.74, average training loss: 21366.70, base loss: 25591.94
[INFO 2017-06-26 11:43:43,674 main.py:50] epoch 432, training loss: 17472.70, average training loss: 21357.71, base loss: 25593.53
[INFO 2017-06-26 11:43:44,406 main.py:50] epoch 433, training loss: 15291.93, average training loss: 21343.73, base loss: 25591.42
[INFO 2017-06-26 11:43:45,138 main.py:50] epoch 434, training loss: 14027.58, average training loss: 21326.91, base loss: 25581.94
[INFO 2017-06-26 11:43:45,870 main.py:50] epoch 435, training loss: 15692.99, average training loss: 21313.99, base loss: 25579.72
[INFO 2017-06-26 11:43:46,603 main.py:50] epoch 436, training loss: 15171.37, average training loss: 21299.94, base loss: 25577.59
[INFO 2017-06-26 11:43:47,335 main.py:50] epoch 437, training loss: 17847.80, average training loss: 21292.05, base loss: 25582.29
[INFO 2017-06-26 11:43:48,068 main.py:50] epoch 438, training loss: 16198.17, average training loss: 21280.45, base loss: 25586.22
[INFO 2017-06-26 11:43:48,801 main.py:50] epoch 439, training loss: 15678.40, average training loss: 21267.72, base loss: 25588.07
[INFO 2017-06-26 11:43:49,533 main.py:50] epoch 440, training loss: 15123.43, average training loss: 21253.79, base loss: 25587.72
[INFO 2017-06-26 11:43:50,265 main.py:50] epoch 441, training loss: 14690.12, average training loss: 21238.94, base loss: 25582.13
[INFO 2017-06-26 11:43:50,996 main.py:50] epoch 442, training loss: 16089.26, average training loss: 21227.31, base loss: 25583.17
[INFO 2017-06-26 11:43:51,729 main.py:50] epoch 443, training loss: 17693.49, average training loss: 21219.35, base loss: 25585.99
[INFO 2017-06-26 11:43:52,463 main.py:50] epoch 444, training loss: 14398.65, average training loss: 21204.03, base loss: 25581.23
[INFO 2017-06-26 11:43:53,195 main.py:50] epoch 445, training loss: 14948.46, average training loss: 21190.00, base loss: 25578.28
[INFO 2017-06-26 11:43:53,927 main.py:50] epoch 446, training loss: 15150.72, average training loss: 21176.49, base loss: 25574.34
[INFO 2017-06-26 11:43:54,660 main.py:50] epoch 447, training loss: 14678.03, average training loss: 21161.98, base loss: 25569.46
[INFO 2017-06-26 11:43:55,393 main.py:50] epoch 448, training loss: 13931.00, average training loss: 21145.88, base loss: 25563.69
[INFO 2017-06-26 11:43:56,125 main.py:50] epoch 449, training loss: 15547.58, average training loss: 21133.44, base loss: 25566.52
[INFO 2017-06-26 11:43:56,858 main.py:50] epoch 450, training loss: 15651.30, average training loss: 21121.28, base loss: 25568.56
[INFO 2017-06-26 11:43:57,606 main.py:50] epoch 451, training loss: 15228.22, average training loss: 21108.24, base loss: 25567.96
[INFO 2017-06-26 11:43:58,339 main.py:50] epoch 452, training loss: 14192.78, average training loss: 21092.98, base loss: 25560.43
[INFO 2017-06-26 11:43:59,073 main.py:50] epoch 453, training loss: 17266.79, average training loss: 21084.55, base loss: 25562.15
[INFO 2017-06-26 11:43:59,806 main.py:50] epoch 454, training loss: 16589.62, average training loss: 21074.67, base loss: 25561.59
[INFO 2017-06-26 11:44:00,539 main.py:50] epoch 455, training loss: 17206.18, average training loss: 21066.19, base loss: 25568.66
[INFO 2017-06-26 11:44:01,270 main.py:50] epoch 456, training loss: 15325.04, average training loss: 21053.63, base loss: 25565.57
[INFO 2017-06-26 11:44:02,002 main.py:50] epoch 457, training loss: 15500.30, average training loss: 21041.50, base loss: 25564.86
[INFO 2017-06-26 11:44:02,734 main.py:50] epoch 458, training loss: 14407.75, average training loss: 21027.05, base loss: 25559.57
[INFO 2017-06-26 11:44:03,466 main.py:50] epoch 459, training loss: 15810.79, average training loss: 21015.71, base loss: 25559.66
[INFO 2017-06-26 11:44:04,198 main.py:50] epoch 460, training loss: 15265.54, average training loss: 21003.24, base loss: 25558.41
[INFO 2017-06-26 11:44:04,931 main.py:50] epoch 461, training loss: 18730.17, average training loss: 20998.32, base loss: 25565.10
[INFO 2017-06-26 11:44:05,663 main.py:50] epoch 462, training loss: 18324.08, average training loss: 20992.54, base loss: 25570.75
[INFO 2017-06-26 11:44:06,396 main.py:50] epoch 463, training loss: 14244.74, average training loss: 20978.00, base loss: 25564.71
[INFO 2017-06-26 11:44:07,127 main.py:50] epoch 464, training loss: 15456.60, average training loss: 20966.12, base loss: 25565.23
[INFO 2017-06-26 11:44:07,860 main.py:50] epoch 465, training loss: 14956.85, average training loss: 20953.23, base loss: 25563.53
[INFO 2017-06-26 11:44:08,593 main.py:50] epoch 466, training loss: 15357.30, average training loss: 20941.24, base loss: 25563.02
[INFO 2017-06-26 11:44:09,326 main.py:50] epoch 467, training loss: 15131.34, average training loss: 20928.83, base loss: 25561.97
[INFO 2017-06-26 11:44:10,059 main.py:50] epoch 468, training loss: 14555.18, average training loss: 20915.24, base loss: 25558.14
[INFO 2017-06-26 11:44:10,791 main.py:50] epoch 469, training loss: 15219.14, average training loss: 20903.12, base loss: 25557.71
[INFO 2017-06-26 11:44:11,522 main.py:50] epoch 470, training loss: 15390.58, average training loss: 20891.42, base loss: 25557.00
[INFO 2017-06-26 11:44:12,255 main.py:50] epoch 471, training loss: 14722.31, average training loss: 20878.35, base loss: 25555.02
[INFO 2017-06-26 11:44:12,987 main.py:50] epoch 472, training loss: 15114.29, average training loss: 20866.16, base loss: 25553.07
[INFO 2017-06-26 11:44:13,719 main.py:50] epoch 473, training loss: 15181.07, average training loss: 20854.17, base loss: 25552.04
[INFO 2017-06-26 11:44:14,452 main.py:50] epoch 474, training loss: 14944.82, average training loss: 20841.73, base loss: 25551.05
[INFO 2017-06-26 11:44:15,184 main.py:50] epoch 475, training loss: 15764.94, average training loss: 20831.06, base loss: 25553.03
[INFO 2017-06-26 11:44:15,916 main.py:50] epoch 476, training loss: 15180.34, average training loss: 20819.21, base loss: 25551.67
[INFO 2017-06-26 11:44:16,648 main.py:50] epoch 477, training loss: 17568.77, average training loss: 20812.41, base loss: 25552.99
[INFO 2017-06-26 11:44:17,380 main.py:50] epoch 478, training loss: 14179.94, average training loss: 20798.57, base loss: 25546.56
[INFO 2017-06-26 11:44:18,112 main.py:50] epoch 479, training loss: 15110.97, average training loss: 20786.72, base loss: 25545.18
[INFO 2017-06-26 11:44:18,844 main.py:50] epoch 480, training loss: 14831.78, average training loss: 20774.34, base loss: 25542.67
[INFO 2017-06-26 11:44:19,577 main.py:50] epoch 481, training loss: 14596.00, average training loss: 20761.52, base loss: 25540.64
[INFO 2017-06-26 11:44:20,309 main.py:50] epoch 482, training loss: 17733.58, average training loss: 20755.25, base loss: 25544.65
[INFO 2017-06-26 11:44:21,041 main.py:50] epoch 483, training loss: 18089.55, average training loss: 20749.74, base loss: 25552.07
[INFO 2017-06-26 11:44:21,774 main.py:50] epoch 484, training loss: 14562.69, average training loss: 20736.99, base loss: 25550.33
[INFO 2017-06-26 11:44:22,506 main.py:50] epoch 485, training loss: 14174.30, average training loss: 20723.48, base loss: 25545.40
[INFO 2017-06-26 11:44:23,238 main.py:50] epoch 486, training loss: 14556.38, average training loss: 20710.82, base loss: 25542.80
[INFO 2017-06-26 11:44:23,971 main.py:50] epoch 487, training loss: 13526.43, average training loss: 20696.10, base loss: 25533.22
[INFO 2017-06-26 11:44:24,703 main.py:50] epoch 488, training loss: 18006.85, average training loss: 20690.60, base loss: 25536.11
[INFO 2017-06-26 11:44:25,435 main.py:50] epoch 489, training loss: 14778.84, average training loss: 20678.53, base loss: 25533.90
[INFO 2017-06-26 11:44:26,167 main.py:50] epoch 490, training loss: 18433.27, average training loss: 20673.96, base loss: 25540.63
[INFO 2017-06-26 11:44:26,899 main.py:50] epoch 491, training loss: 15515.63, average training loss: 20663.48, base loss: 25543.22
[INFO 2017-06-26 11:44:27,632 main.py:50] epoch 492, training loss: 17077.78, average training loss: 20656.20, base loss: 25545.36
[INFO 2017-06-26 11:44:28,364 main.py:50] epoch 493, training loss: 18918.80, average training loss: 20652.69, base loss: 25556.48
[INFO 2017-06-26 11:44:29,097 main.py:50] epoch 494, training loss: 17012.44, average training loss: 20645.33, base loss: 25558.21
[INFO 2017-06-26 11:44:29,844 main.py:50] epoch 495, training loss: 17921.49, average training loss: 20639.84, base loss: 25563.19
[INFO 2017-06-26 11:44:30,577 main.py:50] epoch 496, training loss: 14912.79, average training loss: 20628.32, base loss: 25560.27
[INFO 2017-06-26 11:44:31,310 main.py:50] epoch 497, training loss: 17303.11, average training loss: 20621.64, base loss: 25562.47
[INFO 2017-06-26 11:44:32,043 main.py:50] epoch 498, training loss: 14628.30, average training loss: 20609.63, base loss: 25560.98
[INFO 2017-06-26 11:44:32,775 main.py:50] epoch 499, training loss: 15438.59, average training loss: 20599.29, base loss: 25562.70
[INFO 2017-06-26 11:44:33,508 main.py:50] epoch 500, training loss: 17744.57, average training loss: 20593.59, base loss: 25567.57
[INFO 2017-06-26 11:44:34,241 main.py:50] epoch 501, training loss: 14489.01, average training loss: 20581.43, base loss: 25565.32
[INFO 2017-06-26 11:44:34,973 main.py:50] epoch 502, training loss: 14208.17, average training loss: 20568.76, base loss: 25560.63
[INFO 2017-06-26 11:44:35,706 main.py:50] epoch 503, training loss: 17317.20, average training loss: 20562.31, base loss: 25562.17
[INFO 2017-06-26 11:44:36,438 main.py:50] epoch 504, training loss: 17764.06, average training loss: 20556.77, base loss: 25566.82
[INFO 2017-06-26 11:44:37,170 main.py:50] epoch 505, training loss: 14427.18, average training loss: 20544.65, base loss: 25562.19
[INFO 2017-06-26 11:44:37,902 main.py:50] epoch 506, training loss: 13735.09, average training loss: 20531.22, base loss: 25555.85
[INFO 2017-06-26 11:44:38,634 main.py:50] epoch 507, training loss: 15744.39, average training loss: 20521.80, base loss: 25559.10
[INFO 2017-06-26 11:44:39,367 main.py:50] epoch 508, training loss: 14767.21, average training loss: 20510.49, base loss: 25558.95
[INFO 2017-06-26 11:44:40,099 main.py:50] epoch 509, training loss: 14602.15, average training loss: 20498.91, base loss: 25557.89
[INFO 2017-06-26 11:44:40,833 main.py:50] epoch 510, training loss: 17294.07, average training loss: 20492.64, base loss: 25561.50
[INFO 2017-06-26 11:44:41,566 main.py:50] epoch 511, training loss: 14474.05, average training loss: 20480.88, base loss: 25558.87
[INFO 2017-06-26 11:44:42,298 main.py:50] epoch 512, training loss: 14512.56, average training loss: 20469.25, base loss: 25556.31
[INFO 2017-06-26 11:44:43,030 main.py:50] epoch 513, training loss: 16648.87, average training loss: 20461.81, base loss: 25555.81
[INFO 2017-06-26 11:44:43,762 main.py:50] epoch 514, training loss: 17487.66, average training loss: 20456.04, base loss: 25557.70
[INFO 2017-06-26 11:44:44,497 main.py:50] epoch 515, training loss: 13860.42, average training loss: 20443.26, base loss: 25552.30
[INFO 2017-06-26 11:44:45,230 main.py:50] epoch 516, training loss: 14426.54, average training loss: 20431.62, base loss: 25548.58
[INFO 2017-06-26 11:44:45,961 main.py:50] epoch 517, training loss: 14175.08, average training loss: 20419.54, base loss: 25545.44
[INFO 2017-06-26 11:44:46,695 main.py:50] epoch 518, training loss: 14753.78, average training loss: 20408.62, base loss: 25543.76
[INFO 2017-06-26 11:44:47,430 main.py:50] epoch 519, training loss: 14473.01, average training loss: 20397.21, base loss: 25542.47
[INFO 2017-06-26 11:44:48,161 main.py:50] epoch 520, training loss: 17819.33, average training loss: 20392.26, base loss: 25544.94
[INFO 2017-06-26 11:44:48,895 main.py:50] epoch 521, training loss: 14798.09, average training loss: 20381.54, base loss: 25545.32
[INFO 2017-06-26 11:44:49,627 main.py:50] epoch 522, training loss: 14683.65, average training loss: 20370.65, base loss: 25543.37
[INFO 2017-06-26 11:44:50,361 main.py:50] epoch 523, training loss: 16390.01, average training loss: 20363.05, base loss: 25541.68
[INFO 2017-06-26 11:44:51,094 main.py:50] epoch 524, training loss: 15330.16, average training loss: 20353.47, base loss: 25542.82
[INFO 2017-06-26 11:44:51,826 main.py:50] epoch 525, training loss: 17486.00, average training loss: 20348.01, base loss: 25545.52
[INFO 2017-06-26 11:44:52,561 main.py:50] epoch 526, training loss: 17755.05, average training loss: 20343.09, base loss: 25551.38
[INFO 2017-06-26 11:44:53,292 main.py:50] epoch 527, training loss: 17437.84, average training loss: 20337.59, base loss: 25552.93
[INFO 2017-06-26 11:44:54,026 main.py:50] epoch 528, training loss: 16868.16, average training loss: 20331.03, base loss: 25552.13
[INFO 2017-06-26 11:44:54,759 main.py:50] epoch 529, training loss: 14617.92, average training loss: 20320.25, base loss: 25550.65
[INFO 2017-06-26 11:44:55,493 main.py:50] epoch 530, training loss: 14268.94, average training loss: 20308.86, base loss: 25546.69
[INFO 2017-06-26 11:44:56,225 main.py:50] epoch 531, training loss: 17630.23, average training loss: 20303.82, base loss: 25551.26
[INFO 2017-06-26 11:44:56,957 main.py:50] epoch 532, training loss: 17824.17, average training loss: 20299.17, base loss: 25557.30
[INFO 2017-06-26 11:44:57,690 main.py:50] epoch 533, training loss: 14161.86, average training loss: 20287.68, base loss: 25552.92
[INFO 2017-06-26 11:44:58,424 main.py:50] epoch 534, training loss: 14013.91, average training loss: 20275.95, base loss: 25549.85
[INFO 2017-06-26 11:44:59,157 main.py:50] epoch 535, training loss: 17406.26, average training loss: 20270.60, base loss: 25552.13
[INFO 2017-06-26 11:44:59,891 main.py:50] epoch 536, training loss: 14233.58, average training loss: 20259.36, base loss: 25549.73
[INFO 2017-06-26 11:45:00,624 main.py:50] epoch 537, training loss: 14766.67, average training loss: 20249.15, base loss: 25547.25
[INFO 2017-06-26 11:45:01,357 main.py:50] epoch 538, training loss: 15041.09, average training loss: 20239.48, base loss: 25546.55
[INFO 2017-06-26 11:45:02,107 main.py:50] epoch 539, training loss: 14224.69, average training loss: 20228.34, base loss: 25542.07
[INFO 2017-06-26 11:45:02,841 main.py:50] epoch 540, training loss: 14709.04, average training loss: 20218.14, base loss: 25541.66
[INFO 2017-06-26 11:45:03,575 main.py:50] epoch 541, training loss: 13832.81, average training loss: 20206.36, base loss: 25536.88
[INFO 2017-06-26 11:45:04,307 main.py:50] epoch 542, training loss: 15260.62, average training loss: 20197.25, base loss: 25537.32
[INFO 2017-06-26 11:45:05,040 main.py:50] epoch 543, training loss: 13440.94, average training loss: 20184.83, base loss: 25531.51
[INFO 2017-06-26 11:45:05,772 main.py:50] epoch 544, training loss: 14851.86, average training loss: 20175.05, base loss: 25530.14
[INFO 2017-06-26 11:45:06,506 main.py:50] epoch 545, training loss: 14376.49, average training loss: 20164.43, base loss: 25528.67
[INFO 2017-06-26 11:45:07,241 main.py:50] epoch 546, training loss: 17443.57, average training loss: 20159.45, base loss: 25533.25
[INFO 2017-06-26 11:45:07,974 main.py:50] epoch 547, training loss: 14495.73, average training loss: 20149.12, base loss: 25531.35
[INFO 2017-06-26 11:45:08,707 main.py:50] epoch 548, training loss: 14278.68, average training loss: 20138.43, base loss: 25527.70
[INFO 2017-06-26 11:45:09,441 main.py:50] epoch 549, training loss: 14505.24, average training loss: 20128.18, base loss: 25527.12
[INFO 2017-06-26 11:45:10,194 main.py:50] epoch 550, training loss: 13930.16, average training loss: 20116.94, base loss: 25523.75
[INFO 2017-06-26 11:45:10,926 main.py:50] epoch 551, training loss: 17045.54, average training loss: 20111.37, base loss: 25525.72
[INFO 2017-06-26 11:45:11,662 main.py:50] epoch 552, training loss: 17432.18, average training loss: 20106.53, base loss: 25530.99
[INFO 2017-06-26 11:45:12,394 main.py:50] epoch 553, training loss: 17191.85, average training loss: 20101.27, base loss: 25533.11
[INFO 2017-06-26 11:45:13,127 main.py:50] epoch 554, training loss: 15309.42, average training loss: 20092.63, base loss: 25535.59
[INFO 2017-06-26 11:45:13,859 main.py:50] epoch 555, training loss: 16822.66, average training loss: 20086.75, base loss: 25536.91
[INFO 2017-06-26 11:45:14,593 main.py:50] epoch 556, training loss: 14072.11, average training loss: 20075.95, base loss: 25532.84
[INFO 2017-06-26 11:45:15,330 main.py:50] epoch 557, training loss: 14095.67, average training loss: 20065.23, base loss: 25531.20
[INFO 2017-06-26 11:45:16,061 main.py:50] epoch 558, training loss: 13954.76, average training loss: 20054.30, base loss: 25530.08
[INFO 2017-06-26 11:45:16,792 main.py:50] epoch 559, training loss: 16557.23, average training loss: 20048.06, base loss: 25530.31
[INFO 2017-06-26 11:45:17,522 main.py:50] epoch 560, training loss: 17238.58, average training loss: 20043.05, base loss: 25532.37
[INFO 2017-06-26 11:45:18,254 main.py:50] epoch 561, training loss: 14816.70, average training loss: 20033.75, base loss: 25532.28
[INFO 2017-06-26 11:45:18,983 main.py:50] epoch 562, training loss: 14128.43, average training loss: 20023.26, base loss: 25529.04
[INFO 2017-06-26 11:45:19,713 main.py:50] epoch 563, training loss: 13714.90, average training loss: 20012.08, base loss: 25524.50
[INFO 2017-06-26 11:45:20,445 main.py:50] epoch 564, training loss: 17537.98, average training loss: 20007.70, base loss: 25529.29
[INFO 2017-06-26 11:45:21,174 main.py:50] epoch 565, training loss: 13576.29, average training loss: 19996.33, base loss: 25522.49
[INFO 2017-06-26 11:45:21,905 main.py:50] epoch 566, training loss: 16311.43, average training loss: 19989.84, base loss: 25521.45
[INFO 2017-06-26 11:45:22,635 main.py:50] epoch 567, training loss: 14878.05, average training loss: 19980.84, base loss: 25521.34
[INFO 2017-06-26 11:45:23,367 main.py:50] epoch 568, training loss: 14730.76, average training loss: 19971.61, base loss: 25521.70
[INFO 2017-06-26 11:45:24,098 main.py:50] epoch 569, training loss: 14770.33, average training loss: 19962.48, base loss: 25522.54
[INFO 2017-06-26 11:45:24,829 main.py:50] epoch 570, training loss: 14347.03, average training loss: 19952.65, base loss: 25519.76
[INFO 2017-06-26 11:45:25,561 main.py:50] epoch 571, training loss: 14561.25, average training loss: 19943.22, base loss: 25516.71
[INFO 2017-06-26 11:45:26,293 main.py:50] epoch 572, training loss: 17317.99, average training loss: 19938.64, base loss: 25520.45
[INFO 2017-06-26 11:45:27,026 main.py:50] epoch 573, training loss: 15506.47, average training loss: 19930.92, base loss: 25524.06
[INFO 2017-06-26 11:45:27,760 main.py:50] epoch 574, training loss: 15146.30, average training loss: 19922.60, base loss: 25525.95
[INFO 2017-06-26 11:45:28,492 main.py:50] epoch 575, training loss: 13861.03, average training loss: 19912.08, base loss: 25521.55
[INFO 2017-06-26 11:45:29,225 main.py:50] epoch 576, training loss: 14698.57, average training loss: 19903.04, base loss: 25520.29
[INFO 2017-06-26 11:45:29,960 main.py:50] epoch 577, training loss: 13600.03, average training loss: 19892.14, base loss: 25515.91
[INFO 2017-06-26 11:45:30,693 main.py:50] epoch 578, training loss: 16994.17, average training loss: 19887.13, base loss: 25517.24
[INFO 2017-06-26 11:45:31,429 main.py:50] epoch 579, training loss: 16517.26, average training loss: 19881.32, base loss: 25518.03
[INFO 2017-06-26 11:45:32,162 main.py:50] epoch 580, training loss: 16818.98, average training loss: 19876.05, base loss: 25517.91
[INFO 2017-06-26 11:45:32,894 main.py:50] epoch 581, training loss: 14465.75, average training loss: 19866.75, base loss: 25517.15
[INFO 2017-06-26 11:45:33,628 main.py:50] epoch 582, training loss: 17674.15, average training loss: 19862.99, base loss: 25521.86
[INFO 2017-06-26 11:45:34,379 main.py:50] epoch 583, training loss: 14111.80, average training loss: 19853.15, base loss: 25520.56
[INFO 2017-06-26 11:45:35,111 main.py:50] epoch 584, training loss: 17473.39, average training loss: 19849.08, base loss: 25523.46
[INFO 2017-06-26 11:45:35,844 main.py:50] epoch 585, training loss: 13858.33, average training loss: 19838.85, base loss: 25520.05
[INFO 2017-06-26 11:45:36,576 main.py:50] epoch 586, training loss: 14535.74, average training loss: 19829.82, base loss: 25518.49
[INFO 2017-06-26 11:45:37,308 main.py:50] epoch 587, training loss: 16052.80, average training loss: 19823.40, base loss: 25515.74
[INFO 2017-06-26 11:45:38,042 main.py:50] epoch 588, training loss: 17288.37, average training loss: 19819.09, base loss: 25520.39
[INFO 2017-06-26 11:45:38,774 main.py:50] epoch 589, training loss: 14412.31, average training loss: 19809.93, base loss: 25517.85
[INFO 2017-06-26 11:45:39,507 main.py:50] epoch 590, training loss: 16707.55, average training loss: 19804.68, base loss: 25516.93
[INFO 2017-06-26 11:45:40,239 main.py:50] epoch 591, training loss: 16799.91, average training loss: 19799.60, base loss: 25518.81
[INFO 2017-06-26 11:45:40,971 main.py:50] epoch 592, training loss: 15723.50, average training loss: 19792.73, base loss: 25522.88
[INFO 2017-06-26 11:45:41,705 main.py:50] epoch 593, training loss: 12904.03, average training loss: 19781.13, base loss: 25515.24
[INFO 2017-06-26 11:45:42,437 main.py:50] epoch 594, training loss: 17851.07, average training loss: 19777.89, base loss: 25519.60
[INFO 2017-06-26 11:45:43,172 main.py:50] epoch 595, training loss: 14556.17, average training loss: 19769.13, base loss: 25518.62
[INFO 2017-06-26 11:45:43,905 main.py:50] epoch 596, training loss: 14147.45, average training loss: 19759.71, base loss: 25517.47
[INFO 2017-06-26 11:45:44,641 main.py:50] epoch 597, training loss: 14770.77, average training loss: 19751.37, base loss: 25516.91
[INFO 2017-06-26 11:45:45,375 main.py:50] epoch 598, training loss: 17079.12, average training loss: 19746.91, base loss: 25520.96
[INFO 2017-06-26 11:45:46,108 main.py:50] epoch 599, training loss: 14586.40, average training loss: 19738.31, base loss: 25520.92
[INFO 2017-06-26 11:45:46,840 main.py:50] epoch 600, training loss: 16876.91, average training loss: 19733.55, base loss: 25520.70
[INFO 2017-06-26 11:45:47,573 main.py:50] epoch 601, training loss: 17269.07, average training loss: 19729.45, base loss: 25524.30
[INFO 2017-06-26 11:45:48,305 main.py:50] epoch 602, training loss: 14257.43, average training loss: 19720.38, base loss: 25522.41
[INFO 2017-06-26 11:45:49,039 main.py:50] epoch 603, training loss: 14039.20, average training loss: 19710.97, base loss: 25519.77
[INFO 2017-06-26 11:45:49,771 main.py:50] epoch 604, training loss: 14822.70, average training loss: 19702.89, base loss: 25518.33
[INFO 2017-06-26 11:45:50,503 main.py:50] epoch 605, training loss: 14022.61, average training loss: 19693.52, base loss: 25515.45
[INFO 2017-06-26 11:45:51,236 main.py:50] epoch 606, training loss: 15168.10, average training loss: 19686.06, base loss: 25517.15
[INFO 2017-06-26 11:45:51,969 main.py:50] epoch 607, training loss: 17377.19, average training loss: 19682.26, base loss: 25521.75
[INFO 2017-06-26 11:45:52,701 main.py:50] epoch 608, training loss: 14997.01, average training loss: 19674.57, base loss: 25524.49
[INFO 2017-06-26 11:45:53,435 main.py:50] epoch 609, training loss: 14062.87, average training loss: 19665.37, base loss: 25521.18
[INFO 2017-06-26 11:45:54,168 main.py:50] epoch 610, training loss: 13870.30, average training loss: 19655.89, base loss: 25518.82
[INFO 2017-06-26 11:45:54,901 main.py:50] epoch 611, training loss: 14651.82, average training loss: 19647.71, base loss: 25518.64
[INFO 2017-06-26 11:45:55,634 main.py:50] epoch 612, training loss: 17248.62, average training loss: 19643.80, base loss: 25522.85
[INFO 2017-06-26 11:45:56,367 main.py:50] epoch 613, training loss: 16549.00, average training loss: 19638.76, base loss: 25522.28
[INFO 2017-06-26 11:45:57,101 main.py:50] epoch 614, training loss: 17881.65, average training loss: 19635.90, base loss: 25528.80
[INFO 2017-06-26 11:45:57,836 main.py:50] epoch 615, training loss: 17604.58, average training loss: 19632.60, base loss: 25535.03
[INFO 2017-06-26 11:45:58,568 main.py:50] epoch 616, training loss: 14058.98, average training loss: 19623.57, base loss: 25533.19
[INFO 2017-06-26 11:45:59,298 main.py:50] epoch 617, training loss: 18100.67, average training loss: 19621.10, base loss: 25539.47
[INFO 2017-06-26 11:46:00,028 main.py:50] epoch 618, training loss: 14606.09, average training loss: 19613.00, base loss: 25537.68
[INFO 2017-06-26 11:46:00,761 main.py:50] epoch 619, training loss: 14784.78, average training loss: 19605.22, base loss: 25537.63
[INFO 2017-06-26 11:46:01,496 main.py:50] epoch 620, training loss: 15175.19, average training loss: 19598.08, base loss: 25540.97
[INFO 2017-06-26 11:46:02,227 main.py:50] epoch 621, training loss: 15043.18, average training loss: 19590.76, base loss: 25540.76
[INFO 2017-06-26 11:46:02,957 main.py:50] epoch 622, training loss: 17408.05, average training loss: 19587.25, base loss: 25544.06
[INFO 2017-06-26 11:46:03,686 main.py:50] epoch 623, training loss: 17429.57, average training loss: 19583.80, base loss: 25548.42
[INFO 2017-06-26 11:46:04,416 main.py:50] epoch 624, training loss: 16224.12, average training loss: 19578.42, base loss: 25546.98
[INFO 2017-06-26 11:46:05,146 main.py:50] epoch 625, training loss: 17265.71, average training loss: 19574.73, base loss: 25550.03
[INFO 2017-06-26 11:46:05,878 main.py:50] epoch 626, training loss: 14095.61, average training loss: 19565.99, base loss: 25547.78
[INFO 2017-06-26 11:46:06,625 main.py:50] epoch 627, training loss: 13666.20, average training loss: 19556.59, base loss: 25543.17
[INFO 2017-06-26 11:46:07,358 main.py:50] epoch 628, training loss: 14555.53, average training loss: 19548.64, base loss: 25543.93
[INFO 2017-06-26 11:46:08,088 main.py:50] epoch 629, training loss: 14075.62, average training loss: 19539.96, base loss: 25541.95
[INFO 2017-06-26 11:46:08,819 main.py:50] epoch 630, training loss: 13360.83, average training loss: 19530.16, base loss: 25538.26
[INFO 2017-06-26 11:46:09,551 main.py:50] epoch 631, training loss: 14299.36, average training loss: 19521.89, base loss: 25537.51
[INFO 2017-06-26 11:46:10,281 main.py:50] epoch 632, training loss: 14461.80, average training loss: 19513.89, base loss: 25535.19
[INFO 2017-06-26 11:46:11,012 main.py:50] epoch 633, training loss: 13778.53, average training loss: 19504.85, base loss: 25533.34
[INFO 2017-06-26 11:46:11,744 main.py:50] epoch 634, training loss: 13743.30, average training loss: 19495.77, base loss: 25529.83
[INFO 2017-06-26 11:46:12,475 main.py:50] epoch 635, training loss: 13616.10, average training loss: 19486.53, base loss: 25524.80
[INFO 2017-06-26 11:46:13,209 main.py:50] epoch 636, training loss: 14082.92, average training loss: 19478.05, base loss: 25522.77
[INFO 2017-06-26 11:46:13,941 main.py:50] epoch 637, training loss: 17137.50, average training loss: 19474.38, base loss: 25525.59
[INFO 2017-06-26 11:46:14,678 main.py:50] epoch 638, training loss: 15278.56, average training loss: 19467.81, base loss: 25527.54
[INFO 2017-06-26 11:46:15,412 main.py:50] epoch 639, training loss: 14601.86, average training loss: 19460.21, base loss: 25527.94
[INFO 2017-06-26 11:46:16,144 main.py:50] epoch 640, training loss: 14690.45, average training loss: 19452.77, base loss: 25527.34
[INFO 2017-06-26 11:46:16,878 main.py:50] epoch 641, training loss: 16121.06, average training loss: 19447.58, base loss: 25526.53
[INFO 2017-06-26 11:46:17,611 main.py:50] epoch 642, training loss: 14712.77, average training loss: 19440.21, base loss: 25525.92
[INFO 2017-06-26 11:46:18,344 main.py:50] epoch 643, training loss: 17321.13, average training loss: 19436.92, base loss: 25530.29
[INFO 2017-06-26 11:46:19,077 main.py:50] epoch 644, training loss: 16758.88, average training loss: 19432.77, base loss: 25531.35
[INFO 2017-06-26 11:46:19,810 main.py:50] epoch 645, training loss: 18355.65, average training loss: 19431.10, base loss: 25539.29
[INFO 2017-06-26 11:46:20,542 main.py:50] epoch 646, training loss: 14338.17, average training loss: 19423.23, base loss: 25537.74
[INFO 2017-06-26 11:46:21,274 main.py:50] epoch 647, training loss: 13647.94, average training loss: 19414.32, base loss: 25533.69
[INFO 2017-06-26 11:46:22,007 main.py:50] epoch 648, training loss: 17096.62, average training loss: 19410.75, base loss: 25535.75
[INFO 2017-06-26 11:46:22,739 main.py:50] epoch 649, training loss: 14819.86, average training loss: 19403.69, base loss: 25536.01
[INFO 2017-06-26 11:46:23,469 main.py:50] epoch 650, training loss: 14248.30, average training loss: 19395.77, base loss: 25534.39
[INFO 2017-06-26 11:46:24,201 main.py:50] epoch 651, training loss: 14261.15, average training loss: 19387.89, base loss: 25532.56
[INFO 2017-06-26 11:46:24,934 main.py:50] epoch 652, training loss: 17363.24, average training loss: 19384.79, base loss: 25536.90
[INFO 2017-06-26 11:46:25,667 main.py:50] epoch 653, training loss: 12892.02, average training loss: 19374.86, base loss: 25529.83
[INFO 2017-06-26 11:46:26,398 main.py:50] epoch 654, training loss: 17796.74, average training loss: 19372.45, base loss: 25533.20
[INFO 2017-06-26 11:46:27,127 main.py:50] epoch 655, training loss: 14187.84, average training loss: 19364.55, base loss: 25530.93
[INFO 2017-06-26 11:46:27,864 main.py:50] epoch 656, training loss: 13792.52, average training loss: 19356.07, base loss: 25526.26
[INFO 2017-06-26 11:46:28,596 main.py:50] epoch 657, training loss: 14466.50, average training loss: 19348.64, base loss: 25526.76
[INFO 2017-06-26 11:46:29,333 main.py:50] epoch 658, training loss: 13925.71, average training loss: 19340.41, base loss: 25523.96
[INFO 2017-06-26 11:46:30,064 main.py:50] epoch 659, training loss: 13982.23, average training loss: 19332.29, base loss: 25522.32
[INFO 2017-06-26 11:46:30,800 main.py:50] epoch 660, training loss: 17935.66, average training loss: 19330.18, base loss: 25528.21
[INFO 2017-06-26 11:46:31,532 main.py:50] epoch 661, training loss: 16200.81, average training loss: 19325.45, base loss: 25528.61
[INFO 2017-06-26 11:46:32,266 main.py:50] epoch 662, training loss: 14577.19, average training loss: 19318.29, base loss: 25528.22
[INFO 2017-06-26 11:46:33,000 main.py:50] epoch 663, training loss: 14054.85, average training loss: 19310.36, base loss: 25525.88
[INFO 2017-06-26 11:46:33,732 main.py:50] epoch 664, training loss: 13812.99, average training loss: 19302.10, base loss: 25522.58
[INFO 2017-06-26 11:46:34,463 main.py:50] epoch 665, training loss: 16226.35, average training loss: 19297.48, base loss: 25521.57
[INFO 2017-06-26 11:46:35,193 main.py:50] epoch 666, training loss: 13777.56, average training loss: 19289.20, base loss: 25518.73
[INFO 2017-06-26 11:46:35,924 main.py:50] epoch 667, training loss: 14254.01, average training loss: 19281.66, base loss: 25518.22
[INFO 2017-06-26 11:46:36,656 main.py:50] epoch 668, training loss: 14613.95, average training loss: 19274.69, base loss: 25517.44
[INFO 2017-06-26 11:46:37,386 main.py:50] epoch 669, training loss: 13785.02, average training loss: 19266.49, base loss: 25513.68
[INFO 2017-06-26 11:46:38,124 main.py:50] epoch 670, training loss: 17517.28, average training loss: 19263.89, base loss: 25518.02
[INFO 2017-06-26 11:46:38,874 main.py:50] epoch 671, training loss: 17960.98, average training loss: 19261.95, base loss: 25523.40
[INFO 2017-06-26 11:46:39,639 main.py:50] epoch 672, training loss: 16760.18, average training loss: 19258.23, base loss: 25525.63
[INFO 2017-06-26 11:46:40,372 main.py:50] epoch 673, training loss: 14184.01, average training loss: 19250.70, base loss: 25523.75
[INFO 2017-06-26 11:46:41,101 main.py:50] epoch 674, training loss: 14462.26, average training loss: 19243.61, base loss: 25522.55
[INFO 2017-06-26 11:46:41,832 main.py:50] epoch 675, training loss: 13558.91, average training loss: 19235.20, base loss: 25519.64
[INFO 2017-06-26 11:46:42,564 main.py:50] epoch 676, training loss: 16460.38, average training loss: 19231.10, base loss: 25520.77
[INFO 2017-06-26 11:46:43,296 main.py:50] epoch 677, training loss: 13633.69, average training loss: 19222.84, base loss: 25518.50
[INFO 2017-06-26 11:46:44,028 main.py:50] epoch 678, training loss: 14453.04, average training loss: 19215.82, base loss: 25516.56
[INFO 2017-06-26 11:46:44,773 main.py:50] epoch 679, training loss: 14754.69, average training loss: 19209.26, base loss: 25517.87
[INFO 2017-06-26 11:46:45,508 main.py:50] epoch 680, training loss: 13250.21, average training loss: 19200.51, base loss: 25513.79
[INFO 2017-06-26 11:46:46,240 main.py:50] epoch 681, training loss: 15298.32, average training loss: 19194.79, base loss: 25516.93
[INFO 2017-06-26 11:46:46,975 main.py:50] epoch 682, training loss: 16078.55, average training loss: 19190.22, base loss: 25515.35
[INFO 2017-06-26 11:46:47,710 main.py:50] epoch 683, training loss: 17035.88, average training loss: 19187.07, base loss: 25517.95
[INFO 2017-06-26 11:46:48,444 main.py:50] epoch 684, training loss: 13041.74, average training loss: 19178.10, base loss: 25512.95
[INFO 2017-06-26 11:46:49,183 main.py:50] epoch 685, training loss: 14234.15, average training loss: 19170.90, base loss: 25511.76
[INFO 2017-06-26 11:46:49,919 main.py:50] epoch 686, training loss: 17247.40, average training loss: 19168.10, base loss: 25513.47
[INFO 2017-06-26 11:46:50,652 main.py:50] epoch 687, training loss: 17348.72, average training loss: 19165.45, base loss: 25516.91
[INFO 2017-06-26 11:46:51,389 main.py:50] epoch 688, training loss: 14367.60, average training loss: 19158.49, base loss: 25516.25
[INFO 2017-06-26 11:46:52,127 main.py:50] epoch 689, training loss: 16823.18, average training loss: 19155.10, base loss: 25518.19
[INFO 2017-06-26 11:46:52,865 main.py:50] epoch 690, training loss: 16399.32, average training loss: 19151.12, base loss: 25518.89
[INFO 2017-06-26 11:46:53,598 main.py:50] epoch 691, training loss: 14275.83, average training loss: 19144.07, base loss: 25517.25
[INFO 2017-06-26 11:46:54,330 main.py:50] epoch 692, training loss: 17361.87, average training loss: 19141.50, base loss: 25523.11
[INFO 2017-06-26 11:46:55,063 main.py:50] epoch 693, training loss: 16811.97, average training loss: 19138.14, base loss: 25526.60
[INFO 2017-06-26 11:46:55,796 main.py:50] epoch 694, training loss: 15201.32, average training loss: 19132.48, base loss: 25530.50
[INFO 2017-06-26 11:46:56,529 main.py:50] epoch 695, training loss: 14254.47, average training loss: 19125.47, base loss: 25530.22
[INFO 2017-06-26 11:46:57,262 main.py:50] epoch 696, training loss: 13646.19, average training loss: 19117.61, base loss: 25527.34
[INFO 2017-06-26 11:46:57,994 main.py:50] epoch 697, training loss: 14063.91, average training loss: 19110.37, base loss: 25524.71
[INFO 2017-06-26 11:46:58,726 main.py:50] epoch 698, training loss: 17076.63, average training loss: 19107.46, base loss: 25528.82
[INFO 2017-06-26 11:46:59,458 main.py:50] epoch 699, training loss: 17583.42, average training loss: 19105.28, base loss: 25533.77
[INFO 2017-06-26 11:47:00,190 main.py:50] epoch 700, training loss: 17888.79, average training loss: 19103.55, base loss: 25538.70
[INFO 2017-06-26 11:47:00,922 main.py:50] epoch 701, training loss: 14323.45, average training loss: 19096.74, base loss: 25539.11
[INFO 2017-06-26 11:47:01,655 main.py:50] epoch 702, training loss: 14549.05, average training loss: 19090.27, base loss: 25539.02
[INFO 2017-06-26 11:47:02,387 main.py:50] epoch 703, training loss: 13887.03, average training loss: 19082.88, base loss: 25537.66
[INFO 2017-06-26 11:47:03,118 main.py:50] epoch 704, training loss: 17898.76, average training loss: 19081.20, base loss: 25543.80
[INFO 2017-06-26 11:47:03,850 main.py:50] epoch 705, training loss: 13974.15, average training loss: 19073.96, base loss: 25543.34
[INFO 2017-06-26 11:47:04,583 main.py:50] epoch 706, training loss: 13933.64, average training loss: 19066.69, base loss: 25541.00
[INFO 2017-06-26 11:47:05,315 main.py:50] epoch 707, training loss: 13867.20, average training loss: 19059.35, base loss: 25539.49
[INFO 2017-06-26 11:47:06,048 main.py:50] epoch 708, training loss: 17345.89, average training loss: 19056.93, base loss: 25543.69
[INFO 2017-06-26 11:47:06,782 main.py:50] epoch 709, training loss: 13651.41, average training loss: 19049.32, base loss: 25541.19
[INFO 2017-06-26 11:47:07,514 main.py:50] epoch 710, training loss: 17544.11, average training loss: 19047.20, base loss: 25546.05
[INFO 2017-06-26 11:47:08,245 main.py:50] epoch 711, training loss: 14630.21, average training loss: 19041.00, base loss: 25547.81
[INFO 2017-06-26 11:47:08,975 main.py:50] epoch 712, training loss: 14171.24, average training loss: 19034.17, base loss: 25546.87
[INFO 2017-06-26 11:47:09,706 main.py:50] epoch 713, training loss: 13581.73, average training loss: 19026.53, base loss: 25543.36
[INFO 2017-06-26 11:47:10,438 main.py:50] epoch 714, training loss: 14154.22, average training loss: 19019.72, base loss: 25543.19
[INFO 2017-06-26 11:47:11,183 main.py:50] epoch 715, training loss: 14252.51, average training loss: 19013.06, base loss: 25541.87
[INFO 2017-06-26 11:47:11,914 main.py:50] epoch 716, training loss: 13791.54, average training loss: 19005.78, base loss: 25538.86
[INFO 2017-06-26 11:47:12,645 main.py:50] epoch 717, training loss: 16294.73, average training loss: 19002.00, base loss: 25539.57
[INFO 2017-06-26 11:47:13,376 main.py:50] epoch 718, training loss: 13445.27, average training loss: 18994.27, base loss: 25536.11
[INFO 2017-06-26 11:47:14,109 main.py:50] epoch 719, training loss: 13674.73, average training loss: 18986.88, base loss: 25534.40
[INFO 2017-06-26 11:47:14,840 main.py:50] epoch 720, training loss: 16666.12, average training loss: 18983.66, base loss: 25536.71
[INFO 2017-06-26 11:47:15,572 main.py:50] epoch 721, training loss: 14296.63, average training loss: 18977.17, base loss: 25536.01
[INFO 2017-06-26 11:47:16,302 main.py:50] epoch 722, training loss: 14095.36, average training loss: 18970.42, base loss: 25534.34
[INFO 2017-06-26 11:47:17,034 main.py:50] epoch 723, training loss: 15433.36, average training loss: 18965.54, base loss: 25538.00
[INFO 2017-06-26 11:47:17,765 main.py:50] epoch 724, training loss: 14678.95, average training loss: 18959.62, base loss: 25540.61
[INFO 2017-06-26 11:47:18,497 main.py:50] epoch 725, training loss: 16509.51, average training loss: 18956.25, base loss: 25541.36
[INFO 2017-06-26 11:47:19,232 main.py:50] epoch 726, training loss: 13908.09, average training loss: 18949.30, base loss: 25539.29
[INFO 2017-06-26 11:47:19,967 main.py:50] epoch 727, training loss: 14428.05, average training loss: 18943.09, base loss: 25538.86
[INFO 2017-06-26 11:47:20,698 main.py:50] epoch 728, training loss: 17235.56, average training loss: 18940.75, base loss: 25542.65
[INFO 2017-06-26 11:47:21,427 main.py:50] epoch 729, training loss: 14904.75, average training loss: 18935.22, base loss: 25543.45
[INFO 2017-06-26 11:47:22,157 main.py:50] epoch 730, training loss: 13778.80, average training loss: 18928.17, base loss: 25541.66
[INFO 2017-06-26 11:47:22,890 main.py:50] epoch 731, training loss: 13808.17, average training loss: 18921.17, base loss: 25540.49
[INFO 2017-06-26 11:47:23,622 main.py:50] epoch 732, training loss: 13991.22, average training loss: 18914.45, base loss: 25538.37
[INFO 2017-06-26 11:47:24,353 main.py:50] epoch 733, training loss: 14684.35, average training loss: 18908.69, base loss: 25539.87
[INFO 2017-06-26 11:47:25,086 main.py:50] epoch 734, training loss: 13485.60, average training loss: 18901.31, base loss: 25536.68
[INFO 2017-06-26 11:47:25,817 main.py:50] epoch 735, training loss: 14004.95, average training loss: 18894.65, base loss: 25536.06
[INFO 2017-06-26 11:47:26,548 main.py:50] epoch 736, training loss: 13603.59, average training loss: 18887.47, base loss: 25533.34
[INFO 2017-06-26 11:47:27,280 main.py:50] epoch 737, training loss: 13653.96, average training loss: 18880.38, base loss: 25531.31
[INFO 2017-06-26 11:47:28,013 main.py:50] epoch 738, training loss: 13465.66, average training loss: 18873.06, base loss: 25528.28
[INFO 2017-06-26 11:47:28,746 main.py:50] epoch 739, training loss: 14679.89, average training loss: 18867.39, base loss: 25529.04
[INFO 2017-06-26 11:47:29,477 main.py:50] epoch 740, training loss: 16066.29, average training loss: 18863.61, base loss: 25529.49
[INFO 2017-06-26 11:47:30,209 main.py:50] epoch 741, training loss: 14350.34, average training loss: 18857.53, base loss: 25529.00
[INFO 2017-06-26 11:47:30,940 main.py:50] epoch 742, training loss: 14073.35, average training loss: 18851.09, base loss: 25527.02
[INFO 2017-06-26 11:47:31,672 main.py:50] epoch 743, training loss: 16683.42, average training loss: 18848.17, base loss: 25529.61
[INFO 2017-06-26 11:47:32,403 main.py:50] epoch 744, training loss: 15200.59, average training loss: 18843.28, base loss: 25532.38
[INFO 2017-06-26 11:47:33,133 main.py:50] epoch 745, training loss: 14162.94, average training loss: 18837.00, base loss: 25532.59
[INFO 2017-06-26 11:47:33,865 main.py:50] epoch 746, training loss: 14466.91, average training loss: 18831.15, base loss: 25532.10
[INFO 2017-06-26 11:47:34,595 main.py:50] epoch 747, training loss: 17217.50, average training loss: 18829.00, base loss: 25535.62
[INFO 2017-06-26 11:47:35,326 main.py:50] epoch 748, training loss: 13314.19, average training loss: 18821.63, base loss: 25532.87
[INFO 2017-06-26 11:47:36,058 main.py:50] epoch 749, training loss: 16380.88, average training loss: 18818.38, base loss: 25533.76
[INFO 2017-06-26 11:47:36,791 main.py:50] epoch 750, training loss: 14530.01, average training loss: 18812.67, base loss: 25534.76
[INFO 2017-06-26 11:47:37,522 main.py:50] epoch 751, training loss: 16865.35, average training loss: 18810.08, base loss: 25538.52
[INFO 2017-06-26 11:47:38,253 main.py:50] epoch 752, training loss: 16091.86, average training loss: 18806.47, base loss: 25540.49
[INFO 2017-06-26 11:47:38,984 main.py:50] epoch 753, training loss: 14205.03, average training loss: 18800.37, base loss: 25539.70
[INFO 2017-06-26 11:47:39,716 main.py:50] epoch 754, training loss: 16505.44, average training loss: 18797.33, base loss: 25540.85
[INFO 2017-06-26 11:47:40,448 main.py:50] epoch 755, training loss: 14277.01, average training loss: 18791.35, base loss: 25541.28
[INFO 2017-06-26 11:47:41,182 main.py:50] epoch 756, training loss: 17322.66, average training loss: 18789.41, base loss: 25545.77
[INFO 2017-06-26 11:47:41,914 main.py:50] epoch 757, training loss: 14495.41, average training loss: 18783.74, base loss: 25544.23
[INFO 2017-06-26 11:47:42,647 main.py:50] epoch 758, training loss: 13695.17, average training loss: 18777.04, base loss: 25541.71
[INFO 2017-06-26 11:47:43,392 main.py:50] epoch 759, training loss: 13727.49, average training loss: 18770.40, base loss: 25540.26
[INFO 2017-06-26 11:47:44,123 main.py:50] epoch 760, training loss: 17407.83, average training loss: 18768.60, base loss: 25544.27
[INFO 2017-06-26 11:47:44,854 main.py:50] epoch 761, training loss: 15948.11, average training loss: 18764.90, base loss: 25545.57
[INFO 2017-06-26 11:47:45,587 main.py:50] epoch 762, training loss: 16420.16, average training loss: 18761.83, base loss: 25545.75
[INFO 2017-06-26 11:47:46,317 main.py:50] epoch 763, training loss: 14379.64, average training loss: 18756.09, base loss: 25545.90
[INFO 2017-06-26 11:47:47,048 main.py:50] epoch 764, training loss: 16577.57, average training loss: 18753.25, base loss: 25547.79
[INFO 2017-06-26 11:47:47,778 main.py:50] epoch 765, training loss: 16220.21, average training loss: 18749.94, base loss: 25548.07
[INFO 2017-06-26 11:47:48,509 main.py:50] epoch 766, training loss: 13089.54, average training loss: 18742.56, base loss: 25543.77
[INFO 2017-06-26 11:47:49,238 main.py:50] epoch 767, training loss: 14149.70, average training loss: 18736.58, base loss: 25541.88
[INFO 2017-06-26 11:47:49,972 main.py:50] epoch 768, training loss: 13761.58, average training loss: 18730.11, base loss: 25539.77
[INFO 2017-06-26 11:47:50,705 main.py:50] epoch 769, training loss: 14457.32, average training loss: 18724.56, base loss: 25539.26
[INFO 2017-06-26 11:47:51,436 main.py:50] epoch 770, training loss: 16357.79, average training loss: 18721.49, base loss: 25539.46
[INFO 2017-06-26 11:47:52,170 main.py:50] epoch 771, training loss: 17169.75, average training loss: 18719.48, base loss: 25543.27
[INFO 2017-06-26 11:47:52,904 main.py:50] epoch 772, training loss: 13623.36, average training loss: 18712.89, base loss: 25541.69
[INFO 2017-06-26 11:47:53,637 main.py:50] epoch 773, training loss: 14406.14, average training loss: 18707.32, base loss: 25541.78
[INFO 2017-06-26 11:47:54,369 main.py:50] epoch 774, training loss: 12911.55, average training loss: 18699.85, base loss: 25536.66
[INFO 2017-06-26 11:47:55,101 main.py:50] epoch 775, training loss: 13875.43, average training loss: 18693.63, base loss: 25535.46
[INFO 2017-06-26 11:47:55,835 main.py:50] epoch 776, training loss: 13945.56, average training loss: 18687.52, base loss: 25533.70
[INFO 2017-06-26 11:47:56,570 main.py:50] epoch 777, training loss: 16425.86, average training loss: 18684.61, base loss: 25535.79
[INFO 2017-06-26 11:47:57,304 main.py:50] epoch 778, training loss: 13606.93, average training loss: 18678.09, base loss: 25534.09
[INFO 2017-06-26 11:47:58,039 main.py:50] epoch 779, training loss: 13574.11, average training loss: 18671.55, base loss: 25532.96
[INFO 2017-06-26 11:47:58,773 main.py:50] epoch 780, training loss: 13004.14, average training loss: 18664.29, base loss: 25528.18
[INFO 2017-06-26 11:47:59,507 main.py:50] epoch 781, training loss: 16777.12, average training loss: 18661.88, base loss: 25530.66
[INFO 2017-06-26 11:48:00,241 main.py:50] epoch 782, training loss: 17952.98, average training loss: 18660.97, base loss: 25537.25
[INFO 2017-06-26 11:48:00,974 main.py:50] epoch 783, training loss: 13766.75, average training loss: 18654.73, base loss: 25535.43
[INFO 2017-06-26 11:48:01,707 main.py:50] epoch 784, training loss: 14163.30, average training loss: 18649.01, base loss: 25533.97
[INFO 2017-06-26 11:48:02,442 main.py:50] epoch 785, training loss: 16348.21, average training loss: 18646.08, base loss: 25536.01
[INFO 2017-06-26 11:48:03,176 main.py:50] epoch 786, training loss: 13747.26, average training loss: 18639.86, base loss: 25533.49
[INFO 2017-06-26 11:48:03,909 main.py:50] epoch 787, training loss: 13764.65, average training loss: 18633.67, base loss: 25531.03
[INFO 2017-06-26 11:48:04,642 main.py:50] epoch 788, training loss: 13731.44, average training loss: 18627.46, base loss: 25528.56
[INFO 2017-06-26 11:48:05,376 main.py:50] epoch 789, training loss: 14490.00, average training loss: 18622.22, base loss: 25528.10
[INFO 2017-06-26 11:48:06,110 main.py:50] epoch 790, training loss: 13670.94, average training loss: 18615.96, base loss: 25526.27
[INFO 2017-06-26 11:48:06,844 main.py:50] epoch 791, training loss: 16623.39, average training loss: 18613.45, base loss: 25528.42
[INFO 2017-06-26 11:48:07,580 main.py:50] epoch 792, training loss: 13189.43, average training loss: 18606.61, base loss: 25524.56
[INFO 2017-06-26 11:48:08,312 main.py:50] epoch 793, training loss: 14456.72, average training loss: 18601.38, base loss: 25523.94
[INFO 2017-06-26 11:48:09,048 main.py:50] epoch 794, training loss: 12833.47, average training loss: 18594.12, base loss: 25518.71
[INFO 2017-06-26 11:48:09,784 main.py:50] epoch 795, training loss: 14552.35, average training loss: 18589.05, base loss: 25520.13
[INFO 2017-06-26 11:48:10,516 main.py:50] epoch 796, training loss: 17004.72, average training loss: 18587.06, base loss: 25523.65
[INFO 2017-06-26 11:48:11,249 main.py:50] epoch 797, training loss: 16813.26, average training loss: 18584.84, base loss: 25527.95
[INFO 2017-06-26 11:48:11,983 main.py:50] epoch 798, training loss: 13646.14, average training loss: 18578.65, base loss: 25525.82
[INFO 2017-06-26 11:48:12,718 main.py:50] epoch 799, training loss: 17405.82, average training loss: 18577.19, base loss: 25530.21
[INFO 2017-06-26 11:48:13,453 main.py:50] epoch 800, training loss: 17297.74, average training loss: 18575.59, base loss: 25534.28
[INFO 2017-06-26 11:48:14,189 main.py:50] epoch 801, training loss: 13479.29, average training loss: 18569.24, base loss: 25532.82
[INFO 2017-06-26 11:48:14,930 main.py:50] epoch 802, training loss: 16506.99, average training loss: 18566.67, base loss: 25535.73
[INFO 2017-06-26 11:48:15,729 main.py:50] epoch 803, training loss: 15907.49, average training loss: 18563.36, base loss: 25536.96
[INFO 2017-06-26 11:48:16,462 main.py:50] epoch 804, training loss: 13574.57, average training loss: 18557.16, base loss: 25534.47
[INFO 2017-06-26 11:48:17,222 main.py:50] epoch 805, training loss: 13980.09, average training loss: 18551.48, base loss: 25533.19
[INFO 2017-06-26 11:48:17,966 main.py:50] epoch 806, training loss: 13760.99, average training loss: 18545.55, base loss: 25531.59
[INFO 2017-06-26 11:48:18,700 main.py:50] epoch 807, training loss: 14139.70, average training loss: 18540.10, base loss: 25531.35
[INFO 2017-06-26 11:48:19,433 main.py:50] epoch 808, training loss: 15740.71, average training loss: 18536.64, base loss: 25532.30
[INFO 2017-06-26 11:48:20,251 main.py:50] epoch 809, training loss: 15354.06, average training loss: 18532.71, base loss: 25535.39
[INFO 2017-06-26 11:48:21,036 main.py:50] epoch 810, training loss: 17181.60, average training loss: 18531.04, base loss: 25539.97
[INFO 2017-06-26 11:48:21,805 main.py:50] epoch 811, training loss: 14122.70, average training loss: 18525.61, base loss: 25540.61
[INFO 2017-06-26 11:48:22,571 main.py:50] epoch 812, training loss: 13466.17, average training loss: 18519.39, base loss: 25537.82
[INFO 2017-06-26 11:48:23,325 main.py:50] epoch 813, training loss: 16908.31, average training loss: 18517.41, base loss: 25541.74
[INFO 2017-06-26 11:48:24,058 main.py:50] epoch 814, training loss: 14017.55, average training loss: 18511.89, base loss: 25540.43
[INFO 2017-06-26 11:48:24,834 main.py:50] epoch 815, training loss: 14003.51, average training loss: 18506.36, base loss: 25540.67
[INFO 2017-06-26 11:48:25,588 main.py:50] epoch 816, training loss: 14219.00, average training loss: 18501.12, base loss: 25540.34
[INFO 2017-06-26 11:48:26,323 main.py:50] epoch 817, training loss: 17124.62, average training loss: 18499.43, base loss: 25544.28
[INFO 2017-06-26 11:48:27,056 main.py:50] epoch 818, training loss: 14147.92, average training loss: 18494.12, base loss: 25543.33
[INFO 2017-06-26 11:48:27,791 main.py:50] epoch 819, training loss: 14161.67, average training loss: 18488.84, base loss: 25542.98
[INFO 2017-06-26 11:48:28,523 main.py:50] epoch 820, training loss: 13746.58, average training loss: 18483.06, base loss: 25540.58
[INFO 2017-06-26 11:48:29,254 main.py:50] epoch 821, training loss: 16222.25, average training loss: 18480.31, base loss: 25542.09
[INFO 2017-06-26 11:48:29,986 main.py:50] epoch 822, training loss: 14127.62, average training loss: 18475.02, base loss: 25541.94
[INFO 2017-06-26 11:48:30,717 main.py:50] epoch 823, training loss: 14096.63, average training loss: 18469.71, base loss: 25542.02
[INFO 2017-06-26 11:48:31,449 main.py:50] epoch 824, training loss: 15913.65, average training loss: 18466.61, base loss: 25542.14
[INFO 2017-06-26 11:48:32,182 main.py:50] epoch 825, training loss: 16433.72, average training loss: 18464.15, base loss: 25543.78
[INFO 2017-06-26 11:48:32,915 main.py:50] epoch 826, training loss: 13617.66, average training loss: 18458.29, base loss: 25540.99
[INFO 2017-06-26 11:48:33,648 main.py:50] epoch 827, training loss: 16976.53, average training loss: 18456.50, base loss: 25542.96
[INFO 2017-06-26 11:48:34,396 main.py:50] epoch 828, training loss: 12875.85, average training loss: 18449.77, base loss: 25538.97
[INFO 2017-06-26 11:48:35,129 main.py:50] epoch 829, training loss: 16990.19, average training loss: 18448.01, base loss: 25543.25
[INFO 2017-06-26 11:48:35,862 main.py:50] epoch 830, training loss: 14832.83, average training loss: 18443.66, base loss: 25544.67
[INFO 2017-06-26 11:48:36,639 main.py:50] epoch 831, training loss: 16599.11, average training loss: 18441.44, base loss: 25546.24
[INFO 2017-06-26 11:48:37,389 main.py:50] epoch 832, training loss: 14795.89, average training loss: 18437.06, base loss: 25545.56
[INFO 2017-06-26 11:48:38,126 main.py:50] epoch 833, training loss: 13687.68, average training loss: 18431.37, base loss: 25545.07
[INFO 2017-06-26 11:48:38,860 main.py:50] epoch 834, training loss: 13733.01, average training loss: 18425.74, base loss: 25543.57
[INFO 2017-06-26 11:48:39,593 main.py:50] epoch 835, training loss: 13966.58, average training loss: 18420.41, base loss: 25541.62
[INFO 2017-06-26 11:48:40,358 main.py:50] epoch 836, training loss: 13946.12, average training loss: 18415.06, base loss: 25541.15
[INFO 2017-06-26 11:48:41,092 main.py:50] epoch 837, training loss: 14015.84, average training loss: 18409.81, base loss: 25540.91
[INFO 2017-06-26 11:48:41,825 main.py:50] epoch 838, training loss: 13299.27, average training loss: 18403.72, base loss: 25537.88
[INFO 2017-06-26 11:48:42,560 main.py:50] epoch 839, training loss: 16349.33, average training loss: 18401.28, base loss: 25539.14
[INFO 2017-06-26 11:48:43,327 main.py:50] epoch 840, training loss: 17131.70, average training loss: 18399.77, base loss: 25542.83
[INFO 2017-06-26 11:48:44,092 main.py:50] epoch 841, training loss: 13963.29, average training loss: 18394.50, base loss: 25542.38
[INFO 2017-06-26 11:48:44,826 main.py:50] epoch 842, training loss: 13562.08, average training loss: 18388.76, base loss: 25539.53
[INFO 2017-06-26 11:48:45,559 main.py:50] epoch 843, training loss: 13499.94, average training loss: 18382.97, base loss: 25537.21
[INFO 2017-06-26 11:48:46,290 main.py:50] epoch 844, training loss: 16686.72, average training loss: 18380.96, base loss: 25539.82
[INFO 2017-06-26 11:48:47,021 main.py:50] epoch 845, training loss: 13002.45, average training loss: 18374.61, base loss: 25537.06
[INFO 2017-06-26 11:48:47,756 main.py:50] epoch 846, training loss: 13382.80, average training loss: 18368.71, base loss: 25535.38
[INFO 2017-06-26 11:48:48,504 main.py:50] epoch 847, training loss: 14104.83, average training loss: 18363.69, base loss: 25535.40
[INFO 2017-06-26 11:48:49,237 main.py:50] epoch 848, training loss: 14675.52, average training loss: 18359.34, base loss: 25537.03
[INFO 2017-06-26 11:48:49,972 main.py:50] epoch 849, training loss: 16244.65, average training loss: 18356.85, base loss: 25538.49
[INFO 2017-06-26 11:48:50,706 main.py:50] epoch 850, training loss: 13793.63, average training loss: 18351.49, base loss: 25537.76
[INFO 2017-06-26 11:48:51,444 main.py:50] epoch 851, training loss: 14411.73, average training loss: 18346.87, base loss: 25537.86
[INFO 2017-06-26 11:48:52,180 main.py:50] epoch 852, training loss: 14342.04, average training loss: 18342.17, base loss: 25538.05
[INFO 2017-06-26 11:48:52,914 main.py:50] epoch 853, training loss: 16770.49, average training loss: 18340.33, base loss: 25540.71
[INFO 2017-06-26 11:48:53,650 main.py:50] epoch 854, training loss: 14009.05, average training loss: 18335.27, base loss: 25541.08
[INFO 2017-06-26 11:48:54,385 main.py:50] epoch 855, training loss: 16309.91, average training loss: 18332.90, base loss: 25543.11
[INFO 2017-06-26 11:48:55,119 main.py:50] epoch 856, training loss: 13676.54, average training loss: 18327.47, base loss: 25541.73
[INFO 2017-06-26 11:48:55,853 main.py:50] epoch 857, training loss: 16459.09, average training loss: 18325.29, base loss: 25543.69
[INFO 2017-06-26 11:48:56,589 main.py:50] epoch 858, training loss: 13145.29, average training loss: 18319.26, base loss: 25541.19
[INFO 2017-06-26 11:48:57,325 main.py:50] epoch 859, training loss: 13003.87, average training loss: 18313.08, base loss: 25537.96
[INFO 2017-06-26 11:48:58,061 main.py:50] epoch 860, training loss: 13177.13, average training loss: 18307.11, base loss: 25535.35
[INFO 2017-06-26 11:48:58,798 main.py:50] epoch 861, training loss: 13346.61, average training loss: 18301.36, base loss: 25532.79
[INFO 2017-06-26 11:48:59,552 main.py:50] epoch 862, training loss: 16354.10, average training loss: 18299.10, base loss: 25535.58
[INFO 2017-06-26 11:49:00,289 main.py:50] epoch 863, training loss: 12743.91, average training loss: 18292.67, base loss: 25531.82
[INFO 2017-06-26 11:49:01,048 main.py:50] epoch 864, training loss: 16022.42, average training loss: 18290.05, base loss: 25532.89
[INFO 2017-06-26 11:49:01,785 main.py:50] epoch 865, training loss: 13874.36, average training loss: 18284.95, base loss: 25533.02
[INFO 2017-06-26 11:49:02,520 main.py:50] epoch 866, training loss: 16442.37, average training loss: 18282.82, base loss: 25536.32
[INFO 2017-06-26 11:49:03,257 main.py:50] epoch 867, training loss: 15914.70, average training loss: 18280.10, base loss: 25538.07
[INFO 2017-06-26 11:49:03,994 main.py:50] epoch 868, training loss: 12477.02, average training loss: 18273.42, base loss: 25533.95
[INFO 2017-06-26 11:49:04,760 main.py:50] epoch 869, training loss: 15374.76, average training loss: 18270.09, base loss: 25533.16
[INFO 2017-06-26 11:49:05,496 main.py:50] epoch 870, training loss: 16976.49, average training loss: 18268.60, base loss: 25537.37
[INFO 2017-06-26 11:49:06,270 main.py:50] epoch 871, training loss: 16606.62, average training loss: 18266.69, base loss: 25539.60
[INFO 2017-06-26 11:49:07,032 main.py:50] epoch 872, training loss: 13352.76, average training loss: 18261.07, base loss: 25536.73
[INFO 2017-06-26 11:49:07,767 main.py:50] epoch 873, training loss: 15344.82, average training loss: 18257.73, base loss: 25536.27
[INFO 2017-06-26 11:49:08,501 main.py:50] epoch 874, training loss: 15705.68, average training loss: 18254.81, base loss: 25536.83
[INFO 2017-06-26 11:49:09,235 main.py:50] epoch 875, training loss: 16220.00, average training loss: 18252.49, base loss: 25538.09
[INFO 2017-06-26 11:49:09,967 main.py:50] epoch 876, training loss: 16305.98, average training loss: 18250.27, base loss: 25539.76
[INFO 2017-06-26 11:49:10,698 main.py:50] epoch 877, training loss: 15331.30, average training loss: 18246.95, base loss: 25539.01
[INFO 2017-06-26 11:49:11,430 main.py:50] epoch 878, training loss: 14619.26, average training loss: 18242.82, base loss: 25537.28
[INFO 2017-06-26 11:49:12,162 main.py:50] epoch 879, training loss: 14422.17, average training loss: 18238.48, base loss: 25536.68
[INFO 2017-06-26 11:49:12,895 main.py:50] epoch 880, training loss: 14016.62, average training loss: 18233.68, base loss: 25535.19
[INFO 2017-06-26 11:49:13,628 main.py:50] epoch 881, training loss: 17426.98, average training loss: 18232.77, base loss: 25538.11
[INFO 2017-06-26 11:49:14,359 main.py:50] epoch 882, training loss: 14606.39, average training loss: 18228.66, base loss: 25538.67
[INFO 2017-06-26 11:49:15,091 main.py:50] epoch 883, training loss: 13861.77, average training loss: 18223.72, base loss: 25537.22
[INFO 2017-06-26 11:49:15,823 main.py:50] epoch 884, training loss: 14121.02, average training loss: 18219.09, base loss: 25535.95
[INFO 2017-06-26 11:49:16,555 main.py:50] epoch 885, training loss: 13999.58, average training loss: 18214.33, base loss: 25534.59
[INFO 2017-06-26 11:49:17,287 main.py:50] epoch 886, training loss: 15926.52, average training loss: 18211.75, base loss: 25533.85
[INFO 2017-06-26 11:49:18,020 main.py:50] epoch 887, training loss: 17047.03, average training loss: 18210.43, base loss: 25538.22
[INFO 2017-06-26 11:49:18,751 main.py:50] epoch 888, training loss: 13980.07, average training loss: 18205.68, base loss: 25537.46
[INFO 2017-06-26 11:49:19,485 main.py:50] epoch 889, training loss: 13642.66, average training loss: 18200.55, base loss: 25534.34
[INFO 2017-06-26 11:49:20,222 main.py:50] epoch 890, training loss: 13078.88, average training loss: 18194.80, base loss: 25531.19
[INFO 2017-06-26 11:49:21,009 main.py:50] epoch 891, training loss: 16434.97, average training loss: 18192.83, base loss: 25532.81
[INFO 2017-06-26 11:49:21,742 main.py:50] epoch 892, training loss: 14521.00, average training loss: 18188.72, base loss: 25532.12
[INFO 2017-06-26 11:49:22,477 main.py:50] epoch 893, training loss: 13614.62, average training loss: 18183.60, base loss: 25528.89
[INFO 2017-06-26 11:49:23,212 main.py:50] epoch 894, training loss: 15793.94, average training loss: 18180.93, base loss: 25529.52
[INFO 2017-06-26 11:49:23,946 main.py:50] epoch 895, training loss: 13776.78, average training loss: 18176.01, base loss: 25528.13
[INFO 2017-06-26 11:49:24,701 main.py:50] epoch 896, training loss: 14118.07, average training loss: 18171.49, base loss: 25527.67
[INFO 2017-06-26 11:49:25,473 main.py:50] epoch 897, training loss: 12872.02, average training loss: 18165.59, base loss: 25524.55
[INFO 2017-06-26 11:49:26,212 main.py:50] epoch 898, training loss: 16890.60, average training loss: 18164.17, base loss: 25527.46
[INFO 2017-06-26 11:49:26,945 main.py:50] epoch 899, training loss: 16667.54, average training loss: 18162.51, base loss: 25529.50
[INFO 2017-06-26 11:49:27,680 main.py:50] epoch 900, training loss: 15337.13, average training loss: 18159.37, base loss: 25529.47
[INFO 2017-06-26 11:49:28,437 main.py:50] epoch 901, training loss: 13884.24, average training loss: 18154.63, base loss: 25528.59
[INFO 2017-06-26 11:49:29,170 main.py:50] epoch 902, training loss: 15271.67, average training loss: 18151.44, base loss: 25530.58
[INFO 2017-06-26 11:49:29,905 main.py:50] epoch 903, training loss: 15578.95, average training loss: 18148.59, base loss: 25530.90
[INFO 2017-06-26 11:49:30,638 main.py:50] epoch 904, training loss: 13658.47, average training loss: 18143.63, base loss: 25528.64
[INFO 2017-06-26 11:49:31,373 main.py:50] epoch 905, training loss: 18089.51, average training loss: 18143.57, base loss: 25533.70
[INFO 2017-06-26 11:49:32,110 main.py:50] epoch 906, training loss: 13919.73, average training loss: 18138.92, base loss: 25532.02
[INFO 2017-06-26 11:49:32,846 main.py:50] epoch 907, training loss: 13541.50, average training loss: 18133.85, base loss: 25528.65
[INFO 2017-06-26 11:49:33,582 main.py:50] epoch 908, training loss: 14456.96, average training loss: 18129.81, base loss: 25528.33
[INFO 2017-06-26 11:49:34,319 main.py:50] epoch 909, training loss: 13955.11, average training loss: 18125.22, base loss: 25527.63
[INFO 2017-06-26 11:49:35,055 main.py:50] epoch 910, training loss: 16283.66, average training loss: 18123.20, base loss: 25529.13
[INFO 2017-06-26 11:49:35,788 main.py:50] epoch 911, training loss: 14158.44, average training loss: 18118.85, base loss: 25527.73
[INFO 2017-06-26 11:49:36,523 main.py:50] epoch 912, training loss: 13477.74, average training loss: 18113.77, base loss: 25525.39
[INFO 2017-06-26 11:49:37,260 main.py:50] epoch 913, training loss: 13677.28, average training loss: 18108.91, base loss: 25524.63
[INFO 2017-06-26 11:49:38,027 main.py:50] epoch 914, training loss: 14164.52, average training loss: 18104.60, base loss: 25524.80
[INFO 2017-06-26 11:49:38,786 main.py:50] epoch 915, training loss: 16185.67, average training loss: 18102.51, base loss: 25524.89
[INFO 2017-06-26 11:49:39,552 main.py:50] epoch 916, training loss: 13629.46, average training loss: 18097.63, base loss: 25521.59
[INFO 2017-06-26 11:49:40,297 main.py:50] epoch 917, training loss: 15008.98, average training loss: 18094.27, base loss: 25522.84
[INFO 2017-06-26 11:49:41,032 main.py:50] epoch 918, training loss: 14222.74, average training loss: 18090.05, base loss: 25522.46
[INFO 2017-06-26 11:49:41,765 main.py:50] epoch 919, training loss: 17214.73, average training loss: 18089.10, base loss: 25526.48
[INFO 2017-06-26 11:49:42,521 main.py:50] epoch 920, training loss: 15890.29, average training loss: 18086.71, base loss: 25526.21
[INFO 2017-06-26 11:49:43,283 main.py:50] epoch 921, training loss: 14537.11, average training loss: 18082.86, base loss: 25527.11
[INFO 2017-06-26 11:49:44,049 main.py:50] epoch 922, training loss: 17534.04, average training loss: 18082.27, base loss: 25531.81
[INFO 2017-06-26 11:49:44,783 main.py:50] epoch 923, training loss: 15995.81, average training loss: 18080.01, base loss: 25532.39
[INFO 2017-06-26 11:49:45,554 main.py:50] epoch 924, training loss: 14138.11, average training loss: 18075.75, base loss: 25532.48
[INFO 2017-06-26 11:49:46,289 main.py:50] epoch 925, training loss: 14284.24, average training loss: 18071.66, base loss: 25532.83
[INFO 2017-06-26 11:49:47,021 main.py:50] epoch 926, training loss: 16397.59, average training loss: 18069.85, base loss: 25536.08
[INFO 2017-06-26 11:49:47,754 main.py:50] epoch 927, training loss: 15000.01, average training loss: 18066.54, base loss: 25536.61
[INFO 2017-06-26 11:49:48,487 main.py:50] epoch 928, training loss: 15915.24, average training loss: 18064.23, base loss: 25537.47
[INFO 2017-06-26 11:49:49,221 main.py:50] epoch 929, training loss: 13474.84, average training loss: 18059.29, base loss: 25535.74
[INFO 2017-06-26 11:49:49,955 main.py:50] epoch 930, training loss: 14094.86, average training loss: 18055.03, base loss: 25534.10
[INFO 2017-06-26 11:49:50,689 main.py:50] epoch 931, training loss: 13677.47, average training loss: 18050.34, base loss: 25532.73
[INFO 2017-06-26 11:49:51,423 main.py:50] epoch 932, training loss: 13215.76, average training loss: 18045.15, base loss: 25530.46
[INFO 2017-06-26 11:49:52,158 main.py:50] epoch 933, training loss: 16027.58, average training loss: 18042.99, base loss: 25532.57
[INFO 2017-06-26 11:49:52,891 main.py:50] epoch 934, training loss: 16007.85, average training loss: 18040.82, base loss: 25533.88
[INFO 2017-06-26 11:49:53,640 main.py:50] epoch 935, training loss: 13594.77, average training loss: 18036.07, base loss: 25531.83
[INFO 2017-06-26 11:49:54,374 main.py:50] epoch 936, training loss: 13787.94, average training loss: 18031.53, base loss: 25531.23
[INFO 2017-06-26 11:49:55,108 main.py:50] epoch 937, training loss: 15073.42, average training loss: 18028.38, base loss: 25529.68
[INFO 2017-06-26 11:49:55,842 main.py:50] epoch 938, training loss: 16144.75, average training loss: 18026.37, base loss: 25531.28
[INFO 2017-06-26 11:49:56,577 main.py:50] epoch 939, training loss: 15854.62, average training loss: 18024.06, base loss: 25534.13
[INFO 2017-06-26 11:49:57,311 main.py:50] epoch 940, training loss: 14005.94, average training loss: 18019.79, base loss: 25534.36
[INFO 2017-06-26 11:49:58,046 main.py:50] epoch 941, training loss: 13957.51, average training loss: 18015.48, base loss: 25533.80
[INFO 2017-06-26 11:49:58,781 main.py:50] epoch 942, training loss: 14150.82, average training loss: 18011.38, base loss: 25534.11
[INFO 2017-06-26 11:49:59,532 main.py:50] epoch 943, training loss: 13468.11, average training loss: 18006.57, base loss: 25533.39
[INFO 2017-06-26 11:50:00,301 main.py:50] epoch 944, training loss: 15627.01, average training loss: 18004.05, base loss: 25534.76
[INFO 2017-06-26 11:50:01,035 main.py:50] epoch 945, training loss: 13499.58, average training loss: 17999.29, base loss: 25532.73
[INFO 2017-06-26 11:50:01,809 main.py:50] epoch 946, training loss: 13944.18, average training loss: 17995.01, base loss: 25532.91
[INFO 2017-06-26 11:50:02,560 main.py:50] epoch 947, training loss: 13217.72, average training loss: 17989.97, base loss: 25529.85
[INFO 2017-06-26 11:50:03,295 main.py:50] epoch 948, training loss: 15242.80, average training loss: 17987.07, base loss: 25529.94
[INFO 2017-06-26 11:50:04,056 main.py:50] epoch 949, training loss: 14516.16, average training loss: 17983.42, base loss: 25530.63
[INFO 2017-06-26 11:50:04,824 main.py:50] epoch 950, training loss: 13850.42, average training loss: 17979.07, base loss: 25529.46
[INFO 2017-06-26 11:50:05,603 main.py:50] epoch 951, training loss: 14063.07, average training loss: 17974.96, base loss: 25529.82
[INFO 2017-06-26 11:50:06,337 main.py:50] epoch 952, training loss: 13182.45, average training loss: 17969.93, base loss: 25527.49
[INFO 2017-06-26 11:50:07,071 main.py:50] epoch 953, training loss: 15372.26, average training loss: 17967.21, base loss: 25529.07
[INFO 2017-06-26 11:50:07,806 main.py:50] epoch 954, training loss: 13763.23, average training loss: 17962.81, base loss: 25528.66
[INFO 2017-06-26 11:50:08,541 main.py:50] epoch 955, training loss: 13824.67, average training loss: 17958.48, base loss: 25528.42
[INFO 2017-06-26 11:50:09,275 main.py:50] epoch 956, training loss: 13486.80, average training loss: 17953.81, base loss: 25527.40
[INFO 2017-06-26 11:50:10,007 main.py:50] epoch 957, training loss: 13629.03, average training loss: 17949.29, base loss: 25526.76
[INFO 2017-06-26 11:50:10,739 main.py:50] epoch 958, training loss: 16197.29, average training loss: 17947.46, base loss: 25529.98
[INFO 2017-06-26 11:50:11,470 main.py:50] epoch 959, training loss: 15575.47, average training loss: 17944.99, base loss: 25531.67
[INFO 2017-06-26 11:50:12,202 main.py:50] epoch 960, training loss: 13495.11, average training loss: 17940.36, base loss: 25531.22
[INFO 2017-06-26 11:50:12,934 main.py:50] epoch 961, training loss: 13737.08, average training loss: 17935.99, base loss: 25531.41
[INFO 2017-06-26 11:50:13,665 main.py:50] epoch 962, training loss: 14342.50, average training loss: 17932.26, base loss: 25532.85
[INFO 2017-06-26 11:50:14,397 main.py:50] epoch 963, training loss: 13668.47, average training loss: 17927.84, base loss: 25532.45
[INFO 2017-06-26 11:50:15,129 main.py:50] epoch 964, training loss: 16147.62, average training loss: 17925.99, base loss: 25535.95
[INFO 2017-06-26 11:50:15,862 main.py:50] epoch 965, training loss: 13120.67, average training loss: 17921.02, base loss: 25533.66
[INFO 2017-06-26 11:50:16,595 main.py:50] epoch 966, training loss: 16219.40, average training loss: 17919.26, base loss: 25536.96
[INFO 2017-06-26 11:50:17,328 main.py:50] epoch 967, training loss: 13224.63, average training loss: 17914.41, base loss: 25535.70
[INFO 2017-06-26 11:50:18,061 main.py:50] epoch 968, training loss: 13633.92, average training loss: 17909.99, base loss: 25535.28
[INFO 2017-06-26 11:50:18,794 main.py:50] epoch 969, training loss: 13219.80, average training loss: 17905.16, base loss: 25532.78
[INFO 2017-06-26 11:50:19,525 main.py:50] epoch 970, training loss: 14237.52, average training loss: 17901.38, base loss: 25534.61
[INFO 2017-06-26 11:50:20,256 main.py:50] epoch 971, training loss: 13316.83, average training loss: 17896.66, base loss: 25533.10
[INFO 2017-06-26 11:50:20,988 main.py:50] epoch 972, training loss: 15903.50, average training loss: 17894.62, base loss: 25536.08
[INFO 2017-06-26 11:50:21,721 main.py:50] epoch 973, training loss: 13555.22, average training loss: 17890.16, base loss: 25535.50
[INFO 2017-06-26 11:50:22,453 main.py:50] epoch 974, training loss: 13476.86, average training loss: 17885.63, base loss: 25534.21
[INFO 2017-06-26 11:50:23,185 main.py:50] epoch 975, training loss: 12786.50, average training loss: 17880.41, base loss: 25531.18
[INFO 2017-06-26 11:50:23,917 main.py:50] epoch 976, training loss: 13927.05, average training loss: 17876.36, base loss: 25531.61
[INFO 2017-06-26 11:50:24,651 main.py:50] epoch 977, training loss: 13287.13, average training loss: 17871.67, base loss: 25530.29
[INFO 2017-06-26 11:50:25,384 main.py:50] epoch 978, training loss: 15084.34, average training loss: 17868.82, base loss: 25531.32
[INFO 2017-06-26 11:50:26,129 main.py:50] epoch 979, training loss: 13126.34, average training loss: 17863.98, base loss: 25529.19
[INFO 2017-06-26 11:50:26,861 main.py:50] epoch 980, training loss: 16633.32, average training loss: 17862.73, base loss: 25533.80
[INFO 2017-06-26 11:50:27,615 main.py:50] epoch 981, training loss: 13311.76, average training loss: 17858.10, base loss: 25531.10
[INFO 2017-06-26 11:50:28,350 main.py:50] epoch 982, training loss: 12732.25, average training loss: 17852.88, base loss: 25528.30
[INFO 2017-06-26 11:50:29,080 main.py:50] epoch 983, training loss: 13046.84, average training loss: 17848.00, base loss: 25525.06
[INFO 2017-06-26 11:50:29,812 main.py:50] epoch 984, training loss: 13444.67, average training loss: 17843.53, base loss: 25524.45
[INFO 2017-06-26 11:50:30,545 main.py:50] epoch 985, training loss: 13661.12, average training loss: 17839.28, base loss: 25523.97
[INFO 2017-06-26 11:50:31,276 main.py:50] epoch 986, training loss: 13641.04, average training loss: 17835.03, base loss: 25523.06
[INFO 2017-06-26 11:50:32,007 main.py:50] epoch 987, training loss: 14648.27, average training loss: 17831.81, base loss: 25522.95
[INFO 2017-06-26 11:50:32,738 main.py:50] epoch 988, training loss: 15018.57, average training loss: 17828.96, base loss: 25523.36
[INFO 2017-06-26 11:50:33,469 main.py:50] epoch 989, training loss: 16110.71, average training loss: 17827.23, base loss: 25525.44
[INFO 2017-06-26 11:50:34,200 main.py:50] epoch 990, training loss: 13980.20, average training loss: 17823.34, base loss: 25525.10
[INFO 2017-06-26 11:50:34,930 main.py:50] epoch 991, training loss: 13450.14, average training loss: 17818.93, base loss: 25523.27
[INFO 2017-06-26 11:50:35,661 main.py:50] epoch 992, training loss: 13947.50, average training loss: 17815.04, base loss: 25523.32
[INFO 2017-06-26 11:50:36,396 main.py:50] epoch 993, training loss: 16041.06, average training loss: 17813.25, base loss: 25526.75
[INFO 2017-06-26 11:50:37,127 main.py:50] epoch 994, training loss: 13850.47, average training loss: 17809.27, base loss: 25526.46
[INFO 2017-06-26 11:50:37,858 main.py:50] epoch 995, training loss: 13974.35, average training loss: 17805.42, base loss: 25526.39
[INFO 2017-06-26 11:50:38,591 main.py:50] epoch 996, training loss: 13808.34, average training loss: 17801.41, base loss: 25525.88
[INFO 2017-06-26 11:50:39,323 main.py:50] epoch 997, training loss: 15581.01, average training loss: 17799.18, base loss: 25527.99
[INFO 2017-06-26 11:50:40,055 main.py:50] epoch 998, training loss: 13393.02, average training loss: 17794.77, base loss: 25527.03
[INFO 2017-06-26 11:50:40,787 main.py:50] epoch 999, training loss: 12794.33, average training loss: 17789.77, base loss: 25524.70
[INFO 2017-06-26 11:50:40,787 main.py:52] epoch 999, testing
[INFO 2017-06-26 11:51:08,236 main.py:103] average testing loss: 14251.18, base loss: 25263.91
[INFO 2017-06-26 11:51:08,237 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 11:51:08,242 main.py:76] current best accuracy: 14251.18
[INFO 2017-06-26 11:51:08,974 main.py:50] epoch 1000, training loss: 14508.77, average training loss: 17585.93, base loss: 25523.63
[INFO 2017-06-26 11:51:09,708 main.py:50] epoch 1001, training loss: 12907.87, average training loss: 17417.96, base loss: 25518.46
[INFO 2017-06-26 11:51:10,439 main.py:50] epoch 1002, training loss: 15887.86, average training loss: 17288.41, base loss: 25521.24
[INFO 2017-06-26 11:51:11,170 main.py:50] epoch 1003, training loss: 13472.61, average training loss: 17181.87, base loss: 25521.29
[INFO 2017-06-26 11:51:11,901 main.py:50] epoch 1004, training loss: 13068.42, average training loss: 17089.05, base loss: 25520.94
[INFO 2017-06-26 11:51:12,634 main.py:50] epoch 1005, training loss: 14092.23, average training loss: 17009.07, base loss: 25523.33
[INFO 2017-06-26 11:51:13,366 main.py:50] epoch 1006, training loss: 13841.84, average training loss: 16940.31, base loss: 25523.81
[INFO 2017-06-26 11:51:14,097 main.py:50] epoch 1007, training loss: 13518.54, average training loss: 16880.36, base loss: 25524.34
[INFO 2017-06-26 11:51:14,829 main.py:50] epoch 1008, training loss: 13115.93, average training loss: 16827.76, base loss: 25519.78
[INFO 2017-06-26 11:51:15,561 main.py:50] epoch 1009, training loss: 15862.54, average training loss: 16786.99, base loss: 25524.26
[INFO 2017-06-26 11:51:16,307 main.py:50] epoch 1010, training loss: 15635.37, average training loss: 16752.14, base loss: 25526.99
[INFO 2017-06-26 11:51:17,042 main.py:50] epoch 1011, training loss: 13858.49, average training loss: 16719.70, base loss: 25526.58
[INFO 2017-06-26 11:51:17,780 main.py:50] epoch 1012, training loss: 13298.13, average training loss: 16689.47, base loss: 25524.14
[INFO 2017-06-26 11:51:18,513 main.py:50] epoch 1013, training loss: 13319.96, average training loss: 16664.66, base loss: 25522.92
[INFO 2017-06-26 11:51:19,251 main.py:50] epoch 1014, training loss: 16226.17, average training loss: 16645.31, base loss: 25526.78
[INFO 2017-06-26 11:51:19,989 main.py:50] epoch 1015, training loss: 12913.63, average training loss: 16622.33, base loss: 25521.82
[INFO 2017-06-26 11:51:20,722 main.py:50] epoch 1016, training loss: 15204.34, average training loss: 16606.70, base loss: 25526.59
[INFO 2017-06-26 11:51:21,455 main.py:50] epoch 1017, training loss: 13873.35, average training loss: 16589.88, base loss: 25527.00
[INFO 2017-06-26 11:51:22,188 main.py:50] epoch 1018, training loss: 15877.07, average training loss: 16575.84, base loss: 25529.79
[INFO 2017-06-26 11:51:22,920 main.py:50] epoch 1019, training loss: 13789.93, average training loss: 16561.23, base loss: 25529.23
[INFO 2017-06-26 11:51:23,654 main.py:50] epoch 1020, training loss: 13803.58, average training loss: 16546.52, base loss: 25529.12
[INFO 2017-06-26 11:51:24,387 main.py:50] epoch 1021, training loss: 15396.54, average training loss: 16535.45, base loss: 25532.38
[INFO 2017-06-26 11:51:25,121 main.py:50] epoch 1022, training loss: 13341.31, average training loss: 16522.63, base loss: 25531.53
[INFO 2017-06-26 11:51:25,855 main.py:50] epoch 1023, training loss: 14255.34, average training loss: 16511.28, base loss: 25533.19
[INFO 2017-06-26 11:51:26,588 main.py:50] epoch 1024, training loss: 13282.23, average training loss: 16497.42, base loss: 25530.23
[INFO 2017-06-26 11:51:27,320 main.py:50] epoch 1025, training loss: 16195.26, average training loss: 16488.85, base loss: 25534.58
[INFO 2017-06-26 11:51:28,053 main.py:50] epoch 1026, training loss: 14505.06, average training loss: 16475.19, base loss: 25530.27
[INFO 2017-06-26 11:51:28,785 main.py:50] epoch 1027, training loss: 13916.02, average training loss: 16466.68, base loss: 25533.11
[INFO 2017-06-26 11:51:29,518 main.py:50] epoch 1028, training loss: 15516.83, average training loss: 16455.85, base loss: 25533.13
[INFO 2017-06-26 11:51:30,252 main.py:50] epoch 1029, training loss: 15703.80, average training loss: 16445.78, base loss: 25534.45
[INFO 2017-06-26 11:51:30,985 main.py:50] epoch 1030, training loss: 13105.17, average training loss: 16433.67, base loss: 25531.78
[INFO 2017-06-26 11:51:31,717 main.py:50] epoch 1031, training loss: 15449.23, average training loss: 16426.61, base loss: 25535.89
[INFO 2017-06-26 11:51:32,451 main.py:50] epoch 1032, training loss: 14527.40, average training loss: 16415.63, base loss: 25533.20
[INFO 2017-06-26 11:51:33,184 main.py:50] epoch 1033, training loss: 13131.87, average training loss: 16402.56, base loss: 25529.64
[INFO 2017-06-26 11:51:33,918 main.py:50] epoch 1034, training loss: 13467.91, average training loss: 16392.24, base loss: 25529.23
[INFO 2017-06-26 11:51:34,650 main.py:50] epoch 1035, training loss: 12549.05, average training loss: 16381.31, base loss: 25527.82
[INFO 2017-06-26 11:51:35,383 main.py:50] epoch 1036, training loss: 13120.17, average training loss: 16371.12, base loss: 25525.87
[INFO 2017-06-26 11:51:36,116 main.py:50] epoch 1037, training loss: 15072.78, average training loss: 16358.13, base loss: 25522.29
[INFO 2017-06-26 11:51:36,848 main.py:50] epoch 1038, training loss: 13921.02, average training loss: 16346.36, base loss: 25518.59
[INFO 2017-06-26 11:51:37,580 main.py:50] epoch 1039, training loss: 16161.17, average training loss: 16336.53, base loss: 25519.87
[INFO 2017-06-26 11:51:38,312 main.py:50] epoch 1040, training loss: 12844.54, average training loss: 16325.42, base loss: 25516.62
[INFO 2017-06-26 11:51:39,045 main.py:50] epoch 1041, training loss: 13769.69, average training loss: 16316.44, base loss: 25516.86
[INFO 2017-06-26 11:51:39,778 main.py:50] epoch 1042, training loss: 13455.88, average training loss: 16306.98, base loss: 25517.91
[INFO 2017-06-26 11:51:40,510 main.py:50] epoch 1043, training loss: 15175.48, average training loss: 16298.93, base loss: 25520.57
[INFO 2017-06-26 11:51:41,245 main.py:50] epoch 1044, training loss: 13952.53, average training loss: 16290.98, base loss: 25522.95
[INFO 2017-06-26 11:51:41,978 main.py:50] epoch 1045, training loss: 15989.53, average training loss: 16281.07, base loss: 25523.61
[INFO 2017-06-26 11:51:42,710 main.py:50] epoch 1046, training loss: 13882.27, average training loss: 16269.28, base loss: 25521.26
[INFO 2017-06-26 11:51:43,445 main.py:50] epoch 1047, training loss: 13780.04, average training loss: 16260.05, base loss: 25521.31
[INFO 2017-06-26 11:51:44,178 main.py:50] epoch 1048, training loss: 13832.73, average training loss: 16250.16, base loss: 25521.22
[INFO 2017-06-26 11:51:44,912 main.py:50] epoch 1049, training loss: 13860.74, average training loss: 16242.42, base loss: 25523.52
[INFO 2017-06-26 11:51:45,645 main.py:50] epoch 1050, training loss: 16433.56, average training loss: 16236.28, base loss: 25527.23
[INFO 2017-06-26 11:51:46,377 main.py:50] epoch 1051, training loss: 13804.88, average training loss: 16228.71, base loss: 25530.50
[INFO 2017-06-26 11:51:47,109 main.py:50] epoch 1052, training loss: 13823.20, average training loss: 16219.02, base loss: 25531.11
[INFO 2017-06-26 11:51:47,840 main.py:50] epoch 1053, training loss: 12676.84, average training loss: 16210.34, base loss: 25530.66
[INFO 2017-06-26 11:51:48,586 main.py:50] epoch 1054, training loss: 15448.53, average training loss: 16205.48, base loss: 25536.39
[INFO 2017-06-26 11:51:49,319 main.py:50] epoch 1055, training loss: 13640.20, average training loss: 16196.54, base loss: 25536.24
[INFO 2017-06-26 11:51:50,054 main.py:50] epoch 1056, training loss: 12766.53, average training loss: 16187.19, base loss: 25535.46
[INFO 2017-06-26 11:51:50,788 main.py:50] epoch 1057, training loss: 13080.41, average training loss: 16177.34, base loss: 25534.18
[INFO 2017-06-26 11:51:51,519 main.py:50] epoch 1058, training loss: 14136.56, average training loss: 16166.49, base loss: 25532.80
[INFO 2017-06-26 11:51:52,253 main.py:50] epoch 1059, training loss: 14171.84, average training loss: 16157.29, base loss: 25533.25
[INFO 2017-06-26 11:51:52,986 main.py:50] epoch 1060, training loss: 13776.79, average training loss: 16147.82, base loss: 25532.62
[INFO 2017-06-26 11:51:53,719 main.py:50] epoch 1061, training loss: 14944.56, average training loss: 16140.96, base loss: 25534.92
[INFO 2017-06-26 11:51:54,450 main.py:50] epoch 1062, training loss: 15574.70, average training loss: 16135.43, base loss: 25539.52
[INFO 2017-06-26 11:51:55,182 main.py:50] epoch 1063, training loss: 15304.05, average training loss: 16129.07, base loss: 25542.29
[INFO 2017-06-26 11:51:55,914 main.py:50] epoch 1064, training loss: 13336.86, average training loss: 16122.21, base loss: 25545.83
[INFO 2017-06-26 11:51:56,646 main.py:50] epoch 1065, training loss: 13132.19, average training loss: 16112.95, base loss: 25543.54
[INFO 2017-06-26 11:51:57,378 main.py:50] epoch 1066, training loss: 15533.44, average training loss: 16104.44, base loss: 25543.85
[INFO 2017-06-26 11:51:58,109 main.py:50] epoch 1067, training loss: 13223.48, average training loss: 16097.08, base loss: 25545.37
[INFO 2017-06-26 11:51:58,841 main.py:50] epoch 1068, training loss: 15128.54, average training loss: 16090.40, base loss: 25548.22
[INFO 2017-06-26 11:51:59,572 main.py:50] epoch 1069, training loss: 12950.54, average training loss: 16083.33, base loss: 25548.79
[INFO 2017-06-26 11:52:00,304 main.py:50] epoch 1070, training loss: 14829.62, average training loss: 16077.87, base loss: 25551.90
[INFO 2017-06-26 11:52:01,037 main.py:50] epoch 1071, training loss: 15868.78, average training loss: 16072.51, base loss: 25555.62
[INFO 2017-06-26 11:52:01,768 main.py:50] epoch 1072, training loss: 15574.79, average training loss: 16065.39, base loss: 25559.32
[INFO 2017-06-26 11:52:02,500 main.py:50] epoch 1073, training loss: 12727.94, average training loss: 16057.43, base loss: 25556.60
[INFO 2017-06-26 11:52:03,233 main.py:50] epoch 1074, training loss: 12617.74, average training loss: 16049.00, base loss: 25554.19
[INFO 2017-06-26 11:52:03,964 main.py:50] epoch 1075, training loss: 14024.30, average training loss: 16042.19, base loss: 25555.76
[INFO 2017-06-26 11:52:04,697 main.py:50] epoch 1076, training loss: 13179.98, average training loss: 16034.77, base loss: 25555.35
[INFO 2017-06-26 11:52:05,429 main.py:50] epoch 1077, training loss: 14639.11, average training loss: 16027.74, base loss: 25555.02
[INFO 2017-06-26 11:52:06,160 main.py:50] epoch 1078, training loss: 13015.77, average training loss: 16020.44, base loss: 25553.63
[INFO 2017-06-26 11:52:06,892 main.py:50] epoch 1079, training loss: 12631.62, average training loss: 16013.48, base loss: 25554.25
[INFO 2017-06-26 11:52:07,627 main.py:50] epoch 1080, training loss: 15050.76, average training loss: 16008.15, base loss: 25556.97
[INFO 2017-06-26 11:52:08,360 main.py:50] epoch 1081, training loss: 12036.72, average training loss: 15999.64, base loss: 25553.23
[INFO 2017-06-26 11:52:09,093 main.py:50] epoch 1082, training loss: 12659.35, average training loss: 15991.17, base loss: 25551.52
[INFO 2017-06-26 11:52:09,827 main.py:50] epoch 1083, training loss: 13354.65, average training loss: 15985.00, base loss: 25553.16
[INFO 2017-06-26 11:52:10,559 main.py:50] epoch 1084, training loss: 15621.18, average training loss: 15979.93, base loss: 25556.58
[INFO 2017-06-26 11:52:11,291 main.py:50] epoch 1085, training loss: 13198.93, average training loss: 15973.62, base loss: 25559.33
[INFO 2017-06-26 11:52:12,023 main.py:50] epoch 1086, training loss: 12852.60, average training loss: 15966.22, base loss: 25557.20
[INFO 2017-06-26 11:52:12,755 main.py:50] epoch 1087, training loss: 13351.31, average training loss: 15958.78, base loss: 25556.05
[INFO 2017-06-26 11:52:13,486 main.py:50] epoch 1088, training loss: 13782.65, average training loss: 15952.85, base loss: 25557.91
[INFO 2017-06-26 11:52:14,217 main.py:50] epoch 1089, training loss: 15083.86, average training loss: 15946.77, base loss: 25559.37
[INFO 2017-06-26 11:52:14,950 main.py:50] epoch 1090, training loss: 13250.09, average training loss: 15940.23, base loss: 25560.53
[INFO 2017-06-26 11:52:15,682 main.py:50] epoch 1091, training loss: 13213.06, average training loss: 15931.27, base loss: 25556.23
[INFO 2017-06-26 11:52:16,413 main.py:50] epoch 1092, training loss: 13992.40, average training loss: 15922.44, base loss: 25554.09
[INFO 2017-06-26 11:52:17,146 main.py:50] epoch 1093, training loss: 13166.27, average training loss: 15913.12, base loss: 25549.71
[INFO 2017-06-26 11:52:17,878 main.py:50] epoch 1094, training loss: 12910.60, average training loss: 15905.85, base loss: 25547.05
[INFO 2017-06-26 11:52:18,609 main.py:50] epoch 1095, training loss: 14019.63, average training loss: 15896.83, base loss: 25544.94
[INFO 2017-06-26 11:52:19,341 main.py:50] epoch 1096, training loss: 13469.07, average training loss: 15887.26, base loss: 25542.66
[INFO 2017-06-26 11:52:20,073 main.py:50] epoch 1097, training loss: 15583.23, average training loss: 15883.30, base loss: 25546.12
[INFO 2017-06-26 11:52:20,819 main.py:50] epoch 1098, training loss: 12996.47, average training loss: 15876.96, base loss: 25548.08
[INFO 2017-06-26 11:52:21,550 main.py:50] epoch 1099, training loss: 13474.24, average training loss: 15870.59, base loss: 25547.56
[INFO 2017-06-26 11:52:22,282 main.py:50] epoch 1100, training loss: 12821.07, average training loss: 15860.40, base loss: 25542.39
[INFO 2017-06-26 11:52:23,014 main.py:50] epoch 1101, training loss: 13238.47, average training loss: 15851.51, base loss: 25540.29
[INFO 2017-06-26 11:52:23,745 main.py:50] epoch 1102, training loss: 13417.19, average training loss: 15845.60, base loss: 25541.95
[INFO 2017-06-26 11:52:24,477 main.py:50] epoch 1103, training loss: 13244.63, average training loss: 15839.90, base loss: 25543.55
[INFO 2017-06-26 11:52:25,208 main.py:50] epoch 1104, training loss: 13913.02, average training loss: 15832.88, base loss: 25542.17
[INFO 2017-06-26 11:52:25,940 main.py:50] epoch 1105, training loss: 13039.17, average training loss: 15824.22, base loss: 25539.58
[INFO 2017-06-26 11:52:26,672 main.py:50] epoch 1106, training loss: 13241.76, average training loss: 15814.76, base loss: 25536.74
[INFO 2017-06-26 11:52:27,403 main.py:50] epoch 1107, training loss: 13318.29, average training loss: 15808.82, base loss: 25537.11
[INFO 2017-06-26 11:52:28,134 main.py:50] epoch 1108, training loss: 13879.21, average training loss: 15803.46, base loss: 25538.32
[INFO 2017-06-26 11:52:28,867 main.py:50] epoch 1109, training loss: 13604.71, average training loss: 15793.64, base loss: 25535.63
[INFO 2017-06-26 11:52:29,601 main.py:50] epoch 1110, training loss: 12411.72, average training loss: 15786.34, base loss: 25533.00
[INFO 2017-06-26 11:52:30,333 main.py:50] epoch 1111, training loss: 13867.66, average training loss: 15780.65, base loss: 25533.42
[INFO 2017-06-26 11:52:31,065 main.py:50] epoch 1112, training loss: 14326.04, average training loss: 15775.58, base loss: 25534.57
[INFO 2017-06-26 11:52:31,797 main.py:50] epoch 1113, training loss: 13715.60, average training loss: 15766.81, base loss: 25531.74
[INFO 2017-06-26 11:52:32,529 main.py:50] epoch 1114, training loss: 12907.28, average training loss: 15760.10, base loss: 25529.71
[INFO 2017-06-26 11:52:33,260 main.py:50] epoch 1115, training loss: 14698.78, average training loss: 15753.21, base loss: 25528.35
[INFO 2017-06-26 11:52:33,993 main.py:50] epoch 1116, training loss: 14406.19, average training loss: 15746.34, base loss: 25527.20
[INFO 2017-06-26 11:52:34,724 main.py:50] epoch 1117, training loss: 13509.51, average training loss: 15740.69, base loss: 25526.55
[INFO 2017-06-26 11:52:35,455 main.py:50] epoch 1118, training loss: 15173.70, average training loss: 15735.67, base loss: 25527.47
[INFO 2017-06-26 11:52:36,187 main.py:50] epoch 1119, training loss: 14028.76, average training loss: 15730.90, base loss: 25530.23
[INFO 2017-06-26 11:52:36,919 main.py:50] epoch 1120, training loss: 13130.26, average training loss: 15724.73, base loss: 25529.08
[INFO 2017-06-26 11:52:37,651 main.py:50] epoch 1121, training loss: 13837.47, average training loss: 15717.20, base loss: 25526.88
[INFO 2017-06-26 11:52:38,382 main.py:50] epoch 1122, training loss: 12947.97, average training loss: 15708.97, base loss: 25524.14
[INFO 2017-06-26 11:52:39,114 main.py:50] epoch 1123, training loss: 13063.47, average training loss: 15702.65, base loss: 25521.63
[INFO 2017-06-26 11:52:39,846 main.py:50] epoch 1124, training loss: 13911.73, average training loss: 15698.80, base loss: 25525.09
[INFO 2017-06-26 11:52:40,577 main.py:50] epoch 1125, training loss: 12932.95, average training loss: 15691.35, base loss: 25521.94
[INFO 2017-06-26 11:52:41,308 main.py:50] epoch 1126, training loss: 13095.66, average training loss: 15685.17, base loss: 25521.26
[INFO 2017-06-26 11:52:42,040 main.py:50] epoch 1127, training loss: 15598.86, average training loss: 15682.64, base loss: 25527.33
[INFO 2017-06-26 11:52:42,772 main.py:50] epoch 1128, training loss: 13731.37, average training loss: 15677.82, base loss: 25529.12
[INFO 2017-06-26 11:52:43,506 main.py:50] epoch 1129, training loss: 13762.32, average training loss: 15674.11, base loss: 25533.67
[INFO 2017-06-26 11:52:44,237 main.py:50] epoch 1130, training loss: 12823.23, average training loss: 15665.86, base loss: 25529.57
[INFO 2017-06-26 11:52:44,968 main.py:50] epoch 1131, training loss: 14615.41, average training loss: 15659.27, base loss: 25527.95
[INFO 2017-06-26 11:52:45,701 main.py:50] epoch 1132, training loss: 13181.48, average training loss: 15650.18, base loss: 25523.09
[INFO 2017-06-26 11:52:46,431 main.py:50] epoch 1133, training loss: 13200.85, average training loss: 15645.40, base loss: 25523.94
[INFO 2017-06-26 11:52:47,164 main.py:50] epoch 1134, training loss: 13978.65, average training loss: 15642.14, base loss: 25528.75
[INFO 2017-06-26 11:52:47,897 main.py:50] epoch 1135, training loss: 14368.99, average training loss: 15635.24, base loss: 25526.01
[INFO 2017-06-26 11:52:48,629 main.py:50] epoch 1136, training loss: 14159.01, average training loss: 15629.85, base loss: 25527.14
[INFO 2017-06-26 11:52:49,360 main.py:50] epoch 1137, training loss: 15053.04, average training loss: 15623.21, base loss: 25526.36
[INFO 2017-06-26 11:52:50,092 main.py:50] epoch 1138, training loss: 12960.50, average training loss: 15615.08, base loss: 25523.63
[INFO 2017-06-26 11:52:50,822 main.py:50] epoch 1139, training loss: 13097.32, average training loss: 15610.42, base loss: 25524.19
[INFO 2017-06-26 11:52:51,553 main.py:50] epoch 1140, training loss: 14655.19, average training loss: 15604.13, base loss: 25522.46
[INFO 2017-06-26 11:52:52,285 main.py:50] epoch 1141, training loss: 14706.72, average training loss: 15600.00, base loss: 25524.67
[INFO 2017-06-26 11:52:53,029 main.py:50] epoch 1142, training loss: 14018.99, average training loss: 15593.47, base loss: 25524.84
[INFO 2017-06-26 11:52:53,760 main.py:50] epoch 1143, training loss: 14638.68, average training loss: 15590.11, base loss: 25527.09
[INFO 2017-06-26 11:52:54,493 main.py:50] epoch 1144, training loss: 13560.13, average training loss: 15585.50, base loss: 25527.71
[INFO 2017-06-26 11:52:55,225 main.py:50] epoch 1145, training loss: 14106.11, average training loss: 15580.77, base loss: 25528.10
[INFO 2017-06-26 11:52:55,956 main.py:50] epoch 1146, training loss: 14754.96, average training loss: 15576.98, base loss: 25529.60
[INFO 2017-06-26 11:52:56,688 main.py:50] epoch 1147, training loss: 15401.57, average training loss: 15571.29, base loss: 25531.07
[INFO 2017-06-26 11:52:57,419 main.py:50] epoch 1148, training loss: 14566.02, average training loss: 15567.53, base loss: 25531.84
[INFO 2017-06-26 11:52:58,153 main.py:50] epoch 1149, training loss: 13853.35, average training loss: 15560.59, base loss: 25530.67
[INFO 2017-06-26 11:52:58,885 main.py:50] epoch 1150, training loss: 12942.47, average training loss: 15555.30, base loss: 25528.89
[INFO 2017-06-26 11:52:59,617 main.py:50] epoch 1151, training loss: 13278.82, average training loss: 15549.48, base loss: 25527.29
[INFO 2017-06-26 11:53:00,349 main.py:50] epoch 1152, training loss: 14379.12, average training loss: 15545.34, base loss: 25530.69
[INFO 2017-06-26 11:53:01,081 main.py:50] epoch 1153, training loss: 11985.15, average training loss: 15539.28, base loss: 25528.50
[INFO 2017-06-26 11:53:01,813 main.py:50] epoch 1154, training loss: 13107.34, average training loss: 15533.99, base loss: 25527.80
[INFO 2017-06-26 11:53:02,545 main.py:50] epoch 1155, training loss: 13110.30, average training loss: 15528.36, base loss: 25525.68
[INFO 2017-06-26 11:53:03,276 main.py:50] epoch 1156, training loss: 12983.44, average training loss: 15520.50, base loss: 25523.33
[INFO 2017-06-26 11:53:04,009 main.py:50] epoch 1157, training loss: 12758.48, average training loss: 15516.16, base loss: 25523.00
[INFO 2017-06-26 11:53:04,741 main.py:50] epoch 1158, training loss: 15136.16, average training loss: 15510.72, base loss: 25523.97
[INFO 2017-06-26 11:53:05,473 main.py:50] epoch 1159, training loss: 12801.55, average training loss: 15505.17, base loss: 25522.67
[INFO 2017-06-26 11:53:06,203 main.py:50] epoch 1160, training loss: 15375.48, average training loss: 15502.84, base loss: 25525.95
[INFO 2017-06-26 11:53:06,933 main.py:50] epoch 1161, training loss: 13071.09, average training loss: 15497.75, base loss: 25525.31
[INFO 2017-06-26 11:53:07,664 main.py:50] epoch 1162, training loss: 13525.42, average training loss: 15493.24, base loss: 25526.37
[INFO 2017-06-26 11:53:08,395 main.py:50] epoch 1163, training loss: 14881.47, average training loss: 15491.23, base loss: 25530.70
[INFO 2017-06-26 11:53:09,127 main.py:50] epoch 1164, training loss: 14931.07, average training loss: 15488.50, base loss: 25534.30
[INFO 2017-06-26 11:53:09,859 main.py:50] epoch 1165, training loss: 12508.68, average training loss: 15480.62, base loss: 25530.24
[INFO 2017-06-26 11:53:10,591 main.py:50] epoch 1166, training loss: 13057.40, average training loss: 15473.16, base loss: 25528.03
[INFO 2017-06-26 11:53:11,323 main.py:50] epoch 1167, training loss: 12923.97, average training loss: 15468.00, base loss: 25526.07
[INFO 2017-06-26 11:53:12,054 main.py:50] epoch 1168, training loss: 13873.44, average training loss: 15461.62, base loss: 25524.76
[INFO 2017-06-26 11:53:12,787 main.py:50] epoch 1169, training loss: 13587.77, average training loss: 15457.22, base loss: 25525.72
[INFO 2017-06-26 11:53:13,518 main.py:50] epoch 1170, training loss: 14721.73, average training loss: 15454.42, base loss: 25527.55
[INFO 2017-06-26 11:53:14,249 main.py:50] epoch 1171, training loss: 14849.25, average training loss: 15451.58, base loss: 25529.13
[INFO 2017-06-26 11:53:14,981 main.py:50] epoch 1172, training loss: 12854.45, average training loss: 15446.12, base loss: 25526.44
[INFO 2017-06-26 11:53:15,714 main.py:50] epoch 1173, training loss: 12425.64, average training loss: 15441.68, base loss: 25525.18
[INFO 2017-06-26 11:53:16,446 main.py:50] epoch 1174, training loss: 13138.02, average training loss: 15436.34, base loss: 25522.42
[INFO 2017-06-26 11:53:17,177 main.py:50] epoch 1175, training loss: 14043.90, average training loss: 15431.68, base loss: 25522.73
[INFO 2017-06-26 11:53:17,910 main.py:50] epoch 1176, training loss: 14149.86, average training loss: 15428.70, base loss: 25526.13
[INFO 2017-06-26 11:53:18,641 main.py:50] epoch 1177, training loss: 14919.54, average training loss: 15425.58, base loss: 25528.13
[INFO 2017-06-26 11:53:19,373 main.py:50] epoch 1178, training loss: 12694.37, average training loss: 15418.07, base loss: 25524.63
[INFO 2017-06-26 11:53:20,104 main.py:50] epoch 1179, training loss: 13167.56, average training loss: 15410.44, base loss: 25521.99
[INFO 2017-06-26 11:53:20,835 main.py:50] epoch 1180, training loss: 12497.26, average training loss: 15403.21, base loss: 25518.20
[INFO 2017-06-26 11:53:21,566 main.py:50] epoch 1181, training loss: 12825.34, average training loss: 15396.25, base loss: 25515.67
[INFO 2017-06-26 11:53:22,298 main.py:50] epoch 1182, training loss: 13956.05, average training loss: 15389.96, base loss: 25514.56
[INFO 2017-06-26 11:53:23,030 main.py:50] epoch 1183, training loss: 13384.82, average training loss: 15383.83, base loss: 25512.11
[INFO 2017-06-26 11:53:23,761 main.py:50] epoch 1184, training loss: 12866.79, average training loss: 15379.39, base loss: 25512.67
[INFO 2017-06-26 11:53:24,493 main.py:50] epoch 1185, training loss: 13425.48, average training loss: 15375.41, base loss: 25513.26
[INFO 2017-06-26 11:53:25,237 main.py:50] epoch 1186, training loss: 12598.53, average training loss: 15371.07, base loss: 25511.33
[INFO 2017-06-26 11:53:25,969 main.py:50] epoch 1187, training loss: 14594.23, average training loss: 15364.56, base loss: 25508.78
[INFO 2017-06-26 11:53:26,701 main.py:50] epoch 1188, training loss: 14682.54, average training loss: 15361.41, base loss: 25510.22
[INFO 2017-06-26 11:53:27,432 main.py:50] epoch 1189, training loss: 13538.00, average training loss: 15353.42, base loss: 25504.40
[INFO 2017-06-26 11:53:28,161 main.py:50] epoch 1190, training loss: 13173.95, average training loss: 15348.73, base loss: 25503.88
[INFO 2017-06-26 11:53:28,893 main.py:50] epoch 1191, training loss: 12622.52, average training loss: 15341.01, base loss: 25499.25
[INFO 2017-06-26 11:53:29,626 main.py:50] epoch 1192, training loss: 12954.06, average training loss: 15337.44, base loss: 25499.66
[INFO 2017-06-26 11:53:30,357 main.py:50] epoch 1193, training loss: 13771.22, average training loss: 15335.30, base loss: 25504.42
[INFO 2017-06-26 11:53:31,089 main.py:50] epoch 1194, training loss: 14985.08, average training loss: 15332.62, base loss: 25505.81
[INFO 2017-06-26 11:53:31,820 main.py:50] epoch 1195, training loss: 12945.74, average training loss: 15325.24, base loss: 25502.83
[INFO 2017-06-26 11:53:32,551 main.py:50] epoch 1196, training loss: 13534.64, average training loss: 15320.54, base loss: 25502.71
[INFO 2017-06-26 11:53:33,284 main.py:50] epoch 1197, training loss: 13347.00, average training loss: 15316.40, base loss: 25501.93
[INFO 2017-06-26 11:53:34,017 main.py:50] epoch 1198, training loss: 12784.97, average training loss: 15312.25, base loss: 25502.67
[INFO 2017-06-26 11:53:34,750 main.py:50] epoch 1199, training loss: 13793.88, average training loss: 15308.32, base loss: 25502.69
[INFO 2017-06-26 11:53:35,485 main.py:50] epoch 1200, training loss: 13409.28, average training loss: 15304.41, base loss: 25502.41
[INFO 2017-06-26 11:53:36,218 main.py:50] epoch 1201, training loss: 13271.34, average training loss: 15301.70, base loss: 25505.17
[INFO 2017-06-26 11:53:36,950 main.py:50] epoch 1202, training loss: 14972.96, average training loss: 15296.86, base loss: 25506.23
[INFO 2017-06-26 11:53:37,682 main.py:50] epoch 1203, training loss: 13515.77, average training loss: 15294.83, base loss: 25510.45
[INFO 2017-06-26 11:53:38,413 main.py:50] epoch 1204, training loss: 13587.69, average training loss: 15291.81, base loss: 25513.99
[INFO 2017-06-26 11:53:39,147 main.py:50] epoch 1205, training loss: 14048.94, average training loss: 15288.35, base loss: 25514.82
[INFO 2017-06-26 11:53:39,880 main.py:50] epoch 1206, training loss: 14142.08, average training loss: 15282.86, base loss: 25513.01
[INFO 2017-06-26 11:53:40,613 main.py:50] epoch 1207, training loss: 14708.07, average training loss: 15280.73, base loss: 25515.80
[INFO 2017-06-26 11:53:41,347 main.py:50] epoch 1208, training loss: 15408.80, average training loss: 15275.65, base loss: 25516.61
[INFO 2017-06-26 11:53:42,082 main.py:50] epoch 1209, training loss: 15001.93, average training loss: 15271.75, base loss: 25516.07
[INFO 2017-06-26 11:53:42,815 main.py:50] epoch 1210, training loss: 14648.83, average training loss: 15269.41, base loss: 25519.20
[INFO 2017-06-26 11:53:43,547 main.py:50] epoch 1211, training loss: 13717.99, average training loss: 15263.38, base loss: 25518.55
[INFO 2017-06-26 11:53:44,280 main.py:50] epoch 1212, training loss: 13129.79, average training loss: 15259.26, base loss: 25517.29
[INFO 2017-06-26 11:53:45,013 main.py:50] epoch 1213, training loss: 12854.83, average training loss: 15254.59, base loss: 25515.12
[INFO 2017-06-26 11:53:45,747 main.py:50] epoch 1214, training loss: 14806.59, average training loss: 15249.59, base loss: 25515.15
[INFO 2017-06-26 11:53:46,480 main.py:50] epoch 1215, training loss: 15599.57, average training loss: 15245.78, base loss: 25518.34
[INFO 2017-06-26 11:53:47,213 main.py:50] epoch 1216, training loss: 12957.65, average training loss: 15242.83, base loss: 25520.00
[INFO 2017-06-26 11:53:47,946 main.py:50] epoch 1217, training loss: 13484.65, average training loss: 15235.52, base loss: 25516.63
[INFO 2017-06-26 11:53:48,679 main.py:50] epoch 1218, training loss: 13414.24, average training loss: 15228.66, base loss: 25514.53
[INFO 2017-06-26 11:53:49,411 main.py:50] epoch 1219, training loss: 13123.28, average training loss: 15226.17, base loss: 25518.57
[INFO 2017-06-26 11:53:50,142 main.py:50] epoch 1220, training loss: 15278.08, average training loss: 15221.73, base loss: 25520.10
[INFO 2017-06-26 11:53:50,875 main.py:50] epoch 1221, training loss: 13116.14, average training loss: 15214.95, base loss: 25516.95
[INFO 2017-06-26 11:53:51,609 main.py:50] epoch 1222, training loss: 14427.39, average training loss: 15209.47, base loss: 25516.86
[INFO 2017-06-26 11:53:52,342 main.py:50] epoch 1223, training loss: 14736.52, average training loss: 15206.85, base loss: 25519.13
[INFO 2017-06-26 11:53:53,076 main.py:50] epoch 1224, training loss: 15084.04, average training loss: 15205.04, base loss: 25523.84
[INFO 2017-06-26 11:53:53,808 main.py:50] epoch 1225, training loss: 14018.84, average training loss: 15201.72, base loss: 25524.68
[INFO 2017-06-26 11:53:54,541 main.py:50] epoch 1226, training loss: 12465.82, average training loss: 15195.50, base loss: 25521.97
[INFO 2017-06-26 11:53:55,273 main.py:50] epoch 1227, training loss: 15022.70, average training loss: 15193.95, base loss: 25525.60
[INFO 2017-06-26 11:53:56,006 main.py:50] epoch 1228, training loss: 14403.40, average training loss: 15191.44, base loss: 25526.22
[INFO 2017-06-26 11:53:56,739 main.py:50] epoch 1229, training loss: 12878.14, average training loss: 15184.29, base loss: 25521.89
[INFO 2017-06-26 11:53:57,485 main.py:50] epoch 1230, training loss: 12893.93, average training loss: 15177.01, base loss: 25516.91
[INFO 2017-06-26 11:53:58,216 main.py:50] epoch 1231, training loss: 12773.63, average training loss: 15170.23, base loss: 25513.45
[INFO 2017-06-26 11:53:58,949 main.py:50] epoch 1232, training loss: 13212.19, average training loss: 15163.70, base loss: 25511.49
[INFO 2017-06-26 11:53:59,680 main.py:50] epoch 1233, training loss: 14695.94, average training loss: 15161.31, base loss: 25515.01
[INFO 2017-06-26 11:54:00,411 main.py:50] epoch 1234, training loss: 14079.05, average training loss: 15159.73, base loss: 25520.71
[INFO 2017-06-26 11:54:01,142 main.py:50] epoch 1235, training loss: 12669.87, average training loss: 15155.87, base loss: 25519.08
[INFO 2017-06-26 11:54:01,873 main.py:50] epoch 1236, training loss: 13374.74, average training loss: 15152.29, base loss: 25519.00
[INFO 2017-06-26 11:54:02,604 main.py:50] epoch 1237, training loss: 14844.74, average training loss: 15146.87, base loss: 25517.73
[INFO 2017-06-26 11:54:03,335 main.py:50] epoch 1238, training loss: 12383.00, average training loss: 15143.05, base loss: 25515.56
[INFO 2017-06-26 11:54:04,068 main.py:50] epoch 1239, training loss: 13073.97, average training loss: 15139.17, base loss: 25516.16
[INFO 2017-06-26 11:54:04,799 main.py:50] epoch 1240, training loss: 13247.38, average training loss: 15135.37, base loss: 25515.23
[INFO 2017-06-26 11:54:05,531 main.py:50] epoch 1241, training loss: 13231.81, average training loss: 15128.79, base loss: 25512.27
[INFO 2017-06-26 11:54:06,263 main.py:50] epoch 1242, training loss: 14610.41, average training loss: 15122.91, base loss: 25510.46
[INFO 2017-06-26 11:54:06,994 main.py:50] epoch 1243, training loss: 12406.82, average training loss: 15118.86, base loss: 25509.13
[INFO 2017-06-26 11:54:07,724 main.py:50] epoch 1244, training loss: 13147.47, average training loss: 15110.95, base loss: 25502.13
[INFO 2017-06-26 11:54:08,456 main.py:50] epoch 1245, training loss: 13601.48, average training loss: 15105.20, base loss: 25500.50
[INFO 2017-06-26 11:54:09,187 main.py:50] epoch 1246, training loss: 13178.63, average training loss: 15098.75, base loss: 25497.26
[INFO 2017-06-26 11:54:09,918 main.py:50] epoch 1247, training loss: 15167.37, average training loss: 15097.05, base loss: 25499.54
[INFO 2017-06-26 11:54:10,649 main.py:50] epoch 1248, training loss: 14318.74, average training loss: 15092.07, base loss: 25499.05
[INFO 2017-06-26 11:54:11,381 main.py:50] epoch 1249, training loss: 13477.07, average training loss: 15085.70, base loss: 25496.20
[INFO 2017-06-26 11:54:12,112 main.py:50] epoch 1250, training loss: 12772.06, average training loss: 15079.19, base loss: 25491.42
[INFO 2017-06-26 11:54:12,842 main.py:50] epoch 1251, training loss: 13610.20, average training loss: 15076.48, base loss: 25492.81
[INFO 2017-06-26 11:54:13,576 main.py:50] epoch 1252, training loss: 14550.23, average training loss: 15075.07, base loss: 25496.87
[INFO 2017-06-26 11:54:14,309 main.py:50] epoch 1253, training loss: 14684.66, average training loss: 15074.28, base loss: 25502.96
[INFO 2017-06-26 11:54:15,042 main.py:50] epoch 1254, training loss: 13292.00, average training loss: 15070.79, base loss: 25502.27
[INFO 2017-06-26 11:54:15,773 main.py:50] epoch 1255, training loss: 12883.94, average training loss: 15066.90, base loss: 25500.92
[INFO 2017-06-26 11:54:16,504 main.py:50] epoch 1256, training loss: 13568.60, average training loss: 15063.44, base loss: 25500.25
[INFO 2017-06-26 11:54:17,236 main.py:50] epoch 1257, training loss: 14937.71, average training loss: 15062.59, base loss: 25504.32
[INFO 2017-06-26 11:54:17,967 main.py:50] epoch 1258, training loss: 14132.47, average training loss: 15057.24, base loss: 25503.30
[INFO 2017-06-26 11:54:18,697 main.py:50] epoch 1259, training loss: 13413.68, average training loss: 15055.11, base loss: 25505.86
[INFO 2017-06-26 11:54:19,428 main.py:50] epoch 1260, training loss: 14429.60, average training loss: 15049.32, base loss: 25503.01
[INFO 2017-06-26 11:54:20,159 main.py:50] epoch 1261, training loss: 14224.44, average training loss: 15045.48, base loss: 25503.36
[INFO 2017-06-26 11:54:20,891 main.py:50] epoch 1262, training loss: 12721.87, average training loss: 15039.30, base loss: 25500.47
[INFO 2017-06-26 11:54:21,622 main.py:50] epoch 1263, training loss: 12875.67, average training loss: 15034.54, base loss: 25496.94
[INFO 2017-06-26 11:54:22,353 main.py:50] epoch 1264, training loss: 12822.19, average training loss: 15027.60, base loss: 25490.69
[INFO 2017-06-26 11:54:23,084 main.py:50] epoch 1265, training loss: 12323.50, average training loss: 15019.84, base loss: 25484.13
[INFO 2017-06-26 11:54:23,816 main.py:50] epoch 1266, training loss: 11857.65, average training loss: 15012.57, base loss: 25479.92
[INFO 2017-06-26 11:54:24,547 main.py:50] epoch 1267, training loss: 13287.98, average training loss: 15008.09, base loss: 25476.64
[INFO 2017-06-26 11:54:25,278 main.py:50] epoch 1268, training loss: 14684.13, average training loss: 15003.80, base loss: 25476.19
[INFO 2017-06-26 11:54:26,011 main.py:50] epoch 1269, training loss: 13160.44, average training loss: 15000.96, base loss: 25477.49
[INFO 2017-06-26 11:54:26,742 main.py:50] epoch 1270, training loss: 14622.60, average training loss: 15000.79, base loss: 25483.69
[INFO 2017-06-26 11:54:27,475 main.py:50] epoch 1271, training loss: 12574.02, average training loss: 14994.21, base loss: 25479.51
[INFO 2017-06-26 11:54:28,209 main.py:50] epoch 1272, training loss: 13201.36, average training loss: 14990.99, base loss: 25478.37
[INFO 2017-06-26 11:54:28,942 main.py:50] epoch 1273, training loss: 15284.38, average training loss: 14990.66, base loss: 25483.79
[INFO 2017-06-26 11:54:29,688 main.py:50] epoch 1274, training loss: 15182.90, average training loss: 14990.33, base loss: 25488.43
[INFO 2017-06-26 11:54:30,420 main.py:50] epoch 1275, training loss: 12978.22, average training loss: 14983.91, base loss: 25484.07
[INFO 2017-06-26 11:54:31,152 main.py:50] epoch 1276, training loss: 13259.61, average training loss: 14978.09, base loss: 25482.21
[INFO 2017-06-26 11:54:31,889 main.py:50] epoch 1277, training loss: 12669.83, average training loss: 14975.97, base loss: 25486.33
[INFO 2017-06-26 11:54:32,620 main.py:50] epoch 1278, training loss: 14828.26, average training loss: 14974.14, base loss: 25488.07
[INFO 2017-06-26 11:54:33,351 main.py:50] epoch 1279, training loss: 13270.16, average training loss: 14968.75, base loss: 25486.45
[INFO 2017-06-26 11:54:34,083 main.py:50] epoch 1280, training loss: 12669.27, average training loss: 14964.82, base loss: 25483.29
[INFO 2017-06-26 11:54:34,815 main.py:50] epoch 1281, training loss: 14547.97, average training loss: 14963.32, base loss: 25485.75
[INFO 2017-06-26 11:54:35,547 main.py:50] epoch 1282, training loss: 12257.84, average training loss: 14959.12, base loss: 25482.81
[INFO 2017-06-26 11:54:36,279 main.py:50] epoch 1283, training loss: 14142.18, average training loss: 14954.37, base loss: 25482.54
[INFO 2017-06-26 11:54:37,011 main.py:50] epoch 1284, training loss: 14387.28, average training loss: 14951.89, base loss: 25483.37
[INFO 2017-06-26 11:54:37,741 main.py:50] epoch 1285, training loss: 13043.46, average training loss: 14948.39, base loss: 25482.74
[INFO 2017-06-26 11:54:38,474 main.py:50] epoch 1286, training loss: 14375.66, average training loss: 14946.41, base loss: 25485.39
[INFO 2017-06-26 11:54:39,208 main.py:50] epoch 1287, training loss: 14463.10, average training loss: 14942.57, base loss: 25485.51
[INFO 2017-06-26 11:54:39,938 main.py:50] epoch 1288, training loss: 14036.12, average training loss: 14941.01, base loss: 25490.66
[INFO 2017-06-26 11:54:40,681 main.py:50] epoch 1289, training loss: 12547.15, average training loss: 14937.35, base loss: 25488.96
[INFO 2017-06-26 11:54:41,447 main.py:50] epoch 1290, training loss: 13254.43, average training loss: 14931.23, base loss: 25485.48
[INFO 2017-06-26 11:54:42,214 main.py:50] epoch 1291, training loss: 12918.11, average training loss: 14928.00, base loss: 25485.00
[INFO 2017-06-26 11:54:42,947 main.py:50] epoch 1292, training loss: 12425.93, average training loss: 14925.48, base loss: 25485.22
[INFO 2017-06-26 11:54:43,735 main.py:50] epoch 1293, training loss: 13733.88, average training loss: 14921.71, base loss: 25483.22
[INFO 2017-06-26 11:54:44,504 main.py:50] epoch 1294, training loss: 14116.51, average training loss: 14917.10, base loss: 25482.14
[INFO 2017-06-26 11:54:45,284 main.py:50] epoch 1295, training loss: 13038.71, average training loss: 14910.35, base loss: 25479.54
[INFO 2017-06-26 11:54:46,028 main.py:50] epoch 1296, training loss: 12802.15, average training loss: 14906.92, base loss: 25477.20
[INFO 2017-06-26 11:54:46,794 main.py:50] epoch 1297, training loss: 12332.67, average training loss: 14903.63, base loss: 25475.27
[INFO 2017-06-26 11:54:47,552 main.py:50] epoch 1298, training loss: 13813.35, average training loss: 14897.56, base loss: 25470.63
[INFO 2017-06-26 11:54:48,320 main.py:50] epoch 1299, training loss: 13961.07, average training loss: 14895.88, base loss: 25473.33
[INFO 2017-06-26 11:54:49,078 main.py:50] epoch 1300, training loss: 14200.25, average training loss: 14893.78, base loss: 25473.94
[INFO 2017-06-26 11:54:49,812 main.py:50] epoch 1301, training loss: 15439.50, average training loss: 14893.92, base loss: 25480.54
[INFO 2017-06-26 11:54:50,556 main.py:50] epoch 1302, training loss: 13565.08, average training loss: 14889.58, base loss: 25481.48
[INFO 2017-06-26 11:54:51,291 main.py:50] epoch 1303, training loss: 12731.48, average training loss: 14883.81, base loss: 25476.66
[INFO 2017-06-26 11:54:52,024 main.py:50] epoch 1304, training loss: 12388.16, average training loss: 14877.31, base loss: 25471.70
[INFO 2017-06-26 11:54:52,766 main.py:50] epoch 1305, training loss: 15086.96, average training loss: 14877.04, base loss: 25475.60
[INFO 2017-06-26 11:54:53,528 main.py:50] epoch 1306, training loss: 13235.46, average training loss: 14875.02, base loss: 25476.36
[INFO 2017-06-26 11:54:54,262 main.py:50] epoch 1307, training loss: 12704.26, average training loss: 14871.98, base loss: 25475.33
[INFO 2017-06-26 11:54:55,029 main.py:50] epoch 1308, training loss: 13843.89, average training loss: 14869.21, base loss: 25475.13
[INFO 2017-06-26 11:54:55,784 main.py:50] epoch 1309, training loss: 13420.84, average training loss: 14866.61, base loss: 25475.13
[INFO 2017-06-26 11:54:56,517 main.py:50] epoch 1310, training loss: 15290.02, average training loss: 14864.00, base loss: 25477.73
[INFO 2017-06-26 11:54:57,269 main.py:50] epoch 1311, training loss: 14644.78, average training loss: 14860.17, base loss: 25477.36
[INFO 2017-06-26 11:54:58,036 main.py:50] epoch 1312, training loss: 14907.28, average training loss: 14859.53, base loss: 25480.55
[INFO 2017-06-26 11:54:58,770 main.py:50] epoch 1313, training loss: 14440.08, average training loss: 14854.60, base loss: 25478.63
[INFO 2017-06-26 11:54:59,537 main.py:50] epoch 1314, training loss: 12943.55, average training loss: 14851.14, base loss: 25476.64
[INFO 2017-06-26 11:55:00,272 main.py:50] epoch 1315, training loss: 14521.62, average training loss: 14849.17, base loss: 25477.32
[INFO 2017-06-26 11:55:01,008 main.py:50] epoch 1316, training loss: 12948.03, average training loss: 14843.08, base loss: 25473.03
[INFO 2017-06-26 11:55:01,743 main.py:50] epoch 1317, training loss: 13124.46, average training loss: 14840.59, base loss: 25472.97
[INFO 2017-06-26 11:55:02,488 main.py:50] epoch 1318, training loss: 12972.45, average training loss: 14834.87, base loss: 25468.87
[INFO 2017-06-26 11:55:03,220 main.py:50] epoch 1319, training loss: 15250.20, average training loss: 14834.85, base loss: 25474.81
[INFO 2017-06-26 11:55:03,953 main.py:50] epoch 1320, training loss: 12854.56, average training loss: 14832.53, base loss: 25474.97
[INFO 2017-06-26 11:55:04,686 main.py:50] epoch 1321, training loss: 13672.88, average training loss: 14827.21, base loss: 25473.02
[INFO 2017-06-26 11:55:05,419 main.py:50] epoch 1322, training loss: 15126.42, average training loss: 14823.10, base loss: 25472.23
[INFO 2017-06-26 11:55:06,152 main.py:50] epoch 1323, training loss: 12963.66, average training loss: 14819.86, base loss: 25470.62
[INFO 2017-06-26 11:55:06,886 main.py:50] epoch 1324, training loss: 12872.50, average training loss: 14817.60, base loss: 25470.95
[INFO 2017-06-26 11:55:07,619 main.py:50] epoch 1325, training loss: 13317.96, average training loss: 14811.77, base loss: 25466.48
[INFO 2017-06-26 11:55:08,355 main.py:50] epoch 1326, training loss: 13005.49, average training loss: 14809.56, base loss: 25467.71
[INFO 2017-06-26 11:55:09,088 main.py:50] epoch 1327, training loss: 14950.06, average training loss: 14808.29, base loss: 25470.21
[INFO 2017-06-26 11:55:09,822 main.py:50] epoch 1328, training loss: 13257.27, average training loss: 14803.43, base loss: 25470.40
[INFO 2017-06-26 11:55:10,556 main.py:50] epoch 1329, training loss: 13042.28, average training loss: 14798.93, base loss: 25470.91
[INFO 2017-06-26 11:55:11,291 main.py:50] epoch 1330, training loss: 12822.16, average training loss: 14796.96, base loss: 25473.00
[INFO 2017-06-26 11:55:12,025 main.py:50] epoch 1331, training loss: 13087.50, average training loss: 14794.30, base loss: 25473.55
[INFO 2017-06-26 11:55:12,759 main.py:50] epoch 1332, training loss: 12397.85, average training loss: 14788.16, base loss: 25468.43
[INFO 2017-06-26 11:55:13,495 main.py:50] epoch 1333, training loss: 15258.28, average training loss: 14785.73, base loss: 25472.63
[INFO 2017-06-26 11:55:14,231 main.py:50] epoch 1334, training loss: 13162.59, average training loss: 14782.66, base loss: 25471.79
[INFO 2017-06-26 11:55:14,994 main.py:50] epoch 1335, training loss: 14537.29, average training loss: 14782.91, base loss: 25477.19
[INFO 2017-06-26 11:55:15,726 main.py:50] epoch 1336, training loss: 11643.22, average training loss: 14779.17, base loss: 25472.84
[INFO 2017-06-26 11:55:16,459 main.py:50] epoch 1337, training loss: 12953.37, average training loss: 14777.50, base loss: 25474.80
[INFO 2017-06-26 11:55:17,193 main.py:50] epoch 1338, training loss: 12735.25, average training loss: 14774.63, base loss: 25473.71
[INFO 2017-06-26 11:55:17,926 main.py:50] epoch 1339, training loss: 14721.20, average training loss: 14773.62, base loss: 25476.60
[INFO 2017-06-26 11:55:18,658 main.py:50] epoch 1340, training loss: 13303.98, average training loss: 14770.37, base loss: 25475.45
[INFO 2017-06-26 11:55:19,392 main.py:50] epoch 1341, training loss: 13559.16, average training loss: 14768.62, base loss: 25477.55
[INFO 2017-06-26 11:55:20,126 main.py:50] epoch 1342, training loss: 12731.02, average training loss: 14765.16, base loss: 25476.54
[INFO 2017-06-26 11:55:20,861 main.py:50] epoch 1343, training loss: 12731.02, average training loss: 14762.54, base loss: 25477.23
[INFO 2017-06-26 11:55:21,596 main.py:50] epoch 1344, training loss: 12865.27, average training loss: 14759.96, base loss: 25477.86
[INFO 2017-06-26 11:55:22,329 main.py:50] epoch 1345, training loss: 13287.85, average training loss: 14756.47, base loss: 25476.08
[INFO 2017-06-26 11:55:23,064 main.py:50] epoch 1346, training loss: 12798.47, average training loss: 14750.46, base loss: 25471.16
[INFO 2017-06-26 11:55:23,797 main.py:50] epoch 1347, training loss: 13613.93, average training loss: 14745.96, base loss: 25470.74
[INFO 2017-06-26 11:55:24,532 main.py:50] epoch 1348, training loss: 12856.27, average training loss: 14740.24, base loss: 25467.22
[INFO 2017-06-26 11:55:25,265 main.py:50] epoch 1349, training loss: 12853.90, average training loss: 14734.16, base loss: 25462.13
[INFO 2017-06-26 11:55:25,999 main.py:50] epoch 1350, training loss: 13490.80, average training loss: 14729.15, base loss: 25458.72
[INFO 2017-06-26 11:55:26,733 main.py:50] epoch 1351, training loss: 12849.37, average training loss: 14722.88, base loss: 25453.92
[INFO 2017-06-26 11:55:27,467 main.py:50] epoch 1352, training loss: 14724.73, average training loss: 14722.20, base loss: 25456.99
[INFO 2017-06-26 11:55:28,200 main.py:50] epoch 1353, training loss: 14735.13, average training loss: 14717.69, base loss: 25454.99
[INFO 2017-06-26 11:55:28,934 main.py:50] epoch 1354, training loss: 12510.15, average training loss: 14711.91, base loss: 25451.50
[INFO 2017-06-26 11:55:29,668 main.py:50] epoch 1355, training loss: 12701.60, average training loss: 14709.62, base loss: 25452.14
[INFO 2017-06-26 11:55:30,401 main.py:50] epoch 1356, training loss: 13590.00, average training loss: 14706.74, base loss: 25451.44
[INFO 2017-06-26 11:55:31,134 main.py:50] epoch 1357, training loss: 12668.76, average training loss: 14703.99, base loss: 25450.52
[INFO 2017-06-26 11:55:31,868 main.py:50] epoch 1358, training loss: 13887.16, average training loss: 14699.22, base loss: 25448.89
[INFO 2017-06-26 11:55:32,600 main.py:50] epoch 1359, training loss: 13432.79, average training loss: 14693.94, base loss: 25445.48
[INFO 2017-06-26 11:55:33,334 main.py:50] epoch 1360, training loss: 14906.81, average training loss: 14690.49, base loss: 25445.52
[INFO 2017-06-26 11:55:34,069 main.py:50] epoch 1361, training loss: 13439.92, average training loss: 14688.80, base loss: 25446.95
[INFO 2017-06-26 11:55:34,817 main.py:50] epoch 1362, training loss: 13306.60, average training loss: 14686.54, base loss: 25446.41
[INFO 2017-06-26 11:55:35,551 main.py:50] epoch 1363, training loss: 13601.55, average training loss: 14684.71, base loss: 25446.77
[INFO 2017-06-26 11:55:36,284 main.py:50] epoch 1364, training loss: 13354.69, average training loss: 14681.87, base loss: 25446.36
[INFO 2017-06-26 11:55:37,017 main.py:50] epoch 1365, training loss: 14731.09, average training loss: 14681.45, base loss: 25450.38
[INFO 2017-06-26 11:55:37,750 main.py:50] epoch 1366, training loss: 12611.78, average training loss: 14677.86, base loss: 25446.50
[INFO 2017-06-26 11:55:38,485 main.py:50] epoch 1367, training loss: 13928.71, average training loss: 14673.29, base loss: 25446.18
[INFO 2017-06-26 11:55:39,220 main.py:50] epoch 1368, training loss: 12858.42, average training loss: 14667.43, base loss: 25442.87
[INFO 2017-06-26 11:55:39,952 main.py:50] epoch 1369, training loss: 13340.80, average training loss: 14664.78, base loss: 25442.27
[INFO 2017-06-26 11:55:40,686 main.py:50] epoch 1370, training loss: 13307.76, average training loss: 14660.02, base loss: 25442.15
[INFO 2017-06-26 11:55:41,420 main.py:50] epoch 1371, training loss: 12981.09, average training loss: 14654.13, base loss: 25437.42
[INFO 2017-06-26 11:55:42,152 main.py:50] epoch 1372, training loss: 12766.59, average training loss: 14648.66, base loss: 25433.13
[INFO 2017-06-26 11:55:42,886 main.py:50] epoch 1373, training loss: 12815.10, average training loss: 14643.08, base loss: 25430.22
[INFO 2017-06-26 11:55:43,620 main.py:50] epoch 1374, training loss: 12349.28, average training loss: 14637.25, base loss: 25426.15
[INFO 2017-06-26 11:55:44,353 main.py:50] epoch 1375, training loss: 13378.10, average training loss: 14635.44, base loss: 25427.25
[INFO 2017-06-26 11:55:45,088 main.py:50] epoch 1376, training loss: 12389.63, average training loss: 14628.86, base loss: 25421.29
[INFO 2017-06-26 11:55:45,821 main.py:50] epoch 1377, training loss: 12825.40, average training loss: 14626.28, base loss: 25420.42
[INFO 2017-06-26 11:55:46,555 main.py:50] epoch 1378, training loss: 13120.07, average training loss: 14624.57, base loss: 25422.05
[INFO 2017-06-26 11:55:47,288 main.py:50] epoch 1379, training loss: 13263.29, average training loss: 14623.26, base loss: 25424.43
[INFO 2017-06-26 11:55:48,024 main.py:50] epoch 1380, training loss: 12781.19, average training loss: 14617.67, base loss: 25421.15
[INFO 2017-06-26 11:55:48,757 main.py:50] epoch 1381, training loss: 12555.23, average training loss: 14611.84, base loss: 25415.43
[INFO 2017-06-26 11:55:49,491 main.py:50] epoch 1382, training loss: 13193.22, average training loss: 14610.39, base loss: 25418.78
[INFO 2017-06-26 11:55:50,225 main.py:50] epoch 1383, training loss: 14373.85, average training loss: 14606.14, base loss: 25417.33
[INFO 2017-06-26 11:55:50,959 main.py:50] epoch 1384, training loss: 13225.02, average training loss: 14602.93, base loss: 25416.08
[INFO 2017-06-26 11:55:51,694 main.py:50] epoch 1385, training loss: 13237.13, average training loss: 14600.30, base loss: 25415.54
[INFO 2017-06-26 11:55:52,430 main.py:50] epoch 1386, training loss: 12044.02, average training loss: 14594.70, base loss: 25411.61
[INFO 2017-06-26 11:55:53,163 main.py:50] epoch 1387, training loss: 12295.00, average training loss: 14591.58, base loss: 25408.89
[INFO 2017-06-26 11:55:53,898 main.py:50] epoch 1388, training loss: 14344.92, average training loss: 14590.79, base loss: 25412.29
[INFO 2017-06-26 11:55:54,633 main.py:50] epoch 1389, training loss: 14274.04, average training loss: 14590.60, base loss: 25416.42
[INFO 2017-06-26 11:55:55,366 main.py:50] epoch 1390, training loss: 12913.29, average training loss: 14588.44, base loss: 25416.77
[INFO 2017-06-26 11:55:56,100 main.py:50] epoch 1391, training loss: 14626.24, average training loss: 14587.80, base loss: 25421.58
[INFO 2017-06-26 11:55:56,834 main.py:50] epoch 1392, training loss: 15394.62, average training loss: 14587.12, base loss: 25424.13
[INFO 2017-06-26 11:55:57,565 main.py:50] epoch 1393, training loss: 12525.72, average training loss: 14581.48, base loss: 25420.35
[INFO 2017-06-26 11:55:58,297 main.py:50] epoch 1394, training loss: 13503.88, average training loss: 14580.15, base loss: 25422.60
[INFO 2017-06-26 11:55:59,030 main.py:50] epoch 1395, training loss: 12575.13, average training loss: 14576.84, base loss: 25420.08
[INFO 2017-06-26 11:55:59,763 main.py:50] epoch 1396, training loss: 12403.64, average training loss: 14571.04, base loss: 25415.59
[INFO 2017-06-26 11:56:00,534 main.py:50] epoch 1397, training loss: 13118.08, average training loss: 14565.90, base loss: 25413.82
[INFO 2017-06-26 11:56:01,268 main.py:50] epoch 1398, training loss: 12414.74, average training loss: 14563.50, base loss: 25414.09
[INFO 2017-06-26 11:56:02,090 main.py:50] epoch 1399, training loss: 12617.78, average training loss: 14561.52, base loss: 25415.84
[INFO 2017-06-26 11:56:02,873 main.py:50] epoch 1400, training loss: 14233.93, average training loss: 14561.28, base loss: 25418.84
[INFO 2017-06-26 11:56:03,633 main.py:50] epoch 1401, training loss: 12279.64, average training loss: 14556.12, base loss: 25416.33
[INFO 2017-06-26 11:56:04,458 main.py:50] epoch 1402, training loss: 13858.97, average training loss: 14555.32, base loss: 25419.30
[INFO 2017-06-26 11:56:05,317 main.py:50] epoch 1403, training loss: 12655.10, average training loss: 14553.19, base loss: 25419.63
[INFO 2017-06-26 11:56:06,152 main.py:50] epoch 1404, training loss: 12955.76, average training loss: 14548.16, base loss: 25417.82
[INFO 2017-06-26 11:56:06,957 main.py:50] epoch 1405, training loss: 12533.96, average training loss: 14542.36, base loss: 25413.85
[INFO 2017-06-26 11:56:07,738 main.py:50] epoch 1406, training loss: 12722.11, average training loss: 14540.04, base loss: 25413.40
[INFO 2017-06-26 11:56:08,469 main.py:50] epoch 1407, training loss: 14407.78, average training loss: 14536.67, base loss: 25413.64
[INFO 2017-06-26 11:56:09,200 main.py:50] epoch 1408, training loss: 14022.92, average training loss: 14531.95, base loss: 25411.32
[INFO 2017-06-26 11:56:09,933 main.py:50] epoch 1409, training loss: 14705.52, average training loss: 14529.28, base loss: 25412.90
[INFO 2017-06-26 11:56:10,667 main.py:50] epoch 1410, training loss: 12770.99, average training loss: 14526.29, base loss: 25412.00
[INFO 2017-06-26 11:56:11,399 main.py:50] epoch 1411, training loss: 14834.64, average training loss: 14524.52, base loss: 25412.19
[INFO 2017-06-26 11:56:12,133 main.py:50] epoch 1412, training loss: 13210.06, average training loss: 14522.84, base loss: 25414.08
[INFO 2017-06-26 11:56:12,866 main.py:50] epoch 1413, training loss: 13011.42, average training loss: 14521.17, base loss: 25415.90
[INFO 2017-06-26 11:56:13,601 main.py:50] epoch 1414, training loss: 12208.22, average training loss: 14517.58, base loss: 25414.57
[INFO 2017-06-26 11:56:14,334 main.py:50] epoch 1415, training loss: 12923.06, average training loss: 14515.17, base loss: 25414.84
[INFO 2017-06-26 11:56:15,070 main.py:50] epoch 1416, training loss: 12851.26, average training loss: 14512.28, base loss: 25413.32
[INFO 2017-06-26 11:56:15,805 main.py:50] epoch 1417, training loss: 13959.13, average training loss: 14510.70, base loss: 25415.14
[INFO 2017-06-26 11:56:16,538 main.py:50] epoch 1418, training loss: 14589.02, average training loss: 14507.99, base loss: 25417.12
[INFO 2017-06-26 11:56:17,270 main.py:50] epoch 1419, training loss: 13346.84, average training loss: 14503.54, base loss: 25417.31
[INFO 2017-06-26 11:56:18,003 main.py:50] epoch 1420, training loss: 15231.54, average training loss: 14503.86, base loss: 25424.15
[INFO 2017-06-26 11:56:18,740 main.py:50] epoch 1421, training loss: 12354.29, average training loss: 14501.11, base loss: 25422.48
[INFO 2017-06-26 11:56:19,494 main.py:50] epoch 1422, training loss: 12392.84, average training loss: 14497.93, base loss: 25420.16
[INFO 2017-06-26 11:56:20,228 main.py:50] epoch 1423, training loss: 15197.14, average training loss: 14495.95, base loss: 25422.55
[INFO 2017-06-26 11:56:20,962 main.py:50] epoch 1424, training loss: 13115.92, average training loss: 14493.80, base loss: 25423.24
[INFO 2017-06-26 11:56:21,698 main.py:50] epoch 1425, training loss: 15035.91, average training loss: 14494.34, base loss: 25429.25
[INFO 2017-06-26 11:56:22,432 main.py:50] epoch 1426, training loss: 15072.94, average training loss: 14491.23, base loss: 25431.50
[INFO 2017-06-26 11:56:23,166 main.py:50] epoch 1427, training loss: 12541.48, average training loss: 14488.27, base loss: 25429.36
[INFO 2017-06-26 11:56:23,899 main.py:50] epoch 1428, training loss: 12928.16, average training loss: 14485.67, base loss: 25428.30
[INFO 2017-06-26 11:56:24,633 main.py:50] epoch 1429, training loss: 14148.09, average training loss: 14483.35, base loss: 25428.03
[INFO 2017-06-26 11:56:25,368 main.py:50] epoch 1430, training loss: 12401.22, average training loss: 14477.16, base loss: 25421.67
[INFO 2017-06-26 11:56:26,101 main.py:50] epoch 1431, training loss: 12749.39, average training loss: 14472.37, base loss: 25419.45
[INFO 2017-06-26 11:56:26,841 main.py:50] epoch 1432, training loss: 12817.68, average training loss: 14467.71, base loss: 25416.95
[INFO 2017-06-26 11:56:27,596 main.py:50] epoch 1433, training loss: 12434.46, average training loss: 14464.86, base loss: 25414.73
[INFO 2017-06-26 11:56:28,331 main.py:50] epoch 1434, training loss: 13181.43, average training loss: 14464.01, base loss: 25419.26
[INFO 2017-06-26 11:56:29,064 main.py:50] epoch 1435, training loss: 12717.97, average training loss: 14461.04, base loss: 25418.59
[INFO 2017-06-26 11:56:29,797 main.py:50] epoch 1436, training loss: 12757.78, average training loss: 14458.62, base loss: 25418.92
[INFO 2017-06-26 11:56:30,533 main.py:50] epoch 1437, training loss: 13221.59, average training loss: 14454.00, base loss: 25416.34
[INFO 2017-06-26 11:56:31,269 main.py:50] epoch 1438, training loss: 12841.67, average training loss: 14450.64, base loss: 25413.46
[INFO 2017-06-26 11:56:32,004 main.py:50] epoch 1439, training loss: 12691.76, average training loss: 14447.65, base loss: 25411.63
[INFO 2017-06-26 11:56:32,778 main.py:50] epoch 1440, training loss: 13968.83, average training loss: 14446.50, base loss: 25412.51
[INFO 2017-06-26 11:56:33,568 main.py:50] epoch 1441, training loss: 14919.93, average training loss: 14446.73, base loss: 25418.12
[INFO 2017-06-26 11:56:34,327 main.py:50] epoch 1442, training loss: 12515.38, average training loss: 14443.15, base loss: 25416.32
[INFO 2017-06-26 11:56:35,109 main.py:50] epoch 1443, training loss: 14578.37, average training loss: 14440.04, base loss: 25416.36
[INFO 2017-06-26 11:56:35,844 main.py:50] epoch 1444, training loss: 13345.35, average training loss: 14438.99, base loss: 25418.95
[INFO 2017-06-26 11:56:36,580 main.py:50] epoch 1445, training loss: 13798.31, average training loss: 14437.84, base loss: 25421.35
[INFO 2017-06-26 11:56:37,313 main.py:50] epoch 1446, training loss: 12524.66, average training loss: 14435.21, base loss: 25421.86
[INFO 2017-06-26 11:56:38,045 main.py:50] epoch 1447, training loss: 13344.55, average training loss: 14433.88, base loss: 25422.59
[INFO 2017-06-26 11:56:38,778 main.py:50] epoch 1448, training loss: 14413.22, average training loss: 14434.36, base loss: 25427.44
[INFO 2017-06-26 11:56:39,511 main.py:50] epoch 1449, training loss: 14557.17, average training loss: 14433.37, base loss: 25429.45
[INFO 2017-06-26 11:56:40,258 main.py:50] epoch 1450, training loss: 15141.77, average training loss: 14432.86, base loss: 25432.86
[INFO 2017-06-26 11:56:40,990 main.py:50] epoch 1451, training loss: 13714.12, average training loss: 14431.34, base loss: 25434.13
[INFO 2017-06-26 11:56:41,726 main.py:50] epoch 1452, training loss: 13376.07, average training loss: 14430.53, base loss: 25437.21
[INFO 2017-06-26 11:56:42,459 main.py:50] epoch 1453, training loss: 14124.18, average training loss: 14427.39, base loss: 25437.91
[INFO 2017-06-26 11:56:43,193 main.py:50] epoch 1454, training loss: 14326.01, average training loss: 14425.12, base loss: 25439.74
[INFO 2017-06-26 11:56:43,927 main.py:50] epoch 1455, training loss: 14154.55, average training loss: 14422.07, base loss: 25437.58
[INFO 2017-06-26 11:56:44,661 main.py:50] epoch 1456, training loss: 14354.05, average training loss: 14421.10, base loss: 25440.83
[INFO 2017-06-26 11:56:45,395 main.py:50] epoch 1457, training loss: 14704.33, average training loss: 14420.30, base loss: 25444.46
[INFO 2017-06-26 11:56:46,129 main.py:50] epoch 1458, training loss: 12888.23, average training loss: 14418.78, base loss: 25445.62
[INFO 2017-06-26 11:56:46,862 main.py:50] epoch 1459, training loss: 14404.26, average training loss: 14417.38, base loss: 25446.43
[INFO 2017-06-26 11:56:47,596 main.py:50] epoch 1460, training loss: 14246.99, average training loss: 14416.36, base loss: 25448.58
[INFO 2017-06-26 11:56:48,330 main.py:50] epoch 1461, training loss: 12578.06, average training loss: 14410.21, base loss: 25443.77
[INFO 2017-06-26 11:56:49,065 main.py:50] epoch 1462, training loss: 12255.55, average training loss: 14404.14, base loss: 25438.74
[INFO 2017-06-26 11:56:49,798 main.py:50] epoch 1463, training loss: 12697.27, average training loss: 14402.59, base loss: 25439.37
[INFO 2017-06-26 11:56:50,532 main.py:50] epoch 1464, training loss: 13175.66, average training loss: 14400.31, base loss: 25438.84
[INFO 2017-06-26 11:56:51,266 main.py:50] epoch 1465, training loss: 12750.61, average training loss: 14398.10, base loss: 25438.00
[INFO 2017-06-26 11:56:52,000 main.py:50] epoch 1466, training loss: 13169.28, average training loss: 14395.92, base loss: 25438.27
[INFO 2017-06-26 11:56:52,734 main.py:50] epoch 1467, training loss: 13500.70, average training loss: 14394.28, base loss: 25438.81
[INFO 2017-06-26 11:56:53,468 main.py:50] epoch 1468, training loss: 14854.87, average training loss: 14394.58, base loss: 25443.87
[INFO 2017-06-26 11:56:54,201 main.py:50] epoch 1469, training loss: 13149.64, average training loss: 14392.51, base loss: 25444.20
[INFO 2017-06-26 11:56:54,936 main.py:50] epoch 1470, training loss: 12555.31, average training loss: 14389.68, base loss: 25442.60
[INFO 2017-06-26 11:56:55,670 main.py:50] epoch 1471, training loss: 12721.55, average training loss: 14387.68, base loss: 25442.16
[INFO 2017-06-26 11:56:56,404 main.py:50] epoch 1472, training loss: 13312.42, average training loss: 14385.88, base loss: 25443.97
[INFO 2017-06-26 11:56:57,138 main.py:50] epoch 1473, training loss: 12910.72, average training loss: 14383.61, base loss: 25443.05
[INFO 2017-06-26 11:56:57,872 main.py:50] epoch 1474, training loss: 12739.46, average training loss: 14381.40, base loss: 25441.81
[INFO 2017-06-26 11:56:58,607 main.py:50] epoch 1475, training loss: 12731.16, average training loss: 14378.37, base loss: 25439.86
[INFO 2017-06-26 11:56:59,341 main.py:50] epoch 1476, training loss: 13278.12, average training loss: 14376.46, base loss: 25440.34
[INFO 2017-06-26 11:57:00,075 main.py:50] epoch 1477, training loss: 12414.67, average training loss: 14371.31, base loss: 25437.12
[INFO 2017-06-26 11:57:00,813 main.py:50] epoch 1478, training loss: 11846.61, average training loss: 14368.98, base loss: 25437.01
[INFO 2017-06-26 11:57:01,555 main.py:50] epoch 1479, training loss: 12986.76, average training loss: 14366.85, base loss: 25437.69
[INFO 2017-06-26 11:57:02,292 main.py:50] epoch 1480, training loss: 13416.84, average training loss: 14365.44, base loss: 25439.76
[INFO 2017-06-26 11:57:03,030 main.py:50] epoch 1481, training loss: 12958.61, average training loss: 14363.80, base loss: 25440.69
[INFO 2017-06-26 11:57:03,769 main.py:50] epoch 1482, training loss: 13036.67, average training loss: 14359.10, base loss: 25437.75
[INFO 2017-06-26 11:57:04,538 main.py:50] epoch 1483, training loss: 15095.04, average training loss: 14356.11, base loss: 25438.26
[INFO 2017-06-26 11:57:05,278 main.py:50] epoch 1484, training loss: 14291.50, average training loss: 14355.84, base loss: 25439.62
[INFO 2017-06-26 11:57:06,013 main.py:50] epoch 1485, training loss: 12763.03, average training loss: 14354.43, base loss: 25441.12
[INFO 2017-06-26 11:57:06,750 main.py:50] epoch 1486, training loss: 12976.52, average training loss: 14352.85, base loss: 25441.76
[INFO 2017-06-26 11:57:07,486 main.py:50] epoch 1487, training loss: 12191.24, average training loss: 14351.51, base loss: 25444.35
[INFO 2017-06-26 11:57:08,222 main.py:50] epoch 1488, training loss: 13329.83, average training loss: 14346.84, base loss: 25443.21
[INFO 2017-06-26 11:57:08,957 main.py:50] epoch 1489, training loss: 12601.46, average training loss: 14344.66, base loss: 25443.13
[INFO 2017-06-26 11:57:09,691 main.py:50] epoch 1490, training loss: 12819.11, average training loss: 14339.04, base loss: 25438.87
[INFO 2017-06-26 11:57:10,426 main.py:50] epoch 1491, training loss: 14423.01, average training loss: 14337.95, base loss: 25439.68
[INFO 2017-06-26 11:57:11,160 main.py:50] epoch 1492, training loss: 13502.71, average training loss: 14334.38, base loss: 25439.29
[INFO 2017-06-26 11:57:11,895 main.py:50] epoch 1493, training loss: 12782.62, average training loss: 14328.24, base loss: 25433.08
[INFO 2017-06-26 11:57:12,644 main.py:50] epoch 1494, training loss: 14313.08, average training loss: 14325.54, base loss: 25433.46
[INFO 2017-06-26 11:57:13,378 main.py:50] epoch 1495, training loss: 14044.10, average training loss: 14321.66, base loss: 25431.49
[INFO 2017-06-26 11:57:14,113 main.py:50] epoch 1496, training loss: 12096.90, average training loss: 14318.85, base loss: 25428.98
[INFO 2017-06-26 11:57:14,850 main.py:50] epoch 1497, training loss: 14750.37, average training loss: 14316.29, base loss: 25430.32
[INFO 2017-06-26 11:57:15,584 main.py:50] epoch 1498, training loss: 15001.94, average training loss: 14316.67, base loss: 25435.07
[INFO 2017-06-26 11:57:16,319 main.py:50] epoch 1499, training loss: 12078.95, average training loss: 14313.31, base loss: 25429.97
[INFO 2017-06-26 11:57:17,053 main.py:50] epoch 1500, training loss: 13374.65, average training loss: 14308.94, base loss: 25427.13
[INFO 2017-06-26 11:57:17,787 main.py:50] epoch 1501, training loss: 12843.06, average training loss: 14307.29, base loss: 25428.29
[INFO 2017-06-26 11:57:18,521 main.py:50] epoch 1502, training loss: 13969.55, average training loss: 14307.05, base loss: 25431.75
[INFO 2017-06-26 11:57:19,255 main.py:50] epoch 1503, training loss: 14964.56, average training loss: 14304.70, base loss: 25434.83
[INFO 2017-06-26 11:57:19,990 main.py:50] epoch 1504, training loss: 13953.01, average training loss: 14300.89, base loss: 25433.60
[INFO 2017-06-26 11:57:20,724 main.py:50] epoch 1505, training loss: 13639.69, average training loss: 14300.10, base loss: 25437.18
[INFO 2017-06-26 11:57:21,460 main.py:50] epoch 1506, training loss: 14260.95, average training loss: 14300.63, base loss: 25442.19
[INFO 2017-06-26 11:57:22,195 main.py:50] epoch 1507, training loss: 12297.06, average training loss: 14297.18, base loss: 25438.59
[INFO 2017-06-26 11:57:22,930 main.py:50] epoch 1508, training loss: 14689.01, average training loss: 14297.10, base loss: 25441.73
[INFO 2017-06-26 11:57:23,666 main.py:50] epoch 1509, training loss: 12968.91, average training loss: 14295.47, base loss: 25441.43
[INFO 2017-06-26 11:57:24,403 main.py:50] epoch 1510, training loss: 12924.28, average training loss: 14291.10, base loss: 25439.28
[INFO 2017-06-26 11:57:25,141 main.py:50] epoch 1511, training loss: 12769.86, average training loss: 14289.40, base loss: 25440.36
[INFO 2017-06-26 11:57:25,878 main.py:50] epoch 1512, training loss: 13091.22, average training loss: 14287.97, base loss: 25440.58
[INFO 2017-06-26 11:57:26,616 main.py:50] epoch 1513, training loss: 14001.79, average training loss: 14285.33, base loss: 25442.03
[INFO 2017-06-26 11:57:27,353 main.py:50] epoch 1514, training loss: 14382.72, average training loss: 14282.22, base loss: 25443.97
[INFO 2017-06-26 11:57:28,091 main.py:50] epoch 1515, training loss: 12803.71, average training loss: 14281.17, base loss: 25445.01
[INFO 2017-06-26 11:57:28,829 main.py:50] epoch 1516, training loss: 13103.22, average training loss: 14279.84, base loss: 25446.48
[INFO 2017-06-26 11:57:29,567 main.py:50] epoch 1517, training loss: 12829.77, average training loss: 14278.50, base loss: 25446.04
[INFO 2017-06-26 11:57:30,301 main.py:50] epoch 1518, training loss: 12710.18, average training loss: 14276.45, base loss: 25445.32
[INFO 2017-06-26 11:57:31,033 main.py:50] epoch 1519, training loss: 14073.19, average training loss: 14276.05, base loss: 25447.35
[INFO 2017-06-26 11:57:31,766 main.py:50] epoch 1520, training loss: 13564.29, average training loss: 14271.80, base loss: 25446.13
[INFO 2017-06-26 11:57:32,499 main.py:50] epoch 1521, training loss: 13949.66, average training loss: 14270.95, base loss: 25447.16
[INFO 2017-06-26 11:57:33,233 main.py:50] epoch 1522, training loss: 13527.10, average training loss: 14269.79, base loss: 25448.45
[INFO 2017-06-26 11:57:33,967 main.py:50] epoch 1523, training loss: 12373.39, average training loss: 14265.78, base loss: 25448.30
[INFO 2017-06-26 11:57:34,701 main.py:50] epoch 1524, training loss: 11792.19, average training loss: 14262.24, base loss: 25444.15
[INFO 2017-06-26 11:57:35,434 main.py:50] epoch 1525, training loss: 12811.53, average training loss: 14257.56, base loss: 25442.19
[INFO 2017-06-26 11:57:36,168 main.py:50] epoch 1526, training loss: 12798.53, average training loss: 14252.61, base loss: 25437.76
[INFO 2017-06-26 11:57:36,902 main.py:50] epoch 1527, training loss: 14759.41, average training loss: 14249.93, base loss: 25439.59
[INFO 2017-06-26 11:57:37,636 main.py:50] epoch 1528, training loss: 13732.02, average training loss: 14246.79, base loss: 25441.05
[INFO 2017-06-26 11:57:38,369 main.py:50] epoch 1529, training loss: 14417.24, average training loss: 14246.59, base loss: 25443.07
[INFO 2017-06-26 11:57:39,102 main.py:50] epoch 1530, training loss: 14175.40, average training loss: 14246.50, base loss: 25446.34
[INFO 2017-06-26 11:57:39,843 main.py:50] epoch 1531, training loss: 13065.24, average training loss: 14241.93, base loss: 25443.63
[INFO 2017-06-26 11:57:40,575 main.py:50] epoch 1532, training loss: 12969.38, average training loss: 14237.08, base loss: 25439.47
[INFO 2017-06-26 11:57:41,308 main.py:50] epoch 1533, training loss: 14150.13, average training loss: 14237.07, base loss: 25444.82
[INFO 2017-06-26 11:57:42,041 main.py:50] epoch 1534, training loss: 12621.56, average training loss: 14235.68, base loss: 25444.64
[INFO 2017-06-26 11:57:42,774 main.py:50] epoch 1535, training loss: 14154.71, average training loss: 14232.42, base loss: 25443.34
[INFO 2017-06-26 11:57:43,507 main.py:50] epoch 1536, training loss: 13831.92, average training loss: 14232.02, base loss: 25446.27
[INFO 2017-06-26 11:57:44,241 main.py:50] epoch 1537, training loss: 12775.26, average training loss: 14230.03, base loss: 25447.03
[INFO 2017-06-26 11:57:44,988 main.py:50] epoch 1538, training loss: 14101.94, average training loss: 14229.09, base loss: 25448.48
[INFO 2017-06-26 11:57:45,722 main.py:50] epoch 1539, training loss: 12519.53, average training loss: 14227.39, base loss: 25449.86
[INFO 2017-06-26 11:57:46,456 main.py:50] epoch 1540, training loss: 13034.97, average training loss: 14225.71, base loss: 25450.65
[INFO 2017-06-26 11:57:47,188 main.py:50] epoch 1541, training loss: 14160.64, average training loss: 14226.04, base loss: 25454.12
[INFO 2017-06-26 11:57:47,923 main.py:50] epoch 1542, training loss: 13434.23, average training loss: 14224.21, base loss: 25455.23
[INFO 2017-06-26 11:57:48,656 main.py:50] epoch 1543, training loss: 14641.38, average training loss: 14225.41, base loss: 25461.22
[INFO 2017-06-26 11:57:49,390 main.py:50] epoch 1544, training loss: 12796.33, average training loss: 14223.36, base loss: 25461.23
[INFO 2017-06-26 11:57:50,123 main.py:50] epoch 1545, training loss: 12510.39, average training loss: 14221.49, base loss: 25460.60
[INFO 2017-06-26 11:57:50,857 main.py:50] epoch 1546, training loss: 12143.66, average training loss: 14216.19, base loss: 25455.96
[INFO 2017-06-26 11:57:51,590 main.py:50] epoch 1547, training loss: 14544.08, average training loss: 14216.24, base loss: 25459.68
[INFO 2017-06-26 11:57:52,324 main.py:50] epoch 1548, training loss: 12692.30, average training loss: 14214.65, base loss: 25461.09
[INFO 2017-06-26 11:57:53,057 main.py:50] epoch 1549, training loss: 12955.44, average training loss: 14213.10, base loss: 25460.89
[INFO 2017-06-26 11:57:53,790 main.py:50] epoch 1550, training loss: 12969.12, average training loss: 14212.14, base loss: 25461.26
[INFO 2017-06-26 11:57:54,524 main.py:50] epoch 1551, training loss: 14925.70, average training loss: 14210.02, base loss: 25462.65
[INFO 2017-06-26 11:57:55,257 main.py:50] epoch 1552, training loss: 12679.13, average training loss: 14205.27, base loss: 25458.08
[INFO 2017-06-26 11:57:55,990 main.py:50] epoch 1553, training loss: 14709.25, average training loss: 14202.79, base loss: 25459.88
[INFO 2017-06-26 11:57:56,723 main.py:50] epoch 1554, training loss: 14818.40, average training loss: 14202.30, base loss: 25461.73
[INFO 2017-06-26 11:57:57,456 main.py:50] epoch 1555, training loss: 12606.61, average training loss: 14198.08, base loss: 25459.02
[INFO 2017-06-26 11:57:58,193 main.py:50] epoch 1556, training loss: 12670.99, average training loss: 14196.68, base loss: 25458.66
[INFO 2017-06-26 11:57:58,930 main.py:50] epoch 1557, training loss: 12643.64, average training loss: 14195.23, base loss: 25458.68
[INFO 2017-06-26 11:57:59,668 main.py:50] epoch 1558, training loss: 12900.48, average training loss: 14194.17, base loss: 25458.29
[INFO 2017-06-26 11:58:00,406 main.py:50] epoch 1559, training loss: 12253.14, average training loss: 14189.87, base loss: 25455.92
[INFO 2017-06-26 11:58:01,144 main.py:50] epoch 1560, training loss: 13146.56, average training loss: 14185.78, base loss: 25455.10
[INFO 2017-06-26 11:58:01,881 main.py:50] epoch 1561, training loss: 13967.84, average training loss: 14184.93, base loss: 25456.32
[INFO 2017-06-26 11:58:02,618 main.py:50] epoch 1562, training loss: 13112.80, average training loss: 14183.91, base loss: 25458.38
[INFO 2017-06-26 11:58:03,358 main.py:50] epoch 1563, training loss: 12599.12, average training loss: 14182.80, base loss: 25460.07
[INFO 2017-06-26 11:58:04,096 main.py:50] epoch 1564, training loss: 14200.59, average training loss: 14179.46, base loss: 25458.81
[INFO 2017-06-26 11:58:04,830 main.py:50] epoch 1565, training loss: 12544.66, average training loss: 14178.43, base loss: 25460.06
[INFO 2017-06-26 11:58:05,566 main.py:50] epoch 1566, training loss: 13128.93, average training loss: 14175.25, base loss: 25460.44
[INFO 2017-06-26 11:58:06,304 main.py:50] epoch 1567, training loss: 14112.15, average training loss: 14174.48, base loss: 25462.67
[INFO 2017-06-26 11:58:07,040 main.py:50] epoch 1568, training loss: 12368.03, average training loss: 14172.12, base loss: 25459.29
[INFO 2017-06-26 11:58:07,810 main.py:50] epoch 1569, training loss: 14176.99, average training loss: 14171.52, base loss: 25460.58
[INFO 2017-06-26 11:58:08,544 main.py:50] epoch 1570, training loss: 13131.39, average training loss: 14170.31, base loss: 25461.90
[INFO 2017-06-26 11:58:09,280 main.py:50] epoch 1571, training loss: 13457.41, average training loss: 14169.20, base loss: 25465.07
[INFO 2017-06-26 11:58:10,014 main.py:50] epoch 1572, training loss: 15129.06, average training loss: 14167.02, base loss: 25467.75
[INFO 2017-06-26 11:58:10,744 main.py:50] epoch 1573, training loss: 14185.88, average training loss: 14165.69, base loss: 25467.72
[INFO 2017-06-26 11:58:11,478 main.py:50] epoch 1574, training loss: 14811.56, average training loss: 14165.36, base loss: 25471.20
[INFO 2017-06-26 11:58:12,213 main.py:50] epoch 1575, training loss: 12214.29, average training loss: 14163.71, base loss: 25470.76
[INFO 2017-06-26 11:58:12,947 main.py:50] epoch 1576, training loss: 12455.87, average training loss: 14161.47, base loss: 25469.27
[INFO 2017-06-26 11:58:13,682 main.py:50] epoch 1577, training loss: 12823.68, average training loss: 14160.69, base loss: 25470.98
[INFO 2017-06-26 11:58:14,416 main.py:50] epoch 1578, training loss: 12832.79, average training loss: 14156.53, base loss: 25469.90
[INFO 2017-06-26 11:58:15,150 main.py:50] epoch 1579, training loss: 12963.06, average training loss: 14152.98, base loss: 25468.52
[INFO 2017-06-26 11:58:15,914 main.py:50] epoch 1580, training loss: 12543.82, average training loss: 14148.70, base loss: 25466.89
[INFO 2017-06-26 11:58:16,650 main.py:50] epoch 1581, training loss: 14084.95, average training loss: 14148.32, base loss: 25468.57
[INFO 2017-06-26 11:58:17,394 main.py:50] epoch 1582, training loss: 14423.82, average training loss: 14145.07, base loss: 25467.89
[INFO 2017-06-26 11:58:18,124 main.py:50] epoch 1583, training loss: 12918.34, average training loss: 14143.88, base loss: 25468.49
[INFO 2017-06-26 11:58:18,855 main.py:50] epoch 1584, training loss: 13427.66, average training loss: 14139.83, base loss: 25466.93
[INFO 2017-06-26 11:58:19,585 main.py:50] epoch 1585, training loss: 13123.59, average training loss: 14139.10, base loss: 25468.32
[INFO 2017-06-26 11:58:20,314 main.py:50] epoch 1586, training loss: 14071.84, average training loss: 14138.63, base loss: 25470.53
[INFO 2017-06-26 11:58:21,045 main.py:50] epoch 1587, training loss: 14473.81, average training loss: 14137.06, base loss: 25474.86
[INFO 2017-06-26 11:58:21,775 main.py:50] epoch 1588, training loss: 13446.71, average training loss: 14133.21, base loss: 25471.01
[INFO 2017-06-26 11:58:22,504 main.py:50] epoch 1589, training loss: 13351.87, average training loss: 14132.15, base loss: 25472.58
[INFO 2017-06-26 11:58:23,234 main.py:50] epoch 1590, training loss: 13177.87, average training loss: 14128.62, base loss: 25473.46
[INFO 2017-06-26 11:58:23,964 main.py:50] epoch 1591, training loss: 14477.88, average training loss: 14126.30, base loss: 25473.96
[INFO 2017-06-26 11:58:24,694 main.py:50] epoch 1592, training loss: 12551.58, average training loss: 14123.13, base loss: 25470.19
[INFO 2017-06-26 11:58:25,424 main.py:50] epoch 1593, training loss: 12849.99, average training loss: 14123.08, base loss: 25474.06
[INFO 2017-06-26 11:58:26,155 main.py:50] epoch 1594, training loss: 12487.52, average training loss: 14117.71, base loss: 25469.60
[INFO 2017-06-26 11:58:26,889 main.py:50] epoch 1595, training loss: 13597.16, average training loss: 14116.75, base loss: 25472.05
[INFO 2017-06-26 11:58:27,622 main.py:50] epoch 1596, training loss: 14487.48, average training loss: 14117.09, base loss: 25474.47
[INFO 2017-06-26 11:58:28,354 main.py:50] epoch 1597, training loss: 14185.81, average training loss: 14116.51, base loss: 25475.76
[INFO 2017-06-26 11:58:29,086 main.py:50] epoch 1598, training loss: 12820.35, average training loss: 14112.25, base loss: 25471.85
[INFO 2017-06-26 11:58:29,819 main.py:50] epoch 1599, training loss: 14140.20, average training loss: 14111.80, base loss: 25473.36
[INFO 2017-06-26 11:58:30,555 main.py:50] epoch 1600, training loss: 14264.80, average training loss: 14109.19, base loss: 25475.42
[INFO 2017-06-26 11:58:31,288 main.py:50] epoch 1601, training loss: 13660.45, average training loss: 14105.58, base loss: 25474.52
[INFO 2017-06-26 11:58:32,021 main.py:50] epoch 1602, training loss: 12309.00, average training loss: 14103.63, base loss: 25473.20
[INFO 2017-06-26 11:58:32,753 main.py:50] epoch 1603, training loss: 14987.13, average training loss: 14104.58, base loss: 25477.94
[INFO 2017-06-26 11:58:33,487 main.py:50] epoch 1604, training loss: 14335.91, average training loss: 14104.10, base loss: 25481.22
[INFO 2017-06-26 11:58:34,222 main.py:50] epoch 1605, training loss: 12796.44, average training loss: 14102.87, base loss: 25482.75
[INFO 2017-06-26 11:58:34,955 main.py:50] epoch 1606, training loss: 12220.31, average training loss: 14099.92, base loss: 25479.24
[INFO 2017-06-26 11:58:35,689 main.py:50] epoch 1607, training loss: 12194.23, average training loss: 14094.74, base loss: 25474.31
[INFO 2017-06-26 11:58:36,422 main.py:50] epoch 1608, training loss: 13443.68, average training loss: 14093.19, base loss: 25472.82
[INFO 2017-06-26 11:58:37,186 main.py:50] epoch 1609, training loss: 12971.15, average training loss: 14092.09, base loss: 25474.33
[INFO 2017-06-26 11:58:37,919 main.py:50] epoch 1610, training loss: 14623.54, average training loss: 14092.85, base loss: 25479.39
[INFO 2017-06-26 11:58:38,661 main.py:50] epoch 1611, training loss: 12187.97, average training loss: 14090.38, base loss: 25477.65
[INFO 2017-06-26 11:58:39,393 main.py:50] epoch 1612, training loss: 13355.12, average training loss: 14086.49, base loss: 25475.47
[INFO 2017-06-26 11:58:40,122 main.py:50] epoch 1613, training loss: 12779.92, average training loss: 14082.72, base loss: 25475.47
[INFO 2017-06-26 11:58:40,854 main.py:50] epoch 1614, training loss: 12477.03, average training loss: 14077.32, base loss: 25470.10
[INFO 2017-06-26 11:58:41,584 main.py:50] epoch 1615, training loss: 13052.92, average training loss: 14072.76, base loss: 25465.44
[INFO 2017-06-26 11:58:42,318 main.py:50] epoch 1616, training loss: 11679.37, average training loss: 14070.38, base loss: 25462.18
[INFO 2017-06-26 11:58:43,085 main.py:50] epoch 1617, training loss: 12940.19, average training loss: 14065.22, base loss: 25458.23
[INFO 2017-06-26 11:58:43,843 main.py:50] epoch 1618, training loss: 13243.44, average training loss: 14063.86, base loss: 25457.97
[INFO 2017-06-26 11:58:44,573 main.py:50] epoch 1619, training loss: 13942.09, average training loss: 14063.02, base loss: 25459.35
[INFO 2017-06-26 11:58:45,305 main.py:50] epoch 1620, training loss: 15296.88, average training loss: 14063.14, base loss: 25462.57
[INFO 2017-06-26 11:58:46,036 main.py:50] epoch 1621, training loss: 14013.41, average training loss: 14062.11, base loss: 25463.49
[INFO 2017-06-26 11:58:46,767 main.py:50] epoch 1622, training loss: 14312.18, average training loss: 14059.01, base loss: 25463.54
[INFO 2017-06-26 11:58:47,498 main.py:50] epoch 1623, training loss: 12118.15, average training loss: 14053.70, base loss: 25458.27
[INFO 2017-06-26 11:58:48,228 main.py:50] epoch 1624, training loss: 12845.90, average training loss: 14050.32, base loss: 25457.25
[INFO 2017-06-26 11:58:48,959 main.py:50] epoch 1625, training loss: 12788.67, average training loss: 14045.85, base loss: 25454.63
[INFO 2017-06-26 11:58:49,702 main.py:50] epoch 1626, training loss: 12845.21, average training loss: 14044.60, base loss: 25455.20
[INFO 2017-06-26 11:58:50,432 main.py:50] epoch 1627, training loss: 12493.20, average training loss: 14043.42, base loss: 25455.28
[INFO 2017-06-26 11:58:51,162 main.py:50] epoch 1628, training loss: 14497.20, average training loss: 14043.37, base loss: 25457.96
[INFO 2017-06-26 11:58:51,892 main.py:50] epoch 1629, training loss: 13719.78, average training loss: 14043.01, base loss: 25459.40
[INFO 2017-06-26 11:58:52,622 main.py:50] epoch 1630, training loss: 11992.05, average training loss: 14041.64, base loss: 25457.26
[INFO 2017-06-26 11:58:53,352 main.py:50] epoch 1631, training loss: 12686.50, average training loss: 14040.03, base loss: 25456.67
[INFO 2017-06-26 11:58:54,084 main.py:50] epoch 1632, training loss: 12084.25, average training loss: 14037.65, base loss: 25455.71
[INFO 2017-06-26 11:58:54,813 main.py:50] epoch 1633, training loss: 12014.32, average training loss: 14035.89, base loss: 25453.35
[INFO 2017-06-26 11:58:55,544 main.py:50] epoch 1634, training loss: 12876.04, average training loss: 14035.02, base loss: 25455.07
[INFO 2017-06-26 11:58:56,275 main.py:50] epoch 1635, training loss: 13229.98, average training loss: 14034.63, base loss: 25456.78
[INFO 2017-06-26 11:58:57,005 main.py:50] epoch 1636, training loss: 13643.47, average training loss: 14034.19, base loss: 25458.81
[INFO 2017-06-26 11:58:57,735 main.py:50] epoch 1637, training loss: 13331.25, average training loss: 14030.39, base loss: 25458.11
[INFO 2017-06-26 11:58:58,465 main.py:50] epoch 1638, training loss: 12692.77, average training loss: 14027.80, base loss: 25456.33
[INFO 2017-06-26 11:58:59,196 main.py:50] epoch 1639, training loss: 13820.36, average training loss: 14027.02, base loss: 25457.96
[INFO 2017-06-26 11:58:59,927 main.py:50] epoch 1640, training loss: 12774.99, average training loss: 14025.11, base loss: 25457.97
[INFO 2017-06-26 11:59:00,659 main.py:50] epoch 1641, training loss: 13748.50, average training loss: 14022.73, base loss: 25458.59
[INFO 2017-06-26 11:59:01,423 main.py:50] epoch 1642, training loss: 13022.81, average training loss: 14021.04, base loss: 25458.90
[INFO 2017-06-26 11:59:02,196 main.py:50] epoch 1643, training loss: 12747.82, average training loss: 14016.47, base loss: 25455.12
[INFO 2017-06-26 11:59:02,928 main.py:50] epoch 1644, training loss: 13195.46, average training loss: 14012.91, base loss: 25454.62
[INFO 2017-06-26 11:59:03,658 main.py:50] epoch 1645, training loss: 14885.83, average training loss: 14009.44, base loss: 25452.68
[INFO 2017-06-26 11:59:04,388 main.py:50] epoch 1646, training loss: 14217.38, average training loss: 14009.32, base loss: 25455.35
[INFO 2017-06-26 11:59:05,119 main.py:50] epoch 1647, training loss: 12936.07, average training loss: 14008.60, base loss: 25458.38
[INFO 2017-06-26 11:59:05,849 main.py:50] epoch 1648, training loss: 13251.65, average training loss: 14004.76, base loss: 25457.92
[INFO 2017-06-26 11:59:06,580 main.py:50] epoch 1649, training loss: 13161.83, average training loss: 14003.10, base loss: 25458.22
[INFO 2017-06-26 11:59:07,313 main.py:50] epoch 1650, training loss: 12607.00, average training loss: 14001.46, base loss: 25458.71
[INFO 2017-06-26 11:59:08,046 main.py:50] epoch 1651, training loss: 12740.92, average training loss: 13999.94, base loss: 25459.30
[INFO 2017-06-26 11:59:08,811 main.py:50] epoch 1652, training loss: 12802.27, average training loss: 13995.38, base loss: 25455.85
[INFO 2017-06-26 11:59:09,552 main.py:50] epoch 1653, training loss: 14408.64, average training loss: 13996.89, base loss: 25462.00
[INFO 2017-06-26 11:59:10,282 main.py:50] epoch 1654, training loss: 12320.56, average training loss: 13991.42, base loss: 25457.63
[INFO 2017-06-26 11:59:11,015 main.py:50] epoch 1655, training loss: 11865.57, average training loss: 13989.10, base loss: 25454.38
[INFO 2017-06-26 11:59:11,746 main.py:50] epoch 1656, training loss: 12532.11, average training loss: 13987.84, base loss: 25456.08
[INFO 2017-06-26 11:59:12,475 main.py:50] epoch 1657, training loss: 13587.48, average training loss: 13986.96, base loss: 25455.95
[INFO 2017-06-26 11:59:13,204 main.py:50] epoch 1658, training loss: 12834.00, average training loss: 13985.86, base loss: 25456.91
[INFO 2017-06-26 11:59:13,936 main.py:50] epoch 1659, training loss: 13286.41, average training loss: 13985.17, base loss: 25458.85
[INFO 2017-06-26 11:59:14,672 main.py:50] epoch 1660, training loss: 13637.33, average training loss: 13980.87, base loss: 25456.03
[INFO 2017-06-26 11:59:15,410 main.py:50] epoch 1661, training loss: 12102.02, average training loss: 13976.77, base loss: 25453.77
[INFO 2017-06-26 11:59:16,143 main.py:50] epoch 1662, training loss: 14437.37, average training loss: 13976.63, base loss: 25457.56
[INFO 2017-06-26 11:59:16,876 main.py:50] epoch 1663, training loss: 13718.84, average training loss: 13976.30, base loss: 25460.66
[INFO 2017-06-26 11:59:17,607 main.py:50] epoch 1664, training loss: 13591.16, average training loss: 13976.07, base loss: 25463.55
[INFO 2017-06-26 11:59:18,336 main.py:50] epoch 1665, training loss: 14135.67, average training loss: 13973.98, base loss: 25466.43
[INFO 2017-06-26 11:59:19,066 main.py:50] epoch 1666, training loss: 13372.74, average training loss: 13973.58, base loss: 25468.34
[INFO 2017-06-26 11:59:19,797 main.py:50] epoch 1667, training loss: 12637.67, average training loss: 13971.96, base loss: 25467.95
[INFO 2017-06-26 11:59:20,528 main.py:50] epoch 1668, training loss: 12941.03, average training loss: 13970.29, base loss: 25469.60
[INFO 2017-06-26 11:59:21,260 main.py:50] epoch 1669, training loss: 14116.67, average training loss: 13970.62, base loss: 25474.39
[INFO 2017-06-26 11:59:22,004 main.py:50] epoch 1670, training loss: 12073.83, average training loss: 13965.18, base loss: 25468.66
[INFO 2017-06-26 11:59:22,733 main.py:50] epoch 1671, training loss: 13944.89, average training loss: 13961.16, base loss: 25466.45
[INFO 2017-06-26 11:59:23,463 main.py:50] epoch 1672, training loss: 13498.11, average training loss: 13957.90, base loss: 25466.18
[INFO 2017-06-26 11:59:24,194 main.py:50] epoch 1673, training loss: 12882.18, average training loss: 13956.60, base loss: 25466.18
[INFO 2017-06-26 11:59:24,924 main.py:50] epoch 1674, training loss: 12597.86, average training loss: 13954.73, base loss: 25466.14
[INFO 2017-06-26 11:59:25,655 main.py:50] epoch 1675, training loss: 13474.22, average training loss: 13954.65, base loss: 25468.85
[INFO 2017-06-26 11:59:26,387 main.py:50] epoch 1676, training loss: 11924.57, average training loss: 13950.11, base loss: 25465.07
[INFO 2017-06-26 11:59:27,119 main.py:50] epoch 1677, training loss: 13198.59, average training loss: 13949.68, base loss: 25465.75
[INFO 2017-06-26 11:59:27,849 main.py:50] epoch 1678, training loss: 11742.85, average training loss: 13946.97, base loss: 25463.19
[INFO 2017-06-26 11:59:28,579 main.py:50] epoch 1679, training loss: 14702.12, average training loss: 13946.92, base loss: 25465.78
[INFO 2017-06-26 11:59:29,314 main.py:50] epoch 1680, training loss: 12608.08, average training loss: 13946.27, base loss: 25467.25
[INFO 2017-06-26 11:59:30,050 main.py:50] epoch 1681, training loss: 13468.22, average training loss: 13944.44, base loss: 25465.78
[INFO 2017-06-26 11:59:30,784 main.py:50] epoch 1682, training loss: 13817.97, average training loss: 13942.18, base loss: 25468.29
[INFO 2017-06-26 11:59:31,521 main.py:50] epoch 1683, training loss: 13100.58, average training loss: 13938.25, base loss: 25466.90
[INFO 2017-06-26 11:59:32,257 main.py:50] epoch 1684, training loss: 14362.96, average training loss: 13939.57, base loss: 25473.14
[INFO 2017-06-26 11:59:32,994 main.py:50] epoch 1685, training loss: 13886.62, average training loss: 13939.22, base loss: 25475.26
[INFO 2017-06-26 11:59:33,730 main.py:50] epoch 1686, training loss: 12742.71, average training loss: 13934.72, base loss: 25472.49
[INFO 2017-06-26 11:59:34,468 main.py:50] epoch 1687, training loss: 12413.95, average training loss: 13929.78, base loss: 25468.42
[INFO 2017-06-26 11:59:35,206 main.py:50] epoch 1688, training loss: 12995.67, average training loss: 13928.41, base loss: 25469.18
[INFO 2017-06-26 11:59:35,943 main.py:50] epoch 1689, training loss: 12287.43, average training loss: 13923.87, base loss: 25465.66
[INFO 2017-06-26 11:59:36,680 main.py:50] epoch 1690, training loss: 13936.93, average training loss: 13921.41, base loss: 25466.59
[INFO 2017-06-26 11:59:37,418 main.py:50] epoch 1691, training loss: 13201.62, average training loss: 13920.34, base loss: 25467.87
[INFO 2017-06-26 11:59:38,156 main.py:50] epoch 1692, training loss: 13693.60, average training loss: 13916.67, base loss: 25464.35
[INFO 2017-06-26 11:59:38,893 main.py:50] epoch 1693, training loss: 12185.64, average training loss: 13912.04, base loss: 25459.36
[INFO 2017-06-26 11:59:39,631 main.py:50] epoch 1694, training loss: 13589.59, average training loss: 13910.43, base loss: 25457.51
[INFO 2017-06-26 11:59:40,364 main.py:50] epoch 1695, training loss: 14467.15, average training loss: 13910.64, base loss: 25459.75
[INFO 2017-06-26 11:59:41,099 main.py:50] epoch 1696, training loss: 11582.58, average training loss: 13908.58, base loss: 25458.32
[INFO 2017-06-26 11:59:41,833 main.py:50] epoch 1697, training loss: 12811.06, average training loss: 13907.33, base loss: 25459.56
[INFO 2017-06-26 11:59:42,600 main.py:50] epoch 1698, training loss: 11972.92, average training loss: 13902.22, base loss: 25453.08
[INFO 2017-06-26 11:59:43,340 main.py:50] epoch 1699, training loss: 12471.23, average training loss: 13897.11, base loss: 25448.35
[INFO 2017-06-26 11:59:44,071 main.py:50] epoch 1700, training loss: 14475.54, average training loss: 13893.70, base loss: 25447.60
[INFO 2017-06-26 11:59:44,802 main.py:50] epoch 1701, training loss: 13155.48, average training loss: 13892.53, base loss: 25447.60
[INFO 2017-06-26 11:59:45,532 main.py:50] epoch 1702, training loss: 13518.39, average training loss: 13891.50, base loss: 25446.83
[INFO 2017-06-26 11:59:46,264 main.py:50] epoch 1703, training loss: 12772.23, average training loss: 13890.38, base loss: 25446.41
[INFO 2017-06-26 11:59:46,996 main.py:50] epoch 1704, training loss: 12073.72, average training loss: 13884.56, base loss: 25439.36
[INFO 2017-06-26 11:59:47,729 main.py:50] epoch 1705, training loss: 13516.16, average training loss: 13884.10, base loss: 25438.83
[INFO 2017-06-26 11:59:48,462 main.py:50] epoch 1706, training loss: 14235.07, average training loss: 13884.40, base loss: 25442.99
[INFO 2017-06-26 11:59:49,196 main.py:50] epoch 1707, training loss: 11670.78, average training loss: 13882.21, base loss: 25440.97
[INFO 2017-06-26 11:59:49,929 main.py:50] epoch 1708, training loss: 14348.80, average training loss: 13879.21, base loss: 25441.52
[INFO 2017-06-26 11:59:50,662 main.py:50] epoch 1709, training loss: 12543.58, average training loss: 13878.10, base loss: 25442.20
[INFO 2017-06-26 11:59:51,395 main.py:50] epoch 1710, training loss: 14945.29, average training loss: 13875.50, base loss: 25443.10
[INFO 2017-06-26 11:59:52,127 main.py:50] epoch 1711, training loss: 13236.93, average training loss: 13874.11, base loss: 25441.30
[INFO 2017-06-26 11:59:52,861 main.py:50] epoch 1712, training loss: 13881.50, average training loss: 13873.82, base loss: 25444.20
[INFO 2017-06-26 11:59:53,595 main.py:50] epoch 1713, training loss: 13719.48, average training loss: 13873.96, base loss: 25447.79
[INFO 2017-06-26 11:59:54,341 main.py:50] epoch 1714, training loss: 14273.51, average training loss: 13874.08, base loss: 25449.68
[INFO 2017-06-26 11:59:55,075 main.py:50] epoch 1715, training loss: 12405.31, average training loss: 13872.23, base loss: 25449.02
[INFO 2017-06-26 11:59:55,809 main.py:50] epoch 1716, training loss: 14267.09, average training loss: 13872.70, base loss: 25453.70
[INFO 2017-06-26 11:59:56,541 main.py:50] epoch 1717, training loss: 13196.83, average training loss: 13869.61, base loss: 25453.72
[INFO 2017-06-26 11:59:57,274 main.py:50] epoch 1718, training loss: 14777.29, average training loss: 13870.94, base loss: 25459.66
[INFO 2017-06-26 11:59:58,008 main.py:50] epoch 1719, training loss: 12651.50, average training loss: 13869.92, base loss: 25461.09
[INFO 2017-06-26 11:59:58,741 main.py:50] epoch 1720, training loss: 12325.59, average training loss: 13865.58, base loss: 25457.50
[INFO 2017-06-26 11:59:59,474 main.py:50] epoch 1721, training loss: 14072.88, average training loss: 13865.35, base loss: 25459.09
[INFO 2017-06-26 12:00:00,209 main.py:50] epoch 1722, training loss: 12441.19, average training loss: 13863.70, base loss: 25460.21
[INFO 2017-06-26 12:00:00,942 main.py:50] epoch 1723, training loss: 11721.88, average training loss: 13859.99, base loss: 25453.70
[INFO 2017-06-26 12:00:01,676 main.py:50] epoch 1724, training loss: 12276.39, average training loss: 13857.58, base loss: 25449.28
[INFO 2017-06-26 12:00:02,409 main.py:50] epoch 1725, training loss: 14598.23, average training loss: 13855.67, base loss: 25451.41
[INFO 2017-06-26 12:00:03,142 main.py:50] epoch 1726, training loss: 13357.34, average training loss: 13855.12, base loss: 25453.84
[INFO 2017-06-26 12:00:03,874 main.py:50] epoch 1727, training loss: 14695.30, average training loss: 13855.39, base loss: 25456.32
[INFO 2017-06-26 12:00:04,607 main.py:50] epoch 1728, training loss: 14037.99, average training loss: 13852.19, base loss: 25455.43
[INFO 2017-06-26 12:00:05,341 main.py:50] epoch 1729, training loss: 13554.84, average training loss: 13850.84, base loss: 25455.36
[INFO 2017-06-26 12:00:06,074 main.py:50] epoch 1730, training loss: 12871.61, average training loss: 13849.93, base loss: 25456.57
[INFO 2017-06-26 12:00:06,808 main.py:50] epoch 1731, training loss: 12020.02, average training loss: 13848.15, base loss: 25455.30
[INFO 2017-06-26 12:00:07,542 main.py:50] epoch 1732, training loss: 12588.36, average training loss: 13846.74, base loss: 25455.41
[INFO 2017-06-26 12:00:08,275 main.py:50] epoch 1733, training loss: 13471.62, average training loss: 13845.53, base loss: 25456.31
[INFO 2017-06-26 12:00:09,009 main.py:50] epoch 1734, training loss: 12131.49, average training loss: 13844.18, base loss: 25456.99
[INFO 2017-06-26 12:00:09,743 main.py:50] epoch 1735, training loss: 13185.39, average training loss: 13843.36, base loss: 25459.25
[INFO 2017-06-26 12:00:10,477 main.py:50] epoch 1736, training loss: 12943.92, average training loss: 13842.70, base loss: 25460.32
[INFO 2017-06-26 12:00:11,209 main.py:50] epoch 1737, training loss: 12432.33, average training loss: 13841.48, base loss: 25459.57
[INFO 2017-06-26 12:00:11,942 main.py:50] epoch 1738, training loss: 11977.88, average training loss: 13839.99, base loss: 25458.34
[INFO 2017-06-26 12:00:12,675 main.py:50] epoch 1739, training loss: 14215.34, average training loss: 13839.52, base loss: 25459.39
[INFO 2017-06-26 12:00:13,409 main.py:50] epoch 1740, training loss: 12240.79, average training loss: 13835.70, base loss: 25456.90
[INFO 2017-06-26 12:00:14,175 main.py:50] epoch 1741, training loss: 13344.26, average training loss: 13834.69, base loss: 25457.45
[INFO 2017-06-26 12:00:14,919 main.py:50] epoch 1742, training loss: 13339.66, average training loss: 13833.96, base loss: 25459.88
[INFO 2017-06-26 12:00:15,649 main.py:50] epoch 1743, training loss: 12568.51, average training loss: 13829.84, base loss: 25457.07
[INFO 2017-06-26 12:00:16,380 main.py:50] epoch 1744, training loss: 12376.48, average training loss: 13827.02, base loss: 25452.43
[INFO 2017-06-26 12:00:17,111 main.py:50] epoch 1745, training loss: 13846.50, average training loss: 13826.70, base loss: 25453.83
[INFO 2017-06-26 12:00:17,843 main.py:50] epoch 1746, training loss: 15047.85, average training loss: 13827.28, base loss: 25457.85
[INFO 2017-06-26 12:00:18,576 main.py:50] epoch 1747, training loss: 12262.30, average training loss: 13822.33, base loss: 25453.00
[INFO 2017-06-26 12:00:19,309 main.py:50] epoch 1748, training loss: 15121.98, average training loss: 13824.14, base loss: 25460.11
[INFO 2017-06-26 12:00:20,041 main.py:50] epoch 1749, training loss: 12422.04, average training loss: 13820.18, base loss: 25458.40
[INFO 2017-06-26 12:00:20,771 main.py:50] epoch 1750, training loss: 14598.79, average training loss: 13820.25, base loss: 25460.97
[INFO 2017-06-26 12:00:21,502 main.py:50] epoch 1751, training loss: 12441.27, average training loss: 13815.82, base loss: 25456.71
[INFO 2017-06-26 12:00:22,268 main.py:50] epoch 1752, training loss: 12290.15, average training loss: 13812.02, base loss: 25453.76
[INFO 2017-06-26 12:00:23,003 main.py:50] epoch 1753, training loss: 12638.13, average training loss: 13810.45, base loss: 25452.33
[INFO 2017-06-26 12:00:23,733 main.py:50] epoch 1754, training loss: 12360.34, average training loss: 13806.31, base loss: 25449.23
[INFO 2017-06-26 12:00:24,467 main.py:50] epoch 1755, training loss: 14138.93, average training loss: 13806.17, base loss: 25450.59
[INFO 2017-06-26 12:00:25,197 main.py:50] epoch 1756, training loss: 12618.68, average training loss: 13801.47, base loss: 25446.37
[INFO 2017-06-26 12:00:25,928 main.py:50] epoch 1757, training loss: 12463.45, average training loss: 13799.43, base loss: 25446.07
[INFO 2017-06-26 12:00:26,672 main.py:50] epoch 1758, training loss: 13121.83, average training loss: 13798.86, base loss: 25447.77
[INFO 2017-06-26 12:00:27,435 main.py:50] epoch 1759, training loss: 13743.58, average training loss: 13798.88, base loss: 25449.24
[INFO 2017-06-26 12:00:28,169 main.py:50] epoch 1760, training loss: 13175.13, average training loss: 13794.64, base loss: 25446.76
[INFO 2017-06-26 12:00:28,904 main.py:50] epoch 1761, training loss: 12400.79, average training loss: 13791.10, base loss: 25444.98
[INFO 2017-06-26 12:00:29,635 main.py:50] epoch 1762, training loss: 13901.74, average training loss: 13788.58, base loss: 25446.56
[INFO 2017-06-26 12:00:30,365 main.py:50] epoch 1763, training loss: 13927.59, average training loss: 13788.13, base loss: 25448.16
[INFO 2017-06-26 12:00:31,097 main.py:50] epoch 1764, training loss: 12112.07, average training loss: 13783.66, base loss: 25445.44
[INFO 2017-06-26 12:00:31,827 main.py:50] epoch 1765, training loss: 14296.17, average training loss: 13781.74, base loss: 25448.00
[INFO 2017-06-26 12:00:32,557 main.py:50] epoch 1766, training loss: 12482.70, average training loss: 13781.13, base loss: 25450.17
[INFO 2017-06-26 12:00:33,289 main.py:50] epoch 1767, training loss: 12728.23, average training loss: 13779.71, base loss: 25451.01
[INFO 2017-06-26 12:00:34,021 main.py:50] epoch 1768, training loss: 13899.29, average training loss: 13779.85, base loss: 25453.35
[INFO 2017-06-26 12:00:34,753 main.py:50] epoch 1769, training loss: 12351.77, average training loss: 13777.74, base loss: 25452.51
[INFO 2017-06-26 12:00:35,506 main.py:50] epoch 1770, training loss: 14699.99, average training loss: 13776.08, base loss: 25455.23
[INFO 2017-06-26 12:00:36,247 main.py:50] epoch 1771, training loss: 13901.46, average training loss: 13772.81, base loss: 25453.83
[INFO 2017-06-26 12:00:36,981 main.py:50] epoch 1772, training loss: 11849.54, average training loss: 13771.04, base loss: 25450.84
[INFO 2017-06-26 12:00:37,714 main.py:50] epoch 1773, training loss: 14131.96, average training loss: 13770.77, base loss: 25453.12
[INFO 2017-06-26 12:00:38,448 main.py:50] epoch 1774, training loss: 12205.37, average training loss: 13770.06, base loss: 25455.40
[INFO 2017-06-26 12:00:39,182 main.py:50] epoch 1775, training loss: 12407.43, average training loss: 13768.59, base loss: 25454.85
[INFO 2017-06-26 12:00:39,918 main.py:50] epoch 1776, training loss: 12197.10, average training loss: 13766.84, base loss: 25453.69
[INFO 2017-06-26 12:00:40,652 main.py:50] epoch 1777, training loss: 14227.91, average training loss: 13764.65, base loss: 25453.95
[INFO 2017-06-26 12:00:41,388 main.py:50] epoch 1778, training loss: 14018.34, average training loss: 13765.06, base loss: 25457.90
[INFO 2017-06-26 12:00:42,122 main.py:50] epoch 1779, training loss: 14047.19, average training loss: 13765.53, base loss: 25460.14
[INFO 2017-06-26 12:00:42,856 main.py:50] epoch 1780, training loss: 12462.21, average training loss: 13764.99, base loss: 25462.69
[INFO 2017-06-26 12:00:43,591 main.py:50] epoch 1781, training loss: 14778.91, average training loss: 13762.99, base loss: 25464.70
[INFO 2017-06-26 12:00:44,327 main.py:50] epoch 1782, training loss: 13589.06, average training loss: 13758.63, base loss: 25459.78
[INFO 2017-06-26 12:00:45,064 main.py:50] epoch 1783, training loss: 12300.10, average training loss: 13757.16, base loss: 25458.37
[INFO 2017-06-26 12:00:45,830 main.py:50] epoch 1784, training loss: 13750.94, average training loss: 13756.75, base loss: 25461.01
[INFO 2017-06-26 12:00:46,573 main.py:50] epoch 1785, training loss: 12761.24, average training loss: 13753.16, base loss: 25459.04
[INFO 2017-06-26 12:00:47,306 main.py:50] epoch 1786, training loss: 12587.04, average training loss: 13752.00, base loss: 25460.55
[INFO 2017-06-26 12:00:48,037 main.py:50] epoch 1787, training loss: 12598.71, average training loss: 13750.83, base loss: 25461.58
[INFO 2017-06-26 12:00:48,772 main.py:50] epoch 1788, training loss: 12157.30, average training loss: 13749.26, base loss: 25462.44
[INFO 2017-06-26 12:00:49,505 main.py:50] epoch 1789, training loss: 15352.87, average training loss: 13750.12, base loss: 25468.77
[INFO 2017-06-26 12:00:50,238 main.py:50] epoch 1790, training loss: 12486.71, average training loss: 13748.94, base loss: 25468.41
[INFO 2017-06-26 12:00:50,971 main.py:50] epoch 1791, training loss: 13906.79, average training loss: 13746.22, base loss: 25468.41
[INFO 2017-06-26 12:00:51,705 main.py:50] epoch 1792, training loss: 12837.47, average training loss: 13745.87, base loss: 25471.43
[INFO 2017-06-26 12:00:52,438 main.py:50] epoch 1793, training loss: 14082.15, average training loss: 13745.50, base loss: 25474.04
[INFO 2017-06-26 12:00:53,173 main.py:50] epoch 1794, training loss: 12456.06, average training loss: 13745.12, base loss: 25477.49
[INFO 2017-06-26 12:00:53,938 main.py:50] epoch 1795, training loss: 13002.06, average training loss: 13743.57, base loss: 25476.66
[INFO 2017-06-26 12:00:54,675 main.py:50] epoch 1796, training loss: 14251.34, average training loss: 13740.81, base loss: 25476.28
[INFO 2017-06-26 12:00:55,408 main.py:50] epoch 1797, training loss: 13634.66, average training loss: 13737.64, base loss: 25473.65
[INFO 2017-06-26 12:00:56,142 main.py:50] epoch 1798, training loss: 11923.28, average training loss: 13735.91, base loss: 25472.97
[INFO 2017-06-26 12:00:56,909 main.py:50] epoch 1799, training loss: 13300.07, average training loss: 13731.81, base loss: 25469.37
[INFO 2017-06-26 12:00:57,647 main.py:50] epoch 1800, training loss: 13614.70, average training loss: 13728.12, base loss: 25466.36
[INFO 2017-06-26 12:00:58,380 main.py:50] epoch 1801, training loss: 13348.43, average training loss: 13727.99, base loss: 25468.92
[INFO 2017-06-26 12:00:59,129 main.py:50] epoch 1802, training loss: 14880.65, average training loss: 13726.37, base loss: 25471.70
[INFO 2017-06-26 12:00:59,862 main.py:50] epoch 1803, training loss: 12904.14, average training loss: 13723.36, base loss: 25470.42
[INFO 2017-06-26 12:01:00,598 main.py:50] epoch 1804, training loss: 13000.32, average training loss: 13722.79, base loss: 25471.27
[INFO 2017-06-26 12:01:01,363 main.py:50] epoch 1805, training loss: 12477.66, average training loss: 13721.29, base loss: 25471.12
[INFO 2017-06-26 12:01:02,099 main.py:50] epoch 1806, training loss: 13105.74, average training loss: 13720.63, base loss: 25473.64
[INFO 2017-06-26 12:01:02,863 main.py:50] epoch 1807, training loss: 12464.05, average training loss: 13718.96, base loss: 25473.22
[INFO 2017-06-26 12:01:03,628 main.py:50] epoch 1808, training loss: 11333.21, average training loss: 13714.55, base loss: 25467.59
[INFO 2017-06-26 12:01:04,363 main.py:50] epoch 1809, training loss: 14164.17, average training loss: 13713.36, base loss: 25467.24
[INFO 2017-06-26 12:01:05,097 main.py:50] epoch 1810, training loss: 12240.83, average training loss: 13708.42, base loss: 25461.34
[INFO 2017-06-26 12:01:05,832 main.py:50] epoch 1811, training loss: 12452.73, average training loss: 13706.75, base loss: 25459.13
[INFO 2017-06-26 12:01:06,566 main.py:50] epoch 1812, training loss: 12312.76, average training loss: 13705.59, base loss: 25459.71
[INFO 2017-06-26 12:01:07,301 main.py:50] epoch 1813, training loss: 13790.89, average training loss: 13702.48, base loss: 25457.91
[INFO 2017-06-26 12:01:08,036 main.py:50] epoch 1814, training loss: 12890.39, average training loss: 13701.35, base loss: 25458.39
[INFO 2017-06-26 12:01:08,770 main.py:50] epoch 1815, training loss: 12885.20, average training loss: 13700.23, base loss: 25458.01
[INFO 2017-06-26 12:01:09,504 main.py:50] epoch 1816, training loss: 13569.14, average training loss: 13699.58, base loss: 25458.59
[INFO 2017-06-26 12:01:10,239 main.py:50] epoch 1817, training loss: 12743.29, average training loss: 13695.20, base loss: 25454.86
[INFO 2017-06-26 12:01:10,973 main.py:50] epoch 1818, training loss: 12330.99, average training loss: 13693.38, base loss: 25453.40
[INFO 2017-06-26 12:01:11,709 main.py:50] epoch 1819, training loss: 13253.31, average training loss: 13692.47, base loss: 25453.41
[INFO 2017-06-26 12:01:12,443 main.py:50] epoch 1820, training loss: 14126.43, average training loss: 13692.85, base loss: 25457.78
[INFO 2017-06-26 12:01:13,178 main.py:50] epoch 1821, training loss: 14148.06, average training loss: 13690.78, base loss: 25459.19
[INFO 2017-06-26 12:01:13,912 main.py:50] epoch 1822, training loss: 12727.88, average training loss: 13689.38, base loss: 25458.58
[INFO 2017-06-26 12:01:14,647 main.py:50] epoch 1823, training loss: 12254.34, average training loss: 13687.54, base loss: 25455.94
[INFO 2017-06-26 12:01:15,381 main.py:50] epoch 1824, training loss: 14182.89, average training loss: 13685.81, base loss: 25457.55
[INFO 2017-06-26 12:01:16,116 main.py:50] epoch 1825, training loss: 13775.67, average training loss: 13683.15, base loss: 25457.98
[INFO 2017-06-26 12:01:16,852 main.py:50] epoch 1826, training loss: 14014.57, average training loss: 13683.55, base loss: 25463.49
[INFO 2017-06-26 12:01:17,587 main.py:50] epoch 1827, training loss: 12816.53, average training loss: 13679.39, base loss: 25461.85
[INFO 2017-06-26 12:01:18,325 main.py:50] epoch 1828, training loss: 12832.76, average training loss: 13679.34, base loss: 25464.40
[INFO 2017-06-26 12:01:19,061 main.py:50] epoch 1829, training loss: 12174.94, average training loss: 13674.53, base loss: 25458.02
[INFO 2017-06-26 12:01:19,797 main.py:50] epoch 1830, training loss: 13524.90, average training loss: 13673.22, base loss: 25457.24
[INFO 2017-06-26 12:01:20,566 main.py:50] epoch 1831, training loss: 12737.77, average training loss: 13669.36, base loss: 25454.57
[INFO 2017-06-26 12:01:21,303 main.py:50] epoch 1832, training loss: 12792.43, average training loss: 13667.36, base loss: 25455.63
[INFO 2017-06-26 12:01:22,039 main.py:50] epoch 1833, training loss: 14368.77, average training loss: 13668.04, base loss: 25458.54
[INFO 2017-06-26 12:01:22,777 main.py:50] epoch 1834, training loss: 13288.24, average training loss: 13667.59, base loss: 25460.71
[INFO 2017-06-26 12:01:23,511 main.py:50] epoch 1835, training loss: 14469.05, average training loss: 13668.09, base loss: 25465.74
[INFO 2017-06-26 12:01:24,247 main.py:50] epoch 1836, training loss: 11963.98, average training loss: 13666.11, base loss: 25462.72
[INFO 2017-06-26 12:01:24,981 main.py:50] epoch 1837, training loss: 12896.64, average training loss: 13664.99, base loss: 25463.03
[INFO 2017-06-26 12:01:25,715 main.py:50] epoch 1838, training loss: 13516.18, average training loss: 13665.21, base loss: 25466.72
[INFO 2017-06-26 12:01:26,451 main.py:50] epoch 1839, training loss: 12154.35, average training loss: 13661.01, base loss: 25463.46
[INFO 2017-06-26 12:01:27,187 main.py:50] epoch 1840, training loss: 13816.75, average training loss: 13657.70, base loss: 25462.22
[INFO 2017-06-26 12:01:27,921 main.py:50] epoch 1841, training loss: 12690.38, average training loss: 13656.43, base loss: 25461.54
[INFO 2017-06-26 12:01:28,657 main.py:50] epoch 1842, training loss: 12079.57, average training loss: 13654.94, base loss: 25462.41
[INFO 2017-06-26 12:01:29,394 main.py:50] epoch 1843, training loss: 12652.13, average training loss: 13654.10, base loss: 25464.74
[INFO 2017-06-26 12:01:30,128 main.py:50] epoch 1844, training loss: 13655.27, average training loss: 13651.07, base loss: 25464.26
[INFO 2017-06-26 12:01:30,864 main.py:50] epoch 1845, training loss: 12822.69, average training loss: 13650.89, base loss: 25467.02
[INFO 2017-06-26 12:01:31,612 main.py:50] epoch 1846, training loss: 12609.47, average training loss: 13650.11, base loss: 25467.95
[INFO 2017-06-26 12:01:32,348 main.py:50] epoch 1847, training loss: 11961.05, average training loss: 13647.97, base loss: 25465.40
[INFO 2017-06-26 12:01:33,084 main.py:50] epoch 1848, training loss: 13263.04, average training loss: 13646.56, base loss: 25464.71
[INFO 2017-06-26 12:01:33,819 main.py:50] epoch 1849, training loss: 12196.80, average training loss: 13642.51, base loss: 25461.40
[INFO 2017-06-26 12:01:34,555 main.py:50] epoch 1850, training loss: 12236.91, average training loss: 13640.95, base loss: 25460.88
[INFO 2017-06-26 12:01:35,290 main.py:50] epoch 1851, training loss: 13599.84, average training loss: 13640.14, base loss: 25462.98
[INFO 2017-06-26 12:01:36,026 main.py:50] epoch 1852, training loss: 12960.80, average training loss: 13638.76, base loss: 25462.91
[INFO 2017-06-26 12:01:36,761 main.py:50] epoch 1853, training loss: 13913.91, average training loss: 13635.90, base loss: 25462.63
[INFO 2017-06-26 12:01:37,495 main.py:50] epoch 1854, training loss: 14452.33, average training loss: 13636.34, base loss: 25466.14
[INFO 2017-06-26 12:01:38,228 main.py:50] epoch 1855, training loss: 12462.12, average training loss: 13632.50, base loss: 25463.57
[INFO 2017-06-26 12:01:38,961 main.py:50] epoch 1856, training loss: 13163.09, average training loss: 13631.98, base loss: 25464.85
[INFO 2017-06-26 12:01:39,695 main.py:50] epoch 1857, training loss: 13736.91, average training loss: 13629.26, base loss: 25464.52
[INFO 2017-06-26 12:01:40,428 main.py:50] epoch 1858, training loss: 13184.54, average training loss: 13629.30, base loss: 25466.68
[INFO 2017-06-26 12:01:41,161 main.py:50] epoch 1859, training loss: 14404.98, average training loss: 13630.70, base loss: 25472.67
[INFO 2017-06-26 12:01:41,895 main.py:50] epoch 1860, training loss: 12117.07, average training loss: 13629.64, base loss: 25473.37
[INFO 2017-06-26 12:01:42,628 main.py:50] epoch 1861, training loss: 12940.44, average training loss: 13629.24, base loss: 25476.70
[INFO 2017-06-26 12:01:43,361 main.py:50] epoch 1862, training loss: 13312.78, average training loss: 13626.19, base loss: 25474.61
[INFO 2017-06-26 12:01:44,095 main.py:50] epoch 1863, training loss: 12679.84, average training loss: 13626.13, base loss: 25478.16
[INFO 2017-06-26 12:01:44,828 main.py:50] epoch 1864, training loss: 14219.51, average training loss: 13624.33, base loss: 25479.13
[INFO 2017-06-26 12:01:45,560 main.py:50] epoch 1865, training loss: 14117.39, average training loss: 13624.57, base loss: 25481.58
[INFO 2017-06-26 12:01:46,293 main.py:50] epoch 1866, training loss: 12572.04, average training loss: 13620.70, base loss: 25476.73
[INFO 2017-06-26 12:01:47,026 main.py:50] epoch 1867, training loss: 12337.92, average training loss: 13617.12, base loss: 25474.10
[INFO 2017-06-26 12:01:47,760 main.py:50] epoch 1868, training loss: 14239.56, average training loss: 13618.89, base loss: 25480.73
[INFO 2017-06-26 12:01:48,495 main.py:50] epoch 1869, training loss: 12269.24, average training loss: 13615.78, base loss: 25480.34
[INFO 2017-06-26 12:01:49,228 main.py:50] epoch 1870, training loss: 12632.75, average training loss: 13611.44, base loss: 25476.24
[INFO 2017-06-26 12:01:49,961 main.py:50] epoch 1871, training loss: 12188.76, average training loss: 13607.02, base loss: 25471.87
[INFO 2017-06-26 12:01:50,696 main.py:50] epoch 1872, training loss: 11873.50, average training loss: 13605.54, base loss: 25470.62
[INFO 2017-06-26 12:01:51,429 main.py:50] epoch 1873, training loss: 12479.63, average training loss: 13602.67, base loss: 25469.78
[INFO 2017-06-26 12:01:52,163 main.py:50] epoch 1874, training loss: 12389.66, average training loss: 13599.36, base loss: 25468.15
[INFO 2017-06-26 12:01:52,897 main.py:50] epoch 1875, training loss: 12075.68, average training loss: 13595.21, base loss: 25465.01
[INFO 2017-06-26 12:01:53,665 main.py:50] epoch 1876, training loss: 12576.82, average training loss: 13591.48, base loss: 25462.69
[INFO 2017-06-26 12:01:54,416 main.py:50] epoch 1877, training loss: 12438.81, average training loss: 13588.59, base loss: 25461.67
[INFO 2017-06-26 12:01:55,171 main.py:50] epoch 1878, training loss: 12764.22, average training loss: 13586.74, base loss: 25462.45
[INFO 2017-06-26 12:01:55,903 main.py:50] epoch 1879, training loss: 14053.60, average training loss: 13586.37, base loss: 25463.56
[INFO 2017-06-26 12:01:56,635 main.py:50] epoch 1880, training loss: 13174.85, average training loss: 13585.53, base loss: 25466.11
[INFO 2017-06-26 12:01:57,366 main.py:50] epoch 1881, training loss: 12470.09, average training loss: 13580.57, base loss: 25460.67
[INFO 2017-06-26 12:01:58,099 main.py:50] epoch 1882, training loss: 12197.82, average training loss: 13578.16, base loss: 25457.37
[INFO 2017-06-26 12:01:58,833 main.py:50] epoch 1883, training loss: 11953.29, average training loss: 13576.25, base loss: 25455.57
[INFO 2017-06-26 12:01:59,566 main.py:50] epoch 1884, training loss: 14686.51, average training loss: 13576.82, base loss: 25460.10
[INFO 2017-06-26 12:02:00,300 main.py:50] epoch 1885, training loss: 12798.70, average training loss: 13575.62, base loss: 25461.25
[INFO 2017-06-26 12:02:01,034 main.py:50] epoch 1886, training loss: 13118.23, average training loss: 13572.81, base loss: 25460.91
[INFO 2017-06-26 12:02:01,767 main.py:50] epoch 1887, training loss: 12266.94, average training loss: 13568.03, base loss: 25454.81
[INFO 2017-06-26 12:02:02,500 main.py:50] epoch 1888, training loss: 14162.83, average training loss: 13568.21, base loss: 25457.48
[INFO 2017-06-26 12:02:03,234 main.py:50] epoch 1889, training loss: 12577.59, average training loss: 13567.15, base loss: 25459.50
[INFO 2017-06-26 12:02:03,982 main.py:50] epoch 1890, training loss: 11740.15, average training loss: 13565.81, base loss: 25458.73
[INFO 2017-06-26 12:02:04,714 main.py:50] epoch 1891, training loss: 14285.00, average training loss: 13563.66, base loss: 25459.60
[INFO 2017-06-26 12:02:05,447 main.py:50] epoch 1892, training loss: 12584.69, average training loss: 13561.72, base loss: 25459.66
[INFO 2017-06-26 12:02:06,180 main.py:50] epoch 1893, training loss: 13422.95, average training loss: 13561.53, base loss: 25462.70
[INFO 2017-06-26 12:02:06,913 main.py:50] epoch 1894, training loss: 12252.16, average training loss: 13557.99, base loss: 25460.79
[INFO 2017-06-26 12:02:07,646 main.py:50] epoch 1895, training loss: 14653.33, average training loss: 13558.86, base loss: 25465.37
[INFO 2017-06-26 12:02:08,381 main.py:50] epoch 1896, training loss: 12036.96, average training loss: 13556.78, base loss: 25463.15
[INFO 2017-06-26 12:02:09,149 main.py:50] epoch 1897, training loss: 13418.79, average training loss: 13557.33, base loss: 25467.83
[INFO 2017-06-26 12:02:09,887 main.py:50] epoch 1898, training loss: 14071.18, average training loss: 13554.51, base loss: 25467.03
[INFO 2017-06-26 12:02:10,618 main.py:50] epoch 1899, training loss: 12628.32, average training loss: 13550.47, base loss: 25463.67
[INFO 2017-06-26 12:02:11,351 main.py:50] epoch 1900, training loss: 13516.82, average training loss: 13548.65, base loss: 25464.55
[INFO 2017-06-26 12:02:12,083 main.py:50] epoch 1901, training loss: 12560.41, average training loss: 13547.33, base loss: 25464.18
[INFO 2017-06-26 12:02:12,816 main.py:50] epoch 1902, training loss: 13072.11, average training loss: 13545.13, base loss: 25462.60
[INFO 2017-06-26 12:02:13,548 main.py:50] epoch 1903, training loss: 12853.36, average training loss: 13542.40, base loss: 25462.44
[INFO 2017-06-26 12:02:14,280 main.py:50] epoch 1904, training loss: 14261.67, average training loss: 13543.01, base loss: 25466.58
[INFO 2017-06-26 12:02:15,011 main.py:50] epoch 1905, training loss: 13675.52, average training loss: 13538.59, base loss: 25462.23
[INFO 2017-06-26 12:02:15,742 main.py:50] epoch 1906, training loss: 13937.17, average training loss: 13538.61, base loss: 25465.60
[INFO 2017-06-26 12:02:16,472 main.py:50] epoch 1907, training loss: 12414.27, average training loss: 13537.48, base loss: 25467.39
[INFO 2017-06-26 12:02:17,204 main.py:50] epoch 1908, training loss: 12326.73, average training loss: 13535.35, base loss: 25466.58
[INFO 2017-06-26 12:02:17,934 main.py:50] epoch 1909, training loss: 12869.48, average training loss: 13534.27, base loss: 25467.19
[INFO 2017-06-26 12:02:18,665 main.py:50] epoch 1910, training loss: 12389.31, average training loss: 13530.37, base loss: 25464.16
[INFO 2017-06-26 12:02:19,398 main.py:50] epoch 1911, training loss: 12943.19, average training loss: 13529.16, base loss: 25465.94
[INFO 2017-06-26 12:02:20,130 main.py:50] epoch 1912, training loss: 12380.19, average training loss: 13528.06, base loss: 25467.61
[INFO 2017-06-26 12:02:20,890 main.py:50] epoch 1913, training loss: 12187.98, average training loss: 13526.57, base loss: 25466.65
[INFO 2017-06-26 12:02:21,641 main.py:50] epoch 1914, training loss: 13846.90, average training loss: 13526.25, base loss: 25468.86
[INFO 2017-06-26 12:02:22,377 main.py:50] epoch 1915, training loss: 12037.50, average training loss: 13522.10, base loss: 25466.27
[INFO 2017-06-26 12:02:23,110 main.py:50] epoch 1916, training loss: 14401.77, average training loss: 13522.88, base loss: 25473.02
[INFO 2017-06-26 12:02:23,842 main.py:50] epoch 1917, training loss: 12544.84, average training loss: 13520.41, base loss: 25471.23
[INFO 2017-06-26 12:02:24,572 main.py:50] epoch 1918, training loss: 12941.49, average training loss: 13519.13, base loss: 25472.27
[INFO 2017-06-26 12:02:25,302 main.py:50] epoch 1919, training loss: 12140.41, average training loss: 13514.06, base loss: 25467.28
[INFO 2017-06-26 12:02:26,033 main.py:50] epoch 1920, training loss: 12094.29, average training loss: 13510.26, base loss: 25464.54
[INFO 2017-06-26 12:02:26,762 main.py:50] epoch 1921, training loss: 14145.52, average training loss: 13509.87, base loss: 25465.49
[INFO 2017-06-26 12:02:27,493 main.py:50] epoch 1922, training loss: 12940.87, average training loss: 13505.28, base loss: 25462.63
[INFO 2017-06-26 12:02:28,224 main.py:50] epoch 1923, training loss: 13236.09, average training loss: 13502.52, base loss: 25463.00
[INFO 2017-06-26 12:02:28,987 main.py:50] epoch 1924, training loss: 12849.57, average training loss: 13501.23, base loss: 25462.43
[INFO 2017-06-26 12:02:29,755 main.py:50] epoch 1925, training loss: 14439.64, average training loss: 13501.38, base loss: 25465.44
[INFO 2017-06-26 12:02:30,487 main.py:50] epoch 1926, training loss: 12768.01, average training loss: 13497.75, base loss: 25461.95
[INFO 2017-06-26 12:02:31,218 main.py:50] epoch 1927, training loss: 12804.68, average training loss: 13495.56, base loss: 25461.71
[INFO 2017-06-26 12:02:31,949 main.py:50] epoch 1928, training loss: 13106.24, average training loss: 13492.75, base loss: 25460.46
[INFO 2017-06-26 12:02:32,680 main.py:50] epoch 1929, training loss: 12347.14, average training loss: 13491.62, base loss: 25461.25
[INFO 2017-06-26 12:02:33,414 main.py:50] epoch 1930, training loss: 13164.46, average training loss: 13490.69, base loss: 25463.13
[INFO 2017-06-26 12:02:34,148 main.py:50] epoch 1931, training loss: 12496.12, average training loss: 13489.51, base loss: 25464.30
[INFO 2017-06-26 12:02:34,883 main.py:50] epoch 1932, training loss: 13048.44, average training loss: 13489.34, base loss: 25468.21
[INFO 2017-06-26 12:02:35,618 main.py:50] epoch 1933, training loss: 11878.82, average training loss: 13485.19, base loss: 25463.29
[INFO 2017-06-26 12:02:36,367 main.py:50] epoch 1934, training loss: 12745.30, average training loss: 13481.93, base loss: 25461.67
[INFO 2017-06-26 12:02:37,101 main.py:50] epoch 1935, training loss: 12376.67, average training loss: 13480.71, base loss: 25462.56
[INFO 2017-06-26 12:02:37,836 main.py:50] epoch 1936, training loss: 12933.89, average training loss: 13479.86, base loss: 25463.00
[INFO 2017-06-26 12:02:38,571 main.py:50] epoch 1937, training loss: 13115.28, average training loss: 13477.90, base loss: 25463.57
[INFO 2017-06-26 12:02:39,307 main.py:50] epoch 1938, training loss: 14311.16, average training loss: 13476.07, base loss: 25466.14
[INFO 2017-06-26 12:02:40,040 main.py:50] epoch 1939, training loss: 12235.93, average training loss: 13472.45, base loss: 25461.36
[INFO 2017-06-26 12:02:40,775 main.py:50] epoch 1940, training loss: 11990.87, average training loss: 13470.43, base loss: 25458.90
[INFO 2017-06-26 12:02:41,511 main.py:50] epoch 1941, training loss: 12663.48, average training loss: 13469.14, base loss: 25458.58
[INFO 2017-06-26 12:02:42,246 main.py:50] epoch 1942, training loss: 11993.47, average training loss: 13466.98, base loss: 25455.86
[INFO 2017-06-26 12:02:42,980 main.py:50] epoch 1943, training loss: 12452.99, average training loss: 13465.97, base loss: 25455.67
[INFO 2017-06-26 12:02:43,716 main.py:50] epoch 1944, training loss: 14069.80, average training loss: 13464.41, base loss: 25456.95
[INFO 2017-06-26 12:02:44,451 main.py:50] epoch 1945, training loss: 13765.27, average training loss: 13464.68, base loss: 25460.54
[INFO 2017-06-26 12:02:45,186 main.py:50] epoch 1946, training loss: 13972.81, average training loss: 13464.70, base loss: 25462.51
[INFO 2017-06-26 12:02:45,922 main.py:50] epoch 1947, training loss: 12219.24, average training loss: 13463.71, base loss: 25464.60
[INFO 2017-06-26 12:02:46,656 main.py:50] epoch 1948, training loss: 13448.65, average training loss: 13461.91, base loss: 25466.27
[INFO 2017-06-26 12:02:47,391 main.py:50] epoch 1949, training loss: 12180.43, average training loss: 13459.58, base loss: 25464.51
[INFO 2017-06-26 12:02:48,125 main.py:50] epoch 1950, training loss: 12747.55, average training loss: 13458.47, base loss: 25465.91
[INFO 2017-06-26 12:02:48,893 main.py:50] epoch 1951, training loss: 12568.21, average training loss: 13456.98, base loss: 25465.00
[INFO 2017-06-26 12:02:49,693 main.py:50] epoch 1952, training loss: 12436.89, average training loss: 13456.23, base loss: 25466.68
[INFO 2017-06-26 12:02:50,429 main.py:50] epoch 1953, training loss: 12485.09, average training loss: 13453.35, base loss: 25465.18
[INFO 2017-06-26 12:02:51,164 main.py:50] epoch 1954, training loss: 11395.86, average training loss: 13450.98, base loss: 25462.34
[INFO 2017-06-26 12:02:51,899 main.py:50] epoch 1955, training loss: 13435.26, average training loss: 13450.59, base loss: 25462.80
[INFO 2017-06-26 12:02:52,634 main.py:50] epoch 1956, training loss: 12328.55, average training loss: 13449.43, base loss: 25462.19
[INFO 2017-06-26 12:02:53,368 main.py:50] epoch 1957, training loss: 12057.45, average training loss: 13447.86, base loss: 25460.19
[INFO 2017-06-26 12:02:54,101 main.py:50] epoch 1958, training loss: 13176.09, average training loss: 13444.84, base loss: 25457.36
[INFO 2017-06-26 12:02:54,834 main.py:50] epoch 1959, training loss: 12492.56, average training loss: 13441.75, base loss: 25455.07
[INFO 2017-06-26 12:02:55,567 main.py:50] epoch 1960, training loss: 11862.42, average training loss: 13440.12, base loss: 25452.57
[INFO 2017-06-26 12:02:56,300 main.py:50] epoch 1961, training loss: 14583.92, average training loss: 13440.97, base loss: 25456.43
[INFO 2017-06-26 12:02:57,033 main.py:50] epoch 1962, training loss: 12631.69, average training loss: 13439.26, base loss: 25454.01
[INFO 2017-06-26 12:02:57,766 main.py:50] epoch 1963, training loss: 12461.77, average training loss: 13438.05, base loss: 25452.86
[INFO 2017-06-26 12:02:58,500 main.py:50] epoch 1964, training loss: 11247.11, average training loss: 13433.15, base loss: 25444.34
[INFO 2017-06-26 12:02:59,234 main.py:50] epoch 1965, training loss: 13035.39, average training loss: 13433.07, base loss: 25447.37
[INFO 2017-06-26 12:02:59,967 main.py:50] epoch 1966, training loss: 13474.86, average training loss: 13430.32, base loss: 25444.58
[INFO 2017-06-26 12:03:00,700 main.py:50] epoch 1967, training loss: 13155.00, average training loss: 13430.25, base loss: 25445.91
[INFO 2017-06-26 12:03:01,434 main.py:50] epoch 1968, training loss: 13376.89, average training loss: 13429.99, base loss: 25447.48
[INFO 2017-06-26 12:03:02,167 main.py:50] epoch 1969, training loss: 12635.63, average training loss: 13429.41, base loss: 25449.41
[INFO 2017-06-26 12:03:02,900 main.py:50] epoch 1970, training loss: 13420.96, average training loss: 13428.59, base loss: 25448.79
[INFO 2017-06-26 12:03:03,634 main.py:50] epoch 1971, training loss: 12758.65, average training loss: 13428.04, base loss: 25448.93
[INFO 2017-06-26 12:03:04,367 main.py:50] epoch 1972, training loss: 13828.35, average training loss: 13425.96, base loss: 25448.24
[INFO 2017-06-26 12:03:05,099 main.py:50] epoch 1973, training loss: 12725.82, average training loss: 13425.13, base loss: 25448.35
[INFO 2017-06-26 12:03:05,833 main.py:50] epoch 1974, training loss: 13731.73, average training loss: 13425.39, base loss: 25451.05
[INFO 2017-06-26 12:03:06,567 main.py:50] epoch 1975, training loss: 14152.64, average training loss: 13426.75, base loss: 25457.29
[INFO 2017-06-26 12:03:07,300 main.py:50] epoch 1976, training loss: 12145.87, average training loss: 13424.97, base loss: 25454.69
[INFO 2017-06-26 12:03:08,035 main.py:50] epoch 1977, training loss: 13752.96, average training loss: 13425.44, base loss: 25457.83
[INFO 2017-06-26 12:03:08,780 main.py:50] epoch 1978, training loss: 13862.90, average training loss: 13424.21, base loss: 25458.84
[INFO 2017-06-26 12:03:09,514 main.py:50] epoch 1979, training loss: 12672.90, average training loss: 13423.76, base loss: 25460.83
[INFO 2017-06-26 12:03:10,248 main.py:50] epoch 1980, training loss: 13445.77, average training loss: 13420.57, base loss: 25457.98
[INFO 2017-06-26 12:03:10,982 main.py:50] epoch 1981, training loss: 11544.62, average training loss: 13418.81, base loss: 25457.83
[INFO 2017-06-26 12:03:11,716 main.py:50] epoch 1982, training loss: 12979.89, average training loss: 13419.05, base loss: 25461.77
[INFO 2017-06-26 12:03:12,450 main.py:50] epoch 1983, training loss: 11601.65, average training loss: 13417.61, base loss: 25460.86
[INFO 2017-06-26 12:03:13,183 main.py:50] epoch 1984, training loss: 12636.92, average training loss: 13416.80, base loss: 25461.43
[INFO 2017-06-26 12:03:13,915 main.py:50] epoch 1985, training loss: 13417.53, average training loss: 13416.56, base loss: 25462.07
[INFO 2017-06-26 12:03:14,649 main.py:50] epoch 1986, training loss: 12752.94, average training loss: 13415.67, base loss: 25461.73
[INFO 2017-06-26 12:03:15,382 main.py:50] epoch 1987, training loss: 12553.75, average training loss: 13413.58, base loss: 25461.91
[INFO 2017-06-26 12:03:16,115 main.py:50] epoch 1988, training loss: 12634.77, average training loss: 13411.19, base loss: 25461.18
[INFO 2017-06-26 12:03:16,848 main.py:50] epoch 1989, training loss: 12384.22, average training loss: 13407.46, base loss: 25457.69
[INFO 2017-06-26 12:03:17,581 main.py:50] epoch 1990, training loss: 13174.90, average training loss: 13406.66, base loss: 25459.35
[INFO 2017-06-26 12:03:18,315 main.py:50] epoch 1991, training loss: 12074.07, average training loss: 13405.28, base loss: 25459.35
[INFO 2017-06-26 12:03:19,049 main.py:50] epoch 1992, training loss: 11925.56, average training loss: 13403.26, base loss: 25457.62
[INFO 2017-06-26 12:03:19,783 main.py:50] epoch 1993, training loss: 12945.41, average training loss: 13400.17, base loss: 25454.58
[INFO 2017-06-26 12:03:20,516 main.py:50] epoch 1994, training loss: 11995.92, average training loss: 13398.31, base loss: 25453.64
[INFO 2017-06-26 12:03:21,249 main.py:50] epoch 1995, training loss: 13633.88, average training loss: 13397.97, base loss: 25454.47
[INFO 2017-06-26 12:03:21,983 main.py:50] epoch 1996, training loss: 11259.28, average training loss: 13395.42, base loss: 25450.08
[INFO 2017-06-26 12:03:22,715 main.py:50] epoch 1997, training loss: 11725.55, average training loss: 13391.57, base loss: 25443.94
[INFO 2017-06-26 12:03:23,448 main.py:50] epoch 1998, training loss: 13801.16, average training loss: 13391.97, base loss: 25446.67
[INFO 2017-06-26 12:03:24,181 main.py:50] epoch 1999, training loss: 13241.82, average training loss: 13392.42, base loss: 25450.24
[INFO 2017-06-26 12:03:24,181 main.py:52] epoch 1999, testing
[INFO 2017-06-26 12:03:51,708 main.py:103] average testing loss: 12810.83, base loss: 25296.13
[INFO 2017-06-26 12:03:51,708 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 12:03:51,715 main.py:76] current best accuracy: 12810.83
[INFO 2017-06-26 12:03:52,445 main.py:50] epoch 2000, training loss: 12816.77, average training loss: 13390.73, base loss: 25449.05
[INFO 2017-06-26 12:03:53,179 main.py:50] epoch 2001, training loss: 14203.16, average training loss: 13392.03, base loss: 25455.35
[INFO 2017-06-26 12:03:53,914 main.py:50] epoch 2002, training loss: 13931.34, average training loss: 13390.07, base loss: 25455.07
[INFO 2017-06-26 12:03:54,647 main.py:50] epoch 2003, training loss: 12509.03, average training loss: 13389.11, base loss: 25455.24
[INFO 2017-06-26 12:03:55,381 main.py:50] epoch 2004, training loss: 14018.79, average training loss: 13390.06, base loss: 25459.85
[INFO 2017-06-26 12:03:56,118 main.py:50] epoch 2005, training loss: 12412.20, average training loss: 13388.38, base loss: 25457.98
[INFO 2017-06-26 12:03:56,853 main.py:50] epoch 2006, training loss: 12423.94, average training loss: 13386.96, base loss: 25457.82
[INFO 2017-06-26 12:03:57,588 main.py:50] epoch 2007, training loss: 12176.65, average training loss: 13385.62, base loss: 25457.88
[INFO 2017-06-26 12:03:58,339 main.py:50] epoch 2008, training loss: 12567.29, average training loss: 13385.07, base loss: 25460.43
[INFO 2017-06-26 12:03:59,122 main.py:50] epoch 2009, training loss: 12919.11, average training loss: 13382.12, base loss: 25456.35
[INFO 2017-06-26 12:03:59,859 main.py:50] epoch 2010, training loss: 14588.03, average training loss: 13381.08, base loss: 25458.79
[INFO 2017-06-26 12:04:00,678 main.py:50] epoch 2011, training loss: 13293.23, average training loss: 13380.51, base loss: 25458.61
[INFO 2017-06-26 12:04:01,468 main.py:50] epoch 2012, training loss: 12396.41, average training loss: 13379.61, base loss: 25460.24
[INFO 2017-06-26 12:04:02,235 main.py:50] epoch 2013, training loss: 11783.55, average training loss: 13378.07, base loss: 25459.63
[INFO 2017-06-26 12:04:02,973 main.py:50] epoch 2014, training loss: 13552.66, average training loss: 13375.40, base loss: 25458.70
[INFO 2017-06-26 12:04:03,705 main.py:50] epoch 2015, training loss: 12381.25, average training loss: 13374.87, base loss: 25459.47
[INFO 2017-06-26 12:04:04,439 main.py:50] epoch 2016, training loss: 12442.76, average training loss: 13372.11, base loss: 25455.09
[INFO 2017-06-26 12:04:05,188 main.py:50] epoch 2017, training loss: 14222.79, average training loss: 13372.45, base loss: 25458.86
[INFO 2017-06-26 12:04:05,920 main.py:50] epoch 2018, training loss: 14123.09, average training loss: 13370.70, base loss: 25458.19
[INFO 2017-06-26 12:04:06,651 main.py:50] epoch 2019, training loss: 12101.41, average training loss: 13369.01, base loss: 25457.92
[INFO 2017-06-26 12:04:07,386 main.py:50] epoch 2020, training loss: 12272.31, average training loss: 13367.48, base loss: 25454.97
[INFO 2017-06-26 12:04:08,121 main.py:50] epoch 2021, training loss: 12288.14, average training loss: 13364.37, base loss: 25452.07
[INFO 2017-06-26 12:04:08,890 main.py:50] epoch 2022, training loss: 12123.13, average training loss: 13363.15, base loss: 25451.73
[INFO 2017-06-26 12:04:09,655 main.py:50] epoch 2023, training loss: 12050.34, average training loss: 13360.95, base loss: 25448.81
[INFO 2017-06-26 12:04:10,392 main.py:50] epoch 2024, training loss: 13480.68, average training loss: 13361.15, base loss: 25451.33
[INFO 2017-06-26 12:04:11,126 main.py:50] epoch 2025, training loss: 12195.20, average training loss: 13357.15, base loss: 25446.40
[INFO 2017-06-26 12:04:11,858 main.py:50] epoch 2026, training loss: 13704.65, average training loss: 13356.35, base loss: 25448.56
[INFO 2017-06-26 12:04:12,591 main.py:50] epoch 2027, training loss: 11939.35, average training loss: 13354.37, base loss: 25446.08
[INFO 2017-06-26 12:04:13,325 main.py:50] epoch 2028, training loss: 14148.17, average training loss: 13353.00, base loss: 25446.64
[INFO 2017-06-26 12:04:14,058 main.py:50] epoch 2029, training loss: 13440.88, average training loss: 13350.74, base loss: 25444.58
[INFO 2017-06-26 12:04:14,792 main.py:50] epoch 2030, training loss: 14268.34, average training loss: 13351.90, base loss: 25449.55
[INFO 2017-06-26 12:04:15,525 main.py:50] epoch 2031, training loss: 12167.50, average training loss: 13348.62, base loss: 25447.10
[INFO 2017-06-26 12:04:16,290 main.py:50] epoch 2032, training loss: 14713.88, average training loss: 13348.81, base loss: 25451.94
[INFO 2017-06-26 12:04:17,023 main.py:50] epoch 2033, training loss: 12486.84, average training loss: 13348.16, base loss: 25451.85
[INFO 2017-06-26 12:04:17,758 main.py:50] epoch 2034, training loss: 14479.22, average training loss: 13349.17, base loss: 25455.60
[INFO 2017-06-26 12:04:18,525 main.py:50] epoch 2035, training loss: 13373.54, average training loss: 13350.00, base loss: 25460.33
[INFO 2017-06-26 12:04:19,261 main.py:50] epoch 2036, training loss: 14042.27, average training loss: 13350.92, base loss: 25465.04
[INFO 2017-06-26 12:04:19,994 main.py:50] epoch 2037, training loss: 12520.73, average training loss: 13348.37, base loss: 25463.43
[INFO 2017-06-26 12:04:20,727 main.py:50] epoch 2038, training loss: 12364.68, average training loss: 13346.81, base loss: 25463.04
[INFO 2017-06-26 12:04:21,458 main.py:50] epoch 2039, training loss: 12220.17, average training loss: 13342.87, base loss: 25458.41
[INFO 2017-06-26 12:04:22,191 main.py:50] epoch 2040, training loss: 12677.90, average training loss: 13342.70, base loss: 25461.84
[INFO 2017-06-26 12:04:22,925 main.py:50] epoch 2041, training loss: 14362.29, average training loss: 13343.30, base loss: 25465.03
[INFO 2017-06-26 12:04:23,658 main.py:50] epoch 2042, training loss: 12901.38, average training loss: 13342.74, base loss: 25465.83
[INFO 2017-06-26 12:04:24,388 main.py:50] epoch 2043, training loss: 12092.85, average training loss: 13339.66, base loss: 25461.28
[INFO 2017-06-26 12:04:25,122 main.py:50] epoch 2044, training loss: 12808.11, average training loss: 13338.52, base loss: 25461.44
[INFO 2017-06-26 12:04:25,854 main.py:50] epoch 2045, training loss: 14731.44, average training loss: 13337.26, base loss: 25462.00
[INFO 2017-06-26 12:04:26,588 main.py:50] epoch 2046, training loss: 13882.48, average training loss: 13337.26, base loss: 25464.94
[INFO 2017-06-26 12:04:27,322 main.py:50] epoch 2047, training loss: 12441.93, average training loss: 13335.92, base loss: 25464.90
[INFO 2017-06-26 12:04:28,055 main.py:50] epoch 2048, training loss: 11939.19, average training loss: 13334.03, base loss: 25461.32
[INFO 2017-06-26 12:04:28,789 main.py:50] epoch 2049, training loss: 12359.28, average training loss: 13332.52, base loss: 25460.05
[INFO 2017-06-26 12:04:29,522 main.py:50] epoch 2050, training loss: 12933.88, average training loss: 13329.02, base loss: 25456.53
[INFO 2017-06-26 12:04:30,286 main.py:50] epoch 2051, training loss: 12592.04, average training loss: 13327.81, base loss: 25455.44
[INFO 2017-06-26 12:04:31,021 main.py:50] epoch 2052, training loss: 13797.65, average training loss: 13327.79, base loss: 25456.76
[INFO 2017-06-26 12:04:31,768 main.py:50] epoch 2053, training loss: 12636.82, average training loss: 13327.75, base loss: 25458.36
[INFO 2017-06-26 12:04:32,502 main.py:50] epoch 2054, training loss: 12706.03, average training loss: 13325.00, base loss: 25455.36
[INFO 2017-06-26 12:04:33,235 main.py:50] epoch 2055, training loss: 13747.70, average training loss: 13325.11, base loss: 25456.10
[INFO 2017-06-26 12:04:33,968 main.py:50] epoch 2056, training loss: 12758.66, average training loss: 13325.10, base loss: 25457.65
[INFO 2017-06-26 12:04:34,700 main.py:50] epoch 2057, training loss: 13665.69, average training loss: 13325.69, base loss: 25460.94
[INFO 2017-06-26 12:04:35,434 main.py:50] epoch 2058, training loss: 12685.19, average training loss: 13324.24, base loss: 25459.78
[INFO 2017-06-26 12:04:36,167 main.py:50] epoch 2059, training loss: 12699.48, average training loss: 13322.76, base loss: 25456.29
[INFO 2017-06-26 12:04:36,900 main.py:50] epoch 2060, training loss: 13152.11, average training loss: 13322.14, base loss: 25457.64
[INFO 2017-06-26 12:04:37,632 main.py:50] epoch 2061, training loss: 13107.74, average training loss: 13320.30, base loss: 25457.30
[INFO 2017-06-26 12:04:38,365 main.py:50] epoch 2062, training loss: 13542.94, average training loss: 13318.27, base loss: 25454.99
[INFO 2017-06-26 12:04:39,099 main.py:50] epoch 2063, training loss: 12893.01, average training loss: 13315.86, base loss: 25451.96
[INFO 2017-06-26 12:04:39,833 main.py:50] epoch 2064, training loss: 13539.22, average training loss: 13316.06, base loss: 25451.37
[INFO 2017-06-26 12:04:40,565 main.py:50] epoch 2065, training loss: 13579.62, average training loss: 13316.51, base loss: 25454.97
[INFO 2017-06-26 12:04:41,299 main.py:50] epoch 2066, training loss: 12908.22, average training loss: 13313.89, base loss: 25452.77
[INFO 2017-06-26 12:04:42,033 main.py:50] epoch 2067, training loss: 11837.67, average training loss: 13312.50, base loss: 25449.67
[INFO 2017-06-26 12:04:42,766 main.py:50] epoch 2068, training loss: 12853.67, average training loss: 13310.22, base loss: 25447.47
[INFO 2017-06-26 12:04:43,501 main.py:50] epoch 2069, training loss: 11080.29, average training loss: 13308.35, base loss: 25445.24
[INFO 2017-06-26 12:04:44,235 main.py:50] epoch 2070, training loss: 12559.51, average training loss: 13306.08, base loss: 25444.65
[INFO 2017-06-26 12:04:44,969 main.py:50] epoch 2071, training loss: 13407.48, average training loss: 13303.62, base loss: 25441.73
[INFO 2017-06-26 12:04:45,703 main.py:50] epoch 2072, training loss: 11872.64, average training loss: 13299.92, base loss: 25434.13
[INFO 2017-06-26 12:04:46,438 main.py:50] epoch 2073, training loss: 13628.02, average training loss: 13300.82, base loss: 25439.38
[INFO 2017-06-26 12:04:47,172 main.py:50] epoch 2074, training loss: 12018.77, average training loss: 13300.22, base loss: 25441.03
[INFO 2017-06-26 12:04:47,905 main.py:50] epoch 2075, training loss: 12947.97, average training loss: 13299.15, base loss: 25441.07
[INFO 2017-06-26 12:04:48,639 main.py:50] epoch 2076, training loss: 13769.53, average training loss: 13299.73, base loss: 25443.20
[INFO 2017-06-26 12:04:49,372 main.py:50] epoch 2077, training loss: 14011.94, average training loss: 13299.11, base loss: 25446.25
[INFO 2017-06-26 12:04:50,106 main.py:50] epoch 2078, training loss: 12567.24, average training loss: 13298.66, base loss: 25447.89
[INFO 2017-06-26 12:04:50,841 main.py:50] epoch 2079, training loss: 12604.50, average training loss: 13298.63, base loss: 25449.14
[INFO 2017-06-26 12:04:51,576 main.py:50] epoch 2080, training loss: 11962.33, average training loss: 13295.54, base loss: 25444.76
[INFO 2017-06-26 12:04:52,311 main.py:50] epoch 2081, training loss: 12142.11, average training loss: 13295.65, base loss: 25447.10
[INFO 2017-06-26 12:04:53,046 main.py:50] epoch 2082, training loss: 11831.29, average training loss: 13294.82, base loss: 25446.14
[INFO 2017-06-26 12:04:53,779 main.py:50] epoch 2083, training loss: 12987.11, average training loss: 13294.45, base loss: 25446.94
[INFO 2017-06-26 12:04:54,514 main.py:50] epoch 2084, training loss: 14546.57, average training loss: 13293.38, base loss: 25447.96
[INFO 2017-06-26 12:04:55,249 main.py:50] epoch 2085, training loss: 11764.61, average training loss: 13291.94, base loss: 25443.95
[INFO 2017-06-26 12:04:55,983 main.py:50] epoch 2086, training loss: 12221.40, average training loss: 13291.31, base loss: 25445.75
[INFO 2017-06-26 12:04:56,718 main.py:50] epoch 2087, training loss: 12453.66, average training loss: 13290.42, base loss: 25446.91
[INFO 2017-06-26 12:04:57,453 main.py:50] epoch 2088, training loss: 13237.78, average training loss: 13289.87, base loss: 25447.00
[INFO 2017-06-26 12:04:58,188 main.py:50] epoch 2089, training loss: 12258.51, average training loss: 13287.05, base loss: 25443.77
[INFO 2017-06-26 12:04:58,927 main.py:50] epoch 2090, training loss: 12453.67, average training loss: 13286.25, base loss: 25444.01
[INFO 2017-06-26 12:04:59,664 main.py:50] epoch 2091, training loss: 13129.37, average training loss: 13286.17, base loss: 25446.67
[INFO 2017-06-26 12:05:00,401 main.py:50] epoch 2092, training loss: 12549.08, average training loss: 13284.72, base loss: 25446.38
[INFO 2017-06-26 12:05:01,172 main.py:50] epoch 2093, training loss: 14622.36, average training loss: 13286.18, base loss: 25453.46
[INFO 2017-06-26 12:05:01,940 main.py:50] epoch 2094, training loss: 11712.88, average training loss: 13284.98, base loss: 25453.20
[INFO 2017-06-26 12:05:02,684 main.py:50] epoch 2095, training loss: 12368.40, average training loss: 13283.33, base loss: 25452.22
[INFO 2017-06-26 12:05:03,415 main.py:50] epoch 2096, training loss: 14833.76, average training loss: 13284.69, base loss: 25458.03
[INFO 2017-06-26 12:05:04,160 main.py:50] epoch 2097, training loss: 13665.36, average training loss: 13282.78, base loss: 25456.96
[INFO 2017-06-26 12:05:04,894 main.py:50] epoch 2098, training loss: 12136.42, average training loss: 13281.92, base loss: 25456.08
[INFO 2017-06-26 12:05:05,626 main.py:50] epoch 2099, training loss: 12851.28, average training loss: 13281.29, base loss: 25457.41
[INFO 2017-06-26 12:05:06,359 main.py:50] epoch 2100, training loss: 13473.00, average training loss: 13281.94, base loss: 25461.42
[INFO 2017-06-26 12:05:07,092 main.py:50] epoch 2101, training loss: 11951.25, average training loss: 13280.66, base loss: 25458.53
[INFO 2017-06-26 12:05:07,825 main.py:50] epoch 2102, training loss: 13849.82, average training loss: 13281.09, base loss: 25460.70
[INFO 2017-06-26 12:05:08,558 main.py:50] epoch 2103, training loss: 11902.59, average training loss: 13279.75, base loss: 25458.85
[INFO 2017-06-26 12:05:09,290 main.py:50] epoch 2104, training loss: 12146.01, average training loss: 13277.98, base loss: 25457.34
[INFO 2017-06-26 12:05:10,023 main.py:50] epoch 2105, training loss: 12644.77, average training loss: 13277.59, base loss: 25457.62
[INFO 2017-06-26 12:05:10,755 main.py:50] epoch 2106, training loss: 12579.75, average training loss: 13276.92, base loss: 25457.83
[INFO 2017-06-26 12:05:11,488 main.py:50] epoch 2107, training loss: 11651.46, average training loss: 13275.26, base loss: 25455.15
[INFO 2017-06-26 12:05:12,221 main.py:50] epoch 2108, training loss: 13666.25, average training loss: 13275.05, base loss: 25456.61
[INFO 2017-06-26 12:05:12,955 main.py:50] epoch 2109, training loss: 13932.09, average training loss: 13275.37, base loss: 25456.77
[INFO 2017-06-26 12:05:13,689 main.py:50] epoch 2110, training loss: 12238.09, average training loss: 13275.20, base loss: 25459.39
[INFO 2017-06-26 12:05:14,420 main.py:50] epoch 2111, training loss: 11995.80, average training loss: 13273.33, base loss: 25456.40
[INFO 2017-06-26 12:05:15,153 main.py:50] epoch 2112, training loss: 12119.77, average training loss: 13271.12, base loss: 25454.22
[INFO 2017-06-26 12:05:15,918 main.py:50] epoch 2113, training loss: 14472.68, average training loss: 13271.88, base loss: 25458.16
[INFO 2017-06-26 12:05:16,653 main.py:50] epoch 2114, training loss: 12066.81, average training loss: 13271.04, base loss: 25458.45
[INFO 2017-06-26 12:05:17,390 main.py:50] epoch 2115, training loss: 12655.15, average training loss: 13268.99, base loss: 25458.72
[INFO 2017-06-26 12:05:18,124 main.py:50] epoch 2116, training loss: 11893.58, average training loss: 13266.48, base loss: 25456.83
[INFO 2017-06-26 12:05:18,856 main.py:50] epoch 2117, training loss: 13954.12, average training loss: 13266.93, base loss: 25460.39
[INFO 2017-06-26 12:05:19,591 main.py:50] epoch 2118, training loss: 11747.29, average training loss: 13263.50, base loss: 25455.74
[INFO 2017-06-26 12:05:20,324 main.py:50] epoch 2119, training loss: 12657.25, average training loss: 13262.13, base loss: 25454.16
[INFO 2017-06-26 12:05:21,057 main.py:50] epoch 2120, training loss: 11502.59, average training loss: 13260.50, base loss: 25451.77
[INFO 2017-06-26 12:05:21,790 main.py:50] epoch 2121, training loss: 13853.38, average training loss: 13260.52, base loss: 25452.51
[INFO 2017-06-26 12:05:22,523 main.py:50] epoch 2122, training loss: 12556.99, average training loss: 13260.12, base loss: 25455.25
[INFO 2017-06-26 12:05:23,257 main.py:50] epoch 2123, training loss: 13444.87, average training loss: 13260.51, base loss: 25458.20
[INFO 2017-06-26 12:05:23,991 main.py:50] epoch 2124, training loss: 12241.49, average training loss: 13258.84, base loss: 25457.19
[INFO 2017-06-26 12:05:24,725 main.py:50] epoch 2125, training loss: 13367.06, average training loss: 13259.27, base loss: 25460.32
[INFO 2017-06-26 12:05:25,459 main.py:50] epoch 2126, training loss: 12721.16, average training loss: 13258.90, base loss: 25462.72
[INFO 2017-06-26 12:05:26,193 main.py:50] epoch 2127, training loss: 12071.19, average training loss: 13255.37, base loss: 25458.42
[INFO 2017-06-26 12:05:26,926 main.py:50] epoch 2128, training loss: 12335.31, average training loss: 13253.97, base loss: 25455.96
[INFO 2017-06-26 12:05:27,659 main.py:50] epoch 2129, training loss: 13170.46, average training loss: 13253.38, base loss: 25454.73
[INFO 2017-06-26 12:05:28,393 main.py:50] epoch 2130, training loss: 12075.72, average training loss: 13252.63, base loss: 25454.58
[INFO 2017-06-26 12:05:29,127 main.py:50] epoch 2131, training loss: 12354.92, average training loss: 13250.37, base loss: 25454.72
[INFO 2017-06-26 12:05:29,860 main.py:50] epoch 2132, training loss: 13629.98, average training loss: 13250.82, base loss: 25458.13
[INFO 2017-06-26 12:05:30,593 main.py:50] epoch 2133, training loss: 12198.92, average training loss: 13249.82, base loss: 25457.55
[INFO 2017-06-26 12:05:31,327 main.py:50] epoch 2134, training loss: 11720.80, average training loss: 13247.56, base loss: 25453.96
[INFO 2017-06-26 12:05:32,061 main.py:50] epoch 2135, training loss: 11998.18, average training loss: 13245.19, base loss: 25453.60
[INFO 2017-06-26 12:05:32,795 main.py:50] epoch 2136, training loss: 12356.04, average training loss: 13243.39, base loss: 25451.71
[INFO 2017-06-26 12:05:33,541 main.py:50] epoch 2137, training loss: 12197.52, average training loss: 13240.53, base loss: 25447.56
[INFO 2017-06-26 12:05:34,275 main.py:50] epoch 2138, training loss: 12838.77, average training loss: 13240.41, base loss: 25448.95
[INFO 2017-06-26 12:05:35,008 main.py:50] epoch 2139, training loss: 13868.50, average training loss: 13241.18, base loss: 25453.08
[INFO 2017-06-26 12:05:35,742 main.py:50] epoch 2140, training loss: 13917.92, average training loss: 13240.44, base loss: 25456.22
[INFO 2017-06-26 12:05:36,489 main.py:50] epoch 2141, training loss: 12073.37, average training loss: 13237.81, base loss: 25453.07
[INFO 2017-06-26 12:05:37,222 main.py:50] epoch 2142, training loss: 13177.72, average training loss: 13236.97, base loss: 25452.49
[INFO 2017-06-26 12:05:37,955 main.py:50] epoch 2143, training loss: 13709.83, average training loss: 13236.04, base loss: 25452.57
[INFO 2017-06-26 12:05:38,689 main.py:50] epoch 2144, training loss: 14003.12, average training loss: 13236.48, base loss: 25455.90
[INFO 2017-06-26 12:05:39,422 main.py:50] epoch 2145, training loss: 13078.54, average training loss: 13235.46, base loss: 25456.70
[INFO 2017-06-26 12:05:40,155 main.py:50] epoch 2146, training loss: 13316.50, average training loss: 13234.02, base loss: 25456.23
[INFO 2017-06-26 12:05:40,889 main.py:50] epoch 2147, training loss: 13992.13, average training loss: 13232.61, base loss: 25455.19
[INFO 2017-06-26 12:05:41,622 main.py:50] epoch 2148, training loss: 11852.52, average training loss: 13229.89, base loss: 25452.25
[INFO 2017-06-26 12:05:42,356 main.py:50] epoch 2149, training loss: 11979.17, average training loss: 13228.02, base loss: 25449.08
[INFO 2017-06-26 12:05:43,122 main.py:50] epoch 2150, training loss: 14499.19, average training loss: 13229.58, base loss: 25455.40
[INFO 2017-06-26 12:05:43,856 main.py:50] epoch 2151, training loss: 12262.50, average training loss: 13228.56, base loss: 25454.81
[INFO 2017-06-26 12:05:44,589 main.py:50] epoch 2152, training loss: 12547.40, average training loss: 13226.73, base loss: 25451.79
[INFO 2017-06-26 12:05:45,321 main.py:50] epoch 2153, training loss: 12094.00, average training loss: 13226.84, base loss: 25454.58
[INFO 2017-06-26 12:05:46,088 main.py:50] epoch 2154, training loss: 12069.60, average training loss: 13225.80, base loss: 25453.88
[INFO 2017-06-26 12:05:46,848 main.py:50] epoch 2155, training loss: 11976.06, average training loss: 13224.67, base loss: 25452.89
[INFO 2017-06-26 12:05:47,584 main.py:50] epoch 2156, training loss: 13960.04, average training loss: 13225.64, base loss: 25456.14
[INFO 2017-06-26 12:05:48,320 main.py:50] epoch 2157, training loss: 12479.73, average training loss: 13225.36, base loss: 25459.33
[INFO 2017-06-26 12:05:49,055 main.py:50] epoch 2158, training loss: 13837.77, average training loss: 13224.07, base loss: 25458.58
[INFO 2017-06-26 12:05:49,792 main.py:50] epoch 2159, training loss: 12476.18, average training loss: 13223.74, base loss: 25460.34
[INFO 2017-06-26 12:05:50,558 main.py:50] epoch 2160, training loss: 12447.22, average training loss: 13220.81, base loss: 25456.07
[INFO 2017-06-26 12:05:51,294 main.py:50] epoch 2161, training loss: 12386.80, average training loss: 13220.13, base loss: 25456.10
[INFO 2017-06-26 12:05:52,026 main.py:50] epoch 2162, training loss: 13975.48, average training loss: 13220.58, base loss: 25457.67
[INFO 2017-06-26 12:05:52,757 main.py:50] epoch 2163, training loss: 14021.14, average training loss: 13219.72, base loss: 25459.62
[INFO 2017-06-26 12:05:53,488 main.py:50] epoch 2164, training loss: 12161.75, average training loss: 13216.95, base loss: 25455.95
[INFO 2017-06-26 12:05:54,220 main.py:50] epoch 2165, training loss: 12355.87, average training loss: 13216.79, base loss: 25457.39
[INFO 2017-06-26 12:05:54,986 main.py:50] epoch 2166, training loss: 12333.72, average training loss: 13216.07, base loss: 25457.23
[INFO 2017-06-26 12:05:55,756 main.py:50] epoch 2167, training loss: 12445.83, average training loss: 13215.59, base loss: 25458.48
[INFO 2017-06-26 12:05:56,498 main.py:50] epoch 2168, training loss: 12069.87, average training loss: 13213.79, base loss: 25454.86
[INFO 2017-06-26 12:05:57,229 main.py:50] epoch 2169, training loss: 12060.84, average training loss: 13212.26, base loss: 25453.82
[INFO 2017-06-26 12:05:57,959 main.py:50] epoch 2170, training loss: 13601.86, average training loss: 13211.14, base loss: 25455.79
[INFO 2017-06-26 12:05:58,689 main.py:50] epoch 2171, training loss: 13847.51, average training loss: 13210.14, base loss: 25455.33
[INFO 2017-06-26 12:05:59,419 main.py:50] epoch 2172, training loss: 13067.01, average training loss: 13210.35, base loss: 25458.71
[INFO 2017-06-26 12:06:00,150 main.py:50] epoch 2173, training loss: 13046.66, average training loss: 13210.97, base loss: 25463.76
[INFO 2017-06-26 12:06:00,880 main.py:50] epoch 2174, training loss: 13705.91, average training loss: 13211.54, base loss: 25467.55
[INFO 2017-06-26 12:06:01,609 main.py:50] epoch 2175, training loss: 12947.93, average training loss: 13210.45, base loss: 25466.61
[INFO 2017-06-26 12:06:02,339 main.py:50] epoch 2176, training loss: 12643.77, average training loss: 13208.94, base loss: 25463.72
[INFO 2017-06-26 12:06:03,069 main.py:50] epoch 2177, training loss: 14256.55, average training loss: 13208.28, base loss: 25466.15
[INFO 2017-06-26 12:06:03,800 main.py:50] epoch 2178, training loss: 13152.88, average training loss: 13208.74, base loss: 25468.22
[INFO 2017-06-26 12:06:04,529 main.py:50] epoch 2179, training loss: 11956.53, average training loss: 13207.52, base loss: 25467.24
[INFO 2017-06-26 12:06:05,261 main.py:50] epoch 2180, training loss: 12230.64, average training loss: 13207.26, base loss: 25469.77
[INFO 2017-06-26 12:06:05,992 main.py:50] epoch 2181, training loss: 12812.89, average training loss: 13207.25, base loss: 25471.18
[INFO 2017-06-26 12:06:06,722 main.py:50] epoch 2182, training loss: 12583.06, average training loss: 13205.87, base loss: 25469.81
[INFO 2017-06-26 12:06:07,451 main.py:50] epoch 2183, training loss: 13562.96, average training loss: 13206.05, base loss: 25472.73
[INFO 2017-06-26 12:06:08,182 main.py:50] epoch 2184, training loss: 12539.30, average training loss: 13205.72, base loss: 25473.28
[INFO 2017-06-26 12:06:08,925 main.py:50] epoch 2185, training loss: 11599.92, average training loss: 13203.90, base loss: 25470.55
[INFO 2017-06-26 12:06:09,654 main.py:50] epoch 2186, training loss: 13820.87, average training loss: 13205.12, base loss: 25475.36
[INFO 2017-06-26 12:06:10,384 main.py:50] epoch 2187, training loss: 13331.22, average training loss: 13203.86, base loss: 25476.30
[INFO 2017-06-26 12:06:11,115 main.py:50] epoch 2188, training loss: 11816.06, average training loss: 13200.99, base loss: 25472.91
[INFO 2017-06-26 12:06:11,844 main.py:50] epoch 2189, training loss: 13336.66, average training loss: 13200.79, base loss: 25474.40
[INFO 2017-06-26 12:06:12,575 main.py:50] epoch 2190, training loss: 13657.15, average training loss: 13201.27, base loss: 25477.96
[INFO 2017-06-26 12:06:13,305 main.py:50] epoch 2191, training loss: 13044.01, average training loss: 13201.69, base loss: 25480.07
[INFO 2017-06-26 12:06:14,036 main.py:50] epoch 2192, training loss: 13756.91, average training loss: 13202.50, base loss: 25484.64
[INFO 2017-06-26 12:06:14,767 main.py:50] epoch 2193, training loss: 11888.44, average training loss: 13200.61, base loss: 25481.56
[INFO 2017-06-26 12:06:15,497 main.py:50] epoch 2194, training loss: 13203.35, average training loss: 13198.83, base loss: 25481.36
[INFO 2017-06-26 12:06:16,227 main.py:50] epoch 2195, training loss: 12323.92, average training loss: 13198.21, base loss: 25481.28
[INFO 2017-06-26 12:06:16,958 main.py:50] epoch 2196, training loss: 12359.65, average training loss: 13197.04, base loss: 25479.34
[INFO 2017-06-26 12:06:17,689 main.py:50] epoch 2197, training loss: 11996.52, average training loss: 13195.69, base loss: 25478.30
[INFO 2017-06-26 12:06:18,419 main.py:50] epoch 2198, training loss: 12399.76, average training loss: 13195.30, base loss: 25479.95
[INFO 2017-06-26 12:06:19,151 main.py:50] epoch 2199, training loss: 12219.68, average training loss: 13193.73, base loss: 25479.11
[INFO 2017-06-26 12:06:19,881 main.py:50] epoch 2200, training loss: 13092.39, average training loss: 13193.41, base loss: 25480.50
[INFO 2017-06-26 12:06:20,612 main.py:50] epoch 2201, training loss: 12121.44, average training loss: 13192.26, base loss: 25480.25
[INFO 2017-06-26 12:06:21,343 main.py:50] epoch 2202, training loss: 11706.49, average training loss: 13188.99, base loss: 25474.17
[INFO 2017-06-26 12:06:22,074 main.py:50] epoch 2203, training loss: 14351.30, average training loss: 13189.83, base loss: 25476.96
[INFO 2017-06-26 12:06:22,805 main.py:50] epoch 2204, training loss: 11831.33, average training loss: 13188.07, base loss: 25473.25
[INFO 2017-06-26 12:06:23,535 main.py:50] epoch 2205, training loss: 11818.50, average training loss: 13185.84, base loss: 25470.07
[INFO 2017-06-26 12:06:24,266 main.py:50] epoch 2206, training loss: 12426.96, average training loss: 13184.13, base loss: 25470.24
[INFO 2017-06-26 12:06:24,995 main.py:50] epoch 2207, training loss: 13865.72, average training loss: 13183.28, base loss: 25470.56
[INFO 2017-06-26 12:06:25,727 main.py:50] epoch 2208, training loss: 12620.03, average training loss: 13180.49, base loss: 25467.68
[INFO 2017-06-26 12:06:26,457 main.py:50] epoch 2209, training loss: 12461.04, average training loss: 13177.95, base loss: 25464.82
[INFO 2017-06-26 12:06:27,188 main.py:50] epoch 2210, training loss: 12611.12, average training loss: 13175.92, base loss: 25463.71
[INFO 2017-06-26 12:06:27,918 main.py:50] epoch 2211, training loss: 12189.93, average training loss: 13174.39, base loss: 25461.79
[INFO 2017-06-26 12:06:28,650 main.py:50] epoch 2212, training loss: 12513.87, average training loss: 13173.77, base loss: 25463.03
[INFO 2017-06-26 12:06:29,380 main.py:50] epoch 2213, training loss: 13288.64, average training loss: 13174.21, base loss: 25464.82
[INFO 2017-06-26 12:06:30,110 main.py:50] epoch 2214, training loss: 12706.17, average training loss: 13172.11, base loss: 25464.81
[INFO 2017-06-26 12:06:30,841 main.py:50] epoch 2215, training loss: 11636.86, average training loss: 13168.14, base loss: 25458.16
[INFO 2017-06-26 12:06:31,571 main.py:50] epoch 2216, training loss: 12669.26, average training loss: 13167.85, base loss: 25460.51
[INFO 2017-06-26 12:06:32,303 main.py:50] epoch 2217, training loss: 11774.01, average training loss: 13166.14, base loss: 25457.98
[INFO 2017-06-26 12:06:33,033 main.py:50] epoch 2218, training loss: 12802.60, average training loss: 13165.53, base loss: 25458.81
[INFO 2017-06-26 12:06:33,764 main.py:50] epoch 2219, training loss: 13341.69, average training loss: 13165.75, base loss: 25461.16
[INFO 2017-06-26 12:06:34,494 main.py:50] epoch 2220, training loss: 11508.18, average training loss: 13161.98, base loss: 25455.28
[INFO 2017-06-26 12:06:35,223 main.py:50] epoch 2221, training loss: 12886.39, average training loss: 13161.75, base loss: 25457.50
[INFO 2017-06-26 12:06:35,952 main.py:50] epoch 2222, training loss: 12249.57, average training loss: 13159.57, base loss: 25454.49
[INFO 2017-06-26 12:06:36,682 main.py:50] epoch 2223, training loss: 13945.05, average training loss: 13158.78, base loss: 25454.94
[INFO 2017-06-26 12:06:37,412 main.py:50] epoch 2224, training loss: 13307.89, average training loss: 13157.01, base loss: 25453.74
[INFO 2017-06-26 12:06:38,143 main.py:50] epoch 2225, training loss: 12136.34, average training loss: 13155.12, base loss: 25451.28
[INFO 2017-06-26 12:06:38,875 main.py:50] epoch 2226, training loss: 14327.63, average training loss: 13156.98, base loss: 25458.45
[INFO 2017-06-26 12:06:39,606 main.py:50] epoch 2227, training loss: 13587.09, average training loss: 13155.55, base loss: 25459.08
[INFO 2017-06-26 12:06:40,335 main.py:50] epoch 2228, training loss: 12656.63, average training loss: 13153.80, base loss: 25458.47
[INFO 2017-06-26 12:06:41,083 main.py:50] epoch 2229, training loss: 13930.75, average training loss: 13154.86, base loss: 25462.76
[INFO 2017-06-26 12:06:41,814 main.py:50] epoch 2230, training loss: 12188.46, average training loss: 13154.15, base loss: 25463.48
[INFO 2017-06-26 12:06:42,552 main.py:50] epoch 2231, training loss: 13434.80, average training loss: 13154.81, base loss: 25466.19
[INFO 2017-06-26 12:06:43,289 main.py:50] epoch 2232, training loss: 11419.30, average training loss: 13153.02, base loss: 25463.86
[INFO 2017-06-26 12:06:44,060 main.py:50] epoch 2233, training loss: 12733.64, average training loss: 13151.06, base loss: 25461.77
[INFO 2017-06-26 12:06:44,862 main.py:50] epoch 2234, training loss: 12281.11, average training loss: 13149.26, base loss: 25459.05
[INFO 2017-06-26 12:06:45,627 main.py:50] epoch 2235, training loss: 11510.67, average training loss: 13148.10, base loss: 25459.78
[INFO 2017-06-26 12:06:46,361 main.py:50] epoch 2236, training loss: 13859.88, average training loss: 13148.58, base loss: 25462.77
[INFO 2017-06-26 12:06:47,125 main.py:50] epoch 2237, training loss: 11935.03, average training loss: 13145.67, base loss: 25459.09
[INFO 2017-06-26 12:06:47,862 main.py:50] epoch 2238, training loss: 11422.32, average training loss: 13144.71, base loss: 25458.85
[INFO 2017-06-26 12:06:48,594 main.py:50] epoch 2239, training loss: 11763.31, average training loss: 13143.40, base loss: 25457.31
[INFO 2017-06-26 12:06:49,327 main.py:50] epoch 2240, training loss: 12314.93, average training loss: 13142.47, base loss: 25457.79
[INFO 2017-06-26 12:06:50,059 main.py:50] epoch 2241, training loss: 12824.33, average training loss: 13142.06, base loss: 25458.98
[INFO 2017-06-26 12:06:50,791 main.py:50] epoch 2242, training loss: 12875.77, average training loss: 13140.33, base loss: 25457.64
[INFO 2017-06-26 12:06:51,525 main.py:50] epoch 2243, training loss: 12617.04, average training loss: 13140.54, base loss: 25460.14
[INFO 2017-06-26 12:06:52,259 main.py:50] epoch 2244, training loss: 12641.73, average training loss: 13140.03, base loss: 25462.45
[INFO 2017-06-26 12:06:52,993 main.py:50] epoch 2245, training loss: 11933.86, average training loss: 13138.36, base loss: 25461.46
[INFO 2017-06-26 12:06:53,726 main.py:50] epoch 2246, training loss: 12429.18, average training loss: 13137.62, base loss: 25461.78
[INFO 2017-06-26 12:06:54,491 main.py:50] epoch 2247, training loss: 13179.35, average training loss: 13135.63, base loss: 25460.59
[INFO 2017-06-26 12:06:55,224 main.py:50] epoch 2248, training loss: 12981.75, average training loss: 13134.29, base loss: 25459.70
[INFO 2017-06-26 12:06:55,988 main.py:50] epoch 2249, training loss: 12513.01, average training loss: 13133.33, base loss: 25461.27
[INFO 2017-06-26 12:06:56,722 main.py:50] epoch 2250, training loss: 12387.88, average training loss: 13132.94, base loss: 25462.96
[INFO 2017-06-26 12:06:57,487 main.py:50] epoch 2251, training loss: 12345.55, average training loss: 13131.68, base loss: 25463.74
[INFO 2017-06-26 12:06:58,254 main.py:50] epoch 2252, training loss: 13532.15, average training loss: 13130.66, base loss: 25464.07
[INFO 2017-06-26 12:06:58,990 main.py:50] epoch 2253, training loss: 11909.93, average training loss: 13127.88, base loss: 25459.49
[INFO 2017-06-26 12:06:59,751 main.py:50] epoch 2254, training loss: 13614.38, average training loss: 13128.21, base loss: 25462.20
[INFO 2017-06-26 12:07:00,517 main.py:50] epoch 2255, training loss: 11786.55, average training loss: 13127.11, base loss: 25461.57
[INFO 2017-06-26 12:07:01,285 main.py:50] epoch 2256, training loss: 11506.70, average training loss: 13125.05, base loss: 25458.55
[INFO 2017-06-26 12:07:02,064 main.py:50] epoch 2257, training loss: 12299.23, average training loss: 13122.41, base loss: 25455.80
[INFO 2017-06-26 12:07:02,828 main.py:50] epoch 2258, training loss: 12599.48, average training loss: 13120.88, base loss: 25454.06
[INFO 2017-06-26 12:07:03,566 main.py:50] epoch 2259, training loss: 12167.21, average training loss: 13119.63, base loss: 25452.12
[INFO 2017-06-26 12:07:04,296 main.py:50] epoch 2260, training loss: 12206.47, average training loss: 13117.41, base loss: 25450.18
[INFO 2017-06-26 12:07:05,061 main.py:50] epoch 2261, training loss: 14152.73, average training loss: 13117.34, base loss: 25451.87
[INFO 2017-06-26 12:07:05,802 main.py:50] epoch 2262, training loss: 12178.21, average training loss: 13116.79, base loss: 25452.10
[INFO 2017-06-26 12:07:06,570 main.py:50] epoch 2263, training loss: 12557.09, average training loss: 13116.47, base loss: 25454.04
[INFO 2017-06-26 12:07:07,340 main.py:50] epoch 2264, training loss: 12209.58, average training loss: 13115.86, base loss: 25455.87
[INFO 2017-06-26 12:07:08,106 main.py:50] epoch 2265, training loss: 13847.85, average training loss: 13117.38, base loss: 25459.88
[INFO 2017-06-26 12:07:08,841 main.py:50] epoch 2266, training loss: 13490.18, average training loss: 13119.02, base loss: 25465.73
[INFO 2017-06-26 12:07:09,573 main.py:50] epoch 2267, training loss: 11674.18, average training loss: 13117.40, base loss: 25463.17
[INFO 2017-06-26 12:07:10,309 main.py:50] epoch 2268, training loss: 12472.49, average training loss: 13115.19, base loss: 25461.71
[INFO 2017-06-26 12:07:11,042 main.py:50] epoch 2269, training loss: 12605.45, average training loss: 13114.64, base loss: 25461.93
[INFO 2017-06-26 12:07:11,775 main.py:50] epoch 2270, training loss: 11616.29, average training loss: 13111.63, base loss: 25457.21
[INFO 2017-06-26 12:07:12,508 main.py:50] epoch 2271, training loss: 11976.82, average training loss: 13111.03, base loss: 25457.47
[INFO 2017-06-26 12:07:13,242 main.py:50] epoch 2272, training loss: 12339.23, average training loss: 13110.17, base loss: 25458.56
[INFO 2017-06-26 12:07:13,989 main.py:50] epoch 2273, training loss: 11764.60, average training loss: 13106.65, base loss: 25451.63
[INFO 2017-06-26 12:07:14,723 main.py:50] epoch 2274, training loss: 12851.34, average training loss: 13104.32, base loss: 25448.60
[INFO 2017-06-26 12:07:15,456 main.py:50] epoch 2275, training loss: 12364.13, average training loss: 13103.71, base loss: 25448.13
[INFO 2017-06-26 12:07:16,189 main.py:50] epoch 2276, training loss: 14074.87, average training loss: 13104.52, base loss: 25450.93
[INFO 2017-06-26 12:07:16,924 main.py:50] epoch 2277, training loss: 13611.68, average training loss: 13105.46, base loss: 25453.28
[INFO 2017-06-26 12:07:17,663 main.py:50] epoch 2278, training loss: 14325.87, average training loss: 13104.96, base loss: 25455.23
[INFO 2017-06-26 12:07:18,399 main.py:50] epoch 2279, training loss: 13144.79, average training loss: 13104.83, base loss: 25456.70
[INFO 2017-06-26 12:07:19,134 main.py:50] epoch 2280, training loss: 12624.11, average training loss: 13104.79, base loss: 25459.09
[INFO 2017-06-26 12:07:19,868 main.py:50] epoch 2281, training loss: 13580.33, average training loss: 13103.82, base loss: 25459.55
[INFO 2017-06-26 12:07:20,601 main.py:50] epoch 2282, training loss: 13210.99, average training loss: 13104.78, base loss: 25463.75
[INFO 2017-06-26 12:07:21,338 main.py:50] epoch 2283, training loss: 12095.43, average training loss: 13102.73, base loss: 25461.34
[INFO 2017-06-26 12:07:22,071 main.py:50] epoch 2284, training loss: 12233.27, average training loss: 13100.57, base loss: 25459.69
[INFO 2017-06-26 12:07:22,808 main.py:50] epoch 2285, training loss: 11918.92, average training loss: 13099.45, base loss: 25458.36
[INFO 2017-06-26 12:07:23,541 main.py:50] epoch 2286, training loss: 13187.80, average training loss: 13098.26, base loss: 25456.66
[INFO 2017-06-26 12:07:24,277 main.py:50] epoch 2287, training loss: 11918.60, average training loss: 13095.72, base loss: 25454.26
[INFO 2017-06-26 12:07:25,011 main.py:50] epoch 2288, training loss: 12471.47, average training loss: 13094.15, base loss: 25452.06
[INFO 2017-06-26 12:07:25,769 main.py:50] epoch 2289, training loss: 13337.92, average training loss: 13094.94, base loss: 25455.19
[INFO 2017-06-26 12:07:26,506 main.py:50] epoch 2290, training loss: 11529.02, average training loss: 13093.22, base loss: 25453.01
[INFO 2017-06-26 12:07:27,254 main.py:50] epoch 2291, training loss: 13434.15, average training loss: 13093.73, base loss: 25456.01
[INFO 2017-06-26 12:07:28,018 main.py:50] epoch 2292, training loss: 13581.28, average training loss: 13094.89, base loss: 25460.39
[INFO 2017-06-26 12:07:28,748 main.py:50] epoch 2293, training loss: 13772.75, average training loss: 13094.93, base loss: 25462.15
[INFO 2017-06-26 12:07:29,513 main.py:50] epoch 2294, training loss: 11660.99, average training loss: 13092.47, base loss: 25458.19
[INFO 2017-06-26 12:07:30,274 main.py:50] epoch 2295, training loss: 11780.71, average training loss: 13091.22, base loss: 25455.81
[INFO 2017-06-26 12:07:31,024 main.py:50] epoch 2296, training loss: 11914.75, average training loss: 13090.33, base loss: 25455.05
[INFO 2017-06-26 12:07:31,757 main.py:50] epoch 2297, training loss: 12356.82, average training loss: 13090.35, base loss: 25458.93
[INFO 2017-06-26 12:07:32,521 main.py:50] epoch 2298, training loss: 12241.64, average training loss: 13088.78, base loss: 25457.79
[INFO 2017-06-26 12:07:33,252 main.py:50] epoch 2299, training loss: 13847.41, average training loss: 13088.67, base loss: 25460.51
[INFO 2017-06-26 12:07:34,017 main.py:50] epoch 2300, training loss: 13993.27, average training loss: 13088.46, base loss: 25462.80
[INFO 2017-06-26 12:07:34,766 main.py:50] epoch 2301, training loss: 13641.04, average training loss: 13086.66, base loss: 25459.98
[INFO 2017-06-26 12:07:35,510 main.py:50] epoch 2302, training loss: 11632.60, average training loss: 13084.73, base loss: 25456.59
[INFO 2017-06-26 12:07:36,243 main.py:50] epoch 2303, training loss: 11839.00, average training loss: 13083.84, base loss: 25458.13
[INFO 2017-06-26 12:07:36,974 main.py:50] epoch 2304, training loss: 13305.23, average training loss: 13084.75, base loss: 25461.81
[INFO 2017-06-26 12:07:37,705 main.py:50] epoch 2305, training loss: 12002.41, average training loss: 13081.67, base loss: 25459.04
[INFO 2017-06-26 12:07:38,468 main.py:50] epoch 2306, training loss: 13159.24, average training loss: 13081.59, base loss: 25460.75
[INFO 2017-06-26 12:07:39,199 main.py:50] epoch 2307, training loss: 12310.86, average training loss: 13081.20, base loss: 25461.75
[INFO 2017-06-26 12:07:39,931 main.py:50] epoch 2308, training loss: 13164.04, average training loss: 13080.52, base loss: 25462.40
[INFO 2017-06-26 12:07:40,663 main.py:50] epoch 2309, training loss: 12547.37, average training loss: 13079.65, base loss: 25462.86
[INFO 2017-06-26 12:07:41,397 main.py:50] epoch 2310, training loss: 13097.92, average training loss: 13077.45, base loss: 25462.68
[INFO 2017-06-26 12:07:42,129 main.py:50] epoch 2311, training loss: 12524.90, average training loss: 13075.33, base loss: 25462.81
[INFO 2017-06-26 12:07:42,860 main.py:50] epoch 2312, training loss: 12444.13, average training loss: 13072.87, base loss: 25458.39
[INFO 2017-06-26 12:07:43,591 main.py:50] epoch 2313, training loss: 13381.83, average training loss: 13071.81, base loss: 25458.81
[INFO 2017-06-26 12:07:44,322 main.py:50] epoch 2314, training loss: 13252.29, average training loss: 13072.12, base loss: 25461.55
[INFO 2017-06-26 12:07:45,052 main.py:50] epoch 2315, training loss: 12564.10, average training loss: 13070.16, base loss: 25459.64
[INFO 2017-06-26 12:07:45,785 main.py:50] epoch 2316, training loss: 12198.88, average training loss: 13069.41, base loss: 25460.29
[INFO 2017-06-26 12:07:46,528 main.py:50] epoch 2317, training loss: 12709.59, average training loss: 13069.00, base loss: 25460.78
[INFO 2017-06-26 12:07:47,259 main.py:50] epoch 2318, training loss: 13731.00, average training loss: 13069.76, base loss: 25464.18
[INFO 2017-06-26 12:07:47,989 main.py:50] epoch 2319, training loss: 13170.54, average training loss: 13067.68, base loss: 25461.18
[INFO 2017-06-26 12:07:48,720 main.py:50] epoch 2320, training loss: 11977.40, average training loss: 13066.80, base loss: 25461.49
[INFO 2017-06-26 12:07:49,452 main.py:50] epoch 2321, training loss: 13648.39, average training loss: 13066.78, base loss: 25463.66
[INFO 2017-06-26 12:07:50,182 main.py:50] epoch 2322, training loss: 13000.86, average training loss: 13064.65, base loss: 25460.38
[INFO 2017-06-26 12:07:50,913 main.py:50] epoch 2323, training loss: 13562.62, average training loss: 13065.25, base loss: 25463.52
[INFO 2017-06-26 12:07:51,642 main.py:50] epoch 2324, training loss: 12506.77, average training loss: 13064.88, base loss: 25464.58
[INFO 2017-06-26 12:07:52,372 main.py:50] epoch 2325, training loss: 12686.04, average training loss: 13064.25, base loss: 25466.30
[INFO 2017-06-26 12:07:53,103 main.py:50] epoch 2326, training loss: 12643.46, average training loss: 13063.89, base loss: 25466.77
[INFO 2017-06-26 12:07:53,833 main.py:50] epoch 2327, training loss: 11825.18, average training loss: 13060.77, base loss: 25461.66
[INFO 2017-06-26 12:07:54,565 main.py:50] epoch 2328, training loss: 14346.98, average training loss: 13061.86, base loss: 25466.33
[INFO 2017-06-26 12:07:55,295 main.py:50] epoch 2329, training loss: 12817.30, average training loss: 13061.63, base loss: 25467.45
[INFO 2017-06-26 12:07:56,026 main.py:50] epoch 2330, training loss: 13665.06, average training loss: 13062.47, base loss: 25471.97
[INFO 2017-06-26 12:07:56,756 main.py:50] epoch 2331, training loss: 13228.55, average training loss: 13062.61, base loss: 25473.43
[INFO 2017-06-26 12:07:57,488 main.py:50] epoch 2332, training loss: 13301.86, average training loss: 13063.52, base loss: 25478.74
[INFO 2017-06-26 12:07:58,220 main.py:50] epoch 2333, training loss: 11471.14, average training loss: 13059.73, base loss: 25472.34
[INFO 2017-06-26 12:07:58,951 main.py:50] epoch 2334, training loss: 12960.72, average training loss: 13059.53, base loss: 25473.43
[INFO 2017-06-26 12:07:59,680 main.py:50] epoch 2335, training loss: 13637.33, average training loss: 13058.63, base loss: 25473.95
[INFO 2017-06-26 12:08:00,411 main.py:50] epoch 2336, training loss: 12915.71, average training loss: 13059.90, base loss: 25479.45
[INFO 2017-06-26 12:08:01,142 main.py:50] epoch 2337, training loss: 13276.47, average training loss: 13060.22, base loss: 25481.83
[INFO 2017-06-26 12:08:01,872 main.py:50] epoch 2338, training loss: 13620.36, average training loss: 13061.11, base loss: 25485.50
[INFO 2017-06-26 12:08:02,602 main.py:50] epoch 2339, training loss: 12692.28, average training loss: 13059.08, base loss: 25485.04
[INFO 2017-06-26 12:08:03,333 main.py:50] epoch 2340, training loss: 12530.37, average training loss: 13058.31, base loss: 25483.96
[INFO 2017-06-26 12:08:04,065 main.py:50] epoch 2341, training loss: 13664.88, average training loss: 13058.41, base loss: 25485.19
[INFO 2017-06-26 12:08:04,795 main.py:50] epoch 2342, training loss: 12156.79, average training loss: 13057.84, base loss: 25485.52
[INFO 2017-06-26 12:08:05,526 main.py:50] epoch 2343, training loss: 12438.15, average training loss: 13057.55, base loss: 25485.85
[INFO 2017-06-26 12:08:06,258 main.py:50] epoch 2344, training loss: 11830.19, average training loss: 13056.51, base loss: 25484.27
[INFO 2017-06-26 12:08:06,989 main.py:50] epoch 2345, training loss: 13596.76, average training loss: 13056.82, base loss: 25485.94
[INFO 2017-06-26 12:08:07,721 main.py:50] epoch 2346, training loss: 11703.20, average training loss: 13055.72, base loss: 25485.71
[INFO 2017-06-26 12:08:08,451 main.py:50] epoch 2347, training loss: 11955.56, average training loss: 13054.07, base loss: 25483.86
[INFO 2017-06-26 12:08:09,181 main.py:50] epoch 2348, training loss: 12032.28, average training loss: 13053.24, base loss: 25483.46
[INFO 2017-06-26 12:08:09,912 main.py:50] epoch 2349, training loss: 11991.10, average training loss: 13052.38, base loss: 25483.41
[INFO 2017-06-26 12:08:10,642 main.py:50] epoch 2350, training loss: 13699.35, average training loss: 13052.59, base loss: 25486.46
[INFO 2017-06-26 12:08:11,373 main.py:50] epoch 2351, training loss: 11931.81, average training loss: 13051.67, base loss: 25485.43
[INFO 2017-06-26 12:08:12,106 main.py:50] epoch 2352, training loss: 13270.90, average training loss: 13050.22, base loss: 25485.26
[INFO 2017-06-26 12:08:12,836 main.py:50] epoch 2353, training loss: 11872.17, average training loss: 13047.35, base loss: 25481.12
[INFO 2017-06-26 12:08:13,567 main.py:50] epoch 2354, training loss: 11688.07, average training loss: 13046.53, base loss: 25480.65
[INFO 2017-06-26 12:08:14,299 main.py:50] epoch 2355, training loss: 13388.37, average training loss: 13047.22, base loss: 25483.33
[INFO 2017-06-26 12:08:15,030 main.py:50] epoch 2356, training loss: 12217.76, average training loss: 13045.85, base loss: 25480.95
[INFO 2017-06-26 12:08:15,760 main.py:50] epoch 2357, training loss: 12189.72, average training loss: 13045.37, base loss: 25481.98
[INFO 2017-06-26 12:08:16,492 main.py:50] epoch 2358, training loss: 13491.69, average training loss: 13044.97, base loss: 25482.05
[INFO 2017-06-26 12:08:17,223 main.py:50] epoch 2359, training loss: 11905.19, average training loss: 13043.44, base loss: 25480.32
[INFO 2017-06-26 12:08:17,955 main.py:50] epoch 2360, training loss: 12062.51, average training loss: 13040.60, base loss: 25477.07
[INFO 2017-06-26 12:08:18,699 main.py:50] epoch 2361, training loss: 11757.24, average training loss: 13038.92, base loss: 25473.83
[INFO 2017-06-26 12:08:19,431 main.py:50] epoch 2362, training loss: 12045.29, average training loss: 13037.66, base loss: 25473.05
[INFO 2017-06-26 12:08:20,163 main.py:50] epoch 2363, training loss: 12389.56, average training loss: 13036.44, base loss: 25472.14
[INFO 2017-06-26 12:08:20,895 main.py:50] epoch 2364, training loss: 12540.02, average training loss: 13035.63, base loss: 25472.86
[INFO 2017-06-26 12:08:21,651 main.py:50] epoch 2365, training loss: 11645.66, average training loss: 13032.54, base loss: 25468.39
[INFO 2017-06-26 12:08:22,383 main.py:50] epoch 2366, training loss: 12196.43, average training loss: 13032.13, base loss: 25469.70
[INFO 2017-06-26 12:08:23,116 main.py:50] epoch 2367, training loss: 13583.95, average training loss: 13031.78, base loss: 25470.87
[INFO 2017-06-26 12:08:23,849 main.py:50] epoch 2368, training loss: 13585.86, average training loss: 13032.51, base loss: 25474.02
[INFO 2017-06-26 12:08:24,580 main.py:50] epoch 2369, training loss: 12161.34, average training loss: 13031.33, base loss: 25474.30
[INFO 2017-06-26 12:08:25,312 main.py:50] epoch 2370, training loss: 11757.71, average training loss: 13029.78, base loss: 25472.04
[INFO 2017-06-26 12:08:26,042 main.py:50] epoch 2371, training loss: 13561.64, average training loss: 13030.36, base loss: 25476.24
[INFO 2017-06-26 12:08:26,772 main.py:50] epoch 2372, training loss: 12796.34, average training loss: 13030.39, base loss: 25478.98
[INFO 2017-06-26 12:08:27,503 main.py:50] epoch 2373, training loss: 12512.22, average training loss: 13030.09, base loss: 25480.17
[INFO 2017-06-26 12:08:28,233 main.py:50] epoch 2374, training loss: 12114.66, average training loss: 13029.85, base loss: 25481.24
[INFO 2017-06-26 12:08:28,965 main.py:50] epoch 2375, training loss: 11583.31, average training loss: 13028.06, base loss: 25479.55
[INFO 2017-06-26 12:08:29,695 main.py:50] epoch 2376, training loss: 12214.77, average training loss: 13027.88, base loss: 25479.83
[INFO 2017-06-26 12:08:30,425 main.py:50] epoch 2377, training loss: 11910.99, average training loss: 13026.97, base loss: 25479.55
[INFO 2017-06-26 12:08:31,154 main.py:50] epoch 2378, training loss: 12914.14, average training loss: 13026.76, base loss: 25481.10
[INFO 2017-06-26 12:08:31,884 main.py:50] epoch 2379, training loss: 12161.29, average training loss: 13025.66, base loss: 25480.90
[INFO 2017-06-26 12:08:32,614 main.py:50] epoch 2380, training loss: 13839.74, average training loss: 13026.72, base loss: 25484.32
[INFO 2017-06-26 12:08:33,343 main.py:50] epoch 2381, training loss: 12357.04, average training loss: 13026.52, base loss: 25485.56
[INFO 2017-06-26 12:08:34,074 main.py:50] epoch 2382, training loss: 12391.51, average training loss: 13025.72, base loss: 25484.62
[INFO 2017-06-26 12:08:34,806 main.py:50] epoch 2383, training loss: 12667.91, average training loss: 13024.02, base loss: 25484.12
[INFO 2017-06-26 12:08:35,536 main.py:50] epoch 2384, training loss: 11647.50, average training loss: 13022.44, base loss: 25482.80
[INFO 2017-06-26 12:08:36,266 main.py:50] epoch 2385, training loss: 12426.18, average training loss: 13021.63, base loss: 25481.44
[INFO 2017-06-26 12:08:36,994 main.py:50] epoch 2386, training loss: 13232.62, average training loss: 13022.82, base loss: 25484.93
[INFO 2017-06-26 12:08:37,724 main.py:50] epoch 2387, training loss: 12636.36, average training loss: 13023.16, base loss: 25488.12
[INFO 2017-06-26 12:08:38,454 main.py:50] epoch 2388, training loss: 13683.61, average training loss: 13022.50, base loss: 25488.70
[INFO 2017-06-26 12:08:39,184 main.py:50] epoch 2389, training loss: 12056.42, average training loss: 13020.28, base loss: 25485.10
[INFO 2017-06-26 12:08:39,914 main.py:50] epoch 2390, training loss: 13133.46, average training loss: 13020.50, base loss: 25487.41
[INFO 2017-06-26 12:08:40,644 main.py:50] epoch 2391, training loss: 12258.30, average training loss: 13018.13, base loss: 25482.91
[INFO 2017-06-26 12:08:41,373 main.py:50] epoch 2392, training loss: 13465.84, average training loss: 13016.20, base loss: 25480.11
[INFO 2017-06-26 12:08:42,105 main.py:50] epoch 2393, training loss: 12989.92, average training loss: 13016.67, base loss: 25484.01
[INFO 2017-06-26 12:08:42,837 main.py:50] epoch 2394, training loss: 13685.35, average training loss: 13016.85, base loss: 25486.51
[INFO 2017-06-26 12:08:43,569 main.py:50] epoch 2395, training loss: 13576.90, average training loss: 13017.85, base loss: 25491.63
[INFO 2017-06-26 12:08:44,300 main.py:50] epoch 2396, training loss: 11769.71, average training loss: 13017.21, base loss: 25491.07
[INFO 2017-06-26 12:08:45,031 main.py:50] epoch 2397, training loss: 13898.26, average training loss: 13017.99, base loss: 25494.67
[INFO 2017-06-26 12:08:45,760 main.py:50] epoch 2398, training loss: 11812.33, average training loss: 13017.39, base loss: 25493.57
[INFO 2017-06-26 12:08:46,491 main.py:50] epoch 2399, training loss: 12089.47, average training loss: 13016.86, base loss: 25494.94
[INFO 2017-06-26 12:08:47,222 main.py:50] epoch 2400, training loss: 12695.84, average training loss: 13015.33, base loss: 25494.95
[INFO 2017-06-26 12:08:47,953 main.py:50] epoch 2401, training loss: 12138.72, average training loss: 13015.19, base loss: 25496.63
[INFO 2017-06-26 12:08:48,683 main.py:50] epoch 2402, training loss: 12328.58, average training loss: 13013.65, base loss: 25495.08
[INFO 2017-06-26 12:08:49,412 main.py:50] epoch 2403, training loss: 12359.16, average training loss: 13013.36, base loss: 25495.23
[INFO 2017-06-26 12:08:50,142 main.py:50] epoch 2404, training loss: 12008.77, average training loss: 13012.41, base loss: 25494.47
[INFO 2017-06-26 12:08:50,884 main.py:50] epoch 2405, training loss: 12622.73, average training loss: 13012.50, base loss: 25496.01
[INFO 2017-06-26 12:08:51,616 main.py:50] epoch 2406, training loss: 12487.41, average training loss: 13012.27, base loss: 25496.08
[INFO 2017-06-26 12:08:52,345 main.py:50] epoch 2407, training loss: 13622.85, average training loss: 13011.48, base loss: 25496.01
[INFO 2017-06-26 12:08:53,076 main.py:50] epoch 2408, training loss: 13414.95, average training loss: 13010.87, base loss: 25496.40
[INFO 2017-06-26 12:08:53,806 main.py:50] epoch 2409, training loss: 12358.27, average training loss: 13008.53, base loss: 25494.22
[INFO 2017-06-26 12:08:54,537 main.py:50] epoch 2410, training loss: 11681.78, average training loss: 13007.44, base loss: 25493.41
[INFO 2017-06-26 12:08:55,269 main.py:50] epoch 2411, training loss: 11583.90, average training loss: 13004.19, base loss: 25488.38
[INFO 2017-06-26 12:08:56,001 main.py:50] epoch 2412, training loss: 12350.51, average training loss: 13003.33, base loss: 25487.93
[INFO 2017-06-26 12:08:56,732 main.py:50] epoch 2413, training loss: 12220.59, average training loss: 13002.54, base loss: 25486.12
[INFO 2017-06-26 12:08:57,462 main.py:50] epoch 2414, training loss: 12185.02, average training loss: 13002.51, base loss: 25486.37
[INFO 2017-06-26 12:08:58,195 main.py:50] epoch 2415, training loss: 11830.90, average training loss: 13001.42, base loss: 25485.69
[INFO 2017-06-26 12:08:58,925 main.py:50] epoch 2416, training loss: 11928.51, average training loss: 13000.50, base loss: 25485.18
[INFO 2017-06-26 12:08:59,656 main.py:50] epoch 2417, training loss: 12576.22, average training loss: 12999.11, base loss: 25484.59
[INFO 2017-06-26 12:09:00,387 main.py:50] epoch 2418, training loss: 12676.91, average training loss: 12997.20, base loss: 25482.78
[INFO 2017-06-26 12:09:01,117 main.py:50] epoch 2419, training loss: 11697.03, average training loss: 12995.55, base loss: 25479.30
[INFO 2017-06-26 12:09:01,847 main.py:50] epoch 2420, training loss: 14242.13, average training loss: 12994.56, base loss: 25478.87
[INFO 2017-06-26 12:09:02,577 main.py:50] epoch 2421, training loss: 13014.50, average training loss: 12995.22, base loss: 25482.75
[INFO 2017-06-26 12:09:03,308 main.py:50] epoch 2422, training loss: 11990.88, average training loss: 12994.82, base loss: 25483.40
[INFO 2017-06-26 12:09:04,039 main.py:50] epoch 2423, training loss: 12314.91, average training loss: 12991.94, base loss: 25480.11
[INFO 2017-06-26 12:09:04,770 main.py:50] epoch 2424, training loss: 12261.49, average training loss: 12991.08, base loss: 25480.40
[INFO 2017-06-26 12:09:05,501 main.py:50] epoch 2425, training loss: 13035.91, average training loss: 12989.08, base loss: 25476.74
[INFO 2017-06-26 12:09:06,233 main.py:50] epoch 2426, training loss: 12714.02, average training loss: 12986.73, base loss: 25473.75
[INFO 2017-06-26 12:09:06,964 main.py:50] epoch 2427, training loss: 11742.21, average training loss: 12985.93, base loss: 25474.57
[INFO 2017-06-26 12:09:07,727 main.py:50] epoch 2428, training loss: 12533.80, average training loss: 12985.53, base loss: 25475.27
[INFO 2017-06-26 12:09:08,460 main.py:50] epoch 2429, training loss: 12196.55, average training loss: 12983.58, base loss: 25473.95
[INFO 2017-06-26 12:09:09,225 main.py:50] epoch 2430, training loss: 13595.29, average training loss: 12984.77, base loss: 25479.24
[INFO 2017-06-26 12:09:09,956 main.py:50] epoch 2431, training loss: 14355.65, average training loss: 12986.38, base loss: 25484.81
[INFO 2017-06-26 12:09:10,686 main.py:50] epoch 2432, training loss: 14361.81, average training loss: 12987.92, base loss: 25490.59
[INFO 2017-06-26 12:09:11,423 main.py:50] epoch 2433, training loss: 12101.67, average training loss: 12987.59, base loss: 25490.97
[INFO 2017-06-26 12:09:12,162 main.py:50] epoch 2434, training loss: 11897.62, average training loss: 12986.31, base loss: 25487.68
[INFO 2017-06-26 12:09:12,892 main.py:50] epoch 2435, training loss: 13237.13, average training loss: 12986.83, base loss: 25490.21
[INFO 2017-06-26 12:09:13,623 main.py:50] epoch 2436, training loss: 13497.78, average training loss: 12987.57, base loss: 25492.06
[INFO 2017-06-26 12:09:14,353 main.py:50] epoch 2437, training loss: 11797.94, average training loss: 12986.14, base loss: 25489.39
[INFO 2017-06-26 12:09:15,083 main.py:50] epoch 2438, training loss: 12667.99, average training loss: 12985.97, base loss: 25489.80
[INFO 2017-06-26 12:09:15,813 main.py:50] epoch 2439, training loss: 14138.88, average training loss: 12987.42, base loss: 25494.87
[INFO 2017-06-26 12:09:16,545 main.py:50] epoch 2440, training loss: 13108.09, average training loss: 12986.56, base loss: 25494.74
[INFO 2017-06-26 12:09:17,278 main.py:50] epoch 2441, training loss: 13401.37, average training loss: 12985.04, base loss: 25492.03
[INFO 2017-06-26 12:09:18,010 main.py:50] epoch 2442, training loss: 11778.97, average training loss: 12984.30, base loss: 25490.10
[INFO 2017-06-26 12:09:18,741 main.py:50] epoch 2443, training loss: 13799.36, average training loss: 12983.52, base loss: 25491.23
[INFO 2017-06-26 12:09:19,471 main.py:50] epoch 2444, training loss: 12338.10, average training loss: 12982.52, base loss: 25489.28
[INFO 2017-06-26 12:09:20,204 main.py:50] epoch 2445, training loss: 12310.63, average training loss: 12981.03, base loss: 25487.16
[INFO 2017-06-26 12:09:20,935 main.py:50] epoch 2446, training loss: 13396.83, average training loss: 12981.90, base loss: 25489.61
[INFO 2017-06-26 12:09:21,666 main.py:50] epoch 2447, training loss: 13399.34, average training loss: 12981.95, base loss: 25492.38
[INFO 2017-06-26 12:09:22,397 main.py:50] epoch 2448, training loss: 12859.96, average training loss: 12980.40, base loss: 25491.63
[INFO 2017-06-26 12:09:23,140 main.py:50] epoch 2449, training loss: 11797.82, average training loss: 12977.64, base loss: 25485.78
[INFO 2017-06-26 12:09:23,871 main.py:50] epoch 2450, training loss: 12660.33, average training loss: 12975.16, base loss: 25481.07
[INFO 2017-06-26 12:09:24,603 main.py:50] epoch 2451, training loss: 12754.53, average training loss: 12974.20, base loss: 25480.37
[INFO 2017-06-26 12:09:25,335 main.py:50] epoch 2452, training loss: 11600.91, average training loss: 12972.43, base loss: 25477.70
[INFO 2017-06-26 12:09:26,067 main.py:50] epoch 2453, training loss: 13689.78, average training loss: 12971.99, base loss: 25478.44
[INFO 2017-06-26 12:09:26,799 main.py:50] epoch 2454, training loss: 13455.66, average training loss: 12971.12, base loss: 25479.10
[INFO 2017-06-26 12:09:27,531 main.py:50] epoch 2455, training loss: 13759.20, average training loss: 12970.73, base loss: 25479.82
[INFO 2017-06-26 12:09:28,264 main.py:50] epoch 2456, training loss: 11624.02, average training loss: 12968.00, base loss: 25475.08
[INFO 2017-06-26 12:09:28,994 main.py:50] epoch 2457, training loss: 13169.37, average training loss: 12966.46, base loss: 25470.72
[INFO 2017-06-26 12:09:29,724 main.py:50] epoch 2458, training loss: 12008.54, average training loss: 12965.58, base loss: 25469.20
[INFO 2017-06-26 12:09:30,455 main.py:50] epoch 2459, training loss: 12091.86, average training loss: 12963.27, base loss: 25467.43
[INFO 2017-06-26 12:09:31,189 main.py:50] epoch 2460, training loss: 12814.41, average training loss: 12961.84, base loss: 25467.03
[INFO 2017-06-26 12:09:31,920 main.py:50] epoch 2461, training loss: 12166.86, average training loss: 12961.42, base loss: 25468.03
[INFO 2017-06-26 12:09:32,651 main.py:50] epoch 2462, training loss: 13065.66, average training loss: 12962.23, base loss: 25470.86
[INFO 2017-06-26 12:09:33,382 main.py:50] epoch 2463, training loss: 12673.76, average training loss: 12962.21, base loss: 25473.20
[INFO 2017-06-26 12:09:34,112 main.py:50] epoch 2464, training loss: 12354.37, average training loss: 12961.39, base loss: 25472.53
[INFO 2017-06-26 12:09:34,843 main.py:50] epoch 2465, training loss: 12222.88, average training loss: 12960.86, base loss: 25473.11
[INFO 2017-06-26 12:09:35,575 main.py:50] epoch 2466, training loss: 12156.61, average training loss: 12959.85, base loss: 25472.44
[INFO 2017-06-26 12:09:36,305 main.py:50] epoch 2467, training loss: 11966.64, average training loss: 12958.32, base loss: 25470.82
[INFO 2017-06-26 12:09:37,037 main.py:50] epoch 2468, training loss: 13707.96, average training loss: 12957.17, base loss: 25469.73
[INFO 2017-06-26 12:09:37,769 main.py:50] epoch 2469, training loss: 11840.64, average training loss: 12955.86, base loss: 25468.17
[INFO 2017-06-26 12:09:38,500 main.py:50] epoch 2470, training loss: 11877.32, average training loss: 12955.18, base loss: 25468.50
[INFO 2017-06-26 12:09:39,263 main.py:50] epoch 2471, training loss: 13555.74, average training loss: 12956.02, base loss: 25472.39
[INFO 2017-06-26 12:09:39,995 main.py:50] epoch 2472, training loss: 12999.11, average training loss: 12955.70, base loss: 25473.05
[INFO 2017-06-26 12:09:40,729 main.py:50] epoch 2473, training loss: 12356.58, average training loss: 12955.15, base loss: 25474.64
[INFO 2017-06-26 12:09:41,494 main.py:50] epoch 2474, training loss: 13779.81, average training loss: 12956.19, base loss: 25480.03
[INFO 2017-06-26 12:09:42,228 main.py:50] epoch 2475, training loss: 13293.06, average training loss: 12956.75, base loss: 25482.68
[INFO 2017-06-26 12:09:42,960 main.py:50] epoch 2476, training loss: 11983.67, average training loss: 12955.46, base loss: 25481.32
[INFO 2017-06-26 12:09:43,695 main.py:50] epoch 2477, training loss: 12272.99, average training loss: 12955.31, base loss: 25483.35
[INFO 2017-06-26 12:09:44,429 main.py:50] epoch 2478, training loss: 12101.52, average training loss: 12955.57, base loss: 25485.84
[INFO 2017-06-26 12:09:45,163 main.py:50] epoch 2479, training loss: 14076.61, average training loss: 12956.66, base loss: 25490.04
[INFO 2017-06-26 12:09:45,896 main.py:50] epoch 2480, training loss: 13279.08, average training loss: 12956.52, base loss: 25490.58
[INFO 2017-06-26 12:09:46,629 main.py:50] epoch 2481, training loss: 13501.96, average training loss: 12957.06, base loss: 25491.47
[INFO 2017-06-26 12:09:47,362 main.py:50] epoch 2482, training loss: 12668.60, average training loss: 12956.70, base loss: 25493.10
[INFO 2017-06-26 12:09:48,095 main.py:50] epoch 2483, training loss: 13664.70, average training loss: 12955.27, base loss: 25490.03
[INFO 2017-06-26 12:09:48,828 main.py:50] epoch 2484, training loss: 12172.41, average training loss: 12953.15, base loss: 25487.55
[INFO 2017-06-26 12:09:49,562 main.py:50] epoch 2485, training loss: 11663.57, average training loss: 12952.05, base loss: 25486.22
[INFO 2017-06-26 12:09:50,295 main.py:50] epoch 2486, training loss: 13487.81, average training loss: 12952.56, base loss: 25488.87
[INFO 2017-06-26 12:09:51,028 main.py:50] epoch 2487, training loss: 13201.99, average training loss: 12953.57, base loss: 25490.97
[INFO 2017-06-26 12:09:51,761 main.py:50] epoch 2488, training loss: 12604.98, average training loss: 12952.85, base loss: 25491.47
[INFO 2017-06-26 12:09:52,494 main.py:50] epoch 2489, training loss: 12084.39, average training loss: 12952.33, base loss: 25491.19
[INFO 2017-06-26 12:09:53,228 main.py:50] epoch 2490, training loss: 12400.68, average training loss: 12951.91, base loss: 25491.42
[INFO 2017-06-26 12:09:53,960 main.py:50] epoch 2491, training loss: 14054.12, average training loss: 12951.54, base loss: 25492.69
[INFO 2017-06-26 12:09:54,692 main.py:50] epoch 2492, training loss: 13360.48, average training loss: 12951.40, base loss: 25492.44
[INFO 2017-06-26 12:09:55,439 main.py:50] epoch 2493, training loss: 13945.23, average training loss: 12952.56, base loss: 25496.46
[INFO 2017-06-26 12:09:56,173 main.py:50] epoch 2494, training loss: 11757.74, average training loss: 12950.01, base loss: 25493.04
[INFO 2017-06-26 12:09:56,907 main.py:50] epoch 2495, training loss: 14170.61, average training loss: 12950.13, base loss: 25496.06
[INFO 2017-06-26 12:09:57,641 main.py:50] epoch 2496, training loss: 12284.93, average training loss: 12950.32, base loss: 25498.74
[INFO 2017-06-26 12:09:58,374 main.py:50] epoch 2497, training loss: 13363.64, average training loss: 12948.93, base loss: 25497.68
[INFO 2017-06-26 12:09:59,107 main.py:50] epoch 2498, training loss: 13740.24, average training loss: 12947.67, base loss: 25496.48
[INFO 2017-06-26 12:09:59,842 main.py:50] epoch 2499, training loss: 13371.62, average training loss: 12948.96, base loss: 25501.81
[INFO 2017-06-26 12:10:00,576 main.py:50] epoch 2500, training loss: 12149.68, average training loss: 12947.74, base loss: 25501.37
[INFO 2017-06-26 12:10:01,308 main.py:50] epoch 2501, training loss: 13644.55, average training loss: 12948.54, base loss: 25503.60
[INFO 2017-06-26 12:10:02,041 main.py:50] epoch 2502, training loss: 12096.16, average training loss: 12946.67, base loss: 25500.79
[INFO 2017-06-26 12:10:02,775 main.py:50] epoch 2503, training loss: 12343.16, average training loss: 12944.05, base loss: 25496.13
[INFO 2017-06-26 12:10:03,544 main.py:50] epoch 2504, training loss: 11930.18, average training loss: 12942.02, base loss: 25493.38
[INFO 2017-06-26 12:10:04,277 main.py:50] epoch 2505, training loss: 13198.07, average training loss: 12941.58, base loss: 25492.31
[INFO 2017-06-26 12:10:05,009 main.py:50] epoch 2506, training loss: 12322.36, average training loss: 12939.64, base loss: 25489.73
[INFO 2017-06-26 12:10:05,743 main.py:50] epoch 2507, training loss: 13508.50, average training loss: 12940.85, base loss: 25493.39
[INFO 2017-06-26 12:10:06,474 main.py:50] epoch 2508, training loss: 13218.85, average training loss: 12939.38, base loss: 25490.75
[INFO 2017-06-26 12:10:07,209 main.py:50] epoch 2509, training loss: 12080.32, average training loss: 12938.50, base loss: 25489.81
[INFO 2017-06-26 12:10:07,975 main.py:50] epoch 2510, training loss: 11978.82, average training loss: 12937.55, base loss: 25488.30
[INFO 2017-06-26 12:10:08,708 main.py:50] epoch 2511, training loss: 12682.22, average training loss: 12937.46, base loss: 25488.57
[INFO 2017-06-26 12:10:09,441 main.py:50] epoch 2512, training loss: 12476.81, average training loss: 12936.85, base loss: 25488.98
[INFO 2017-06-26 12:10:10,175 main.py:50] epoch 2513, training loss: 12561.62, average training loss: 12935.41, base loss: 25488.05
[INFO 2017-06-26 12:10:10,909 main.py:50] epoch 2514, training loss: 13249.06, average training loss: 12934.27, base loss: 25486.17
[INFO 2017-06-26 12:10:11,644 main.py:50] epoch 2515, training loss: 12012.79, average training loss: 12933.48, base loss: 25487.27
[INFO 2017-06-26 12:10:12,410 main.py:50] epoch 2516, training loss: 12614.27, average training loss: 12932.99, base loss: 25488.90
[INFO 2017-06-26 12:10:13,144 main.py:50] epoch 2517, training loss: 12218.91, average training loss: 12932.38, base loss: 25490.54
[INFO 2017-06-26 12:10:13,877 main.py:50] epoch 2518, training loss: 13251.70, average training loss: 12932.93, base loss: 25491.70
[INFO 2017-06-26 12:10:14,610 main.py:50] epoch 2519, training loss: 13893.65, average training loss: 12932.75, base loss: 25493.19
[INFO 2017-06-26 12:10:15,344 main.py:50] epoch 2520, training loss: 14251.64, average training loss: 12933.43, base loss: 25496.02
[INFO 2017-06-26 12:10:16,077 main.py:50] epoch 2521, training loss: 12153.03, average training loss: 12931.64, base loss: 25493.97
[INFO 2017-06-26 12:10:16,811 main.py:50] epoch 2522, training loss: 13269.56, average training loss: 12931.38, base loss: 25494.88
[INFO 2017-06-26 12:10:17,546 main.py:50] epoch 2523, training loss: 13768.78, average training loss: 12932.77, base loss: 25498.92
[INFO 2017-06-26 12:10:18,279 main.py:50] epoch 2524, training loss: 11682.09, average training loss: 12932.66, base loss: 25499.91
[INFO 2017-06-26 12:10:19,043 main.py:50] epoch 2525, training loss: 13754.96, average training loss: 12933.61, base loss: 25502.70
[INFO 2017-06-26 12:10:19,779 main.py:50] epoch 2526, training loss: 12232.41, average training loss: 12933.04, base loss: 25502.52
[INFO 2017-06-26 12:10:20,517 main.py:50] epoch 2527, training loss: 12711.90, average training loss: 12930.99, base loss: 25502.05
[INFO 2017-06-26 12:10:21,249 main.py:50] epoch 2528, training loss: 11505.27, average training loss: 12928.77, base loss: 25498.40
[INFO 2017-06-26 12:10:21,983 main.py:50] epoch 2529, training loss: 12052.17, average training loss: 12926.40, base loss: 25496.53
[INFO 2017-06-26 12:10:22,746 main.py:50] epoch 2530, training loss: 11981.73, average training loss: 12924.21, base loss: 25495.14
[INFO 2017-06-26 12:10:23,497 main.py:50] epoch 2531, training loss: 12906.39, average training loss: 12924.05, base loss: 25496.49
[INFO 2017-06-26 12:10:24,264 main.py:50] epoch 2532, training loss: 11898.38, average training loss: 12922.98, base loss: 25495.19
[INFO 2017-06-26 12:10:24,996 main.py:50] epoch 2533, training loss: 11318.03, average training loss: 12920.15, base loss: 25488.32
[INFO 2017-06-26 12:10:25,739 main.py:50] epoch 2534, training loss: 12715.61, average training loss: 12920.24, base loss: 25490.66
[INFO 2017-06-26 12:10:26,470 main.py:50] epoch 2535, training loss: 13487.86, average training loss: 12919.57, base loss: 25491.68
[INFO 2017-06-26 12:10:27,201 main.py:50] epoch 2536, training loss: 11702.68, average training loss: 12917.44, base loss: 25487.26
[INFO 2017-06-26 12:10:27,947 main.py:50] epoch 2537, training loss: 13215.73, average training loss: 12917.89, base loss: 25488.73
[INFO 2017-06-26 12:10:28,679 main.py:50] epoch 2538, training loss: 12106.04, average training loss: 12915.89, base loss: 25486.33
[INFO 2017-06-26 12:10:29,410 main.py:50] epoch 2539, training loss: 13121.87, average training loss: 12916.49, base loss: 25488.62
[INFO 2017-06-26 12:10:30,142 main.py:50] epoch 2540, training loss: 11720.66, average training loss: 12915.18, base loss: 25486.95
[INFO 2017-06-26 12:10:30,875 main.py:50] epoch 2541, training loss: 11514.90, average training loss: 12912.53, base loss: 25483.84
[INFO 2017-06-26 12:10:31,607 main.py:50] epoch 2542, training loss: 11639.83, average training loss: 12910.74, base loss: 25480.85
[INFO 2017-06-26 12:10:32,340 main.py:50] epoch 2543, training loss: 11856.16, average training loss: 12907.95, base loss: 25476.76
[INFO 2017-06-26 12:10:33,104 main.py:50] epoch 2544, training loss: 11396.69, average training loss: 12906.55, base loss: 25473.35
[INFO 2017-06-26 12:10:33,853 main.py:50] epoch 2545, training loss: 12364.09, average training loss: 12906.41, base loss: 25475.13
[INFO 2017-06-26 12:10:34,617 main.py:50] epoch 2546, training loss: 11809.83, average training loss: 12906.07, base loss: 25474.93
[INFO 2017-06-26 12:10:35,350 main.py:50] epoch 2547, training loss: 11617.56, average training loss: 12903.15, base loss: 25469.27
[INFO 2017-06-26 12:10:36,083 main.py:50] epoch 2548, training loss: 12369.15, average training loss: 12902.82, base loss: 25470.51
[INFO 2017-06-26 12:10:36,817 main.py:50] epoch 2549, training loss: 11637.73, average training loss: 12901.50, base loss: 25468.71
[INFO 2017-06-26 12:10:37,551 main.py:50] epoch 2550, training loss: 11296.86, average training loss: 12899.83, base loss: 25466.67
[INFO 2017-06-26 12:10:38,285 main.py:50] epoch 2551, training loss: 12425.60, average training loss: 12897.33, base loss: 25464.95
[INFO 2017-06-26 12:10:39,020 main.py:50] epoch 2552, training loss: 13594.57, average training loss: 12898.25, base loss: 25468.33
[INFO 2017-06-26 12:10:39,786 main.py:50] epoch 2553, training loss: 12363.91, average training loss: 12895.90, base loss: 25464.06
[INFO 2017-06-26 12:10:40,519 main.py:50] epoch 2554, training loss: 13442.63, average training loss: 12894.53, base loss: 25462.83
[INFO 2017-06-26 12:10:41,252 main.py:50] epoch 2555, training loss: 13407.68, average training loss: 12895.33, base loss: 25465.94
[INFO 2017-06-26 12:10:41,986 main.py:50] epoch 2556, training loss: 13262.07, average training loss: 12895.92, base loss: 25469.97
[INFO 2017-06-26 12:10:42,719 main.py:50] epoch 2557, training loss: 13104.40, average training loss: 12896.38, base loss: 25470.23
[INFO 2017-06-26 12:10:43,455 main.py:50] epoch 2558, training loss: 12423.75, average training loss: 12895.90, base loss: 25469.85
[INFO 2017-06-26 12:10:44,189 main.py:50] epoch 2559, training loss: 11282.98, average training loss: 12894.93, base loss: 25468.44
[INFO 2017-06-26 12:10:44,924 main.py:50] epoch 2560, training loss: 12587.86, average training loss: 12894.37, base loss: 25468.88
[INFO 2017-06-26 12:10:45,657 main.py:50] epoch 2561, training loss: 13342.59, average training loss: 12893.75, base loss: 25468.47
[INFO 2017-06-26 12:10:46,392 main.py:50] epoch 2562, training loss: 11774.26, average training loss: 12892.41, base loss: 25466.01
[INFO 2017-06-26 12:10:47,125 main.py:50] epoch 2563, training loss: 11498.49, average training loss: 12891.31, base loss: 25463.23
[INFO 2017-06-26 12:10:47,858 main.py:50] epoch 2564, training loss: 11490.72, average training loss: 12888.60, base loss: 25458.42
[INFO 2017-06-26 12:10:48,596 main.py:50] epoch 2565, training loss: 12643.13, average training loss: 12888.70, base loss: 25461.00
[INFO 2017-06-26 12:10:49,340 main.py:50] epoch 2566, training loss: 11782.05, average training loss: 12887.35, base loss: 25459.69
[INFO 2017-06-26 12:10:50,071 main.py:50] epoch 2567, training loss: 11462.97, average training loss: 12884.70, base loss: 25453.41
[INFO 2017-06-26 12:10:50,803 main.py:50] epoch 2568, training loss: 12530.60, average training loss: 12884.86, base loss: 25457.30
[INFO 2017-06-26 12:10:51,534 main.py:50] epoch 2569, training loss: 12202.33, average training loss: 12882.89, base loss: 25454.60
[INFO 2017-06-26 12:10:52,264 main.py:50] epoch 2570, training loss: 11973.20, average training loss: 12881.73, base loss: 25453.14
[INFO 2017-06-26 12:10:52,997 main.py:50] epoch 2571, training loss: 13541.50, average training loss: 12881.82, base loss: 25453.67
[INFO 2017-06-26 12:10:53,727 main.py:50] epoch 2572, training loss: 13383.25, average training loss: 12880.07, base loss: 25450.17
[INFO 2017-06-26 12:10:54,457 main.py:50] epoch 2573, training loss: 13662.05, average training loss: 12879.55, base loss: 25450.46
[INFO 2017-06-26 12:10:55,188 main.py:50] epoch 2574, training loss: 11900.47, average training loss: 12876.64, base loss: 25443.89
[INFO 2017-06-26 12:10:55,918 main.py:50] epoch 2575, training loss: 11776.32, average training loss: 12876.20, base loss: 25444.28
[INFO 2017-06-26 12:10:56,649 main.py:50] epoch 2576, training loss: 12079.84, average training loss: 12875.82, base loss: 25445.90
[INFO 2017-06-26 12:10:57,379 main.py:50] epoch 2577, training loss: 12240.13, average training loss: 12875.24, base loss: 25445.57
[INFO 2017-06-26 12:10:58,110 main.py:50] epoch 2578, training loss: 13296.21, average training loss: 12875.70, base loss: 25446.51
[INFO 2017-06-26 12:10:58,841 main.py:50] epoch 2579, training loss: 11558.79, average training loss: 12874.30, base loss: 25444.78
[INFO 2017-06-26 12:10:59,573 main.py:50] epoch 2580, training loss: 11642.51, average training loss: 12873.40, base loss: 25444.06
[INFO 2017-06-26 12:11:00,316 main.py:50] epoch 2581, training loss: 13644.91, average training loss: 12872.96, base loss: 25444.30
[INFO 2017-06-26 12:11:01,048 main.py:50] epoch 2582, training loss: 11396.37, average training loss: 12869.93, base loss: 25438.73
[INFO 2017-06-26 12:11:01,781 main.py:50] epoch 2583, training loss: 12359.33, average training loss: 12869.37, base loss: 25439.17
[INFO 2017-06-26 12:11:02,545 main.py:50] epoch 2584, training loss: 11975.48, average training loss: 12867.92, base loss: 25438.02
[INFO 2017-06-26 12:11:03,278 main.py:50] epoch 2585, training loss: 11954.02, average training loss: 12866.75, base loss: 25436.97
[INFO 2017-06-26 12:11:04,041 main.py:50] epoch 2586, training loss: 13109.93, average training loss: 12865.79, base loss: 25436.39
[INFO 2017-06-26 12:11:04,802 main.py:50] epoch 2587, training loss: 12234.60, average training loss: 12863.55, base loss: 25433.59
[INFO 2017-06-26 12:11:05,580 main.py:50] epoch 2588, training loss: 12088.43, average training loss: 12862.19, base loss: 25433.95
[INFO 2017-06-26 12:11:06,313 main.py:50] epoch 2589, training loss: 13123.59, average training loss: 12861.96, base loss: 25435.05
[INFO 2017-06-26 12:11:07,044 main.py:50] epoch 2590, training loss: 12915.76, average training loss: 12861.70, base loss: 25435.68
[INFO 2017-06-26 12:11:07,780 main.py:50] epoch 2591, training loss: 11857.36, average training loss: 12859.08, base loss: 25431.19
[INFO 2017-06-26 12:11:08,515 main.py:50] epoch 2592, training loss: 12232.24, average training loss: 12858.76, base loss: 25433.05
[INFO 2017-06-26 12:11:09,248 main.py:50] epoch 2593, training loss: 11551.98, average training loss: 12857.46, base loss: 25430.64
[INFO 2017-06-26 12:11:09,980 main.py:50] epoch 2594, training loss: 11808.00, average training loss: 12856.78, base loss: 25429.77
[INFO 2017-06-26 12:11:10,713 main.py:50] epoch 2595, training loss: 13775.77, average training loss: 12856.96, base loss: 25430.49
[INFO 2017-06-26 12:11:11,446 main.py:50] epoch 2596, training loss: 13258.56, average training loss: 12855.73, base loss: 25430.37
[INFO 2017-06-26 12:11:12,179 main.py:50] epoch 2597, training loss: 11753.96, average training loss: 12853.30, base loss: 25427.41
[INFO 2017-06-26 12:11:12,913 main.py:50] epoch 2598, training loss: 11659.38, average training loss: 12852.14, base loss: 25426.25
[INFO 2017-06-26 12:11:13,646 main.py:50] epoch 2599, training loss: 12046.79, average training loss: 12850.04, base loss: 25423.20
[INFO 2017-06-26 12:11:14,378 main.py:50] epoch 2600, training loss: 13036.62, average training loss: 12848.82, base loss: 25421.76
[INFO 2017-06-26 12:11:15,112 main.py:50] epoch 2601, training loss: 12522.88, average training loss: 12847.68, base loss: 25421.20
[INFO 2017-06-26 12:11:15,845 main.py:50] epoch 2602, training loss: 12537.10, average training loss: 12847.91, base loss: 25423.69
[INFO 2017-06-26 12:11:16,578 main.py:50] epoch 2603, training loss: 12152.71, average training loss: 12845.07, base loss: 25418.71
[INFO 2017-06-26 12:11:17,314 main.py:50] epoch 2604, training loss: 11665.23, average training loss: 12842.40, base loss: 25414.38
[INFO 2017-06-26 12:11:18,048 main.py:50] epoch 2605, training loss: 13029.68, average training loss: 12842.63, base loss: 25414.78
[INFO 2017-06-26 12:11:18,780 main.py:50] epoch 2606, training loss: 12776.96, average training loss: 12843.19, base loss: 25416.24
[INFO 2017-06-26 12:11:19,513 main.py:50] epoch 2607, training loss: 12783.96, average training loss: 12843.78, base loss: 25417.89
[INFO 2017-06-26 12:11:20,246 main.py:50] epoch 2608, training loss: 12701.96, average training loss: 12843.04, base loss: 25417.82
[INFO 2017-06-26 12:11:20,980 main.py:50] epoch 2609, training loss: 13125.14, average training loss: 12843.19, base loss: 25420.37
[INFO 2017-06-26 12:11:21,713 main.py:50] epoch 2610, training loss: 11724.53, average training loss: 12840.29, base loss: 25415.41
[INFO 2017-06-26 12:11:22,447 main.py:50] epoch 2611, training loss: 11601.28, average training loss: 12839.71, base loss: 25414.06
[INFO 2017-06-26 12:11:23,212 main.py:50] epoch 2612, training loss: 13851.29, average training loss: 12840.20, base loss: 25417.23
[INFO 2017-06-26 12:11:23,945 main.py:50] epoch 2613, training loss: 12481.16, average training loss: 12839.90, base loss: 25417.60
[INFO 2017-06-26 12:11:24,677 main.py:50] epoch 2614, training loss: 13175.50, average training loss: 12840.60, base loss: 25419.56
[INFO 2017-06-26 12:11:25,411 main.py:50] epoch 2615, training loss: 11875.98, average training loss: 12839.43, base loss: 25418.41
[INFO 2017-06-26 12:11:26,145 main.py:50] epoch 2616, training loss: 11588.28, average training loss: 12839.33, base loss: 25420.27
[INFO 2017-06-26 12:11:26,879 main.py:50] epoch 2617, training loss: 12143.06, average training loss: 12838.54, base loss: 25419.52
[INFO 2017-06-26 12:11:27,612 main.py:50] epoch 2618, training loss: 11478.38, average training loss: 12836.77, base loss: 25418.17
[INFO 2017-06-26 12:11:28,348 main.py:50] epoch 2619, training loss: 12208.22, average training loss: 12835.04, base loss: 25416.49
[INFO 2017-06-26 12:11:29,084 main.py:50] epoch 2620, training loss: 12391.54, average training loss: 12832.13, base loss: 25410.74
[INFO 2017-06-26 12:11:29,818 main.py:50] epoch 2621, training loss: 12041.53, average training loss: 12830.16, base loss: 25408.01
[INFO 2017-06-26 12:11:30,554 main.py:50] epoch 2622, training loss: 13817.65, average training loss: 12829.67, base loss: 25408.54
[INFO 2017-06-26 12:11:31,290 main.py:50] epoch 2623, training loss: 11915.95, average training loss: 12829.46, base loss: 25408.93
[INFO 2017-06-26 12:11:32,025 main.py:50] epoch 2624, training loss: 11746.27, average training loss: 12828.37, base loss: 25408.48
[INFO 2017-06-26 12:11:32,775 main.py:50] epoch 2625, training loss: 11650.69, average training loss: 12827.23, base loss: 25406.85
[INFO 2017-06-26 12:11:33,508 main.py:50] epoch 2626, training loss: 12136.20, average training loss: 12826.52, base loss: 25407.09
[INFO 2017-06-26 12:11:34,241 main.py:50] epoch 2627, training loss: 13146.34, average training loss: 12827.17, base loss: 25411.01
[INFO 2017-06-26 12:11:34,974 main.py:50] epoch 2628, training loss: 13566.62, average training loss: 12826.24, base loss: 25409.59
[INFO 2017-06-26 12:11:35,707 main.py:50] epoch 2629, training loss: 13420.03, average training loss: 12825.94, base loss: 25410.97
[INFO 2017-06-26 12:11:36,439 main.py:50] epoch 2630, training loss: 12012.89, average training loss: 12825.96, base loss: 25414.01
[INFO 2017-06-26 12:11:37,172 main.py:50] epoch 2631, training loss: 12289.21, average training loss: 12825.56, base loss: 25413.94
[INFO 2017-06-26 12:11:37,904 main.py:50] epoch 2632, training loss: 12268.66, average training loss: 12825.75, base loss: 25416.25
[INFO 2017-06-26 12:11:38,637 main.py:50] epoch 2633, training loss: 13489.80, average training loss: 12827.22, base loss: 25420.71
[INFO 2017-06-26 12:11:39,368 main.py:50] epoch 2634, training loss: 12458.46, average training loss: 12826.81, base loss: 25420.89
[INFO 2017-06-26 12:11:40,101 main.py:50] epoch 2635, training loss: 12316.31, average training loss: 12825.89, base loss: 25422.25
[INFO 2017-06-26 12:11:40,832 main.py:50] epoch 2636, training loss: 11975.28, average training loss: 12824.23, base loss: 25420.69
[INFO 2017-06-26 12:11:41,565 main.py:50] epoch 2637, training loss: 11486.25, average training loss: 12822.38, base loss: 25416.45
[INFO 2017-06-26 12:11:42,297 main.py:50] epoch 2638, training loss: 12820.96, average training loss: 12822.51, base loss: 25416.66
[INFO 2017-06-26 12:11:43,029 main.py:50] epoch 2639, training loss: 14029.22, average training loss: 12822.72, base loss: 25417.71
[INFO 2017-06-26 12:11:43,763 main.py:50] epoch 2640, training loss: 12194.14, average training loss: 12822.14, base loss: 25417.95
[INFO 2017-06-26 12:11:44,496 main.py:50] epoch 2641, training loss: 12262.43, average training loss: 12820.65, base loss: 25418.99
[INFO 2017-06-26 12:11:45,229 main.py:50] epoch 2642, training loss: 12152.02, average training loss: 12819.78, base loss: 25417.37
[INFO 2017-06-26 12:11:45,962 main.py:50] epoch 2643, training loss: 12311.20, average training loss: 12819.34, base loss: 25417.56
[INFO 2017-06-26 12:11:46,697 main.py:50] epoch 2644, training loss: 11897.34, average training loss: 12818.04, base loss: 25415.75
[INFO 2017-06-26 12:11:47,430 main.py:50] epoch 2645, training loss: 12029.06, average training loss: 12815.19, base loss: 25411.27
[INFO 2017-06-26 12:11:48,164 main.py:50] epoch 2646, training loss: 12012.49, average training loss: 12812.98, base loss: 25409.16
[INFO 2017-06-26 12:11:48,898 main.py:50] epoch 2647, training loss: 13029.20, average training loss: 12813.08, base loss: 25408.96
[INFO 2017-06-26 12:11:49,631 main.py:50] epoch 2648, training loss: 12737.34, average training loss: 12812.56, base loss: 25408.77
[INFO 2017-06-26 12:11:50,365 main.py:50] epoch 2649, training loss: 12053.63, average training loss: 12811.45, base loss: 25406.38
[INFO 2017-06-26 12:11:51,099 main.py:50] epoch 2650, training loss: 14216.86, average training loss: 12813.06, base loss: 25411.81
[INFO 2017-06-26 12:11:51,833 main.py:50] epoch 2651, training loss: 11908.69, average training loss: 12812.23, base loss: 25410.80
[INFO 2017-06-26 12:11:52,570 main.py:50] epoch 2652, training loss: 12527.49, average training loss: 12811.96, base loss: 25411.71
[INFO 2017-06-26 12:11:53,303 main.py:50] epoch 2653, training loss: 13461.20, average training loss: 12811.01, base loss: 25412.16
[INFO 2017-06-26 12:11:54,036 main.py:50] epoch 2654, training loss: 12368.54, average training loss: 12811.06, base loss: 25415.05
[INFO 2017-06-26 12:11:54,770 main.py:50] epoch 2655, training loss: 12153.52, average training loss: 12811.34, base loss: 25418.94
[INFO 2017-06-26 12:11:55,505 main.py:50] epoch 2656, training loss: 13930.75, average training loss: 12812.74, base loss: 25423.56
[INFO 2017-06-26 12:11:56,239 main.py:50] epoch 2657, training loss: 11945.97, average training loss: 12811.10, base loss: 25422.21
[INFO 2017-06-26 12:11:56,977 main.py:50] epoch 2658, training loss: 11495.66, average training loss: 12809.76, base loss: 25420.41
[INFO 2017-06-26 12:11:57,711 main.py:50] epoch 2659, training loss: 12250.17, average training loss: 12808.73, base loss: 25419.04
[INFO 2017-06-26 12:11:58,445 main.py:50] epoch 2660, training loss: 13515.26, average training loss: 12808.61, base loss: 25420.08
[INFO 2017-06-26 12:11:59,178 main.py:50] epoch 2661, training loss: 12484.09, average training loss: 12808.99, base loss: 25422.62
[INFO 2017-06-26 12:11:59,913 main.py:50] epoch 2662, training loss: 12529.62, average training loss: 12807.08, base loss: 25418.84
[INFO 2017-06-26 12:12:00,646 main.py:50] epoch 2663, training loss: 13234.75, average training loss: 12806.60, base loss: 25418.38
[INFO 2017-06-26 12:12:01,380 main.py:50] epoch 2664, training loss: 11435.15, average training loss: 12804.44, base loss: 25414.25
[INFO 2017-06-26 12:12:02,114 main.py:50] epoch 2665, training loss: 12014.63, average training loss: 12802.32, base loss: 25410.43
[INFO 2017-06-26 12:12:02,849 main.py:50] epoch 2666, training loss: 13114.25, average training loss: 12802.06, base loss: 25411.57
[INFO 2017-06-26 12:12:03,584 main.py:50] epoch 2667, training loss: 14138.07, average training loss: 12803.56, base loss: 25416.83
[INFO 2017-06-26 12:12:04,318 main.py:50] epoch 2668, training loss: 11830.82, average training loss: 12802.45, base loss: 25414.04
[INFO 2017-06-26 12:12:05,066 main.py:50] epoch 2669, training loss: 12920.20, average training loss: 12801.25, base loss: 25414.02
[INFO 2017-06-26 12:12:05,801 main.py:50] epoch 2670, training loss: 11478.60, average training loss: 12800.66, base loss: 25414.93
[INFO 2017-06-26 12:12:06,536 main.py:50] epoch 2671, training loss: 12579.11, average training loss: 12799.29, base loss: 25412.63
[INFO 2017-06-26 12:12:07,270 main.py:50] epoch 2672, training loss: 11878.99, average training loss: 12797.67, base loss: 25409.48
[INFO 2017-06-26 12:12:08,005 main.py:50] epoch 2673, training loss: 13057.75, average training loss: 12797.85, base loss: 25411.27
[INFO 2017-06-26 12:12:08,743 main.py:50] epoch 2674, training loss: 11764.34, average training loss: 12797.02, base loss: 25409.82
[INFO 2017-06-26 12:12:09,477 main.py:50] epoch 2675, training loss: 11662.21, average training loss: 12795.20, base loss: 25408.04
[INFO 2017-06-26 12:12:10,211 main.py:50] epoch 2676, training loss: 13458.61, average training loss: 12796.74, base loss: 25413.48
[INFO 2017-06-26 12:12:10,947 main.py:50] epoch 2677, training loss: 11947.73, average training loss: 12795.49, base loss: 25411.46
[INFO 2017-06-26 12:12:11,681 main.py:50] epoch 2678, training loss: 13513.24, average training loss: 12797.26, base loss: 25417.88
[INFO 2017-06-26 12:12:12,418 main.py:50] epoch 2679, training loss: 12140.50, average training loss: 12794.70, base loss: 25414.85
[INFO 2017-06-26 12:12:13,152 main.py:50] epoch 2680, training loss: 12158.04, average training loss: 12794.25, base loss: 25416.23
[INFO 2017-06-26 12:12:13,886 main.py:50] epoch 2681, training loss: 13149.35, average training loss: 12793.93, base loss: 25417.08
[INFO 2017-06-26 12:12:14,625 main.py:50] epoch 2682, training loss: 13624.55, average training loss: 12793.73, base loss: 25417.38
[INFO 2017-06-26 12:12:15,358 main.py:50] epoch 2683, training loss: 12805.43, average training loss: 12793.44, base loss: 25418.02
[INFO 2017-06-26 12:12:16,091 main.py:50] epoch 2684, training loss: 12675.65, average training loss: 12791.75, base loss: 25415.92
[INFO 2017-06-26 12:12:16,826 main.py:50] epoch 2685, training loss: 12828.01, average training loss: 12790.69, base loss: 25416.22
[INFO 2017-06-26 12:12:17,560 main.py:50] epoch 2686, training loss: 13569.04, average training loss: 12791.52, base loss: 25419.88
[INFO 2017-06-26 12:12:18,293 main.py:50] epoch 2687, training loss: 13463.61, average training loss: 12792.57, base loss: 25422.80
[INFO 2017-06-26 12:12:19,027 main.py:50] epoch 2688, training loss: 12350.87, average training loss: 12791.92, base loss: 25421.52
[INFO 2017-06-26 12:12:19,761 main.py:50] epoch 2689, training loss: 13118.63, average training loss: 12792.75, base loss: 25424.82
[INFO 2017-06-26 12:12:20,494 main.py:50] epoch 2690, training loss: 12118.95, average training loss: 12790.94, base loss: 25421.94
[INFO 2017-06-26 12:12:21,229 main.py:50] epoch 2691, training loss: 12326.80, average training loss: 12790.06, base loss: 25421.78
[INFO 2017-06-26 12:12:21,964 main.py:50] epoch 2692, training loss: 12927.24, average training loss: 12789.30, base loss: 25421.33
[INFO 2017-06-26 12:12:22,698 main.py:50] epoch 2693, training loss: 12376.69, average training loss: 12789.49, base loss: 25423.08
[INFO 2017-06-26 12:12:23,432 main.py:50] epoch 2694, training loss: 12879.58, average training loss: 12788.78, base loss: 25422.46
[INFO 2017-06-26 12:12:24,165 main.py:50] epoch 2695, training loss: 13033.54, average training loss: 12787.34, base loss: 25420.83
[INFO 2017-06-26 12:12:24,897 main.py:50] epoch 2696, training loss: 12266.69, average training loss: 12788.03, base loss: 25423.53
[INFO 2017-06-26 12:12:25,628 main.py:50] epoch 2697, training loss: 11652.52, average training loss: 12786.87, base loss: 25421.42
[INFO 2017-06-26 12:12:26,360 main.py:50] epoch 2698, training loss: 12289.16, average training loss: 12787.18, base loss: 25425.15
[INFO 2017-06-26 12:12:27,090 main.py:50] epoch 2699, training loss: 11428.50, average training loss: 12786.14, base loss: 25424.10
[INFO 2017-06-26 12:12:27,824 main.py:50] epoch 2700, training loss: 12021.59, average training loss: 12783.69, base loss: 25419.83
[INFO 2017-06-26 12:12:28,557 main.py:50] epoch 2701, training loss: 13858.72, average training loss: 12784.39, base loss: 25422.85
[INFO 2017-06-26 12:12:29,289 main.py:50] epoch 2702, training loss: 11391.49, average training loss: 12782.26, base loss: 25420.58
[INFO 2017-06-26 12:12:30,021 main.py:50] epoch 2703, training loss: 13996.20, average training loss: 12783.49, base loss: 25425.89
[INFO 2017-06-26 12:12:30,754 main.py:50] epoch 2704, training loss: 12339.21, average training loss: 12783.75, base loss: 25428.96
[INFO 2017-06-26 12:12:31,487 main.py:50] epoch 2705, training loss: 11977.89, average training loss: 12782.22, base loss: 25429.77
[INFO 2017-06-26 12:12:32,223 main.py:50] epoch 2706, training loss: 12028.46, average training loss: 12780.01, base loss: 25426.98
[INFO 2017-06-26 12:12:32,958 main.py:50] epoch 2707, training loss: 12230.69, average training loss: 12780.57, base loss: 25429.92
[INFO 2017-06-26 12:12:33,690 main.py:50] epoch 2708, training loss: 12065.06, average training loss: 12778.29, base loss: 25425.11
[INFO 2017-06-26 12:12:34,423 main.py:50] epoch 2709, training loss: 11782.67, average training loss: 12777.52, base loss: 25424.12
[INFO 2017-06-26 12:12:35,155 main.py:50] epoch 2710, training loss: 13255.50, average training loss: 12775.83, base loss: 25421.27
[INFO 2017-06-26 12:12:35,888 main.py:50] epoch 2711, training loss: 12110.86, average training loss: 12774.71, base loss: 25421.34
[INFO 2017-06-26 12:12:36,621 main.py:50] epoch 2712, training loss: 13606.68, average training loss: 12774.43, base loss: 25420.48
[INFO 2017-06-26 12:12:37,366 main.py:50] epoch 2713, training loss: 12127.46, average training loss: 12772.84, base loss: 25418.69
[INFO 2017-06-26 12:12:38,099 main.py:50] epoch 2714, training loss: 11988.02, average training loss: 12770.56, base loss: 25414.42
[INFO 2017-06-26 12:12:38,833 main.py:50] epoch 2715, training loss: 10991.14, average training loss: 12769.14, base loss: 25412.16
[INFO 2017-06-26 12:12:39,570 main.py:50] epoch 2716, training loss: 13040.38, average training loss: 12767.92, base loss: 25409.13
[INFO 2017-06-26 12:12:40,304 main.py:50] epoch 2717, training loss: 12089.32, average training loss: 12766.81, base loss: 25408.43
[INFO 2017-06-26 12:12:41,038 main.py:50] epoch 2718, training loss: 12376.27, average training loss: 12764.41, base loss: 25405.91
[INFO 2017-06-26 12:12:41,772 main.py:50] epoch 2719, training loss: 12213.83, average training loss: 12763.97, base loss: 25403.65
[INFO 2017-06-26 12:12:42,504 main.py:50] epoch 2720, training loss: 13487.05, average training loss: 12765.13, base loss: 25407.61
[INFO 2017-06-26 12:12:43,243 main.py:50] epoch 2721, training loss: 11845.40, average training loss: 12762.90, base loss: 25404.56
[INFO 2017-06-26 12:12:43,978 main.py:50] epoch 2722, training loss: 13081.52, average training loss: 12763.54, base loss: 25404.12
[INFO 2017-06-26 12:12:44,712 main.py:50] epoch 2723, training loss: 13256.81, average training loss: 12765.08, base loss: 25409.59
[INFO 2017-06-26 12:12:45,447 main.py:50] epoch 2724, training loss: 12246.07, average training loss: 12765.05, base loss: 25412.03
[INFO 2017-06-26 12:12:46,181 main.py:50] epoch 2725, training loss: 11734.82, average training loss: 12762.18, base loss: 25405.89
[INFO 2017-06-26 12:12:46,916 main.py:50] epoch 2726, training loss: 11692.77, average training loss: 12760.52, base loss: 25403.04
[INFO 2017-06-26 12:12:47,651 main.py:50] epoch 2727, training loss: 13458.07, average training loss: 12759.28, base loss: 25404.34
[INFO 2017-06-26 12:12:48,389 main.py:50] epoch 2728, training loss: 13565.22, average training loss: 12758.81, base loss: 25405.66
[INFO 2017-06-26 12:12:49,128 main.py:50] epoch 2729, training loss: 12857.70, average training loss: 12758.11, base loss: 25404.10
[INFO 2017-06-26 12:12:49,862 main.py:50] epoch 2730, training loss: 14361.56, average training loss: 12759.60, base loss: 25408.43
[INFO 2017-06-26 12:12:50,602 main.py:50] epoch 2731, training loss: 13237.37, average training loss: 12760.82, base loss: 25412.81
[INFO 2017-06-26 12:12:51,336 main.py:50] epoch 2732, training loss: 13480.80, average training loss: 12761.71, base loss: 25416.31
[INFO 2017-06-26 12:12:52,070 main.py:50] epoch 2733, training loss: 12285.89, average training loss: 12760.53, base loss: 25413.32
[INFO 2017-06-26 12:12:52,805 main.py:50] epoch 2734, training loss: 12310.71, average training loss: 12760.71, base loss: 25415.38
[INFO 2017-06-26 12:12:53,538 main.py:50] epoch 2735, training loss: 12881.22, average training loss: 12760.40, base loss: 25412.34
[INFO 2017-06-26 12:12:54,273 main.py:50] epoch 2736, training loss: 13035.39, average training loss: 12760.49, base loss: 25413.33
[INFO 2017-06-26 12:12:55,006 main.py:50] epoch 2737, training loss: 12473.69, average training loss: 12760.53, base loss: 25416.39
[INFO 2017-06-26 12:12:55,741 main.py:50] epoch 2738, training loss: 11572.88, average training loss: 12760.13, base loss: 25418.55
[INFO 2017-06-26 12:12:56,480 main.py:50] epoch 2739, training loss: 12289.72, average training loss: 12758.20, base loss: 25417.24
[INFO 2017-06-26 12:12:57,213 main.py:50] epoch 2740, training loss: 12737.86, average training loss: 12758.70, base loss: 25420.91
[INFO 2017-06-26 12:12:57,946 main.py:50] epoch 2741, training loss: 13588.23, average training loss: 12758.95, base loss: 25422.92
[INFO 2017-06-26 12:12:58,680 main.py:50] epoch 2742, training loss: 11928.59, average training loss: 12757.53, base loss: 25421.11
[INFO 2017-06-26 12:12:59,419 main.py:50] epoch 2743, training loss: 11625.41, average training loss: 12756.59, base loss: 25420.11
[INFO 2017-06-26 12:13:00,152 main.py:50] epoch 2744, training loss: 11877.79, average training loss: 12756.09, base loss: 25422.29
[INFO 2017-06-26 12:13:00,885 main.py:50] epoch 2745, training loss: 13880.14, average training loss: 12756.13, base loss: 25423.94
[INFO 2017-06-26 12:13:01,619 main.py:50] epoch 2746, training loss: 11659.31, average training loss: 12752.74, base loss: 25417.92
[INFO 2017-06-26 12:13:02,354 main.py:50] epoch 2747, training loss: 12365.59, average training loss: 12752.84, base loss: 25420.77
[INFO 2017-06-26 12:13:03,088 main.py:50] epoch 2748, training loss: 13741.17, average training loss: 12751.46, base loss: 25418.61
[INFO 2017-06-26 12:13:03,821 main.py:50] epoch 2749, training loss: 13727.35, average training loss: 12752.77, base loss: 25422.82
[INFO 2017-06-26 12:13:04,554 main.py:50] epoch 2750, training loss: 13624.39, average training loss: 12751.79, base loss: 25422.62
[INFO 2017-06-26 12:13:05,292 main.py:50] epoch 2751, training loss: 11804.16, average training loss: 12751.15, base loss: 25422.17
[INFO 2017-06-26 12:13:06,042 main.py:50] epoch 2752, training loss: 13409.31, average training loss: 12752.27, base loss: 25425.00
[INFO 2017-06-26 12:13:06,777 main.py:50] epoch 2753, training loss: 13406.26, average training loss: 12753.04, base loss: 25428.99
[INFO 2017-06-26 12:13:07,511 main.py:50] epoch 2754, training loss: 12442.36, average training loss: 12753.12, base loss: 25430.85
[INFO 2017-06-26 12:13:08,245 main.py:50] epoch 2755, training loss: 12655.93, average training loss: 12751.64, base loss: 25429.88
[INFO 2017-06-26 12:13:08,977 main.py:50] epoch 2756, training loss: 11988.25, average training loss: 12751.01, base loss: 25430.22
[INFO 2017-06-26 12:13:09,726 main.py:50] epoch 2757, training loss: 14394.78, average training loss: 12752.94, base loss: 25435.47
[INFO 2017-06-26 12:13:10,465 main.py:50] epoch 2758, training loss: 13285.31, average training loss: 12753.10, base loss: 25438.26
[INFO 2017-06-26 12:13:11,205 main.py:50] epoch 2759, training loss: 11170.82, average training loss: 12750.53, base loss: 25434.12
[INFO 2017-06-26 12:13:11,939 main.py:50] epoch 2760, training loss: 13085.18, average training loss: 12750.44, base loss: 25434.27
[INFO 2017-06-26 12:13:12,675 main.py:50] epoch 2761, training loss: 12294.79, average training loss: 12750.34, base loss: 25434.23
[INFO 2017-06-26 12:13:13,409 main.py:50] epoch 2762, training loss: 12281.70, average training loss: 12748.72, base loss: 25432.35
[INFO 2017-06-26 12:13:14,145 main.py:50] epoch 2763, training loss: 12818.39, average training loss: 12747.61, base loss: 25430.98
[INFO 2017-06-26 12:13:14,878 main.py:50] epoch 2764, training loss: 14091.84, average training loss: 12749.59, base loss: 25434.76
[INFO 2017-06-26 12:13:15,612 main.py:50] epoch 2765, training loss: 12176.47, average training loss: 12747.47, base loss: 25431.23
[INFO 2017-06-26 12:13:16,347 main.py:50] epoch 2766, training loss: 11831.48, average training loss: 12746.82, base loss: 25431.75
[INFO 2017-06-26 12:13:17,081 main.py:50] epoch 2767, training loss: 11698.03, average training loss: 12745.78, base loss: 25429.75
[INFO 2017-06-26 12:13:17,815 main.py:50] epoch 2768, training loss: 11011.77, average training loss: 12742.90, base loss: 25424.42
[INFO 2017-06-26 12:13:18,550 main.py:50] epoch 2769, training loss: 12202.04, average training loss: 12742.75, base loss: 25425.14
[INFO 2017-06-26 12:13:19,286 main.py:50] epoch 2770, training loss: 11584.45, average training loss: 12739.63, base loss: 25419.31
[INFO 2017-06-26 12:13:20,021 main.py:50] epoch 2771, training loss: 12757.37, average training loss: 12738.49, base loss: 25418.96
[INFO 2017-06-26 12:13:20,761 main.py:50] epoch 2772, training loss: 13889.08, average training loss: 12740.53, base loss: 25426.92
[INFO 2017-06-26 12:13:21,495 main.py:50] epoch 2773, training loss: 11602.03, average training loss: 12738.00, base loss: 25422.03
[INFO 2017-06-26 12:13:22,227 main.py:50] epoch 2774, training loss: 13547.06, average training loss: 12739.34, base loss: 25425.66
[INFO 2017-06-26 12:13:22,961 main.py:50] epoch 2775, training loss: 13829.80, average training loss: 12740.76, base loss: 25429.79
[INFO 2017-06-26 12:13:23,693 main.py:50] epoch 2776, training loss: 11585.31, average training loss: 12740.15, base loss: 25429.61
[INFO 2017-06-26 12:13:24,428 main.py:50] epoch 2777, training loss: 11591.11, average training loss: 12737.51, base loss: 25425.98
[INFO 2017-06-26 12:13:25,161 main.py:50] epoch 2778, training loss: 12485.87, average training loss: 12735.98, base loss: 25423.75
[INFO 2017-06-26 12:13:25,898 main.py:50] epoch 2779, training loss: 12359.61, average training loss: 12734.29, base loss: 25422.37
[INFO 2017-06-26 12:13:26,634 main.py:50] epoch 2780, training loss: 12298.12, average training loss: 12734.13, base loss: 25423.64
[INFO 2017-06-26 12:13:27,367 main.py:50] epoch 2781, training loss: 11767.89, average training loss: 12731.12, base loss: 25418.23
[INFO 2017-06-26 12:13:28,100 main.py:50] epoch 2782, training loss: 11790.85, average training loss: 12729.32, base loss: 25416.80
[INFO 2017-06-26 12:13:28,834 main.py:50] epoch 2783, training loss: 11965.87, average training loss: 12728.99, base loss: 25417.57
[INFO 2017-06-26 12:13:29,568 main.py:50] epoch 2784, training loss: 13439.05, average training loss: 12728.67, base loss: 25418.25
[INFO 2017-06-26 12:13:30,301 main.py:50] epoch 2785, training loss: 11697.97, average training loss: 12727.61, base loss: 25416.54
[INFO 2017-06-26 12:13:31,035 main.py:50] epoch 2786, training loss: 12091.87, average training loss: 12727.12, base loss: 25415.73
[INFO 2017-06-26 12:13:31,772 main.py:50] epoch 2787, training loss: 13316.82, average training loss: 12727.83, base loss: 25417.74
[INFO 2017-06-26 12:13:32,508 main.py:50] epoch 2788, training loss: 13225.80, average training loss: 12728.90, base loss: 25419.95
[INFO 2017-06-26 12:13:33,243 main.py:50] epoch 2789, training loss: 13361.04, average training loss: 12726.91, base loss: 25415.78
[INFO 2017-06-26 12:13:33,978 main.py:50] epoch 2790, training loss: 12068.57, average training loss: 12726.49, base loss: 25417.10
[INFO 2017-06-26 12:13:34,710 main.py:50] epoch 2791, training loss: 13892.88, average training loss: 12726.48, base loss: 25418.26
[INFO 2017-06-26 12:13:35,444 main.py:50] epoch 2792, training loss: 13118.74, average training loss: 12726.76, base loss: 25419.41
[INFO 2017-06-26 12:13:36,177 main.py:50] epoch 2793, training loss: 11979.13, average training loss: 12724.66, base loss: 25416.08
[INFO 2017-06-26 12:13:36,911 main.py:50] epoch 2794, training loss: 12406.17, average training loss: 12724.61, base loss: 25417.43
[INFO 2017-06-26 12:13:37,644 main.py:50] epoch 2795, training loss: 13828.60, average training loss: 12725.43, base loss: 25419.87
[INFO 2017-06-26 12:13:38,380 main.py:50] epoch 2796, training loss: 11832.33, average training loss: 12723.01, base loss: 25415.33
[INFO 2017-06-26 12:13:39,113 main.py:50] epoch 2797, training loss: 11776.69, average training loss: 12721.16, base loss: 25411.84
[INFO 2017-06-26 12:13:39,846 main.py:50] epoch 2798, training loss: 13503.29, average training loss: 12722.74, base loss: 25416.66
[INFO 2017-06-26 12:13:40,580 main.py:50] epoch 2799, training loss: 11262.82, average training loss: 12720.70, base loss: 25412.10
[INFO 2017-06-26 12:13:41,314 main.py:50] epoch 2800, training loss: 13608.70, average training loss: 12720.69, base loss: 25415.11
[INFO 2017-06-26 12:13:42,063 main.py:50] epoch 2801, training loss: 13609.66, average training loss: 12720.95, base loss: 25415.79
[INFO 2017-06-26 12:13:42,798 main.py:50] epoch 2802, training loss: 12233.45, average training loss: 12718.31, base loss: 25409.47
[INFO 2017-06-26 12:13:43,532 main.py:50] epoch 2803, training loss: 12166.83, average training loss: 12717.57, base loss: 25409.56
[INFO 2017-06-26 12:13:44,267 main.py:50] epoch 2804, training loss: 11408.97, average training loss: 12715.98, base loss: 25408.13
[INFO 2017-06-26 12:13:45,004 main.py:50] epoch 2805, training loss: 11930.48, average training loss: 12715.43, base loss: 25408.18
[INFO 2017-06-26 12:13:45,739 main.py:50] epoch 2806, training loss: 12000.23, average training loss: 12714.33, base loss: 25405.69
[INFO 2017-06-26 12:13:46,473 main.py:50] epoch 2807, training loss: 13206.81, average training loss: 12715.07, base loss: 25408.79
[INFO 2017-06-26 12:13:47,209 main.py:50] epoch 2808, training loss: 11970.73, average training loss: 12715.71, base loss: 25412.87
[INFO 2017-06-26 12:13:47,944 main.py:50] epoch 2809, training loss: 12025.24, average training loss: 12713.57, base loss: 25409.64
[INFO 2017-06-26 12:13:48,679 main.py:50] epoch 2810, training loss: 11570.54, average training loss: 12712.90, base loss: 25409.65
[INFO 2017-06-26 12:13:49,414 main.py:50] epoch 2811, training loss: 13421.90, average training loss: 12713.87, base loss: 25413.35
[INFO 2017-06-26 12:13:50,147 main.py:50] epoch 2812, training loss: 14164.16, average training loss: 12715.72, base loss: 25418.56
[INFO 2017-06-26 12:13:50,882 main.py:50] epoch 2813, training loss: 11786.35, average training loss: 12713.71, base loss: 25415.52
[INFO 2017-06-26 12:13:51,618 main.py:50] epoch 2814, training loss: 12347.34, average training loss: 12713.17, base loss: 25416.08
[INFO 2017-06-26 12:13:52,351 main.py:50] epoch 2815, training loss: 12072.70, average training loss: 12712.36, base loss: 25415.09
[INFO 2017-06-26 12:13:53,104 main.py:50] epoch 2816, training loss: 12060.42, average training loss: 12710.85, base loss: 25414.01
[INFO 2017-06-26 12:13:53,854 main.py:50] epoch 2817, training loss: 13423.50, average training loss: 12711.53, base loss: 25416.53
[INFO 2017-06-26 12:13:54,592 main.py:50] epoch 2818, training loss: 12995.23, average training loss: 12712.19, base loss: 25418.41
[INFO 2017-06-26 12:13:55,327 main.py:50] epoch 2819, training loss: 12705.01, average training loss: 12711.64, base loss: 25419.64
[INFO 2017-06-26 12:13:56,060 main.py:50] epoch 2820, training loss: 13155.86, average training loss: 12710.67, base loss: 25419.10
[INFO 2017-06-26 12:13:56,795 main.py:50] epoch 2821, training loss: 11987.27, average training loss: 12708.51, base loss: 25416.09
[INFO 2017-06-26 12:13:57,529 main.py:50] epoch 2822, training loss: 11459.66, average training loss: 12707.24, base loss: 25413.69
[INFO 2017-06-26 12:13:58,263 main.py:50] epoch 2823, training loss: 11562.59, average training loss: 12706.55, base loss: 25414.00
[INFO 2017-06-26 12:13:58,995 main.py:50] epoch 2824, training loss: 12140.91, average training loss: 12704.51, base loss: 25411.19
[INFO 2017-06-26 12:13:59,728 main.py:50] epoch 2825, training loss: 11850.22, average training loss: 12702.59, base loss: 25408.46
[INFO 2017-06-26 12:14:00,461 main.py:50] epoch 2826, training loss: 11725.39, average training loss: 12700.30, base loss: 25403.58
[INFO 2017-06-26 12:14:01,201 main.py:50] epoch 2827, training loss: 12305.73, average training loss: 12699.79, base loss: 25403.96
[INFO 2017-06-26 12:14:01,937 main.py:50] epoch 2828, training loss: 11805.58, average training loss: 12698.76, base loss: 25402.96
[INFO 2017-06-26 12:14:02,677 main.py:50] epoch 2829, training loss: 11788.83, average training loss: 12698.37, base loss: 25404.95
[INFO 2017-06-26 12:14:03,411 main.py:50] epoch 2830, training loss: 12053.38, average training loss: 12696.90, base loss: 25403.25
[INFO 2017-06-26 12:14:04,144 main.py:50] epoch 2831, training loss: 12356.08, average training loss: 12696.52, base loss: 25402.68
[INFO 2017-06-26 12:14:04,877 main.py:50] epoch 2832, training loss: 13268.38, average training loss: 12696.99, base loss: 25402.46
[INFO 2017-06-26 12:14:05,610 main.py:50] epoch 2833, training loss: 12257.38, average training loss: 12694.88, base loss: 25399.65
[INFO 2017-06-26 12:14:06,343 main.py:50] epoch 2834, training loss: 13528.35, average training loss: 12695.12, base loss: 25401.07
[INFO 2017-06-26 12:14:07,077 main.py:50] epoch 2835, training loss: 12025.61, average training loss: 12692.68, base loss: 25396.54
[INFO 2017-06-26 12:14:07,812 main.py:50] epoch 2836, training loss: 11833.04, average training loss: 12692.55, base loss: 25398.11
[INFO 2017-06-26 12:14:08,545 main.py:50] epoch 2837, training loss: 12906.20, average training loss: 12692.56, base loss: 25398.68
[INFO 2017-06-26 12:14:09,280 main.py:50] epoch 2838, training loss: 11467.23, average training loss: 12690.51, base loss: 25394.62
[INFO 2017-06-26 12:14:10,013 main.py:50] epoch 2839, training loss: 13421.11, average training loss: 12691.78, base loss: 25399.19
[INFO 2017-06-26 12:14:10,748 main.py:50] epoch 2840, training loss: 13380.10, average training loss: 12691.34, base loss: 25399.40
[INFO 2017-06-26 12:14:11,486 main.py:50] epoch 2841, training loss: 12157.24, average training loss: 12690.81, base loss: 25399.83
[INFO 2017-06-26 12:14:12,223 main.py:50] epoch 2842, training loss: 12915.14, average training loss: 12691.64, base loss: 25402.97
[INFO 2017-06-26 12:14:12,957 main.py:50] epoch 2843, training loss: 11370.86, average training loss: 12690.36, base loss: 25399.47
[INFO 2017-06-26 12:14:13,692 main.py:50] epoch 2844, training loss: 11751.63, average training loss: 12688.46, base loss: 25394.66
[INFO 2017-06-26 12:14:14,439 main.py:50] epoch 2845, training loss: 14022.07, average training loss: 12689.66, base loss: 25397.15
[INFO 2017-06-26 12:14:15,172 main.py:50] epoch 2846, training loss: 11802.97, average training loss: 12688.85, base loss: 25396.50
[INFO 2017-06-26 12:14:15,906 main.py:50] epoch 2847, training loss: 13364.75, average training loss: 12690.25, base loss: 25400.64
[INFO 2017-06-26 12:14:16,640 main.py:50] epoch 2848, training loss: 13914.89, average training loss: 12690.91, base loss: 25404.47
[INFO 2017-06-26 12:14:17,374 main.py:50] epoch 2849, training loss: 11597.67, average training loss: 12690.31, base loss: 25405.08
[INFO 2017-06-26 12:14:18,113 main.py:50] epoch 2850, training loss: 11998.42, average training loss: 12690.07, base loss: 25404.99
[INFO 2017-06-26 12:14:18,853 main.py:50] epoch 2851, training loss: 12881.47, average training loss: 12689.35, base loss: 25404.05
[INFO 2017-06-26 12:14:19,587 main.py:50] epoch 2852, training loss: 13763.72, average training loss: 12690.15, base loss: 25406.03
[INFO 2017-06-26 12:14:20,320 main.py:50] epoch 2853, training loss: 12739.34, average training loss: 12688.98, base loss: 25403.42
[INFO 2017-06-26 12:14:21,058 main.py:50] epoch 2854, training loss: 13845.34, average training loss: 12688.37, base loss: 25402.91
[INFO 2017-06-26 12:14:21,793 main.py:50] epoch 2855, training loss: 12582.48, average training loss: 12688.49, base loss: 25403.46
[INFO 2017-06-26 12:14:22,536 main.py:50] epoch 2856, training loss: 12681.15, average training loss: 12688.01, base loss: 25403.15
[INFO 2017-06-26 12:14:23,269 main.py:50] epoch 2857, training loss: 12299.71, average training loss: 12686.57, base loss: 25401.93
[INFO 2017-06-26 12:14:24,004 main.py:50] epoch 2858, training loss: 13473.75, average training loss: 12686.86, base loss: 25404.04
[INFO 2017-06-26 12:14:24,739 main.py:50] epoch 2859, training loss: 11318.82, average training loss: 12683.78, base loss: 25398.40
[INFO 2017-06-26 12:14:25,476 main.py:50] epoch 2860, training loss: 12059.10, average training loss: 12683.72, base loss: 25398.64
[INFO 2017-06-26 12:14:26,207 main.py:50] epoch 2861, training loss: 13455.07, average training loss: 12684.23, base loss: 25399.77
[INFO 2017-06-26 12:14:26,940 main.py:50] epoch 2862, training loss: 11967.33, average training loss: 12682.89, base loss: 25398.70
[INFO 2017-06-26 12:14:27,672 main.py:50] epoch 2863, training loss: 13882.17, average training loss: 12684.09, base loss: 25401.24
[INFO 2017-06-26 12:14:28,410 main.py:50] epoch 2864, training loss: 12103.59, average training loss: 12681.97, base loss: 25399.31
[INFO 2017-06-26 12:14:29,143 main.py:50] epoch 2865, training loss: 12919.65, average training loss: 12680.78, base loss: 25395.42
[INFO 2017-06-26 12:14:29,882 main.py:50] epoch 2866, training loss: 11978.88, average training loss: 12680.18, base loss: 25397.09
[INFO 2017-06-26 12:14:30,614 main.py:50] epoch 2867, training loss: 12513.99, average training loss: 12680.36, base loss: 25398.99
[INFO 2017-06-26 12:14:31,346 main.py:50] epoch 2868, training loss: 13353.16, average training loss: 12679.47, base loss: 25398.03
[INFO 2017-06-26 12:14:32,078 main.py:50] epoch 2869, training loss: 12013.32, average training loss: 12679.22, base loss: 25398.17
[INFO 2017-06-26 12:14:32,810 main.py:50] epoch 2870, training loss: 13094.73, average training loss: 12679.68, base loss: 25399.32
[INFO 2017-06-26 12:14:33,542 main.py:50] epoch 2871, training loss: 11995.76, average training loss: 12679.49, base loss: 25400.38
[INFO 2017-06-26 12:14:34,274 main.py:50] epoch 2872, training loss: 12135.36, average training loss: 12679.75, base loss: 25403.72
[INFO 2017-06-26 12:14:35,007 main.py:50] epoch 2873, training loss: 11883.10, average training loss: 12679.15, base loss: 25404.59
[INFO 2017-06-26 12:14:35,742 main.py:50] epoch 2874, training loss: 11549.95, average training loss: 12678.31, base loss: 25403.78
[INFO 2017-06-26 12:14:36,481 main.py:50] epoch 2875, training loss: 12916.59, average training loss: 12679.15, base loss: 25407.94
[INFO 2017-06-26 12:14:37,222 main.py:50] epoch 2876, training loss: 13551.41, average training loss: 12680.13, base loss: 25412.03
[INFO 2017-06-26 12:14:37,960 main.py:50] epoch 2877, training loss: 13771.36, average training loss: 12681.46, base loss: 25417.74
[INFO 2017-06-26 12:14:38,699 main.py:50] epoch 2878, training loss: 11428.84, average training loss: 12680.12, base loss: 25416.05
[INFO 2017-06-26 12:14:39,433 main.py:50] epoch 2879, training loss: 12947.06, average training loss: 12679.02, base loss: 25417.43
[INFO 2017-06-26 12:14:40,167 main.py:50] epoch 2880, training loss: 12116.00, average training loss: 12677.96, base loss: 25414.97
[INFO 2017-06-26 12:14:40,901 main.py:50] epoch 2881, training loss: 13646.96, average training loss: 12679.13, base loss: 25419.98
[INFO 2017-06-26 12:14:41,636 main.py:50] epoch 2882, training loss: 13095.55, average training loss: 12680.03, base loss: 25424.30
[INFO 2017-06-26 12:14:42,371 main.py:50] epoch 2883, training loss: 14020.23, average training loss: 12682.10, base loss: 25431.01
[INFO 2017-06-26 12:14:43,104 main.py:50] epoch 2884, training loss: 13424.38, average training loss: 12680.84, base loss: 25430.30
[INFO 2017-06-26 12:14:43,841 main.py:50] epoch 2885, training loss: 12844.04, average training loss: 12680.88, base loss: 25431.10
[INFO 2017-06-26 12:14:44,576 main.py:50] epoch 2886, training loss: 11314.76, average training loss: 12679.08, base loss: 25429.62
[INFO 2017-06-26 12:14:45,309 main.py:50] epoch 2887, training loss: 11581.37, average training loss: 12678.39, base loss: 25429.98
[INFO 2017-06-26 12:14:46,042 main.py:50] epoch 2888, training loss: 11738.51, average training loss: 12675.97, base loss: 25425.65
[INFO 2017-06-26 12:14:46,790 main.py:50] epoch 2889, training loss: 11870.04, average training loss: 12675.26, base loss: 25425.25
[INFO 2017-06-26 12:14:47,526 main.py:50] epoch 2890, training loss: 12046.90, average training loss: 12675.57, base loss: 25428.32
[INFO 2017-06-26 12:14:48,258 main.py:50] epoch 2891, training loss: 14193.06, average training loss: 12675.48, base loss: 25429.11
[INFO 2017-06-26 12:14:48,991 main.py:50] epoch 2892, training loss: 12119.97, average training loss: 12675.01, base loss: 25428.78
[INFO 2017-06-26 12:14:49,725 main.py:50] epoch 2893, training loss: 13077.75, average training loss: 12674.67, base loss: 25429.64
[INFO 2017-06-26 12:14:50,457 main.py:50] epoch 2894, training loss: 13196.06, average training loss: 12675.61, base loss: 25432.03
[INFO 2017-06-26 12:14:51,190 main.py:50] epoch 2895, training loss: 11424.14, average training loss: 12672.38, base loss: 25425.42
[INFO 2017-06-26 12:14:51,924 main.py:50] epoch 2896, training loss: 13579.71, average training loss: 12673.92, base loss: 25430.75
[INFO 2017-06-26 12:14:52,657 main.py:50] epoch 2897, training loss: 12327.77, average training loss: 12672.83, base loss: 25427.95
[INFO 2017-06-26 12:14:53,398 main.py:50] epoch 2898, training loss: 12156.87, average training loss: 12670.92, base loss: 25426.55
[INFO 2017-06-26 12:14:54,132 main.py:50] epoch 2899, training loss: 12251.91, average training loss: 12670.54, base loss: 25428.75
[INFO 2017-06-26 12:14:54,868 main.py:50] epoch 2900, training loss: 12160.31, average training loss: 12669.19, base loss: 25427.78
[INFO 2017-06-26 12:14:55,609 main.py:50] epoch 2901, training loss: 13733.13, average training loss: 12670.36, base loss: 25431.62
[INFO 2017-06-26 12:14:56,344 main.py:50] epoch 2902, training loss: 11547.91, average training loss: 12668.83, base loss: 25428.53
[INFO 2017-06-26 12:14:57,151 main.py:50] epoch 2903, training loss: 13214.10, average training loss: 12669.20, base loss: 25429.82
[INFO 2017-06-26 12:14:57,931 main.py:50] epoch 2904, training loss: 12368.70, average training loss: 12667.30, base loss: 25427.63
[INFO 2017-06-26 12:14:58,700 main.py:50] epoch 2905, training loss: 13704.32, average training loss: 12667.33, base loss: 25430.40
[INFO 2017-06-26 12:14:59,435 main.py:50] epoch 2906, training loss: 12452.80, average training loss: 12665.85, base loss: 25429.75
[INFO 2017-06-26 12:15:00,175 main.py:50] epoch 2907, training loss: 12188.08, average training loss: 12665.62, base loss: 25430.31
[INFO 2017-06-26 12:15:00,907 main.py:50] epoch 2908, training loss: 11500.01, average training loss: 12664.79, base loss: 25429.39
[INFO 2017-06-26 12:15:01,639 main.py:50] epoch 2909, training loss: 13879.32, average training loss: 12665.80, base loss: 25432.50
[INFO 2017-06-26 12:15:02,404 main.py:50] epoch 2910, training loss: 11575.43, average training loss: 12664.99, base loss: 25431.65
[INFO 2017-06-26 12:15:03,139 main.py:50] epoch 2911, training loss: 13355.78, average training loss: 12665.40, base loss: 25433.97
[INFO 2017-06-26 12:15:03,869 main.py:50] epoch 2912, training loss: 14153.77, average training loss: 12667.18, base loss: 25438.34
[INFO 2017-06-26 12:15:04,603 main.py:50] epoch 2913, training loss: 13552.09, average training loss: 12668.54, base loss: 25443.10
[INFO 2017-06-26 12:15:05,337 main.py:50] epoch 2914, training loss: 12151.91, average training loss: 12666.84, base loss: 25440.70
[INFO 2017-06-26 12:15:06,069 main.py:50] epoch 2915, training loss: 14140.21, average training loss: 12668.95, base loss: 25447.07
[INFO 2017-06-26 12:15:06,803 main.py:50] epoch 2916, training loss: 12343.77, average training loss: 12666.89, base loss: 25443.34
[INFO 2017-06-26 12:15:07,537 main.py:50] epoch 2917, training loss: 12448.68, average training loss: 12666.79, base loss: 25443.61
[INFO 2017-06-26 12:15:08,271 main.py:50] epoch 2918, training loss: 13610.89, average training loss: 12667.46, base loss: 25444.82
[INFO 2017-06-26 12:15:09,005 main.py:50] epoch 2919, training loss: 12173.21, average training loss: 12667.50, base loss: 25445.83
[INFO 2017-06-26 12:15:09,770 main.py:50] epoch 2920, training loss: 13275.98, average training loss: 12668.68, base loss: 25450.64
[INFO 2017-06-26 12:15:10,540 main.py:50] epoch 2921, training loss: 11894.68, average training loss: 12666.43, base loss: 25447.87
[INFO 2017-06-26 12:15:11,273 main.py:50] epoch 2922, training loss: 12828.03, average training loss: 12666.31, base loss: 25447.91
[INFO 2017-06-26 12:15:12,037 main.py:50] epoch 2923, training loss: 12250.10, average training loss: 12665.33, base loss: 25446.29
[INFO 2017-06-26 12:15:12,789 main.py:50] epoch 2924, training loss: 13441.65, average training loss: 12665.92, base loss: 25448.63
[INFO 2017-06-26 12:15:13,557 main.py:50] epoch 2925, training loss: 13598.14, average training loss: 12665.08, base loss: 25447.85
[INFO 2017-06-26 12:15:14,326 main.py:50] epoch 2926, training loss: 13287.83, average training loss: 12665.60, base loss: 25450.33
[INFO 2017-06-26 12:15:15,065 main.py:50] epoch 2927, training loss: 10967.98, average training loss: 12663.76, base loss: 25444.93
[INFO 2017-06-26 12:15:15,798 main.py:50] epoch 2928, training loss: 12039.78, average training loss: 12662.69, base loss: 25444.59
[INFO 2017-06-26 12:15:16,530 main.py:50] epoch 2929, training loss: 13367.45, average training loss: 12663.72, base loss: 25446.83
[INFO 2017-06-26 12:15:17,262 main.py:50] epoch 2930, training loss: 13281.50, average training loss: 12663.83, base loss: 25449.29
[INFO 2017-06-26 12:15:17,995 main.py:50] epoch 2931, training loss: 11901.72, average training loss: 12663.24, base loss: 25448.93
[INFO 2017-06-26 12:15:18,727 main.py:50] epoch 2932, training loss: 13741.40, average training loss: 12663.93, base loss: 25450.46
[INFO 2017-06-26 12:15:19,474 main.py:50] epoch 2933, training loss: 12465.81, average training loss: 12664.52, base loss: 25453.47
[INFO 2017-06-26 12:15:20,207 main.py:50] epoch 2934, training loss: 11450.93, average training loss: 12663.22, base loss: 25451.66
[INFO 2017-06-26 12:15:20,940 main.py:50] epoch 2935, training loss: 12515.04, average training loss: 12663.36, base loss: 25452.17
[INFO 2017-06-26 12:15:21,672 main.py:50] epoch 2936, training loss: 14257.36, average training loss: 12664.69, base loss: 25456.99
[INFO 2017-06-26 12:15:22,405 main.py:50] epoch 2937, training loss: 13718.92, average training loss: 12665.29, base loss: 25460.78
[INFO 2017-06-26 12:15:23,139 main.py:50] epoch 2938, training loss: 12535.48, average training loss: 12663.51, base loss: 25456.80
[INFO 2017-06-26 12:15:23,871 main.py:50] epoch 2939, training loss: 12252.23, average training loss: 12663.53, base loss: 25457.87
[INFO 2017-06-26 12:15:24,602 main.py:50] epoch 2940, training loss: 11646.67, average training loss: 12663.19, base loss: 25458.12
[INFO 2017-06-26 12:15:25,335 main.py:50] epoch 2941, training loss: 11976.08, average training loss: 12662.50, base loss: 25457.27
[INFO 2017-06-26 12:15:26,068 main.py:50] epoch 2942, training loss: 13104.00, average training loss: 12663.61, base loss: 25459.94
[INFO 2017-06-26 12:15:26,800 main.py:50] epoch 2943, training loss: 11869.45, average training loss: 12663.02, base loss: 25459.42
[INFO 2017-06-26 12:15:27,534 main.py:50] epoch 2944, training loss: 12283.32, average training loss: 12661.24, base loss: 25457.03
[INFO 2017-06-26 12:15:28,266 main.py:50] epoch 2945, training loss: 12587.66, average training loss: 12660.06, base loss: 25455.36
[INFO 2017-06-26 12:15:29,000 main.py:50] epoch 2946, training loss: 11544.46, average training loss: 12657.63, base loss: 25450.68
[INFO 2017-06-26 12:15:29,734 main.py:50] epoch 2947, training loss: 11515.27, average training loss: 12656.93, base loss: 25448.06
[INFO 2017-06-26 12:15:30,500 main.py:50] epoch 2948, training loss: 12009.73, average training loss: 12655.49, base loss: 25444.97
[INFO 2017-06-26 12:15:31,241 main.py:50] epoch 2949, training loss: 14355.02, average training loss: 12657.66, base loss: 25450.29
[INFO 2017-06-26 12:15:31,972 main.py:50] epoch 2950, training loss: 13797.23, average training loss: 12658.71, base loss: 25453.70
[INFO 2017-06-26 12:15:32,704 main.py:50] epoch 2951, training loss: 13719.56, average training loss: 12659.87, base loss: 25456.78
[INFO 2017-06-26 12:15:33,435 main.py:50] epoch 2952, training loss: 11871.20, average training loss: 12659.30, base loss: 25454.73
[INFO 2017-06-26 12:15:34,165 main.py:50] epoch 2953, training loss: 11541.90, average training loss: 12658.36, base loss: 25452.01
[INFO 2017-06-26 12:15:34,894 main.py:50] epoch 2954, training loss: 12895.24, average training loss: 12659.86, base loss: 25456.03
[INFO 2017-06-26 12:15:35,625 main.py:50] epoch 2955, training loss: 13994.88, average training loss: 12660.42, base loss: 25459.81
[INFO 2017-06-26 12:15:36,355 main.py:50] epoch 2956, training loss: 12297.08, average training loss: 12660.38, base loss: 25460.35
[INFO 2017-06-26 12:15:37,086 main.py:50] epoch 2957, training loss: 11690.13, average training loss: 12660.02, base loss: 25460.63
[INFO 2017-06-26 12:15:37,817 main.py:50] epoch 2958, training loss: 13080.99, average training loss: 12659.92, base loss: 25461.18
[INFO 2017-06-26 12:15:38,556 main.py:50] epoch 2959, training loss: 14541.71, average training loss: 12661.97, base loss: 25465.97
[INFO 2017-06-26 12:15:39,315 main.py:50] epoch 2960, training loss: 14167.11, average training loss: 12664.28, base loss: 25473.02
[INFO 2017-06-26 12:15:40,055 main.py:50] epoch 2961, training loss: 12516.12, average training loss: 12662.21, base loss: 25469.22
[INFO 2017-06-26 12:15:40,820 main.py:50] epoch 2962, training loss: 12302.53, average training loss: 12661.88, base loss: 25469.82
[INFO 2017-06-26 12:15:41,607 main.py:50] epoch 2963, training loss: 13174.44, average training loss: 12662.59, base loss: 25473.17
[INFO 2017-06-26 12:15:42,375 main.py:50] epoch 2964, training loss: 12078.15, average training loss: 12663.42, base loss: 25477.63
[INFO 2017-06-26 12:15:43,183 main.py:50] epoch 2965, training loss: 11640.91, average training loss: 12662.03, base loss: 25475.31
[INFO 2017-06-26 12:15:43,982 main.py:50] epoch 2966, training loss: 12151.53, average training loss: 12660.70, base loss: 25474.48
[INFO 2017-06-26 12:15:44,717 main.py:50] epoch 2967, training loss: 11979.55, average training loss: 12659.53, base loss: 25473.26
[INFO 2017-06-26 12:15:45,448 main.py:50] epoch 2968, training loss: 13274.66, average training loss: 12659.43, base loss: 25473.90
[INFO 2017-06-26 12:15:46,177 main.py:50] epoch 2969, training loss: 12289.24, average training loss: 12659.08, base loss: 25473.69
[INFO 2017-06-26 12:15:46,909 main.py:50] epoch 2970, training loss: 13218.04, average training loss: 12658.88, base loss: 25475.62
[INFO 2017-06-26 12:15:47,642 main.py:50] epoch 2971, training loss: 11972.84, average training loss: 12658.09, base loss: 25475.60
[INFO 2017-06-26 12:15:48,374 main.py:50] epoch 2972, training loss: 11940.30, average training loss: 12656.20, base loss: 25471.96
[INFO 2017-06-26 12:15:49,106 main.py:50] epoch 2973, training loss: 12437.12, average training loss: 12655.91, base loss: 25471.79
[INFO 2017-06-26 12:15:49,836 main.py:50] epoch 2974, training loss: 12228.91, average training loss: 12654.41, base loss: 25469.96
[INFO 2017-06-26 12:15:50,568 main.py:50] epoch 2975, training loss: 11916.04, average training loss: 12652.18, base loss: 25465.07
[INFO 2017-06-26 12:15:51,299 main.py:50] epoch 2976, training loss: 12335.83, average training loss: 12652.37, base loss: 25467.12
[INFO 2017-06-26 12:15:52,043 main.py:50] epoch 2977, training loss: 12663.71, average training loss: 12651.28, base loss: 25464.95
[INFO 2017-06-26 12:15:52,775 main.py:50] epoch 2978, training loss: 12397.60, average training loss: 12649.81, base loss: 25463.62
[INFO 2017-06-26 12:15:53,505 main.py:50] epoch 2979, training loss: 13299.26, average training loss: 12650.44, base loss: 25464.65
[INFO 2017-06-26 12:15:54,269 main.py:50] epoch 2980, training loss: 13574.02, average training loss: 12650.57, base loss: 25465.53
[INFO 2017-06-26 12:15:55,003 main.py:50] epoch 2981, training loss: 13217.51, average training loss: 12652.24, base loss: 25469.88
[INFO 2017-06-26 12:15:55,734 main.py:50] epoch 2982, training loss: 11759.96, average training loss: 12651.02, base loss: 25466.92
[INFO 2017-06-26 12:15:56,468 main.py:50] epoch 2983, training loss: 12207.66, average training loss: 12651.62, base loss: 25471.09
[INFO 2017-06-26 12:15:57,199 main.py:50] epoch 2984, training loss: 12348.85, average training loss: 12651.34, base loss: 25471.11
[INFO 2017-06-26 12:15:57,932 main.py:50] epoch 2985, training loss: 12181.35, average training loss: 12650.10, base loss: 25470.53
[INFO 2017-06-26 12:15:58,663 main.py:50] epoch 2986, training loss: 12031.85, average training loss: 12649.38, base loss: 25470.88
[INFO 2017-06-26 12:15:59,396 main.py:50] epoch 2987, training loss: 11816.93, average training loss: 12648.64, base loss: 25468.86
[INFO 2017-06-26 12:16:00,126 main.py:50] epoch 2988, training loss: 12055.01, average training loss: 12648.06, base loss: 25468.42
[INFO 2017-06-26 12:16:00,860 main.py:50] epoch 2989, training loss: 12081.22, average training loss: 12647.76, base loss: 25469.80
[INFO 2017-06-26 12:16:01,624 main.py:50] epoch 2990, training loss: 13635.57, average training loss: 12648.22, base loss: 25470.78
[INFO 2017-06-26 12:16:02,389 main.py:50] epoch 2991, training loss: 13477.66, average training loss: 12649.62, base loss: 25474.04
[INFO 2017-06-26 12:16:03,154 main.py:50] epoch 2992, training loss: 13198.66, average training loss: 12650.90, base loss: 25476.90
[INFO 2017-06-26 12:16:03,888 main.py:50] epoch 2993, training loss: 12319.48, average training loss: 12650.27, base loss: 25476.62
[INFO 2017-06-26 12:16:04,690 main.py:50] epoch 2994, training loss: 11662.77, average training loss: 12649.94, base loss: 25476.28
[INFO 2017-06-26 12:16:05,481 main.py:50] epoch 2995, training loss: 11905.40, average training loss: 12648.21, base loss: 25474.43
[INFO 2017-06-26 12:16:06,215 main.py:50] epoch 2996, training loss: 12350.54, average training loss: 12649.30, base loss: 25479.40
[INFO 2017-06-26 12:16:06,979 main.py:50] epoch 2997, training loss: 12250.22, average training loss: 12649.83, base loss: 25483.86
[INFO 2017-06-26 12:16:07,717 main.py:50] epoch 2998, training loss: 12981.47, average training loss: 12649.01, base loss: 25481.94
[INFO 2017-06-26 12:16:08,448 main.py:50] epoch 2999, training loss: 12635.74, average training loss: 12648.40, base loss: 25480.16
[INFO 2017-06-26 12:16:08,448 main.py:52] epoch 2999, testing
[INFO 2017-06-26 12:16:36,839 main.py:103] average testing loss: 12450.03, base loss: 25414.19
[INFO 2017-06-26 12:16:36,840 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 12:16:36,846 main.py:76] current best accuracy: 12450.03
[INFO 2017-06-26 12:16:37,613 main.py:50] epoch 3000, training loss: 11417.82, average training loss: 12647.00, base loss: 25477.83
[INFO 2017-06-26 12:16:38,350 main.py:50] epoch 3001, training loss: 13312.59, average training loss: 12646.11, base loss: 25475.43
[INFO 2017-06-26 12:16:39,078 main.py:50] epoch 3002, training loss: 13790.32, average training loss: 12645.97, base loss: 25476.19
[INFO 2017-06-26 12:16:39,808 main.py:50] epoch 3003, training loss: 12056.12, average training loss: 12645.52, base loss: 25474.99
[INFO 2017-06-26 12:16:40,537 main.py:50] epoch 3004, training loss: 13178.60, average training loss: 12644.68, base loss: 25472.81
[INFO 2017-06-26 12:16:41,267 main.py:50] epoch 3005, training loss: 12757.07, average training loss: 12645.02, base loss: 25473.41
[INFO 2017-06-26 12:16:41,999 main.py:50] epoch 3006, training loss: 11852.50, average training loss: 12644.45, base loss: 25472.90
[INFO 2017-06-26 12:16:42,728 main.py:50] epoch 3007, training loss: 11564.85, average training loss: 12643.84, base loss: 25472.09
[INFO 2017-06-26 12:16:43,471 main.py:50] epoch 3008, training loss: 11459.84, average training loss: 12642.73, base loss: 25470.18
[INFO 2017-06-26 12:16:44,203 main.py:50] epoch 3009, training loss: 11549.00, average training loss: 12641.36, base loss: 25467.78
[INFO 2017-06-26 12:16:44,969 main.py:50] epoch 3010, training loss: 13131.14, average training loss: 12639.90, base loss: 25464.19
[INFO 2017-06-26 12:16:45,719 main.py:50] epoch 3011, training loss: 13753.66, average training loss: 12640.36, base loss: 25467.90
[INFO 2017-06-26 12:16:46,450 main.py:50] epoch 3012, training loss: 13736.12, average training loss: 12641.70, base loss: 25471.20
[INFO 2017-06-26 12:16:47,180 main.py:50] epoch 3013, training loss: 11594.09, average training loss: 12641.51, base loss: 25470.52
[INFO 2017-06-26 12:16:47,911 main.py:50] epoch 3014, training loss: 12079.18, average training loss: 12640.04, base loss: 25467.90
[INFO 2017-06-26 12:16:48,642 main.py:50] epoch 3015, training loss: 11948.52, average training loss: 12639.61, base loss: 25469.48
[INFO 2017-06-26 12:16:49,373 main.py:50] epoch 3016, training loss: 12701.16, average training loss: 12639.87, base loss: 25471.09
