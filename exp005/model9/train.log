[INFO 2017-07-01 18:43:08,263 main.py:175] Namespace(batch_size=32, display=False, flow_dir='flow', flow_video_dir='flow-video', flow_video_fps=1, image_size=64, init_model_path='', learning_rate=0.001, method='unsupervised', motion_range=4, num_channel=3, num_inputs=2, save_dir='./model', test=False, test_dir='/home/yi/Downloads/mpii-test-64', test_epoch=10, test_interval=100, test_video=False, train=True, train_dir='/home/yi/Downloads/mpii-64', train_epoch=100000)
[INFO 2017-07-01 18:43:14,414 main.py:52] epoch 0, training loss: 41713.17, average training loss: 41713.17, base loss: 16262.62
[INFO 2017-07-01 18:43:17,793 main.py:52] epoch 1, training loss: 32746.91, average training loss: 37230.04, base loss: 15849.46
[INFO 2017-07-01 18:43:21,272 main.py:52] epoch 2, training loss: 28414.40, average training loss: 34291.49, base loss: 15426.84
[INFO 2017-07-01 18:43:24,703 main.py:52] epoch 3, training loss: 24347.02, average training loss: 31805.37, base loss: 15467.67
[INFO 2017-07-01 18:43:28,120 main.py:52] epoch 4, training loss: 21492.15, average training loss: 29742.73, base loss: 15336.94
[INFO 2017-07-01 18:43:31,502 main.py:52] epoch 5, training loss: 17991.81, average training loss: 27784.24, base loss: 15102.63
[INFO 2017-07-01 18:43:34,916 main.py:52] epoch 6, training loss: 17110.50, average training loss: 26259.42, base loss: 15152.54
[INFO 2017-07-01 18:43:38,759 main.py:52] epoch 7, training loss: 17005.93, average training loss: 25102.74, base loss: 15248.47
[INFO 2017-07-01 18:43:43,013 main.py:52] epoch 8, training loss: 16942.35, average training loss: 24196.03, base loss: 15440.28
[INFO 2017-07-01 18:43:47,170 main.py:52] epoch 9, training loss: 15140.39, average training loss: 23290.46, base loss: 15538.42
[INFO 2017-07-01 18:43:51,348 main.py:52] epoch 10, training loss: 14006.04, average training loss: 22446.42, base loss: 15527.19
[INFO 2017-07-01 18:43:55,483 main.py:52] epoch 11, training loss: 11803.10, average training loss: 21559.48, base loss: 15287.07
[INFO 2017-07-01 18:43:59,667 main.py:52] epoch 12, training loss: 12553.29, average training loss: 20866.70, base loss: 15355.72
[INFO 2017-07-01 18:44:03,872 main.py:52] epoch 13, training loss: 12723.64, average training loss: 20285.05, base loss: 15438.03
[INFO 2017-07-01 18:44:08,019 main.py:52] epoch 14, training loss: 11700.71, average training loss: 19712.76, base loss: 15482.39
[INFO 2017-07-01 18:44:12,169 main.py:52] epoch 15, training loss: 10817.53, average training loss: 19156.81, base loss: 15448.88
[INFO 2017-07-01 18:44:16,315 main.py:52] epoch 16, training loss: 11575.62, average training loss: 18710.86, base loss: 15501.97
[INFO 2017-07-01 18:44:20,571 main.py:52] epoch 17, training loss: 10619.96, average training loss: 18261.36, base loss: 15472.19
[INFO 2017-07-01 18:44:24,812 main.py:52] epoch 18, training loss: 11673.63, average training loss: 17914.64, base loss: 15538.06
[INFO 2017-07-01 18:44:29,015 main.py:52] epoch 19, training loss: 11570.63, average training loss: 17597.44, base loss: 15578.12
[INFO 2017-07-01 18:44:33,200 main.py:52] epoch 20, training loss: 11003.40, average training loss: 17283.44, base loss: 15618.18
[INFO 2017-07-01 18:44:37,375 main.py:52] epoch 21, training loss: 11423.95, average training loss: 17017.10, base loss: 15645.41
[INFO 2017-07-01 18:44:41,512 main.py:52] epoch 22, training loss: 12210.66, average training loss: 16808.12, base loss: 15730.25
[INFO 2017-07-01 18:44:45,625 main.py:52] epoch 23, training loss: 11391.92, average training loss: 16582.45, base loss: 15751.77
[INFO 2017-07-01 18:44:49,881 main.py:52] epoch 24, training loss: 9006.91, average training loss: 16279.42, base loss: 15643.76
[INFO 2017-07-01 18:44:54,093 main.py:52] epoch 25, training loss: 9643.41, average training loss: 16024.19, base loss: 15602.86
[INFO 2017-07-01 18:44:58,204 main.py:52] epoch 26, training loss: 11554.07, average training loss: 15858.63, base loss: 15644.75
[INFO 2017-07-01 18:45:02,351 main.py:52] epoch 27, training loss: 10844.66, average training loss: 15679.56, base loss: 15646.55
[INFO 2017-07-01 18:45:06,482 main.py:52] epoch 28, training loss: 10425.85, average training loss: 15498.40, base loss: 15648.55
[INFO 2017-07-01 18:45:10,711 main.py:52] epoch 29, training loss: 10276.42, average training loss: 15324.33, base loss: 15644.67
[INFO 2017-07-01 18:45:14,783 main.py:52] epoch 30, training loss: 10135.11, average training loss: 15156.94, base loss: 15617.63
[INFO 2017-07-01 18:45:18,997 main.py:52] epoch 31, training loss: 9971.34, average training loss: 14994.89, base loss: 15581.69
[INFO 2017-07-01 18:45:23,181 main.py:52] epoch 32, training loss: 9139.45, average training loss: 14817.45, base loss: 15528.15
[INFO 2017-07-01 18:45:27,328 main.py:52] epoch 33, training loss: 10140.92, average training loss: 14679.91, base loss: 15509.12
[INFO 2017-07-01 18:45:31,499 main.py:52] epoch 34, training loss: 9921.71, average training loss: 14543.96, base loss: 15493.46
[INFO 2017-07-01 18:45:35,631 main.py:52] epoch 35, training loss: 11206.86, average training loss: 14451.26, base loss: 15510.21
[INFO 2017-07-01 18:45:39,853 main.py:52] epoch 36, training loss: 8896.43, average training loss: 14301.13, base loss: 15451.74
[INFO 2017-07-01 18:45:44,033 main.py:52] epoch 37, training loss: 10384.94, average training loss: 14198.07, base loss: 15444.12
[INFO 2017-07-01 18:45:48,253 main.py:52] epoch 38, training loss: 10621.73, average training loss: 14106.37, base loss: 15454.36
[INFO 2017-07-01 18:45:52,441 main.py:52] epoch 39, training loss: 9354.96, average training loss: 13987.59, base loss: 15419.91
[INFO 2017-07-01 18:45:56,606 main.py:52] epoch 40, training loss: 11786.07, average training loss: 13933.89, base loss: 15461.14
[INFO 2017-07-01 18:46:00,757 main.py:52] epoch 41, training loss: 10363.75, average training loss: 13848.89, base loss: 15465.09
[INFO 2017-07-01 18:46:04,970 main.py:52] epoch 42, training loss: 9635.31, average training loss: 13750.90, base loss: 15457.41
[INFO 2017-07-01 18:46:09,046 main.py:52] epoch 43, training loss: 11391.93, average training loss: 13697.28, base loss: 15490.39
[INFO 2017-07-01 18:46:13,206 main.py:52] epoch 44, training loss: 10023.29, average training loss: 13615.64, base loss: 15492.46
[INFO 2017-07-01 18:46:17,342 main.py:52] epoch 45, training loss: 9637.05, average training loss: 13529.15, base loss: 15478.72
[INFO 2017-07-01 18:46:21,467 main.py:52] epoch 46, training loss: 9253.49, average training loss: 13438.18, base loss: 15451.06
[INFO 2017-07-01 18:46:25,602 main.py:52] epoch 47, training loss: 8568.00, average training loss: 13336.72, base loss: 15407.61
[INFO 2017-07-01 18:46:29,818 main.py:52] epoch 48, training loss: 11896.04, average training loss: 13307.31, base loss: 15446.27
[INFO 2017-07-01 18:46:34,004 main.py:52] epoch 49, training loss: 11926.51, average training loss: 13279.70, base loss: 15492.36
[INFO 2017-07-01 18:46:38,128 main.py:52] epoch 50, training loss: 9275.56, average training loss: 13201.19, base loss: 15460.80
[INFO 2017-07-01 18:46:42,391 main.py:52] epoch 51, training loss: 11070.92, average training loss: 13160.22, base loss: 15483.17
[INFO 2017-07-01 18:46:46,578 main.py:52] epoch 52, training loss: 11497.34, average training loss: 13128.84, base loss: 15515.49
[INFO 2017-07-01 18:46:50,726 main.py:52] epoch 53, training loss: 9999.43, average training loss: 13070.89, base loss: 15510.23
[INFO 2017-07-01 18:46:54,847 main.py:52] epoch 54, training loss: 9795.48, average training loss: 13011.34, base loss: 15500.01
[INFO 2017-07-01 18:46:59,017 main.py:52] epoch 55, training loss: 10832.76, average training loss: 12972.44, base loss: 15513.50
[INFO 2017-07-01 18:47:03,281 main.py:52] epoch 56, training loss: 10488.83, average training loss: 12928.86, base loss: 15508.00
[INFO 2017-07-01 18:47:07,467 main.py:52] epoch 57, training loss: 9956.56, average training loss: 12877.62, base loss: 15503.40
[INFO 2017-07-01 18:47:11,585 main.py:52] epoch 58, training loss: 9481.20, average training loss: 12820.05, base loss: 15487.46
[INFO 2017-07-01 18:47:15,779 main.py:52] epoch 59, training loss: 10314.61, average training loss: 12778.29, base loss: 15485.43
[INFO 2017-07-01 18:47:19,955 main.py:52] epoch 60, training loss: 8877.96, average training loss: 12714.35, base loss: 15467.69
[INFO 2017-07-01 18:47:24,110 main.py:52] epoch 61, training loss: 10023.28, average training loss: 12670.95, base loss: 15461.25
[INFO 2017-07-01 18:47:28,304 main.py:52] epoch 62, training loss: 9716.66, average training loss: 12624.06, base loss: 15455.79
[INFO 2017-07-01 18:47:32,473 main.py:52] epoch 63, training loss: 9571.63, average training loss: 12576.36, base loss: 15438.22
[INFO 2017-07-01 18:47:36,779 main.py:52] epoch 64, training loss: 10235.79, average training loss: 12540.35, base loss: 15439.51
[INFO 2017-07-01 18:47:40,934 main.py:52] epoch 65, training loss: 9510.44, average training loss: 12494.44, base loss: 15428.84
[INFO 2017-07-01 18:47:45,127 main.py:52] epoch 66, training loss: 9693.71, average training loss: 12452.64, base loss: 15420.56
[INFO 2017-07-01 18:47:49,317 main.py:52] epoch 67, training loss: 10433.21, average training loss: 12422.95, base loss: 15441.61
[INFO 2017-07-01 18:47:53,538 main.py:52] epoch 68, training loss: 9253.41, average training loss: 12377.01, base loss: 15428.19
[INFO 2017-07-01 18:47:57,675 main.py:52] epoch 69, training loss: 9296.11, average training loss: 12333.00, base loss: 15415.78
[INFO 2017-07-01 18:48:01,864 main.py:52] epoch 70, training loss: 9853.05, average training loss: 12298.07, base loss: 15413.19
[INFO 2017-07-01 18:48:06,024 main.py:52] epoch 71, training loss: 8945.41, average training loss: 12251.50, base loss: 15392.37
[INFO 2017-07-01 18:48:10,200 main.py:52] epoch 72, training loss: 10662.69, average training loss: 12229.74, base loss: 15408.24
[INFO 2017-07-01 18:48:14,321 main.py:52] epoch 73, training loss: 9467.03, average training loss: 12192.40, base loss: 15398.89
[INFO 2017-07-01 18:48:18,434 main.py:52] epoch 74, training loss: 9154.50, average training loss: 12151.90, base loss: 15392.00
[INFO 2017-07-01 18:48:22,618 main.py:52] epoch 75, training loss: 10158.78, average training loss: 12125.67, base loss: 15405.46
[INFO 2017-07-01 18:48:26,708 main.py:52] epoch 76, training loss: 9493.26, average training loss: 12091.49, base loss: 15407.70
[INFO 2017-07-01 18:48:30,865 main.py:52] epoch 77, training loss: 8311.66, average training loss: 12043.03, base loss: 15381.35
[INFO 2017-07-01 18:48:35,065 main.py:52] epoch 78, training loss: 9281.77, average training loss: 12008.08, base loss: 15375.60
[INFO 2017-07-01 18:48:39,175 main.py:52] epoch 79, training loss: 10175.03, average training loss: 11985.16, base loss: 15384.58
[INFO 2017-07-01 18:48:43,340 main.py:52] epoch 80, training loss: 9389.34, average training loss: 11953.11, base loss: 15376.49
[INFO 2017-07-01 18:48:47,511 main.py:52] epoch 81, training loss: 10063.29, average training loss: 11930.07, base loss: 15380.53
[INFO 2017-07-01 18:48:51,674 main.py:52] epoch 82, training loss: 9817.91, average training loss: 11904.62, base loss: 15377.19
[INFO 2017-07-01 18:48:55,888 main.py:52] epoch 83, training loss: 9346.35, average training loss: 11874.16, base loss: 15369.93
[INFO 2017-07-01 18:49:00,064 main.py:52] epoch 84, training loss: 9497.32, average training loss: 11846.20, base loss: 15362.62
[INFO 2017-07-01 18:49:04,221 main.py:52] epoch 85, training loss: 9463.04, average training loss: 11818.49, base loss: 15368.37
[INFO 2017-07-01 18:49:08,396 main.py:52] epoch 86, training loss: 9881.72, average training loss: 11796.23, base loss: 15381.70
[INFO 2017-07-01 18:49:12,538 main.py:52] epoch 87, training loss: 9599.38, average training loss: 11771.26, base loss: 15379.94
[INFO 2017-07-01 18:49:16,702 main.py:52] epoch 88, training loss: 9740.89, average training loss: 11748.45, base loss: 15383.17
[INFO 2017-07-01 18:49:20,892 main.py:52] epoch 89, training loss: 9683.04, average training loss: 11725.50, base loss: 15391.95
[INFO 2017-07-01 18:49:25,112 main.py:52] epoch 90, training loss: 10359.68, average training loss: 11710.49, base loss: 15412.29
[INFO 2017-07-01 18:49:29,333 main.py:52] epoch 91, training loss: 9522.47, average training loss: 11686.71, base loss: 15405.63
[INFO 2017-07-01 18:49:33,494 main.py:52] epoch 92, training loss: 9724.70, average training loss: 11665.61, base loss: 15405.90
[INFO 2017-07-01 18:49:37,671 main.py:52] epoch 93, training loss: 9524.29, average training loss: 11642.83, base loss: 15403.61
[INFO 2017-07-01 18:49:41,797 main.py:52] epoch 94, training loss: 8675.50, average training loss: 11611.60, base loss: 15381.97
[INFO 2017-07-01 18:49:46,004 main.py:52] epoch 95, training loss: 9172.54, average training loss: 11586.19, base loss: 15387.18
[INFO 2017-07-01 18:49:50,213 main.py:52] epoch 96, training loss: 10350.32, average training loss: 11573.45, base loss: 15401.51
[INFO 2017-07-01 18:49:54,327 main.py:52] epoch 97, training loss: 9641.88, average training loss: 11553.74, base loss: 15402.05
[INFO 2017-07-01 18:49:58,498 main.py:52] epoch 98, training loss: 9014.52, average training loss: 11528.09, base loss: 15392.67
[INFO 2017-07-01 18:50:02,693 main.py:52] epoch 99, training loss: 9318.51, average training loss: 11506.00, base loss: 15394.00
[INFO 2017-07-01 18:50:02,693 main.py:54] epoch 99, testing
[INFO 2017-07-01 18:50:02,693 main.py:61] model save to ./model/final.pth
[INFO 2017-07-01 18:50:06,954 main.py:52] epoch 100, training loss: 8302.87, average training loss: 11474.28, base loss: 15380.61
[INFO 2017-07-01 18:50:11,160 main.py:52] epoch 101, training loss: 10185.01, average training loss: 11461.64, base loss: 15390.06
[INFO 2017-07-01 18:50:15,349 main.py:52] epoch 102, training loss: 11370.54, average training loss: 11460.76, base loss: 15418.94
[INFO 2017-07-01 18:50:19,508 main.py:52] epoch 103, training loss: 8760.77, average training loss: 11434.80, base loss: 15410.16
[INFO 2017-07-01 18:50:23,668 main.py:52] epoch 104, training loss: 9857.27, average training loss: 11419.77, base loss: 15416.87
[INFO 2017-07-01 18:50:27,799 main.py:52] epoch 105, training loss: 8980.93, average training loss: 11396.76, base loss: 15409.27
[INFO 2017-07-01 18:50:32,003 main.py:52] epoch 106, training loss: 9254.96, average training loss: 11376.75, base loss: 15406.30
[INFO 2017-07-01 18:50:36,196 main.py:52] epoch 107, training loss: 8764.89, average training loss: 11352.56, base loss: 15394.97
[INFO 2017-07-01 18:50:40,418 main.py:52] epoch 108, training loss: 8713.86, average training loss: 11328.36, base loss: 15384.04
[INFO 2017-07-01 18:50:44,574 main.py:52] epoch 109, training loss: 9778.58, average training loss: 11314.27, base loss: 15388.42
[INFO 2017-07-01 18:50:48,780 main.py:52] epoch 110, training loss: 9595.08, average training loss: 11298.78, base loss: 15388.62
[INFO 2017-07-01 18:50:52,935 main.py:52] epoch 111, training loss: 10656.61, average training loss: 11293.04, base loss: 15401.70
[INFO 2017-07-01 18:50:57,092 main.py:52] epoch 112, training loss: 9269.35, average training loss: 11275.14, base loss: 15395.97
[INFO 2017-07-01 18:51:01,291 main.py:52] epoch 113, training loss: 9369.68, average training loss: 11258.42, base loss: 15400.53
[INFO 2017-07-01 18:51:05,397 main.py:52] epoch 114, training loss: 9037.12, average training loss: 11239.11, base loss: 15393.18
[INFO 2017-07-01 18:51:09,575 main.py:52] epoch 115, training loss: 9236.19, average training loss: 11221.84, base loss: 15389.54
[INFO 2017-07-01 18:51:13,690 main.py:52] epoch 116, training loss: 9032.30, average training loss: 11203.13, base loss: 15390.19
[INFO 2017-07-01 18:51:17,868 main.py:52] epoch 117, training loss: 8055.61, average training loss: 11176.45, base loss: 15372.37
[INFO 2017-07-01 18:51:22,043 main.py:52] epoch 118, training loss: 8717.66, average training loss: 11155.79, base loss: 15363.25
[INFO 2017-07-01 18:51:26,163 main.py:52] epoch 119, training loss: 9775.36, average training loss: 11144.29, base loss: 15375.53
[INFO 2017-07-01 18:51:30,292 main.py:52] epoch 120, training loss: 8018.67, average training loss: 11118.45, base loss: 15358.76
[INFO 2017-07-01 18:51:34,485 main.py:52] epoch 121, training loss: 8898.31, average training loss: 11100.26, base loss: 15356.77
[INFO 2017-07-01 18:51:38,698 main.py:52] epoch 122, training loss: 8839.21, average training loss: 11081.87, base loss: 15346.35
[INFO 2017-07-01 18:51:42,820 main.py:52] epoch 123, training loss: 10246.72, average training loss: 11075.14, base loss: 15356.06
[INFO 2017-07-01 18:51:47,034 main.py:52] epoch 124, training loss: 9985.17, average training loss: 11066.42, base loss: 15362.18
[INFO 2017-07-01 18:51:51,209 main.py:52] epoch 125, training loss: 10011.62, average training loss: 11058.05, base loss: 15369.91
[INFO 2017-07-01 18:51:55,373 main.py:52] epoch 126, training loss: 9263.44, average training loss: 11043.92, base loss: 15374.04
[INFO 2017-07-01 18:51:59,584 main.py:52] epoch 127, training loss: 8384.01, average training loss: 11023.14, base loss: 15368.86
[INFO 2017-07-01 18:52:03,762 main.py:52] epoch 128, training loss: 9471.49, average training loss: 11011.11, base loss: 15372.78
[INFO 2017-07-01 18:52:07,901 main.py:52] epoch 129, training loss: 9169.48, average training loss: 10996.94, base loss: 15377.35
[INFO 2017-07-01 18:52:12,053 main.py:52] epoch 130, training loss: 8734.75, average training loss: 10979.67, base loss: 15365.60
[INFO 2017-07-01 18:52:16,199 main.py:52] epoch 131, training loss: 8451.39, average training loss: 10960.52, base loss: 15358.06
[INFO 2017-07-01 18:52:20,385 main.py:52] epoch 132, training loss: 9284.10, average training loss: 10947.91, base loss: 15353.20
[INFO 2017-07-01 18:52:24,525 main.py:52] epoch 133, training loss: 9039.15, average training loss: 10933.67, base loss: 15350.85
[INFO 2017-07-01 18:52:28,706 main.py:52] epoch 134, training loss: 8569.71, average training loss: 10916.16, base loss: 15338.04
[INFO 2017-07-01 18:52:32,830 main.py:52] epoch 135, training loss: 8482.70, average training loss: 10898.27, base loss: 15327.81
[INFO 2017-07-01 18:52:36,941 main.py:52] epoch 136, training loss: 11170.48, average training loss: 10900.25, base loss: 15352.47
[INFO 2017-07-01 18:52:41,115 main.py:52] epoch 137, training loss: 10301.00, average training loss: 10895.91, base loss: 15362.73
[INFO 2017-07-01 18:52:45,327 main.py:52] epoch 138, training loss: 10671.92, average training loss: 10894.30, base loss: 15378.15
[INFO 2017-07-01 18:52:49,561 main.py:52] epoch 139, training loss: 9069.44, average training loss: 10881.26, base loss: 15378.21
[INFO 2017-07-01 18:52:53,746 main.py:52] epoch 140, training loss: 8425.53, average training loss: 10863.85, base loss: 15371.96
[INFO 2017-07-01 18:52:57,895 main.py:52] epoch 141, training loss: 9138.56, average training loss: 10851.70, base loss: 15376.95
[INFO 2017-07-01 18:53:02,067 main.py:52] epoch 142, training loss: 9422.25, average training loss: 10841.70, base loss: 15379.31
[INFO 2017-07-01 18:53:06,221 main.py:52] epoch 143, training loss: 9236.19, average training loss: 10830.55, base loss: 15377.06
[INFO 2017-07-01 18:53:10,470 main.py:52] epoch 144, training loss: 8528.81, average training loss: 10814.68, base loss: 15370.43
[INFO 2017-07-01 18:53:14,579 main.py:52] epoch 145, training loss: 9287.27, average training loss: 10804.22, base loss: 15370.44
[INFO 2017-07-01 18:53:18,788 main.py:52] epoch 146, training loss: 10659.14, average training loss: 10803.23, base loss: 15382.58
[INFO 2017-07-01 18:53:22,962 main.py:52] epoch 147, training loss: 10016.63, average training loss: 10797.92, base loss: 15389.74
[INFO 2017-07-01 18:53:27,187 main.py:52] epoch 148, training loss: 8842.64, average training loss: 10784.79, base loss: 15386.76
[INFO 2017-07-01 18:53:31,292 main.py:52] epoch 149, training loss: 8979.88, average training loss: 10772.76, base loss: 15385.63
[INFO 2017-07-01 18:53:35,501 main.py:52] epoch 150, training loss: 9633.31, average training loss: 10765.21, base loss: 15382.66
[INFO 2017-07-01 18:53:39,727 main.py:52] epoch 151, training loss: 9191.04, average training loss: 10754.86, base loss: 15377.90
[INFO 2017-07-01 18:53:43,880 main.py:52] epoch 152, training loss: 9077.61, average training loss: 10743.89, base loss: 15381.19
[INFO 2017-07-01 18:53:47,970 main.py:52] epoch 153, training loss: 8756.93, average training loss: 10730.99, base loss: 15375.25
[INFO 2017-07-01 18:53:52,097 main.py:52] epoch 154, training loss: 9092.36, average training loss: 10720.42, base loss: 15371.18
[INFO 2017-07-01 18:53:56,282 main.py:52] epoch 155, training loss: 10378.78, average training loss: 10718.23, base loss: 15379.64
[INFO 2017-07-01 18:54:00,404 main.py:52] epoch 156, training loss: 10621.64, average training loss: 10717.62, base loss: 15387.57
[INFO 2017-07-01 18:54:04,531 main.py:52] epoch 157, training loss: 9020.10, average training loss: 10706.87, base loss: 15387.40
[INFO 2017-07-01 18:54:08,643 main.py:52] epoch 158, training loss: 10008.32, average training loss: 10702.48, base loss: 15394.43
[INFO 2017-07-01 18:54:12,920 main.py:52] epoch 159, training loss: 9158.15, average training loss: 10692.83, base loss: 15391.63
[INFO 2017-07-01 18:54:17,019 main.py:52] epoch 160, training loss: 9348.68, average training loss: 10684.48, base loss: 15391.93
[INFO 2017-07-01 18:54:21,191 main.py:52] epoch 161, training loss: 9572.78, average training loss: 10677.62, base loss: 15390.94
[INFO 2017-07-01 18:54:25,300 main.py:52] epoch 162, training loss: 8960.55, average training loss: 10667.08, base loss: 15387.09
[INFO 2017-07-01 18:54:29,447 main.py:52] epoch 163, training loss: 9814.39, average training loss: 10661.88, base loss: 15390.81
[INFO 2017-07-01 18:54:33,561 main.py:52] epoch 164, training loss: 9143.64, average training loss: 10652.68, base loss: 15389.33
[INFO 2017-07-01 18:54:37,773 main.py:52] epoch 165, training loss: 10174.65, average training loss: 10649.80, base loss: 15395.73
[INFO 2017-07-01 18:54:41,976 main.py:52] epoch 166, training loss: 10303.01, average training loss: 10647.72, base loss: 15408.53
[INFO 2017-07-01 18:54:46,125 main.py:52] epoch 167, training loss: 9516.01, average training loss: 10640.99, base loss: 15409.86
[INFO 2017-07-01 18:54:50,351 main.py:52] epoch 168, training loss: 9168.58, average training loss: 10632.28, base loss: 15413.99
[INFO 2017-07-01 18:54:54,531 main.py:52] epoch 169, training loss: 8387.14, average training loss: 10619.07, base loss: 15405.85
[INFO 2017-07-01 18:54:58,749 main.py:52] epoch 170, training loss: 9615.88, average training loss: 10613.20, base loss: 15406.78
[INFO 2017-07-01 18:55:02,856 main.py:52] epoch 171, training loss: 9251.44, average training loss: 10605.28, base loss: 15405.44
[INFO 2017-07-01 18:55:07,017 main.py:52] epoch 172, training loss: 9674.60, average training loss: 10599.90, base loss: 15409.14
[INFO 2017-07-01 18:55:11,204 main.py:52] epoch 173, training loss: 9511.92, average training loss: 10593.65, base loss: 15414.71
[INFO 2017-07-01 18:55:15,350 main.py:52] epoch 174, training loss: 9858.32, average training loss: 10589.45, base loss: 15422.70
[INFO 2017-07-01 18:55:19,469 main.py:52] epoch 175, training loss: 8983.29, average training loss: 10580.32, base loss: 15420.27
[INFO 2017-07-01 18:55:23,594 main.py:52] epoch 176, training loss: 8360.64, average training loss: 10567.78, base loss: 15415.55
[INFO 2017-07-01 18:55:27,763 main.py:52] epoch 177, training loss: 9372.08, average training loss: 10561.07, base loss: 15416.15
[INFO 2017-07-01 18:55:31,885 main.py:52] epoch 178, training loss: 9344.50, average training loss: 10554.27, base loss: 15419.98
[INFO 2017-07-01 18:55:36,045 main.py:52] epoch 179, training loss: 8521.43, average training loss: 10542.98, base loss: 15416.96
[INFO 2017-07-01 18:55:40,262 main.py:52] epoch 180, training loss: 8217.73, average training loss: 10530.13, base loss: 15408.42
[INFO 2017-07-01 18:55:44,429 main.py:52] epoch 181, training loss: 8977.67, average training loss: 10521.60, base loss: 15405.41
[INFO 2017-07-01 18:55:48,657 main.py:52] epoch 182, training loss: 8721.93, average training loss: 10511.77, base loss: 15404.87
[INFO 2017-07-01 18:55:52,822 main.py:52] epoch 183, training loss: 10458.55, average training loss: 10511.48, base loss: 15414.42
[INFO 2017-07-01 18:55:56,987 main.py:52] epoch 184, training loss: 7929.20, average training loss: 10497.52, base loss: 15409.74
[INFO 2017-07-01 18:56:01,199 main.py:52] epoch 185, training loss: 8589.20, average training loss: 10487.26, base loss: 15403.72
[INFO 2017-07-01 18:56:05,331 main.py:52] epoch 186, training loss: 8800.38, average training loss: 10478.24, base loss: 15399.39
[INFO 2017-07-01 18:56:09,495 main.py:52] epoch 187, training loss: 9274.20, average training loss: 10471.83, base loss: 15396.68
[INFO 2017-07-01 18:56:13,590 main.py:52] epoch 188, training loss: 9757.19, average training loss: 10468.05, base loss: 15398.03
[INFO 2017-07-01 18:56:17,809 main.py:52] epoch 189, training loss: 9186.91, average training loss: 10461.31, base loss: 15402.71
[INFO 2017-07-01 18:56:21,963 main.py:52] epoch 190, training loss: 8442.03, average training loss: 10450.74, base loss: 15395.61
[INFO 2017-07-01 18:56:26,266 main.py:52] epoch 191, training loss: 9327.65, average training loss: 10444.89, base loss: 15400.06
[INFO 2017-07-01 18:56:30,421 main.py:52] epoch 192, training loss: 7326.61, average training loss: 10428.73, base loss: 15386.44
[INFO 2017-07-01 18:56:34,560 main.py:52] epoch 193, training loss: 8471.32, average training loss: 10418.64, base loss: 15382.52
[INFO 2017-07-01 18:56:38,730 main.py:52] epoch 194, training loss: 8165.74, average training loss: 10407.09, base loss: 15381.48
[INFO 2017-07-01 18:56:42,872 main.py:52] epoch 195, training loss: 8303.80, average training loss: 10396.36, base loss: 15374.93
[INFO 2017-07-01 18:56:47,062 main.py:52] epoch 196, training loss: 10119.43, average training loss: 10394.95, base loss: 15382.36
[INFO 2017-07-01 18:56:51,220 main.py:52] epoch 197, training loss: 9479.36, average training loss: 10390.33, base loss: 15381.59
[INFO 2017-07-01 18:56:55,340 main.py:52] epoch 198, training loss: 8483.02, average training loss: 10380.74, base loss: 15383.97
[INFO 2017-07-01 18:56:59,525 main.py:52] epoch 199, training loss: 8679.72, average training loss: 10372.24, base loss: 15381.77
[INFO 2017-07-01 18:56:59,525 main.py:54] epoch 199, testing
[INFO 2017-07-01 18:56:59,525 main.py:61] model save to ./model/final.pth
[INFO 2017-07-01 18:57:03,739 main.py:52] epoch 200, training loss: 8673.65, average training loss: 10363.79, base loss: 15381.79
[INFO 2017-07-01 18:57:07,852 main.py:52] epoch 201, training loss: 9141.48, average training loss: 10357.74, base loss: 15381.75
[INFO 2017-07-01 18:57:12,064 main.py:52] epoch 202, training loss: 8611.14, average training loss: 10349.13, base loss: 15379.47
[INFO 2017-07-01 18:57:16,271 main.py:52] epoch 203, training loss: 8990.60, average training loss: 10342.47, base loss: 15382.04
[INFO 2017-07-01 18:57:20,440 main.py:52] epoch 204, training loss: 8757.04, average training loss: 10334.74, base loss: 15380.18
[INFO 2017-07-01 18:57:24,570 main.py:52] epoch 205, training loss: 8506.37, average training loss: 10325.86, base loss: 15374.25
[INFO 2017-07-01 18:57:28,768 main.py:52] epoch 206, training loss: 8643.35, average training loss: 10317.73, base loss: 15373.48
[INFO 2017-07-01 18:57:32,960 main.py:52] epoch 207, training loss: 8321.29, average training loss: 10308.14, base loss: 15372.12
[INFO 2017-07-01 18:57:37,150 main.py:52] epoch 208, training loss: 9818.21, average training loss: 10305.79, base loss: 15377.15
[INFO 2017-07-01 18:57:41,295 main.py:52] epoch 209, training loss: 9188.89, average training loss: 10300.47, base loss: 15378.27
[INFO 2017-07-01 18:57:45,424 main.py:52] epoch 210, training loss: 9370.83, average training loss: 10296.07, base loss: 15377.63
[INFO 2017-07-01 18:57:49,603 main.py:52] epoch 211, training loss: 9825.29, average training loss: 10293.85, base loss: 15378.37
[INFO 2017-07-01 18:57:53,745 main.py:52] epoch 212, training loss: 8757.77, average training loss: 10286.64, base loss: 15378.31
[INFO 2017-07-01 18:57:57,954 main.py:52] epoch 213, training loss: 8863.00, average training loss: 10279.98, base loss: 15377.78
[INFO 2017-07-01 18:58:02,147 main.py:52] epoch 214, training loss: 10021.67, average training loss: 10278.78, base loss: 15385.20
[INFO 2017-07-01 18:58:06,295 main.py:52] epoch 215, training loss: 8694.01, average training loss: 10271.44, base loss: 15381.65
[INFO 2017-07-01 18:58:10,423 main.py:52] epoch 216, training loss: 8695.73, average training loss: 10264.18, base loss: 15382.40
[INFO 2017-07-01 18:58:14,625 main.py:52] epoch 217, training loss: 10318.33, average training loss: 10264.43, base loss: 15392.12
[INFO 2017-07-01 18:58:18,849 main.py:52] epoch 218, training loss: 8210.10, average training loss: 10255.05, base loss: 15387.42
[INFO 2017-07-01 18:58:23,038 main.py:52] epoch 219, training loss: 8852.40, average training loss: 10248.68, base loss: 15386.82
[INFO 2017-07-01 18:58:27,256 main.py:52] epoch 220, training loss: 8072.37, average training loss: 10238.83, base loss: 15377.83
[INFO 2017-07-01 18:58:31,469 main.py:52] epoch 221, training loss: 8489.96, average training loss: 10230.95, base loss: 15374.90
[INFO 2017-07-01 18:58:35,734 main.py:52] epoch 222, training loss: 9287.61, average training loss: 10226.72, base loss: 15378.98
[INFO 2017-07-01 18:58:39,914 main.py:52] epoch 223, training loss: 9267.65, average training loss: 10222.44, base loss: 15379.18
[INFO 2017-07-01 18:58:44,065 main.py:52] epoch 224, training loss: 8488.41, average training loss: 10214.73, base loss: 15377.21
[INFO 2017-07-01 18:58:48,209 main.py:52] epoch 225, training loss: 9522.14, average training loss: 10211.67, base loss: 15379.86
[INFO 2017-07-01 18:58:52,334 main.py:52] epoch 226, training loss: 8347.32, average training loss: 10203.45, base loss: 15379.57
[INFO 2017-07-01 18:58:56,528 main.py:52] epoch 227, training loss: 9047.19, average training loss: 10198.38, base loss: 15377.53
[INFO 2017-07-01 18:59:00,754 main.py:52] epoch 228, training loss: 7712.25, average training loss: 10187.53, base loss: 15367.44
[INFO 2017-07-01 18:59:04,949 main.py:52] epoch 229, training loss: 9247.40, average training loss: 10183.44, base loss: 15371.79
[INFO 2017-07-01 18:59:09,079 main.py:52] epoch 230, training loss: 8799.36, average training loss: 10177.45, base loss: 15366.83
[INFO 2017-07-01 18:59:13,259 main.py:52] epoch 231, training loss: 9770.24, average training loss: 10175.69, base loss: 15369.56
[INFO 2017-07-01 18:59:17,467 main.py:52] epoch 232, training loss: 9089.40, average training loss: 10171.03, base loss: 15369.35
[INFO 2017-07-01 18:59:21,690 main.py:52] epoch 233, training loss: 9579.09, average training loss: 10168.50, base loss: 15368.81
[INFO 2017-07-01 18:59:25,828 main.py:52] epoch 234, training loss: 9023.67, average training loss: 10163.63, base loss: 15367.73
[INFO 2017-07-01 18:59:29,924 main.py:52] epoch 235, training loss: 9011.55, average training loss: 10158.75, base loss: 15367.98
[INFO 2017-07-01 18:59:34,135 main.py:52] epoch 236, training loss: 8915.69, average training loss: 10153.50, base loss: 15361.69
[INFO 2017-07-01 18:59:38,301 main.py:52] epoch 237, training loss: 10265.16, average training loss: 10153.97, base loss: 15367.03
[INFO 2017-07-01 18:59:42,465 main.py:52] epoch 238, training loss: 8159.24, average training loss: 10145.62, base loss: 15362.71
[INFO 2017-07-01 18:59:46,583 main.py:52] epoch 239, training loss: 8571.85, average training loss: 10139.07, base loss: 15358.65
[INFO 2017-07-01 18:59:50,725 main.py:52] epoch 240, training loss: 8829.28, average training loss: 10133.63, base loss: 15357.05
[INFO 2017-07-01 18:59:54,872 main.py:52] epoch 241, training loss: 8555.09, average training loss: 10127.11, base loss: 15354.54
[INFO 2017-07-01 18:59:59,047 main.py:52] epoch 242, training loss: 9573.14, average training loss: 10124.83, base loss: 15358.80
[INFO 2017-07-01 19:00:03,181 main.py:52] epoch 243, training loss: 8061.81, average training loss: 10116.37, base loss: 15353.00
[INFO 2017-07-01 19:00:07,409 main.py:52] epoch 244, training loss: 7988.92, average training loss: 10107.69, base loss: 15345.84
[INFO 2017-07-01 19:00:11,585 main.py:52] epoch 245, training loss: 7690.91, average training loss: 10097.87, base loss: 15340.24
[INFO 2017-07-01 19:00:15,753 main.py:52] epoch 246, training loss: 10179.04, average training loss: 10098.20, base loss: 15350.67
[INFO 2017-07-01 19:00:19,913 main.py:52] epoch 247, training loss: 8068.15, average training loss: 10090.01, base loss: 15347.72
[INFO 2017-07-01 19:00:24,104 main.py:52] epoch 248, training loss: 9748.67, average training loss: 10088.64, base loss: 15355.62
[INFO 2017-07-01 19:00:28,247 main.py:52] epoch 249, training loss: 9524.04, average training loss: 10086.38, base loss: 15357.87
[INFO 2017-07-01 19:00:32,376 main.py:52] epoch 250, training loss: 8711.18, average training loss: 10080.90, base loss: 15359.01
[INFO 2017-07-01 19:00:36,528 main.py:52] epoch 251, training loss: 9049.14, average training loss: 10076.81, base loss: 15360.72
[INFO 2017-07-01 19:00:40,707 main.py:52] epoch 252, training loss: 9876.91, average training loss: 10076.02, base loss: 15364.39
[INFO 2017-07-01 19:00:44,874 main.py:52] epoch 253, training loss: 8650.38, average training loss: 10070.40, base loss: 15362.73
[INFO 2017-07-01 19:00:48,958 main.py:52] epoch 254, training loss: 9717.12, average training loss: 10069.02, base loss: 15360.55
[INFO 2017-07-01 19:00:53,189 main.py:52] epoch 255, training loss: 9202.56, average training loss: 10065.63, base loss: 15364.73
[INFO 2017-07-01 19:00:57,378 main.py:52] epoch 256, training loss: 8414.97, average training loss: 10059.21, base loss: 15362.46
[INFO 2017-07-01 19:01:01,450 main.py:52] epoch 257, training loss: 9212.86, average training loss: 10055.93, base loss: 15366.73
[INFO 2017-07-01 19:01:05,577 main.py:52] epoch 258, training loss: 7935.20, average training loss: 10047.74, base loss: 15358.29
[INFO 2017-07-01 19:01:09,752 main.py:52] epoch 259, training loss: 10430.91, average training loss: 10049.22, base loss: 15367.82
[INFO 2017-07-01 19:01:13,927 main.py:52] epoch 260, training loss: 9439.93, average training loss: 10046.88, base loss: 15368.51
[INFO 2017-07-01 19:01:18,076 main.py:52] epoch 261, training loss: 8397.14, average training loss: 10040.59, base loss: 15368.23
[INFO 2017-07-01 19:01:22,388 main.py:52] epoch 262, training loss: 9372.44, average training loss: 10038.05, base loss: 15372.50
[INFO 2017-07-01 19:01:26,550 main.py:52] epoch 263, training loss: 9564.58, average training loss: 10036.25, base loss: 15380.15
[INFO 2017-07-01 19:01:30,739 main.py:52] epoch 264, training loss: 8070.39, average training loss: 10028.83, base loss: 15372.55
[INFO 2017-07-01 19:01:34,846 main.py:52] epoch 265, training loss: 8152.90, average training loss: 10021.78, base loss: 15366.18
[INFO 2017-07-01 19:01:38,990 main.py:52] epoch 266, training loss: 9464.79, average training loss: 10019.69, base loss: 15368.97
[INFO 2017-07-01 19:01:43,191 main.py:52] epoch 267, training loss: 9068.39, average training loss: 10016.15, base loss: 15371.12
[INFO 2017-07-01 19:01:47,296 main.py:52] epoch 268, training loss: 8517.69, average training loss: 10010.57, base loss: 15367.02
[INFO 2017-07-01 19:01:51,460 main.py:52] epoch 269, training loss: 8364.64, average training loss: 10004.48, base loss: 15367.08
[INFO 2017-07-01 19:01:55,611 main.py:52] epoch 270, training loss: 9426.83, average training loss: 10002.35, base loss: 15367.17
[INFO 2017-07-01 19:01:59,666 main.py:52] epoch 271, training loss: 7821.56, average training loss: 9994.33, base loss: 15360.74
[INFO 2017-07-01 19:02:03,804 main.py:52] epoch 272, training loss: 8990.75, average training loss: 9990.65, base loss: 15362.14
[INFO 2017-07-01 19:02:07,929 main.py:52] epoch 273, training loss: 9603.89, average training loss: 9989.24, base loss: 15364.46
[INFO 2017-07-01 19:02:12,104 main.py:52] epoch 274, training loss: 9998.65, average training loss: 9989.28, base loss: 15368.79
[INFO 2017-07-01 19:02:16,241 main.py:52] epoch 275, training loss: 9076.87, average training loss: 9985.97, base loss: 15370.65
[INFO 2017-07-01 19:02:20,383 main.py:52] epoch 276, training loss: 8862.70, average training loss: 9981.92, base loss: 15370.66
[INFO 2017-07-01 19:02:24,660 main.py:52] epoch 277, training loss: 9200.99, average training loss: 9979.11, base loss: 15374.39
[INFO 2017-07-01 19:02:28,941 main.py:52] epoch 278, training loss: 9056.82, average training loss: 9975.80, base loss: 15375.52
[INFO 2017-07-01 19:02:33,052 main.py:52] epoch 279, training loss: 8330.32, average training loss: 9969.92, base loss: 15372.70
[INFO 2017-07-01 19:02:37,204 main.py:52] epoch 280, training loss: 9135.11, average training loss: 9966.95, base loss: 15372.99
[INFO 2017-07-01 19:02:41,368 main.py:52] epoch 281, training loss: 7873.93, average training loss: 9959.53, base loss: 15364.69
[INFO 2017-07-01 19:02:45,599 main.py:52] epoch 282, training loss: 9294.08, average training loss: 9957.18, base loss: 15367.62
[INFO 2017-07-01 19:02:49,699 main.py:52] epoch 283, training loss: 7812.29, average training loss: 9949.63, base loss: 15361.24
[INFO 2017-07-01 19:02:53,858 main.py:52] epoch 284, training loss: 9017.37, average training loss: 9946.36, base loss: 15361.14
[INFO 2017-07-01 19:02:58,087 main.py:52] epoch 285, training loss: 8885.27, average training loss: 9942.65, base loss: 15361.98
[INFO 2017-07-01 19:03:02,240 main.py:52] epoch 286, training loss: 8061.03, average training loss: 9936.09, base loss: 15357.67
[INFO 2017-07-01 19:03:06,371 main.py:52] epoch 287, training loss: 9624.25, average training loss: 9935.01, base loss: 15362.58
[INFO 2017-07-01 19:03:10,639 main.py:52] epoch 288, training loss: 7665.35, average training loss: 9927.15, base loss: 15356.96
[INFO 2017-07-01 19:03:14,795 main.py:52] epoch 289, training loss: 8326.55, average training loss: 9921.63, base loss: 15352.09
[INFO 2017-07-01 19:03:19,026 main.py:52] epoch 290, training loss: 9137.29, average training loss: 9918.94, base loss: 15352.38
[INFO 2017-07-01 19:03:23,171 main.py:52] epoch 291, training loss: 8427.82, average training loss: 9913.83, base loss: 15351.82
[INFO 2017-07-01 19:03:27,278 main.py:52] epoch 292, training loss: 9248.28, average training loss: 9911.56, base loss: 15351.15
[INFO 2017-07-01 19:03:31,430 main.py:52] epoch 293, training loss: 8526.77, average training loss: 9906.85, base loss: 15345.78
[INFO 2017-07-01 19:03:35,536 main.py:52] epoch 294, training loss: 9262.37, average training loss: 9904.67, base loss: 15351.10
[INFO 2017-07-01 19:03:39,662 main.py:52] epoch 295, training loss: 9118.28, average training loss: 9902.01, base loss: 15351.75
[INFO 2017-07-01 19:03:43,811 main.py:52] epoch 296, training loss: 8896.61, average training loss: 9898.62, base loss: 15352.11
[INFO 2017-07-01 19:03:47,999 main.py:52] epoch 297, training loss: 9103.02, average training loss: 9895.95, base loss: 15355.57
[INFO 2017-07-01 19:03:52,173 main.py:52] epoch 298, training loss: 7614.10, average training loss: 9888.32, base loss: 15351.04
[INFO 2017-07-01 19:03:56,339 main.py:52] epoch 299, training loss: 8586.35, average training loss: 9883.98, base loss: 15349.74
[INFO 2017-07-01 19:03:56,339 main.py:54] epoch 299, testing
[INFO 2017-07-01 19:03:56,339 main.py:61] model save to ./model/final.pth
[INFO 2017-07-01 19:04:00,531 main.py:52] epoch 300, training loss: 9653.14, average training loss: 9883.22, base loss: 15349.59
[INFO 2017-07-01 19:04:04,699 main.py:52] epoch 301, training loss: 8809.08, average training loss: 9879.66, base loss: 15350.80
[INFO 2017-07-01 19:04:08,849 main.py:52] epoch 302, training loss: 9121.11, average training loss: 9877.16, base loss: 15354.70
[INFO 2017-07-01 19:04:13,026 main.py:52] epoch 303, training loss: 9085.45, average training loss: 9874.55, base loss: 15356.84
[INFO 2017-07-01 19:04:17,190 main.py:52] epoch 304, training loss: 8314.03, average training loss: 9869.43, base loss: 15354.30
[INFO 2017-07-01 19:04:21,341 main.py:52] epoch 305, training loss: 8867.07, average training loss: 9866.16, base loss: 15356.15
[INFO 2017-07-01 19:04:25,579 main.py:52] epoch 306, training loss: 8854.57, average training loss: 9862.86, base loss: 15354.64
[INFO 2017-07-01 19:04:29,893 main.py:52] epoch 307, training loss: 8746.94, average training loss: 9859.24, base loss: 15359.71
[INFO 2017-07-01 19:04:34,039 main.py:52] epoch 308, training loss: 8360.25, average training loss: 9854.39, base loss: 15359.99
[INFO 2017-07-01 19:04:38,201 main.py:52] epoch 309, training loss: 7862.63, average training loss: 9847.96, base loss: 15354.75
[INFO 2017-07-01 19:04:42,405 main.py:52] epoch 310, training loss: 8418.30, average training loss: 9843.37, base loss: 15352.40
[INFO 2017-07-01 19:04:46,626 main.py:52] epoch 311, training loss: 8851.66, average training loss: 9840.19, base loss: 15354.12
[INFO 2017-07-01 19:04:50,819 main.py:52] epoch 312, training loss: 9404.01, average training loss: 9838.80, base loss: 15356.86
[INFO 2017-07-01 19:04:54,981 main.py:52] epoch 313, training loss: 9304.65, average training loss: 9837.09, base loss: 15357.87
[INFO 2017-07-01 19:04:59,210 main.py:52] epoch 314, training loss: 8580.31, average training loss: 9833.10, base loss: 15357.45
[INFO 2017-07-01 19:05:03,343 main.py:52] epoch 315, training loss: 7855.12, average training loss: 9826.85, base loss: 15354.19
[INFO 2017-07-01 19:05:07,571 main.py:52] epoch 316, training loss: 9303.33, average training loss: 9825.19, base loss: 15356.64
[INFO 2017-07-01 19:05:11,724 main.py:52] epoch 317, training loss: 8842.22, average training loss: 9822.10, base loss: 15355.02
[INFO 2017-07-01 19:05:15,929 main.py:52] epoch 318, training loss: 8486.80, average training loss: 9817.92, base loss: 15351.91
[INFO 2017-07-01 19:05:20,129 main.py:52] epoch 319, training loss: 9132.09, average training loss: 9815.77, base loss: 15355.02
[INFO 2017-07-01 19:05:24,325 main.py:52] epoch 320, training loss: 8595.48, average training loss: 9811.97, base loss: 15354.78
[INFO 2017-07-01 19:05:28,507 main.py:52] epoch 321, training loss: 8585.71, average training loss: 9808.16, base loss: 15355.11
[INFO 2017-07-01 19:05:32,680 main.py:52] epoch 322, training loss: 7578.87, average training loss: 9801.26, base loss: 15349.28
[INFO 2017-07-01 19:05:36,851 main.py:52] epoch 323, training loss: 8730.79, average training loss: 9797.96, base loss: 15346.47
[INFO 2017-07-01 19:05:41,022 main.py:52] epoch 324, training loss: 9346.13, average training loss: 9796.57, base loss: 15350.24
[INFO 2017-07-01 19:05:45,163 main.py:52] epoch 325, training loss: 8549.55, average training loss: 9792.74, base loss: 15345.52
[INFO 2017-07-01 19:05:49,408 main.py:52] epoch 326, training loss: 8092.40, average training loss: 9787.54, base loss: 15342.59
[INFO 2017-07-01 19:05:53,591 main.py:52] epoch 327, training loss: 7768.29, average training loss: 9781.39, base loss: 15338.22
[INFO 2017-07-01 19:05:57,843 main.py:52] epoch 328, training loss: 9524.40, average training loss: 9780.61, base loss: 15342.73
[INFO 2017-07-01 19:06:01,990 main.py:52] epoch 329, training loss: 10010.50, average training loss: 9781.30, base loss: 15350.15
[INFO 2017-07-01 19:06:06,142 main.py:52] epoch 330, training loss: 8511.57, average training loss: 9777.47, base loss: 15349.62
[INFO 2017-07-01 19:06:10,316 main.py:52] epoch 331, training loss: 9136.05, average training loss: 9775.53, base loss: 15352.20
[INFO 2017-07-01 19:06:14,477 main.py:52] epoch 332, training loss: 8119.24, average training loss: 9770.56, base loss: 15351.88
[INFO 2017-07-01 19:06:18,618 main.py:52] epoch 333, training loss: 7600.15, average training loss: 9764.06, base loss: 15346.28
[INFO 2017-07-01 19:06:22,844 main.py:52] epoch 334, training loss: 8385.64, average training loss: 9759.95, base loss: 15345.55
[INFO 2017-07-01 19:06:26,977 main.py:52] epoch 335, training loss: 8897.88, average training loss: 9757.38, base loss: 15346.43
[INFO 2017-07-01 19:06:31,155 main.py:52] epoch 336, training loss: 10278.34, average training loss: 9758.93, base loss: 15354.54
[INFO 2017-07-01 19:06:35,328 main.py:52] epoch 337, training loss: 9125.24, average training loss: 9757.05, base loss: 15356.44
[INFO 2017-07-01 19:06:39,505 main.py:52] epoch 338, training loss: 8740.81, average training loss: 9754.05, base loss: 15358.76
[INFO 2017-07-01 19:06:43,735 main.py:52] epoch 339, training loss: 7948.03, average training loss: 9748.74, base loss: 15355.49
[INFO 2017-07-01 19:06:47,945 main.py:52] epoch 340, training loss: 8370.48, average training loss: 9744.70, base loss: 15357.94
[INFO 2017-07-01 19:06:52,117 main.py:52] epoch 341, training loss: 8873.43, average training loss: 9742.15, base loss: 15361.99
[INFO 2017-07-01 19:06:56,323 main.py:52] epoch 342, training loss: 8566.80, average training loss: 9738.73, base loss: 15362.07
[INFO 2017-07-01 19:07:00,523 main.py:52] epoch 343, training loss: 8921.32, average training loss: 9736.35, base loss: 15363.29
[INFO 2017-07-01 19:07:04,698 main.py:52] epoch 344, training loss: 9627.43, average training loss: 9736.03, base loss: 15368.51
[INFO 2017-07-01 19:07:08,863 main.py:52] epoch 345, training loss: 9186.29, average training loss: 9734.45, base loss: 15370.26
[INFO 2017-07-01 19:07:13,036 main.py:52] epoch 346, training loss: 10649.86, average training loss: 9737.08, base loss: 15376.69
[INFO 2017-07-01 19:07:17,199 main.py:52] epoch 347, training loss: 8126.81, average training loss: 9732.46, base loss: 15374.80
[INFO 2017-07-01 19:07:21,394 main.py:52] epoch 348, training loss: 8349.92, average training loss: 9728.50, base loss: 15372.05
[INFO 2017-07-01 19:07:25,547 main.py:52] epoch 349, training loss: 7951.81, average training loss: 9723.42, base loss: 15371.83
[INFO 2017-07-01 19:07:29,759 main.py:52] epoch 350, training loss: 9161.61, average training loss: 9721.82, base loss: 15375.45
[INFO 2017-07-01 19:07:33,920 main.py:52] epoch 351, training loss: 8748.56, average training loss: 9719.05, base loss: 15374.98
[INFO 2017-07-01 19:07:38,092 main.py:52] epoch 352, training loss: 8050.53, average training loss: 9714.33, base loss: 15371.07
[INFO 2017-07-01 19:07:42,200 main.py:52] epoch 353, training loss: 8682.31, average training loss: 9711.41, base loss: 15371.06
[INFO 2017-07-01 19:07:46,332 main.py:52] epoch 354, training loss: 8666.62, average training loss: 9708.47, base loss: 15369.79
[INFO 2017-07-01 19:07:50,524 main.py:52] epoch 355, training loss: 8360.10, average training loss: 9704.68, base loss: 15369.14
[INFO 2017-07-01 19:07:54,741 main.py:52] epoch 356, training loss: 8347.54, average training loss: 9700.88, base loss: 15368.20
[INFO 2017-07-01 19:07:58,912 main.py:52] epoch 357, training loss: 8153.17, average training loss: 9696.56, base loss: 15366.86
[INFO 2017-07-01 19:08:03,147 main.py:52] epoch 358, training loss: 8604.72, average training loss: 9693.51, base loss: 15365.86
[INFO 2017-07-01 19:08:07,279 main.py:52] epoch 359, training loss: 8342.86, average training loss: 9689.76, base loss: 15363.48
[INFO 2017-07-01 19:08:11,433 main.py:52] epoch 360, training loss: 8581.99, average training loss: 9686.69, base loss: 15362.17
[INFO 2017-07-01 19:08:15,604 main.py:52] epoch 361, training loss: 8662.46, average training loss: 9683.87, base loss: 15365.92
[INFO 2017-07-01 19:08:19,842 main.py:52] epoch 362, training loss: 8398.83, average training loss: 9680.33, base loss: 15364.82
[INFO 2017-07-01 19:08:24,043 main.py:52] epoch 363, training loss: 8911.99, average training loss: 9678.21, base loss: 15363.50
[INFO 2017-07-01 19:08:28,210 main.py:52] epoch 364, training loss: 8942.41, average training loss: 9676.20, base loss: 15366.51
[INFO 2017-07-01 19:08:32,364 main.py:52] epoch 365, training loss: 9204.89, average training loss: 9674.91, base loss: 15370.39
[INFO 2017-07-01 19:08:36,511 main.py:52] epoch 366, training loss: 8255.89, average training loss: 9671.04, base loss: 15369.16
[INFO 2017-07-01 19:08:40,716 main.py:52] epoch 367, training loss: 8327.30, average training loss: 9667.39, base loss: 15369.38
[INFO 2017-07-01 19:08:44,887 main.py:52] epoch 368, training loss: 8352.74, average training loss: 9663.83, base loss: 15368.06
[INFO 2017-07-01 19:08:49,153 main.py:52] epoch 369, training loss: 8436.42, average training loss: 9660.51, base loss: 15365.90
[INFO 2017-07-01 19:08:53,315 main.py:52] epoch 370, training loss: 8205.66, average training loss: 9656.59, base loss: 15365.02
[INFO 2017-07-01 19:08:57,418 main.py:52] epoch 371, training loss: 8176.44, average training loss: 9652.61, base loss: 15361.42
[INFO 2017-07-01 19:09:01,564 main.py:52] epoch 372, training loss: 7971.55, average training loss: 9648.11, base loss: 15359.73
[INFO 2017-07-01 19:09:05,724 main.py:52] epoch 373, training loss: 8430.74, average training loss: 9644.85, base loss: 15357.95
[INFO 2017-07-01 19:09:09,864 main.py:52] epoch 374, training loss: 10107.80, average training loss: 9646.08, base loss: 15363.95
[INFO 2017-07-01 19:09:14,009 main.py:52] epoch 375, training loss: 8687.27, average training loss: 9643.53, base loss: 15365.72
[INFO 2017-07-01 19:09:18,270 main.py:52] epoch 376, training loss: 8093.79, average training loss: 9639.42, base loss: 15361.71
[INFO 2017-07-01 19:09:22,368 main.py:52] epoch 377, training loss: 6902.51, average training loss: 9632.18, base loss: 15355.80
[INFO 2017-07-01 19:09:26,527 main.py:52] epoch 378, training loss: 8050.15, average training loss: 9628.01, base loss: 15350.41
[INFO 2017-07-01 19:09:30,721 main.py:52] epoch 379, training loss: 9199.56, average training loss: 9626.88, base loss: 15353.83
[INFO 2017-07-01 19:09:34,894 main.py:52] epoch 380, training loss: 9918.39, average training loss: 9627.65, base loss: 15358.54
[INFO 2017-07-01 19:09:39,025 main.py:52] epoch 381, training loss: 9608.58, average training loss: 9627.60, base loss: 15361.91
[INFO 2017-07-01 19:09:43,172 main.py:52] epoch 382, training loss: 8508.30, average training loss: 9624.67, base loss: 15360.38
[INFO 2017-07-01 19:09:47,332 main.py:52] epoch 383, training loss: 8500.37, average training loss: 9621.75, base loss: 15358.89
[INFO 2017-07-01 19:09:51,462 main.py:52] epoch 384, training loss: 8517.15, average training loss: 9618.88, base loss: 15357.57
[INFO 2017-07-01 19:09:55,594 main.py:52] epoch 385, training loss: 9408.56, average training loss: 9618.33, base loss: 15364.47
[INFO 2017-07-01 19:09:59,758 main.py:52] epoch 386, training loss: 8787.67, average training loss: 9616.19, base loss: 15362.92
[INFO 2017-07-01 19:10:03,945 main.py:52] epoch 387, training loss: 8825.71, average training loss: 9614.15, base loss: 15362.12
[INFO 2017-07-01 19:10:08,057 main.py:52] epoch 388, training loss: 9066.83, average training loss: 9612.74, base loss: 15362.52
[INFO 2017-07-01 19:10:12,191 main.py:52] epoch 389, training loss: 8066.87, average training loss: 9608.78, base loss: 15361.18
[INFO 2017-07-01 19:10:16,354 main.py:52] epoch 390, training loss: 7950.73, average training loss: 9604.54, base loss: 15356.59
[INFO 2017-07-01 19:10:20,504 main.py:52] epoch 391, training loss: 9692.91, average training loss: 9604.76, base loss: 15361.47
[INFO 2017-07-01 19:10:24,633 main.py:52] epoch 392, training loss: 9421.85, average training loss: 9604.30, base loss: 15365.10
[INFO 2017-07-01 19:10:28,796 main.py:52] epoch 393, training loss: 8096.07, average training loss: 9600.47, base loss: 15364.09
[INFO 2017-07-01 19:10:32,926 main.py:52] epoch 394, training loss: 9741.34, average training loss: 9600.83, base loss: 15371.42
[INFO 2017-07-01 19:10:37,040 main.py:52] epoch 395, training loss: 8674.43, average training loss: 9598.49, base loss: 15370.54
[INFO 2017-07-01 19:10:41,097 main.py:52] epoch 396, training loss: 8902.69, average training loss: 9596.73, base loss: 15374.20
[INFO 2017-07-01 19:10:45,229 main.py:52] epoch 397, training loss: 8584.96, average training loss: 9594.19, base loss: 15370.46
[INFO 2017-07-01 19:10:49,394 main.py:52] epoch 398, training loss: 8826.39, average training loss: 9592.27, base loss: 15371.24
[INFO 2017-07-01 19:10:53,570 main.py:52] epoch 399, training loss: 8039.18, average training loss: 9588.39, base loss: 15370.28
[INFO 2017-07-01 19:10:53,571 main.py:54] epoch 399, testing
[INFO 2017-07-01 19:10:53,571 main.py:61] model save to ./model/final.pth
[INFO 2017-07-01 19:10:57,840 main.py:52] epoch 400, training loss: 9192.36, average training loss: 9587.40, base loss: 15370.10
[INFO 2017-07-01 19:11:02,137 main.py:52] epoch 401, training loss: 7934.23, average training loss: 9583.29, base loss: 15366.88
[INFO 2017-07-01 19:11:06,308 main.py:52] epoch 402, training loss: 8104.30, average training loss: 9579.62, base loss: 15362.21
[INFO 2017-07-01 19:11:10,444 main.py:52] epoch 403, training loss: 8117.43, average training loss: 9576.00, base loss: 15361.30
[INFO 2017-07-01 19:11:14,609 main.py:52] epoch 404, training loss: 8308.53, average training loss: 9572.87, base loss: 15359.91
[INFO 2017-07-01 19:11:18,856 main.py:52] epoch 405, training loss: 9774.22, average training loss: 9573.36, base loss: 15363.95
[INFO 2017-07-01 19:11:22,978 main.py:52] epoch 406, training loss: 7871.37, average training loss: 9569.18, base loss: 15361.99
[INFO 2017-07-01 19:11:27,143 main.py:52] epoch 407, training loss: 8226.10, average training loss: 9565.89, base loss: 15358.07
[INFO 2017-07-01 19:11:31,278 main.py:52] epoch 408, training loss: 9504.22, average training loss: 9565.74, base loss: 15363.22
[INFO 2017-07-01 19:11:35,428 main.py:52] epoch 409, training loss: 8447.23, average training loss: 9563.01, base loss: 15362.01
[INFO 2017-07-01 19:11:39,586 main.py:52] epoch 410, training loss: 9352.74, average training loss: 9562.50, base loss: 15364.04
[INFO 2017-07-01 19:11:43,723 main.py:52] epoch 411, training loss: 7708.13, average training loss: 9558.00, base loss: 15361.11
[INFO 2017-07-01 19:11:47,889 main.py:52] epoch 412, training loss: 8091.58, average training loss: 9554.45, base loss: 15361.21
[INFO 2017-07-01 19:11:52,085 main.py:52] epoch 413, training loss: 8524.76, average training loss: 9551.96, base loss: 15360.64
[INFO 2017-07-01 19:11:56,313 main.py:52] epoch 414, training loss: 7323.24, average training loss: 9546.59, base loss: 15357.42
[INFO 2017-07-01 19:12:00,551 main.py:52] epoch 415, training loss: 8050.21, average training loss: 9542.99, base loss: 15355.04
[INFO 2017-07-01 19:12:04,736 main.py:52] epoch 416, training loss: 9745.52, average training loss: 9543.48, base loss: 15357.77
[INFO 2017-07-01 19:12:08,926 main.py:52] epoch 417, training loss: 8275.87, average training loss: 9540.45, base loss: 15358.01
[INFO 2017-07-01 19:12:13,062 main.py:52] epoch 418, training loss: 10087.11, average training loss: 9541.75, base loss: 15363.23
[INFO 2017-07-01 19:12:17,292 main.py:52] epoch 419, training loss: 9444.83, average training loss: 9541.52, base loss: 15364.92
[INFO 2017-07-01 19:12:21,508 main.py:52] epoch 420, training loss: 8617.46, average training loss: 9539.32, base loss: 15363.64
[INFO 2017-07-01 19:12:25,760 main.py:52] epoch 421, training loss: 7982.16, average training loss: 9535.63, base loss: 15361.67
[INFO 2017-07-01 19:12:29,895 main.py:52] epoch 422, training loss: 9532.48, average training loss: 9535.63, base loss: 15363.01
[INFO 2017-07-01 19:12:34,022 main.py:52] epoch 423, training loss: 9194.13, average training loss: 9534.82, base loss: 15365.71
[INFO 2017-07-01 19:12:38,213 main.py:52] epoch 424, training loss: 9375.46, average training loss: 9534.45, base loss: 15368.29
[INFO 2017-07-01 19:12:42,431 main.py:52] epoch 425, training loss: 7511.70, average training loss: 9529.70, base loss: 15363.81
[INFO 2017-07-01 19:12:46,590 main.py:52] epoch 426, training loss: 9311.99, average training loss: 9529.19, base loss: 15363.98
[INFO 2017-07-01 19:12:50,714 main.py:52] epoch 427, training loss: 9191.73, average training loss: 9528.40, base loss: 15368.44
[INFO 2017-07-01 19:12:54,883 main.py:52] epoch 428, training loss: 9021.75, average training loss: 9527.22, base loss: 15369.67
[INFO 2017-07-01 19:12:59,021 main.py:52] epoch 429, training loss: 9303.32, average training loss: 9526.70, base loss: 15374.82
[INFO 2017-07-01 19:13:03,148 main.py:52] epoch 430, training loss: 9091.87, average training loss: 9525.69, base loss: 15375.23
[INFO 2017-07-01 19:13:07,364 main.py:52] epoch 431, training loss: 8698.23, average training loss: 9523.77, base loss: 15377.55
[INFO 2017-07-01 19:13:11,471 main.py:52] epoch 432, training loss: 8348.24, average training loss: 9521.06, base loss: 15374.37
[INFO 2017-07-01 19:13:15,608 main.py:52] epoch 433, training loss: 8582.55, average training loss: 9518.90, base loss: 15375.79
[INFO 2017-07-01 19:13:19,724 main.py:52] epoch 434, training loss: 8998.82, average training loss: 9517.70, base loss: 15377.70
[INFO 2017-07-01 19:13:23,864 main.py:52] epoch 435, training loss: 8701.42, average training loss: 9515.83, base loss: 15378.06
[INFO 2017-07-01 19:13:28,037 main.py:52] epoch 436, training loss: 7497.25, average training loss: 9511.21, base loss: 15374.46
[INFO 2017-07-01 19:13:32,241 main.py:52] epoch 437, training loss: 8498.11, average training loss: 9508.90, base loss: 15373.63
[INFO 2017-07-01 19:13:36,400 main.py:52] epoch 438, training loss: 8130.80, average training loss: 9505.76, base loss: 15371.82
[INFO 2017-07-01 19:13:40,603 main.py:52] epoch 439, training loss: 9777.28, average training loss: 9506.37, base loss: 15375.96
[INFO 2017-07-01 19:13:44,770 main.py:52] epoch 440, training loss: 8755.25, average training loss: 9504.67, base loss: 15376.80
[INFO 2017-07-01 19:13:48,914 main.py:52] epoch 441, training loss: 8259.44, average training loss: 9501.85, base loss: 15375.02
[INFO 2017-07-01 19:13:53,074 main.py:52] epoch 442, training loss: 9246.41, average training loss: 9501.28, base loss: 15380.10
[INFO 2017-07-01 19:13:57,237 main.py:52] epoch 443, training loss: 7439.12, average training loss: 9496.63, base loss: 15375.60
[INFO 2017-07-01 19:14:01,465 main.py:52] epoch 444, training loss: 7458.95, average training loss: 9492.05, base loss: 15372.77
[INFO 2017-07-01 19:14:05,593 main.py:52] epoch 445, training loss: 9102.74, average training loss: 9491.18, base loss: 15371.90
[INFO 2017-07-01 19:14:09,842 main.py:52] epoch 446, training loss: 9739.97, average training loss: 9491.74, base loss: 15375.57
[INFO 2017-07-01 19:14:14,032 main.py:52] epoch 447, training loss: 8506.71, average training loss: 9489.54, base loss: 15374.83
[INFO 2017-07-01 19:14:18,188 main.py:52] epoch 448, training loss: 8193.22, average training loss: 9486.65, base loss: 15374.08
[INFO 2017-07-01 19:14:22,329 main.py:52] epoch 449, training loss: 8253.01, average training loss: 9483.91, base loss: 15374.24
[INFO 2017-07-01 19:14:26,491 main.py:52] epoch 450, training loss: 8208.06, average training loss: 9481.08, base loss: 15372.06
[INFO 2017-07-01 19:14:30,663 main.py:52] epoch 451, training loss: 8446.76, average training loss: 9478.79, base loss: 15369.95
[INFO 2017-07-01 19:14:34,767 main.py:52] epoch 452, training loss: 8710.16, average training loss: 9477.10, base loss: 15368.98
[INFO 2017-07-01 19:14:38,950 main.py:52] epoch 453, training loss: 7959.67, average training loss: 9473.75, base loss: 15368.65
[INFO 2017-07-01 19:14:43,020 main.py:52] epoch 454, training loss: 8027.43, average training loss: 9470.58, base loss: 15368.51
[INFO 2017-07-01 19:14:47,254 main.py:52] epoch 455, training loss: 9465.33, average training loss: 9470.56, base loss: 15372.58
[INFO 2017-07-01 19:14:51,400 main.py:52] epoch 456, training loss: 10819.02, average training loss: 9473.51, base loss: 15377.62
[INFO 2017-07-01 19:14:55,540 main.py:52] epoch 457, training loss: 8085.47, average training loss: 9470.48, base loss: 15378.49
[INFO 2017-07-01 19:14:59,748 main.py:52] epoch 458, training loss: 8087.26, average training loss: 9467.47, base loss: 15375.12
[INFO 2017-07-01 19:15:03,902 main.py:52] epoch 459, training loss: 9458.88, average training loss: 9467.45, base loss: 15375.68
[INFO 2017-07-01 19:15:08,091 main.py:52] epoch 460, training loss: 8664.31, average training loss: 9465.71, base loss: 15377.60
[INFO 2017-07-01 19:15:12,302 main.py:52] epoch 461, training loss: 8922.36, average training loss: 9464.53, base loss: 15379.15
[INFO 2017-07-01 19:15:16,578 main.py:52] epoch 462, training loss: 7590.13, average training loss: 9460.48, base loss: 15373.78
[INFO 2017-07-01 19:15:20,764 main.py:52] epoch 463, training loss: 8686.89, average training loss: 9458.82, base loss: 15374.50
[INFO 2017-07-01 19:15:24,975 main.py:52] epoch 464, training loss: 8639.33, average training loss: 9457.06, base loss: 15376.29
[INFO 2017-07-01 19:15:29,188 main.py:52] epoch 465, training loss: 8816.07, average training loss: 9455.68, base loss: 15377.30
[INFO 2017-07-01 19:15:33,374 main.py:52] epoch 466, training loss: 8134.35, average training loss: 9452.85, base loss: 15377.45
[INFO 2017-07-01 19:15:37,563 main.py:52] epoch 467, training loss: 8519.93, average training loss: 9450.86, base loss: 15377.66
[INFO 2017-07-01 19:15:41,662 main.py:52] epoch 468, training loss: 8324.94, average training loss: 9448.46, base loss: 15377.45
[INFO 2017-07-01 19:15:45,831 main.py:52] epoch 469, training loss: 7883.22, average training loss: 9445.13, base loss: 15373.41
[INFO 2017-07-01 19:15:50,055 main.py:52] epoch 470, training loss: 8853.62, average training loss: 9443.87, base loss: 15375.07
[INFO 2017-07-01 19:15:54,312 main.py:52] epoch 471, training loss: 8062.45, average training loss: 9440.94, base loss: 15375.38
[INFO 2017-07-01 19:15:58,504 main.py:52] epoch 472, training loss: 9634.04, average training loss: 9441.35, base loss: 15379.08
[INFO 2017-07-01 19:16:02,644 main.py:52] epoch 473, training loss: 9570.19, average training loss: 9441.62, base loss: 15381.73
[INFO 2017-07-01 19:16:06,803 main.py:52] epoch 474, training loss: 8540.31, average training loss: 9439.73, base loss: 15383.03
[INFO 2017-07-01 19:16:10,949 main.py:52] epoch 475, training loss: 7862.75, average training loss: 9436.41, base loss: 15381.70
[INFO 2017-07-01 19:16:15,082 main.py:52] epoch 476, training loss: 8209.59, average training loss: 9433.84, base loss: 15380.46
[INFO 2017-07-01 19:16:19,248 main.py:52] epoch 477, training loss: 8257.57, average training loss: 9431.38, base loss: 15380.65
[INFO 2017-07-01 19:16:23,459 main.py:52] epoch 478, training loss: 8669.83, average training loss: 9429.79, base loss: 15380.31
[INFO 2017-07-01 19:16:27,655 main.py:52] epoch 479, training loss: 8109.95, average training loss: 9427.04, base loss: 15379.23
[INFO 2017-07-01 19:16:31,775 main.py:52] epoch 480, training loss: 8514.36, average training loss: 9425.14, base loss: 15380.26
[INFO 2017-07-01 19:16:35,900 main.py:52] epoch 481, training loss: 8010.87, average training loss: 9422.21, base loss: 15378.44
[INFO 2017-07-01 19:16:39,996 main.py:52] epoch 482, training loss: 8079.43, average training loss: 9419.43, base loss: 15378.40
[INFO 2017-07-01 19:16:44,208 main.py:52] epoch 483, training loss: 8077.72, average training loss: 9416.66, base loss: 15376.97
[INFO 2017-07-01 19:16:48,402 main.py:52] epoch 484, training loss: 8916.52, average training loss: 9415.63, base loss: 15377.33
[INFO 2017-07-01 19:16:52,605 main.py:52] epoch 485, training loss: 7250.29, average training loss: 9411.17, base loss: 15372.50
[INFO 2017-07-01 19:16:56,740 main.py:52] epoch 486, training loss: 8861.09, average training loss: 9410.04, base loss: 15375.89
[INFO 2017-07-01 19:17:00,864 main.py:52] epoch 487, training loss: 8689.43, average training loss: 9408.56, base loss: 15374.40
[INFO 2017-07-01 19:17:05,026 main.py:52] epoch 488, training loss: 9772.78, average training loss: 9409.31, base loss: 15377.93
[INFO 2017-07-01 19:17:09,172 main.py:52] epoch 489, training loss: 9769.40, average training loss: 9410.04, base loss: 15383.22
[INFO 2017-07-01 19:17:13,355 main.py:52] epoch 490, training loss: 9294.42, average training loss: 9409.81, base loss: 15384.60
[INFO 2017-07-01 19:17:17,512 main.py:52] epoch 491, training loss: 8606.03, average training loss: 9408.17, base loss: 15386.31
[INFO 2017-07-01 19:17:21,645 main.py:52] epoch 492, training loss: 8335.87, average training loss: 9406.00, base loss: 15383.07
[INFO 2017-07-01 19:17:25,832 main.py:52] epoch 493, training loss: 8422.45, average training loss: 9404.01, base loss: 15383.40
[INFO 2017-07-01 19:17:30,048 main.py:52] epoch 494, training loss: 8532.61, average training loss: 9402.25, base loss: 15382.16
[INFO 2017-07-01 19:17:34,181 main.py:52] epoch 495, training loss: 8746.15, average training loss: 9400.93, base loss: 15383.97
[INFO 2017-07-01 19:17:38,315 main.py:52] epoch 496, training loss: 8339.36, average training loss: 9398.79, base loss: 15382.20
[INFO 2017-07-01 19:17:42,502 main.py:52] epoch 497, training loss: 9056.44, average training loss: 9398.10, base loss: 15381.56
[INFO 2017-07-01 19:17:46,669 main.py:52] epoch 498, training loss: 8350.46, average training loss: 9396.00, base loss: 15380.45
[INFO 2017-07-01 19:17:50,806 main.py:52] epoch 499, training loss: 9152.39, average training loss: 9395.52, base loss: 15381.64
[INFO 2017-07-01 19:17:50,806 main.py:54] epoch 499, testing
[INFO 2017-07-01 19:17:50,806 main.py:61] model save to ./model/final.pth
[INFO 2017-07-01 19:17:55,058 main.py:52] epoch 500, training loss: 7703.38, average training loss: 9392.14, base loss: 15377.81
[INFO 2017-07-01 19:17:59,173 main.py:52] epoch 501, training loss: 8223.81, average training loss: 9389.81, base loss: 15374.21
[INFO 2017-07-01 19:18:03,297 main.py:52] epoch 502, training loss: 8822.19, average training loss: 9388.68, base loss: 15373.71
[INFO 2017-07-01 19:18:07,429 main.py:52] epoch 503, training loss: 8380.82, average training loss: 9386.68, base loss: 15371.54
[INFO 2017-07-01 19:18:11,605 main.py:52] epoch 504, training loss: 9922.11, average training loss: 9387.74, base loss: 15376.93
[INFO 2017-07-01 19:18:15,844 main.py:52] epoch 505, training loss: 8763.51, average training loss: 9386.51, base loss: 15377.46
[INFO 2017-07-01 19:18:20,003 main.py:52] epoch 506, training loss: 8104.12, average training loss: 9383.98, base loss: 15374.78
[INFO 2017-07-01 19:18:24,165 main.py:52] epoch 507, training loss: 8209.39, average training loss: 9381.67, base loss: 15372.33
[INFO 2017-07-01 19:18:28,290 main.py:52] epoch 508, training loss: 9299.02, average training loss: 9381.50, base loss: 15375.85
[INFO 2017-07-01 19:18:32,449 main.py:52] epoch 509, training loss: 8636.83, average training loss: 9380.04, base loss: 15375.00
[INFO 2017-07-01 19:18:36,654 main.py:52] epoch 510, training loss: 8771.36, average training loss: 9378.85, base loss: 15379.29
[INFO 2017-07-01 19:18:40,878 main.py:52] epoch 511, training loss: 8939.19, average training loss: 9377.99, base loss: 15382.29
[INFO 2017-07-01 19:18:45,103 main.py:52] epoch 512, training loss: 8949.69, average training loss: 9377.16, base loss: 15382.96
[INFO 2017-07-01 19:18:49,318 main.py:52] epoch 513, training loss: 8264.50, average training loss: 9375.00, base loss: 15382.46
[INFO 2017-07-01 19:18:53,543 main.py:52] epoch 514, training loss: 9794.78, average training loss: 9375.81, base loss: 15386.60
[INFO 2017-07-01 19:18:57,761 main.py:52] epoch 515, training loss: 8450.38, average training loss: 9374.02, base loss: 15386.70
[INFO 2017-07-01 19:19:01,920 main.py:52] epoch 516, training loss: 8244.43, average training loss: 9371.83, base loss: 15382.02
[INFO 2017-07-01 19:19:06,166 main.py:52] epoch 517, training loss: 8477.72, average training loss: 9370.11, base loss: 15380.09
[INFO 2017-07-01 19:19:10,319 main.py:52] epoch 518, training loss: 7739.80, average training loss: 9366.96, base loss: 15377.77
[INFO 2017-07-01 19:19:14,528 main.py:52] epoch 519, training loss: 8159.90, average training loss: 9364.64, base loss: 15375.51
[INFO 2017-07-01 19:19:18,776 main.py:52] epoch 520, training loss: 7825.34, average training loss: 9361.69, base loss: 15372.03
[INFO 2017-07-01 19:19:22,985 main.py:52] epoch 521, training loss: 8627.92, average training loss: 9360.28, base loss: 15372.15
[INFO 2017-07-01 19:19:27,087 main.py:52] epoch 522, training loss: 9091.19, average training loss: 9359.77, base loss: 15376.89
[INFO 2017-07-01 19:19:31,235 main.py:52] epoch 523, training loss: 7859.69, average training loss: 9356.91, base loss: 15375.63
[INFO 2017-07-01 19:19:35,456 main.py:52] epoch 524, training loss: 7145.73, average training loss: 9352.69, base loss: 15368.51
[INFO 2017-07-01 19:19:39,706 main.py:52] epoch 525, training loss: 8929.56, average training loss: 9351.89, base loss: 15367.45
[INFO 2017-07-01 19:19:43,924 main.py:52] epoch 526, training loss: 8532.78, average training loss: 9350.34, base loss: 15367.20
[INFO 2017-07-01 19:19:48,172 main.py:52] epoch 527, training loss: 8437.92, average training loss: 9348.61, base loss: 15365.97
[INFO 2017-07-01 19:19:52,317 main.py:52] epoch 528, training loss: 7313.85, average training loss: 9344.76, base loss: 15362.40
[INFO 2017-07-01 19:19:56,503 main.py:52] epoch 529, training loss: 7917.49, average training loss: 9342.07, base loss: 15358.89
[INFO 2017-07-01 19:20:00,614 main.py:52] epoch 530, training loss: 8269.14, average training loss: 9340.05, base loss: 15355.86
[INFO 2017-07-01 19:20:04,762 main.py:52] epoch 531, training loss: 8664.48, average training loss: 9338.78, base loss: 15356.19
[INFO 2017-07-01 19:20:08,927 main.py:52] epoch 532, training loss: 8629.02, average training loss: 9337.45, base loss: 15356.06
[INFO 2017-07-01 19:20:13,100 main.py:52] epoch 533, training loss: 9182.51, average training loss: 9337.16, base loss: 15357.03
[INFO 2017-07-01 19:20:17,247 main.py:52] epoch 534, training loss: 9449.53, average training loss: 9337.37, base loss: 15360.83
[INFO 2017-07-01 19:20:21,397 main.py:52] epoch 535, training loss: 7763.86, average training loss: 9334.43, base loss: 15360.00
[INFO 2017-07-01 19:20:25,563 main.py:52] epoch 536, training loss: 8137.56, average training loss: 9332.20, base loss: 15357.13
[INFO 2017-07-01 19:20:29,727 main.py:52] epoch 537, training loss: 7590.39, average training loss: 9328.96, base loss: 15353.57
[INFO 2017-07-01 19:20:33,836 main.py:52] epoch 538, training loss: 8308.03, average training loss: 9327.07, base loss: 15353.56
[INFO 2017-07-01 19:20:37,932 main.py:52] epoch 539, training loss: 8484.05, average training loss: 9325.51, base loss: 15351.59
[INFO 2017-07-01 19:20:42,002 main.py:52] epoch 540, training loss: 8487.51, average training loss: 9323.96, base loss: 15351.47
[INFO 2017-07-01 19:20:46,147 main.py:52] epoch 541, training loss: 8548.65, average training loss: 9322.53, base loss: 15351.18
[INFO 2017-07-01 19:20:50,266 main.py:52] epoch 542, training loss: 8078.00, average training loss: 9320.24, base loss: 15349.87
[INFO 2017-07-01 19:20:54,357 main.py:52] epoch 543, training loss: 9048.01, average training loss: 9319.74, base loss: 15351.24
[INFO 2017-07-01 19:20:58,516 main.py:52] epoch 544, training loss: 8287.58, average training loss: 9317.84, base loss: 15349.78
[INFO 2017-07-01 19:21:02,696 main.py:52] epoch 545, training loss: 7858.43, average training loss: 9315.17, base loss: 15344.42
[INFO 2017-07-01 19:21:06,815 main.py:52] epoch 546, training loss: 8628.39, average training loss: 9313.91, base loss: 15343.88
[INFO 2017-07-01 19:21:10,969 main.py:52] epoch 547, training loss: 8090.84, average training loss: 9311.68, base loss: 15341.93
[INFO 2017-07-01 19:21:15,118 main.py:52] epoch 548, training loss: 8779.65, average training loss: 9310.71, base loss: 15346.56
[INFO 2017-07-01 19:21:19,314 main.py:52] epoch 549, training loss: 7698.19, average training loss: 9307.78, base loss: 15345.69
[INFO 2017-07-01 19:21:23,501 main.py:52] epoch 550, training loss: 8309.23, average training loss: 9305.97, base loss: 15344.70
[INFO 2017-07-01 19:21:27,647 main.py:52] epoch 551, training loss: 7849.76, average training loss: 9303.33, base loss: 15343.94
[INFO 2017-07-01 19:21:31,784 main.py:52] epoch 552, training loss: 7406.74, average training loss: 9299.90, base loss: 15340.09
[INFO 2017-07-01 19:21:36,001 main.py:52] epoch 553, training loss: 8025.88, average training loss: 9297.60, base loss: 15337.11
[INFO 2017-07-01 19:21:40,163 main.py:52] epoch 554, training loss: 8049.32, average training loss: 9295.35, base loss: 15334.67
[INFO 2017-07-01 19:21:44,366 main.py:52] epoch 555, training loss: 9255.05, average training loss: 9295.28, base loss: 15337.52
[INFO 2017-07-01 19:21:48,552 main.py:52] epoch 556, training loss: 9201.38, average training loss: 9295.11, base loss: 15337.35
[INFO 2017-07-01 19:21:52,717 main.py:52] epoch 557, training loss: 8360.25, average training loss: 9293.44, base loss: 15335.87
[INFO 2017-07-01 19:21:56,962 main.py:52] epoch 558, training loss: 8018.77, average training loss: 9291.16, base loss: 15333.66
[INFO 2017-07-01 19:22:01,119 main.py:52] epoch 559, training loss: 9063.72, average training loss: 9290.75, base loss: 15336.78
[INFO 2017-07-01 19:22:05,282 main.py:52] epoch 560, training loss: 8100.35, average training loss: 9288.63, base loss: 15336.36
[INFO 2017-07-01 19:22:09,452 main.py:52] epoch 561, training loss: 8143.43, average training loss: 9286.59, base loss: 15335.77
[INFO 2017-07-01 19:22:13,669 main.py:52] epoch 562, training loss: 7567.88, average training loss: 9283.54, base loss: 15335.09
[INFO 2017-07-01 19:22:17,839 main.py:52] epoch 563, training loss: 7679.22, average training loss: 9280.69, base loss: 15333.81
[INFO 2017-07-01 19:22:21,986 main.py:52] epoch 564, training loss: 7654.10, average training loss: 9277.81, base loss: 15333.06
[INFO 2017-07-01 19:22:26,154 main.py:52] epoch 565, training loss: 8886.96, average training loss: 9277.12, base loss: 15335.43
[INFO 2017-07-01 19:22:30,349 main.py:52] epoch 566, training loss: 7760.81, average training loss: 9274.45, base loss: 15332.86
[INFO 2017-07-01 19:22:34,470 main.py:52] epoch 567, training loss: 8966.34, average training loss: 9273.91, base loss: 15336.03
[INFO 2017-07-01 19:22:38,604 main.py:52] epoch 568, training loss: 8199.23, average training loss: 9272.02, base loss: 15333.76
[INFO 2017-07-01 19:22:42,781 main.py:52] epoch 569, training loss: 8449.96, average training loss: 9270.58, base loss: 15335.80
[INFO 2017-07-01 19:22:46,867 main.py:52] epoch 570, training loss: 9165.42, average training loss: 9270.39, base loss: 15336.84
[INFO 2017-07-01 19:22:51,005 main.py:52] epoch 571, training loss: 9292.62, average training loss: 9270.43, base loss: 15339.20
[INFO 2017-07-01 19:22:55,176 main.py:52] epoch 572, training loss: 8706.72, average training loss: 9269.45, base loss: 15342.52
[INFO 2017-07-01 19:22:59,303 main.py:52] epoch 573, training loss: 8372.15, average training loss: 9267.88, base loss: 15345.92
[INFO 2017-07-01 19:23:03,513 main.py:52] epoch 574, training loss: 8509.33, average training loss: 9266.56, base loss: 15345.47
[INFO 2017-07-01 19:23:07,662 main.py:52] epoch 575, training loss: 8349.59, average training loss: 9264.97, base loss: 15344.60
[INFO 2017-07-01 19:23:11,814 main.py:52] epoch 576, training loss: 8743.46, average training loss: 9264.07, base loss: 15347.22
[INFO 2017-07-01 19:23:15,942 main.py:52] epoch 577, training loss: 7911.35, average training loss: 9261.73, base loss: 15343.46
[INFO 2017-07-01 19:23:20,126 main.py:52] epoch 578, training loss: 8370.37, average training loss: 9260.19, base loss: 15345.77
[INFO 2017-07-01 19:23:24,275 main.py:52] epoch 579, training loss: 8375.23, average training loss: 9258.66, base loss: 15347.31
[INFO 2017-07-01 19:23:28,435 main.py:52] epoch 580, training loss: 9048.92, average training loss: 9258.30, base loss: 15347.78
[INFO 2017-07-01 19:23:32,548 main.py:52] epoch 581, training loss: 9420.47, average training loss: 9258.58, base loss: 15349.29
[INFO 2017-07-01 19:23:36,771 main.py:52] epoch 582, training loss: 8645.50, average training loss: 9257.53, base loss: 15350.03
[INFO 2017-07-01 19:23:40,946 main.py:52] epoch 583, training loss: 8884.49, average training loss: 9256.89, base loss: 15350.99
[INFO 2017-07-01 19:23:45,131 main.py:52] epoch 584, training loss: 8312.18, average training loss: 9255.28, base loss: 15348.66
[INFO 2017-07-01 19:23:49,316 main.py:52] epoch 585, training loss: 8947.96, average training loss: 9254.75, base loss: 15348.24
[INFO 2017-07-01 19:23:53,524 main.py:52] epoch 586, training loss: 7999.01, average training loss: 9252.61, base loss: 15347.13
[INFO 2017-07-01 19:23:57,762 main.py:52] epoch 587, training loss: 7895.79, average training loss: 9250.30, base loss: 15345.05
[INFO 2017-07-01 19:24:01,919 main.py:52] epoch 588, training loss: 8285.67, average training loss: 9248.67, base loss: 15346.28
[INFO 2017-07-01 19:24:06,044 main.py:52] epoch 589, training loss: 8796.00, average training loss: 9247.90, base loss: 15348.59
[INFO 2017-07-01 19:24:10,278 main.py:52] epoch 590, training loss: 9048.95, average training loss: 9247.56, base loss: 15351.10
[INFO 2017-07-01 19:24:14,482 main.py:52] epoch 591, training loss: 7693.78, average training loss: 9244.94, base loss: 15349.77
[INFO 2017-07-01 19:24:18,632 main.py:52] epoch 592, training loss: 8237.01, average training loss: 9243.24, base loss: 15350.90
[INFO 2017-07-01 19:24:22,899 main.py:52] epoch 593, training loss: 8491.70, average training loss: 9241.97, base loss: 15352.41
[INFO 2017-07-01 19:24:27,016 main.py:52] epoch 594, training loss: 8502.23, average training loss: 9240.73, base loss: 15355.97
[INFO 2017-07-01 19:24:31,123 main.py:52] epoch 595, training loss: 7837.54, average training loss: 9238.38, base loss: 15354.29
[INFO 2017-07-01 19:24:35,228 main.py:52] epoch 596, training loss: 8197.41, average training loss: 9236.63, base loss: 15353.12
[INFO 2017-07-01 19:24:39,400 main.py:52] epoch 597, training loss: 8346.25, average training loss: 9235.14, base loss: 15352.37
[INFO 2017-07-01 19:24:43,570 main.py:52] epoch 598, training loss: 8056.62, average training loss: 9233.18, base loss: 15348.81
[INFO 2017-07-01 19:24:47,700 main.py:52] epoch 599, training loss: 9017.00, average training loss: 9232.81, base loss: 15346.33
[INFO 2017-07-01 19:24:47,701 main.py:54] epoch 599, testing
[INFO 2017-07-01 19:24:47,701 main.py:61] model save to ./model/final.pth
[INFO 2017-07-01 19:24:51,938 main.py:52] epoch 600, training loss: 7832.17, average training loss: 9230.48, base loss: 15346.75
[INFO 2017-07-01 19:24:56,073 main.py:52] epoch 601, training loss: 8444.61, average training loss: 9229.18, base loss: 15348.13
[INFO 2017-07-01 19:25:00,302 main.py:52] epoch 602, training loss: 7861.81, average training loss: 9226.91, base loss: 15345.22
[INFO 2017-07-01 19:25:04,512 main.py:52] epoch 603, training loss: 8807.10, average training loss: 9226.22, base loss: 15344.85
[INFO 2017-07-01 19:25:08,710 main.py:52] epoch 604, training loss: 8687.37, average training loss: 9225.33, base loss: 15344.72
[INFO 2017-07-01 19:25:12,953 main.py:52] epoch 605, training loss: 9307.54, average training loss: 9225.46, base loss: 15344.62
[INFO 2017-07-01 19:25:17,114 main.py:52] epoch 606, training loss: 9386.29, average training loss: 9225.73, base loss: 15348.16
[INFO 2017-07-01 19:25:21,374 main.py:52] epoch 607, training loss: 8060.69, average training loss: 9223.81, base loss: 15345.99
[INFO 2017-07-01 19:25:25,531 main.py:52] epoch 608, training loss: 7649.84, average training loss: 9221.23, base loss: 15346.05
[INFO 2017-07-01 19:25:29,803 main.py:52] epoch 609, training loss: 7402.75, average training loss: 9218.24, base loss: 15344.90
[INFO 2017-07-01 19:25:33,923 main.py:52] epoch 610, training loss: 7738.63, average training loss: 9215.82, base loss: 15341.25
[INFO 2017-07-01 19:25:38,052 main.py:52] epoch 611, training loss: 8378.98, average training loss: 9214.46, base loss: 15342.60
[INFO 2017-07-01 19:25:42,283 main.py:52] epoch 612, training loss: 8482.60, average training loss: 9213.26, base loss: 15341.40
[INFO 2017-07-01 19:25:46,459 main.py:52] epoch 613, training loss: 8558.76, average training loss: 9212.20, base loss: 15339.19
[INFO 2017-07-01 19:25:50,645 main.py:52] epoch 614, training loss: 7035.18, average training loss: 9208.66, base loss: 15336.80
[INFO 2017-07-01 19:25:54,870 main.py:52] epoch 615, training loss: 8007.65, average training loss: 9206.71, base loss: 15336.40
[INFO 2017-07-01 19:25:58,997 main.py:52] epoch 616, training loss: 8458.57, average training loss: 9205.49, base loss: 15338.39
[INFO 2017-07-01 19:26:03,157 main.py:52] epoch 617, training loss: 8150.08, average training loss: 9203.79, base loss: 15337.36
[INFO 2017-07-01 19:26:07,275 main.py:52] epoch 618, training loss: 7914.96, average training loss: 9201.70, base loss: 15336.62
[INFO 2017-07-01 19:26:11,396 main.py:52] epoch 619, training loss: 7583.84, average training loss: 9199.09, base loss: 15335.30
[INFO 2017-07-01 19:26:15,550 main.py:52] epoch 620, training loss: 7692.08, average training loss: 9196.67, base loss: 15335.45
[INFO 2017-07-01 19:26:19,729 main.py:52] epoch 621, training loss: 7960.45, average training loss: 9194.68, base loss: 15334.45
[INFO 2017-07-01 19:26:23,902 main.py:52] epoch 622, training loss: 8875.93, average training loss: 9194.17, base loss: 15335.97
[INFO 2017-07-01 19:26:28,066 main.py:52] epoch 623, training loss: 8868.38, average training loss: 9193.65, base loss: 15338.59
[INFO 2017-07-01 19:26:32,288 main.py:52] epoch 624, training loss: 8338.05, average training loss: 9192.28, base loss: 15338.66
[INFO 2017-07-01 19:26:36,452 main.py:52] epoch 625, training loss: 8354.70, average training loss: 9190.94, base loss: 15338.26
[INFO 2017-07-01 19:26:40,633 main.py:52] epoch 626, training loss: 8645.29, average training loss: 9190.07, base loss: 15339.32
[INFO 2017-07-01 19:26:44,769 main.py:52] epoch 627, training loss: 7807.23, average training loss: 9187.87, base loss: 15336.71
[INFO 2017-07-01 19:26:48,926 main.py:52] epoch 628, training loss: 8736.61, average training loss: 9187.15, base loss: 15337.45
[INFO 2017-07-01 19:26:53,094 main.py:52] epoch 629, training loss: 8235.06, average training loss: 9185.64, base loss: 15335.52
[INFO 2017-07-01 19:26:57,305 main.py:52] epoch 630, training loss: 9224.06, average training loss: 9185.70, base loss: 15337.81
[INFO 2017-07-01 19:27:01,506 main.py:52] epoch 631, training loss: 7738.08, average training loss: 9183.41, base loss: 15336.13
[INFO 2017-07-01 19:27:05,662 main.py:52] epoch 632, training loss: 7317.72, average training loss: 9180.46, base loss: 15333.53
[INFO 2017-07-01 19:27:09,832 main.py:52] epoch 633, training loss: 8174.84, average training loss: 9178.88, base loss: 15333.58
[INFO 2017-07-01 19:27:14,036 main.py:52] epoch 634, training loss: 8719.19, average training loss: 9178.15, base loss: 15333.13
[INFO 2017-07-01 19:27:18,265 main.py:52] epoch 635, training loss: 8659.50, average training loss: 9177.34, base loss: 15333.16
[INFO 2017-07-01 19:27:22,480 main.py:52] epoch 636, training loss: 9372.66, average training loss: 9177.64, base loss: 15337.51
[INFO 2017-07-01 19:27:26,699 main.py:52] epoch 637, training loss: 8444.90, average training loss: 9176.49, base loss: 15337.68
[INFO 2017-07-01 19:27:30,878 main.py:52] epoch 638, training loss: 8737.43, average training loss: 9175.81, base loss: 15338.58
[INFO 2017-07-01 19:27:35,075 main.py:52] epoch 639, training loss: 8326.61, average training loss: 9174.48, base loss: 15338.72
[INFO 2017-07-01 19:27:39,241 main.py:52] epoch 640, training loss: 7793.42, average training loss: 9172.33, base loss: 15337.85
[INFO 2017-07-01 19:27:43,454 main.py:52] epoch 641, training loss: 10088.70, average training loss: 9173.75, base loss: 15340.90
[INFO 2017-07-01 19:27:47,580 main.py:52] epoch 642, training loss: 8098.84, average training loss: 9172.08, base loss: 15343.35
[INFO 2017-07-01 19:27:51,877 main.py:52] epoch 643, training loss: 8299.68, average training loss: 9170.73, base loss: 15344.45
[INFO 2017-07-01 19:27:56,112 main.py:52] epoch 644, training loss: 9318.82, average training loss: 9170.96, base loss: 15345.67
[INFO 2017-07-01 19:28:00,297 main.py:52] epoch 645, training loss: 7639.71, average training loss: 9168.59, base loss: 15343.37
[INFO 2017-07-01 19:28:04,524 main.py:52] epoch 646, training loss: 8965.06, average training loss: 9168.27, base loss: 15346.59
[INFO 2017-07-01 19:28:08,684 main.py:52] epoch 647, training loss: 7439.37, average training loss: 9165.60, base loss: 15342.86
[INFO 2017-07-01 19:28:12,846 main.py:52] epoch 648, training loss: 8613.11, average training loss: 9164.75, base loss: 15343.35
[INFO 2017-07-01 19:28:16,975 main.py:52] epoch 649, training loss: 9723.01, average training loss: 9165.61, base loss: 15346.88
[INFO 2017-07-01 19:28:21,153 main.py:52] epoch 650, training loss: 8761.72, average training loss: 9164.99, base loss: 15346.30
[INFO 2017-07-01 19:28:25,365 main.py:52] epoch 651, training loss: 8313.95, average training loss: 9163.68, base loss: 15344.52
[INFO 2017-07-01 19:28:29,548 main.py:52] epoch 652, training loss: 8034.73, average training loss: 9161.96, base loss: 15343.10
[INFO 2017-07-01 19:28:33,715 main.py:52] epoch 653, training loss: 8753.67, average training loss: 9161.33, base loss: 15344.68
[INFO 2017-07-01 19:28:37,814 main.py:52] epoch 654, training loss: 8514.75, average training loss: 9160.34, base loss: 15346.72
[INFO 2017-07-01 19:28:41,984 main.py:52] epoch 655, training loss: 8940.52, average training loss: 9160.01, base loss: 15350.30
[INFO 2017-07-01 19:28:46,108 main.py:52] epoch 656, training loss: 8672.79, average training loss: 9159.27, base loss: 15349.65
[INFO 2017-07-01 19:28:50,216 main.py:52] epoch 657, training loss: 8128.56, average training loss: 9157.70, base loss: 15349.40
[INFO 2017-07-01 19:28:54,369 main.py:52] epoch 658, training loss: 8163.85, average training loss: 9156.19, base loss: 15349.31
[INFO 2017-07-01 19:28:58,525 main.py:52] epoch 659, training loss: 8105.82, average training loss: 9154.60, base loss: 15346.87
[INFO 2017-07-01 19:29:02,656 main.py:52] epoch 660, training loss: 8724.72, average training loss: 9153.95, base loss: 15348.98
[INFO 2017-07-01 19:29:06,834 main.py:52] epoch 661, training loss: 7895.08, average training loss: 9152.05, base loss: 15347.91
[INFO 2017-07-01 19:29:11,016 main.py:52] epoch 662, training loss: 9989.24, average training loss: 9153.31, base loss: 15351.71
[INFO 2017-07-01 19:29:15,247 main.py:52] epoch 663, training loss: 8707.34, average training loss: 9152.64, base loss: 15352.42
[INFO 2017-07-01 19:29:19,434 main.py:52] epoch 664, training loss: 8168.31, average training loss: 9151.16, base loss: 15352.31
[INFO 2017-07-01 19:29:23,656 main.py:52] epoch 665, training loss: 7587.18, average training loss: 9148.81, base loss: 15350.52
[INFO 2017-07-01 19:29:27,907 main.py:52] epoch 666, training loss: 7834.61, average training loss: 9146.84, base loss: 15349.00
[INFO 2017-07-01 19:29:32,037 main.py:52] epoch 667, training loss: 9790.02, average training loss: 9147.80, base loss: 15353.00
[INFO 2017-07-01 19:29:36,212 main.py:52] epoch 668, training loss: 8906.81, average training loss: 9147.44, base loss: 15355.70
[INFO 2017-07-01 19:29:40,444 main.py:52] epoch 669, training loss: 8733.33, average training loss: 9146.83, base loss: 15358.58
[INFO 2017-07-01 19:29:44,582 main.py:52] epoch 670, training loss: 8868.65, average training loss: 9146.41, base loss: 15360.58
[INFO 2017-07-01 19:29:48,730 main.py:52] epoch 671, training loss: 8768.28, average training loss: 9145.85, base loss: 15363.39
[INFO 2017-07-01 19:29:52,875 main.py:52] epoch 672, training loss: 8420.06, average training loss: 9144.77, base loss: 15364.74
[INFO 2017-07-01 19:29:57,065 main.py:52] epoch 673, training loss: 7753.20, average training loss: 9142.71, base loss: 15361.24
[INFO 2017-07-01 19:30:01,232 main.py:52] epoch 674, training loss: 8446.40, average training loss: 9141.67, base loss: 15360.57
[INFO 2017-07-01 19:30:05,443 main.py:52] epoch 675, training loss: 8477.18, average training loss: 9140.69, base loss: 15360.66
[INFO 2017-07-01 19:30:09,682 main.py:52] epoch 676, training loss: 7509.15, average training loss: 9138.28, base loss: 15358.45
[INFO 2017-07-01 19:30:13,971 main.py:52] epoch 677, training loss: 8906.86, average training loss: 9137.94, base loss: 15360.25
[INFO 2017-07-01 19:30:18,180 main.py:52] epoch 678, training loss: 7800.63, average training loss: 9135.97, base loss: 15357.11
[INFO 2017-07-01 19:30:22,469 main.py:52] epoch 679, training loss: 8272.36, average training loss: 9134.70, base loss: 15356.91
[INFO 2017-07-01 19:30:26,688 main.py:52] epoch 680, training loss: 7816.93, average training loss: 9132.77, base loss: 15353.09
[INFO 2017-07-01 19:30:30,856 main.py:52] epoch 681, training loss: 7813.59, average training loss: 9130.83, base loss: 15347.88
[INFO 2017-07-01 19:30:34,933 main.py:52] epoch 682, training loss: 8378.80, average training loss: 9129.73, base loss: 15346.26
[INFO 2017-07-01 19:30:39,087 main.py:52] epoch 683, training loss: 7282.98, average training loss: 9127.03, base loss: 15343.30
[INFO 2017-07-01 19:30:43,257 main.py:52] epoch 684, training loss: 8765.66, average training loss: 9126.50, base loss: 15345.49
[INFO 2017-07-01 19:30:47,386 main.py:52] epoch 685, training loss: 9063.23, average training loss: 9126.41, base loss: 15346.32
[INFO 2017-07-01 19:30:51,583 main.py:52] epoch 686, training loss: 8109.71, average training loss: 9124.93, base loss: 15347.10
[INFO 2017-07-01 19:30:55,789 main.py:52] epoch 687, training loss: 8643.90, average training loss: 9124.23, base loss: 15347.55
[INFO 2017-07-01 19:30:59,986 main.py:52] epoch 688, training loss: 8387.05, average training loss: 9123.16, base loss: 15348.86
[INFO 2017-07-01 19:31:04,141 main.py:52] epoch 689, training loss: 7946.38, average training loss: 9121.46, base loss: 15348.40
[INFO 2017-07-01 19:31:08,343 main.py:52] epoch 690, training loss: 8991.37, average training loss: 9121.27, base loss: 15348.94
[INFO 2017-07-01 19:31:12,506 main.py:52] epoch 691, training loss: 7716.18, average training loss: 9119.24, base loss: 15347.17
[INFO 2017-07-01 19:31:16,651 main.py:52] epoch 692, training loss: 8931.86, average training loss: 9118.97, base loss: 15346.40
[INFO 2017-07-01 19:31:20,868 main.py:52] epoch 693, training loss: 8613.71, average training loss: 9118.24, base loss: 15345.38
[INFO 2017-07-01 19:31:25,022 main.py:52] epoch 694, training loss: 7937.00, average training loss: 9116.54, base loss: 15344.55
[INFO 2017-07-01 19:31:29,168 main.py:52] epoch 695, training loss: 7953.66, average training loss: 9114.87, base loss: 15342.11
[INFO 2017-07-01 19:31:33,336 main.py:52] epoch 696, training loss: 8076.00, average training loss: 9113.38, base loss: 15341.60
[INFO 2017-07-01 19:31:37,496 main.py:52] epoch 697, training loss: 7902.24, average training loss: 9111.64, base loss: 15341.69
[INFO 2017-07-01 19:31:41,617 main.py:52] epoch 698, training loss: 8308.36, average training loss: 9110.49, base loss: 15342.21
[INFO 2017-07-01 19:31:45,762 main.py:52] epoch 699, training loss: 8233.78, average training loss: 9109.24, base loss: 15340.66
[INFO 2017-07-01 19:31:45,762 main.py:54] epoch 699, testing
[INFO 2017-07-01 19:31:45,762 main.py:61] model save to ./model/final.pth
[INFO 2017-07-01 19:31:50,028 main.py:52] epoch 700, training loss: 7588.25, average training loss: 9107.07, base loss: 15340.10
[INFO 2017-07-01 19:31:54,172 main.py:52] epoch 701, training loss: 8635.65, average training loss: 9106.40, base loss: 15341.61
[INFO 2017-07-01 19:31:58,340 main.py:52] epoch 702, training loss: 7746.59, average training loss: 9104.47, base loss: 15338.42
[INFO 2017-07-01 19:32:02,574 main.py:52] epoch 703, training loss: 7632.08, average training loss: 9102.37, base loss: 15335.54
[INFO 2017-07-01 19:32:06,719 main.py:52] epoch 704, training loss: 8698.02, average training loss: 9101.80, base loss: 15333.01
[INFO 2017-07-01 19:32:10,891 main.py:52] epoch 705, training loss: 7574.84, average training loss: 9099.64, base loss: 15328.33
[INFO 2017-07-01 19:32:15,073 main.py:52] epoch 706, training loss: 9188.85, average training loss: 9099.76, base loss: 15332.41
[INFO 2017-07-01 19:32:19,234 main.py:52] epoch 707, training loss: 8735.42, average training loss: 9099.25, base loss: 15333.20
[INFO 2017-07-01 19:32:23,543 main.py:52] epoch 708, training loss: 8552.83, average training loss: 9098.48, base loss: 15331.61
[INFO 2017-07-01 19:32:27,717 main.py:52] epoch 709, training loss: 8415.80, average training loss: 9097.52, base loss: 15333.02
[INFO 2017-07-01 19:32:31,880 main.py:52] epoch 710, training loss: 9113.89, average training loss: 9097.54, base loss: 15337.05
[INFO 2017-07-01 19:32:36,097 main.py:52] epoch 711, training loss: 8936.33, average training loss: 9097.31, base loss: 15340.58
[INFO 2017-07-01 19:32:40,269 main.py:52] epoch 712, training loss: 8856.42, average training loss: 9096.98, base loss: 15341.87
[INFO 2017-07-01 19:32:44,417 main.py:52] epoch 713, training loss: 7320.37, average training loss: 9094.49, base loss: 15337.95
[INFO 2017-07-01 19:32:48,602 main.py:52] epoch 714, training loss: 9025.81, average training loss: 9094.39, base loss: 15340.76
[INFO 2017-07-01 19:32:52,774 main.py:52] epoch 715, training loss: 8147.81, average training loss: 9093.07, base loss: 15341.36
[INFO 2017-07-01 19:32:56,880 main.py:52] epoch 716, training loss: 7835.28, average training loss: 9091.32, base loss: 15341.98
[INFO 2017-07-01 19:33:01,084 main.py:52] epoch 717, training loss: 8867.76, average training loss: 9091.00, base loss: 15345.21
[INFO 2017-07-01 19:33:05,276 main.py:52] epoch 718, training loss: 8381.80, average training loss: 9090.02, base loss: 15346.68
[INFO 2017-07-01 19:33:09,449 main.py:52] epoch 719, training loss: 7526.25, average training loss: 9087.85, base loss: 15342.72
[INFO 2017-07-01 19:33:13,668 main.py:52] epoch 720, training loss: 8904.71, average training loss: 9087.59, base loss: 15344.71
[INFO 2017-07-01 19:33:17,850 main.py:52] epoch 721, training loss: 7693.72, average training loss: 9085.66, base loss: 15343.73
[INFO 2017-07-01 19:33:21,985 main.py:52] epoch 722, training loss: 8402.69, average training loss: 9084.72, base loss: 15342.36
[INFO 2017-07-01 19:33:26,105 main.py:52] epoch 723, training loss: 9335.29, average training loss: 9085.06, base loss: 15344.52
[INFO 2017-07-01 19:33:30,251 main.py:52] epoch 724, training loss: 7715.79, average training loss: 9083.17, base loss: 15342.72
[INFO 2017-07-01 19:33:34,392 main.py:52] epoch 725, training loss: 8459.69, average training loss: 9082.32, base loss: 15343.28
[INFO 2017-07-01 19:33:38,607 main.py:52] epoch 726, training loss: 8601.82, average training loss: 9081.65, base loss: 15342.75
[INFO 2017-07-01 19:33:42,794 main.py:52] epoch 727, training loss: 8335.38, average training loss: 9080.63, base loss: 15341.77
[INFO 2017-07-01 19:33:46,967 main.py:52] epoch 728, training loss: 8822.90, average training loss: 9080.28, base loss: 15344.72
[INFO 2017-07-01 19:33:51,161 main.py:52] epoch 729, training loss: 9780.92, average training loss: 9081.24, base loss: 15348.59
[INFO 2017-07-01 19:33:55,304 main.py:52] epoch 730, training loss: 8941.70, average training loss: 9081.04, base loss: 15351.21
[INFO 2017-07-01 19:33:59,417 main.py:52] epoch 731, training loss: 9520.74, average training loss: 9081.65, base loss: 15353.21
[INFO 2017-07-01 19:34:03,605 main.py:52] epoch 732, training loss: 8729.00, average training loss: 9081.16, base loss: 15353.04
[INFO 2017-07-01 19:34:07,805 main.py:52] epoch 733, training loss: 9650.37, average training loss: 9081.94, base loss: 15356.34
[INFO 2017-07-01 19:34:11,947 main.py:52] epoch 734, training loss: 9620.88, average training loss: 9082.67, base loss: 15359.89
[INFO 2017-07-01 19:34:16,087 main.py:52] epoch 735, training loss: 7702.05, average training loss: 9080.80, base loss: 15358.50
[INFO 2017-07-01 19:34:20,262 main.py:52] epoch 736, training loss: 8461.45, average training loss: 9079.96, base loss: 15359.92
[INFO 2017-07-01 19:34:24,488 main.py:52] epoch 737, training loss: 8400.31, average training loss: 9079.04, base loss: 15359.04
[INFO 2017-07-01 19:34:28,636 main.py:52] epoch 738, training loss: 7672.44, average training loss: 9077.13, base loss: 15356.18
[INFO 2017-07-01 19:34:32,791 main.py:52] epoch 739, training loss: 8833.41, average training loss: 9076.80, base loss: 15354.82
[INFO 2017-07-01 19:34:37,000 main.py:52] epoch 740, training loss: 9023.40, average training loss: 9076.73, base loss: 15359.41
[INFO 2017-07-01 19:34:41,165 main.py:52] epoch 741, training loss: 7535.11, average training loss: 9074.65, base loss: 15358.76
[INFO 2017-07-01 19:34:45,389 main.py:52] epoch 742, training loss: 8214.51, average training loss: 9073.50, base loss: 15357.98
[INFO 2017-07-01 19:34:49,544 main.py:52] epoch 743, training loss: 8714.75, average training loss: 9073.01, base loss: 15358.43
[INFO 2017-07-01 19:34:53,686 main.py:52] epoch 744, training loss: 10011.15, average training loss: 9074.27, base loss: 15363.04
[INFO 2017-07-01 19:34:57,914 main.py:52] epoch 745, training loss: 9564.08, average training loss: 9074.93, base loss: 15367.36
[INFO 2017-07-01 19:35:02,150 main.py:52] epoch 746, training loss: 7519.63, average training loss: 9072.85, base loss: 15365.52
[INFO 2017-07-01 19:35:06,263 main.py:52] epoch 747, training loss: 8225.65, average training loss: 9071.71, base loss: 15364.06
[INFO 2017-07-01 19:35:10,399 main.py:52] epoch 748, training loss: 7907.88, average training loss: 9070.16, base loss: 15360.92
[INFO 2017-07-01 19:35:14,625 main.py:52] epoch 749, training loss: 9039.57, average training loss: 9070.12, base loss: 15362.89
[INFO 2017-07-01 19:35:18,757 main.py:52] epoch 750, training loss: 9100.64, average training loss: 9070.16, base loss: 15363.98
[INFO 2017-07-01 19:35:22,903 main.py:52] epoch 751, training loss: 6729.88, average training loss: 9067.05, base loss: 15359.64
[INFO 2017-07-01 19:35:27,140 main.py:52] epoch 752, training loss: 8131.52, average training loss: 9065.81, base loss: 15358.66
[INFO 2017-07-01 19:35:31,333 main.py:52] epoch 753, training loss: 8384.84, average training loss: 9064.90, base loss: 15357.63
[INFO 2017-07-01 19:35:35,501 main.py:52] epoch 754, training loss: 8327.33, average training loss: 9063.93, base loss: 15357.75
[INFO 2017-07-01 19:35:39,687 main.py:52] epoch 755, training loss: 7759.05, average training loss: 9062.20, base loss: 15355.79
[INFO 2017-07-01 19:35:43,862 main.py:52] epoch 756, training loss: 7365.02, average training loss: 9059.96, base loss: 15353.74
[INFO 2017-07-01 19:35:48,044 main.py:52] epoch 757, training loss: 7391.48, average training loss: 9057.76, base loss: 15350.99
[INFO 2017-07-01 19:35:52,152 main.py:52] epoch 758, training loss: 8557.03, average training loss: 9057.10, base loss: 15349.33
[INFO 2017-07-01 19:35:56,348 main.py:52] epoch 759, training loss: 8240.54, average training loss: 9056.02, base loss: 15349.03
[INFO 2017-07-01 19:36:00,483 main.py:52] epoch 760, training loss: 8499.95, average training loss: 9055.29, base loss: 15349.28
[INFO 2017-07-01 19:36:04,603 main.py:52] epoch 761, training loss: 8344.70, average training loss: 9054.36, base loss: 15351.31
[INFO 2017-07-01 19:36:08,707 main.py:52] epoch 762, training loss: 8081.06, average training loss: 9053.08, base loss: 15351.35
[INFO 2017-07-01 19:36:12,952 main.py:52] epoch 763, training loss: 7515.51, average training loss: 9051.07, base loss: 15347.90
[INFO 2017-07-01 19:36:17,103 main.py:52] epoch 764, training loss: 8297.50, average training loss: 9050.09, base loss: 15347.17
[INFO 2017-07-01 19:36:21,270 main.py:52] epoch 765, training loss: 7758.95, average training loss: 9048.40, base loss: 15345.24
[INFO 2017-07-01 19:36:25,425 main.py:52] epoch 766, training loss: 8578.93, average training loss: 9047.79, base loss: 15345.96
[INFO 2017-07-01 19:36:29,657 main.py:52] epoch 767, training loss: 9468.92, average training loss: 9048.34, base loss: 15349.73
[INFO 2017-07-01 19:36:33,869 main.py:52] epoch 768, training loss: 8824.04, average training loss: 9048.05, base loss: 15352.87
[INFO 2017-07-01 19:36:38,110 main.py:52] epoch 769, training loss: 7578.07, average training loss: 9046.14, base loss: 15352.45
[INFO 2017-07-01 19:36:42,242 main.py:52] epoch 770, training loss: 8045.43, average training loss: 9044.84, base loss: 15354.46
[INFO 2017-07-01 19:36:46,405 main.py:52] epoch 771, training loss: 8559.85, average training loss: 9044.21, base loss: 15355.28
[INFO 2017-07-01 19:36:50,584 main.py:52] epoch 772, training loss: 7573.33, average training loss: 9042.31, base loss: 15351.45
[INFO 2017-07-01 19:36:54,740 main.py:52] epoch 773, training loss: 8697.00, average training loss: 9041.86, base loss: 15349.70
[INFO 2017-07-01 19:36:58,908 main.py:52] epoch 774, training loss: 8037.70, average training loss: 9040.57, base loss: 15350.08
[INFO 2017-07-01 19:37:03,069 main.py:52] epoch 775, training loss: 10095.58, average training loss: 9041.93, base loss: 15351.77
[INFO 2017-07-01 19:37:07,292 main.py:52] epoch 776, training loss: 8928.41, average training loss: 9041.78, base loss: 15351.21
[INFO 2017-07-01 19:37:11,448 main.py:52] epoch 777, training loss: 8754.65, average training loss: 9041.41, base loss: 15349.34
[INFO 2017-07-01 19:37:15,618 main.py:52] epoch 778, training loss: 8513.66, average training loss: 9040.73, base loss: 15349.92
[INFO 2017-07-01 19:37:19,779 main.py:52] epoch 779, training loss: 8170.80, average training loss: 9039.62, base loss: 15348.35
[INFO 2017-07-01 19:37:23,992 main.py:52] epoch 780, training loss: 7759.56, average training loss: 9037.98, base loss: 15347.55
[INFO 2017-07-01 19:37:28,148 main.py:52] epoch 781, training loss: 8709.18, average training loss: 9037.56, base loss: 15349.18
[INFO 2017-07-01 19:37:32,339 main.py:52] epoch 782, training loss: 9644.48, average training loss: 9038.33, base loss: 15352.30
[INFO 2017-07-01 19:37:36,484 main.py:52] epoch 783, training loss: 9135.33, average training loss: 9038.46, base loss: 15354.12
[INFO 2017-07-01 19:37:40,651 main.py:52] epoch 784, training loss: 9026.41, average training loss: 9038.44, base loss: 15355.25
[INFO 2017-07-01 19:37:44,764 main.py:52] epoch 785, training loss: 8661.92, average training loss: 9037.96, base loss: 15357.45
[INFO 2017-07-01 19:37:48,990 main.py:52] epoch 786, training loss: 8136.65, average training loss: 9036.82, base loss: 15357.15
[INFO 2017-07-01 19:37:53,262 main.py:52] epoch 787, training loss: 8010.84, average training loss: 9035.52, base loss: 15357.70
[INFO 2017-07-01 19:37:57,393 main.py:52] epoch 788, training loss: 7350.98, average training loss: 9033.38, base loss: 15356.61
[INFO 2017-07-01 19:38:01,482 main.py:52] epoch 789, training loss: 7966.26, average training loss: 9032.03, base loss: 15355.57
[INFO 2017-07-01 19:38:05,664 main.py:52] epoch 790, training loss: 9370.90, average training loss: 9032.46, base loss: 15355.71
[INFO 2017-07-01 19:38:09,782 main.py:52] epoch 791, training loss: 7892.80, average training loss: 9031.02, base loss: 15353.34
[INFO 2017-07-01 19:38:13,979 main.py:52] epoch 792, training loss: 8403.68, average training loss: 9030.23, base loss: 15351.46
[INFO 2017-07-01 19:38:18,163 main.py:52] epoch 793, training loss: 7742.62, average training loss: 9028.61, base loss: 15349.10
[INFO 2017-07-01 19:38:22,282 main.py:52] epoch 794, training loss: 8840.31, average training loss: 9028.37, base loss: 15350.31
[INFO 2017-07-01 19:38:26,521 main.py:52] epoch 795, training loss: 8663.02, average training loss: 9027.91, base loss: 15350.11
[INFO 2017-07-01 19:38:30,645 main.py:52] epoch 796, training loss: 9461.34, average training loss: 9028.45, base loss: 15352.63
[INFO 2017-07-01 19:38:34,732 main.py:52] epoch 797, training loss: 8144.18, average training loss: 9027.35, base loss: 15350.99
[INFO 2017-07-01 19:38:38,871 main.py:52] epoch 798, training loss: 8932.58, average training loss: 9027.23, base loss: 15350.34
[INFO 2017-07-01 19:38:43,058 main.py:52] epoch 799, training loss: 8730.98, average training loss: 9026.86, base loss: 15350.09
[INFO 2017-07-01 19:38:43,059 main.py:54] epoch 799, testing
[INFO 2017-07-01 19:38:43,059 main.py:61] model save to ./model/final.pth
[INFO 2017-07-01 19:38:47,317 main.py:52] epoch 800, training loss: 7424.10, average training loss: 9024.86, base loss: 15346.64
[INFO 2017-07-01 19:38:51,596 main.py:52] epoch 801, training loss: 9812.37, average training loss: 9025.84, base loss: 15350.01
[INFO 2017-07-01 19:38:55,761 main.py:52] epoch 802, training loss: 9261.29, average training loss: 9026.13, base loss: 15351.97
[INFO 2017-07-01 19:38:59,894 main.py:52] epoch 803, training loss: 7952.42, average training loss: 9024.80, base loss: 15352.35
[INFO 2017-07-01 19:39:04,076 main.py:52] epoch 804, training loss: 8559.21, average training loss: 9024.22, base loss: 15354.70
[INFO 2017-07-01 19:39:08,337 main.py:52] epoch 805, training loss: 7777.20, average training loss: 9022.67, base loss: 15352.61
[INFO 2017-07-01 19:39:12,475 main.py:52] epoch 806, training loss: 8875.50, average training loss: 9022.49, base loss: 15354.16
[INFO 2017-07-01 19:39:16,703 main.py:52] epoch 807, training loss: 9491.31, average training loss: 9023.07, base loss: 15358.31
[INFO 2017-07-01 19:39:20,896 main.py:52] epoch 808, training loss: 8332.37, average training loss: 9022.21, base loss: 15357.98
[INFO 2017-07-01 19:39:25,036 main.py:52] epoch 809, training loss: 9405.85, average training loss: 9022.69, base loss: 15360.59
[INFO 2017-07-01 19:39:29,234 main.py:52] epoch 810, training loss: 7384.83, average training loss: 9020.67, base loss: 15358.60
[INFO 2017-07-01 19:39:33,336 main.py:52] epoch 811, training loss: 9347.29, average training loss: 9021.07, base loss: 15360.47
[INFO 2017-07-01 19:39:37,581 main.py:52] epoch 812, training loss: 8088.98, average training loss: 9019.92, base loss: 15359.36
[INFO 2017-07-01 19:39:41,743 main.py:52] epoch 813, training loss: 7583.09, average training loss: 9018.16, base loss: 15356.23
[INFO 2017-07-01 19:39:45,974 main.py:52] epoch 814, training loss: 8501.08, average training loss: 9017.52, base loss: 15357.31
[INFO 2017-07-01 19:39:50,069 main.py:52] epoch 815, training loss: 7749.42, average training loss: 9015.97, base loss: 15355.74
[INFO 2017-07-01 19:39:54,263 main.py:52] epoch 816, training loss: 7489.47, average training loss: 9014.10, base loss: 15351.69
[INFO 2017-07-01 19:39:58,433 main.py:52] epoch 817, training loss: 7625.79, average training loss: 9012.40, base loss: 15350.47
[INFO 2017-07-01 19:40:02,533 main.py:52] epoch 818, training loss: 8489.75, average training loss: 9011.77, base loss: 15353.03
[INFO 2017-07-01 19:40:06,728 main.py:52] epoch 819, training loss: 8454.12, average training loss: 9011.09, base loss: 15353.57
[INFO 2017-07-01 19:40:10,879 main.py:52] epoch 820, training loss: 7778.71, average training loss: 9009.59, base loss: 15351.91
[INFO 2017-07-01 19:40:15,030 main.py:52] epoch 821, training loss: 8847.04, average training loss: 9009.39, base loss: 15352.35
[INFO 2017-07-01 19:40:19,252 main.py:52] epoch 822, training loss: 8517.51, average training loss: 9008.79, base loss: 15354.25
[INFO 2017-07-01 19:40:23,431 main.py:52] epoch 823, training loss: 8581.87, average training loss: 9008.27, base loss: 15355.24
[INFO 2017-07-01 19:40:27,586 main.py:52] epoch 824, training loss: 7832.88, average training loss: 9006.85, base loss: 15354.87
[INFO 2017-07-01 19:40:31,734 main.py:52] epoch 825, training loss: 7401.80, average training loss: 9004.90, base loss: 15353.93
[INFO 2017-07-01 19:40:35,827 main.py:52] epoch 826, training loss: 9395.96, average training loss: 9005.38, base loss: 15355.07
[INFO 2017-07-01 19:40:39,990 main.py:52] epoch 827, training loss: 9912.54, average training loss: 9006.47, base loss: 15357.73
[INFO 2017-07-01 19:40:44,175 main.py:52] epoch 828, training loss: 8376.97, average training loss: 9005.71, base loss: 15357.53
[INFO 2017-07-01 19:40:48,258 main.py:52] epoch 829, training loss: 8472.82, average training loss: 9005.07, base loss: 15356.63
[INFO 2017-07-01 19:40:52,430 main.py:52] epoch 830, training loss: 9460.53, average training loss: 9005.62, base loss: 15357.13
[INFO 2017-07-01 19:40:56,586 main.py:52] epoch 831, training loss: 8321.08, average training loss: 9004.80, base loss: 15357.10
[INFO 2017-07-01 19:41:00,725 main.py:52] epoch 832, training loss: 8185.33, average training loss: 9003.81, base loss: 15354.42
[INFO 2017-07-01 19:41:04,897 main.py:52] epoch 833, training loss: 7671.21, average training loss: 9002.21, base loss: 15352.37
[INFO 2017-07-01 19:41:09,075 main.py:52] epoch 834, training loss: 7819.53, average training loss: 9000.80, base loss: 15350.22
[INFO 2017-07-01 19:41:13,177 main.py:52] epoch 835, training loss: 8377.41, average training loss: 9000.05, base loss: 15351.07
[INFO 2017-07-01 19:41:17,368 main.py:52] epoch 836, training loss: 8261.53, average training loss: 8999.17, base loss: 15350.75
[INFO 2017-07-01 19:41:21,574 main.py:52] epoch 837, training loss: 8059.93, average training loss: 8998.05, base loss: 15350.68
[INFO 2017-07-01 19:41:25,732 main.py:52] epoch 838, training loss: 8353.67, average training loss: 8997.28, base loss: 15353.02
[INFO 2017-07-01 19:41:29,966 main.py:52] epoch 839, training loss: 8523.26, average training loss: 8996.72, base loss: 15355.71
[INFO 2017-07-01 19:41:34,158 main.py:52] epoch 840, training loss: 8636.79, average training loss: 8996.29, base loss: 15356.21
[INFO 2017-07-01 19:41:38,327 main.py:52] epoch 841, training loss: 9182.93, average training loss: 8996.51, base loss: 15360.18
[INFO 2017-07-01 19:41:42,553 main.py:52] epoch 842, training loss: 8017.76, average training loss: 8995.35, base loss: 15358.85
[INFO 2017-07-01 19:41:46,841 main.py:52] epoch 843, training loss: 7770.88, average training loss: 8993.90, base loss: 15356.73
[INFO 2017-07-01 19:41:51,038 main.py:52] epoch 844, training loss: 7634.06, average training loss: 8992.29, base loss: 15354.38
[INFO 2017-07-01 19:41:55,273 main.py:52] epoch 845, training loss: 9142.76, average training loss: 8992.47, base loss: 15355.54
[INFO 2017-07-01 19:41:59,437 main.py:52] epoch 846, training loss: 9142.94, average training loss: 8992.65, base loss: 15356.50
[INFO 2017-07-01 19:42:03,651 main.py:52] epoch 847, training loss: 8485.60, average training loss: 8992.05, base loss: 15356.07
[INFO 2017-07-01 19:42:07,762 main.py:52] epoch 848, training loss: 8279.93, average training loss: 8991.21, base loss: 15355.11
[INFO 2017-07-01 19:42:11,925 main.py:52] epoch 849, training loss: 7974.01, average training loss: 8990.01, base loss: 15353.00
[INFO 2017-07-01 19:42:16,153 main.py:52] epoch 850, training loss: 8209.80, average training loss: 8989.10, base loss: 15353.50
[INFO 2017-07-01 19:42:20,367 main.py:52] epoch 851, training loss: 8406.43, average training loss: 8988.41, base loss: 15353.27
[INFO 2017-07-01 19:42:24,525 main.py:52] epoch 852, training loss: 7854.11, average training loss: 8987.08, base loss: 15352.52
[INFO 2017-07-01 19:42:28,648 main.py:52] epoch 853, training loss: 8913.77, average training loss: 8987.00, base loss: 15355.89
[INFO 2017-07-01 19:42:32,740 main.py:52] epoch 854, training loss: 7420.91, average training loss: 8985.16, base loss: 15353.48
[INFO 2017-07-01 19:42:36,907 main.py:52] epoch 855, training loss: 9106.30, average training loss: 8985.31, base loss: 15355.31
[INFO 2017-07-01 19:42:41,025 main.py:52] epoch 856, training loss: 8234.43, average training loss: 8984.43, base loss: 15353.65
[INFO 2017-07-01 19:42:45,203 main.py:52] epoch 857, training loss: 8072.01, average training loss: 8983.37, base loss: 15352.90
[INFO 2017-07-01 19:42:49,310 main.py:52] epoch 858, training loss: 8653.09, average training loss: 8982.98, base loss: 15353.03
[INFO 2017-07-01 19:42:53,487 main.py:52] epoch 859, training loss: 7972.11, average training loss: 8981.81, base loss: 15352.46
[INFO 2017-07-01 19:42:57,741 main.py:52] epoch 860, training loss: 7774.94, average training loss: 8980.40, base loss: 15351.02
[INFO 2017-07-01 19:43:01,971 main.py:52] epoch 861, training loss: 8173.74, average training loss: 8979.47, base loss: 15353.50
[INFO 2017-07-01 19:43:06,075 main.py:52] epoch 862, training loss: 8468.99, average training loss: 8978.88, base loss: 15356.24
[INFO 2017-07-01 19:43:10,247 main.py:52] epoch 863, training loss: 8076.00, average training loss: 8977.83, base loss: 15355.26
[INFO 2017-07-01 19:43:14,449 main.py:52] epoch 864, training loss: 8654.23, average training loss: 8977.46, base loss: 15354.57
[INFO 2017-07-01 19:43:18,595 main.py:52] epoch 865, training loss: 7959.93, average training loss: 8976.28, base loss: 15354.69
[INFO 2017-07-01 19:43:22,754 main.py:52] epoch 866, training loss: 8463.35, average training loss: 8975.69, base loss: 15353.05
[INFO 2017-07-01 19:43:26,900 main.py:52] epoch 867, training loss: 7616.80, average training loss: 8974.13, base loss: 15350.92
[INFO 2017-07-01 19:43:31,025 main.py:52] epoch 868, training loss: 8575.61, average training loss: 8973.67, base loss: 15353.57
[INFO 2017-07-01 19:43:35,173 main.py:52] epoch 869, training loss: 7414.08, average training loss: 8971.87, base loss: 15350.95
[INFO 2017-07-01 19:43:39,272 main.py:52] epoch 870, training loss: 7934.63, average training loss: 8970.68, base loss: 15350.66
[INFO 2017-07-01 19:43:43,436 main.py:52] epoch 871, training loss: 7544.70, average training loss: 8969.05, base loss: 15348.70
[INFO 2017-07-01 19:43:47,619 main.py:52] epoch 872, training loss: 7925.59, average training loss: 8967.85, base loss: 15349.98
[INFO 2017-07-01 19:43:51,802 main.py:52] epoch 873, training loss: 8830.70, average training loss: 8967.70, base loss: 15352.35
[INFO 2017-07-01 19:43:55,976 main.py:52] epoch 874, training loss: 8483.57, average training loss: 8967.14, base loss: 15353.70
[INFO 2017-07-01 19:44:00,113 main.py:52] epoch 875, training loss: 9278.57, average training loss: 8967.50, base loss: 15355.54
[INFO 2017-07-01 19:44:04,299 main.py:52] epoch 876, training loss: 7964.35, average training loss: 8966.35, base loss: 15354.88
[INFO 2017-07-01 19:44:08,418 main.py:52] epoch 877, training loss: 9254.95, average training loss: 8966.68, base loss: 15356.01
[INFO 2017-07-01 19:44:12,557 main.py:52] epoch 878, training loss: 8886.20, average training loss: 8966.59, base loss: 15355.20
[INFO 2017-07-01 19:44:16,779 main.py:52] epoch 879, training loss: 8142.48, average training loss: 8965.66, base loss: 15353.93
[INFO 2017-07-01 19:44:21,012 main.py:52] epoch 880, training loss: 8617.56, average training loss: 8965.26, base loss: 15355.13
[INFO 2017-07-01 19:44:25,151 main.py:52] epoch 881, training loss: 7481.61, average training loss: 8963.58, base loss: 15354.96
[INFO 2017-07-01 19:44:29,346 main.py:52] epoch 882, training loss: 8831.81, average training loss: 8963.43, base loss: 15357.21
[INFO 2017-07-01 19:44:33,507 main.py:52] epoch 883, training loss: 9861.34, average training loss: 8964.44, base loss: 15359.53
[INFO 2017-07-01 19:44:37,693 main.py:52] epoch 884, training loss: 8926.28, average training loss: 8964.40, base loss: 15361.45
[INFO 2017-07-01 19:44:41,837 main.py:52] epoch 885, training loss: 8462.53, average training loss: 8963.83, base loss: 15362.41
[INFO 2017-07-01 19:44:46,083 main.py:52] epoch 886, training loss: 8251.69, average training loss: 8963.03, base loss: 15362.49
[INFO 2017-07-01 19:44:50,282 main.py:52] epoch 887, training loss: 8623.55, average training loss: 8962.65, base loss: 15363.46
[INFO 2017-07-01 19:44:54,488 main.py:52] epoch 888, training loss: 7851.96, average training loss: 8961.40, base loss: 15361.09
[INFO 2017-07-01 19:44:58,614 main.py:52] epoch 889, training loss: 7555.53, average training loss: 8959.82, base loss: 15358.60
[INFO 2017-07-01 19:45:02,791 main.py:52] epoch 890, training loss: 8698.82, average training loss: 8959.53, base loss: 15360.52
[INFO 2017-07-01 19:45:06,968 main.py:52] epoch 891, training loss: 8916.66, average training loss: 8959.48, base loss: 15363.24
[INFO 2017-07-01 19:45:11,090 main.py:52] epoch 892, training loss: 7450.72, average training loss: 8957.79, base loss: 15360.73
[INFO 2017-07-01 19:45:15,276 main.py:52] epoch 893, training loss: 8490.90, average training loss: 8957.27, base loss: 15360.76
[INFO 2017-07-01 19:45:19,464 main.py:52] epoch 894, training loss: 8357.98, average training loss: 8956.60, base loss: 15362.08
[INFO 2017-07-01 19:45:23,681 main.py:52] epoch 895, training loss: 9307.41, average training loss: 8956.99, base loss: 15363.90
[INFO 2017-07-01 19:45:27,807 main.py:52] epoch 896, training loss: 8048.26, average training loss: 8955.98, base loss: 15363.68
[INFO 2017-07-01 19:45:32,015 main.py:52] epoch 897, training loss: 8644.44, average training loss: 8955.63, base loss: 15365.03
[INFO 2017-07-01 19:45:36,154 main.py:52] epoch 898, training loss: 8040.12, average training loss: 8954.61, base loss: 15365.06
[INFO 2017-07-01 19:45:40,323 main.py:52] epoch 899, training loss: 8388.63, average training loss: 8953.98, base loss: 15364.93
[INFO 2017-07-01 19:45:40,323 main.py:54] epoch 899, testing
[INFO 2017-07-01 19:45:40,323 main.py:61] model save to ./model/final.pth
[INFO 2017-07-01 19:45:44,558 main.py:52] epoch 900, training loss: 6956.50, average training loss: 8951.77, base loss: 15361.92
[INFO 2017-07-01 19:45:48,741 main.py:52] epoch 901, training loss: 7911.26, average training loss: 8950.61, base loss: 15361.88
[INFO 2017-07-01 19:45:52,912 main.py:52] epoch 902, training loss: 8583.14, average training loss: 8950.21, base loss: 15362.16
[INFO 2017-07-01 19:45:57,136 main.py:52] epoch 903, training loss: 8638.83, average training loss: 8949.86, base loss: 15362.20
[INFO 2017-07-01 19:46:01,286 main.py:52] epoch 904, training loss: 8491.75, average training loss: 8949.35, base loss: 15364.63
[INFO 2017-07-01 19:46:05,441 main.py:52] epoch 905, training loss: 7458.28, average training loss: 8947.71, base loss: 15362.20
[INFO 2017-07-01 19:46:09,667 main.py:52] epoch 906, training loss: 7520.99, average training loss: 8946.14, base loss: 15359.43
[INFO 2017-07-01 19:46:13,778 main.py:52] epoch 907, training loss: 8838.22, average training loss: 8946.02, base loss: 15359.52
[INFO 2017-07-01 19:46:17,932 main.py:52] epoch 908, training loss: 8049.88, average training loss: 8945.03, base loss: 15360.76
[INFO 2017-07-01 19:46:22,069 main.py:52] epoch 909, training loss: 9272.31, average training loss: 8945.39, base loss: 15361.24
[INFO 2017-07-01 19:46:26,273 main.py:52] epoch 910, training loss: 8189.59, average training loss: 8944.56, base loss: 15360.81
[INFO 2017-07-01 19:46:30,495 main.py:52] epoch 911, training loss: 8533.33, average training loss: 8944.11, base loss: 15361.67
[INFO 2017-07-01 19:46:34,661 main.py:52] epoch 912, training loss: 8694.85, average training loss: 8943.84, base loss: 15362.51
[INFO 2017-07-01 19:46:38,804 main.py:52] epoch 913, training loss: 7613.21, average training loss: 8942.38, base loss: 15362.53
[INFO 2017-07-01 19:46:42,951 main.py:52] epoch 914, training loss: 7894.81, average training loss: 8941.24, base loss: 15362.91
[INFO 2017-07-01 19:46:47,086 main.py:52] epoch 915, training loss: 8255.40, average training loss: 8940.49, base loss: 15361.74
[INFO 2017-07-01 19:46:51,264 main.py:52] epoch 916, training loss: 7931.56, average training loss: 8939.39, base loss: 15359.08
[INFO 2017-07-01 19:46:55,482 main.py:52] epoch 917, training loss: 8450.98, average training loss: 8938.86, base loss: 15358.95
[INFO 2017-07-01 19:46:59,658 main.py:52] epoch 918, training loss: 8762.55, average training loss: 8938.66, base loss: 15359.56
[INFO 2017-07-01 19:47:03,830 main.py:52] epoch 919, training loss: 7520.70, average training loss: 8937.12, base loss: 15357.22
[INFO 2017-07-01 19:47:08,028 main.py:52] epoch 920, training loss: 8040.75, average training loss: 8936.15, base loss: 15359.03
[INFO 2017-07-01 19:47:12,188 main.py:52] epoch 921, training loss: 8932.68, average training loss: 8936.15, base loss: 15359.23
[INFO 2017-07-01 19:47:16,409 main.py:52] epoch 922, training loss: 8911.83, average training loss: 8936.12, base loss: 15360.21
[INFO 2017-07-01 19:47:20,513 main.py:52] epoch 923, training loss: 8243.06, average training loss: 8935.37, base loss: 15359.91
[INFO 2017-07-01 19:47:24,703 main.py:52] epoch 924, training loss: 8732.54, average training loss: 8935.15, base loss: 15361.42
[INFO 2017-07-01 19:47:28,817 main.py:52] epoch 925, training loss: 8889.10, average training loss: 8935.10, base loss: 15363.23
[INFO 2017-07-01 19:47:33,001 main.py:52] epoch 926, training loss: 8586.62, average training loss: 8934.72, base loss: 15366.12
[INFO 2017-07-01 19:47:37,204 main.py:52] epoch 927, training loss: 7758.90, average training loss: 8933.46, base loss: 15365.00
[INFO 2017-07-01 19:47:41,307 main.py:52] epoch 928, training loss: 8440.98, average training loss: 8932.93, base loss: 15363.12
[INFO 2017-07-01 19:47:45,475 main.py:52] epoch 929, training loss: 8279.82, average training loss: 8932.22, base loss: 15362.98
[INFO 2017-07-01 19:47:49,642 main.py:52] epoch 930, training loss: 9120.50, average training loss: 8932.43, base loss: 15364.11
[INFO 2017-07-01 19:47:53,776 main.py:52] epoch 931, training loss: 8400.92, average training loss: 8931.86, base loss: 15365.47
[INFO 2017-07-01 19:47:57,915 main.py:52] epoch 932, training loss: 8416.18, average training loss: 8931.30, base loss: 15365.49
[INFO 2017-07-01 19:48:02,098 main.py:52] epoch 933, training loss: 7407.06, average training loss: 8929.67, base loss: 15363.39
[INFO 2017-07-01 19:48:06,314 main.py:52] epoch 934, training loss: 8904.30, average training loss: 8929.64, base loss: 15363.56
[INFO 2017-07-01 19:48:10,451 main.py:52] epoch 935, training loss: 7228.32, average training loss: 8927.83, base loss: 15359.13
[INFO 2017-07-01 19:48:14,604 main.py:52] epoch 936, training loss: 8507.59, average training loss: 8927.38, base loss: 15360.79
[INFO 2017-07-01 19:48:18,759 main.py:52] epoch 937, training loss: 8031.89, average training loss: 8926.42, base loss: 15361.10
[INFO 2017-07-01 19:48:22,930 main.py:52] epoch 938, training loss: 7885.77, average training loss: 8925.32, base loss: 15360.64
[INFO 2017-07-01 19:48:27,142 main.py:52] epoch 939, training loss: 9111.73, average training loss: 8925.51, base loss: 15360.99
[INFO 2017-07-01 19:48:31,406 main.py:52] epoch 940, training loss: 8168.60, average training loss: 8924.71, base loss: 15362.48
[INFO 2017-07-01 19:48:35,608 main.py:52] epoch 941, training loss: 8711.89, average training loss: 8924.48, base loss: 15364.79
[INFO 2017-07-01 19:48:39,769 main.py:52] epoch 942, training loss: 8164.72, average training loss: 8923.68, base loss: 15365.06
[INFO 2017-07-01 19:48:43,923 main.py:52] epoch 943, training loss: 7374.81, average training loss: 8922.04, base loss: 15364.18
[INFO 2017-07-01 19:48:48,136 main.py:52] epoch 944, training loss: 7585.46, average training loss: 8920.62, base loss: 15362.16
[INFO 2017-07-01 19:48:52,258 main.py:52] epoch 945, training loss: 9692.47, average training loss: 8921.44, base loss: 15362.61
[INFO 2017-07-01 19:48:56,443 main.py:52] epoch 946, training loss: 8518.91, average training loss: 8921.01, base loss: 15362.03
[INFO 2017-07-01 19:49:00,568 main.py:52] epoch 947, training loss: 8936.91, average training loss: 8921.03, base loss: 15363.20
[INFO 2017-07-01 19:49:04,790 main.py:52] epoch 948, training loss: 7493.12, average training loss: 8919.53, base loss: 15360.88
[INFO 2017-07-01 19:49:08,970 main.py:52] epoch 949, training loss: 8194.82, average training loss: 8918.76, base loss: 15362.00
[INFO 2017-07-01 19:49:13,150 main.py:52] epoch 950, training loss: 7648.18, average training loss: 8917.43, base loss: 15360.23
[INFO 2017-07-01 19:49:17,288 main.py:52] epoch 951, training loss: 6965.54, average training loss: 8915.38, base loss: 15355.82
[INFO 2017-07-01 19:49:21,449 main.py:52] epoch 952, training loss: 8244.06, average training loss: 8914.67, base loss: 15355.29
[INFO 2017-07-01 19:49:25,595 main.py:52] epoch 953, training loss: 8362.62, average training loss: 8914.09, base loss: 15355.61
[INFO 2017-07-01 19:49:29,829 main.py:52] epoch 954, training loss: 7546.47, average training loss: 8912.66, base loss: 15354.81
[INFO 2017-07-01 19:49:33,990 main.py:52] epoch 955, training loss: 7168.16, average training loss: 8910.84, base loss: 15352.62
[INFO 2017-07-01 19:49:38,221 main.py:52] epoch 956, training loss: 8798.86, average training loss: 8910.72, base loss: 15355.09
[INFO 2017-07-01 19:49:42,357 main.py:52] epoch 957, training loss: 8945.81, average training loss: 8910.76, base loss: 15358.49
[INFO 2017-07-01 19:49:46,510 main.py:52] epoch 958, training loss: 8926.35, average training loss: 8910.77, base loss: 15359.45
[INFO 2017-07-01 19:49:50,718 main.py:52] epoch 959, training loss: 8048.35, average training loss: 8909.87, base loss: 15358.10
[INFO 2017-07-01 19:49:54,860 main.py:52] epoch 960, training loss: 7973.91, average training loss: 8908.90, base loss: 15358.69
[INFO 2017-07-01 19:49:59,019 main.py:52] epoch 961, training loss: 8440.06, average training loss: 8908.41, base loss: 15358.62
[INFO 2017-07-01 19:50:03,196 main.py:52] epoch 962, training loss: 7930.47, average training loss: 8907.40, base loss: 15358.81
[INFO 2017-07-01 19:50:07,363 main.py:52] epoch 963, training loss: 7961.16, average training loss: 8906.42, base loss: 15358.40
[INFO 2017-07-01 19:50:11,594 main.py:52] epoch 964, training loss: 8965.95, average training loss: 8906.48, base loss: 15359.97
[INFO 2017-07-01 19:50:15,767 main.py:52] epoch 965, training loss: 8452.40, average training loss: 8906.01, base loss: 15360.52
[INFO 2017-07-01 19:50:19,900 main.py:52] epoch 966, training loss: 8061.20, average training loss: 8905.13, base loss: 15360.99
[INFO 2017-07-01 19:50:24,097 main.py:52] epoch 967, training loss: 7916.35, average training loss: 8904.11, base loss: 15360.52
[INFO 2017-07-01 19:50:28,200 main.py:52] epoch 968, training loss: 7654.46, average training loss: 8902.82, base loss: 15358.54
[INFO 2017-07-01 19:50:32,326 main.py:52] epoch 969, training loss: 9048.48, average training loss: 8902.97, base loss: 15359.94
[INFO 2017-07-01 19:50:36,479 main.py:52] epoch 970, training loss: 7042.83, average training loss: 8901.06, base loss: 15357.01
[INFO 2017-07-01 19:50:40,645 main.py:52] epoch 971, training loss: 7636.40, average training loss: 8899.76, base loss: 15355.23
[INFO 2017-07-01 19:50:44,816 main.py:52] epoch 972, training loss: 7917.90, average training loss: 8898.75, base loss: 15355.53
[INFO 2017-07-01 19:50:48,993 main.py:52] epoch 973, training loss: 7831.10, average training loss: 8897.65, base loss: 15355.51
[INFO 2017-07-01 19:50:53,197 main.py:52] epoch 974, training loss: 7401.25, average training loss: 8896.12, base loss: 15352.88
[INFO 2017-07-01 19:50:57,341 main.py:52] epoch 975, training loss: 8427.76, average training loss: 8895.64, base loss: 15352.42
[INFO 2017-07-01 19:51:01,509 main.py:52] epoch 976, training loss: 7652.85, average training loss: 8894.36, base loss: 15353.07
[INFO 2017-07-01 19:51:05,596 main.py:52] epoch 977, training loss: 7514.10, average training loss: 8892.95, base loss: 15352.14
[INFO 2017-07-01 19:51:09,814 main.py:52] epoch 978, training loss: 8521.66, average training loss: 8892.57, base loss: 15353.16
[INFO 2017-07-01 19:51:14,032 main.py:52] epoch 979, training loss: 8247.57, average training loss: 8891.92, base loss: 15354.22
[INFO 2017-07-01 19:51:18,209 main.py:52] epoch 980, training loss: 7787.09, average training loss: 8890.79, base loss: 15351.73
[INFO 2017-07-01 19:51:22,385 main.py:52] epoch 981, training loss: 8205.86, average training loss: 8890.09, base loss: 15353.01
[INFO 2017-07-01 19:51:26,588 main.py:52] epoch 982, training loss: 8722.22, average training loss: 8889.92, base loss: 15354.66
[INFO 2017-07-01 19:51:30,799 main.py:52] epoch 983, training loss: 7361.62, average training loss: 8888.37, base loss: 15353.54
[INFO 2017-07-01 19:51:35,001 main.py:52] epoch 984, training loss: 8284.30, average training loss: 8887.75, base loss: 15352.09
[INFO 2017-07-01 19:51:39,187 main.py:52] epoch 985, training loss: 8514.30, average training loss: 8887.38, base loss: 15352.58
[INFO 2017-07-01 19:51:43,382 main.py:52] epoch 986, training loss: 7631.41, average training loss: 8886.10, base loss: 15351.68
[INFO 2017-07-01 19:51:47,544 main.py:52] epoch 987, training loss: 8081.06, average training loss: 8885.29, base loss: 15352.74
[INFO 2017-07-01 19:51:51,688 main.py:52] epoch 988, training loss: 9339.59, average training loss: 8885.75, base loss: 15357.42
[INFO 2017-07-01 19:51:55,833 main.py:52] epoch 989, training loss: 7681.67, average training loss: 8884.53, base loss: 15357.86
[INFO 2017-07-01 19:52:00,024 main.py:52] epoch 990, training loss: 8570.45, average training loss: 8884.21, base loss: 15358.92
[INFO 2017-07-01 19:52:04,256 main.py:52] epoch 991, training loss: 7160.52, average training loss: 8882.48, base loss: 15357.94
[INFO 2017-07-01 19:52:08,416 main.py:52] epoch 992, training loss: 8584.09, average training loss: 8882.18, base loss: 15356.54
[INFO 2017-07-01 19:52:12,579 main.py:52] epoch 993, training loss: 8584.69, average training loss: 8881.88, base loss: 15355.00
[INFO 2017-07-01 19:52:16,748 main.py:52] epoch 994, training loss: 8878.01, average training loss: 8881.87, base loss: 15356.50
[INFO 2017-07-01 19:52:20,855 main.py:52] epoch 995, training loss: 8114.00, average training loss: 8881.10, base loss: 15356.37
[INFO 2017-07-01 19:52:24,980 main.py:52] epoch 996, training loss: 9202.42, average training loss: 8881.42, base loss: 15356.59
[INFO 2017-07-01 19:52:29,144 main.py:52] epoch 997, training loss: 7936.32, average training loss: 8880.48, base loss: 15354.14
[INFO 2017-07-01 19:52:33,345 main.py:52] epoch 998, training loss: 8308.76, average training loss: 8879.91, base loss: 15353.54
[INFO 2017-07-01 19:52:37,517 main.py:52] epoch 999, training loss: 7615.75, average training loss: 8878.64, base loss: 15351.01
[INFO 2017-07-01 19:52:37,517 main.py:54] epoch 999, testing
[INFO 2017-07-01 19:52:37,517 main.py:61] model save to ./model/final.pth
[INFO 2017-07-01 19:52:41,780 main.py:52] epoch 1000, training loss: 8629.12, average training loss: 8845.56, base loss: 15351.93
[INFO 2017-07-01 19:52:46,028 main.py:52] epoch 1001, training loss: 7700.15, average training loss: 8820.51, base loss: 15349.74
[INFO 2017-07-01 19:52:50,162 main.py:52] epoch 1002, training loss: 7687.42, average training loss: 8799.78, base loss: 15348.84
[INFO 2017-07-01 19:52:54,316 main.py:52] epoch 1003, training loss: 8701.26, average training loss: 8784.14, base loss: 15350.23
[INFO 2017-07-01 19:52:58,489 main.py:52] epoch 1004, training loss: 7110.78, average training loss: 8769.76, base loss: 15347.05
[INFO 2017-07-01 19:53:02,679 main.py:52] epoch 1005, training loss: 9443.09, average training loss: 8761.21, base loss: 15349.64
[INFO 2017-07-01 19:53:06,888 main.py:52] epoch 1006, training loss: 8434.50, average training loss: 8752.53, base loss: 15348.64
[INFO 2017-07-01 19:53:11,122 main.py:52] epoch 1007, training loss: 7865.65, average training loss: 8743.39, base loss: 15346.82
[INFO 2017-07-01 19:53:15,315 main.py:52] epoch 1008, training loss: 7993.63, average training loss: 8734.44, base loss: 15346.35
[INFO 2017-07-01 19:53:19,429 main.py:52] epoch 1009, training loss: 9010.64, average training loss: 8728.31, base loss: 15347.65
[INFO 2017-07-01 19:53:23,602 main.py:52] epoch 1010, training loss: 8517.74, average training loss: 8722.82, base loss: 15346.07
[INFO 2017-07-01 19:53:27,778 main.py:52] epoch 1011, training loss: 8097.90, average training loss: 8719.12, base loss: 15344.65
[INFO 2017-07-01 19:53:31,948 main.py:52] epoch 1012, training loss: 8552.21, average training loss: 8715.12, base loss: 15346.12
[INFO 2017-07-01 19:53:36,134 main.py:52] epoch 1013, training loss: 8170.64, average training loss: 8710.57, base loss: 15346.28
[INFO 2017-07-01 19:53:40,347 main.py:52] epoch 1014, training loss: 8819.08, average training loss: 8707.68, base loss: 15346.14
[INFO 2017-07-01 19:53:44,530 main.py:52] epoch 1015, training loss: 8433.41, average training loss: 8705.30, base loss: 15345.12
[INFO 2017-07-01 19:53:48,630 main.py:52] epoch 1016, training loss: 7747.84, average training loss: 8701.47, base loss: 15345.50
[INFO 2017-07-01 19:53:52,799 main.py:52] epoch 1017, training loss: 7379.74, average training loss: 8698.23, base loss: 15343.42
[INFO 2017-07-01 19:53:56,964 main.py:52] epoch 1018, training loss: 8288.77, average training loss: 8694.85, base loss: 15342.89
[INFO 2017-07-01 19:54:01,155 main.py:52] epoch 1019, training loss: 8176.54, average training loss: 8691.45, base loss: 15341.89
[INFO 2017-07-01 19:54:05,372 main.py:52] epoch 1020, training loss: 8673.63, average training loss: 8689.12, base loss: 15343.46
[INFO 2017-07-01 19:54:09,590 main.py:52] epoch 1021, training loss: 8210.51, average training loss: 8685.91, base loss: 15342.46
[INFO 2017-07-01 19:54:13,780 main.py:52] epoch 1022, training loss: 7501.17, average training loss: 8681.20, base loss: 15341.20
[INFO 2017-07-01 19:54:17,915 main.py:52] epoch 1023, training loss: 8585.33, average training loss: 8678.39, base loss: 15341.15
[INFO 2017-07-01 19:54:22,128 main.py:52] epoch 1024, training loss: 8317.16, average training loss: 8677.70, base loss: 15340.63
[INFO 2017-07-01 19:54:26,251 main.py:52] epoch 1025, training loss: 8620.28, average training loss: 8676.68, base loss: 15341.38
[INFO 2017-07-01 19:54:30,390 main.py:52] epoch 1026, training loss: 7228.51, average training loss: 8672.35, base loss: 15337.93
[INFO 2017-07-01 19:54:34,573 main.py:52] epoch 1027, training loss: 7227.77, average training loss: 8668.74, base loss: 15334.81
[INFO 2017-07-01 19:54:38,669 main.py:52] epoch 1028, training loss: 8224.74, average training loss: 8666.54, base loss: 15337.17
[INFO 2017-07-01 19:54:42,787 main.py:52] epoch 1029, training loss: 7616.06, average training loss: 8663.88, base loss: 15338.32
[INFO 2017-07-01 19:54:46,944 main.py:52] epoch 1030, training loss: 7670.35, average training loss: 8661.41, base loss: 15339.47
[INFO 2017-07-01 19:54:51,043 main.py:52] epoch 1031, training loss: 7719.76, average training loss: 8659.16, base loss: 15338.69
[INFO 2017-07-01 19:54:55,158 main.py:52] epoch 1032, training loss: 8236.83, average training loss: 8658.26, base loss: 15337.20
[INFO 2017-07-01 19:54:59,271 main.py:52] epoch 1033, training loss: 7613.04, average training loss: 8655.73, base loss: 15335.49
[INFO 2017-07-01 19:55:03,461 main.py:52] epoch 1034, training loss: 8442.97, average training loss: 8654.25, base loss: 15334.76
[INFO 2017-07-01 19:55:07,707 main.py:52] epoch 1035, training loss: 8760.24, average training loss: 8651.80, base loss: 15336.05
[INFO 2017-07-01 19:55:11,895 main.py:52] epoch 1036, training loss: 8517.34, average training loss: 8651.43, base loss: 15337.26
[INFO 2017-07-01 19:55:16,070 main.py:52] epoch 1037, training loss: 7843.63, average training loss: 8648.88, base loss: 15336.77
[INFO 2017-07-01 19:55:20,187 main.py:52] epoch 1038, training loss: 8795.46, average training loss: 8647.06, base loss: 15338.16
[INFO 2017-07-01 19:55:24,334 main.py:52] epoch 1039, training loss: 9006.51, average training loss: 8646.71, base loss: 15339.35
[INFO 2017-07-01 19:55:28,524 main.py:52] epoch 1040, training loss: 7957.26, average training loss: 8642.88, base loss: 15338.39
[INFO 2017-07-01 19:55:32,689 main.py:52] epoch 1041, training loss: 8447.96, average training loss: 8640.96, base loss: 15339.71
[INFO 2017-07-01 19:55:36,849 main.py:52] epoch 1042, training loss: 9002.03, average training loss: 8640.33, base loss: 15342.52
[INFO 2017-07-01 19:55:41,000 main.py:52] epoch 1043, training loss: 8281.21, average training loss: 8637.22, base loss: 15344.44
[INFO 2017-07-01 19:55:45,160 main.py:52] epoch 1044, training loss: 7472.63, average training loss: 8634.67, base loss: 15344.08
[INFO 2017-07-01 19:55:49,313 main.py:52] epoch 1045, training loss: 8259.64, average training loss: 8633.29, base loss: 15345.18
[INFO 2017-07-01 19:55:53,508 main.py:52] epoch 1046, training loss: 7858.97, average training loss: 8631.90, base loss: 15343.92
[INFO 2017-07-01 19:55:57,712 main.py:52] epoch 1047, training loss: 7418.39, average training loss: 8630.75, base loss: 15341.03
[INFO 2017-07-01 19:56:01,838 main.py:52] epoch 1048, training loss: 8247.96, average training loss: 8627.10, base loss: 15340.05
[INFO 2017-07-01 19:56:05,996 main.py:52] epoch 1049, training loss: 8251.09, average training loss: 8623.42, base loss: 15339.73
[INFO 2017-07-01 19:56:10,193 main.py:52] epoch 1050, training loss: 8897.49, average training loss: 8623.05, base loss: 15341.01
[INFO 2017-07-01 19:56:14,405 main.py:52] epoch 1051, training loss: 7875.76, average training loss: 8619.85, base loss: 15340.39
[INFO 2017-07-01 19:56:18,610 main.py:52] epoch 1052, training loss: 8251.65, average training loss: 8616.61, base loss: 15340.96
[INFO 2017-07-01 19:56:22,824 main.py:52] epoch 1053, training loss: 8319.58, average training loss: 8614.93, base loss: 15341.44
[INFO 2017-07-01 19:56:26,972 main.py:52] epoch 1054, training loss: 9421.23, average training loss: 8614.55, base loss: 15344.20
[INFO 2017-07-01 19:56:31,073 main.py:52] epoch 1055, training loss: 8150.36, average training loss: 8611.87, base loss: 15343.42
[INFO 2017-07-01 19:56:35,262 main.py:52] epoch 1056, training loss: 7623.84, average training loss: 8609.00, base loss: 15342.43
[INFO 2017-07-01 19:56:39,459 main.py:52] epoch 1057, training loss: 8248.85, average training loss: 8607.30, base loss: 15341.71
[INFO 2017-07-01 19:56:43,644 main.py:52] epoch 1058, training loss: 8316.97, average training loss: 8606.13, base loss: 15341.33
[INFO 2017-07-01 19:56:47,843 main.py:52] epoch 1059, training loss: 8842.44, average training loss: 8604.66, base loss: 15343.27
[INFO 2017-07-01 19:56:52,008 main.py:52] epoch 1060, training loss: 7881.63, average training loss: 8603.66, base loss: 15342.78
[INFO 2017-07-01 19:56:56,123 main.py:52] epoch 1061, training loss: 7191.45, average training loss: 8600.83, base loss: 15339.22
[INFO 2017-07-01 19:57:00,234 main.py:52] epoch 1062, training loss: 9366.62, average training loss: 8600.48, base loss: 15342.67
[INFO 2017-07-01 19:57:04,402 main.py:52] epoch 1063, training loss: 8599.34, average training loss: 8599.51, base loss: 15346.14
[INFO 2017-07-01 19:57:08,578 main.py:52] epoch 1064, training loss: 7784.31, average training loss: 8597.06, base loss: 15344.23
[INFO 2017-07-01 19:57:12,721 main.py:52] epoch 1065, training loss: 8916.88, average training loss: 8596.46, base loss: 15345.94
[INFO 2017-07-01 19:57:16,965 main.py:52] epoch 1066, training loss: 8097.48, average training loss: 8594.87, base loss: 15347.89
[INFO 2017-07-01 19:57:21,164 main.py:52] epoch 1067, training loss: 8663.53, average training loss: 8593.10, base loss: 15350.67
[INFO 2017-07-01 19:57:25,302 main.py:52] epoch 1068, training loss: 8364.82, average training loss: 8592.21, base loss: 15351.05
[INFO 2017-07-01 19:57:29,505 main.py:52] epoch 1069, training loss: 8436.51, average training loss: 8591.35, base loss: 15351.21
[INFO 2017-07-01 19:57:33,639 main.py:52] epoch 1070, training loss: 8822.16, average training loss: 8590.32, base loss: 15352.91
[INFO 2017-07-01 19:57:37,798 main.py:52] epoch 1071, training loss: 8742.71, average training loss: 8590.12, base loss: 15354.59
[INFO 2017-07-01 19:57:41,970 main.py:52] epoch 1072, training loss: 8394.71, average training loss: 8587.85, base loss: 15355.31
[INFO 2017-07-01 19:57:46,196 main.py:52] epoch 1073, training loss: 7854.16, average training loss: 8586.24, base loss: 15357.15
[INFO 2017-07-01 19:57:50,405 main.py:52] epoch 1074, training loss: 7532.22, average training loss: 8584.61, base loss: 15354.36
[INFO 2017-07-01 19:57:54,585 main.py:52] epoch 1075, training loss: 8254.08, average training loss: 8582.71, base loss: 15353.68
[INFO 2017-07-01 19:57:58,757 main.py:52] epoch 1076, training loss: 7915.80, average training loss: 8581.13, base loss: 15353.28
[INFO 2017-07-01 19:58:02,922 main.py:52] epoch 1077, training loss: 8147.34, average training loss: 8580.97, base loss: 15354.55
[INFO 2017-07-01 19:58:07,053 main.py:52] epoch 1078, training loss: 7898.14, average training loss: 8579.58, base loss: 15353.88
[INFO 2017-07-01 19:58:11,198 main.py:52] epoch 1079, training loss: 8151.27, average training loss: 8577.56, base loss: 15354.76
[INFO 2017-07-01 19:58:15,418 main.py:52] epoch 1080, training loss: 7471.00, average training loss: 8575.64, base loss: 15353.20
[INFO 2017-07-01 19:58:19,601 main.py:52] epoch 1081, training loss: 8136.70, average training loss: 8573.72, base loss: 15353.89
[INFO 2017-07-01 19:58:23,775 main.py:52] epoch 1082, training loss: 8303.95, average training loss: 8572.20, base loss: 15355.37
[INFO 2017-07-01 19:58:27,941 main.py:52] epoch 1083, training loss: 8134.47, average training loss: 8570.99, base loss: 15355.36
[INFO 2017-07-01 19:58:32,121 main.py:52] epoch 1084, training loss: 8670.88, average training loss: 8570.16, base loss: 15356.89
[INFO 2017-07-01 19:58:36,329 main.py:52] epoch 1085, training loss: 7996.05, average training loss: 8568.70, base loss: 15355.50
[INFO 2017-07-01 19:58:40,518 main.py:52] epoch 1086, training loss: 7738.94, average training loss: 8566.55, base loss: 15356.16
[INFO 2017-07-01 19:58:44,732 main.py:52] epoch 1087, training loss: 8158.64, average training loss: 8565.11, base loss: 15358.26
[INFO 2017-07-01 19:58:48,859 main.py:52] epoch 1088, training loss: 7195.46, average training loss: 8562.57, base loss: 15355.44
[INFO 2017-07-01 19:58:52,983 main.py:52] epoch 1089, training loss: 7870.46, average training loss: 8560.75, base loss: 15353.55
[INFO 2017-07-01 19:58:57,220 main.py:52] epoch 1090, training loss: 7702.92, average training loss: 8558.10, base loss: 15352.60
[INFO 2017-07-01 19:59:01,413 main.py:52] epoch 1091, training loss: 8692.22, average training loss: 8557.27, base loss: 15353.50
[INFO 2017-07-01 19:59:05,644 main.py:52] epoch 1092, training loss: 8191.93, average training loss: 8555.73, base loss: 15353.02
[INFO 2017-07-01 19:59:09,800 main.py:52] epoch 1093, training loss: 7921.09, average training loss: 8554.13, base loss: 15351.19
[INFO 2017-07-01 19:59:13,986 main.py:52] epoch 1094, training loss: 9032.18, average training loss: 8554.49, base loss: 15352.34
[INFO 2017-07-01 19:59:18,156 main.py:52] epoch 1095, training loss: 8469.85, average training loss: 8553.79, base loss: 15351.79
[INFO 2017-07-01 19:59:22,412 main.py:52] epoch 1096, training loss: 7518.18, average training loss: 8550.95, base loss: 15351.63
[INFO 2017-07-01 19:59:26,575 main.py:52] epoch 1097, training loss: 9176.58, average training loss: 8550.49, base loss: 15354.15
[INFO 2017-07-01 19:59:30,716 main.py:52] epoch 1098, training loss: 8165.98, average training loss: 8549.64, base loss: 15354.86
[INFO 2017-07-01 19:59:34,867 main.py:52] epoch 1099, training loss: 8120.56, average training loss: 8548.44, base loss: 15354.25
[INFO 2017-07-01 19:59:34,868 main.py:54] epoch 1099, testing
[INFO 2017-07-01 19:59:34,868 main.py:61] model save to ./model/final.pth
[INFO 2017-07-01 19:59:39,159 main.py:52] epoch 1100, training loss: 7413.79, average training loss: 8547.55, base loss: 15352.15
[INFO 2017-07-01 19:59:43,279 main.py:52] epoch 1101, training loss: 7364.72, average training loss: 8544.73, base loss: 15351.09
[INFO 2017-07-01 19:59:47,501 main.py:52] epoch 1102, training loss: 7918.73, average training loss: 8541.28, base loss: 15352.58
[INFO 2017-07-01 19:59:51,682 main.py:52] epoch 1103, training loss: 8383.19, average training loss: 8540.90, base loss: 15353.48
[INFO 2017-07-01 19:59:55,872 main.py:52] epoch 1104, training loss: 8677.58, average training loss: 8539.72, base loss: 15354.29
[INFO 2017-07-01 20:00:00,093 main.py:52] epoch 1105, training loss: 7864.27, average training loss: 8538.61, base loss: 15354.00
[INFO 2017-07-01 20:00:04,211 main.py:52] epoch 1106, training loss: 7836.58, average training loss: 8537.19, base loss: 15352.33
[INFO 2017-07-01 20:00:08,452 main.py:52] epoch 1107, training loss: 7474.67, average training loss: 8535.90, base loss: 15350.73
[INFO 2017-07-01 20:00:12,602 main.py:52] epoch 1108, training loss: 7906.67, average training loss: 8535.09, base loss: 15350.92
[INFO 2017-07-01 20:00:16,776 main.py:52] epoch 1109, training loss: 7822.27, average training loss: 8533.13, base loss: 15351.04
[INFO 2017-07-01 20:00:20,952 main.py:52] epoch 1110, training loss: 7200.76, average training loss: 8530.74, base loss: 15349.02
[INFO 2017-07-01 20:00:25,148 main.py:52] epoch 1111, training loss: 8430.94, average training loss: 8528.51, base loss: 15348.81
[INFO 2017-07-01 20:00:29,299 main.py:52] epoch 1112, training loss: 7630.71, average training loss: 8526.88, base loss: 15348.95
[INFO 2017-07-01 20:00:33,488 main.py:52] epoch 1113, training loss: 8015.10, average training loss: 8525.52, base loss: 15349.62
[INFO 2017-07-01 20:00:37,642 main.py:52] epoch 1114, training loss: 9041.01, average training loss: 8525.52, base loss: 15349.62
[INFO 2017-07-01 20:00:41,763 main.py:52] epoch 1115, training loss: 8286.22, average training loss: 8524.57, base loss: 15347.79
[INFO 2017-07-01 20:00:45,892 main.py:52] epoch 1116, training loss: 8162.43, average training loss: 8523.71, base loss: 15348.82
[INFO 2017-07-01 20:00:50,106 main.py:52] epoch 1117, training loss: 8060.98, average training loss: 8523.71, base loss: 15349.43
[INFO 2017-07-01 20:00:54,247 main.py:52] epoch 1118, training loss: 8242.59, average training loss: 8523.24, base loss: 15351.42
[INFO 2017-07-01 20:00:58,465 main.py:52] epoch 1119, training loss: 7914.36, average training loss: 8521.37, base loss: 15350.89
[INFO 2017-07-01 20:01:02,593 main.py:52] epoch 1120, training loss: 8387.58, average training loss: 8521.74, base loss: 15350.95
[INFO 2017-07-01 20:01:06,723 main.py:52] epoch 1121, training loss: 9280.45, average training loss: 8522.13, base loss: 15353.82
[INFO 2017-07-01 20:01:10,865 main.py:52] epoch 1122, training loss: 8545.41, average training loss: 8521.83, base loss: 15355.14
[INFO 2017-07-01 20:01:15,052 main.py:52] epoch 1123, training loss: 8069.00, average training loss: 8519.65, base loss: 15354.22
[INFO 2017-07-01 20:01:19,160 main.py:52] epoch 1124, training loss: 7664.27, average training loss: 8517.33, base loss: 15352.06
[INFO 2017-07-01 20:01:23,424 main.py:52] epoch 1125, training loss: 8042.15, average training loss: 8515.36, base loss: 15352.83
[INFO 2017-07-01 20:01:27,606 main.py:52] epoch 1126, training loss: 8831.65, average training loss: 8514.93, base loss: 15352.79
[INFO 2017-07-01 20:01:31,783 main.py:52] epoch 1127, training loss: 7729.27, average training loss: 8514.28, base loss: 15351.96
[INFO 2017-07-01 20:01:35,981 main.py:52] epoch 1128, training loss: 7929.84, average training loss: 8512.74, base loss: 15352.15
[INFO 2017-07-01 20:01:40,135 main.py:52] epoch 1129, training loss: 9297.52, average training loss: 8512.86, base loss: 15354.97
[INFO 2017-07-01 20:01:44,264 main.py:52] epoch 1130, training loss: 8509.26, average training loss: 8512.64, base loss: 15358.31
[INFO 2017-07-01 20:01:48,403 main.py:52] epoch 1131, training loss: 7369.54, average training loss: 8511.56, base loss: 15356.49
[INFO 2017-07-01 20:01:52,596 main.py:52] epoch 1132, training loss: 8657.22, average training loss: 8510.93, base loss: 15356.52
[INFO 2017-07-01 20:01:56,789 main.py:52] epoch 1133, training loss: 7905.78, average training loss: 8509.80, base loss: 15355.02
[INFO 2017-07-01 20:02:00,899 main.py:52] epoch 1134, training loss: 7858.63, average training loss: 8509.08, base loss: 15353.59
[INFO 2017-07-01 20:02:05,069 main.py:52] epoch 1135, training loss: 8421.53, average training loss: 8509.02, base loss: 15353.47
[INFO 2017-07-01 20:02:09,240 main.py:52] epoch 1136, training loss: 9185.10, average training loss: 8507.04, base loss: 15356.78
[INFO 2017-07-01 20:02:13,391 main.py:52] epoch 1137, training loss: 9404.02, average training loss: 8506.14, base loss: 15359.78
[INFO 2017-07-01 20:02:17,623 main.py:52] epoch 1138, training loss: 8932.08, average training loss: 8504.40, base loss: 15360.86
[INFO 2017-07-01 20:02:21,771 main.py:52] epoch 1139, training loss: 8976.96, average training loss: 8504.31, base loss: 15362.56
[INFO 2017-07-01 20:02:26,009 main.py:52] epoch 1140, training loss: 8086.64, average training loss: 8503.97, base loss: 15362.48
[INFO 2017-07-01 20:02:30,181 main.py:52] epoch 1141, training loss: 8645.13, average training loss: 8503.48, base loss: 15363.44
[INFO 2017-07-01 20:02:34,356 main.py:52] epoch 1142, training loss: 8562.67, average training loss: 8502.62, base loss: 15363.65
[INFO 2017-07-01 20:02:38,620 main.py:52] epoch 1143, training loss: 8827.80, average training loss: 8502.21, base loss: 15364.78
[INFO 2017-07-01 20:02:42,830 main.py:52] epoch 1144, training loss: 7687.50, average training loss: 8501.37, base loss: 15366.19
[INFO 2017-07-01 20:02:47,005 main.py:52] epoch 1145, training loss: 7300.31, average training loss: 8499.38, base loss: 15365.35
[INFO 2017-07-01 20:02:51,169 main.py:52] epoch 1146, training loss: 8039.55, average training loss: 8496.76, base loss: 15365.60
[INFO 2017-07-01 20:02:55,295 main.py:52] epoch 1147, training loss: 7864.08, average training loss: 8494.61, base loss: 15365.24
[INFO 2017-07-01 20:02:59,428 main.py:52] epoch 1148, training loss: 8750.39, average training loss: 8494.52, base loss: 15366.51
[INFO 2017-07-01 20:03:03,561 main.py:52] epoch 1149, training loss: 8301.49, average training loss: 8493.84, base loss: 15367.09
[INFO 2017-07-01 20:03:07,726 main.py:52] epoch 1150, training loss: 8292.19, average training loss: 8492.50, base loss: 15368.61
[INFO 2017-07-01 20:03:11,881 main.py:52] epoch 1151, training loss: 8048.91, average training loss: 8491.35, base loss: 15371.00
[INFO 2017-07-01 20:03:16,028 main.py:52] epoch 1152, training loss: 8553.36, average training loss: 8490.83, base loss: 15371.18
[INFO 2017-07-01 20:03:20,215 main.py:52] epoch 1153, training loss: 8708.38, average training loss: 8490.78, base loss: 15372.46
[INFO 2017-07-01 20:03:24,388 main.py:52] epoch 1154, training loss: 8195.78, average training loss: 8489.88, base loss: 15373.68
[INFO 2017-07-01 20:03:28,661 main.py:52] epoch 1155, training loss: 7585.15, average training loss: 8487.09, base loss: 15373.36
[INFO 2017-07-01 20:03:32,798 main.py:52] epoch 1156, training loss: 7566.60, average training loss: 8484.04, base loss: 15371.78
[INFO 2017-07-01 20:03:36,967 main.py:52] epoch 1157, training loss: 7743.39, average training loss: 8482.76, base loss: 15369.15
[INFO 2017-07-01 20:03:41,120 main.py:52] epoch 1158, training loss: 8040.35, average training loss: 8480.79, base loss: 15366.90
[INFO 2017-07-01 20:03:45,299 main.py:52] epoch 1159, training loss: 7951.15, average training loss: 8479.58, base loss: 15366.31
[INFO 2017-07-01 20:03:49,467 main.py:52] epoch 1160, training loss: 8043.98, average training loss: 8478.28, base loss: 15366.18
[INFO 2017-07-01 20:03:53,634 main.py:52] epoch 1161, training loss: 8622.81, average training loss: 8477.33, base loss: 15367.75
[INFO 2017-07-01 20:03:57,836 main.py:52] epoch 1162, training loss: 7369.68, average training loss: 8475.74, base loss: 15365.07
[INFO 2017-07-01 20:04:02,052 main.py:52] epoch 1163, training loss: 8188.94, average training loss: 8474.11, base loss: 15365.15
[INFO 2017-07-01 20:04:06,132 main.py:52] epoch 1164, training loss: 7794.22, average training loss: 8472.76, base loss: 15362.90
[INFO 2017-07-01 20:04:10,337 main.py:52] epoch 1165, training loss: 7648.42, average training loss: 8470.24, base loss: 15361.38
[INFO 2017-07-01 20:04:14,526 main.py:52] epoch 1166, training loss: 8439.68, average training loss: 8468.37, base loss: 15362.61
[INFO 2017-07-01 20:04:18,684 main.py:52] epoch 1167, training loss: 8207.95, average training loss: 8467.07, base loss: 15364.50
[INFO 2017-07-01 20:04:22,874 main.py:52] epoch 1168, training loss: 7564.81, average training loss: 8465.46, base loss: 15364.14
[INFO 2017-07-01 20:04:27,031 main.py:52] epoch 1169, training loss: 8634.49, average training loss: 8465.71, base loss: 15365.00
[INFO 2017-07-01 20:04:31,164 main.py:52] epoch 1170, training loss: 8206.15, average training loss: 8464.30, base loss: 15364.30
[INFO 2017-07-01 20:04:35,354 main.py:52] epoch 1171, training loss: 7319.44, average training loss: 8462.37, base loss: 15361.11
[INFO 2017-07-01 20:04:39,448 main.py:52] epoch 1172, training loss: 7756.21, average training loss: 8460.45, base loss: 15358.88
[INFO 2017-07-01 20:04:43,601 main.py:52] epoch 1173, training loss: 7393.41, average training loss: 8458.33, base loss: 15357.24
[INFO 2017-07-01 20:04:47,711 main.py:52] epoch 1174, training loss: 7948.10, average training loss: 8456.42, base loss: 15356.19
[INFO 2017-07-01 20:04:51,826 main.py:52] epoch 1175, training loss: 8369.97, average training loss: 8455.81, base loss: 15356.97
[INFO 2017-07-01 20:04:56,037 main.py:52] epoch 1176, training loss: 7955.62, average training loss: 8455.40, base loss: 15356.13
[INFO 2017-07-01 20:05:00,106 main.py:52] epoch 1177, training loss: 7921.66, average training loss: 8453.95, base loss: 15355.58
[INFO 2017-07-01 20:05:04,317 main.py:52] epoch 1178, training loss: 7943.25, average training loss: 8452.55, base loss: 15355.63
[INFO 2017-07-01 20:05:08,499 main.py:52] epoch 1179, training loss: 7533.58, average training loss: 8451.56, base loss: 15355.08
[INFO 2017-07-01 20:05:12,647 main.py:52] epoch 1180, training loss: 8502.31, average training loss: 8451.85, base loss: 15356.30
[INFO 2017-07-01 20:05:16,789 main.py:52] epoch 1181, training loss: 8467.58, average training loss: 8451.34, base loss: 15357.06
[INFO 2017-07-01 20:05:20,969 main.py:52] epoch 1182, training loss: 7756.46, average training loss: 8450.37, base loss: 15355.33
[INFO 2017-07-01 20:05:25,111 main.py:52] epoch 1183, training loss: 8041.61, average training loss: 8447.96, base loss: 15354.98
[INFO 2017-07-01 20:05:29,283 main.py:52] epoch 1184, training loss: 7830.86, average training loss: 8447.86, base loss: 15354.51
[INFO 2017-07-01 20:05:33,437 main.py:52] epoch 1185, training loss: 8124.27, average training loss: 8447.39, base loss: 15355.18
[INFO 2017-07-01 20:05:37,643 main.py:52] epoch 1186, training loss: 8067.08, average training loss: 8446.66, base loss: 15355.68
[INFO 2017-07-01 20:05:41,751 main.py:52] epoch 1187, training loss: 8130.39, average training loss: 8445.52, base loss: 15355.49
[INFO 2017-07-01 20:05:45,871 main.py:52] epoch 1188, training loss: 9304.44, average training loss: 8445.06, base loss: 15358.19
[INFO 2017-07-01 20:05:50,028 main.py:52] epoch 1189, training loss: 8032.32, average training loss: 8443.91, base loss: 15356.07
[INFO 2017-07-01 20:05:54,151 main.py:52] epoch 1190, training loss: 8433.80, average training loss: 8443.90, base loss: 15356.16
[INFO 2017-07-01 20:05:58,393 main.py:52] epoch 1191, training loss: 7564.85, average training loss: 8442.14, base loss: 15354.54
[INFO 2017-07-01 20:06:02,536 main.py:52] epoch 1192, training loss: 7788.56, average training loss: 8442.60, base loss: 15353.46
[INFO 2017-07-01 20:06:06,692 main.py:52] epoch 1193, training loss: 7449.20, average training loss: 8441.58, base loss: 15351.71
[INFO 2017-07-01 20:06:10,879 main.py:52] epoch 1194, training loss: 7778.74, average training loss: 8441.19, base loss: 15352.36
[INFO 2017-07-01 20:06:14,979 main.py:52] epoch 1195, training loss: 8300.31, average training loss: 8441.19, base loss: 15352.62
[INFO 2017-07-01 20:06:19,190 main.py:52] epoch 1196, training loss: 8809.46, average training loss: 8439.88, base loss: 15353.77
[INFO 2017-07-01 20:06:23,324 main.py:52] epoch 1197, training loss: 7465.60, average training loss: 8437.86, base loss: 15352.63
[INFO 2017-07-01 20:06:27,471 main.py:52] epoch 1198, training loss: 9394.31, average training loss: 8438.77, base loss: 15357.29
[INFO 2017-07-01 20:06:31,645 main.py:52] epoch 1199, training loss: 8336.17, average training loss: 8438.43, base loss: 15357.64
[INFO 2017-07-01 20:06:31,646 main.py:54] epoch 1199, testing
[INFO 2017-07-01 20:06:31,646 main.py:61] model save to ./model/final.pth
[INFO 2017-07-01 20:06:35,904 main.py:52] epoch 1200, training loss: 7446.62, average training loss: 8437.20, base loss: 15354.43
[INFO 2017-07-01 20:06:40,096 main.py:52] epoch 1201, training loss: 9485.10, average training loss: 8437.55, base loss: 15355.48
[INFO 2017-07-01 20:06:44,305 main.py:52] epoch 1202, training loss: 7370.99, average training loss: 8436.31, base loss: 15355.07
[INFO 2017-07-01 20:06:48,544 main.py:52] epoch 1203, training loss: 7840.14, average training loss: 8435.16, base loss: 15355.06
[INFO 2017-07-01 20:06:52,797 main.py:52] epoch 1204, training loss: 7280.52, average training loss: 8433.68, base loss: 15353.99
[INFO 2017-07-01 20:06:56,937 main.py:52] epoch 1205, training loss: 8313.94, average training loss: 8433.49, base loss: 15354.58
[INFO 2017-07-01 20:07:01,032 main.py:52] epoch 1206, training loss: 7716.57, average training loss: 8432.56, base loss: 15353.81
[INFO 2017-07-01 20:07:05,242 main.py:52] epoch 1207, training loss: 8438.57, average training loss: 8432.68, base loss: 15354.77
[INFO 2017-07-01 20:07:09,473 main.py:52] epoch 1208, training loss: 8440.25, average training loss: 8431.30, base loss: 15355.54
[INFO 2017-07-01 20:07:13,619 main.py:52] epoch 1209, training loss: 9186.56, average training loss: 8431.30, base loss: 15357.63
[INFO 2017-07-01 20:07:17,832 main.py:52] epoch 1210, training loss: 7280.42, average training loss: 8429.21, base loss: 15355.62
[INFO 2017-07-01 20:07:22,170 main.py:52] epoch 1211, training loss: 7788.96, average training loss: 8427.17, base loss: 15355.87
[INFO 2017-07-01 20:07:26,290 main.py:52] epoch 1212, training loss: 8305.17, average training loss: 8426.72, base loss: 15355.04
[INFO 2017-07-01 20:07:30,428 main.py:52] epoch 1213, training loss: 7616.48, average training loss: 8425.47, base loss: 15353.74
[INFO 2017-07-01 20:07:34,609 main.py:52] epoch 1214, training loss: 7736.11, average training loss: 8423.19, base loss: 15353.59
[INFO 2017-07-01 20:07:38,772 main.py:52] epoch 1215, training loss: 8170.47, average training loss: 8422.66, base loss: 15354.11
[INFO 2017-07-01 20:07:42,933 main.py:52] epoch 1216, training loss: 8329.76, average training loss: 8422.30, base loss: 15354.24
[INFO 2017-07-01 20:07:47,057 main.py:52] epoch 1217, training loss: 7477.85, average training loss: 8419.46, base loss: 15352.18
[INFO 2017-07-01 20:07:51,193 main.py:52] epoch 1218, training loss: 7828.98, average training loss: 8419.07, base loss: 15351.73
[INFO 2017-07-01 20:07:55,340 main.py:52] epoch 1219, training loss: 8110.97, average training loss: 8418.33, base loss: 15352.22
[INFO 2017-07-01 20:07:59,529 main.py:52] epoch 1220, training loss: 8273.82, average training loss: 8418.53, base loss: 15352.35
[INFO 2017-07-01 20:08:03,624 main.py:52] epoch 1221, training loss: 7468.35, average training loss: 8417.51, base loss: 15351.57
[INFO 2017-07-01 20:08:07,842 main.py:52] epoch 1222, training loss: 8061.11, average training loss: 8416.29, base loss: 15350.23
[INFO 2017-07-01 20:08:11,997 main.py:52] epoch 1223, training loss: 7758.56, average training loss: 8414.78, base loss: 15348.72
[INFO 2017-07-01 20:08:16,153 main.py:52] epoch 1224, training loss: 8258.61, average training loss: 8414.55, base loss: 15349.44
[INFO 2017-07-01 20:08:20,409 main.py:52] epoch 1225, training loss: 8465.04, average training loss: 8413.49, base loss: 15350.76
[INFO 2017-07-01 20:08:24,574 main.py:52] epoch 1226, training loss: 8268.60, average training loss: 8413.41, base loss: 15351.62
[INFO 2017-07-01 20:08:28,857 main.py:52] epoch 1227, training loss: 8466.05, average training loss: 8412.83, base loss: 15353.39
[INFO 2017-07-01 20:08:33,067 main.py:52] epoch 1228, training loss: 8633.22, average training loss: 8413.75, base loss: 15354.09
[INFO 2017-07-01 20:08:37,206 main.py:52] epoch 1229, training loss: 8269.06, average training loss: 8412.77, base loss: 15353.69
[INFO 2017-07-01 20:08:41,313 main.py:52] epoch 1230, training loss: 7688.48, average training loss: 8411.66, base loss: 15352.51
[INFO 2017-07-01 20:08:45,488 main.py:52] epoch 1231, training loss: 8692.86, average training loss: 8410.59, base loss: 15353.29
[INFO 2017-07-01 20:08:49,639 main.py:52] epoch 1232, training loss: 6920.47, average training loss: 8408.42, base loss: 15351.00
[INFO 2017-07-01 20:08:53,828 main.py:52] epoch 1233, training loss: 7692.00, average training loss: 8406.53, base loss: 15350.10
[INFO 2017-07-01 20:08:57,964 main.py:52] epoch 1234, training loss: 8436.91, average training loss: 8405.94, base loss: 15350.90
[INFO 2017-07-01 20:09:02,130 main.py:52] epoch 1235, training loss: 6854.03, average training loss: 8403.78, base loss: 15348.00
[INFO 2017-07-01 20:09:06,316 main.py:52] epoch 1236, training loss: 7464.14, average training loss: 8402.33, base loss: 15346.43
[INFO 2017-07-01 20:09:10,535 main.py:52] epoch 1237, training loss: 8509.10, average training loss: 8400.58, base loss: 15347.40
[INFO 2017-07-01 20:09:14,745 main.py:52] epoch 1238, training loss: 9055.19, average training loss: 8401.47, base loss: 15349.29
[INFO 2017-07-01 20:09:18,865 main.py:52] epoch 1239, training loss: 7840.94, average training loss: 8400.74, base loss: 15348.59
[INFO 2017-07-01 20:09:23,038 main.py:52] epoch 1240, training loss: 8102.85, average training loss: 8400.02, base loss: 15348.76
[INFO 2017-07-01 20:09:27,241 main.py:52] epoch 1241, training loss: 8337.02, average training loss: 8399.80, base loss: 15350.72
[INFO 2017-07-01 20:09:31,372 main.py:52] epoch 1242, training loss: 8730.58, average training loss: 8398.96, base loss: 15352.85
[INFO 2017-07-01 20:09:35,532 main.py:52] epoch 1243, training loss: 8559.89, average training loss: 8399.45, base loss: 15354.61
[INFO 2017-07-01 20:09:39,678 main.py:52] epoch 1244, training loss: 8067.16, average training loss: 8399.53, base loss: 15354.79
[INFO 2017-07-01 20:09:43,898 main.py:52] epoch 1245, training loss: 8231.03, average training loss: 8400.07, base loss: 15354.97
[INFO 2017-07-01 20:09:48,073 main.py:52] epoch 1246, training loss: 7067.49, average training loss: 8396.96, base loss: 15353.31
[INFO 2017-07-01 20:09:52,300 main.py:52] epoch 1247, training loss: 7357.55, average training loss: 8396.25, base loss: 15351.73
[INFO 2017-07-01 20:09:56,420 main.py:52] epoch 1248, training loss: 8071.83, average training loss: 8394.57, base loss: 15350.37
[INFO 2017-07-01 20:10:00,575 main.py:52] epoch 1249, training loss: 8340.48, average training loss: 8393.39, base loss: 15350.58
[INFO 2017-07-01 20:10:04,836 main.py:52] epoch 1250, training loss: 7969.58, average training loss: 8392.65, base loss: 15348.88
[INFO 2017-07-01 20:10:09,025 main.py:52] epoch 1251, training loss: 8291.82, average training loss: 8391.89, base loss: 15349.34
[INFO 2017-07-01 20:10:13,206 main.py:52] epoch 1252, training loss: 7636.24, average training loss: 8389.65, base loss: 15349.97
[INFO 2017-07-01 20:10:17,289 main.py:52] epoch 1253, training loss: 7737.64, average training loss: 8388.74, base loss: 15350.21
[INFO 2017-07-01 20:10:21,389 main.py:52] epoch 1254, training loss: 8031.76, average training loss: 8387.05, base loss: 15351.02
[INFO 2017-07-01 20:10:25,602 main.py:52] epoch 1255, training loss: 8847.00, average training loss: 8386.70, base loss: 15353.99
[INFO 2017-07-01 20:10:29,733 main.py:52] epoch 1256, training loss: 8962.70, average training loss: 8387.24, base loss: 15356.21
[INFO 2017-07-01 20:10:33,922 main.py:52] epoch 1257, training loss: 8168.26, average training loss: 8386.20, base loss: 15356.71
[INFO 2017-07-01 20:10:38,109 main.py:52] epoch 1258, training loss: 8690.75, average training loss: 8386.95, base loss: 15356.68
[INFO 2017-07-01 20:10:42,217 main.py:52] epoch 1259, training loss: 7948.69, average training loss: 8384.47, base loss: 15356.39
[INFO 2017-07-01 20:10:46,359 main.py:52] epoch 1260, training loss: 10180.52, average training loss: 8385.21, base loss: 15359.00
[INFO 2017-07-01 20:10:50,521 main.py:52] epoch 1261, training loss: 8235.53, average training loss: 8385.05, base loss: 15357.84
[INFO 2017-07-01 20:10:54,644 main.py:52] epoch 1262, training loss: 7124.90, average training loss: 8382.80, base loss: 15354.39
[INFO 2017-07-01 20:10:58,850 main.py:52] epoch 1263, training loss: 8461.67, average training loss: 8381.70, base loss: 15354.31
[INFO 2017-07-01 20:11:03,145 main.py:52] epoch 1264, training loss: 7720.36, average training loss: 8381.35, base loss: 15354.18
[INFO 2017-07-01 20:11:07,345 main.py:52] epoch 1265, training loss: 8639.97, average training loss: 8381.84, base loss: 15355.47
[INFO 2017-07-01 20:11:11,591 main.py:52] epoch 1266, training loss: 8283.67, average training loss: 8380.66, base loss: 15354.64
[INFO 2017-07-01 20:11:15,775 main.py:52] epoch 1267, training loss: 8242.66, average training loss: 8379.83, base loss: 15353.67
[INFO 2017-07-01 20:11:19,961 main.py:52] epoch 1268, training loss: 9240.78, average training loss: 8380.55, base loss: 15354.69
[INFO 2017-07-01 20:11:24,065 main.py:52] epoch 1269, training loss: 8449.22, average training loss: 8380.64, base loss: 15355.61
[INFO 2017-07-01 20:11:28,194 main.py:52] epoch 1270, training loss: 8366.91, average training loss: 8379.58, base loss: 15355.37
[INFO 2017-07-01 20:11:32,403 main.py:52] epoch 1271, training loss: 8372.76, average training loss: 8380.13, base loss: 15354.95
[INFO 2017-07-01 20:11:36,595 main.py:52] epoch 1272, training loss: 7460.81, average training loss: 8378.60, base loss: 15354.24
[INFO 2017-07-01 20:11:40,753 main.py:52] epoch 1273, training loss: 7822.81, average training loss: 8376.82, base loss: 15353.68
[INFO 2017-07-01 20:11:44,882 main.py:52] epoch 1274, training loss: 8659.28, average training loss: 8375.48, base loss: 15354.98
[INFO 2017-07-01 20:11:48,999 main.py:52] epoch 1275, training loss: 7898.15, average training loss: 8374.30, base loss: 15355.02
[INFO 2017-07-01 20:11:53,120 main.py:52] epoch 1276, training loss: 7938.70, average training loss: 8373.38, base loss: 15355.15
[INFO 2017-07-01 20:11:57,286 main.py:52] epoch 1277, training loss: 8486.41, average training loss: 8372.66, base loss: 15354.39
[INFO 2017-07-01 20:12:01,414 main.py:52] epoch 1278, training loss: 8083.22, average training loss: 8371.69, base loss: 15355.36
[INFO 2017-07-01 20:12:05,613 main.py:52] epoch 1279, training loss: 9236.04, average training loss: 8372.59, base loss: 15357.81
[INFO 2017-07-01 20:12:09,811 main.py:52] epoch 1280, training loss: 8566.40, average training loss: 8372.03, base loss: 15358.94
[INFO 2017-07-01 20:12:13,985 main.py:52] epoch 1281, training loss: 7336.70, average training loss: 8371.49, base loss: 15357.33
[INFO 2017-07-01 20:12:18,174 main.py:52] epoch 1282, training loss: 7720.27, average training loss: 8369.91, base loss: 15354.72
[INFO 2017-07-01 20:12:22,347 main.py:52] epoch 1283, training loss: 8343.20, average training loss: 8370.45, base loss: 15355.07
[INFO 2017-07-01 20:12:26,579 main.py:52] epoch 1284, training loss: 7969.78, average training loss: 8369.40, base loss: 15353.64
[INFO 2017-07-01 20:12:30,738 main.py:52] epoch 1285, training loss: 8993.78, average training loss: 8369.51, base loss: 15353.50
[INFO 2017-07-01 20:12:34,915 main.py:52] epoch 1286, training loss: 8627.21, average training loss: 8370.07, base loss: 15354.72
[INFO 2017-07-01 20:12:39,069 main.py:52] epoch 1287, training loss: 9354.80, average training loss: 8369.80, base loss: 15357.42
[INFO 2017-07-01 20:12:43,295 main.py:52] epoch 1288, training loss: 9464.48, average training loss: 8371.60, base loss: 15357.89
[INFO 2017-07-01 20:12:47,476 main.py:52] epoch 1289, training loss: 8072.91, average training loss: 8371.35, base loss: 15355.63
[INFO 2017-07-01 20:12:51,646 main.py:52] epoch 1290, training loss: 8382.23, average training loss: 8370.59, base loss: 15355.38
[INFO 2017-07-01 20:12:55,867 main.py:52] epoch 1291, training loss: 8676.90, average training loss: 8370.84, base loss: 15355.60
[INFO 2017-07-01 20:12:59,988 main.py:52] epoch 1292, training loss: 9095.03, average training loss: 8370.69, base loss: 15357.02
[INFO 2017-07-01 20:13:04,160 main.py:52] epoch 1293, training loss: 8079.38, average training loss: 8370.24, base loss: 15355.61
[INFO 2017-07-01 20:13:08,366 main.py:52] epoch 1294, training loss: 8242.42, average training loss: 8369.22, base loss: 15356.16
[INFO 2017-07-01 20:13:12,554 main.py:52] epoch 1295, training loss: 8799.97, average training loss: 8368.90, base loss: 15359.42
[INFO 2017-07-01 20:13:16,744 main.py:52] epoch 1296, training loss: 8691.49, average training loss: 8368.70, base loss: 15360.14
[INFO 2017-07-01 20:13:20,917 main.py:52] epoch 1297, training loss: 7573.77, average training loss: 8367.17, base loss: 15358.19
[INFO 2017-07-01 20:13:25,081 main.py:52] epoch 1298, training loss: 9015.15, average training loss: 8368.57, base loss: 15360.43
[INFO 2017-07-01 20:13:29,298 main.py:52] epoch 1299, training loss: 8908.25, average training loss: 8368.89, base loss: 15361.13
[INFO 2017-07-01 20:13:29,298 main.py:54] epoch 1299, testing
[INFO 2017-07-01 20:13:29,298 main.py:61] model save to ./model/final.pth
[INFO 2017-07-01 20:13:33,523 main.py:52] epoch 1300, training loss: 8686.54, average training loss: 8367.93, base loss: 15361.28
[INFO 2017-07-01 20:13:37,709 main.py:52] epoch 1301, training loss: 7117.17, average training loss: 8366.23, base loss: 15359.60
[INFO 2017-07-01 20:13:41,895 main.py:52] epoch 1302, training loss: 8839.78, average training loss: 8365.95, base loss: 15362.02
[INFO 2017-07-01 20:13:46,061 main.py:52] epoch 1303, training loss: 8217.42, average training loss: 8365.08, base loss: 15362.50
[INFO 2017-07-01 20:13:50,187 main.py:52] epoch 1304, training loss: 8219.87, average training loss: 8364.99, base loss: 15363.95
[INFO 2017-07-01 20:13:54,358 main.py:52] epoch 1305, training loss: 7912.61, average training loss: 8364.04, base loss: 15365.27
[INFO 2017-07-01 20:13:58,536 main.py:52] epoch 1306, training loss: 8593.66, average training loss: 8363.78, base loss: 15366.74
[INFO 2017-07-01 20:14:02,705 main.py:52] epoch 1307, training loss: 8033.37, average training loss: 8363.06, base loss: 15366.62
[INFO 2017-07-01 20:14:06,892 main.py:52] epoch 1308, training loss: 8344.35, average training loss: 8363.05, base loss: 15368.16
[INFO 2017-07-01 20:14:11,027 main.py:52] epoch 1309, training loss: 8249.94, average training loss: 8363.43, base loss: 15369.44
[INFO 2017-07-01 20:14:15,179 main.py:52] epoch 1310, training loss: 7267.28, average training loss: 8362.28, base loss: 15368.06
[INFO 2017-07-01 20:14:19,356 main.py:52] epoch 1311, training loss: 8204.22, average training loss: 8361.63, base loss: 15367.16
[INFO 2017-07-01 20:14:23,560 main.py:52] epoch 1312, training loss: 7734.58, average training loss: 8359.96, base loss: 15367.15
[INFO 2017-07-01 20:14:27,697 main.py:52] epoch 1313, training loss: 9346.71, average training loss: 8360.01, base loss: 15370.62
[INFO 2017-07-01 20:14:31,889 main.py:52] epoch 1314, training loss: 7773.15, average training loss: 8359.20, base loss: 15371.88
[INFO 2017-07-01 20:14:36,087 main.py:52] epoch 1315, training loss: 7931.49, average training loss: 8359.28, base loss: 15373.36
[INFO 2017-07-01 20:14:40,254 main.py:52] epoch 1316, training loss: 9771.63, average training loss: 8359.74, base loss: 15374.90
[INFO 2017-07-01 20:14:44,487 main.py:52] epoch 1317, training loss: 7068.22, average training loss: 8357.97, base loss: 15371.81
[INFO 2017-07-01 20:14:48,603 main.py:52] epoch 1318, training loss: 8696.24, average training loss: 8358.18, base loss: 15372.56
[INFO 2017-07-01 20:14:52,748 main.py:52] epoch 1319, training loss: 8361.36, average training loss: 8357.41, base loss: 15375.05
[INFO 2017-07-01 20:14:56,933 main.py:52] epoch 1320, training loss: 8068.72, average training loss: 8356.88, base loss: 15374.07
[INFO 2017-07-01 20:15:01,128 main.py:52] epoch 1321, training loss: 7708.52, average training loss: 8356.01, base loss: 15372.91
[INFO 2017-07-01 20:15:05,276 main.py:52] epoch 1322, training loss: 8233.04, average training loss: 8356.66, base loss: 15374.59
[INFO 2017-07-01 20:15:09,424 main.py:52] epoch 1323, training loss: 8639.05, average training loss: 8356.57, base loss: 15376.12
[INFO 2017-07-01 20:15:13,610 main.py:52] epoch 1324, training loss: 7608.94, average training loss: 8354.83, base loss: 15375.65
[INFO 2017-07-01 20:15:17,812 main.py:52] epoch 1325, training loss: 6629.35, average training loss: 8352.91, base loss: 15373.42
[INFO 2017-07-01 20:15:22,033 main.py:52] epoch 1326, training loss: 8455.73, average training loss: 8353.27, base loss: 15374.08
[INFO 2017-07-01 20:15:26,215 main.py:52] epoch 1327, training loss: 8384.06, average training loss: 8353.89, base loss: 15375.02
[INFO 2017-07-01 20:15:30,301 main.py:52] epoch 1328, training loss: 7347.59, average training loss: 8351.71, base loss: 15372.89
[INFO 2017-07-01 20:15:34,440 main.py:52] epoch 1329, training loss: 8764.75, average training loss: 8350.47, base loss: 15374.64
[INFO 2017-07-01 20:15:38,675 main.py:52] epoch 1330, training loss: 8323.72, average training loss: 8350.28, base loss: 15375.06
[INFO 2017-07-01 20:15:42,808 main.py:52] epoch 1331, training loss: 8084.70, average training loss: 8349.23, base loss: 15375.69
[INFO 2017-07-01 20:15:46,947 main.py:52] epoch 1332, training loss: 8259.53, average training loss: 8349.37, base loss: 15375.21
[INFO 2017-07-01 20:15:51,167 main.py:52] epoch 1333, training loss: 9764.69, average training loss: 8351.53, base loss: 15377.02
[INFO 2017-07-01 20:15:55,318 main.py:52] epoch 1334, training loss: 8185.14, average training loss: 8351.33, base loss: 15377.18
[INFO 2017-07-01 20:15:59,550 main.py:52] epoch 1335, training loss: 8437.34, average training loss: 8350.87, base loss: 15377.73
[INFO 2017-07-01 20:16:03,755 main.py:52] epoch 1336, training loss: 8306.92, average training loss: 8348.90, base loss: 15377.82
[INFO 2017-07-01 20:16:07,960 main.py:52] epoch 1337, training loss: 7724.15, average training loss: 8347.50, base loss: 15377.00
[INFO 2017-07-01 20:16:12,103 main.py:52] epoch 1338, training loss: 8870.48, average training loss: 8347.63, base loss: 15378.83
[INFO 2017-07-01 20:16:16,240 main.py:52] epoch 1339, training loss: 7556.96, average training loss: 8347.24, base loss: 15376.98
[INFO 2017-07-01 20:16:20,370 main.py:52] epoch 1340, training loss: 7953.21, average training loss: 8346.82, base loss: 15375.58
[INFO 2017-07-01 20:16:24,578 main.py:52] epoch 1341, training loss: 8695.80, average training loss: 8346.64, base loss: 15377.16
[INFO 2017-07-01 20:16:28,783 main.py:52] epoch 1342, training loss: 7038.62, average training loss: 8345.11, base loss: 15375.84
[INFO 2017-07-01 20:16:32,991 main.py:52] epoch 1343, training loss: 8529.90, average training loss: 8344.72, base loss: 15377.23
[INFO 2017-07-01 20:16:37,154 main.py:52] epoch 1344, training loss: 8460.21, average training loss: 8343.56, base loss: 15376.40
[INFO 2017-07-01 20:16:41,353 main.py:52] epoch 1345, training loss: 7555.10, average training loss: 8341.92, base loss: 15375.72
[INFO 2017-07-01 20:16:45,579 main.py:52] epoch 1346, training loss: 8109.64, average training loss: 8339.38, base loss: 15374.68
[INFO 2017-07-01 20:16:49,757 main.py:52] epoch 1347, training loss: 8844.94, average training loss: 8340.10, base loss: 15376.54
[INFO 2017-07-01 20:16:53,873 main.py:52] epoch 1348, training loss: 7677.52, average training loss: 8339.43, base loss: 15375.28
[INFO 2017-07-01 20:16:57,961 main.py:52] epoch 1349, training loss: 9194.83, average training loss: 8340.67, base loss: 15376.42
[INFO 2017-07-01 20:17:02,103 main.py:52] epoch 1350, training loss: 7844.53, average training loss: 8339.36, base loss: 15377.16
[INFO 2017-07-01 20:17:06,402 main.py:52] epoch 1351, training loss: 8688.38, average training loss: 8339.30, base loss: 15378.96
[INFO 2017-07-01 20:17:10,577 main.py:52] epoch 1352, training loss: 8020.11, average training loss: 8339.27, base loss: 15378.21
[INFO 2017-07-01 20:17:14,789 main.py:52] epoch 1353, training loss: 9203.26, average training loss: 8339.79, base loss: 15381.04
[INFO 2017-07-01 20:17:18,976 main.py:52] epoch 1354, training loss: 8449.93, average training loss: 8339.57, base loss: 15380.35
[INFO 2017-07-01 20:17:23,137 main.py:52] epoch 1355, training loss: 8709.11, average training loss: 8339.92, base loss: 15381.07
[INFO 2017-07-01 20:17:27,317 main.py:52] epoch 1356, training loss: 8523.43, average training loss: 8340.09, base loss: 15380.81
[INFO 2017-07-01 20:17:31,475 main.py:52] epoch 1357, training loss: 7499.32, average training loss: 8339.44, base loss: 15379.62
[INFO 2017-07-01 20:17:35,566 main.py:52] epoch 1358, training loss: 7820.06, average training loss: 8338.66, base loss: 15377.73
[INFO 2017-07-01 20:17:39,751 main.py:52] epoch 1359, training loss: 8850.28, average training loss: 8339.16, base loss: 15377.17
[INFO 2017-07-01 20:17:43,941 main.py:52] epoch 1360, training loss: 9294.88, average training loss: 8339.88, base loss: 15378.96
[INFO 2017-07-01 20:17:48,145 main.py:52] epoch 1361, training loss: 7934.74, average training loss: 8339.15, base loss: 15379.47
[INFO 2017-07-01 20:17:52,352 main.py:52] epoch 1362, training loss: 7910.94, average training loss: 8338.66, base loss: 15378.74
[INFO 2017-07-01 20:17:56,512 main.py:52] epoch 1363, training loss: 8539.10, average training loss: 8338.29, base loss: 15379.91
[INFO 2017-07-01 20:18:00,666 main.py:52] epoch 1364, training loss: 8434.53, average training loss: 8337.78, base loss: 15381.71
[INFO 2017-07-01 20:18:04,798 main.py:52] epoch 1365, training loss: 8960.34, average training loss: 8337.54, base loss: 15384.01
[INFO 2017-07-01 20:18:08,943 main.py:52] epoch 1366, training loss: 8388.83, average training loss: 8337.67, base loss: 15385.03
[INFO 2017-07-01 20:18:13,135 main.py:52] epoch 1367, training loss: 7835.49, average training loss: 8337.18, base loss: 15384.06
[INFO 2017-07-01 20:18:17,296 main.py:52] epoch 1368, training loss: 8108.29, average training loss: 8336.93, base loss: 15385.18
[INFO 2017-07-01 20:18:21,490 main.py:52] epoch 1369, training loss: 8021.61, average training loss: 8336.52, base loss: 15384.27
[INFO 2017-07-01 20:18:25,630 main.py:52] epoch 1370, training loss: 7429.65, average training loss: 8335.74, base loss: 15382.77
[INFO 2017-07-01 20:18:29,835 main.py:52] epoch 1371, training loss: 7572.45, average training loss: 8335.14, base loss: 15381.73
[INFO 2017-07-01 20:18:33,991 main.py:52] epoch 1372, training loss: 8786.55, average training loss: 8335.95, base loss: 15383.00
[INFO 2017-07-01 20:18:38,143 main.py:52] epoch 1373, training loss: 7698.51, average training loss: 8335.22, base loss: 15383.51
[INFO 2017-07-01 20:18:42,356 main.py:52] epoch 1374, training loss: 8038.47, average training loss: 8333.15, base loss: 15384.65
[INFO 2017-07-01 20:18:46,585 main.py:52] epoch 1375, training loss: 8627.31, average training loss: 8333.09, base loss: 15386.77
[INFO 2017-07-01 20:18:50,815 main.py:52] epoch 1376, training loss: 8359.47, average training loss: 8333.36, base loss: 15387.81
[INFO 2017-07-01 20:18:54,973 main.py:52] epoch 1377, training loss: 7447.29, average training loss: 8333.90, base loss: 15385.58
[INFO 2017-07-01 20:18:59,088 main.py:52] epoch 1378, training loss: 8481.02, average training loss: 8334.33, base loss: 15384.89
[INFO 2017-07-01 20:19:03,240 main.py:52] epoch 1379, training loss: 7815.40, average training loss: 8332.95, base loss: 15382.83
[INFO 2017-07-01 20:19:07,386 main.py:52] epoch 1380, training loss: 7362.40, average training loss: 8330.39, base loss: 15381.14
[INFO 2017-07-01 20:19:11,588 main.py:52] epoch 1381, training loss: 7772.96, average training loss: 8328.56, base loss: 15379.69
[INFO 2017-07-01 20:19:15,765 main.py:52] epoch 1382, training loss: 7807.59, average training loss: 8327.86, base loss: 15380.02
[INFO 2017-07-01 20:19:20,009 main.py:52] epoch 1383, training loss: 8013.58, average training loss: 8327.37, base loss: 15379.31
[INFO 2017-07-01 20:19:24,176 main.py:52] epoch 1384, training loss: 8388.41, average training loss: 8327.24, base loss: 15378.57
[INFO 2017-07-01 20:19:28,373 main.py:52] epoch 1385, training loss: 8151.73, average training loss: 8325.98, base loss: 15376.67
[INFO 2017-07-01 20:19:32,472 main.py:52] epoch 1386, training loss: 7795.08, average training loss: 8324.99, base loss: 15376.39
[INFO 2017-07-01 20:19:36,658 main.py:52] epoch 1387, training loss: 7922.79, average training loss: 8324.09, base loss: 15378.04
[INFO 2017-07-01 20:19:40,792 main.py:52] epoch 1388, training loss: 8015.88, average training loss: 8323.04, base loss: 15378.66
[INFO 2017-07-01 20:19:44,978 main.py:52] epoch 1389, training loss: 8334.70, average training loss: 8323.30, base loss: 15378.56
[INFO 2017-07-01 20:19:49,217 main.py:52] epoch 1390, training loss: 7917.03, average training loss: 8323.27, base loss: 15378.49
[INFO 2017-07-01 20:19:53,384 main.py:52] epoch 1391, training loss: 7597.31, average training loss: 8321.18, base loss: 15377.28
[INFO 2017-07-01 20:19:57,529 main.py:52] epoch 1392, training loss: 7398.76, average training loss: 8319.15, base loss: 15374.27
[INFO 2017-07-01 20:20:01,727 main.py:52] epoch 1393, training loss: 7916.59, average training loss: 8318.97, base loss: 15371.98
[INFO 2017-07-01 20:20:05,889 main.py:52] epoch 1394, training loss: 7916.26, average training loss: 8317.15, base loss: 15372.39
[INFO 2017-07-01 20:20:10,146 main.py:52] epoch 1395, training loss: 8700.25, average training loss: 8317.17, base loss: 15373.72
[INFO 2017-07-01 20:20:14,307 main.py:52] epoch 1396, training loss: 7991.83, average training loss: 8316.26, base loss: 15373.27
[INFO 2017-07-01 20:20:18,497 main.py:52] epoch 1397, training loss: 7669.82, average training loss: 8315.35, base loss: 15372.82
[INFO 2017-07-01 20:20:22,663 main.py:52] epoch 1398, training loss: 8409.54, average training loss: 8314.93, base loss: 15372.93
[INFO 2017-07-01 20:20:26,835 main.py:52] epoch 1399, training loss: 8022.71, average training loss: 8314.91, base loss: 15372.19
[INFO 2017-07-01 20:20:26,836 main.py:54] epoch 1399, testing
[INFO 2017-07-01 20:20:26,836 main.py:61] model save to ./model/final.pth
[INFO 2017-07-01 20:20:31,144 main.py:52] epoch 1400, training loss: 8049.33, average training loss: 8313.77, base loss: 15372.17
[INFO 2017-07-01 20:20:35,323 main.py:52] epoch 1401, training loss: 7941.55, average training loss: 8313.78, base loss: 15373.50
[INFO 2017-07-01 20:20:39,463 main.py:52] epoch 1402, training loss: 7184.09, average training loss: 8312.86, base loss: 15371.90
[INFO 2017-07-01 20:20:43,629 main.py:52] epoch 1403, training loss: 7701.80, average training loss: 8312.44, base loss: 15371.73
[INFO 2017-07-01 20:20:47,840 main.py:52] epoch 1404, training loss: 7826.12, average training loss: 8311.96, base loss: 15370.75
[INFO 2017-07-01 20:20:52,038 main.py:52] epoch 1405, training loss: 8555.40, average training loss: 8310.74, base loss: 15371.98
[INFO 2017-07-01 20:20:56,266 main.py:52] epoch 1406, training loss: 7765.38, average training loss: 8310.64, base loss: 15372.53
[INFO 2017-07-01 20:21:00,407 main.py:52] epoch 1407, training loss: 9088.24, average training loss: 8311.50, base loss: 15374.97
[INFO 2017-07-01 20:21:04,534 main.py:52] epoch 1408, training loss: 7831.03, average training loss: 8309.82, base loss: 15374.92
[INFO 2017-07-01 20:21:08,717 main.py:52] epoch 1409, training loss: 7625.44, average training loss: 8309.00, base loss: 15374.05
[INFO 2017-07-01 20:21:12,953 main.py:52] epoch 1410, training loss: 8435.87, average training loss: 8308.09, base loss: 15373.37
[INFO 2017-07-01 20:21:17,165 main.py:52] epoch 1411, training loss: 8729.87, average training loss: 8309.11, base loss: 15372.78
[INFO 2017-07-01 20:21:21,284 main.py:52] epoch 1412, training loss: 7512.49, average training loss: 8308.53, base loss: 15369.19
[INFO 2017-07-01 20:21:25,400 main.py:52] epoch 1413, training loss: 7862.68, average training loss: 8307.87, base loss: 15367.87
[INFO 2017-07-01 20:21:29,562 main.py:52] epoch 1414, training loss: 8055.33, average training loss: 8308.60, base loss: 15366.93
[INFO 2017-07-01 20:21:33,837 main.py:52] epoch 1415, training loss: 8156.08, average training loss: 8308.70, base loss: 15366.15
[INFO 2017-07-01 20:21:38,097 main.py:52] epoch 1416, training loss: 8057.10, average training loss: 8307.02, base loss: 15368.06
[INFO 2017-07-01 20:21:42,244 main.py:52] epoch 1417, training loss: 8193.14, average training loss: 8306.93, base loss: 15369.63
[INFO 2017-07-01 20:21:46,362 main.py:52] epoch 1418, training loss: 8067.13, average training loss: 8304.91, base loss: 15368.74
[INFO 2017-07-01 20:21:50,479 main.py:52] epoch 1419, training loss: 7509.38, average training loss: 8302.98, base loss: 15367.53
[INFO 2017-07-01 20:21:54,732 main.py:52] epoch 1420, training loss: 8056.56, average training loss: 8302.42, base loss: 15367.69
[INFO 2017-07-01 20:21:58,983 main.py:52] epoch 1421, training loss: 8867.05, average training loss: 8303.30, base loss: 15368.87
[INFO 2017-07-01 20:22:03,081 main.py:52] epoch 1422, training loss: 8570.15, average training loss: 8302.34, base loss: 15368.99
[INFO 2017-07-01 20:22:07,232 main.py:52] epoch 1423, training loss: 7780.75, average training loss: 8300.93, base loss: 15368.10
[INFO 2017-07-01 20:22:11,401 main.py:52] epoch 1424, training loss: 8814.66, average training loss: 8300.37, base loss: 15370.62
[INFO 2017-07-01 20:22:15,527 main.py:52] epoch 1425, training loss: 7758.86, average training loss: 8300.61, base loss: 15370.54
[INFO 2017-07-01 20:22:19,686 main.py:52] epoch 1426, training loss: 7946.92, average training loss: 8299.25, base loss: 15370.09
[INFO 2017-07-01 20:22:23,938 main.py:52] epoch 1427, training loss: 8143.75, average training loss: 8298.20, base loss: 15370.34
[INFO 2017-07-01 20:22:28,095 main.py:52] epoch 1428, training loss: 8701.43, average training loss: 8297.88, base loss: 15371.28
[INFO 2017-07-01 20:22:32,286 main.py:52] epoch 1429, training loss: 7566.63, average training loss: 8296.14, base loss: 15369.42
[INFO 2017-07-01 20:22:36,438 main.py:52] epoch 1430, training loss: 9309.71, average training loss: 8296.36, base loss: 15370.06
[INFO 2017-07-01 20:22:40,530 main.py:52] epoch 1431, training loss: 7966.08, average training loss: 8295.63, base loss: 15368.65
[INFO 2017-07-01 20:22:44,698 main.py:52] epoch 1432, training loss: 8989.63, average training loss: 8296.27, base loss: 15370.52
[INFO 2017-07-01 20:22:48,801 main.py:52] epoch 1433, training loss: 7668.94, average training loss: 8295.36, base loss: 15370.93
[INFO 2017-07-01 20:22:52,979 main.py:52] epoch 1434, training loss: 8422.38, average training loss: 8294.78, base loss: 15371.60
[INFO 2017-07-01 20:22:57,170 main.py:52] epoch 1435, training loss: 8288.20, average training loss: 8294.37, base loss: 15371.05
[INFO 2017-07-01 20:23:01,311 main.py:52] epoch 1436, training loss: 8561.83, average training loss: 8295.43, base loss: 15371.28
[INFO 2017-07-01 20:23:05,484 main.py:52] epoch 1437, training loss: 8733.52, average training loss: 8295.67, base loss: 15372.13
[INFO 2017-07-01 20:23:09,664 main.py:52] epoch 1438, training loss: 8412.41, average training loss: 8295.95, base loss: 15373.22
[INFO 2017-07-01 20:23:13,777 main.py:52] epoch 1439, training loss: 7962.66, average training loss: 8294.13, base loss: 15373.41
[INFO 2017-07-01 20:23:18,022 main.py:52] epoch 1440, training loss: 8559.88, average training loss: 8293.94, base loss: 15374.21
[INFO 2017-07-01 20:23:22,172 main.py:52] epoch 1441, training loss: 8246.94, average training loss: 8293.93, base loss: 15375.06
[INFO 2017-07-01 20:23:26,331 main.py:52] epoch 1442, training loss: 7866.25, average training loss: 8292.55, base loss: 15373.72
[INFO 2017-07-01 20:23:30,504 main.py:52] epoch 1443, training loss: 8532.48, average training loss: 8293.64, base loss: 15373.94
[INFO 2017-07-01 20:23:34,731 main.py:52] epoch 1444, training loss: 8688.33, average training loss: 8294.87, base loss: 15373.19
[INFO 2017-07-01 20:23:38,905 main.py:52] epoch 1445, training loss: 8983.97, average training loss: 8294.75, base loss: 15373.33
[INFO 2017-07-01 20:23:43,082 main.py:52] epoch 1446, training loss: 8964.85, average training loss: 8293.97, base loss: 15374.41
[INFO 2017-07-01 20:23:47,232 main.py:52] epoch 1447, training loss: 8653.12, average training loss: 8294.12, base loss: 15374.87
[INFO 2017-07-01 20:23:51,505 main.py:52] epoch 1448, training loss: 7381.64, average training loss: 8293.31, base loss: 15373.79
[INFO 2017-07-01 20:23:55,746 main.py:52] epoch 1449, training loss: 7069.79, average training loss: 8292.13, base loss: 15371.00
[INFO 2017-07-01 20:23:59,930 main.py:52] epoch 1450, training loss: 9259.48, average training loss: 8293.18, base loss: 15373.77
[INFO 2017-07-01 20:24:04,072 main.py:52] epoch 1451, training loss: 8688.57, average training loss: 8293.42, base loss: 15375.44
[INFO 2017-07-01 20:24:08,330 main.py:52] epoch 1452, training loss: 8143.94, average training loss: 8292.85, base loss: 15375.63
[INFO 2017-07-01 20:24:12,526 main.py:52] epoch 1453, training loss: 6772.53, average training loss: 8291.67, base loss: 15373.79
[INFO 2017-07-01 20:24:16,746 main.py:52] epoch 1454, training loss: 7563.44, average training loss: 8291.20, base loss: 15373.60
[INFO 2017-07-01 20:24:20,926 main.py:52] epoch 1455, training loss: 7691.63, average training loss: 8289.43, base loss: 15371.62
[INFO 2017-07-01 20:24:25,111 main.py:52] epoch 1456, training loss: 7421.69, average training loss: 8286.03, base loss: 15370.62
[INFO 2017-07-01 20:24:29,284 main.py:52] epoch 1457, training loss: 7346.57, average training loss: 8285.29, base loss: 15369.54
[INFO 2017-07-01 20:24:33,469 main.py:52] epoch 1458, training loss: 8542.30, average training loss: 8285.75, base loss: 15369.53
[INFO 2017-07-01 20:24:37,614 main.py:52] epoch 1459, training loss: 8565.69, average training loss: 8284.85, base loss: 15369.81
[INFO 2017-07-01 20:24:41,823 main.py:52] epoch 1460, training loss: 8619.58, average training loss: 8284.81, base loss: 15369.85
[INFO 2017-07-01 20:24:45,945 main.py:52] epoch 1461, training loss: 8185.17, average training loss: 8284.07, base loss: 15369.63
[INFO 2017-07-01 20:24:50,140 main.py:52] epoch 1462, training loss: 8048.21, average training loss: 8284.53, base loss: 15370.56
[INFO 2017-07-01 20:24:54,269 main.py:52] epoch 1463, training loss: 8860.32, average training loss: 8284.70, base loss: 15371.35
[INFO 2017-07-01 20:24:58,508 main.py:52] epoch 1464, training loss: 7491.95, average training loss: 8283.56, base loss: 15371.98
[INFO 2017-07-01 20:25:02,705 main.py:52] epoch 1465, training loss: 7426.75, average training loss: 8282.17, base loss: 15371.25
[INFO 2017-07-01 20:25:06,923 main.py:52] epoch 1466, training loss: 8109.80, average training loss: 8282.14, base loss: 15370.77
[INFO 2017-07-01 20:25:11,108 main.py:52] epoch 1467, training loss: 8470.71, average training loss: 8282.09, base loss: 15372.39
[INFO 2017-07-01 20:25:15,298 main.py:52] epoch 1468, training loss: 7579.71, average training loss: 8281.35, base loss: 15371.03
[INFO 2017-07-01 20:25:19,466 main.py:52] epoch 1469, training loss: 8074.79, average training loss: 8281.54, base loss: 15371.89
[INFO 2017-07-01 20:25:23,676 main.py:52] epoch 1470, training loss: 7978.59, average training loss: 8280.66, base loss: 15371.63
[INFO 2017-07-01 20:25:27,836 main.py:52] epoch 1471, training loss: 7202.20, average training loss: 8279.80, base loss: 15371.86
[INFO 2017-07-01 20:25:31,978 main.py:52] epoch 1472, training loss: 6885.55, average training loss: 8277.06, base loss: 15368.38
[INFO 2017-07-01 20:25:36,078 main.py:52] epoch 1473, training loss: 7484.89, average training loss: 8274.97, base loss: 15366.56
[INFO 2017-07-01 20:25:40,245 main.py:52] epoch 1474, training loss: 7865.83, average training loss: 8274.30, base loss: 15366.76
[INFO 2017-07-01 20:25:44,370 main.py:52] epoch 1475, training loss: 8504.67, average training loss: 8274.94, base loss: 15366.67
[INFO 2017-07-01 20:25:48,505 main.py:52] epoch 1476, training loss: 8096.29, average training loss: 8274.82, base loss: 15366.08
[INFO 2017-07-01 20:25:52,696 main.py:52] epoch 1477, training loss: 8193.34, average training loss: 8274.76, base loss: 15365.04
[INFO 2017-07-01 20:25:56,861 main.py:52] epoch 1478, training loss: 8042.77, average training loss: 8274.13, base loss: 15364.63
[INFO 2017-07-01 20:26:01,054 main.py:52] epoch 1479, training loss: 7734.14, average training loss: 8273.76, base loss: 15363.79
[INFO 2017-07-01 20:26:05,228 main.py:52] epoch 1480, training loss: 8509.91, average training loss: 8273.75, base loss: 15364.53
[INFO 2017-07-01 20:26:09,352 main.py:52] epoch 1481, training loss: 8387.68, average training loss: 8274.13, base loss: 15364.15
[INFO 2017-07-01 20:26:13,528 main.py:52] epoch 1482, training loss: 7499.09, average training loss: 8273.55, base loss: 15363.04
[INFO 2017-07-01 20:26:17,645 main.py:52] epoch 1483, training loss: 7759.61, average training loss: 8273.23, base loss: 15361.29
[INFO 2017-07-01 20:26:21,831 main.py:52] epoch 1484, training loss: 7535.62, average training loss: 8271.85, base loss: 15360.68
[INFO 2017-07-01 20:26:26,008 main.py:52] epoch 1485, training loss: 9195.08, average training loss: 8273.79, base loss: 15362.88
[INFO 2017-07-01 20:26:30,148 main.py:52] epoch 1486, training loss: 8838.36, average training loss: 8273.77, base loss: 15364.39
[INFO 2017-07-01 20:26:34,322 main.py:52] epoch 1487, training loss: 7602.23, average training loss: 8272.68, base loss: 15363.25
[INFO 2017-07-01 20:26:38,505 main.py:52] epoch 1488, training loss: 7717.84, average training loss: 8270.63, base loss: 15363.21
[INFO 2017-07-01 20:26:42,656 main.py:52] epoch 1489, training loss: 8051.22, average training loss: 8268.91, base loss: 15364.09
[INFO 2017-07-01 20:26:46,805 main.py:52] epoch 1490, training loss: 7351.83, average training loss: 8266.97, base loss: 15362.56
[INFO 2017-07-01 20:26:50,972 main.py:52] epoch 1491, training loss: 8390.36, average training loss: 8266.75, base loss: 15364.29
[INFO 2017-07-01 20:26:55,074 main.py:52] epoch 1492, training loss: 7688.04, average training loss: 8266.11, base loss: 15365.93
[INFO 2017-07-01 20:26:59,262 main.py:52] epoch 1493, training loss: 8062.60, average training loss: 8265.75, base loss: 15367.47
[INFO 2017-07-01 20:27:03,424 main.py:52] epoch 1494, training loss: 8390.20, average training loss: 8265.60, base loss: 15366.01
[INFO 2017-07-01 20:27:07,576 main.py:52] epoch 1495, training loss: 7428.12, average training loss: 8264.29, base loss: 15365.10
[INFO 2017-07-01 20:27:11,756 main.py:52] epoch 1496, training loss: 7886.68, average training loss: 8263.83, base loss: 15365.42
[INFO 2017-07-01 20:27:15,969 main.py:52] epoch 1497, training loss: 8394.12, average training loss: 8263.17, base loss: 15366.32
[INFO 2017-07-01 20:27:20,173 main.py:52] epoch 1498, training loss: 7093.79, average training loss: 8261.91, base loss: 15364.29
[INFO 2017-07-01 20:27:24,305 main.py:52] epoch 1499, training loss: 8666.03, average training loss: 8261.43, base loss: 15365.63
[INFO 2017-07-01 20:27:24,306 main.py:54] epoch 1499, testing
[INFO 2017-07-01 20:27:24,306 main.py:61] model save to ./model/final.pth
[INFO 2017-07-01 20:27:28,577 main.py:52] epoch 1500, training loss: 7757.86, average training loss: 8261.48, base loss: 15366.16
[INFO 2017-07-01 20:27:32,683 main.py:52] epoch 1501, training loss: 7806.03, average training loss: 8261.06, base loss: 15367.62
[INFO 2017-07-01 20:27:36,850 main.py:52] epoch 1502, training loss: 8338.95, average training loss: 8260.58, base loss: 15369.18
[INFO 2017-07-01 20:27:41,087 main.py:52] epoch 1503, training loss: 8549.07, average training loss: 8260.75, base loss: 15370.65
[INFO 2017-07-01 20:27:45,260 main.py:52] epoch 1504, training loss: 8763.26, average training loss: 8259.59, base loss: 15372.29
[INFO 2017-07-01 20:27:49,445 main.py:52] epoch 1505, training loss: 7549.85, average training loss: 8258.38, base loss: 15372.48
[INFO 2017-07-01 20:27:53,593 main.py:52] epoch 1506, training loss: 8058.45, average training loss: 8258.33, base loss: 15372.71
[INFO 2017-07-01 20:27:57,833 main.py:52] epoch 1507, training loss: 8534.36, average training loss: 8258.66, base loss: 15373.38
[INFO 2017-07-01 20:28:02,003 main.py:52] epoch 1508, training loss: 8208.69, average training loss: 8257.57, base loss: 15372.81
[INFO 2017-07-01 20:28:06,192 main.py:52] epoch 1509, training loss: 8309.68, average training loss: 8257.24, base loss: 15373.52
[INFO 2017-07-01 20:28:10,322 main.py:52] epoch 1510, training loss: 7013.59, average training loss: 8255.48, base loss: 15372.33
[INFO 2017-07-01 20:28:14,500 main.py:52] epoch 1511, training loss: 7681.45, average training loss: 8254.22, base loss: 15372.15
[INFO 2017-07-01 20:28:18,625 main.py:52] epoch 1512, training loss: 8057.16, average training loss: 8253.33, base loss: 15372.38
[INFO 2017-07-01 20:28:22,850 main.py:52] epoch 1513, training loss: 8666.65, average training loss: 8253.73, base loss: 15373.26
[INFO 2017-07-01 20:28:27,015 main.py:52] epoch 1514, training loss: 7877.62, average training loss: 8251.82, base loss: 15373.77
[INFO 2017-07-01 20:28:31,189 main.py:52] epoch 1515, training loss: 9032.87, average training loss: 8252.40, base loss: 15376.08
[INFO 2017-07-01 20:28:35,384 main.py:52] epoch 1516, training loss: 8441.51, average training loss: 8252.59, base loss: 15377.31
[INFO 2017-07-01 20:28:39,492 main.py:52] epoch 1517, training loss: 7347.55, average training loss: 8251.46, base loss: 15375.17
[INFO 2017-07-01 20:28:43,751 main.py:52] epoch 1518, training loss: 8419.80, average training loss: 8252.14, base loss: 15375.61
[INFO 2017-07-01 20:28:47,911 main.py:52] epoch 1519, training loss: 7874.53, average training loss: 8251.86, base loss: 15376.43
[INFO 2017-07-01 20:28:52,089 main.py:52] epoch 1520, training loss: 7703.25, average training loss: 8251.74, base loss: 15374.11
[INFO 2017-07-01 20:28:56,234 main.py:52] epoch 1521, training loss: 7738.59, average training loss: 8250.85, base loss: 15372.46
[INFO 2017-07-01 20:29:00,501 main.py:52] epoch 1522, training loss: 8310.26, average training loss: 8250.07, base loss: 15372.12
[INFO 2017-07-01 20:29:04,675 main.py:52] epoch 1523, training loss: 8094.23, average training loss: 8250.30, base loss: 15371.80
[INFO 2017-07-01 20:29:08,769 main.py:52] epoch 1524, training loss: 8575.61, average training loss: 8251.73, base loss: 15372.56
[INFO 2017-07-01 20:29:12,937 main.py:52] epoch 1525, training loss: 7697.73, average training loss: 8250.50, base loss: 15370.84
[INFO 2017-07-01 20:29:17,134 main.py:52] epoch 1526, training loss: 8567.33, average training loss: 8250.53, base loss: 15371.02
[INFO 2017-07-01 20:29:21,338 main.py:52] epoch 1527, training loss: 8061.63, average training loss: 8250.16, base loss: 15370.47
[INFO 2017-07-01 20:29:25,511 main.py:52] epoch 1528, training loss: 8025.54, average training loss: 8250.87, base loss: 15369.01
[INFO 2017-07-01 20:29:29,767 main.py:52] epoch 1529, training loss: 7835.72, average training loss: 8250.79, base loss: 15367.12
[INFO 2017-07-01 20:29:34,004 main.py:52] epoch 1530, training loss: 7963.30, average training loss: 8250.48, base loss: 15366.42
[INFO 2017-07-01 20:29:38,215 main.py:52] epoch 1531, training loss: 8092.50, average training loss: 8249.91, base loss: 15365.34
[INFO 2017-07-01 20:29:42,509 main.py:52] epoch 1532, training loss: 11105.42, average training loss: 8252.39, base loss: 15370.68
[INFO 2017-07-01 20:29:46,707 main.py:52] epoch 1533, training loss: 8470.67, average training loss: 8251.67, base loss: 15371.43
[INFO 2017-07-01 20:29:50,827 main.py:52] epoch 1534, training loss: 8572.96, average training loss: 8250.80, base loss: 15372.01
[INFO 2017-07-01 20:29:54,990 main.py:52] epoch 1535, training loss: 9024.56, average training loss: 8252.06, base loss: 15373.20
[INFO 2017-07-01 20:29:59,147 main.py:52] epoch 1536, training loss: 9082.79, average training loss: 8253.00, base loss: 15374.85
[INFO 2017-07-01 20:30:03,328 main.py:52] epoch 1537, training loss: 7736.21, average training loss: 8253.15, base loss: 15374.33
[INFO 2017-07-01 20:30:07,528 main.py:52] epoch 1538, training loss: 8463.32, average training loss: 8253.30, base loss: 15375.27
[INFO 2017-07-01 20:30:11,699 main.py:52] epoch 1539, training loss: 9000.27, average training loss: 8253.82, base loss: 15376.94
[INFO 2017-07-01 20:30:15,853 main.py:52] epoch 1540, training loss: 7963.35, average training loss: 8253.30, base loss: 15378.03
[INFO 2017-07-01 20:30:20,016 main.py:52] epoch 1541, training loss: 9389.54, average training loss: 8254.14, base loss: 15380.74
[INFO 2017-07-01 20:30:24,171 main.py:52] epoch 1542, training loss: 7860.43, average training loss: 8253.92, base loss: 15380.58
[INFO 2017-07-01 20:30:28,358 main.py:52] epoch 1543, training loss: 7980.53, average training loss: 8252.85, base loss: 15380.92
[INFO 2017-07-01 20:30:32,564 main.py:52] epoch 1544, training loss: 7814.31, average training loss: 8252.38, base loss: 15380.33
[INFO 2017-07-01 20:30:36,758 main.py:52] epoch 1545, training loss: 8095.21, average training loss: 8252.62, base loss: 15378.62
[INFO 2017-07-01 20:30:40,923 main.py:52] epoch 1546, training loss: 7470.19, average training loss: 8251.46, base loss: 15378.89
[INFO 2017-07-01 20:30:45,133 main.py:52] epoch 1547, training loss: 8126.15, average training loss: 8251.49, base loss: 15380.31
[INFO 2017-07-01 20:30:49,362 main.py:52] epoch 1548, training loss: 8370.52, average training loss: 8251.08, base loss: 15382.85
[INFO 2017-07-01 20:30:53,458 main.py:52] epoch 1549, training loss: 7511.49, average training loss: 8250.90, base loss: 15383.47
[INFO 2017-07-01 20:30:57,740 main.py:52] epoch 1550, training loss: 8535.73, average training loss: 8251.12, base loss: 15384.76
[INFO 2017-07-01 20:31:01,888 main.py:52] epoch 1551, training loss: 8294.03, average training loss: 8251.57, base loss: 15384.72
[INFO 2017-07-01 20:31:06,080 main.py:52] epoch 1552, training loss: 7623.26, average training loss: 8251.78, base loss: 15383.79
[INFO 2017-07-01 20:31:10,238 main.py:52] epoch 1553, training loss: 8363.84, average training loss: 8252.12, base loss: 15384.24
[INFO 2017-07-01 20:31:14,341 main.py:52] epoch 1554, training loss: 7604.11, average training loss: 8251.68, base loss: 15384.60
[INFO 2017-07-01 20:31:18,457 main.py:52] epoch 1555, training loss: 8954.05, average training loss: 8251.38, base loss: 15385.51
[INFO 2017-07-01 20:31:22,554 main.py:52] epoch 1556, training loss: 7243.11, average training loss: 8249.42, base loss: 15385.92
[INFO 2017-07-01 20:31:26,752 main.py:52] epoch 1557, training loss: 8368.96, average training loss: 8249.43, base loss: 15386.85
[INFO 2017-07-01 20:31:30,917 main.py:52] epoch 1558, training loss: 7723.26, average training loss: 8249.13, base loss: 15386.88
[INFO 2017-07-01 20:31:35,092 main.py:52] epoch 1559, training loss: 8737.89, average training loss: 8248.81, base loss: 15387.43
[INFO 2017-07-01 20:31:39,304 main.py:52] epoch 1560, training loss: 9943.05, average training loss: 8250.65, base loss: 15392.25
[INFO 2017-07-01 20:31:43,471 main.py:52] epoch 1561, training loss: 7466.46, average training loss: 8249.97, base loss: 15392.59
[INFO 2017-07-01 20:31:47,677 main.py:52] epoch 1562, training loss: 8604.24, average training loss: 8251.01, base loss: 15394.16
[INFO 2017-07-01 20:31:51,845 main.py:52] epoch 1563, training loss: 6893.36, average training loss: 8250.22, base loss: 15392.14
[INFO 2017-07-01 20:31:56,007 main.py:52] epoch 1564, training loss: 8325.49, average training loss: 8250.89, base loss: 15391.88
[INFO 2017-07-01 20:32:00,184 main.py:52] epoch 1565, training loss: 8106.92, average training loss: 8250.11, base loss: 15391.69
[INFO 2017-07-01 20:32:04,388 main.py:52] epoch 1566, training loss: 8525.11, average training loss: 8250.88, base loss: 15390.80
[INFO 2017-07-01 20:32:08,628 main.py:52] epoch 1567, training loss: 7722.80, average training loss: 8249.63, base loss: 15389.05
[INFO 2017-07-01 20:32:12,813 main.py:52] epoch 1568, training loss: 8128.94, average training loss: 8249.56, base loss: 15388.29
[INFO 2017-07-01 20:32:16,955 main.py:52] epoch 1569, training loss: 7645.73, average training loss: 8248.76, base loss: 15386.79
[INFO 2017-07-01 20:32:21,081 main.py:52] epoch 1570, training loss: 7656.97, average training loss: 8247.25, base loss: 15386.49
[INFO 2017-07-01 20:32:25,217 main.py:52] epoch 1571, training loss: 7402.88, average training loss: 8245.36, base loss: 15386.83
[INFO 2017-07-01 20:32:29,350 main.py:52] epoch 1572, training loss: 8267.97, average training loss: 8244.92, base loss: 15388.24
[INFO 2017-07-01 20:32:33,463 main.py:52] epoch 1573, training loss: 8307.24, average training loss: 8244.86, base loss: 15388.69
[INFO 2017-07-01 20:32:37,641 main.py:52] epoch 1574, training loss: 8619.08, average training loss: 8244.97, base loss: 15392.02
[INFO 2017-07-01 20:32:41,782 main.py:52] epoch 1575, training loss: 8345.85, average training loss: 8244.96, base loss: 15392.94
[INFO 2017-07-01 20:32:45,975 main.py:52] epoch 1576, training loss: 8193.70, average training loss: 8244.41, base loss: 15392.40
[INFO 2017-07-01 20:32:50,149 main.py:52] epoch 1577, training loss: 7354.83, average training loss: 8243.86, base loss: 15391.18
[INFO 2017-07-01 20:32:54,290 main.py:52] epoch 1578, training loss: 7845.20, average training loss: 8243.33, base loss: 15390.77
[INFO 2017-07-01 20:32:58,452 main.py:52] epoch 1579, training loss: 7565.25, average training loss: 8242.52, base loss: 15389.67
[INFO 2017-07-01 20:33:02,670 main.py:52] epoch 1580, training loss: 8749.76, average training loss: 8242.22, base loss: 15391.37
[INFO 2017-07-01 20:33:06,862 main.py:52] epoch 1581, training loss: 8267.63, average training loss: 8241.07, base loss: 15391.26
[INFO 2017-07-01 20:33:11,078 main.py:52] epoch 1582, training loss: 8292.67, average training loss: 8240.72, base loss: 15391.85
[INFO 2017-07-01 20:33:15,348 main.py:52] epoch 1583, training loss: 8331.10, average training loss: 8240.16, base loss: 15391.80
[INFO 2017-07-01 20:33:19,457 main.py:52] epoch 1584, training loss: 8217.30, average training loss: 8240.07, base loss: 15391.70
[INFO 2017-07-01 20:33:23,676 main.py:52] epoch 1585, training loss: 7264.74, average training loss: 8238.39, base loss: 15391.24
[INFO 2017-07-01 20:33:27,800 main.py:52] epoch 1586, training loss: 8500.56, average training loss: 8238.89, base loss: 15391.01
[INFO 2017-07-01 20:33:31,937 main.py:52] epoch 1587, training loss: 9130.87, average training loss: 8240.12, base loss: 15391.34
[INFO 2017-07-01 20:33:36,154 main.py:52] epoch 1588, training loss: 8592.93, average training loss: 8240.43, base loss: 15392.59
[INFO 2017-07-01 20:33:40,372 main.py:52] epoch 1589, training loss: 7409.65, average training loss: 8239.04, base loss: 15391.55
[INFO 2017-07-01 20:33:44,563 main.py:52] epoch 1590, training loss: 8281.75, average training loss: 8238.28, base loss: 15393.05
[INFO 2017-07-01 20:33:48,740 main.py:52] epoch 1591, training loss: 7156.78, average training loss: 8237.74, base loss: 15392.87
[INFO 2017-07-01 20:33:52,970 main.py:52] epoch 1592, training loss: 8182.58, average training loss: 8237.68, base loss: 15393.68
[INFO 2017-07-01 20:33:57,237 main.py:52] epoch 1593, training loss: 8282.87, average training loss: 8237.48, base loss: 15395.02
[INFO 2017-07-01 20:34:01,397 main.py:52] epoch 1594, training loss: 7854.67, average training loss: 8236.83, base loss: 15396.12
[INFO 2017-07-01 20:34:05,519 main.py:52] epoch 1595, training loss: 8324.16, average training loss: 8237.32, base loss: 15396.43
[INFO 2017-07-01 20:34:09,723 main.py:52] epoch 1596, training loss: 7996.24, average training loss: 8237.11, base loss: 15393.82
[INFO 2017-07-01 20:34:13,923 main.py:52] epoch 1597, training loss: 8842.82, average training loss: 8237.61, base loss: 15393.46
[INFO 2017-07-01 20:34:18,076 main.py:52] epoch 1598, training loss: 7684.53, average training loss: 8237.24, base loss: 15392.39
[INFO 2017-07-01 20:34:22,251 main.py:52] epoch 1599, training loss: 9013.43, average training loss: 8237.23, base loss: 15393.43
[INFO 2017-07-01 20:34:22,252 main.py:54] epoch 1599, testing
[INFO 2017-07-01 20:34:22,252 main.py:61] model save to ./model/final.pth
[INFO 2017-07-01 20:34:26,543 main.py:52] epoch 1600, training loss: 8733.13, average training loss: 8238.14, base loss: 15394.75
[INFO 2017-07-01 20:34:30,753 main.py:52] epoch 1601, training loss: 8359.55, average training loss: 8238.05, base loss: 15395.51
[INFO 2017-07-01 20:34:34,964 main.py:52] epoch 1602, training loss: 7340.90, average training loss: 8237.53, base loss: 15393.45
[INFO 2017-07-01 20:34:39,169 main.py:52] epoch 1603, training loss: 7447.56, average training loss: 8236.17, base loss: 15391.75
[INFO 2017-07-01 20:34:43,318 main.py:52] epoch 1604, training loss: 8915.25, average training loss: 8236.40, base loss: 15393.29
[INFO 2017-07-01 20:34:47,523 main.py:52] epoch 1605, training loss: 8275.02, average training loss: 8235.37, base loss: 15394.06
[INFO 2017-07-01 20:34:51,688 main.py:52] epoch 1606, training loss: 8642.44, average training loss: 8234.62, base loss: 15394.89
[INFO 2017-07-01 20:34:55,838 main.py:52] epoch 1607, training loss: 7990.21, average training loss: 8234.55, base loss: 15395.14
[INFO 2017-07-01 20:35:00,118 main.py:52] epoch 1608, training loss: 7844.13, average training loss: 8234.75, base loss: 15394.95
[INFO 2017-07-01 20:35:04,269 main.py:52] epoch 1609, training loss: 8279.47, average training loss: 8235.62, base loss: 15395.20
[INFO 2017-07-01 20:35:08,405 main.py:52] epoch 1610, training loss: 8219.10, average training loss: 8236.10, base loss: 15394.35
[INFO 2017-07-01 20:35:12,599 main.py:52] epoch 1611, training loss: 8754.93, average training loss: 8236.48, base loss: 15394.11
[INFO 2017-07-01 20:35:16,758 main.py:52] epoch 1612, training loss: 8015.10, average training loss: 8236.01, base loss: 15394.41
[INFO 2017-07-01 20:35:20,917 main.py:52] epoch 1613, training loss: 7730.26, average training loss: 8235.18, base loss: 15393.78
[INFO 2017-07-01 20:35:25,089 main.py:52] epoch 1614, training loss: 7847.00, average training loss: 8235.99, base loss: 15394.58
[INFO 2017-07-01 20:35:29,278 main.py:52] epoch 1615, training loss: 9164.62, average training loss: 8237.15, base loss: 15396.56
[INFO 2017-07-01 20:35:33,430 main.py:52] epoch 1616, training loss: 7662.44, average training loss: 8236.36, base loss: 15396.47
[INFO 2017-07-01 20:35:37,541 main.py:52] epoch 1617, training loss: 8646.05, average training loss: 8236.85, base loss: 15397.74
[INFO 2017-07-01 20:35:41,710 main.py:52] epoch 1618, training loss: 7120.86, average training loss: 8236.06, base loss: 15396.32
[INFO 2017-07-01 20:35:45,931 main.py:52] epoch 1619, training loss: 7681.13, average training loss: 8236.15, base loss: 15395.28
[INFO 2017-07-01 20:35:50,094 main.py:52] epoch 1620, training loss: 7234.17, average training loss: 8235.70, base loss: 15394.82
[INFO 2017-07-01 20:35:54,283 main.py:52] epoch 1621, training loss: 7007.89, average training loss: 8234.74, base loss: 15393.01
[INFO 2017-07-01 20:35:58,456 main.py:52] epoch 1622, training loss: 7887.27, average training loss: 8233.76, base loss: 15393.13
[INFO 2017-07-01 20:36:02,588 main.py:52] epoch 1623, training loss: 7015.32, average training loss: 8231.90, base loss: 15391.86
[INFO 2017-07-01 20:36:06,780 main.py:52] epoch 1624, training loss: 8979.98, average training loss: 8232.54, base loss: 15394.06
[INFO 2017-07-01 20:36:10,876 main.py:52] epoch 1625, training loss: 8183.10, average training loss: 8232.37, base loss: 15394.66
[INFO 2017-07-01 20:36:15,036 main.py:52] epoch 1626, training loss: 8199.24, average training loss: 8231.93, base loss: 15395.57
[INFO 2017-07-01 20:36:19,227 main.py:52] epoch 1627, training loss: 8482.45, average training loss: 8232.60, base loss: 15397.18
[INFO 2017-07-01 20:36:23,351 main.py:52] epoch 1628, training loss: 8263.46, average training loss: 8232.13, base loss: 15397.55
[INFO 2017-07-01 20:36:27,509 main.py:52] epoch 1629, training loss: 8074.11, average training loss: 8231.97, base loss: 15397.71
[INFO 2017-07-01 20:36:31,693 main.py:52] epoch 1630, training loss: 7901.08, average training loss: 8230.64, base loss: 15397.79
[INFO 2017-07-01 20:36:35,865 main.py:52] epoch 1631, training loss: 7067.91, average training loss: 8229.97, base loss: 15396.03
[INFO 2017-07-01 20:36:40,035 main.py:52] epoch 1632, training loss: 8204.38, average training loss: 8230.86, base loss: 15397.20
[INFO 2017-07-01 20:36:44,236 main.py:52] epoch 1633, training loss: 7872.31, average training loss: 8230.56, base loss: 15398.09
[INFO 2017-07-01 20:36:48,459 main.py:52] epoch 1634, training loss: 9464.48, average training loss: 8231.30, base loss: 15399.11
[INFO 2017-07-01 20:36:52,647 main.py:52] epoch 1635, training loss: 7988.83, average training loss: 8230.63, base loss: 15398.53
[INFO 2017-07-01 20:36:56,839 main.py:52] epoch 1636, training loss: 8270.48, average training loss: 8229.53, base loss: 15398.63
[INFO 2017-07-01 20:37:00,977 main.py:52] epoch 1637, training loss: 8908.55, average training loss: 8229.99, base loss: 15400.25
[INFO 2017-07-01 20:37:05,147 main.py:52] epoch 1638, training loss: 7693.38, average training loss: 8228.95, base loss: 15399.85
[INFO 2017-07-01 20:37:09,303 main.py:52] epoch 1639, training loss: 8351.70, average training loss: 8228.98, base loss: 15400.77
[INFO 2017-07-01 20:37:13,507 main.py:52] epoch 1640, training loss: 8677.97, average training loss: 8229.86, base loss: 15400.26
[INFO 2017-07-01 20:37:17,705 main.py:52] epoch 1641, training loss: 7838.50, average training loss: 8227.61, base loss: 15398.96
[INFO 2017-07-01 20:37:21,888 main.py:52] epoch 1642, training loss: 8491.74, average training loss: 8228.00, base loss: 15398.12
[INFO 2017-07-01 20:37:26,056 main.py:52] epoch 1643, training loss: 8462.53, average training loss: 8228.17, base loss: 15397.20
[INFO 2017-07-01 20:37:30,170 main.py:52] epoch 1644, training loss: 7738.24, average training loss: 8226.59, base loss: 15397.16
[INFO 2017-07-01 20:37:34,300 main.py:52] epoch 1645, training loss: 8725.75, average training loss: 8227.67, base loss: 15398.40
[INFO 2017-07-01 20:37:38,456 main.py:52] epoch 1646, training loss: 8530.08, average training loss: 8227.24, base loss: 15399.00
[INFO 2017-07-01 20:37:42,579 main.py:52] epoch 1647, training loss: 8178.37, average training loss: 8227.98, base loss: 15398.68
[INFO 2017-07-01 20:37:46,760 main.py:52] epoch 1648, training loss: 8120.38, average training loss: 8227.48, base loss: 15398.63
[INFO 2017-07-01 20:37:51,003 main.py:52] epoch 1649, training loss: 7745.29, average training loss: 8225.50, base loss: 15398.75
[INFO 2017-07-01 20:37:55,177 main.py:52] epoch 1650, training loss: 8309.33, average training loss: 8225.05, base loss: 15398.87
[INFO 2017-07-01 20:37:59,371 main.py:52] epoch 1651, training loss: 8196.10, average training loss: 8224.93, base loss: 15400.39
[INFO 2017-07-01 20:38:03,665 main.py:52] epoch 1652, training loss: 8248.60, average training loss: 8225.15, base loss: 15399.90
[INFO 2017-07-01 20:38:07,855 main.py:52] epoch 1653, training loss: 8642.83, average training loss: 8225.04, base loss: 15399.68
[INFO 2017-07-01 20:38:12,008 main.py:52] epoch 1654, training loss: 7036.54, average training loss: 8223.56, base loss: 15399.34
[INFO 2017-07-01 20:38:16,230 main.py:52] epoch 1655, training loss: 7642.25, average training loss: 8222.26, base loss: 15400.60
[INFO 2017-07-01 20:38:20,412 main.py:52] epoch 1656, training loss: 8820.69, average training loss: 8222.41, base loss: 15402.05
[INFO 2017-07-01 20:38:24,586 main.py:52] epoch 1657, training loss: 7517.31, average training loss: 8221.80, base loss: 15400.65
[INFO 2017-07-01 20:38:28,697 main.py:52] epoch 1658, training loss: 7911.67, average training loss: 8221.55, base loss: 15399.48
[INFO 2017-07-01 20:38:32,785 main.py:52] epoch 1659, training loss: 7418.80, average training loss: 8220.86, base loss: 15398.57
[INFO 2017-07-01 20:38:36,936 main.py:52] epoch 1660, training loss: 7666.59, average training loss: 8219.80, base loss: 15396.74
[INFO 2017-07-01 20:38:41,132 main.py:52] epoch 1661, training loss: 7907.98, average training loss: 8219.81, base loss: 15396.09
[INFO 2017-07-01 20:38:45,332 main.py:52] epoch 1662, training loss: 8820.83, average training loss: 8218.65, base loss: 15396.73
[INFO 2017-07-01 20:38:49,468 main.py:52] epoch 1663, training loss: 8568.32, average training loss: 8218.51, base loss: 15396.09
[INFO 2017-07-01 20:38:53,680 main.py:52] epoch 1664, training loss: 7440.52, average training loss: 8217.78, base loss: 15394.29
[INFO 2017-07-01 20:38:57,804 main.py:52] epoch 1665, training loss: 7890.42, average training loss: 8218.08, base loss: 15394.34
[INFO 2017-07-01 20:39:01,994 main.py:52] epoch 1666, training loss: 7541.57, average training loss: 8217.79, base loss: 15392.43
[INFO 2017-07-01 20:39:06,149 main.py:52] epoch 1667, training loss: 7419.50, average training loss: 8215.42, base loss: 15391.42
[INFO 2017-07-01 20:39:10,292 main.py:52] epoch 1668, training loss: 7483.69, average training loss: 8213.99, base loss: 15390.52
[INFO 2017-07-01 20:39:14,391 main.py:52] epoch 1669, training loss: 7993.78, average training loss: 8213.26, base loss: 15391.28
[INFO 2017-07-01 20:39:18,598 main.py:52] epoch 1670, training loss: 7834.37, average training loss: 8212.22, base loss: 15390.11
[INFO 2017-07-01 20:39:22,803 main.py:52] epoch 1671, training loss: 7448.82, average training loss: 8210.90, base loss: 15389.20
[INFO 2017-07-01 20:39:27,081 main.py:52] epoch 1672, training loss: 8667.27, average training loss: 8211.15, base loss: 15390.53
[INFO 2017-07-01 20:39:31,292 main.py:52] epoch 1673, training loss: 7312.18, average training loss: 8210.71, base loss: 15390.32
[INFO 2017-07-01 20:39:35,426 main.py:52] epoch 1674, training loss: 7866.89, average training loss: 8210.13, base loss: 15389.71
[INFO 2017-07-01 20:39:39,581 main.py:52] epoch 1675, training loss: 8650.37, average training loss: 8210.30, base loss: 15390.28
[INFO 2017-07-01 20:39:43,730 main.py:52] epoch 1676, training loss: 8372.24, average training loss: 8211.16, base loss: 15390.64
[INFO 2017-07-01 20:39:47,944 main.py:52] epoch 1677, training loss: 8491.60, average training loss: 8210.75, base loss: 15391.66
[INFO 2017-07-01 20:39:52,095 main.py:52] epoch 1678, training loss: 7585.15, average training loss: 8210.53, base loss: 15390.66
[INFO 2017-07-01 20:39:56,321 main.py:52] epoch 1679, training loss: 7452.60, average training loss: 8209.71, base loss: 15388.71
[INFO 2017-07-01 20:40:00,472 main.py:52] epoch 1680, training loss: 8228.60, average training loss: 8210.13, base loss: 15388.34
[INFO 2017-07-01 20:40:04,541 main.py:52] epoch 1681, training loss: 7317.52, average training loss: 8209.63, base loss: 15386.19
[INFO 2017-07-01 20:40:08,779 main.py:52] epoch 1682, training loss: 8113.42, average training loss: 8209.36, base loss: 15387.91
[INFO 2017-07-01 20:40:12,898 main.py:52] epoch 1683, training loss: 8820.46, average training loss: 8210.90, base loss: 15389.62
[INFO 2017-07-01 20:40:17,034 main.py:52] epoch 1684, training loss: 7502.12, average training loss: 8209.64, base loss: 15389.14
[INFO 2017-07-01 20:40:21,199 main.py:52] epoch 1685, training loss: 8238.07, average training loss: 8208.81, base loss: 15388.41
[INFO 2017-07-01 20:40:25,364 main.py:52] epoch 1686, training loss: 7638.66, average training loss: 8208.34, base loss: 15387.92
[INFO 2017-07-01 20:40:29,628 main.py:52] epoch 1687, training loss: 8265.80, average training loss: 8207.96, base loss: 15388.67
[INFO 2017-07-01 20:40:33,817 main.py:52] epoch 1688, training loss: 7713.43, average training loss: 8207.29, base loss: 15387.54
[INFO 2017-07-01 20:40:38,052 main.py:52] epoch 1689, training loss: 8070.07, average training loss: 8207.41, base loss: 15387.96
[INFO 2017-07-01 20:40:42,234 main.py:52] epoch 1690, training loss: 8672.44, average training loss: 8207.09, base loss: 15389.55
[INFO 2017-07-01 20:40:46,340 main.py:52] epoch 1691, training loss: 8201.01, average training loss: 8207.58, base loss: 15390.31
[INFO 2017-07-01 20:40:50,544 main.py:52] epoch 1692, training loss: 7715.56, average training loss: 8206.36, base loss: 15389.89
[INFO 2017-07-01 20:40:54,778 main.py:52] epoch 1693, training loss: 6683.16, average training loss: 8204.43, base loss: 15387.89
[INFO 2017-07-01 20:40:59,010 main.py:52] epoch 1694, training loss: 9308.67, average training loss: 8205.80, base loss: 15389.55
[INFO 2017-07-01 20:41:03,161 main.py:52] epoch 1695, training loss: 7740.37, average training loss: 8205.59, base loss: 15389.25
[INFO 2017-07-01 20:41:07,308 main.py:52] epoch 1696, training loss: 7764.83, average training loss: 8205.28, base loss: 15389.79
[INFO 2017-07-01 20:41:11,448 main.py:52] epoch 1697, training loss: 8394.55, average training loss: 8205.77, base loss: 15390.31
[INFO 2017-07-01 20:41:15,562 main.py:52] epoch 1698, training loss: 8310.99, average training loss: 8205.78, base loss: 15390.62
[INFO 2017-07-01 20:41:19,737 main.py:52] epoch 1699, training loss: 8658.15, average training loss: 8206.20, base loss: 15391.56
[INFO 2017-07-01 20:41:19,738 main.py:54] epoch 1699, testing
[INFO 2017-07-01 20:41:19,738 main.py:61] model save to ./model/final.pth
[INFO 2017-07-01 20:41:23,965 main.py:52] epoch 1700, training loss: 8211.24, average training loss: 8206.82, base loss: 15392.43
[INFO 2017-07-01 20:41:28,195 main.py:52] epoch 1701, training loss: 7498.62, average training loss: 8205.69, base loss: 15392.33
[INFO 2017-07-01 20:41:32,423 main.py:52] epoch 1702, training loss: 7814.02, average training loss: 8205.75, base loss: 15391.37
[INFO 2017-07-01 20:41:36,636 main.py:52] epoch 1703, training loss: 8117.26, average training loss: 8206.24, base loss: 15391.04
[INFO 2017-07-01 20:41:40,775 main.py:52] epoch 1704, training loss: 7892.59, average training loss: 8205.43, base loss: 15390.59
[INFO 2017-07-01 20:41:44,943 main.py:52] epoch 1705, training loss: 7504.99, average training loss: 8205.36, base loss: 15389.79
[INFO 2017-07-01 20:41:49,068 main.py:52] epoch 1706, training loss: 8083.40, average training loss: 8204.26, base loss: 15389.27
[INFO 2017-07-01 20:41:53,286 main.py:52] epoch 1707, training loss: 7537.65, average training loss: 8203.06, base loss: 15388.49
[INFO 2017-07-01 20:41:57,419 main.py:52] epoch 1708, training loss: 8798.35, average training loss: 8203.31, base loss: 15390.45
[INFO 2017-07-01 20:42:01,637 main.py:52] epoch 1709, training loss: 8553.09, average training loss: 8203.44, base loss: 15391.28
[INFO 2017-07-01 20:42:05,779 main.py:52] epoch 1710, training loss: 7826.33, average training loss: 8202.15, base loss: 15389.60
[INFO 2017-07-01 20:42:09,952 main.py:52] epoch 1711, training loss: 7665.87, average training loss: 8200.88, base loss: 15388.67
[INFO 2017-07-01 20:42:14,069 main.py:52] epoch 1712, training loss: 8026.63, average training loss: 8200.05, base loss: 15387.56
[INFO 2017-07-01 20:42:18,235 main.py:52] epoch 1713, training loss: 9618.39, average training loss: 8202.35, base loss: 15388.14
[INFO 2017-07-01 20:42:22,457 main.py:52] epoch 1714, training loss: 7004.99, average training loss: 8200.33, base loss: 15386.52
[INFO 2017-07-01 20:42:26,601 main.py:52] epoch 1715, training loss: 7213.77, average training loss: 8199.40, base loss: 15385.93
[INFO 2017-07-01 20:42:30,792 main.py:52] epoch 1716, training loss: 8521.22, average training loss: 8200.08, base loss: 15386.73
[INFO 2017-07-01 20:42:35,035 main.py:52] epoch 1717, training loss: 7752.54, average training loss: 8198.97, base loss: 15386.89
[INFO 2017-07-01 20:42:39,192 main.py:52] epoch 1718, training loss: 8119.35, average training loss: 8198.71, base loss: 15387.87
[INFO 2017-07-01 20:42:43,366 main.py:52] epoch 1719, training loss: 7475.02, average training loss: 8198.65, base loss: 15388.30
[INFO 2017-07-01 20:42:47,629 main.py:52] epoch 1720, training loss: 8493.75, average training loss: 8198.24, base loss: 15390.50
[INFO 2017-07-01 20:42:51,797 main.py:52] epoch 1721, training loss: 8650.50, average training loss: 8199.20, base loss: 15392.04
[INFO 2017-07-01 20:42:56,020 main.py:52] epoch 1722, training loss: 7624.40, average training loss: 8198.42, base loss: 15391.30
[INFO 2017-07-01 20:43:00,173 main.py:52] epoch 1723, training loss: 7473.77, average training loss: 8196.56, base loss: 15389.32
[INFO 2017-07-01 20:43:04,345 main.py:52] epoch 1724, training loss: 8631.02, average training loss: 8197.48, base loss: 15390.93
[INFO 2017-07-01 20:43:08,447 main.py:52] epoch 1725, training loss: 7794.15, average training loss: 8196.81, base loss: 15391.81
[INFO 2017-07-01 20:43:12,646 main.py:52] epoch 1726, training loss: 8052.01, average training loss: 8196.26, base loss: 15393.77
[INFO 2017-07-01 20:43:16,830 main.py:52] epoch 1727, training loss: 7554.04, average training loss: 8195.48, base loss: 15393.68
[INFO 2017-07-01 20:43:21,001 main.py:52] epoch 1728, training loss: 7486.61, average training loss: 8194.14, base loss: 15393.53
[INFO 2017-07-01 20:43:25,144 main.py:52] epoch 1729, training loss: 7792.60, average training loss: 8192.15, base loss: 15393.26
[INFO 2017-07-01 20:43:29,361 main.py:52] epoch 1730, training loss: 7594.27, average training loss: 8190.81, base loss: 15393.42
[INFO 2017-07-01 20:43:33,467 main.py:52] epoch 1731, training loss: 8042.82, average training loss: 8189.33, base loss: 15394.08
[INFO 2017-07-01 20:43:37,654 main.py:52] epoch 1732, training loss: 8199.19, average training loss: 8188.80, base loss: 15394.47
[INFO 2017-07-01 20:43:41,766 main.py:52] epoch 1733, training loss: 8323.17, average training loss: 8187.47, base loss: 15394.16
[INFO 2017-07-01 20:43:45,972 main.py:52] epoch 1734, training loss: 7330.46, average training loss: 8185.18, base loss: 15393.73
[INFO 2017-07-01 20:43:50,212 main.py:52] epoch 1735, training loss: 7881.84, average training loss: 8185.36, base loss: 15393.97
[INFO 2017-07-01 20:43:54,342 main.py:52] epoch 1736, training loss: 7884.81, average training loss: 8184.79, base loss: 15394.46
[INFO 2017-07-01 20:43:58,527 main.py:52] epoch 1737, training loss: 7714.73, average training loss: 8184.10, base loss: 15394.29
[INFO 2017-07-01 20:44:02,663 main.py:52] epoch 1738, training loss: 7564.49, average training loss: 8183.99, base loss: 15395.07
[INFO 2017-07-01 20:44:06,819 main.py:52] epoch 1739, training loss: 8693.22, average training loss: 8183.85, base loss: 15396.35
[INFO 2017-07-01 20:44:10,988 main.py:52] epoch 1740, training loss: 7637.73, average training loss: 8182.47, base loss: 15396.93
[INFO 2017-07-01 20:44:15,222 main.py:52] epoch 1741, training loss: 7525.41, average training loss: 8182.46, base loss: 15396.76
[INFO 2017-07-01 20:44:19,406 main.py:52] epoch 1742, training loss: 7757.36, average training loss: 8182.00, base loss: 15396.63
[INFO 2017-07-01 20:44:23,588 main.py:52] epoch 1743, training loss: 7326.14, average training loss: 8180.61, base loss: 15395.59
[INFO 2017-07-01 20:44:27,797 main.py:52] epoch 1744, training loss: 7608.61, average training loss: 8178.21, base loss: 15394.70
[INFO 2017-07-01 20:44:31,942 main.py:52] epoch 1745, training loss: 7242.00, average training loss: 8175.89, base loss: 15393.18
[INFO 2017-07-01 20:44:36,087 main.py:52] epoch 1746, training loss: 7758.70, average training loss: 8176.12, base loss: 15392.42
[INFO 2017-07-01 20:44:40,215 main.py:52] epoch 1747, training loss: 8579.78, average training loss: 8176.48, base loss: 15391.73
[INFO 2017-07-01 20:44:44,381 main.py:52] epoch 1748, training loss: 8601.03, average training loss: 8177.17, base loss: 15392.14
[INFO 2017-07-01 20:44:48,585 main.py:52] epoch 1749, training loss: 8041.84, average training loss: 8176.17, base loss: 15391.11
[INFO 2017-07-01 20:44:52,860 main.py:52] epoch 1750, training loss: 8610.87, average training loss: 8175.68, base loss: 15392.09
[INFO 2017-07-01 20:44:57,075 main.py:52] epoch 1751, training loss: 7370.62, average training loss: 8176.33, base loss: 15391.13
[INFO 2017-07-01 20:45:01,287 main.py:52] epoch 1752, training loss: 8434.43, average training loss: 8176.63, base loss: 15392.21
[INFO 2017-07-01 20:45:05,539 main.py:52] epoch 1753, training loss: 8510.76, average training loss: 8176.75, base loss: 15392.34
[INFO 2017-07-01 20:45:09,694 main.py:52] epoch 1754, training loss: 8920.07, average training loss: 8177.35, base loss: 15394.51
[INFO 2017-07-01 20:45:13,913 main.py:52] epoch 1755, training loss: 8165.55, average training loss: 8177.75, base loss: 15395.64
[INFO 2017-07-01 20:45:18,054 main.py:52] epoch 1756, training loss: 8534.90, average training loss: 8178.92, base loss: 15395.09
[INFO 2017-07-01 20:45:22,200 main.py:52] epoch 1757, training loss: 7668.61, average training loss: 8179.20, base loss: 15394.81
[INFO 2017-07-01 20:45:26,379 main.py:52] epoch 1758, training loss: 8678.88, average training loss: 8179.32, base loss: 15397.04
[INFO 2017-07-01 20:45:30,574 main.py:52] epoch 1759, training loss: 8433.57, average training loss: 8179.52, base loss: 15398.04
[INFO 2017-07-01 20:45:34,763 main.py:52] epoch 1760, training loss: 7736.74, average training loss: 8178.75, base loss: 15397.54
[INFO 2017-07-01 20:45:38,990 main.py:52] epoch 1761, training loss: 7927.58, average training loss: 8178.33, base loss: 15396.30
[INFO 2017-07-01 20:45:43,140 main.py:52] epoch 1762, training loss: 8522.43, average training loss: 8178.78, base loss: 15397.68
[INFO 2017-07-01 20:45:47,296 main.py:52] epoch 1763, training loss: 7186.96, average training loss: 8178.45, base loss: 15397.40
[INFO 2017-07-01 20:45:51,431 main.py:52] epoch 1764, training loss: 7280.16, average training loss: 8177.43, base loss: 15397.92
[INFO 2017-07-01 20:45:55,604 main.py:52] epoch 1765, training loss: 7716.62, average training loss: 8177.39, base loss: 15397.93
[INFO 2017-07-01 20:45:59,714 main.py:52] epoch 1766, training loss: 8374.91, average training loss: 8177.18, base loss: 15398.79
[INFO 2017-07-01 20:46:03,957 main.py:52] epoch 1767, training loss: 7366.51, average training loss: 8175.08, base loss: 15398.17
[INFO 2017-07-01 20:46:08,079 main.py:52] epoch 1768, training loss: 8168.85, average training loss: 8174.43, base loss: 15398.15
[INFO 2017-07-01 20:46:12,181 main.py:52] epoch 1769, training loss: 8530.71, average training loss: 8175.38, base loss: 15398.93
[INFO 2017-07-01 20:46:16,397 main.py:52] epoch 1770, training loss: 8280.76, average training loss: 8175.61, base loss: 15398.43
[INFO 2017-07-01 20:46:20,629 main.py:52] epoch 1771, training loss: 7382.45, average training loss: 8174.44, base loss: 15397.50
[INFO 2017-07-01 20:46:24,800 main.py:52] epoch 1772, training loss: 7813.03, average training loss: 8174.68, base loss: 15396.34
[INFO 2017-07-01 20:46:29,022 main.py:52] epoch 1773, training loss: 7874.25, average training loss: 8173.85, base loss: 15395.93
[INFO 2017-07-01 20:46:33,162 main.py:52] epoch 1774, training loss: 8397.65, average training loss: 8174.21, base loss: 15395.61
[INFO 2017-07-01 20:46:37,350 main.py:52] epoch 1775, training loss: 8693.82, average training loss: 8172.81, base loss: 15396.45
[INFO 2017-07-01 20:46:41,469 main.py:52] epoch 1776, training loss: 8225.44, average training loss: 8172.11, base loss: 15397.55
[INFO 2017-07-01 20:46:45,621 main.py:52] epoch 1777, training loss: 8411.31, average training loss: 8171.77, base loss: 15397.45
[INFO 2017-07-01 20:46:49,822 main.py:52] epoch 1778, training loss: 8201.98, average training loss: 8171.45, base loss: 15398.02
[INFO 2017-07-01 20:46:54,069 main.py:52] epoch 1779, training loss: 9789.02, average training loss: 8173.07, base loss: 15400.01
[INFO 2017-07-01 20:46:58,274 main.py:52] epoch 1780, training loss: 8944.79, average training loss: 8174.26, base loss: 15401.66
[INFO 2017-07-01 20:47:02,418 main.py:52] epoch 1781, training loss: 7900.89, average training loss: 8173.45, base loss: 15402.09
[INFO 2017-07-01 20:47:06,598 main.py:52] epoch 1782, training loss: 8079.01, average training loss: 8171.88, base loss: 15400.75
[INFO 2017-07-01 20:47:10,750 main.py:52] epoch 1783, training loss: 6915.45, average training loss: 8169.66, base loss: 15397.73
[INFO 2017-07-01 20:47:14,936 main.py:52] epoch 1784, training loss: 7774.64, average training loss: 8168.41, base loss: 15397.36
[INFO 2017-07-01 20:47:19,108 main.py:52] epoch 1785, training loss: 7059.24, average training loss: 8166.81, base loss: 15396.11
[INFO 2017-07-01 20:47:23,269 main.py:52] epoch 1786, training loss: 8450.28, average training loss: 8167.12, base loss: 15396.02
[INFO 2017-07-01 20:47:27,488 main.py:52] epoch 1787, training loss: 9488.83, average training loss: 8168.60, base loss: 15397.23
[INFO 2017-07-01 20:47:31,587 main.py:52] epoch 1788, training loss: 8548.06, average training loss: 8169.80, base loss: 15397.36
[INFO 2017-07-01 20:47:35,749 main.py:52] epoch 1789, training loss: 8389.95, average training loss: 8170.22, base loss: 15396.94
[INFO 2017-07-01 20:47:39,934 main.py:52] epoch 1790, training loss: 8108.46, average training loss: 8168.96, base loss: 15397.22
[INFO 2017-07-01 20:47:44,156 main.py:52] epoch 1791, training loss: 9133.31, average training loss: 8170.20, base loss: 15399.84
[INFO 2017-07-01 20:47:48,304 main.py:52] epoch 1792, training loss: 7875.39, average training loss: 8169.67, base loss: 15399.02
[INFO 2017-07-01 20:47:52,488 main.py:52] epoch 1793, training loss: 8227.68, average training loss: 8170.16, base loss: 15398.08
[INFO 2017-07-01 20:47:56,661 main.py:52] epoch 1794, training loss: 7118.81, average training loss: 8168.44, base loss: 15397.84
[INFO 2017-07-01 20:48:00,833 main.py:52] epoch 1795, training loss: 8187.87, average training loss: 8167.96, base loss: 15399.05
[INFO 2017-07-01 20:48:04,986 main.py:52] epoch 1796, training loss: 8454.81, average training loss: 8166.95, base loss: 15400.70
[INFO 2017-07-01 20:48:09,205 main.py:52] epoch 1797, training loss: 7317.28, average training loss: 8166.13, base loss: 15400.14
[INFO 2017-07-01 20:48:13,362 main.py:52] epoch 1798, training loss: 6726.92, average training loss: 8163.92, base loss: 15396.62
[INFO 2017-07-01 20:48:17,588 main.py:52] epoch 1799, training loss: 7711.04, average training loss: 8162.90, base loss: 15395.45
[INFO 2017-07-01 20:48:17,588 main.py:54] epoch 1799, testing
[INFO 2017-07-01 20:48:17,589 main.py:61] model save to ./model/final.pth
[INFO 2017-07-01 20:48:21,802 main.py:52] epoch 1800, training loss: 7479.06, average training loss: 8162.96, base loss: 15395.16
[INFO 2017-07-01 20:48:25,999 main.py:52] epoch 1801, training loss: 8451.06, average training loss: 8161.59, base loss: 15395.18
[INFO 2017-07-01 20:48:30,181 main.py:52] epoch 1802, training loss: 6536.46, average training loss: 8158.87, base loss: 15393.29
[INFO 2017-07-01 20:48:34,379 main.py:52] epoch 1803, training loss: 7902.92, average training loss: 8158.82, base loss: 15393.08
[INFO 2017-07-01 20:48:38,543 main.py:52] epoch 1804, training loss: 8209.87, average training loss: 8158.47, base loss: 15393.17
[INFO 2017-07-01 20:48:42,671 main.py:52] epoch 1805, training loss: 7431.31, average training loss: 8158.12, base loss: 15392.97
[INFO 2017-07-01 20:48:46,842 main.py:52] epoch 1806, training loss: 8068.08, average training loss: 8157.32, base loss: 15393.36
[INFO 2017-07-01 20:48:51,081 main.py:52] epoch 1807, training loss: 9052.21, average training loss: 8156.88, base loss: 15394.13
[INFO 2017-07-01 20:48:55,258 main.py:52] epoch 1808, training loss: 8205.29, average training loss: 8156.75, base loss: 15394.18
[INFO 2017-07-01 20:48:59,479 main.py:52] epoch 1809, training loss: 8976.86, average training loss: 8156.32, base loss: 15395.11
[INFO 2017-07-01 20:49:03,581 main.py:52] epoch 1810, training loss: 7715.70, average training loss: 8156.65, base loss: 15393.93
[INFO 2017-07-01 20:49:07,738 main.py:52] epoch 1811, training loss: 7192.38, average training loss: 8154.50, base loss: 15393.23
[INFO 2017-07-01 20:49:12,063 main.py:52] epoch 1812, training loss: 8267.09, average training loss: 8154.68, base loss: 15393.28
[INFO 2017-07-01 20:49:16,226 main.py:52] epoch 1813, training loss: 7946.78, average training loss: 8155.04, base loss: 15392.63
[INFO 2017-07-01 20:49:20,451 main.py:52] epoch 1814, training loss: 7713.33, average training loss: 8154.25, base loss: 15391.97
[INFO 2017-07-01 20:49:24,750 main.py:52] epoch 1815, training loss: 7769.37, average training loss: 8154.27, base loss: 15391.50
[INFO 2017-07-01 20:49:28,986 main.py:52] epoch 1816, training loss: 8945.74, average training loss: 8155.73, base loss: 15392.32
[INFO 2017-07-01 20:49:33,140 main.py:52] epoch 1817, training loss: 8296.85, average training loss: 8156.40, base loss: 15391.26
[INFO 2017-07-01 20:49:37,323 main.py:52] epoch 1818, training loss: 8388.72, average training loss: 8156.30, base loss: 15391.46
[INFO 2017-07-01 20:49:41,449 main.py:52] epoch 1819, training loss: 8712.81, average training loss: 8156.56, base loss: 15393.47
[INFO 2017-07-01 20:49:45,578 main.py:52] epoch 1820, training loss: 7108.92, average training loss: 8155.89, base loss: 15391.88
[INFO 2017-07-01 20:49:49,821 main.py:52] epoch 1821, training loss: 8357.85, average training loss: 8155.40, base loss: 15390.88
[INFO 2017-07-01 20:49:53,987 main.py:52] epoch 1822, training loss: 8406.63, average training loss: 8155.29, base loss: 15392.50
[INFO 2017-07-01 20:49:58,118 main.py:52] epoch 1823, training loss: 8077.17, average training loss: 8154.78, base loss: 15393.31
[INFO 2017-07-01 20:50:02,406 main.py:52] epoch 1824, training loss: 7098.07, average training loss: 8154.05, base loss: 15391.42
[INFO 2017-07-01 20:50:06,575 main.py:52] epoch 1825, training loss: 7890.38, average training loss: 8154.54, base loss: 15390.15
[INFO 2017-07-01 20:50:10,761 main.py:52] epoch 1826, training loss: 7983.41, average training loss: 8153.12, base loss: 15390.64
[INFO 2017-07-01 20:50:14,909 main.py:52] epoch 1827, training loss: 7318.22, average training loss: 8150.53, base loss: 15388.96
[INFO 2017-07-01 20:50:19,098 main.py:52] epoch 1828, training loss: 8705.51, average training loss: 8150.86, base loss: 15389.54
[INFO 2017-07-01 20:50:23,338 main.py:52] epoch 1829, training loss: 7223.05, average training loss: 8149.61, base loss: 15388.07
[INFO 2017-07-01 20:50:27,488 main.py:52] epoch 1830, training loss: 8084.46, average training loss: 8148.23, base loss: 15389.52
[INFO 2017-07-01 20:50:31,686 main.py:52] epoch 1831, training loss: 7506.51, average training loss: 8147.42, base loss: 15389.37
[INFO 2017-07-01 20:50:35,837 main.py:52] epoch 1832, training loss: 7857.57, average training loss: 8147.09, base loss: 15389.59
[INFO 2017-07-01 20:50:39,993 main.py:52] epoch 1833, training loss: 7983.82, average training loss: 8147.40, base loss: 15390.90
[INFO 2017-07-01 20:50:44,124 main.py:52] epoch 1834, training loss: 8156.62, average training loss: 8147.74, base loss: 15390.55
[INFO 2017-07-01 20:50:48,371 main.py:52] epoch 1835, training loss: 7885.02, average training loss: 8147.25, base loss: 15389.56
[INFO 2017-07-01 20:50:52,510 main.py:52] epoch 1836, training loss: 7405.90, average training loss: 8146.39, base loss: 15388.48
[INFO 2017-07-01 20:50:56,643 main.py:52] epoch 1837, training loss: 8069.91, average training loss: 8146.40, base loss: 15388.79
[INFO 2017-07-01 20:51:00,803 main.py:52] epoch 1838, training loss: 7973.27, average training loss: 8146.02, base loss: 15388.32
[INFO 2017-07-01 20:51:04,977 main.py:52] epoch 1839, training loss: 8586.98, average training loss: 8146.09, base loss: 15386.91
[INFO 2017-07-01 20:51:09,183 main.py:52] epoch 1840, training loss: 8178.86, average training loss: 8145.63, base loss: 15386.83
[INFO 2017-07-01 20:51:13,316 main.py:52] epoch 1841, training loss: 8336.42, average training loss: 8144.78, base loss: 15386.63
[INFO 2017-07-01 20:51:17,392 main.py:52] epoch 1842, training loss: 8267.36, average training loss: 8145.03, base loss: 15386.19
[INFO 2017-07-01 20:51:21,478 main.py:52] epoch 1843, training loss: 7549.13, average training loss: 8144.81, base loss: 15383.71
[INFO 2017-07-01 20:51:25,643 main.py:52] epoch 1844, training loss: 7122.87, average training loss: 8144.30, base loss: 15381.77
[INFO 2017-07-01 20:51:29,810 main.py:52] epoch 1845, training loss: 8347.96, average training loss: 8143.50, base loss: 15382.34
[INFO 2017-07-01 20:51:33,940 main.py:52] epoch 1846, training loss: 8535.72, average training loss: 8142.90, base loss: 15384.26
[INFO 2017-07-01 20:51:38,121 main.py:52] epoch 1847, training loss: 8062.80, average training loss: 8142.47, base loss: 15383.47
[INFO 2017-07-01 20:51:42,300 main.py:52] epoch 1848, training loss: 9360.15, average training loss: 8143.55, base loss: 15384.45
[INFO 2017-07-01 20:51:46,507 main.py:52] epoch 1849, training loss: 8295.52, average training loss: 8143.87, base loss: 15385.73
[INFO 2017-07-01 20:51:50,752 main.py:52] epoch 1850, training loss: 8905.57, average training loss: 8144.57, base loss: 15386.99
[INFO 2017-07-01 20:51:54,882 main.py:52] epoch 1851, training loss: 8385.45, average training loss: 8144.55, base loss: 15387.66
[INFO 2017-07-01 20:51:59,101 main.py:52] epoch 1852, training loss: 7208.08, average training loss: 8143.90, base loss: 15387.09
[INFO 2017-07-01 20:52:03,283 main.py:52] epoch 1853, training loss: 8109.45, average training loss: 8143.10, base loss: 15387.84
[INFO 2017-07-01 20:52:07,517 main.py:52] epoch 1854, training loss: 7633.24, average training loss: 8143.31, base loss: 15387.08
[INFO 2017-07-01 20:52:11,727 main.py:52] epoch 1855, training loss: 7955.19, average training loss: 8142.16, base loss: 15387.52
[INFO 2017-07-01 20:52:15,960 main.py:52] epoch 1856, training loss: 8511.85, average training loss: 8142.44, base loss: 15388.96
[INFO 2017-07-01 20:52:20,141 main.py:52] epoch 1857, training loss: 8629.25, average training loss: 8142.99, base loss: 15390.01
[INFO 2017-07-01 20:52:24,307 main.py:52] epoch 1858, training loss: 9333.64, average training loss: 8143.68, base loss: 15391.89
[INFO 2017-07-01 20:52:28,449 main.py:52] epoch 1859, training loss: 8838.82, average training loss: 8144.54, base loss: 15393.26
[INFO 2017-07-01 20:52:32,602 main.py:52] epoch 1860, training loss: 9496.54, average training loss: 8146.26, base loss: 15395.89
[INFO 2017-07-01 20:52:36,858 main.py:52] epoch 1861, training loss: 8019.33, average training loss: 8146.11, base loss: 15395.42
[INFO 2017-07-01 20:52:41,079 main.py:52] epoch 1862, training loss: 7972.83, average training loss: 8145.61, base loss: 15395.75
[INFO 2017-07-01 20:52:45,246 main.py:52] epoch 1863, training loss: 7387.32, average training loss: 8144.92, base loss: 15395.66
[INFO 2017-07-01 20:52:49,485 main.py:52] epoch 1864, training loss: 7969.17, average training loss: 8144.24, base loss: 15395.55
[INFO 2017-07-01 20:52:53,730 main.py:52] epoch 1865, training loss: 7314.98, average training loss: 8143.59, base loss: 15395.15
[INFO 2017-07-01 20:52:57,910 main.py:52] epoch 1866, training loss: 7777.38, average training loss: 8142.91, base loss: 15396.16
[INFO 2017-07-01 20:53:02,146 main.py:52] epoch 1867, training loss: 7768.46, average training loss: 8143.06, base loss: 15396.46
[INFO 2017-07-01 20:53:06,307 main.py:52] epoch 1868, training loss: 8156.78, average training loss: 8142.64, base loss: 15396.95
[INFO 2017-07-01 20:53:10,483 main.py:52] epoch 1869, training loss: 8391.93, average training loss: 8143.62, base loss: 15396.01
[INFO 2017-07-01 20:53:14,693 main.py:52] epoch 1870, training loss: 7386.86, average training loss: 8143.07, base loss: 15395.25
[INFO 2017-07-01 20:53:18,890 main.py:52] epoch 1871, training loss: 8865.12, average training loss: 8144.39, base loss: 15395.79
[INFO 2017-07-01 20:53:23,119 main.py:52] epoch 1872, training loss: 7777.40, average training loss: 8144.24, base loss: 15396.47
[INFO 2017-07-01 20:53:27,302 main.py:52] epoch 1873, training loss: 7482.75, average training loss: 8142.90, base loss: 15396.27
[INFO 2017-07-01 20:53:31,514 main.py:52] epoch 1874, training loss: 7632.75, average training loss: 8142.04, base loss: 15395.60
[INFO 2017-07-01 20:53:35,693 main.py:52] epoch 1875, training loss: 8311.83, average training loss: 8141.08, base loss: 15395.90
[INFO 2017-07-01 20:53:39,797 main.py:52] epoch 1876, training loss: 8184.96, average training loss: 8141.30, base loss: 15396.81
[INFO 2017-07-01 20:53:43,918 main.py:52] epoch 1877, training loss: 7312.99, average training loss: 8139.36, base loss: 15395.81
[INFO 2017-07-01 20:53:48,064 main.py:52] epoch 1878, training loss: 7015.51, average training loss: 8137.49, base loss: 15394.67
[INFO 2017-07-01 20:53:52,255 main.py:52] epoch 1879, training loss: 7470.06, average training loss: 8136.81, base loss: 15394.89
[INFO 2017-07-01 20:53:56,459 main.py:52] epoch 1880, training loss: 8468.94, average training loss: 8136.66, base loss: 15395.74
[INFO 2017-07-01 20:54:00,641 main.py:52] epoch 1881, training loss: 7325.03, average training loss: 8136.51, base loss: 15395.07
[INFO 2017-07-01 20:54:04,787 main.py:52] epoch 1882, training loss: 7430.89, average training loss: 8135.11, base loss: 15394.36
[INFO 2017-07-01 20:54:08,972 main.py:52] epoch 1883, training loss: 7534.67, average training loss: 8132.78, base loss: 15394.14
[INFO 2017-07-01 20:54:13,227 main.py:52] epoch 1884, training loss: 8618.17, average training loss: 8132.47, base loss: 15395.51
[INFO 2017-07-01 20:54:17,398 main.py:52] epoch 1885, training loss: 7417.61, average training loss: 8131.43, base loss: 15394.31
[INFO 2017-07-01 20:54:21,575 main.py:52] epoch 1886, training loss: 7811.09, average training loss: 8130.99, base loss: 15392.85
[INFO 2017-07-01 20:54:25,791 main.py:52] epoch 1887, training loss: 8382.84, average training loss: 8130.75, base loss: 15394.08
[INFO 2017-07-01 20:54:29,959 main.py:52] epoch 1888, training loss: 7505.54, average training loss: 8130.40, base loss: 15393.09
[INFO 2017-07-01 20:54:34,090 main.py:52] epoch 1889, training loss: 8142.06, average training loss: 8130.99, base loss: 15393.25
[INFO 2017-07-01 20:54:38,212 main.py:52] epoch 1890, training loss: 6714.84, average training loss: 8129.00, base loss: 15391.02
[INFO 2017-07-01 20:54:42,374 main.py:52] epoch 1891, training loss: 8353.56, average training loss: 8128.44, base loss: 15392.32
[INFO 2017-07-01 20:54:46,534 main.py:52] epoch 1892, training loss: 7859.99, average training loss: 8128.85, base loss: 15392.18
[INFO 2017-07-01 20:54:50,652 main.py:52] epoch 1893, training loss: 7560.00, average training loss: 8127.92, base loss: 15392.59
[INFO 2017-07-01 20:54:54,801 main.py:52] epoch 1894, training loss: 7337.04, average training loss: 8126.90, base loss: 15392.07
[INFO 2017-07-01 20:54:58,937 main.py:52] epoch 1895, training loss: 8083.52, average training loss: 8125.67, base loss: 15392.62
[INFO 2017-07-01 20:55:03,131 main.py:52] epoch 1896, training loss: 8009.47, average training loss: 8125.63, base loss: 15392.79
[INFO 2017-07-01 20:55:07,298 main.py:52] epoch 1897, training loss: 7920.28, average training loss: 8124.91, base loss: 15392.83
[INFO 2017-07-01 20:55:11,483 main.py:52] epoch 1898, training loss: 7475.74, average training loss: 8124.35, base loss: 15392.47
[INFO 2017-07-01 20:55:15,626 main.py:52] epoch 1899, training loss: 7837.97, average training loss: 8123.79, base loss: 15392.74
[INFO 2017-07-01 20:55:15,627 main.py:54] epoch 1899, testing
[INFO 2017-07-01 20:55:15,627 main.py:61] model save to ./model/final.pth
[INFO 2017-07-01 20:55:19,929 main.py:52] epoch 1900, training loss: 7812.66, average training loss: 8124.65, base loss: 15393.28
[INFO 2017-07-01 20:55:24,091 main.py:52] epoch 1901, training loss: 7663.96, average training loss: 8124.40, base loss: 15394.03
[INFO 2017-07-01 20:55:28,230 main.py:52] epoch 1902, training loss: 7742.60, average training loss: 8123.56, base loss: 15392.76
[INFO 2017-07-01 20:55:32,389 main.py:52] epoch 1903, training loss: 7904.31, average training loss: 8122.83, base loss: 15393.34
[INFO 2017-07-01 20:55:36,661 main.py:52] epoch 1904, training loss: 8306.10, average training loss: 8122.64, base loss: 15393.36
[INFO 2017-07-01 20:55:40,845 main.py:52] epoch 1905, training loss: 7497.85, average training loss: 8122.68, base loss: 15393.18
[INFO 2017-07-01 20:55:45,054 main.py:52] epoch 1906, training loss: 9062.62, average training loss: 8124.22, base loss: 15394.19
[INFO 2017-07-01 20:55:49,279 main.py:52] epoch 1907, training loss: 6734.38, average training loss: 8122.12, base loss: 15392.43
[INFO 2017-07-01 20:55:53,439 main.py:52] epoch 1908, training loss: 8295.10, average training loss: 8122.37, base loss: 15392.22
[INFO 2017-07-01 20:55:57,660 main.py:52] epoch 1909, training loss: 7701.13, average training loss: 8120.79, base loss: 15391.39
[INFO 2017-07-01 20:56:01,859 main.py:52] epoch 1910, training loss: 8698.91, average training loss: 8121.30, base loss: 15392.79
[INFO 2017-07-01 20:56:06,043 main.py:52] epoch 1911, training loss: 7876.37, average training loss: 8120.65, base loss: 15393.10
[INFO 2017-07-01 20:56:10,186 main.py:52] epoch 1912, training loss: 7428.91, average training loss: 8119.38, base loss: 15391.84
[INFO 2017-07-01 20:56:14,403 main.py:52] epoch 1913, training loss: 7498.28, average training loss: 8119.27, base loss: 15390.16
[INFO 2017-07-01 20:56:18,578 main.py:52] epoch 1914, training loss: 9726.59, average training loss: 8121.10, base loss: 15391.31
[INFO 2017-07-01 20:56:22,707 main.py:52] epoch 1915, training loss: 8478.14, average training loss: 8121.32, base loss: 15391.07
[INFO 2017-07-01 20:56:26,866 main.py:52] epoch 1916, training loss: 8160.85, average training loss: 8121.55, base loss: 15391.19
[INFO 2017-07-01 20:56:31,017 main.py:52] epoch 1917, training loss: 7356.42, average training loss: 8120.46, base loss: 15389.67
[INFO 2017-07-01 20:56:35,258 main.py:52] epoch 1918, training loss: 7478.64, average training loss: 8119.17, base loss: 15388.62
[INFO 2017-07-01 20:56:39,461 main.py:52] epoch 1919, training loss: 8300.09, average training loss: 8119.95, base loss: 15388.18
[INFO 2017-07-01 20:56:43,661 main.py:52] epoch 1920, training loss: 7579.71, average training loss: 8119.49, base loss: 15387.41
[INFO 2017-07-01 20:56:47,811 main.py:52] epoch 1921, training loss: 8108.50, average training loss: 8118.67, base loss: 15386.19
[INFO 2017-07-01 20:56:51,928 main.py:52] epoch 1922, training loss: 7971.24, average training loss: 8117.72, base loss: 15384.68
[INFO 2017-07-01 20:56:56,033 main.py:52] epoch 1923, training loss: 8593.93, average training loss: 8118.08, base loss: 15384.72
[INFO 2017-07-01 20:57:00,150 main.py:52] epoch 1924, training loss: 8203.09, average training loss: 8117.55, base loss: 15383.83
[INFO 2017-07-01 20:57:04,309 main.py:52] epoch 1925, training loss: 8590.88, average training loss: 8117.25, base loss: 15383.61
[INFO 2017-07-01 20:57:08,407 main.py:52] epoch 1926, training loss: 8037.36, average training loss: 8116.70, base loss: 15384.78
[INFO 2017-07-01 20:57:12,587 main.py:52] epoch 1927, training loss: 7743.96, average training loss: 8116.68, base loss: 15385.11
[INFO 2017-07-01 20:57:16,817 main.py:52] epoch 1928, training loss: 7945.48, average training loss: 8116.19, base loss: 15384.63
[INFO 2017-07-01 20:57:20,983 main.py:52] epoch 1929, training loss: 8317.89, average training loss: 8116.23, base loss: 15384.06
[INFO 2017-07-01 20:57:25,124 main.py:52] epoch 1930, training loss: 8003.65, average training loss: 8115.11, base loss: 15383.23
[INFO 2017-07-01 20:57:29,227 main.py:52] epoch 1931, training loss: 7948.54, average training loss: 8114.66, base loss: 15383.01
[INFO 2017-07-01 20:57:33,332 main.py:52] epoch 1932, training loss: 7589.59, average training loss: 8113.83, base loss: 15381.97
[INFO 2017-07-01 20:57:37,490 main.py:52] epoch 1933, training loss: 8673.47, average training loss: 8115.10, base loss: 15382.45
[INFO 2017-07-01 20:57:41,649 main.py:52] epoch 1934, training loss: 8013.20, average training loss: 8114.21, base loss: 15381.14
[INFO 2017-07-01 20:57:45,814 main.py:52] epoch 1935, training loss: 7625.95, average training loss: 8114.60, base loss: 15379.66
[INFO 2017-07-01 20:57:50,016 main.py:52] epoch 1936, training loss: 7638.54, average training loss: 8113.73, base loss: 15378.27
[INFO 2017-07-01 20:57:54,227 main.py:52] epoch 1937, training loss: 7407.54, average training loss: 8113.11, base loss: 15376.43
[INFO 2017-07-01 20:57:58,381 main.py:52] epoch 1938, training loss: 7533.77, average training loss: 8112.76, base loss: 15377.55
[INFO 2017-07-01 20:58:02,527 main.py:52] epoch 1939, training loss: 7580.11, average training loss: 8111.23, base loss: 15377.68
[INFO 2017-07-01 20:58:06,634 main.py:52] epoch 1940, training loss: 7734.24, average training loss: 8110.79, base loss: 15377.29
[INFO 2017-07-01 20:58:10,791 main.py:52] epoch 1941, training loss: 8569.46, average training loss: 8110.65, base loss: 15377.87
[INFO 2017-07-01 20:58:14,991 main.py:52] epoch 1942, training loss: 9046.39, average training loss: 8111.53, base loss: 15379.10
[INFO 2017-07-01 20:58:19,146 main.py:52] epoch 1943, training loss: 8300.36, average training loss: 8112.46, base loss: 15380.61
[INFO 2017-07-01 20:58:23,328 main.py:52] epoch 1944, training loss: 7413.77, average training loss: 8112.29, base loss: 15381.15
[INFO 2017-07-01 20:58:27,510 main.py:52] epoch 1945, training loss: 8985.36, average training loss: 8111.58, base loss: 15382.39
[INFO 2017-07-01 20:58:31,627 main.py:52] epoch 1946, training loss: 8975.72, average training loss: 8112.03, base loss: 15383.17
[INFO 2017-07-01 20:58:35,825 main.py:52] epoch 1947, training loss: 8581.84, average training loss: 8111.68, base loss: 15383.08
[INFO 2017-07-01 20:58:40,014 main.py:52] epoch 1948, training loss: 7432.16, average training loss: 8111.62, base loss: 15383.29
[INFO 2017-07-01 20:58:44,193 main.py:52] epoch 1949, training loss: 9218.44, average training loss: 8112.64, base loss: 15384.91
[INFO 2017-07-01 20:58:48,319 main.py:52] epoch 1950, training loss: 8074.57, average training loss: 8113.07, base loss: 15384.90
[INFO 2017-07-01 20:58:52,469 main.py:52] epoch 1951, training loss: 7617.70, average training loss: 8113.72, base loss: 15383.89
[INFO 2017-07-01 20:58:56,628 main.py:52] epoch 1952, training loss: 7913.42, average training loss: 8113.39, base loss: 15382.97
[INFO 2017-07-01 20:59:00,797 main.py:52] epoch 1953, training loss: 8283.20, average training loss: 8113.31, base loss: 15384.47
[INFO 2017-07-01 20:59:04,951 main.py:52] epoch 1954, training loss: 8440.18, average training loss: 8114.20, base loss: 15385.16
[INFO 2017-07-01 20:59:09,110 main.py:52] epoch 1955, training loss: 8848.35, average training loss: 8115.88, base loss: 15386.13
[INFO 2017-07-01 20:59:13,272 main.py:52] epoch 1956, training loss: 7421.84, average training loss: 8114.51, base loss: 15384.12
[INFO 2017-07-01 20:59:17,478 main.py:52] epoch 1957, training loss: 8314.81, average training loss: 8113.88, base loss: 15383.03
[INFO 2017-07-01 20:59:21,665 main.py:52] epoch 1958, training loss: 7440.67, average training loss: 8112.39, base loss: 15381.67
[INFO 2017-07-01 20:59:25,827 main.py:52] epoch 1959, training loss: 7632.79, average training loss: 8111.98, base loss: 15380.17
[INFO 2017-07-01 20:59:30,042 main.py:52] epoch 1960, training loss: 6772.91, average training loss: 8110.77, base loss: 15378.29
[INFO 2017-07-01 20:59:34,195 main.py:52] epoch 1961, training loss: 7413.57, average training loss: 8109.75, base loss: 15376.78
[INFO 2017-07-01 20:59:38,325 main.py:52] epoch 1962, training loss: 7854.77, average training loss: 8109.67, base loss: 15376.75
[INFO 2017-07-01 20:59:42,529 main.py:52] epoch 1963, training loss: 7168.98, average training loss: 8108.88, base loss: 15374.60
[INFO 2017-07-01 20:59:46,711 main.py:52] epoch 1964, training loss: 8860.84, average training loss: 8108.78, base loss: 15376.01
[INFO 2017-07-01 20:59:50,864 main.py:52] epoch 1965, training loss: 7768.13, average training loss: 8108.09, base loss: 15375.14
[INFO 2017-07-01 20:59:55,046 main.py:52] epoch 1966, training loss: 7496.83, average training loss: 8107.53, base loss: 15373.59
[INFO 2017-07-01 20:59:59,234 main.py:52] epoch 1967, training loss: 8690.39, average training loss: 8108.30, base loss: 15373.67
[INFO 2017-07-01 21:00:03,314 main.py:52] epoch 1968, training loss: 8426.17, average training loss: 8109.07, base loss: 15372.95
[INFO 2017-07-01 21:00:07,456 main.py:52] epoch 1969, training loss: 8166.44, average training loss: 8108.19, base loss: 15372.90
[INFO 2017-07-01 21:00:11,685 main.py:52] epoch 1970, training loss: 6539.31, average training loss: 8107.69, base loss: 15372.06
[INFO 2017-07-01 21:00:15,890 main.py:52] epoch 1971, training loss: 8224.41, average training loss: 8108.27, base loss: 15372.82
[INFO 2017-07-01 21:00:20,001 main.py:52] epoch 1972, training loss: 7662.49, average training loss: 8108.02, base loss: 15372.31
[INFO 2017-07-01 21:00:24,188 main.py:52] epoch 1973, training loss: 7382.70, average training loss: 8107.57, base loss: 15371.53
[INFO 2017-07-01 21:00:28,344 main.py:52] epoch 1974, training loss: 7401.08, average training loss: 8107.57, base loss: 15370.51
[INFO 2017-07-01 21:00:32,511 main.py:52] epoch 1975, training loss: 7599.76, average training loss: 8106.74, base loss: 15371.06
[INFO 2017-07-01 21:00:36,773 main.py:52] epoch 1976, training loss: 7592.25, average training loss: 8106.68, base loss: 15370.91
[INFO 2017-07-01 21:00:41,016 main.py:52] epoch 1977, training loss: 8120.51, average training loss: 8107.29, base loss: 15371.68
[INFO 2017-07-01 21:00:45,148 main.py:52] epoch 1978, training loss: 8497.23, average training loss: 8107.26, base loss: 15372.61
[INFO 2017-07-01 21:00:49,297 main.py:52] epoch 1979, training loss: 7544.88, average training loss: 8106.56, base loss: 15372.55
[INFO 2017-07-01 21:00:53,433 main.py:52] epoch 1980, training loss: 7719.85, average training loss: 8106.49, base loss: 15371.55
[INFO 2017-07-01 21:00:57,558 main.py:52] epoch 1981, training loss: 8480.39, average training loss: 8106.77, base loss: 15372.16
[INFO 2017-07-01 21:01:01,734 main.py:52] epoch 1982, training loss: 7777.21, average training loss: 8105.82, base loss: 15372.84
[INFO 2017-07-01 21:01:05,948 main.py:52] epoch 1983, training loss: 8070.81, average training loss: 8106.53, base loss: 15373.50
[INFO 2017-07-01 21:01:10,139 main.py:52] epoch 1984, training loss: 8205.34, average training loss: 8106.45, base loss: 15373.24
[INFO 2017-07-01 21:01:14,270 main.py:52] epoch 1985, training loss: 8256.62, average training loss: 8106.20, base loss: 15374.09
[INFO 2017-07-01 21:01:18,466 main.py:52] epoch 1986, training loss: 7600.17, average training loss: 8106.16, base loss: 15373.75
[INFO 2017-07-01 21:01:22,616 main.py:52] epoch 1987, training loss: 7396.90, average training loss: 8105.48, base loss: 15373.42
[INFO 2017-07-01 21:01:26,730 main.py:52] epoch 1988, training loss: 7233.50, average training loss: 8103.37, base loss: 15371.64
[INFO 2017-07-01 21:01:30,896 main.py:52] epoch 1989, training loss: 8841.73, average training loss: 8104.53, base loss: 15372.27
[INFO 2017-07-01 21:01:35,176 main.py:52] epoch 1990, training loss: 7913.68, average training loss: 8103.88, base loss: 15371.72
[INFO 2017-07-01 21:01:39,310 main.py:52] epoch 1991, training loss: 7671.18, average training loss: 8104.39, base loss: 15370.59
[INFO 2017-07-01 21:01:43,498 main.py:52] epoch 1992, training loss: 7610.11, average training loss: 8103.41, base loss: 15369.94
[INFO 2017-07-01 21:01:47,651 main.py:52] epoch 1993, training loss: 10801.54, average training loss: 8105.63, base loss: 15372.94
[INFO 2017-07-01 21:01:51,832 main.py:52] epoch 1994, training loss: 6979.90, average training loss: 8103.73, base loss: 15371.87
[INFO 2017-07-01 21:01:55,947 main.py:52] epoch 1995, training loss: 7721.13, average training loss: 8103.34, base loss: 15371.70
[INFO 2017-07-01 21:02:00,125 main.py:52] epoch 1996, training loss: 7343.25, average training loss: 8101.48, base loss: 15371.75
[INFO 2017-07-01 21:02:04,243 main.py:52] epoch 1997, training loss: 7595.24, average training loss: 8101.14, base loss: 15372.22
[INFO 2017-07-01 21:02:08,440 main.py:52] epoch 1998, training loss: 7816.83, average training loss: 8100.65, base loss: 15371.66
[INFO 2017-07-01 21:02:12,628 main.py:52] epoch 1999, training loss: 8584.18, average training loss: 8101.62, base loss: 15371.24
[INFO 2017-07-01 21:02:12,628 main.py:54] epoch 1999, testing
[INFO 2017-07-01 21:02:12,628 main.py:61] model save to ./model/final.pth
[INFO 2017-07-01 21:02:16,824 main.py:52] epoch 2000, training loss: 7886.51, average training loss: 8100.87, base loss: 15370.63
[INFO 2017-07-01 21:02:20,945 main.py:52] epoch 2001, training loss: 8126.74, average training loss: 8101.30, base loss: 15369.71
[INFO 2017-07-01 21:02:25,129 main.py:52] epoch 2002, training loss: 7775.68, average training loss: 8101.39, base loss: 15370.22
[INFO 2017-07-01 21:02:29,293 main.py:52] epoch 2003, training loss: 8307.67, average training loss: 8101.00, base loss: 15371.50
[INFO 2017-07-01 21:02:33,501 main.py:52] epoch 2004, training loss: 6964.14, average training loss: 8100.85, base loss: 15369.92
[INFO 2017-07-01 21:02:37,634 main.py:52] epoch 2005, training loss: 7913.86, average training loss: 8099.32, base loss: 15369.67
[INFO 2017-07-01 21:02:41,785 main.py:52] epoch 2006, training loss: 7856.20, average training loss: 8098.74, base loss: 15369.72
[INFO 2017-07-01 21:02:45,933 main.py:52] epoch 2007, training loss: 8891.58, average training loss: 8099.77, base loss: 15371.41
[INFO 2017-07-01 21:02:50,116 main.py:52] epoch 2008, training loss: 7864.95, average training loss: 8099.64, base loss: 15371.16
[INFO 2017-07-01 21:02:54,316 main.py:52] epoch 2009, training loss: 7440.87, average training loss: 8098.07, base loss: 15370.39
[INFO 2017-07-01 21:02:58,594 main.py:52] epoch 2010, training loss: 7333.36, average training loss: 8096.88, base loss: 15370.57
[INFO 2017-07-01 21:03:02,774 main.py:52] epoch 2011, training loss: 7660.86, average training loss: 8096.45, base loss: 15371.57
[INFO 2017-07-01 21:03:06,953 main.py:52] epoch 2012, training loss: 8645.82, average training loss: 8096.54, base loss: 15373.75
[INFO 2017-07-01 21:03:11,151 main.py:52] epoch 2013, training loss: 8203.51, average training loss: 8096.57, base loss: 15374.45
[INFO 2017-07-01 21:03:15,423 main.py:52] epoch 2014, training loss: 7172.74, average training loss: 8094.93, base loss: 15373.71
[INFO 2017-07-01 21:03:19,600 main.py:52] epoch 2015, training loss: 7917.32, average training loss: 8094.41, base loss: 15374.06
[INFO 2017-07-01 21:03:23,824 main.py:52] epoch 2016, training loss: 7224.09, average training loss: 8093.89, base loss: 15372.76
[INFO 2017-07-01 21:03:27,987 main.py:52] epoch 2017, training loss: 7592.31, average training loss: 8094.10, base loss: 15372.61
[INFO 2017-07-01 21:03:32,136 main.py:52] epoch 2018, training loss: 7379.85, average training loss: 8093.19, base loss: 15372.29
[INFO 2017-07-01 21:03:36,281 main.py:52] epoch 2019, training loss: 8681.05, average training loss: 8093.70, base loss: 15372.83
[INFO 2017-07-01 21:03:40,435 main.py:52] epoch 2020, training loss: 7151.84, average training loss: 8092.17, base loss: 15372.87
[INFO 2017-07-01 21:03:44,565 main.py:52] epoch 2021, training loss: 8365.19, average training loss: 8092.33, base loss: 15373.06
[INFO 2017-07-01 21:03:48,751 main.py:52] epoch 2022, training loss: 7872.36, average training loss: 8092.70, base loss: 15372.66
[INFO 2017-07-01 21:03:52,924 main.py:52] epoch 2023, training loss: 7311.22, average training loss: 8091.43, base loss: 15371.34
[INFO 2017-07-01 21:03:57,122 main.py:52] epoch 2024, training loss: 8057.94, average training loss: 8091.17, base loss: 15370.77
[INFO 2017-07-01 21:04:01,272 main.py:52] epoch 2025, training loss: 8478.06, average training loss: 8091.02, base loss: 15371.27
[INFO 2017-07-01 21:04:05,416 main.py:52] epoch 2026, training loss: 8865.26, average training loss: 8092.66, base loss: 15372.55
[INFO 2017-07-01 21:04:09,568 main.py:52] epoch 2027, training loss: 7608.42, average training loss: 8093.04, base loss: 15371.83
[INFO 2017-07-01 21:04:13,769 main.py:52] epoch 2028, training loss: 8467.88, average training loss: 8093.28, base loss: 15372.29
[INFO 2017-07-01 21:04:17,962 main.py:52] epoch 2029, training loss: 8912.51, average training loss: 8094.58, base loss: 15374.20
[INFO 2017-07-01 21:04:22,152 main.py:52] epoch 2030, training loss: 7583.63, average training loss: 8094.49, base loss: 15374.71
[INFO 2017-07-01 21:04:26,291 main.py:52] epoch 2031, training loss: 8708.55, average training loss: 8095.48, base loss: 15376.41
[INFO 2017-07-01 21:04:30,408 main.py:52] epoch 2032, training loss: 8597.30, average training loss: 8095.84, base loss: 15377.52
[INFO 2017-07-01 21:04:34,519 main.py:52] epoch 2033, training loss: 8744.89, average training loss: 8096.98, base loss: 15378.68
[INFO 2017-07-01 21:04:38,691 main.py:52] epoch 2034, training loss: 9092.11, average training loss: 8097.62, base loss: 15379.42
[INFO 2017-07-01 21:04:42,906 main.py:52] epoch 2035, training loss: 7608.76, average training loss: 8096.47, base loss: 15378.26
[INFO 2017-07-01 21:04:47,017 main.py:52] epoch 2036, training loss: 8163.77, average training loss: 8096.12, base loss: 15378.45
[INFO 2017-07-01 21:04:51,259 main.py:52] epoch 2037, training loss: 7779.75, average training loss: 8096.06, base loss: 15377.83
[INFO 2017-07-01 21:04:55,535 main.py:52] epoch 2038, training loss: 7177.97, average training loss: 8094.44, base loss: 15377.54
[INFO 2017-07-01 21:04:59,624 main.py:52] epoch 2039, training loss: 7584.59, average training loss: 8093.02, base loss: 15376.93
[INFO 2017-07-01 21:05:03,719 main.py:52] epoch 2040, training loss: 8435.24, average training loss: 8093.49, base loss: 15377.10
[INFO 2017-07-01 21:05:07,842 main.py:52] epoch 2041, training loss: 8718.14, average training loss: 8093.76, base loss: 15378.56
[INFO 2017-07-01 21:05:12,049 main.py:52] epoch 2042, training loss: 7308.77, average training loss: 8092.07, base loss: 15377.85
[INFO 2017-07-01 21:05:16,249 main.py:52] epoch 2043, training loss: 8814.83, average training loss: 8092.61, base loss: 15378.85
[INFO 2017-07-01 21:05:20,446 main.py:52] epoch 2044, training loss: 7782.31, average training loss: 8092.91, base loss: 15379.77
[INFO 2017-07-01 21:05:24,606 main.py:52] epoch 2045, training loss: 7387.77, average training loss: 8092.04, base loss: 15380.25
[INFO 2017-07-01 21:05:28,747 main.py:52] epoch 2046, training loss: 7645.12, average training loss: 8091.83, base loss: 15379.37
[INFO 2017-07-01 21:05:32,902 main.py:52] epoch 2047, training loss: 8200.45, average training loss: 8092.61, base loss: 15378.72
[INFO 2017-07-01 21:05:37,054 main.py:52] epoch 2048, training loss: 8037.36, average training loss: 8092.40, base loss: 15377.97
[INFO 2017-07-01 21:05:41,238 main.py:52] epoch 2049, training loss: 8377.72, average training loss: 8092.53, base loss: 15378.65
[INFO 2017-07-01 21:05:45,378 main.py:52] epoch 2050, training loss: 7499.07, average training loss: 8091.13, base loss: 15376.97
[INFO 2017-07-01 21:05:49,601 main.py:52] epoch 2051, training loss: 7870.46, average training loss: 8091.12, base loss: 15376.17
[INFO 2017-07-01 21:05:53,772 main.py:52] epoch 2052, training loss: 7515.23, average training loss: 8090.39, base loss: 15374.71
[INFO 2017-07-01 21:05:57,975 main.py:52] epoch 2053, training loss: 7915.11, average training loss: 8089.98, base loss: 15373.74
[INFO 2017-07-01 21:06:02,178 main.py:52] epoch 2054, training loss: 8256.88, average training loss: 8088.82, base loss: 15373.87
[INFO 2017-07-01 21:06:06,368 main.py:52] epoch 2055, training loss: 7166.42, average training loss: 8087.83, base loss: 15373.02
[INFO 2017-07-01 21:06:10,571 main.py:52] epoch 2056, training loss: 7987.79, average training loss: 8088.20, base loss: 15372.18
[INFO 2017-07-01 21:06:14,810 main.py:52] epoch 2057, training loss: 7692.37, average training loss: 8087.64, base loss: 15371.54
[INFO 2017-07-01 21:06:18,999 main.py:52] epoch 2058, training loss: 7954.06, average training loss: 8087.28, base loss: 15371.53
[INFO 2017-07-01 21:06:23,169 main.py:52] epoch 2059, training loss: 7605.29, average training loss: 8086.04, base loss: 15372.16
[INFO 2017-07-01 21:06:27,318 main.py:52] epoch 2060, training loss: 7221.87, average training loss: 8085.38, base loss: 15372.10
[INFO 2017-07-01 21:06:31,526 main.py:52] epoch 2061, training loss: 7701.50, average training loss: 8085.89, base loss: 15372.84
[INFO 2017-07-01 21:06:35,688 main.py:52] epoch 2062, training loss: 6984.68, average training loss: 8083.51, base loss: 15371.72
[INFO 2017-07-01 21:06:39,897 main.py:52] epoch 2063, training loss: 8291.77, average training loss: 8083.20, base loss: 15371.67
[INFO 2017-07-01 21:06:44,031 main.py:52] epoch 2064, training loss: 7169.82, average training loss: 8082.59, base loss: 15369.60
[INFO 2017-07-01 21:06:48,238 main.py:52] epoch 2065, training loss: 7404.40, average training loss: 8081.08, base loss: 15367.73
[INFO 2017-07-01 21:06:52,374 main.py:52] epoch 2066, training loss: 7738.73, average training loss: 8080.72, base loss: 15366.49
[INFO 2017-07-01 21:06:56,562 main.py:52] epoch 2067, training loss: 8432.53, average training loss: 8080.49, base loss: 15366.86
[INFO 2017-07-01 21:07:00,787 main.py:52] epoch 2068, training loss: 8023.08, average training loss: 8080.14, base loss: 15366.72
[INFO 2017-07-01 21:07:04,953 main.py:52] epoch 2069, training loss: 8216.25, average training loss: 8079.92, base loss: 15366.55
[INFO 2017-07-01 21:07:09,096 main.py:52] epoch 2070, training loss: 9032.50, average training loss: 8080.13, base loss: 15367.35
[INFO 2017-07-01 21:07:13,313 main.py:52] epoch 2071, training loss: 7026.83, average training loss: 8078.42, base loss: 15365.15
[INFO 2017-07-01 21:07:17,531 main.py:52] epoch 2072, training loss: 7992.46, average training loss: 8078.02, base loss: 15365.66
[INFO 2017-07-01 21:07:21,755 main.py:52] epoch 2073, training loss: 8051.83, average training loss: 8078.21, base loss: 15366.22
[INFO 2017-07-01 21:07:25,954 main.py:52] epoch 2074, training loss: 8439.79, average training loss: 8079.12, base loss: 15366.87
[INFO 2017-07-01 21:07:30,123 main.py:52] epoch 2075, training loss: 8388.10, average training loss: 8079.26, base loss: 15367.73
[INFO 2017-07-01 21:07:34,288 main.py:52] epoch 2076, training loss: 7114.44, average training loss: 8078.45, base loss: 15367.66
[INFO 2017-07-01 21:07:38,404 main.py:52] epoch 2077, training loss: 8182.37, average training loss: 8078.49, base loss: 15368.56
[INFO 2017-07-01 21:07:42,561 main.py:52] epoch 2078, training loss: 8474.16, average training loss: 8079.06, base loss: 15369.19
[INFO 2017-07-01 21:07:46,703 main.py:52] epoch 2079, training loss: 7669.14, average training loss: 8078.58, base loss: 15369.62
[INFO 2017-07-01 21:07:50,841 main.py:52] epoch 2080, training loss: 7762.61, average training loss: 8078.87, base loss: 15369.09
[INFO 2017-07-01 21:07:54,963 main.py:52] epoch 2081, training loss: 7822.07, average training loss: 8078.56, base loss: 15369.51
[INFO 2017-07-01 21:07:59,118 main.py:52] epoch 2082, training loss: 7269.80, average training loss: 8077.53, base loss: 15369.23
[INFO 2017-07-01 21:08:03,330 main.py:52] epoch 2083, training loss: 7944.82, average training loss: 8077.34, base loss: 15370.07
[INFO 2017-07-01 21:08:07,444 main.py:52] epoch 2084, training loss: 7132.03, average training loss: 8075.80, base loss: 15369.06
[INFO 2017-07-01 21:08:11,563 main.py:52] epoch 2085, training loss: 8224.79, average training loss: 8076.03, base loss: 15369.44
[INFO 2017-07-01 21:08:15,783 main.py:52] epoch 2086, training loss: 7441.19, average training loss: 8075.73, base loss: 15368.46
[INFO 2017-07-01 21:08:19,967 main.py:52] epoch 2087, training loss: 8019.14, average training loss: 8075.59, base loss: 15368.67
[INFO 2017-07-01 21:08:24,192 main.py:52] epoch 2088, training loss: 8447.24, average training loss: 8076.84, base loss: 15368.76
[INFO 2017-07-01 21:08:28,390 main.py:52] epoch 2089, training loss: 9188.53, average training loss: 8078.16, base loss: 15369.74
[INFO 2017-07-01 21:08:32,577 main.py:52] epoch 2090, training loss: 8750.87, average training loss: 8079.21, base loss: 15370.05
[INFO 2017-07-01 21:08:36,669 main.py:52] epoch 2091, training loss: 8069.99, average training loss: 8078.58, base loss: 15369.69
[INFO 2017-07-01 21:08:40,857 main.py:52] epoch 2092, training loss: 8110.14, average training loss: 8078.50, base loss: 15368.72
[INFO 2017-07-01 21:08:45,084 main.py:52] epoch 2093, training loss: 7643.06, average training loss: 8078.22, base loss: 15368.23
[INFO 2017-07-01 21:08:49,289 main.py:52] epoch 2094, training loss: 7074.19, average training loss: 8076.27, base loss: 15367.55
[INFO 2017-07-01 21:08:53,432 main.py:52] epoch 2095, training loss: 7129.32, average training loss: 8074.93, base loss: 15367.31
[INFO 2017-07-01 21:08:57,522 main.py:52] epoch 2096, training loss: 8093.29, average training loss: 8075.50, base loss: 15367.05
[INFO 2017-07-01 21:09:01,678 main.py:52] epoch 2097, training loss: 7222.70, average training loss: 8073.55, base loss: 15366.42
[INFO 2017-07-01 21:09:05,831 main.py:52] epoch 2098, training loss: 7561.80, average training loss: 8072.94, base loss: 15365.82
[INFO 2017-07-01 21:09:09,962 main.py:52] epoch 2099, training loss: 7719.05, average training loss: 8072.54, base loss: 15365.31
[INFO 2017-07-01 21:09:09,962 main.py:54] epoch 2099, testing
[INFO 2017-07-01 21:09:09,962 main.py:61] model save to ./model/final.pth
[INFO 2017-07-01 21:09:14,240 main.py:52] epoch 2100, training loss: 8384.71, average training loss: 8073.51, base loss: 15367.15
[INFO 2017-07-01 21:09:18,417 main.py:52] epoch 2101, training loss: 8864.60, average training loss: 8075.01, base loss: 15368.55
[INFO 2017-07-01 21:09:22,560 main.py:52] epoch 2102, training loss: 7898.38, average training loss: 8074.99, base loss: 15367.41
[INFO 2017-07-01 21:09:26,760 main.py:52] epoch 2103, training loss: 8530.94, average training loss: 8075.14, base loss: 15367.25
[INFO 2017-07-01 21:09:30,942 main.py:52] epoch 2104, training loss: 7933.69, average training loss: 8074.40, base loss: 15366.70
[INFO 2017-07-01 21:09:35,097 main.py:52] epoch 2105, training loss: 7897.00, average training loss: 8074.43, base loss: 15367.03
[INFO 2017-07-01 21:09:39,252 main.py:52] epoch 2106, training loss: 7888.75, average training loss: 8074.48, base loss: 15366.77
[INFO 2017-07-01 21:09:43,482 main.py:52] epoch 2107, training loss: 8122.42, average training loss: 8075.13, base loss: 15366.53
[INFO 2017-07-01 21:09:47,607 main.py:52] epoch 2108, training loss: 7712.05, average training loss: 8074.93, base loss: 15365.90
[INFO 2017-07-01 21:09:51,731 main.py:52] epoch 2109, training loss: 7515.13, average training loss: 8074.63, base loss: 15365.02
[INFO 2017-07-01 21:09:55,816 main.py:52] epoch 2110, training loss: 8441.41, average training loss: 8075.87, base loss: 15366.14
[INFO 2017-07-01 21:09:59,961 main.py:52] epoch 2111, training loss: 7660.02, average training loss: 8075.10, base loss: 15365.51
[INFO 2017-07-01 21:10:04,118 main.py:52] epoch 2112, training loss: 8701.46, average training loss: 8076.17, base loss: 15365.97
[INFO 2017-07-01 21:10:08,243 main.py:52] epoch 2113, training loss: 8493.57, average training loss: 8076.65, base loss: 15366.39
[INFO 2017-07-01 21:10:12,466 main.py:52] epoch 2114, training loss: 8681.07, average training loss: 8076.29, base loss: 15367.16
[INFO 2017-07-01 21:10:16,618 main.py:52] epoch 2115, training loss: 7786.97, average training loss: 8075.79, base loss: 15366.86
[INFO 2017-07-01 21:10:20,782 main.py:52] epoch 2116, training loss: 8844.18, average training loss: 8076.47, base loss: 15368.76
[INFO 2017-07-01 21:10:24,992 main.py:52] epoch 2117, training loss: 9401.77, average training loss: 8077.81, base loss: 15369.59
[INFO 2017-07-01 21:10:29,143 main.py:52] epoch 2118, training loss: 7206.17, average training loss: 8076.77, base loss: 15368.18
[INFO 2017-07-01 21:10:33,269 main.py:52] epoch 2119, training loss: 7399.30, average training loss: 8076.26, base loss: 15368.67
[INFO 2017-07-01 21:10:37,439 main.py:52] epoch 2120, training loss: 7163.01, average training loss: 8075.03, base loss: 15368.14
[INFO 2017-07-01 21:10:41,609 main.py:52] epoch 2121, training loss: 7627.24, average training loss: 8073.38, base loss: 15367.72
[INFO 2017-07-01 21:10:45,761 main.py:52] epoch 2122, training loss: 7337.80, average training loss: 8072.17, base loss: 15366.46
[INFO 2017-07-01 21:10:49,951 main.py:52] epoch 2123, training loss: 8511.12, average training loss: 8072.61, base loss: 15367.40
[INFO 2017-07-01 21:10:54,164 main.py:52] epoch 2124, training loss: 7550.15, average training loss: 8072.50, base loss: 15366.35
[INFO 2017-07-01 21:10:58,366 main.py:52] epoch 2125, training loss: 7953.32, average training loss: 8072.41, base loss: 15366.74
[INFO 2017-07-01 21:11:02,533 main.py:52] epoch 2126, training loss: 8229.66, average training loss: 8071.81, base loss: 15367.90
[INFO 2017-07-01 21:11:06,748 main.py:52] epoch 2127, training loss: 7557.66, average training loss: 8071.64, base loss: 15368.50
[INFO 2017-07-01 21:11:10,923 main.py:52] epoch 2128, training loss: 8922.44, average training loss: 8072.63, base loss: 15369.08
[INFO 2017-07-01 21:11:15,146 main.py:52] epoch 2129, training loss: 8268.53, average training loss: 8071.60, base loss: 15369.76
[INFO 2017-07-01 21:11:19,299 main.py:52] epoch 2130, training loss: 7970.84, average training loss: 8071.06, base loss: 15370.15
[INFO 2017-07-01 21:11:23,544 main.py:52] epoch 2131, training loss: 7989.76, average training loss: 8071.68, base loss: 15370.86
[INFO 2017-07-01 21:11:27,642 main.py:52] epoch 2132, training loss: 8031.54, average training loss: 8071.06, base loss: 15372.02
[INFO 2017-07-01 21:11:31,752 main.py:52] epoch 2133, training loss: 7478.26, average training loss: 8070.63, base loss: 15372.60
[INFO 2017-07-01 21:11:35,941 main.py:52] epoch 2134, training loss: 7467.93, average training loss: 8070.24, base loss: 15372.93
[INFO 2017-07-01 21:11:40,036 main.py:52] epoch 2135, training loss: 7767.96, average training loss: 8069.59, base loss: 15372.61
[INFO 2017-07-01 21:11:44,113 main.py:52] epoch 2136, training loss: 8697.69, average training loss: 8069.10, base loss: 15372.87
[INFO 2017-07-01 21:11:48,268 main.py:52] epoch 2137, training loss: 7333.17, average training loss: 8067.03, base loss: 15371.77
[INFO 2017-07-01 21:11:52,409 main.py:52] epoch 2138, training loss: 7965.60, average training loss: 8066.06, base loss: 15372.08
[INFO 2017-07-01 21:11:56,539 main.py:52] epoch 2139, training loss: 7932.75, average training loss: 8065.02, base loss: 15372.39
[INFO 2017-07-01 21:12:00,731 main.py:52] epoch 2140, training loss: 7425.36, average training loss: 8064.36, base loss: 15372.10
[INFO 2017-07-01 21:12:04,942 main.py:52] epoch 2141, training loss: 7728.46, average training loss: 8063.44, base loss: 15371.83
[INFO 2017-07-01 21:12:09,133 main.py:52] epoch 2142, training loss: 7395.63, average training loss: 8062.27, base loss: 15371.69
[INFO 2017-07-01 21:12:13,295 main.py:52] epoch 2143, training loss: 6952.78, average training loss: 8060.40, base loss: 15370.58
[INFO 2017-07-01 21:12:17,441 main.py:52] epoch 2144, training loss: 7764.66, average training loss: 8060.47, base loss: 15369.87
[INFO 2017-07-01 21:12:21,624 main.py:52] epoch 2145, training loss: 7820.58, average training loss: 8060.99, base loss: 15368.15
[INFO 2017-07-01 21:12:25,818 main.py:52] epoch 2146, training loss: 7501.96, average training loss: 8060.46, base loss: 15367.38
[INFO 2017-07-01 21:12:29,967 main.py:52] epoch 2147, training loss: 8307.62, average training loss: 8060.90, base loss: 15368.22
[INFO 2017-07-01 21:12:34,094 main.py:52] epoch 2148, training loss: 7285.16, average training loss: 8059.43, base loss: 15367.82
[INFO 2017-07-01 21:12:38,257 main.py:52] epoch 2149, training loss: 7308.92, average training loss: 8058.44, base loss: 15366.98
[INFO 2017-07-01 21:12:42,430 main.py:52] epoch 2150, training loss: 7152.89, average training loss: 8057.30, base loss: 15365.69
[INFO 2017-07-01 21:12:46,621 main.py:52] epoch 2151, training loss: 8055.13, average training loss: 8057.31, base loss: 15365.93
[INFO 2017-07-01 21:12:50,830 main.py:52] epoch 2152, training loss: 7944.49, average training loss: 8056.70, base loss: 15365.45
[INFO 2017-07-01 21:12:54,932 main.py:52] epoch 2153, training loss: 8445.70, average training loss: 8056.44, base loss: 15365.93
[INFO 2017-07-01 21:12:59,161 main.py:52] epoch 2154, training loss: 8187.56, average training loss: 8056.43, base loss: 15364.95
[INFO 2017-07-01 21:13:03,335 main.py:52] epoch 2155, training loss: 9159.50, average training loss: 8058.00, base loss: 15365.52
[INFO 2017-07-01 21:13:07,574 main.py:52] epoch 2156, training loss: 7941.29, average training loss: 8058.38, base loss: 15365.72
[INFO 2017-07-01 21:13:11,702 main.py:52] epoch 2157, training loss: 7844.40, average training loss: 8058.48, base loss: 15365.68
[INFO 2017-07-01 21:13:15,903 main.py:52] epoch 2158, training loss: 7244.93, average training loss: 8057.68, base loss: 15364.98
[INFO 2017-07-01 21:13:20,020 main.py:52] epoch 2159, training loss: 8268.12, average training loss: 8058.00, base loss: 15365.89
[INFO 2017-07-01 21:13:24,166 main.py:52] epoch 2160, training loss: 7446.12, average training loss: 8057.40, base loss: 15364.32
[INFO 2017-07-01 21:13:28,337 main.py:52] epoch 2161, training loss: 8834.11, average training loss: 8057.61, base loss: 15364.06
[INFO 2017-07-01 21:13:32,449 main.py:52] epoch 2162, training loss: 8303.23, average training loss: 8058.55, base loss: 15364.16
[INFO 2017-07-01 21:13:36,592 main.py:52] epoch 2163, training loss: 7973.54, average training loss: 8058.33, base loss: 15364.63
[INFO 2017-07-01 21:13:40,760 main.py:52] epoch 2164, training loss: 7563.37, average training loss: 8058.10, base loss: 15363.94
[INFO 2017-07-01 21:13:44,955 main.py:52] epoch 2165, training loss: 8239.43, average training loss: 8058.69, base loss: 15364.83
[INFO 2017-07-01 21:13:49,081 main.py:52] epoch 2166, training loss: 8381.01, average training loss: 8058.63, base loss: 15365.25
[INFO 2017-07-01 21:13:53,258 main.py:52] epoch 2167, training loss: 7592.23, average training loss: 8058.02, base loss: 15366.10
[INFO 2017-07-01 21:13:57,424 main.py:52] epoch 2168, training loss: 8564.75, average training loss: 8059.02, base loss: 15367.76
[INFO 2017-07-01 21:14:01,530 main.py:52] epoch 2169, training loss: 7786.54, average training loss: 8058.17, base loss: 15368.34
[INFO 2017-07-01 21:14:05,616 main.py:52] epoch 2170, training loss: 7787.11, average training loss: 8057.75, base loss: 15367.62
[INFO 2017-07-01 21:14:09,822 main.py:52] epoch 2171, training loss: 9102.96, average training loss: 8059.53, base loss: 15369.03
[INFO 2017-07-01 21:14:14,048 main.py:52] epoch 2172, training loss: 7565.93, average training loss: 8059.34, base loss: 15368.69
[INFO 2017-07-01 21:14:18,261 main.py:52] epoch 2173, training loss: 7708.02, average training loss: 8059.66, base loss: 15368.59
[INFO 2017-07-01 21:14:22,457 main.py:52] epoch 2174, training loss: 7928.46, average training loss: 8059.64, base loss: 15368.33
[INFO 2017-07-01 21:14:26,608 main.py:52] epoch 2175, training loss: 7442.18, average training loss: 8058.71, base loss: 15367.71
[INFO 2017-07-01 21:14:30,797 main.py:52] epoch 2176, training loss: 8956.23, average training loss: 8059.71, base loss: 15368.58
[INFO 2017-07-01 21:14:34,958 main.py:52] epoch 2177, training loss: 7359.09, average training loss: 8059.15, base loss: 15368.15
[INFO 2017-07-01 21:14:39,168 main.py:52] epoch 2178, training loss: 7247.02, average training loss: 8058.45, base loss: 15367.35
[INFO 2017-07-01 21:14:43,299 main.py:52] epoch 2179, training loss: 7631.89, average training loss: 8058.55, base loss: 15366.93
[INFO 2017-07-01 21:14:47,459 main.py:52] epoch 2180, training loss: 8176.57, average training loss: 8058.23, base loss: 15366.86
[INFO 2017-07-01 21:14:51,663 main.py:52] epoch 2181, training loss: 7399.09, average training loss: 8057.16, base loss: 15366.16
[INFO 2017-07-01 21:14:55,882 main.py:52] epoch 2182, training loss: 7322.68, average training loss: 8056.72, base loss: 15365.10
[INFO 2017-07-01 21:15:00,075 main.py:52] epoch 2183, training loss: 8688.95, average training loss: 8057.37, base loss: 15364.53
[INFO 2017-07-01 21:15:04,216 main.py:52] epoch 2184, training loss: 8216.21, average training loss: 8057.76, base loss: 15364.12
[INFO 2017-07-01 21:15:08,433 main.py:52] epoch 2185, training loss: 7404.82, average training loss: 8057.04, base loss: 15363.72
[INFO 2017-07-01 21:15:12,546 main.py:52] epoch 2186, training loss: 7437.46, average training loss: 8056.41, base loss: 15363.52
[INFO 2017-07-01 21:15:16,687 main.py:52] epoch 2187, training loss: 7835.83, average training loss: 8056.11, base loss: 15363.90
[INFO 2017-07-01 21:15:20,814 main.py:52] epoch 2188, training loss: 7067.66, average training loss: 8053.88, base loss: 15363.01
[INFO 2017-07-01 21:15:24,997 main.py:52] epoch 2189, training loss: 7415.95, average training loss: 8053.26, base loss: 15362.67
[INFO 2017-07-01 21:15:29,173 main.py:52] epoch 2190, training loss: 8368.06, average training loss: 8053.19, base loss: 15363.08
[INFO 2017-07-01 21:15:33,352 main.py:52] epoch 2191, training loss: 8056.62, average training loss: 8053.69, base loss: 15363.00
[INFO 2017-07-01 21:15:37,557 main.py:52] epoch 2192, training loss: 7786.19, average training loss: 8053.68, base loss: 15362.51
[INFO 2017-07-01 21:15:41,726 main.py:52] epoch 2193, training loss: 7744.76, average training loss: 8053.98, base loss: 15361.67
[INFO 2017-07-01 21:15:45,869 main.py:52] epoch 2194, training loss: 7376.70, average training loss: 8053.58, base loss: 15360.87
[INFO 2017-07-01 21:15:50,070 main.py:52] epoch 2195, training loss: 8160.68, average training loss: 8053.44, base loss: 15361.43
[INFO 2017-07-01 21:15:54,216 main.py:52] epoch 2196, training loss: 8693.51, average training loss: 8053.32, base loss: 15360.81
[INFO 2017-07-01 21:15:58,415 main.py:52] epoch 2197, training loss: 8288.03, average training loss: 8054.14, base loss: 15361.27
[INFO 2017-07-01 21:16:02,624 main.py:52] epoch 2198, training loss: 8126.37, average training loss: 8052.88, base loss: 15362.14
[INFO 2017-07-01 21:16:06,765 main.py:52] epoch 2199, training loss: 7840.30, average training loss: 8052.38, base loss: 15362.80
[INFO 2017-07-01 21:16:06,765 main.py:54] epoch 2199, testing
[INFO 2017-07-01 21:16:06,765 main.py:61] model save to ./model/final.pth
[INFO 2017-07-01 21:16:10,965 main.py:52] epoch 2200, training loss: 8599.49, average training loss: 8053.53, base loss: 15364.02
[INFO 2017-07-01 21:16:15,124 main.py:52] epoch 2201, training loss: 7917.75, average training loss: 8051.97, base loss: 15364.45
[INFO 2017-07-01 21:16:19,240 main.py:52] epoch 2202, training loss: 8975.98, average training loss: 8053.57, base loss: 15366.50
[INFO 2017-07-01 21:16:23,419 main.py:52] epoch 2203, training loss: 7509.05, average training loss: 8053.24, base loss: 15366.61
[INFO 2017-07-01 21:16:27,614 main.py:52] epoch 2204, training loss: 7690.66, average training loss: 8053.65, base loss: 15366.56
[INFO 2017-07-01 21:16:31,795 main.py:52] epoch 2205, training loss: 7544.75, average training loss: 8052.88, base loss: 15367.00
[INFO 2017-07-01 21:16:36,015 main.py:52] epoch 2206, training loss: 7011.43, average training loss: 8052.17, base loss: 15365.39
[INFO 2017-07-01 21:16:40,232 main.py:52] epoch 2207, training loss: 7715.03, average training loss: 8051.45, base loss: 15364.35
[INFO 2017-07-01 21:16:44,401 main.py:52] epoch 2208, training loss: 7428.31, average training loss: 8050.44, base loss: 15363.89
[INFO 2017-07-01 21:16:48,593 main.py:52] epoch 2209, training loss: 7998.38, average training loss: 8049.25, base loss: 15364.44
[INFO 2017-07-01 21:16:52,791 main.py:52] epoch 2210, training loss: 7880.75, average training loss: 8049.85, base loss: 15362.99
[INFO 2017-07-01 21:16:57,014 main.py:52] epoch 2211, training loss: 8617.72, average training loss: 8050.68, base loss: 15363.13
[INFO 2017-07-01 21:17:01,132 main.py:52] epoch 2212, training loss: 8487.49, average training loss: 8050.86, base loss: 15363.79
[INFO 2017-07-01 21:17:05,279 main.py:52] epoch 2213, training loss: 8377.77, average training loss: 8051.62, base loss: 15364.77
[INFO 2017-07-01 21:17:09,431 main.py:52] epoch 2214, training loss: 7358.04, average training loss: 8051.25, base loss: 15364.04
[INFO 2017-07-01 21:17:13,716 main.py:52] epoch 2215, training loss: 8337.16, average training loss: 8051.41, base loss: 15364.70
[INFO 2017-07-01 21:17:17,896 main.py:52] epoch 2216, training loss: 6839.78, average training loss: 8049.92, base loss: 15363.85
[INFO 2017-07-01 21:17:22,065 main.py:52] epoch 2217, training loss: 7259.66, average training loss: 8049.70, base loss: 15361.36
[INFO 2017-07-01 21:17:26,317 main.py:52] epoch 2218, training loss: 7482.32, average training loss: 8049.36, base loss: 15360.28
[INFO 2017-07-01 21:17:30,477 main.py:52] epoch 2219, training loss: 8300.08, average training loss: 8049.55, base loss: 15360.85
[INFO 2017-07-01 21:17:34,633 main.py:52] epoch 2220, training loss: 8169.18, average training loss: 8049.44, base loss: 15361.01
[INFO 2017-07-01 21:17:38,772 main.py:52] epoch 2221, training loss: 8195.80, average training loss: 8050.17, base loss: 15362.11
[INFO 2017-07-01 21:17:42,879 main.py:52] epoch 2222, training loss: 9211.24, average training loss: 8051.32, base loss: 15362.76
[INFO 2017-07-01 21:17:47,036 main.py:52] epoch 2223, training loss: 7908.35, average training loss: 8051.47, base loss: 15362.25
[INFO 2017-07-01 21:17:51,173 main.py:52] epoch 2224, training loss: 9077.99, average training loss: 8052.29, base loss: 15363.07
[INFO 2017-07-01 21:17:55,287 main.py:52] epoch 2225, training loss: 7826.87, average training loss: 8051.65, base loss: 15362.34
[INFO 2017-07-01 21:17:59,495 main.py:52] epoch 2226, training loss: 7656.10, average training loss: 8051.04, base loss: 15362.59
[INFO 2017-07-01 21:18:03,684 main.py:52] epoch 2227, training loss: 6789.12, average training loss: 8049.36, base loss: 15361.35
[INFO 2017-07-01 21:18:07,807 main.py:52] epoch 2228, training loss: 7886.65, average training loss: 8048.61, base loss: 15362.15
[INFO 2017-07-01 21:18:11,976 main.py:52] epoch 2229, training loss: 7447.07, average training loss: 8047.79, base loss: 15361.55
[INFO 2017-07-01 21:18:16,174 main.py:52] epoch 2230, training loss: 8711.46, average training loss: 8048.82, base loss: 15361.89
[INFO 2017-07-01 21:18:20,326 main.py:52] epoch 2231, training loss: 6742.18, average training loss: 8046.87, base loss: 15359.90
[INFO 2017-07-01 21:18:24,564 main.py:52] epoch 2232, training loss: 8136.48, average training loss: 8048.08, base loss: 15360.35
[INFO 2017-07-01 21:18:28,767 main.py:52] epoch 2233, training loss: 7921.56, average training loss: 8048.31, base loss: 15360.68
[INFO 2017-07-01 21:18:32,957 main.py:52] epoch 2234, training loss: 7762.65, average training loss: 8047.64, base loss: 15361.31
[INFO 2017-07-01 21:18:37,065 main.py:52] epoch 2235, training loss: 8007.01, average training loss: 8048.79, base loss: 15361.39
[INFO 2017-07-01 21:18:41,198 main.py:52] epoch 2236, training loss: 7629.41, average training loss: 8048.95, base loss: 15360.66
[INFO 2017-07-01 21:18:45,368 main.py:52] epoch 2237, training loss: 7982.38, average training loss: 8048.43, base loss: 15360.56
[INFO 2017-07-01 21:18:49,588 main.py:52] epoch 2238, training loss: 8490.37, average training loss: 8047.86, base loss: 15362.43
[INFO 2017-07-01 21:18:53,695 main.py:52] epoch 2239, training loss: 8749.47, average training loss: 8048.77, base loss: 15364.79
[INFO 2017-07-01 21:18:57,854 main.py:52] epoch 2240, training loss: 7705.73, average training loss: 8048.37, base loss: 15365.01
[INFO 2017-07-01 21:19:02,001 main.py:52] epoch 2241, training loss: 7128.58, average training loss: 8047.17, base loss: 15363.48
[INFO 2017-07-01 21:19:06,185 main.py:52] epoch 2242, training loss: 7877.76, average training loss: 8046.31, base loss: 15363.39
[INFO 2017-07-01 21:19:10,374 main.py:52] epoch 2243, training loss: 8053.27, average training loss: 8045.81, base loss: 15364.40
[INFO 2017-07-01 21:19:14,541 main.py:52] epoch 2244, training loss: 8084.29, average training loss: 8045.82, base loss: 15363.80
[INFO 2017-07-01 21:19:18,738 main.py:52] epoch 2245, training loss: 6924.35, average training loss: 8044.52, base loss: 15362.77
[INFO 2017-07-01 21:19:22,907 main.py:52] epoch 2246, training loss: 8098.03, average training loss: 8045.55, base loss: 15362.50
[INFO 2017-07-01 21:19:27,102 main.py:52] epoch 2247, training loss: 8439.50, average training loss: 8046.63, base loss: 15362.69
[INFO 2017-07-01 21:19:31,275 main.py:52] epoch 2248, training loss: 9109.95, average training loss: 8047.67, base loss: 15363.55
[INFO 2017-07-01 21:19:35,458 main.py:52] epoch 2249, training loss: 7915.15, average training loss: 8047.24, base loss: 15362.16
[INFO 2017-07-01 21:19:39,596 main.py:52] epoch 2250, training loss: 8460.08, average training loss: 8047.73, base loss: 15362.78
[INFO 2017-07-01 21:19:43,776 main.py:52] epoch 2251, training loss: 8056.81, average training loss: 8047.50, base loss: 15362.56
[INFO 2017-07-01 21:19:47,948 main.py:52] epoch 2252, training loss: 7837.80, average training loss: 8047.70, base loss: 15362.09
[INFO 2017-07-01 21:19:52,220 main.py:52] epoch 2253, training loss: 7639.19, average training loss: 8047.60, base loss: 15361.00
[INFO 2017-07-01 21:19:56,405 main.py:52] epoch 2254, training loss: 7569.11, average training loss: 8047.14, base loss: 15360.02
[INFO 2017-07-01 21:20:00,580 main.py:52] epoch 2255, training loss: 8200.23, average training loss: 8046.49, base loss: 15359.68
[INFO 2017-07-01 21:20:04,698 main.py:52] epoch 2256, training loss: 8312.12, average training loss: 8045.84, base loss: 15360.76
[INFO 2017-07-01 21:20:08,866 main.py:52] epoch 2257, training loss: 8046.50, average training loss: 8045.72, base loss: 15360.23
[INFO 2017-07-01 21:20:13,006 main.py:52] epoch 2258, training loss: 7530.34, average training loss: 8044.56, base loss: 15359.64
[INFO 2017-07-01 21:20:17,156 main.py:52] epoch 2259, training loss: 7768.39, average training loss: 8044.38, base loss: 15359.89
[INFO 2017-07-01 21:20:21,367 main.py:52] epoch 2260, training loss: 8306.43, average training loss: 8042.50, base loss: 15360.24
[INFO 2017-07-01 21:20:25,475 main.py:52] epoch 2261, training loss: 8256.73, average training loss: 8042.53, base loss: 15360.56
[INFO 2017-07-01 21:20:29,661 main.py:52] epoch 2262, training loss: 8127.63, average training loss: 8043.53, base loss: 15361.08
[INFO 2017-07-01 21:20:33,823 main.py:52] epoch 2263, training loss: 8384.04, average training loss: 8043.45, base loss: 15362.14
[INFO 2017-07-01 21:20:38,008 main.py:52] epoch 2264, training loss: 7044.43, average training loss: 8042.77, base loss: 15360.14
[INFO 2017-07-01 21:20:42,190 main.py:52] epoch 2265, training loss: 7990.34, average training loss: 8042.13, base loss: 15360.13
[INFO 2017-07-01 21:20:46,310 main.py:52] epoch 2266, training loss: 7323.98, average training loss: 8041.17, base loss: 15358.77
[INFO 2017-07-01 21:20:50,486 main.py:52] epoch 2267, training loss: 7871.77, average training loss: 8040.79, base loss: 15357.25
[INFO 2017-07-01 21:20:54,703 main.py:52] epoch 2268, training loss: 7987.60, average training loss: 8039.54, base loss: 15357.09
[INFO 2017-07-01 21:20:58,873 main.py:52] epoch 2269, training loss: 7017.06, average training loss: 8038.11, base loss: 15355.91
[INFO 2017-07-01 21:21:03,113 main.py:52] epoch 2270, training loss: 8433.66, average training loss: 8038.18, base loss: 15356.19
[INFO 2017-07-01 21:21:07,296 main.py:52] epoch 2271, training loss: 7582.18, average training loss: 8037.39, base loss: 15356.00
[INFO 2017-07-01 21:21:11,435 main.py:52] epoch 2272, training loss: 7870.08, average training loss: 8037.79, base loss: 15355.83
[INFO 2017-07-01 21:21:15,567 main.py:52] epoch 2273, training loss: 7944.70, average training loss: 8037.92, base loss: 15355.93
[INFO 2017-07-01 21:21:19,839 main.py:52] epoch 2274, training loss: 7446.30, average training loss: 8036.70, base loss: 15354.58
[INFO 2017-07-01 21:21:24,009 main.py:52] epoch 2275, training loss: 8999.55, average training loss: 8037.81, base loss: 15355.18
[INFO 2017-07-01 21:21:28,197 main.py:52] epoch 2276, training loss: 8111.23, average training loss: 8037.98, base loss: 15355.99
[INFO 2017-07-01 21:21:32,416 main.py:52] epoch 2277, training loss: 8667.45, average training loss: 8038.16, base loss: 15357.25
[INFO 2017-07-01 21:21:36,614 main.py:52] epoch 2278, training loss: 8300.29, average training loss: 8038.38, base loss: 15358.70
[INFO 2017-07-01 21:21:40,805 main.py:52] epoch 2279, training loss: 8214.44, average training loss: 8037.35, base loss: 15360.38
[INFO 2017-07-01 21:21:44,970 main.py:52] epoch 2280, training loss: 7270.20, average training loss: 8036.06, base loss: 15359.23
[INFO 2017-07-01 21:21:49,148 main.py:52] epoch 2281, training loss: 8403.85, average training loss: 8037.13, base loss: 15359.50
[INFO 2017-07-01 21:21:53,339 main.py:52] epoch 2282, training loss: 8346.55, average training loss: 8037.75, base loss: 15359.68
[INFO 2017-07-01 21:21:57,499 main.py:52] epoch 2283, training loss: 7047.57, average training loss: 8036.46, base loss: 15358.38
[INFO 2017-07-01 21:22:01,688 main.py:52] epoch 2284, training loss: 7614.48, average training loss: 8036.10, base loss: 15358.16
[INFO 2017-07-01 21:22:05,779 main.py:52] epoch 2285, training loss: 7338.64, average training loss: 8034.45, base loss: 15358.11
[INFO 2017-07-01 21:22:09,907 main.py:52] epoch 2286, training loss: 8126.43, average training loss: 8033.94, base loss: 15358.20
[INFO 2017-07-01 21:22:14,034 main.py:52] epoch 2287, training loss: 7288.42, average training loss: 8031.88, base loss: 15357.54
[INFO 2017-07-01 21:22:18,191 main.py:52] epoch 2288, training loss: 7153.76, average training loss: 8029.57, base loss: 15356.40
[INFO 2017-07-01 21:22:22,309 main.py:52] epoch 2289, training loss: 7731.91, average training loss: 8029.23, base loss: 15356.11
[INFO 2017-07-01 21:22:26,455 main.py:52] epoch 2290, training loss: 7836.53, average training loss: 8028.68, base loss: 15354.81
[INFO 2017-07-01 21:22:30,565 main.py:52] epoch 2291, training loss: 8313.95, average training loss: 8028.32, base loss: 15355.01
[INFO 2017-07-01 21:22:34,745 main.py:52] epoch 2292, training loss: 8180.95, average training loss: 8027.40, base loss: 15355.27
[INFO 2017-07-01 21:22:38,914 main.py:52] epoch 2293, training loss: 8786.14, average training loss: 8028.11, base loss: 15355.69
[INFO 2017-07-01 21:22:43,076 main.py:52] epoch 2294, training loss: 7419.94, average training loss: 8027.29, base loss: 15354.57
[INFO 2017-07-01 21:22:47,211 main.py:52] epoch 2295, training loss: 7240.55, average training loss: 8025.73, base loss: 15352.86
[INFO 2017-07-01 21:22:51,437 main.py:52] epoch 2296, training loss: 8186.23, average training loss: 8025.22, base loss: 15353.25
[INFO 2017-07-01 21:22:55,538 main.py:52] epoch 2297, training loss: 8562.13, average training loss: 8026.21, base loss: 15354.27
[INFO 2017-07-01 21:22:59,690 main.py:52] epoch 2298, training loss: 6955.50, average training loss: 8024.15, base loss: 15353.03
[INFO 2017-07-01 21:23:03,891 main.py:52] epoch 2299, training loss: 8215.91, average training loss: 8023.46, base loss: 15353.69
[INFO 2017-07-01 21:23:03,891 main.py:54] epoch 2299, testing
[INFO 2017-07-01 21:23:03,891 main.py:61] model save to ./model/final.pth
[INFO 2017-07-01 21:23:08,118 main.py:52] epoch 2300, training loss: 7683.60, average training loss: 8022.46, base loss: 15353.92
[INFO 2017-07-01 21:23:12,308 main.py:52] epoch 2301, training loss: 7799.34, average training loss: 8023.14, base loss: 15354.76
[INFO 2017-07-01 21:23:16,504 main.py:52] epoch 2302, training loss: 9178.24, average training loss: 8023.48, base loss: 15355.27
[INFO 2017-07-01 21:23:20,619 main.py:52] epoch 2303, training loss: 7394.12, average training loss: 8022.65, base loss: 15354.90
[INFO 2017-07-01 21:23:24,779 main.py:52] epoch 2304, training loss: 8039.97, average training loss: 8022.47, base loss: 15354.57
[INFO 2017-07-01 21:23:28,949 main.py:52] epoch 2305, training loss: 7878.30, average training loss: 8022.44, base loss: 15354.07
[INFO 2017-07-01 21:23:33,088 main.py:52] epoch 2306, training loss: 8319.86, average training loss: 8022.17, base loss: 15354.27
[INFO 2017-07-01 21:23:37,263 main.py:52] epoch 2307, training loss: 8825.39, average training loss: 8022.96, base loss: 15355.54
[INFO 2017-07-01 21:23:41,436 main.py:52] epoch 2308, training loss: 7786.75, average training loss: 8022.40, base loss: 15354.06
[INFO 2017-07-01 21:23:45,562 main.py:52] epoch 2309, training loss: 7634.84, average training loss: 8021.79, base loss: 15352.75
[INFO 2017-07-01 21:23:49,677 main.py:52] epoch 2310, training loss: 8229.64, average training loss: 8022.75, base loss: 15353.56
[INFO 2017-07-01 21:23:53,811 main.py:52] epoch 2311, training loss: 8905.36, average training loss: 8023.45, base loss: 15355.49
[INFO 2017-07-01 21:23:57,920 main.py:52] epoch 2312, training loss: 8363.06, average training loss: 8024.08, base loss: 15356.51
[INFO 2017-07-01 21:24:02,068 main.py:52] epoch 2313, training loss: 8160.27, average training loss: 8022.89, base loss: 15357.29
[INFO 2017-07-01 21:24:06,232 main.py:52] epoch 2314, training loss: 7799.76, average training loss: 8022.92, base loss: 15356.28
[INFO 2017-07-01 21:24:10,436 main.py:52] epoch 2315, training loss: 8021.20, average training loss: 8023.01, base loss: 15355.44
[INFO 2017-07-01 21:24:14,674 main.py:52] epoch 2316, training loss: 8365.90, average training loss: 8021.60, base loss: 15355.77
[INFO 2017-07-01 21:24:18,786 main.py:52] epoch 2317, training loss: 7765.08, average training loss: 8022.30, base loss: 15355.52
[INFO 2017-07-01 21:24:23,056 main.py:52] epoch 2318, training loss: 8267.28, average training loss: 8021.87, base loss: 15354.96
[INFO 2017-07-01 21:24:27,244 main.py:52] epoch 2319, training loss: 7391.24, average training loss: 8020.90, base loss: 15354.31
[INFO 2017-07-01 21:24:31,362 main.py:52] epoch 2320, training loss: 7970.57, average training loss: 8020.80, base loss: 15354.77
[INFO 2017-07-01 21:24:35,586 main.py:52] epoch 2321, training loss: 8058.23, average training loss: 8021.15, base loss: 15355.60
[INFO 2017-07-01 21:24:39,742 main.py:52] epoch 2322, training loss: 8496.00, average training loss: 8021.41, base loss: 15356.47
[INFO 2017-07-01 21:24:43,897 main.py:52] epoch 2323, training loss: 8553.41, average training loss: 8021.33, base loss: 15357.33
[INFO 2017-07-01 21:24:48,045 main.py:52] epoch 2324, training loss: 7075.85, average training loss: 8020.79, base loss: 15355.98
[INFO 2017-07-01 21:24:52,209 main.py:52] epoch 2325, training loss: 7670.35, average training loss: 8021.84, base loss: 15355.59
[INFO 2017-07-01 21:24:56,360 main.py:52] epoch 2326, training loss: 7955.20, average training loss: 8021.34, base loss: 15355.38
[INFO 2017-07-01 21:25:00,512 main.py:52] epoch 2327, training loss: 7997.37, average training loss: 8020.95, base loss: 15355.14
[INFO 2017-07-01 21:25:04,708 main.py:52] epoch 2328, training loss: 7869.66, average training loss: 8021.47, base loss: 15355.24
[INFO 2017-07-01 21:25:08,879 main.py:52] epoch 2329, training loss: 7690.12, average training loss: 8020.40, base loss: 15355.02
[INFO 2017-07-01 21:25:13,069 main.py:52] epoch 2330, training loss: 7770.16, average training loss: 8019.84, base loss: 15355.21
[INFO 2017-07-01 21:25:17,323 main.py:52] epoch 2331, training loss: 7480.22, average training loss: 8019.24, base loss: 15355.32
[INFO 2017-07-01 21:25:21,527 main.py:52] epoch 2332, training loss: 8015.43, average training loss: 8018.99, base loss: 15355.85
[INFO 2017-07-01 21:25:25,703 main.py:52] epoch 2333, training loss: 7825.22, average training loss: 8017.05, base loss: 15356.44
[INFO 2017-07-01 21:25:29,811 main.py:52] epoch 2334, training loss: 8364.91, average training loss: 8017.23, base loss: 15356.97
[INFO 2017-07-01 21:25:34,008 main.py:52] epoch 2335, training loss: 6886.26, average training loss: 8015.68, base loss: 15355.29
[INFO 2017-07-01 21:25:38,191 main.py:52] epoch 2336, training loss: 8967.52, average training loss: 8016.34, base loss: 15356.64
[INFO 2017-07-01 21:25:42,327 main.py:52] epoch 2337, training loss: 7849.14, average training loss: 8016.47, base loss: 15356.01
[INFO 2017-07-01 21:25:46,481 main.py:52] epoch 2338, training loss: 8133.13, average training loss: 8015.73, base loss: 15356.92
[INFO 2017-07-01 21:25:50,601 main.py:52] epoch 2339, training loss: 7691.41, average training loss: 8015.87, base loss: 15357.33
[INFO 2017-07-01 21:25:54,713 main.py:52] epoch 2340, training loss: 8516.72, average training loss: 8016.43, base loss: 15357.48
[INFO 2017-07-01 21:25:58,832 main.py:52] epoch 2341, training loss: 7311.08, average training loss: 8015.04, base loss: 15356.81
[INFO 2017-07-01 21:26:03,006 main.py:52] epoch 2342, training loss: 7721.31, average training loss: 8015.73, base loss: 15355.66
[INFO 2017-07-01 21:26:07,208 main.py:52] epoch 2343, training loss: 7524.23, average training loss: 8014.72, base loss: 15354.77
[INFO 2017-07-01 21:26:11,378 main.py:52] epoch 2344, training loss: 8742.20, average training loss: 8015.00, base loss: 15356.11
[INFO 2017-07-01 21:26:15,602 main.py:52] epoch 2345, training loss: 7268.46, average training loss: 8014.72, base loss: 15354.40
[INFO 2017-07-01 21:26:19,746 main.py:52] epoch 2346, training loss: 8671.54, average training loss: 8015.28, base loss: 15355.33
[INFO 2017-07-01 21:26:23,870 main.py:52] epoch 2347, training loss: 7932.90, average training loss: 8014.37, base loss: 15355.90
[INFO 2017-07-01 21:26:28,106 main.py:52] epoch 2348, training loss: 8234.35, average training loss: 8014.92, base loss: 15356.70
[INFO 2017-07-01 21:26:32,276 main.py:52] epoch 2349, training loss: 7409.80, average training loss: 8013.14, base loss: 15356.11
[INFO 2017-07-01 21:26:36,429 main.py:52] epoch 2350, training loss: 9013.00, average training loss: 8014.31, base loss: 15356.68
[INFO 2017-07-01 21:26:40,660 main.py:52] epoch 2351, training loss: 7442.84, average training loss: 8013.06, base loss: 15355.57
[INFO 2017-07-01 21:26:44,740 main.py:52] epoch 2352, training loss: 7433.86, average training loss: 8012.48, base loss: 15353.66
[INFO 2017-07-01 21:26:48,869 main.py:52] epoch 2353, training loss: 8439.89, average training loss: 8011.71, base loss: 15353.99
[INFO 2017-07-01 21:26:53,089 main.py:52] epoch 2354, training loss: 7707.04, average training loss: 8010.97, base loss: 15354.57
[INFO 2017-07-01 21:26:57,189 main.py:52] epoch 2355, training loss: 7812.52, average training loss: 8010.07, base loss: 15355.18
[INFO 2017-07-01 21:27:01,374 main.py:52] epoch 2356, training loss: 6875.73, average training loss: 8008.42, base loss: 15354.18
[INFO 2017-07-01 21:27:05,549 main.py:52] epoch 2357, training loss: 8030.77, average training loss: 8008.96, base loss: 15355.28
[INFO 2017-07-01 21:27:09,810 main.py:52] epoch 2358, training loss: 7436.26, average training loss: 8008.57, base loss: 15354.72
[INFO 2017-07-01 21:27:13,936 main.py:52] epoch 2359, training loss: 7855.99, average training loss: 8007.58, base loss: 15354.59
[INFO 2017-07-01 21:27:18,068 main.py:52] epoch 2360, training loss: 8831.27, average training loss: 8007.11, base loss: 15355.28
[INFO 2017-07-01 21:27:22,213 main.py:52] epoch 2361, training loss: 7959.53, average training loss: 8007.14, base loss: 15354.72
[INFO 2017-07-01 21:27:26,353 main.py:52] epoch 2362, training loss: 8013.30, average training loss: 8007.24, base loss: 15355.09
[INFO 2017-07-01 21:27:30,497 main.py:52] epoch 2363, training loss: 9098.48, average training loss: 8007.80, base loss: 15355.53
[INFO 2017-07-01 21:27:34,664 main.py:52] epoch 2364, training loss: 7841.12, average training loss: 8007.21, base loss: 15355.81
[INFO 2017-07-01 21:27:38,829 main.py:52] epoch 2365, training loss: 7739.47, average training loss: 8005.99, base loss: 15356.23
[INFO 2017-07-01 21:27:42,976 main.py:52] epoch 2366, training loss: 8388.44, average training loss: 8005.99, base loss: 15357.87
[INFO 2017-07-01 21:27:47,107 main.py:52] epoch 2367, training loss: 8673.05, average training loss: 8006.82, base loss: 15360.19
[INFO 2017-07-01 21:27:51,349 main.py:52] epoch 2368, training loss: 6773.47, average training loss: 8005.49, base loss: 15358.82
[INFO 2017-07-01 21:27:55,487 main.py:52] epoch 2369, training loss: 8286.60, average training loss: 8005.75, base loss: 15357.90
[INFO 2017-07-01 21:27:59,672 main.py:52] epoch 2370, training loss: 7461.79, average training loss: 8005.79, base loss: 15358.98
[INFO 2017-07-01 21:28:03,887 main.py:52] epoch 2371, training loss: 7980.35, average training loss: 8006.19, base loss: 15359.44
[INFO 2017-07-01 21:28:07,978 main.py:52] epoch 2372, training loss: 8147.91, average training loss: 8005.56, base loss: 15358.93
[INFO 2017-07-01 21:28:12,128 main.py:52] epoch 2373, training loss: 8193.41, average training loss: 8006.05, base loss: 15358.88
[INFO 2017-07-01 21:28:16,301 main.py:52] epoch 2374, training loss: 6740.03, average training loss: 8004.75, base loss: 15357.84
[INFO 2017-07-01 21:28:20,466 main.py:52] epoch 2375, training loss: 7980.16, average training loss: 8004.10, base loss: 15358.55
[INFO 2017-07-01 21:28:24,638 main.py:52] epoch 2376, training loss: 8012.12, average training loss: 8003.76, base loss: 15359.23
[INFO 2017-07-01 21:28:28,815 main.py:52] epoch 2377, training loss: 7693.38, average training loss: 8004.00, base loss: 15359.33
[INFO 2017-07-01 21:28:32,965 main.py:52] epoch 2378, training loss: 7280.32, average training loss: 8002.80, base loss: 15358.19
[INFO 2017-07-01 21:28:37,239 main.py:52] epoch 2379, training loss: 7737.00, average training loss: 8002.72, base loss: 15357.93
[INFO 2017-07-01 21:28:41,545 main.py:52] epoch 2380, training loss: 7864.77, average training loss: 8003.23, base loss: 15357.18
[INFO 2017-07-01 21:28:45,660 main.py:52] epoch 2381, training loss: 8325.21, average training loss: 8003.78, base loss: 15357.55
[INFO 2017-07-01 21:28:49,948 main.py:52] epoch 2382, training loss: 8158.49, average training loss: 8004.13, base loss: 15357.99
[INFO 2017-07-01 21:28:54,158 main.py:52] epoch 2383, training loss: 7237.63, average training loss: 8003.35, base loss: 15356.83
[INFO 2017-07-01 21:28:58,345 main.py:52] epoch 2384, training loss: 8029.90, average training loss: 8003.00, base loss: 15356.75
[INFO 2017-07-01 21:29:02,557 main.py:52] epoch 2385, training loss: 7033.24, average training loss: 8001.88, base loss: 15355.63
[INFO 2017-07-01 21:29:06,674 main.py:52] epoch 2386, training loss: 7694.77, average training loss: 8001.78, base loss: 15355.81
[INFO 2017-07-01 21:29:10,813 main.py:52] epoch 2387, training loss: 7417.09, average training loss: 8001.27, base loss: 15355.76
[INFO 2017-07-01 21:29:15,029 main.py:52] epoch 2388, training loss: 6803.14, average training loss: 8000.06, base loss: 15354.15
[INFO 2017-07-01 21:29:19,198 main.py:52] epoch 2389, training loss: 8727.52, average training loss: 8000.45, base loss: 15355.23
[INFO 2017-07-01 21:29:23,375 main.py:52] epoch 2390, training loss: 7102.32, average training loss: 7999.64, base loss: 15354.15
[INFO 2017-07-01 21:29:27,521 main.py:52] epoch 2391, training loss: 8662.22, average training loss: 8000.70, base loss: 15354.66
[INFO 2017-07-01 21:29:31,731 main.py:52] epoch 2392, training loss: 8871.26, average training loss: 8002.17, base loss: 15355.96
[INFO 2017-07-01 21:29:35,904 main.py:52] epoch 2393, training loss: 8732.88, average training loss: 8002.99, base loss: 15356.29
[INFO 2017-07-01 21:29:40,103 main.py:52] epoch 2394, training loss: 7931.98, average training loss: 8003.01, base loss: 15355.79
[INFO 2017-07-01 21:29:44,348 main.py:52] epoch 2395, training loss: 7989.30, average training loss: 8002.30, base loss: 15355.57
[INFO 2017-07-01 21:29:48,507 main.py:52] epoch 2396, training loss: 7844.93, average training loss: 8002.15, base loss: 15356.10
[INFO 2017-07-01 21:29:52,664 main.py:52] epoch 2397, training loss: 8083.08, average training loss: 8002.56, base loss: 15356.66
[INFO 2017-07-01 21:29:56,909 main.py:52] epoch 2398, training loss: 7221.59, average training loss: 8001.37, base loss: 15355.60
[INFO 2017-07-01 21:30:01,001 main.py:52] epoch 2399, training loss: 8403.88, average training loss: 8001.75, base loss: 15355.38
[INFO 2017-07-01 21:30:01,002 main.py:54] epoch 2399, testing
[INFO 2017-07-01 21:30:01,002 main.py:61] model save to ./model/final.pth
[INFO 2017-07-01 21:30:05,241 main.py:52] epoch 2400, training loss: 9285.13, average training loss: 8002.99, base loss: 15357.76
[INFO 2017-07-01 21:30:09,430 main.py:52] epoch 2401, training loss: 6881.39, average training loss: 8001.93, base loss: 15356.67
[INFO 2017-07-01 21:30:13,514 main.py:52] epoch 2402, training loss: 8279.85, average training loss: 8003.03, base loss: 15357.28
[INFO 2017-07-01 21:30:17,689 main.py:52] epoch 2403, training loss: 8212.95, average training loss: 8003.54, base loss: 15358.23
[INFO 2017-07-01 21:30:21,851 main.py:52] epoch 2404, training loss: 8198.84, average training loss: 8003.91, base loss: 15358.53
[INFO 2017-07-01 21:30:25,966 main.py:52] epoch 2405, training loss: 7563.36, average training loss: 8002.92, base loss: 15357.79
[INFO 2017-07-01 21:30:30,167 main.py:52] epoch 2406, training loss: 7207.38, average training loss: 8002.36, base loss: 15357.08
[INFO 2017-07-01 21:30:34,343 main.py:52] epoch 2407, training loss: 7259.45, average training loss: 8000.53, base loss: 15356.23
[INFO 2017-07-01 21:30:38,534 main.py:52] epoch 2408, training loss: 7787.14, average training loss: 8000.49, base loss: 15356.65
[INFO 2017-07-01 21:30:42,720 main.py:52] epoch 2409, training loss: 8910.23, average training loss: 8001.77, base loss: 15358.08
[INFO 2017-07-01 21:30:46,871 main.py:52] epoch 2410, training loss: 7195.55, average training loss: 8000.53, base loss: 15357.89
[INFO 2017-07-01 21:30:51,087 main.py:52] epoch 2411, training loss: 8014.82, average training loss: 7999.82, base loss: 15358.99
[INFO 2017-07-01 21:30:55,243 main.py:52] epoch 2412, training loss: 6882.30, average training loss: 7999.19, base loss: 15357.99
[INFO 2017-07-01 21:30:59,485 main.py:52] epoch 2413, training loss: 8009.79, average training loss: 7999.33, base loss: 15358.48
[INFO 2017-07-01 21:31:03,694 main.py:52] epoch 2414, training loss: 7593.87, average training loss: 7998.87, base loss: 15357.31
[INFO 2017-07-01 21:31:07,813 main.py:52] epoch 2415, training loss: 8143.29, average training loss: 7998.86, base loss: 15356.64
[INFO 2017-07-01 21:31:11,938 main.py:52] epoch 2416, training loss: 7851.09, average training loss: 7998.65, base loss: 15355.94
[INFO 2017-07-01 21:31:16,077 main.py:52] epoch 2417, training loss: 7974.19, average training loss: 7998.43, base loss: 15355.45
[INFO 2017-07-01 21:31:20,227 main.py:52] epoch 2418, training loss: 8174.08, average training loss: 7998.54, base loss: 15355.24
[INFO 2017-07-01 21:31:24,390 main.py:52] epoch 2419, training loss: 9312.21, average training loss: 8000.34, base loss: 15356.93
[INFO 2017-07-01 21:31:28,543 main.py:52] epoch 2420, training loss: 7354.95, average training loss: 7999.64, base loss: 15355.95
[INFO 2017-07-01 21:31:32,721 main.py:52] epoch 2421, training loss: 9356.75, average training loss: 8000.13, base loss: 15356.87
[INFO 2017-07-01 21:31:36,852 main.py:52] epoch 2422, training loss: 7413.48, average training loss: 7998.98, base loss: 15356.78
[INFO 2017-07-01 21:31:40,943 main.py:52] epoch 2423, training loss: 8327.13, average training loss: 7999.52, base loss: 15357.69
[INFO 2017-07-01 21:31:45,119 main.py:52] epoch 2424, training loss: 8009.02, average training loss: 7998.72, base loss: 15357.63
[INFO 2017-07-01 21:31:49,272 main.py:52] epoch 2425, training loss: 8832.28, average training loss: 7999.79, base loss: 15358.50
[INFO 2017-07-01 21:31:53,477 main.py:52] epoch 2426, training loss: 9736.73, average training loss: 8001.58, base loss: 15359.72
[INFO 2017-07-01 21:31:57,638 main.py:52] epoch 2427, training loss: 7236.72, average training loss: 8000.67, base loss: 15359.29
[INFO 2017-07-01 21:32:01,738 main.py:52] epoch 2428, training loss: 6943.89, average training loss: 7998.91, base loss: 15358.02
[INFO 2017-07-01 21:32:05,867 main.py:52] epoch 2429, training loss: 7460.67, average training loss: 7998.81, base loss: 15356.21
[INFO 2017-07-01 21:32:10,058 main.py:52] epoch 2430, training loss: 7411.00, average training loss: 7996.91, base loss: 15355.63
[INFO 2017-07-01 21:32:14,293 main.py:52] epoch 2431, training loss: 7926.91, average training loss: 7996.87, base loss: 15355.15
[INFO 2017-07-01 21:32:18,386 main.py:52] epoch 2432, training loss: 7775.09, average training loss: 7995.66, base loss: 15354.33
[INFO 2017-07-01 21:32:22,550 main.py:52] epoch 2433, training loss: 8481.94, average training loss: 7996.47, base loss: 15354.98
[INFO 2017-07-01 21:32:26,733 main.py:52] epoch 2434, training loss: 7537.78, average training loss: 7995.58, base loss: 15354.54
[INFO 2017-07-01 21:32:30,873 main.py:52] epoch 2435, training loss: 7421.70, average training loss: 7994.72, base loss: 15353.67
[INFO 2017-07-01 21:32:35,122 main.py:52] epoch 2436, training loss: 7782.16, average training loss: 7993.94, base loss: 15353.99
[INFO 2017-07-01 21:32:39,260 main.py:52] epoch 2437, training loss: 7810.70, average training loss: 7993.02, base loss: 15355.08
[INFO 2017-07-01 21:32:43,458 main.py:52] epoch 2438, training loss: 8019.61, average training loss: 7992.62, base loss: 15354.57
[INFO 2017-07-01 21:32:47,724 main.py:52] epoch 2439, training loss: 8289.87, average training loss: 7992.95, base loss: 15354.42
[INFO 2017-07-01 21:32:51,920 main.py:52] epoch 2440, training loss: 8183.50, average training loss: 7992.57, base loss: 15355.32
[INFO 2017-07-01 21:32:56,099 main.py:52] epoch 2441, training loss: 8042.40, average training loss: 7992.37, base loss: 15356.27
[INFO 2017-07-01 21:33:00,271 main.py:52] epoch 2442, training loss: 7973.44, average training loss: 7992.48, base loss: 15357.11
[INFO 2017-07-01 21:33:04,477 main.py:52] epoch 2443, training loss: 7652.10, average training loss: 7991.60, base loss: 15356.45
[INFO 2017-07-01 21:33:08,585 main.py:52] epoch 2444, training loss: 7044.76, average training loss: 7989.95, base loss: 15356.11
[INFO 2017-07-01 21:33:12,788 main.py:52] epoch 2445, training loss: 8209.50, average training loss: 7989.18, base loss: 15356.34
[INFO 2017-07-01 21:33:16,958 main.py:52] epoch 2446, training loss: 8905.96, average training loss: 7989.12, base loss: 15358.09
[INFO 2017-07-01 21:33:21,113 main.py:52] epoch 2447, training loss: 7591.71, average training loss: 7988.06, base loss: 15358.30
[INFO 2017-07-01 21:33:25,268 main.py:52] epoch 2448, training loss: 7516.90, average training loss: 7988.19, base loss: 15358.95
[INFO 2017-07-01 21:33:29,354 main.py:52] epoch 2449, training loss: 7948.02, average training loss: 7989.07, base loss: 15359.35
[INFO 2017-07-01 21:33:33,552 main.py:52] epoch 2450, training loss: 7079.79, average training loss: 7986.89, base loss: 15358.13
[INFO 2017-07-01 21:33:37,770 main.py:52] epoch 2451, training loss: 8216.06, average training loss: 7986.42, base loss: 15358.75
[INFO 2017-07-01 21:33:42,037 main.py:52] epoch 2452, training loss: 8517.75, average training loss: 7986.79, base loss: 15359.28
[INFO 2017-07-01 21:33:46,242 main.py:52] epoch 2453, training loss: 7488.46, average training loss: 7987.51, base loss: 15358.46
[INFO 2017-07-01 21:33:50,384 main.py:52] epoch 2454, training loss: 8024.63, average training loss: 7987.97, base loss: 15359.05
[INFO 2017-07-01 21:33:54,543 main.py:52] epoch 2455, training loss: 8006.38, average training loss: 7988.28, base loss: 15359.58
[INFO 2017-07-01 21:33:58,710 main.py:52] epoch 2456, training loss: 7927.73, average training loss: 7988.79, base loss: 15358.42
[INFO 2017-07-01 21:34:02,856 main.py:52] epoch 2457, training loss: 7836.73, average training loss: 7989.28, base loss: 15358.56
[INFO 2017-07-01 21:34:06,963 main.py:52] epoch 2458, training loss: 7568.47, average training loss: 7988.31, base loss: 15357.62
[INFO 2017-07-01 21:34:11,089 main.py:52] epoch 2459, training loss: 7162.94, average training loss: 7986.90, base loss: 15356.20
[INFO 2017-07-01 21:34:15,305 main.py:52] epoch 2460, training loss: 7694.27, average training loss: 7985.98, base loss: 15356.69
[INFO 2017-07-01 21:34:19,498 main.py:52] epoch 2461, training loss: 8120.55, average training loss: 7985.91, base loss: 15357.48
[INFO 2017-07-01 21:34:23,657 main.py:52] epoch 2462, training loss: 8947.32, average training loss: 7986.81, base loss: 15357.90
[INFO 2017-07-01 21:34:27,906 main.py:52] epoch 2463, training loss: 7118.61, average training loss: 7985.07, base loss: 15356.48
[INFO 2017-07-01 21:34:32,024 main.py:52] epoch 2464, training loss: 8310.55, average training loss: 7985.89, base loss: 15356.05
[INFO 2017-07-01 21:34:36,238 main.py:52] epoch 2465, training loss: 8285.91, average training loss: 7986.75, base loss: 15356.28
[INFO 2017-07-01 21:34:40,427 main.py:52] epoch 2466, training loss: 8389.15, average training loss: 7987.03, base loss: 15356.83
[INFO 2017-07-01 21:34:44,574 main.py:52] epoch 2467, training loss: 7474.61, average training loss: 7986.03, base loss: 15357.01
[INFO 2017-07-01 21:34:48,736 main.py:52] epoch 2468, training loss: 7753.81, average training loss: 7986.21, base loss: 15357.30
[INFO 2017-07-01 21:34:52,889 main.py:52] epoch 2469, training loss: 8145.38, average training loss: 7986.28, base loss: 15357.27
[INFO 2017-07-01 21:34:57,036 main.py:52] epoch 2470, training loss: 8969.78, average training loss: 7987.27, base loss: 15356.98
[INFO 2017-07-01 21:35:01,228 main.py:52] epoch 2471, training loss: 8633.19, average training loss: 7988.70, base loss: 15356.89
[INFO 2017-07-01 21:35:05,338 main.py:52] epoch 2472, training loss: 8157.95, average training loss: 7989.97, base loss: 15358.06
[INFO 2017-07-01 21:35:09,577 main.py:52] epoch 2473, training loss: 7517.07, average training loss: 7990.00, base loss: 15357.94
[INFO 2017-07-01 21:35:13,677 main.py:52] epoch 2474, training loss: 7912.56, average training loss: 7990.05, base loss: 15357.88
[INFO 2017-07-01 21:35:17,780 main.py:52] epoch 2475, training loss: 8203.18, average training loss: 7989.75, base loss: 15357.57
[INFO 2017-07-01 21:35:21,909 main.py:52] epoch 2476, training loss: 7995.20, average training loss: 7989.65, base loss: 15356.68
[INFO 2017-07-01 21:35:26,050 main.py:52] epoch 2477, training loss: 7322.14, average training loss: 7988.78, base loss: 15355.63
[INFO 2017-07-01 21:35:30,206 main.py:52] epoch 2478, training loss: 7485.35, average training loss: 7988.22, base loss: 15355.34
[INFO 2017-07-01 21:35:34,391 main.py:52] epoch 2479, training loss: 7370.51, average training loss: 7987.86, base loss: 15355.08
[INFO 2017-07-01 21:35:38,565 main.py:52] epoch 2480, training loss: 8415.36, average training loss: 7987.76, base loss: 15355.38
[INFO 2017-07-01 21:35:42,699 main.py:52] epoch 2481, training loss: 7797.58, average training loss: 7987.17, base loss: 15354.68
[INFO 2017-07-01 21:35:46,831 main.py:52] epoch 2482, training loss: 7510.37, average training loss: 7987.18, base loss: 15353.91
[INFO 2017-07-01 21:35:51,044 main.py:52] epoch 2483, training loss: 7548.08, average training loss: 7986.97, base loss: 15353.95
[INFO 2017-07-01 21:35:55,234 main.py:52] epoch 2484, training loss: 7557.12, average training loss: 7986.99, base loss: 15353.48
[INFO 2017-07-01 21:35:59,407 main.py:52] epoch 2485, training loss: 7455.86, average training loss: 7985.25, base loss: 15353.70
[INFO 2017-07-01 21:36:03,538 main.py:52] epoch 2486, training loss: 7998.17, average training loss: 7984.41, base loss: 15352.57
[INFO 2017-07-01 21:36:07,743 main.py:52] epoch 2487, training loss: 8476.39, average training loss: 7985.29, base loss: 15353.71
[INFO 2017-07-01 21:36:11,961 main.py:52] epoch 2488, training loss: 7559.79, average training loss: 7985.13, base loss: 15352.63
[INFO 2017-07-01 21:36:16,099 main.py:52] epoch 2489, training loss: 7306.29, average training loss: 7984.38, base loss: 15352.38
[INFO 2017-07-01 21:36:20,288 main.py:52] epoch 2490, training loss: 7387.70, average training loss: 7984.42, base loss: 15351.92
[INFO 2017-07-01 21:36:24,443 main.py:52] epoch 2491, training loss: 8074.15, average training loss: 7984.10, base loss: 15352.69
[INFO 2017-07-01 21:36:28,633 main.py:52] epoch 2492, training loss: 7054.76, average training loss: 7983.47, base loss: 15351.19
[INFO 2017-07-01 21:36:32,749 main.py:52] epoch 2493, training loss: 8262.30, average training loss: 7983.67, base loss: 15351.91
[INFO 2017-07-01 21:36:36,932 main.py:52] epoch 2494, training loss: 8420.32, average training loss: 7983.70, base loss: 15352.67
[INFO 2017-07-01 21:36:41,132 main.py:52] epoch 2495, training loss: 8192.85, average training loss: 7984.47, base loss: 15353.16
[INFO 2017-07-01 21:36:45,266 main.py:52] epoch 2496, training loss: 7417.05, average training loss: 7984.00, base loss: 15353.20
[INFO 2017-07-01 21:36:49,395 main.py:52] epoch 2497, training loss: 6986.30, average training loss: 7982.59, base loss: 15351.99
[INFO 2017-07-01 21:36:53,569 main.py:52] epoch 2498, training loss: 8473.90, average training loss: 7983.97, base loss: 15353.32
[INFO 2017-07-01 21:36:57,748 main.py:52] epoch 2499, training loss: 7552.32, average training loss: 7982.85, base loss: 15353.05
[INFO 2017-07-01 21:36:57,748 main.py:54] epoch 2499, testing
[INFO 2017-07-01 21:36:57,748 main.py:61] model save to ./model/final.pth
[INFO 2017-07-01 21:37:01,975 main.py:52] epoch 2500, training loss: 8226.80, average training loss: 7983.32, base loss: 15353.05
[INFO 2017-07-01 21:37:06,151 main.py:52] epoch 2501, training loss: 7918.90, average training loss: 7983.44, base loss: 15353.18
[INFO 2017-07-01 21:37:10,336 main.py:52] epoch 2502, training loss: 7275.58, average training loss: 7982.37, base loss: 15353.33
[INFO 2017-07-01 21:37:14,502 main.py:52] epoch 2503, training loss: 8034.36, average training loss: 7981.86, base loss: 15353.64
[INFO 2017-07-01 21:37:18,694 main.py:52] epoch 2504, training loss: 8652.85, average training loss: 7981.75, base loss: 15355.26
[INFO 2017-07-01 21:37:22,813 main.py:52] epoch 2505, training loss: 8098.50, average training loss: 7982.30, base loss: 15354.77
[INFO 2017-07-01 21:37:26,932 main.py:52] epoch 2506, training loss: 8272.97, average training loss: 7982.51, base loss: 15355.09
[INFO 2017-07-01 21:37:31,116 main.py:52] epoch 2507, training loss: 8116.82, average training loss: 7982.09, base loss: 15355.10
[INFO 2017-07-01 21:37:35,311 main.py:52] epoch 2508, training loss: 8101.27, average training loss: 7981.99, base loss: 15355.92
[INFO 2017-07-01 21:37:39,508 main.py:52] epoch 2509, training loss: 8494.79, average training loss: 7982.17, base loss: 15356.84
[INFO 2017-07-01 21:37:43,600 main.py:52] epoch 2510, training loss: 7803.77, average training loss: 7982.96, base loss: 15355.43
[INFO 2017-07-01 21:37:47,902 main.py:52] epoch 2511, training loss: 7741.25, average training loss: 7983.02, base loss: 15354.17
[INFO 2017-07-01 21:37:52,127 main.py:52] epoch 2512, training loss: 7159.53, average training loss: 7982.12, base loss: 15354.31
[INFO 2017-07-01 21:37:56,364 main.py:52] epoch 2513, training loss: 7189.91, average training loss: 7980.65, base loss: 15354.46
[INFO 2017-07-01 21:38:00,537 main.py:52] epoch 2514, training loss: 8169.14, average training loss: 7980.94, base loss: 15355.24
[INFO 2017-07-01 21:38:04,732 main.py:52] epoch 2515, training loss: 7477.53, average training loss: 7979.38, base loss: 15355.57
[INFO 2017-07-01 21:38:08,909 main.py:52] epoch 2516, training loss: 8432.11, average training loss: 7979.37, base loss: 15357.14
[INFO 2017-07-01 21:38:13,059 main.py:52] epoch 2517, training loss: 7330.25, average training loss: 7979.36, base loss: 15356.16
[INFO 2017-07-01 21:38:17,205 main.py:52] epoch 2518, training loss: 8707.13, average training loss: 7979.64, base loss: 15356.59
[INFO 2017-07-01 21:38:21,365 main.py:52] epoch 2519, training loss: 7042.37, average training loss: 7978.81, base loss: 15355.41
[INFO 2017-07-01 21:38:25,552 main.py:52] epoch 2520, training loss: 7920.34, average training loss: 7979.03, base loss: 15356.20
[INFO 2017-07-01 21:38:29,717 main.py:52] epoch 2521, training loss: 7682.67, average training loss: 7978.97, base loss: 15356.02
[INFO 2017-07-01 21:38:33,858 main.py:52] epoch 2522, training loss: 7392.03, average training loss: 7978.05, base loss: 15355.79
[INFO 2017-07-01 21:38:38,033 main.py:52] epoch 2523, training loss: 8798.38, average training loss: 7978.76, base loss: 15356.73
[INFO 2017-07-01 21:38:42,225 main.py:52] epoch 2524, training loss: 8633.11, average training loss: 7978.82, base loss: 15357.02
[INFO 2017-07-01 21:38:46,397 main.py:52] epoch 2525, training loss: 7196.65, average training loss: 7978.31, base loss: 15356.60
[INFO 2017-07-01 21:38:50,642 main.py:52] epoch 2526, training loss: 7587.17, average training loss: 7977.33, base loss: 15356.33
[INFO 2017-07-01 21:38:54,830 main.py:52] epoch 2527, training loss: 7450.04, average training loss: 7976.72, base loss: 15355.85
[INFO 2017-07-01 21:38:58,994 main.py:52] epoch 2528, training loss: 9895.33, average training loss: 7978.59, base loss: 15357.03
[INFO 2017-07-01 21:39:03,191 main.py:52] epoch 2529, training loss: 7031.05, average training loss: 7977.79, base loss: 15356.30
[INFO 2017-07-01 21:39:07,392 main.py:52] epoch 2530, training loss: 7350.62, average training loss: 7977.18, base loss: 15355.82
[INFO 2017-07-01 21:39:11,549 main.py:52] epoch 2531, training loss: 7717.23, average training loss: 7976.80, base loss: 15355.61
[INFO 2017-07-01 21:39:15,753 main.py:52] epoch 2532, training loss: 8473.87, average training loss: 7974.17, base loss: 15356.33
[INFO 2017-07-01 21:39:19,900 main.py:52] epoch 2533, training loss: 8119.60, average training loss: 7973.82, base loss: 15357.07
[INFO 2017-07-01 21:39:24,042 main.py:52] epoch 2534, training loss: 7726.12, average training loss: 7972.97, base loss: 15358.35
[INFO 2017-07-01 21:39:28,157 main.py:52] epoch 2535, training loss: 8175.91, average training loss: 7972.12, base loss: 15360.15
[INFO 2017-07-01 21:39:32,332 main.py:52] epoch 2536, training loss: 8017.13, average training loss: 7971.06, base loss: 15360.00
[INFO 2017-07-01 21:39:36,486 main.py:52] epoch 2537, training loss: 7676.76, average training loss: 7971.00, base loss: 15359.42
[INFO 2017-07-01 21:39:40,674 main.py:52] epoch 2538, training loss: 8095.72, average training loss: 7970.63, base loss: 15359.66
[INFO 2017-07-01 21:39:44,781 main.py:52] epoch 2539, training loss: 7790.48, average training loss: 7969.42, base loss: 15359.64
[INFO 2017-07-01 21:39:49,063 main.py:52] epoch 2540, training loss: 8481.85, average training loss: 7969.94, base loss: 15360.11
[INFO 2017-07-01 21:39:53,222 main.py:52] epoch 2541, training loss: 8491.08, average training loss: 7969.04, base loss: 15360.84
[INFO 2017-07-01 21:39:57,340 main.py:52] epoch 2542, training loss: 7795.82, average training loss: 7968.98, base loss: 15361.01
[INFO 2017-07-01 21:40:01,449 main.py:52] epoch 2543, training loss: 8512.62, average training loss: 7969.51, base loss: 15362.51
[INFO 2017-07-01 21:40:05,635 main.py:52] epoch 2544, training loss: 7428.39, average training loss: 7969.12, base loss: 15361.91
[INFO 2017-07-01 21:40:09,798 main.py:52] epoch 2545, training loss: 8158.28, average training loss: 7969.18, base loss: 15362.78
[INFO 2017-07-01 21:40:14,044 main.py:52] epoch 2546, training loss: 8080.52, average training loss: 7969.79, base loss: 15363.49
[INFO 2017-07-01 21:40:18,206 main.py:52] epoch 2547, training loss: 7170.46, average training loss: 7968.84, base loss: 15362.94
[INFO 2017-07-01 21:40:22,394 main.py:52] epoch 2548, training loss: 8040.54, average training loss: 7968.51, base loss: 15362.93
[INFO 2017-07-01 21:40:26,636 main.py:52] epoch 2549, training loss: 7376.62, average training loss: 7968.37, base loss: 15361.88
[INFO 2017-07-01 21:40:30,800 main.py:52] epoch 2550, training loss: 7553.16, average training loss: 7967.39, base loss: 15360.70
[INFO 2017-07-01 21:40:34,964 main.py:52] epoch 2551, training loss: 7636.76, average training loss: 7966.73, base loss: 15360.49
[INFO 2017-07-01 21:40:39,214 main.py:52] epoch 2552, training loss: 8085.87, average training loss: 7967.20, base loss: 15361.82
[INFO 2017-07-01 21:40:43,401 main.py:52] epoch 2553, training loss: 8250.57, average training loss: 7967.08, base loss: 15363.14
[INFO 2017-07-01 21:40:47,624 main.py:52] epoch 2554, training loss: 7548.25, average training loss: 7967.03, base loss: 15364.04
[INFO 2017-07-01 21:40:51,740 main.py:52] epoch 2555, training loss: 8578.33, average training loss: 7966.65, base loss: 15365.16
[INFO 2017-07-01 21:40:55,895 main.py:52] epoch 2556, training loss: 6901.35, average training loss: 7966.31, base loss: 15363.95
[INFO 2017-07-01 21:41:00,102 main.py:52] epoch 2557, training loss: 7723.69, average training loss: 7965.67, base loss: 15362.94
[INFO 2017-07-01 21:41:04,284 main.py:52] epoch 2558, training loss: 7481.52, average training loss: 7965.42, base loss: 15362.00
[INFO 2017-07-01 21:41:08,499 main.py:52] epoch 2559, training loss: 7633.84, average training loss: 7964.32, base loss: 15361.95
[INFO 2017-07-01 21:41:12,662 main.py:52] epoch 2560, training loss: 8072.59, average training loss: 7962.45, base loss: 15362.21
[INFO 2017-07-01 21:41:16,798 main.py:52] epoch 2561, training loss: 8525.65, average training loss: 7963.51, base loss: 15362.41
[INFO 2017-07-01 21:41:20,951 main.py:52] epoch 2562, training loss: 7588.02, average training loss: 7962.49, base loss: 15363.07
[INFO 2017-07-01 21:41:25,130 main.py:52] epoch 2563, training loss: 7858.90, average training loss: 7963.46, base loss: 15363.37
[INFO 2017-07-01 21:41:29,314 main.py:52] epoch 2564, training loss: 7852.51, average training loss: 7962.98, base loss: 15363.81
[INFO 2017-07-01 21:41:33,480 main.py:52] epoch 2565, training loss: 7310.87, average training loss: 7962.19, base loss: 15363.97
[INFO 2017-07-01 21:41:37,619 main.py:52] epoch 2566, training loss: 8598.16, average training loss: 7962.26, base loss: 15365.63
[INFO 2017-07-01 21:41:41,802 main.py:52] epoch 2567, training loss: 8369.79, average training loss: 7962.91, base loss: 15366.33
[INFO 2017-07-01 21:41:45,931 main.py:52] epoch 2568, training loss: 8158.41, average training loss: 7962.94, base loss: 15367.20
[INFO 2017-07-01 21:41:50,060 main.py:52] epoch 2569, training loss: 8524.39, average training loss: 7963.82, base loss: 15369.51
[INFO 2017-07-01 21:41:54,207 main.py:52] epoch 2570, training loss: 7584.83, average training loss: 7963.74, base loss: 15368.44
[INFO 2017-07-01 21:41:58,387 main.py:52] epoch 2571, training loss: 8719.40, average training loss: 7965.06, base loss: 15369.48
[INFO 2017-07-01 21:42:02,611 main.py:52] epoch 2572, training loss: 7409.96, average training loss: 7964.20, base loss: 15369.53
[INFO 2017-07-01 21:42:06,722 main.py:52] epoch 2573, training loss: 7343.43, average training loss: 7963.24, base loss: 15368.41
[INFO 2017-07-01 21:42:10,924 main.py:52] epoch 2574, training loss: 7825.76, average training loss: 7962.45, base loss: 15369.60
[INFO 2017-07-01 21:42:15,035 main.py:52] epoch 2575, training loss: 7803.20, average training loss: 7961.90, base loss: 15370.05
[INFO 2017-07-01 21:42:19,141 main.py:52] epoch 2576, training loss: 7606.84, average training loss: 7961.32, base loss: 15369.75
[INFO 2017-07-01 21:42:23,284 main.py:52] epoch 2577, training loss: 7912.76, average training loss: 7961.87, base loss: 15369.45
[INFO 2017-07-01 21:42:27,550 main.py:52] epoch 2578, training loss: 7614.82, average training loss: 7961.64, base loss: 15368.51
[INFO 2017-07-01 21:42:31,744 main.py:52] epoch 2579, training loss: 8578.96, average training loss: 7962.66, base loss: 15368.63
[INFO 2017-07-01 21:42:35,905 main.py:52] epoch 2580, training loss: 7871.54, average training loss: 7961.78, base loss: 15368.76
[INFO 2017-07-01 21:42:40,128 main.py:52] epoch 2581, training loss: 7697.70, average training loss: 7961.21, base loss: 15368.82
[INFO 2017-07-01 21:42:44,309 main.py:52] epoch 2582, training loss: 7632.05, average training loss: 7960.55, base loss: 15368.46
[INFO 2017-07-01 21:42:48,523 main.py:52] epoch 2583, training loss: 7954.14, average training loss: 7960.17, base loss: 15368.16
[INFO 2017-07-01 21:42:52,714 main.py:52] epoch 2584, training loss: 7833.73, average training loss: 7959.79, base loss: 15367.19
[INFO 2017-07-01 21:42:56,865 main.py:52] epoch 2585, training loss: 8019.50, average training loss: 7960.54, base loss: 15367.04
[INFO 2017-07-01 21:43:01,055 main.py:52] epoch 2586, training loss: 8590.29, average training loss: 7960.63, base loss: 15367.55
[INFO 2017-07-01 21:43:05,169 main.py:52] epoch 2587, training loss: 7962.69, average training loss: 7959.46, base loss: 15368.29
[INFO 2017-07-01 21:43:09,355 main.py:52] epoch 2588, training loss: 7609.29, average training loss: 7958.48, base loss: 15368.38
[INFO 2017-07-01 21:43:13,577 main.py:52] epoch 2589, training loss: 8153.68, average training loss: 7959.22, base loss: 15368.25
[INFO 2017-07-01 21:43:17,706 main.py:52] epoch 2590, training loss: 7781.26, average training loss: 7958.72, base loss: 15368.03
[INFO 2017-07-01 21:43:21,837 main.py:52] epoch 2591, training loss: 7028.95, average training loss: 7958.60, base loss: 15366.56
[INFO 2017-07-01 21:43:26,003 main.py:52] epoch 2592, training loss: 7990.11, average training loss: 7958.40, base loss: 15366.86
[INFO 2017-07-01 21:43:30,193 main.py:52] epoch 2593, training loss: 8499.67, average training loss: 7958.62, base loss: 15368.25
[INFO 2017-07-01 21:43:34,354 main.py:52] epoch 2594, training loss: 7942.30, average training loss: 7958.71, base loss: 15369.44
[INFO 2017-07-01 21:43:38,529 main.py:52] epoch 2595, training loss: 7735.02, average training loss: 7958.12, base loss: 15370.24
[INFO 2017-07-01 21:43:42,685 main.py:52] epoch 2596, training loss: 8303.76, average training loss: 7958.43, base loss: 15370.13
[INFO 2017-07-01 21:43:46,876 main.py:52] epoch 2597, training loss: 7485.47, average training loss: 7957.07, base loss: 15370.39
[INFO 2017-07-01 21:43:51,059 main.py:52] epoch 2598, training loss: 7801.75, average training loss: 7957.19, base loss: 15369.90
[INFO 2017-07-01 21:43:55,286 main.py:52] epoch 2599, training loss: 6913.97, average training loss: 7955.09, base loss: 15369.10
[INFO 2017-07-01 21:43:55,286 main.py:54] epoch 2599, testing
[INFO 2017-07-01 21:43:55,286 main.py:61] model save to ./model/final.pth
[INFO 2017-07-01 21:43:59,559 main.py:52] epoch 2600, training loss: 7859.01, average training loss: 7954.21, base loss: 15368.34
[INFO 2017-07-01 21:44:03,716 main.py:52] epoch 2601, training loss: 7686.49, average training loss: 7953.54, base loss: 15367.81
[INFO 2017-07-01 21:44:07,858 main.py:52] epoch 2602, training loss: 7755.25, average training loss: 7953.95, base loss: 15367.79
[INFO 2017-07-01 21:44:12,058 main.py:52] epoch 2603, training loss: 8550.19, average training loss: 7955.06, base loss: 15369.36
[INFO 2017-07-01 21:44:16,261 main.py:52] epoch 2604, training loss: 7638.09, average training loss: 7953.78, base loss: 15368.52
[INFO 2017-07-01 21:44:20,451 main.py:52] epoch 2605, training loss: 7643.95, average training loss: 7953.15, base loss: 15368.38
[INFO 2017-07-01 21:44:24,579 main.py:52] epoch 2606, training loss: 8560.59, average training loss: 7953.07, base loss: 15368.69
[INFO 2017-07-01 21:44:28,673 main.py:52] epoch 2607, training loss: 7728.02, average training loss: 7952.80, base loss: 15367.76
[INFO 2017-07-01 21:44:32,846 main.py:52] epoch 2608, training loss: 7845.34, average training loss: 7952.81, base loss: 15366.37
[INFO 2017-07-01 21:44:37,014 main.py:52] epoch 2609, training loss: 8168.73, average training loss: 7952.70, base loss: 15365.41
[INFO 2017-07-01 21:44:41,218 main.py:52] epoch 2610, training loss: 7702.68, average training loss: 7952.18, base loss: 15365.68
[INFO 2017-07-01 21:44:45,429 main.py:52] epoch 2611, training loss: 7578.62, average training loss: 7951.00, base loss: 15364.73
[INFO 2017-07-01 21:44:49,646 main.py:52] epoch 2612, training loss: 7407.23, average training loss: 7950.39, base loss: 15364.80
[INFO 2017-07-01 21:44:53,902 main.py:52] epoch 2613, training loss: 7469.66, average training loss: 7950.13, base loss: 15365.09
[INFO 2017-07-01 21:44:58,065 main.py:52] epoch 2614, training loss: 8093.57, average training loss: 7950.38, base loss: 15365.81
[INFO 2017-07-01 21:45:02,228 main.py:52] epoch 2615, training loss: 8837.73, average training loss: 7950.05, base loss: 15367.33
[INFO 2017-07-01 21:45:06,359 main.py:52] epoch 2616, training loss: 7555.17, average training loss: 7949.95, base loss: 15366.77
[INFO 2017-07-01 21:45:10,548 main.py:52] epoch 2617, training loss: 7605.47, average training loss: 7948.91, base loss: 15366.62
[INFO 2017-07-01 21:45:14,643 main.py:52] epoch 2618, training loss: 8118.71, average training loss: 7949.90, base loss: 15366.71
[INFO 2017-07-01 21:45:18,859 main.py:52] epoch 2619, training loss: 7501.01, average training loss: 7949.72, base loss: 15366.26
[INFO 2017-07-01 21:45:23,040 main.py:52] epoch 2620, training loss: 8221.17, average training loss: 7950.71, base loss: 15367.05
[INFO 2017-07-01 21:45:27,202 main.py:52] epoch 2621, training loss: 7942.18, average training loss: 7951.64, base loss: 15367.04
[INFO 2017-07-01 21:45:31,354 main.py:52] epoch 2622, training loss: 7922.70, average training loss: 7951.68, base loss: 15367.58
[INFO 2017-07-01 21:45:35,555 main.py:52] epoch 2623, training loss: 6781.46, average training loss: 7951.45, base loss: 15367.82
[INFO 2017-07-01 21:45:39,777 main.py:52] epoch 2624, training loss: 8861.08, average training loss: 7951.33, base loss: 15368.62
[INFO 2017-07-01 21:45:43,954 main.py:52] epoch 2625, training loss: 7564.91, average training loss: 7950.71, base loss: 15367.24
[INFO 2017-07-01 21:45:48,126 main.py:52] epoch 2626, training loss: 8613.08, average training loss: 7951.12, base loss: 15367.60
[INFO 2017-07-01 21:45:52,318 main.py:52] epoch 2627, training loss: 8535.83, average training loss: 7951.18, base loss: 15367.55
[INFO 2017-07-01 21:45:56,409 main.py:52] epoch 2628, training loss: 8601.76, average training loss: 7951.51, base loss: 15368.73
[INFO 2017-07-01 21:46:00,545 main.py:52] epoch 2629, training loss: 7106.81, average training loss: 7950.55, base loss: 15368.53
[INFO 2017-07-01 21:46:04,751 main.py:52] epoch 2630, training loss: 7717.61, average training loss: 7950.36, base loss: 15368.05
[INFO 2017-07-01 21:46:08,952 main.py:52] epoch 2631, training loss: 8544.89, average training loss: 7951.84, base loss: 15367.71
[INFO 2017-07-01 21:46:13,150 main.py:52] epoch 2632, training loss: 7235.89, average training loss: 7950.87, base loss: 15366.95
[INFO 2017-07-01 21:46:17,286 main.py:52] epoch 2633, training loss: 7676.75, average training loss: 7950.68, base loss: 15366.51
[INFO 2017-07-01 21:46:21,458 main.py:52] epoch 2634, training loss: 8689.42, average training loss: 7949.90, base loss: 15366.29
[INFO 2017-07-01 21:46:25,646 main.py:52] epoch 2635, training loss: 8207.31, average training loss: 7950.12, base loss: 15366.33
[INFO 2017-07-01 21:46:29,823 main.py:52] epoch 2636, training loss: 8338.03, average training loss: 7950.19, base loss: 15366.56
[INFO 2017-07-01 21:46:34,000 main.py:52] epoch 2637, training loss: 8182.29, average training loss: 7949.46, base loss: 15366.94
[INFO 2017-07-01 21:46:38,268 main.py:52] epoch 2638, training loss: 8418.80, average training loss: 7950.19, base loss: 15367.68
[INFO 2017-07-01 21:46:42,456 main.py:52] epoch 2639, training loss: 8697.64, average training loss: 7950.53, base loss: 15368.51
[INFO 2017-07-01 21:46:46,597 main.py:52] epoch 2640, training loss: 8239.11, average training loss: 7950.09, base loss: 15368.31
[INFO 2017-07-01 21:46:50,804 main.py:52] epoch 2641, training loss: 7470.84, average training loss: 7949.73, base loss: 15367.70
[INFO 2017-07-01 21:46:54,966 main.py:52] epoch 2642, training loss: 7832.35, average training loss: 7949.07, base loss: 15367.47
[INFO 2017-07-01 21:46:59,053 main.py:52] epoch 2643, training loss: 7338.98, average training loss: 7947.94, base loss: 15366.34
[INFO 2017-07-01 21:47:03,284 main.py:52] epoch 2644, training loss: 7435.31, average training loss: 7947.64, base loss: 15365.51
[INFO 2017-07-01 21:47:07,448 main.py:52] epoch 2645, training loss: 8445.80, average training loss: 7947.36, base loss: 15366.38
[INFO 2017-07-01 21:47:11,666 main.py:52] epoch 2646, training loss: 7447.34, average training loss: 7946.28, base loss: 15365.43
[INFO 2017-07-01 21:47:15,815 main.py:52] epoch 2647, training loss: 8528.10, average training loss: 7946.63, base loss: 15366.05
[INFO 2017-07-01 21:47:19,979 main.py:52] epoch 2648, training loss: 8089.51, average training loss: 7946.60, base loss: 15366.57
[INFO 2017-07-01 21:47:24,145 main.py:52] epoch 2649, training loss: 7332.87, average training loss: 7946.18, base loss: 15366.63
[INFO 2017-07-01 21:47:28,268 main.py:52] epoch 2650, training loss: 8142.58, average training loss: 7946.02, base loss: 15367.64
[INFO 2017-07-01 21:47:32,459 main.py:52] epoch 2651, training loss: 7912.55, average training loss: 7945.73, base loss: 15367.92
[INFO 2017-07-01 21:47:36,654 main.py:52] epoch 2652, training loss: 8594.11, average training loss: 7946.08, base loss: 15369.17
[INFO 2017-07-01 21:47:40,813 main.py:52] epoch 2653, training loss: 8199.58, average training loss: 7945.64, base loss: 15369.82
[INFO 2017-07-01 21:47:44,955 main.py:52] epoch 2654, training loss: 7094.94, average training loss: 7945.69, base loss: 15368.63
[INFO 2017-07-01 21:47:49,043 main.py:52] epoch 2655, training loss: 8017.82, average training loss: 7946.07, base loss: 15368.95
[INFO 2017-07-01 21:47:53,217 main.py:52] epoch 2656, training loss: 7725.92, average training loss: 7944.98, base loss: 15369.17
[INFO 2017-07-01 21:47:57,382 main.py:52] epoch 2657, training loss: 8307.47, average training loss: 7945.77, base loss: 15370.58
[INFO 2017-07-01 21:48:01,585 main.py:52] epoch 2658, training loss: 7853.94, average training loss: 7945.71, base loss: 15369.87
[INFO 2017-07-01 21:48:05,808 main.py:52] epoch 2659, training loss: 7915.24, average training loss: 7946.20, base loss: 15370.08
[INFO 2017-07-01 21:48:09,966 main.py:52] epoch 2660, training loss: 7117.59, average training loss: 7945.66, base loss: 15369.80
[INFO 2017-07-01 21:48:14,221 main.py:52] epoch 2661, training loss: 7413.35, average training loss: 7945.16, base loss: 15369.02
[INFO 2017-07-01 21:48:18,356 main.py:52] epoch 2662, training loss: 7618.16, average training loss: 7943.96, base loss: 15368.26
[INFO 2017-07-01 21:48:22,554 main.py:52] epoch 2663, training loss: 7715.52, average training loss: 7943.11, base loss: 15367.65
[INFO 2017-07-01 21:48:26,768 main.py:52] epoch 2664, training loss: 7797.80, average training loss: 7943.46, base loss: 15366.96
[INFO 2017-07-01 21:48:30,970 main.py:52] epoch 2665, training loss: 7595.78, average training loss: 7943.17, base loss: 15366.15
[INFO 2017-07-01 21:48:35,114 main.py:52] epoch 2666, training loss: 7397.88, average training loss: 7943.02, base loss: 15365.56
[INFO 2017-07-01 21:48:39,391 main.py:52] epoch 2667, training loss: 7565.73, average training loss: 7943.17, base loss: 15365.21
[INFO 2017-07-01 21:48:43,557 main.py:52] epoch 2668, training loss: 7567.44, average training loss: 7943.25, base loss: 15363.87
[INFO 2017-07-01 21:48:47,753 main.py:52] epoch 2669, training loss: 8322.83, average training loss: 7943.58, base loss: 15363.76
[INFO 2017-07-01 21:48:51,984 main.py:52] epoch 2670, training loss: 8216.82, average training loss: 7943.97, base loss: 15364.25
[INFO 2017-07-01 21:48:56,244 main.py:52] epoch 2671, training loss: 7236.74, average training loss: 7943.75, base loss: 15363.18
[INFO 2017-07-01 21:49:00,324 main.py:52] epoch 2672, training loss: 8212.24, average training loss: 7943.30, base loss: 15363.73
[INFO 2017-07-01 21:49:04,462 main.py:52] epoch 2673, training loss: 7850.13, average training loss: 7943.84, base loss: 15364.65
[INFO 2017-07-01 21:49:08,585 main.py:52] epoch 2674, training loss: 8159.25, average training loss: 7944.13, base loss: 15364.55
[INFO 2017-07-01 21:49:12,824 main.py:52] epoch 2675, training loss: 7547.38, average training loss: 7943.03, base loss: 15364.95
[INFO 2017-07-01 21:49:17,007 main.py:52] epoch 2676, training loss: 8621.62, average training loss: 7943.28, base loss: 15366.46
[INFO 2017-07-01 21:49:21,162 main.py:52] epoch 2677, training loss: 7248.17, average training loss: 7942.03, base loss: 15366.01
[INFO 2017-07-01 21:49:25,380 main.py:52] epoch 2678, training loss: 7028.68, average training loss: 7941.48, base loss: 15365.52
[INFO 2017-07-01 21:49:29,495 main.py:52] epoch 2679, training loss: 8031.55, average training loss: 7942.05, base loss: 15366.96
[INFO 2017-07-01 21:49:33,682 main.py:52] epoch 2680, training loss: 8283.68, average training loss: 7942.11, base loss: 15368.41
[INFO 2017-07-01 21:49:37,795 main.py:52] epoch 2681, training loss: 6926.76, average training loss: 7941.72, base loss: 15367.62
[INFO 2017-07-01 21:49:41,931 main.py:52] epoch 2682, training loss: 7769.09, average training loss: 7941.37, base loss: 15367.58
[INFO 2017-07-01 21:49:46,059 main.py:52] epoch 2683, training loss: 7736.22, average training loss: 7940.29, base loss: 15367.01
[INFO 2017-07-01 21:49:50,216 main.py:52] epoch 2684, training loss: 7120.67, average training loss: 7939.91, base loss: 15366.64
[INFO 2017-07-01 21:49:54,363 main.py:52] epoch 2685, training loss: 7459.43, average training loss: 7939.13, base loss: 15366.39
[INFO 2017-07-01 21:49:58,547 main.py:52] epoch 2686, training loss: 8873.44, average training loss: 7940.36, base loss: 15367.92
[INFO 2017-07-01 21:50:02,769 main.py:52] epoch 2687, training loss: 7762.54, average training loss: 7939.86, base loss: 15368.79
[INFO 2017-07-01 21:50:06,955 main.py:52] epoch 2688, training loss: 8361.31, average training loss: 7940.51, base loss: 15368.19
[INFO 2017-07-01 21:50:11,093 main.py:52] epoch 2689, training loss: 8405.72, average training loss: 7940.85, base loss: 15367.75
[INFO 2017-07-01 21:50:15,247 main.py:52] epoch 2690, training loss: 7825.38, average training loss: 7940.00, base loss: 15367.98
[INFO 2017-07-01 21:50:19,393 main.py:52] epoch 2691, training loss: 8639.47, average training loss: 7940.44, base loss: 15368.29
[INFO 2017-07-01 21:50:23,624 main.py:52] epoch 2692, training loss: 7617.54, average training loss: 7940.34, base loss: 15368.19
[INFO 2017-07-01 21:50:27,857 main.py:52] epoch 2693, training loss: 7358.27, average training loss: 7941.01, base loss: 15368.03
[INFO 2017-07-01 21:50:32,002 main.py:52] epoch 2694, training loss: 7281.40, average training loss: 7938.99, base loss: 15366.86
[INFO 2017-07-01 21:50:36,137 main.py:52] epoch 2695, training loss: 8089.21, average training loss: 7939.34, base loss: 15366.76
[INFO 2017-07-01 21:50:40,339 main.py:52] epoch 2696, training loss: 7822.00, average training loss: 7939.39, base loss: 15366.83
[INFO 2017-07-01 21:50:44,527 main.py:52] epoch 2697, training loss: 6842.17, average training loss: 7937.84, base loss: 15366.04
[INFO 2017-07-01 21:50:48,691 main.py:52] epoch 2698, training loss: 7611.52, average training loss: 7937.14, base loss: 15365.08
[INFO 2017-07-01 21:50:52,841 main.py:52] epoch 2699, training loss: 7134.91, average training loss: 7935.62, base loss: 15364.64
[INFO 2017-07-01 21:50:52,842 main.py:54] epoch 2699, testing
[INFO 2017-07-01 21:50:52,842 main.py:61] model save to ./model/final.pth
[INFO 2017-07-01 21:50:57,082 main.py:52] epoch 2700, training loss: 8548.55, average training loss: 7935.95, base loss: 15365.68
[INFO 2017-07-01 21:51:01,274 main.py:52] epoch 2701, training loss: 7410.82, average training loss: 7935.87, base loss: 15365.49
[INFO 2017-07-01 21:51:05,348 main.py:52] epoch 2702, training loss: 9116.39, average training loss: 7937.17, base loss: 15367.13
[INFO 2017-07-01 21:51:09,545 main.py:52] epoch 2703, training loss: 7519.32, average training loss: 7936.57, base loss: 15366.98
[INFO 2017-07-01 21:51:13,775 main.py:52] epoch 2704, training loss: 8094.22, average training loss: 7936.77, base loss: 15366.52
[INFO 2017-07-01 21:51:17,977 main.py:52] epoch 2705, training loss: 8073.39, average training loss: 7937.34, base loss: 15366.22
[INFO 2017-07-01 21:51:22,143 main.py:52] epoch 2706, training loss: 6829.31, average training loss: 7936.09, base loss: 15365.07
[INFO 2017-07-01 21:51:26,342 main.py:52] epoch 2707, training loss: 7789.20, average training loss: 7936.34, base loss: 15364.66
[INFO 2017-07-01 21:51:30,511 main.py:52] epoch 2708, training loss: 7565.89, average training loss: 7935.11, base loss: 15363.52
[INFO 2017-07-01 21:51:34,720 main.py:52] epoch 2709, training loss: 7624.00, average training loss: 7934.18, base loss: 15362.58
[INFO 2017-07-01 21:51:38,844 main.py:52] epoch 2710, training loss: 7572.50, average training loss: 7933.92, base loss: 15362.11
[INFO 2017-07-01 21:51:43,054 main.py:52] epoch 2711, training loss: 7838.68, average training loss: 7934.10, base loss: 15362.13
[INFO 2017-07-01 21:51:47,183 main.py:52] epoch 2712, training loss: 8680.19, average training loss: 7934.75, base loss: 15363.50
[INFO 2017-07-01 21:51:51,448 main.py:52] epoch 2713, training loss: 8139.65, average training loss: 7933.27, base loss: 15364.26
[INFO 2017-07-01 21:51:55,613 main.py:52] epoch 2714, training loss: 7189.43, average training loss: 7933.46, base loss: 15363.57
[INFO 2017-07-01 21:51:59,782 main.py:52] epoch 2715, training loss: 7833.06, average training loss: 7934.07, base loss: 15363.82
[INFO 2017-07-01 21:52:03,935 main.py:52] epoch 2716, training loss: 9598.00, average training loss: 7935.15, base loss: 15364.94
[INFO 2017-07-01 21:52:08,119 main.py:52] epoch 2717, training loss: 7184.02, average training loss: 7934.58, base loss: 15364.04
[INFO 2017-07-01 21:52:12,227 main.py:52] epoch 2718, training loss: 8439.64, average training loss: 7934.90, base loss: 15365.16
[INFO 2017-07-01 21:52:16,376 main.py:52] epoch 2719, training loss: 7637.80, average training loss: 7935.07, base loss: 15364.97
[INFO 2017-07-01 21:52:20,481 main.py:52] epoch 2720, training loss: 7379.41, average training loss: 7933.95, base loss: 15364.53
[INFO 2017-07-01 21:52:24,596 main.py:52] epoch 2721, training loss: 7244.72, average training loss: 7932.55, base loss: 15363.74
[INFO 2017-07-01 21:52:28,764 main.py:52] epoch 2722, training loss: 7406.60, average training loss: 7932.33, base loss: 15362.98
[INFO 2017-07-01 21:52:32,906 main.py:52] epoch 2723, training loss: 8383.14, average training loss: 7933.24, base loss: 15364.29
[INFO 2017-07-01 21:52:37,144 main.py:52] epoch 2724, training loss: 6953.49, average training loss: 7931.56, base loss: 15363.49
[INFO 2017-07-01 21:52:41,317 main.py:52] epoch 2725, training loss: 7235.33, average training loss: 7931.00, base loss: 15362.52
[INFO 2017-07-01 21:52:45,419 main.py:52] epoch 2726, training loss: 7551.19, average training loss: 7930.50, base loss: 15362.13
[INFO 2017-07-01 21:52:49,634 main.py:52] epoch 2727, training loss: 7631.71, average training loss: 7930.58, base loss: 15361.57
[INFO 2017-07-01 21:52:53,734 main.py:52] epoch 2728, training loss: 7221.91, average training loss: 7930.31, base loss: 15360.85
[INFO 2017-07-01 21:52:57,906 main.py:52] epoch 2729, training loss: 8881.36, average training loss: 7931.40, base loss: 15362.16
[INFO 2017-07-01 21:53:02,085 main.py:52] epoch 2730, training loss: 6997.36, average training loss: 7930.81, base loss: 15361.40
[INFO 2017-07-01 21:53:06,204 main.py:52] epoch 2731, training loss: 8604.63, average training loss: 7931.37, base loss: 15362.38
[INFO 2017-07-01 21:53:10,321 main.py:52] epoch 2732, training loss: 7716.68, average training loss: 7930.88, base loss: 15361.88
[INFO 2017-07-01 21:53:14,465 main.py:52] epoch 2733, training loss: 9300.72, average training loss: 7931.86, base loss: 15363.07
[INFO 2017-07-01 21:53:18,616 main.py:52] epoch 2734, training loss: 7330.35, average training loss: 7931.86, base loss: 15362.05
[INFO 2017-07-01 21:53:22,799 main.py:52] epoch 2735, training loss: 8545.60, average training loss: 7932.53, base loss: 15362.05
[INFO 2017-07-01 21:53:26,951 main.py:52] epoch 2736, training loss: 7607.81, average training loss: 7932.25, base loss: 15361.80
[INFO 2017-07-01 21:53:31,146 main.py:52] epoch 2737, training loss: 7657.88, average training loss: 7932.19, base loss: 15362.00
[INFO 2017-07-01 21:53:35,320 main.py:52] epoch 2738, training loss: 7109.81, average training loss: 7931.74, base loss: 15361.59
[INFO 2017-07-01 21:53:39,491 main.py:52] epoch 2739, training loss: 7802.67, average training loss: 7930.85, base loss: 15362.02
[INFO 2017-07-01 21:53:43,603 main.py:52] epoch 2740, training loss: 8211.63, average training loss: 7931.42, base loss: 15363.07
[INFO 2017-07-01 21:53:47,749 main.py:52] epoch 2741, training loss: 6717.38, average training loss: 7930.61, base loss: 15362.06
[INFO 2017-07-01 21:53:51,970 main.py:52] epoch 2742, training loss: 9113.25, average training loss: 7931.97, base loss: 15362.05
[INFO 2017-07-01 21:53:56,081 main.py:52] epoch 2743, training loss: 7036.82, average training loss: 7931.68, base loss: 15360.19
[INFO 2017-07-01 21:54:00,236 main.py:52] epoch 2744, training loss: 8521.91, average training loss: 7932.59, base loss: 15360.47
[INFO 2017-07-01 21:54:04,486 main.py:52] epoch 2745, training loss: 7550.00, average training loss: 7932.90, base loss: 15359.49
[INFO 2017-07-01 21:54:08,610 main.py:52] epoch 2746, training loss: 7851.72, average training loss: 7932.99, base loss: 15359.40
[INFO 2017-07-01 21:54:12,713 main.py:52] epoch 2747, training loss: 8403.60, average training loss: 7932.82, base loss: 15360.03
[INFO 2017-07-01 21:54:16,884 main.py:52] epoch 2748, training loss: 7836.79, average training loss: 7932.05, base loss: 15360.70
[INFO 2017-07-01 21:54:21,065 main.py:52] epoch 2749, training loss: 7772.01, average training loss: 7931.78, base loss: 15361.85
[INFO 2017-07-01 21:54:25,300 main.py:52] epoch 2750, training loss: 7259.77, average training loss: 7930.43, base loss: 15360.48
[INFO 2017-07-01 21:54:29,417 main.py:52] epoch 2751, training loss: 7836.96, average training loss: 7930.90, base loss: 15360.50
[INFO 2017-07-01 21:54:33,602 main.py:52] epoch 2752, training loss: 7611.58, average training loss: 7930.08, base loss: 15359.32
[INFO 2017-07-01 21:54:37,808 main.py:52] epoch 2753, training loss: 7566.45, average training loss: 7929.13, base loss: 15359.04
[INFO 2017-07-01 21:54:41,952 main.py:52] epoch 2754, training loss: 7638.07, average training loss: 7927.85, base loss: 15359.19
[INFO 2017-07-01 21:54:46,094 main.py:52] epoch 2755, training loss: 8078.63, average training loss: 7927.76, base loss: 15359.67
[INFO 2017-07-01 21:54:50,258 main.py:52] epoch 2756, training loss: 8419.05, average training loss: 7927.65, base loss: 15360.45
[INFO 2017-07-01 21:54:54,444 main.py:52] epoch 2757, training loss: 6974.46, average training loss: 7926.95, base loss: 15359.32
[INFO 2017-07-01 21:54:58,622 main.py:52] epoch 2758, training loss: 7277.89, average training loss: 7925.55, base loss: 15358.11
[INFO 2017-07-01 21:55:02,816 main.py:52] epoch 2759, training loss: 8914.12, average training loss: 7926.03, base loss: 15358.12
[INFO 2017-07-01 21:55:06,925 main.py:52] epoch 2760, training loss: 8144.36, average training loss: 7926.44, base loss: 15358.16
[INFO 2017-07-01 21:55:11,119 main.py:52] epoch 2761, training loss: 9040.68, average training loss: 7927.55, base loss: 15358.94
[INFO 2017-07-01 21:55:15,282 main.py:52] epoch 2762, training loss: 8705.70, average training loss: 7927.74, base loss: 15360.51
[INFO 2017-07-01 21:55:19,427 main.py:52] epoch 2763, training loss: 7113.91, average training loss: 7927.66, base loss: 15359.41
[INFO 2017-07-01 21:55:23,573 main.py:52] epoch 2764, training loss: 7393.71, average training loss: 7927.78, base loss: 15358.75
[INFO 2017-07-01 21:55:27,690 main.py:52] epoch 2765, training loss: 7351.35, average training loss: 7927.41, base loss: 15357.73
[INFO 2017-07-01 21:55:31,833 main.py:52] epoch 2766, training loss: 7378.74, average training loss: 7926.41, base loss: 15357.33
[INFO 2017-07-01 21:55:36,032 main.py:52] epoch 2767, training loss: 8472.25, average training loss: 7927.52, base loss: 15358.18
[INFO 2017-07-01 21:55:40,220 main.py:52] epoch 2768, training loss: 8149.22, average training loss: 7927.50, base loss: 15358.60
[INFO 2017-07-01 21:55:44,418 main.py:52] epoch 2769, training loss: 7435.23, average training loss: 7926.41, base loss: 15358.96
[INFO 2017-07-01 21:55:48,617 main.py:52] epoch 2770, training loss: 7875.64, average training loss: 7926.00, base loss: 15359.18
[INFO 2017-07-01 21:55:52,766 main.py:52] epoch 2771, training loss: 8638.48, average training loss: 7927.26, base loss: 15360.33
[INFO 2017-07-01 21:55:56,992 main.py:52] epoch 2772, training loss: 8134.65, average training loss: 7927.58, base loss: 15360.03
[INFO 2017-07-01 21:56:01,170 main.py:52] epoch 2773, training loss: 7885.60, average training loss: 7927.59, base loss: 15358.78
[INFO 2017-07-01 21:56:05,280 main.py:52] epoch 2774, training loss: 7525.03, average training loss: 7926.72, base loss: 15357.05
[INFO 2017-07-01 21:56:09,421 main.py:52] epoch 2775, training loss: 8572.88, average training loss: 7926.60, base loss: 15357.03
[INFO 2017-07-01 21:56:13,631 main.py:52] epoch 2776, training loss: 8045.43, average training loss: 7926.42, base loss: 15357.07
[INFO 2017-07-01 21:56:17,783 main.py:52] epoch 2777, training loss: 7750.78, average training loss: 7925.76, base loss: 15356.10
[INFO 2017-07-01 21:56:22,014 main.py:52] epoch 2778, training loss: 7750.46, average training loss: 7925.30, base loss: 15355.49
[INFO 2017-07-01 21:56:26,147 main.py:52] epoch 2779, training loss: 7640.45, average training loss: 7923.16, base loss: 15355.07
[INFO 2017-07-01 21:56:30,316 main.py:52] epoch 2780, training loss: 8687.63, average training loss: 7922.90, base loss: 15355.96
[INFO 2017-07-01 21:56:34,511 main.py:52] epoch 2781, training loss: 7046.06, average training loss: 7922.04, base loss: 15354.64
[INFO 2017-07-01 21:56:38,645 main.py:52] epoch 2782, training loss: 7191.20, average training loss: 7921.16, base loss: 15353.66
[INFO 2017-07-01 21:56:42,799 main.py:52] epoch 2783, training loss: 7573.40, average training loss: 7921.81, base loss: 15353.62
[INFO 2017-07-01 21:56:46,993 main.py:52] epoch 2784, training loss: 7889.02, average training loss: 7921.93, base loss: 15354.05
[INFO 2017-07-01 21:56:51,146 main.py:52] epoch 2785, training loss: 8721.53, average training loss: 7923.59, base loss: 15355.15
[INFO 2017-07-01 21:56:55,328 main.py:52] epoch 2786, training loss: 7390.06, average training loss: 7922.53, base loss: 15354.34
[INFO 2017-07-01 21:56:59,450 main.py:52] epoch 2787, training loss: 7567.78, average training loss: 7920.61, base loss: 15353.80
[INFO 2017-07-01 21:57:03,620 main.py:52] epoch 2788, training loss: 8886.49, average training loss: 7920.95, base loss: 15354.17
[INFO 2017-07-01 21:57:07,746 main.py:52] epoch 2789, training loss: 8337.66, average training loss: 7920.89, base loss: 15354.21
[INFO 2017-07-01 21:57:11,908 main.py:52] epoch 2790, training loss: 7790.69, average training loss: 7920.58, base loss: 15355.32
[INFO 2017-07-01 21:57:16,043 main.py:52] epoch 2791, training loss: 7523.49, average training loss: 7918.97, base loss: 15355.27
[INFO 2017-07-01 21:57:20,184 main.py:52] epoch 2792, training loss: 8672.63, average training loss: 7919.76, base loss: 15356.50
[INFO 2017-07-01 21:57:24,337 main.py:52] epoch 2793, training loss: 7806.24, average training loss: 7919.34, base loss: 15356.15
[INFO 2017-07-01 21:57:28,523 main.py:52] epoch 2794, training loss: 7480.99, average training loss: 7919.71, base loss: 15355.28
[INFO 2017-07-01 21:57:32,634 main.py:52] epoch 2795, training loss: 7276.78, average training loss: 7918.79, base loss: 15354.80
[INFO 2017-07-01 21:57:36,805 main.py:52] epoch 2796, training loss: 8764.70, average training loss: 7919.10, base loss: 15354.73
[INFO 2017-07-01 21:57:40,917 main.py:52] epoch 2797, training loss: 7551.14, average training loss: 7919.34, base loss: 15354.14
[INFO 2017-07-01 21:57:45,064 main.py:52] epoch 2798, training loss: 7916.21, average training loss: 7920.53, base loss: 15353.88
[INFO 2017-07-01 21:57:49,265 main.py:52] epoch 2799, training loss: 7404.86, average training loss: 7920.22, base loss: 15353.05
[INFO 2017-07-01 21:57:49,265 main.py:54] epoch 2799, testing
[INFO 2017-07-01 21:57:49,265 main.py:61] model save to ./model/final.pth
[INFO 2017-07-01 21:57:53,533 main.py:52] epoch 2800, training loss: 8722.72, average training loss: 7921.46, base loss: 15353.74
[INFO 2017-07-01 21:57:57,742 main.py:52] epoch 2801, training loss: 7789.89, average training loss: 7920.80, base loss: 15353.58
[INFO 2017-07-01 21:58:01,911 main.py:52] epoch 2802, training loss: 7632.07, average training loss: 7921.90, base loss: 15352.76
[INFO 2017-07-01 21:58:06,044 main.py:52] epoch 2803, training loss: 9327.05, average training loss: 7923.32, base loss: 15353.60
[INFO 2017-07-01 21:58:10,257 main.py:52] epoch 2804, training loss: 7981.95, average training loss: 7923.10, base loss: 15353.14
[INFO 2017-07-01 21:58:14,354 main.py:52] epoch 2805, training loss: 7567.26, average training loss: 7923.23, base loss: 15352.88
[INFO 2017-07-01 21:58:18,465 main.py:52] epoch 2806, training loss: 6919.05, average training loss: 7922.08, base loss: 15350.75
[INFO 2017-07-01 21:58:22,627 main.py:52] epoch 2807, training loss: 8281.85, average training loss: 7921.31, base loss: 15350.46
[INFO 2017-07-01 21:58:26,732 main.py:52] epoch 2808, training loss: 7683.87, average training loss: 7920.79, base loss: 15349.80
[INFO 2017-07-01 21:58:30,868 main.py:52] epoch 2809, training loss: 7554.23, average training loss: 7919.37, base loss: 15350.02
[INFO 2017-07-01 21:58:35,012 main.py:52] epoch 2810, training loss: 7102.20, average training loss: 7918.75, base loss: 15349.05
[INFO 2017-07-01 21:58:39,196 main.py:52] epoch 2811, training loss: 7624.39, average training loss: 7919.19, base loss: 15349.33
[INFO 2017-07-01 21:58:43,328 main.py:52] epoch 2812, training loss: 7035.67, average training loss: 7917.95, base loss: 15348.73
[INFO 2017-07-01 21:58:47,519 main.py:52] epoch 2813, training loss: 6779.43, average training loss: 7916.79, base loss: 15347.22
[INFO 2017-07-01 21:58:51,653 main.py:52] epoch 2814, training loss: 8367.59, average training loss: 7917.44, base loss: 15348.12
[INFO 2017-07-01 21:58:55,953 main.py:52] epoch 2815, training loss: 9041.06, average training loss: 7918.71, base loss: 15349.46
[INFO 2017-07-01 21:59:00,098 main.py:52] epoch 2816, training loss: 7963.37, average training loss: 7917.73, base loss: 15349.78
[INFO 2017-07-01 21:59:04,206 main.py:52] epoch 2817, training loss: 7297.11, average training loss: 7916.73, base loss: 15349.32
[INFO 2017-07-01 21:59:08,411 main.py:52] epoch 2818, training loss: 7331.92, average training loss: 7915.67, base loss: 15348.46
[INFO 2017-07-01 21:59:12,545 main.py:52] epoch 2819, training loss: 7044.00, average training loss: 7914.01, base loss: 15347.72
[INFO 2017-07-01 21:59:16,694 main.py:52] epoch 2820, training loss: 7261.73, average training loss: 7914.16, base loss: 15347.29
[INFO 2017-07-01 21:59:20,877 main.py:52] epoch 2821, training loss: 7904.12, average training loss: 7913.70, base loss: 15347.41
[INFO 2017-07-01 21:59:25,015 main.py:52] epoch 2822, training loss: 6997.20, average training loss: 7912.30, base loss: 15346.80
[INFO 2017-07-01 21:59:29,191 main.py:52] epoch 2823, training loss: 6691.15, average training loss: 7910.91, base loss: 15345.17
[INFO 2017-07-01 21:59:33,359 main.py:52] epoch 2824, training loss: 7629.91, average training loss: 7911.44, base loss: 15345.27
[INFO 2017-07-01 21:59:37,543 main.py:52] epoch 2825, training loss: 7532.84, average training loss: 7911.08, base loss: 15345.03
[INFO 2017-07-01 21:59:41,788 main.py:52] epoch 2826, training loss: 7892.62, average training loss: 7910.99, base loss: 15345.04
[INFO 2017-07-01 21:59:45,912 main.py:52] epoch 2827, training loss: 7983.26, average training loss: 7911.66, base loss: 15344.59
[INFO 2017-07-01 21:59:50,013 main.py:52] epoch 2828, training loss: 6918.22, average training loss: 7909.87, base loss: 15345.03
[INFO 2017-07-01 21:59:54,162 main.py:52] epoch 2829, training loss: 7302.63, average training loss: 7909.95, base loss: 15344.39
[INFO 2017-07-01 21:59:58,313 main.py:52] epoch 2830, training loss: 7679.40, average training loss: 7909.55, base loss: 15344.51
[INFO 2017-07-01 22:00:02,477 main.py:52] epoch 2831, training loss: 7709.83, average training loss: 7909.75, base loss: 15344.81
[INFO 2017-07-01 22:00:06,662 main.py:52] epoch 2832, training loss: 7986.78, average training loss: 7909.88, base loss: 15346.24
[INFO 2017-07-01 22:00:10,849 main.py:52] epoch 2833, training loss: 7693.48, average training loss: 7909.59, base loss: 15345.47
[INFO 2017-07-01 22:00:15,025 main.py:52] epoch 2834, training loss: 8120.70, average training loss: 7909.55, base loss: 15345.35
[INFO 2017-07-01 22:00:19,172 main.py:52] epoch 2835, training loss: 6657.94, average training loss: 7908.32, base loss: 15343.59
[INFO 2017-07-01 22:00:23,374 main.py:52] epoch 2836, training loss: 8633.23, average training loss: 7909.55, base loss: 15344.39
[INFO 2017-07-01 22:00:27,564 main.py:52] epoch 2837, training loss: 7569.41, average training loss: 7909.05, base loss: 15344.15
[INFO 2017-07-01 22:00:31,711 main.py:52] epoch 2838, training loss: 7663.08, average training loss: 7908.74, base loss: 15343.74
[INFO 2017-07-01 22:00:35,892 main.py:52] epoch 2839, training loss: 9302.52, average training loss: 7909.46, base loss: 15345.15
[INFO 2017-07-01 22:00:40,038 main.py:52] epoch 2840, training loss: 7958.37, average training loss: 7909.24, base loss: 15345.40
[INFO 2017-07-01 22:00:44,245 main.py:52] epoch 2841, training loss: 7591.48, average training loss: 7908.49, base loss: 15345.49
[INFO 2017-07-01 22:00:48,553 main.py:52] epoch 2842, training loss: 7726.40, average training loss: 7907.95, base loss: 15345.91
[INFO 2017-07-01 22:00:52,685 main.py:52] epoch 2843, training loss: 7591.51, average training loss: 7907.99, base loss: 15346.30
[INFO 2017-07-01 22:00:56,871 main.py:52] epoch 2844, training loss: 9432.81, average training loss: 7910.30, base loss: 15347.98
[INFO 2017-07-01 22:01:00,984 main.py:52] epoch 2845, training loss: 7203.99, average training loss: 7909.16, base loss: 15347.55
[INFO 2017-07-01 22:01:05,135 main.py:52] epoch 2846, training loss: 7847.04, average training loss: 7908.47, base loss: 15347.19
[INFO 2017-07-01 22:01:09,317 main.py:52] epoch 2847, training loss: 7818.52, average training loss: 7908.23, base loss: 15346.70
[INFO 2017-07-01 22:01:13,485 main.py:52] epoch 2848, training loss: 9083.48, average training loss: 7907.95, base loss: 15347.74
[INFO 2017-07-01 22:01:17,618 main.py:52] epoch 2849, training loss: 7508.24, average training loss: 7907.16, base loss: 15347.36
[INFO 2017-07-01 22:01:21,729 main.py:52] epoch 2850, training loss: 7274.37, average training loss: 7905.53, base loss: 15346.55
[INFO 2017-07-01 22:01:25,899 main.py:52] epoch 2851, training loss: 7696.02, average training loss: 7904.84, base loss: 15346.63
[INFO 2017-07-01 22:01:30,037 main.py:52] epoch 2852, training loss: 7509.25, average training loss: 7905.14, base loss: 15346.39
[INFO 2017-07-01 22:01:34,140 main.py:52] epoch 2853, training loss: 8531.81, average training loss: 7905.56, base loss: 15346.73
[INFO 2017-07-01 22:01:38,322 main.py:52] epoch 2854, training loss: 7409.64, average training loss: 7905.34, base loss: 15345.90
[INFO 2017-07-01 22:01:42,531 main.py:52] epoch 2855, training loss: 8513.13, average training loss: 7905.90, base loss: 15346.49
[INFO 2017-07-01 22:01:46,682 main.py:52] epoch 2856, training loss: 7738.42, average training loss: 7905.13, base loss: 15346.01
[INFO 2017-07-01 22:01:50,867 main.py:52] epoch 2857, training loss: 8343.80, average training loss: 7904.84, base loss: 15347.12
[INFO 2017-07-01 22:01:55,025 main.py:52] epoch 2858, training loss: 8132.75, average training loss: 7903.64, base loss: 15348.90
[INFO 2017-07-01 22:01:59,193 main.py:52] epoch 2859, training loss: 7677.20, average training loss: 7902.48, base loss: 15348.26
[INFO 2017-07-01 22:02:03,407 main.py:52] epoch 2860, training loss: 8407.32, average training loss: 7901.39, base loss: 15348.34
[INFO 2017-07-01 22:02:07,568 main.py:52] epoch 2861, training loss: 7784.04, average training loss: 7901.15, base loss: 15347.61
[INFO 2017-07-01 22:02:11,757 main.py:52] epoch 2862, training loss: 7959.58, average training loss: 7901.14, base loss: 15348.20
[INFO 2017-07-01 22:02:15,912 main.py:52] epoch 2863, training loss: 7694.08, average training loss: 7901.45, base loss: 15348.35
[INFO 2017-07-01 22:02:20,121 main.py:52] epoch 2864, training loss: 8158.19, average training loss: 7901.64, base loss: 15349.05
[INFO 2017-07-01 22:02:24,193 main.py:52] epoch 2865, training loss: 8661.60, average training loss: 7902.98, base loss: 15350.31
[INFO 2017-07-01 22:02:28,355 main.py:52] epoch 2866, training loss: 7563.80, average training loss: 7902.77, base loss: 15349.46
[INFO 2017-07-01 22:02:32,478 main.py:52] epoch 2867, training loss: 7298.50, average training loss: 7902.30, base loss: 15348.72
[INFO 2017-07-01 22:02:36,660 main.py:52] epoch 2868, training loss: 8948.59, average training loss: 7903.09, base loss: 15349.35
[INFO 2017-07-01 22:02:40,763 main.py:52] epoch 2869, training loss: 8018.86, average training loss: 7902.72, base loss: 15349.52
[INFO 2017-07-01 22:02:44,941 main.py:52] epoch 2870, training loss: 7694.31, average training loss: 7903.02, base loss: 15348.64
[INFO 2017-07-01 22:02:49,168 main.py:52] epoch 2871, training loss: 7560.73, average training loss: 7901.72, base loss: 15348.00
[INFO 2017-07-01 22:02:53,299 main.py:52] epoch 2872, training loss: 8212.94, average training loss: 7902.16, base loss: 15348.29
[INFO 2017-07-01 22:02:57,406 main.py:52] epoch 2873, training loss: 8306.61, average training loss: 7902.98, base loss: 15348.97
[INFO 2017-07-01 22:03:01,610 main.py:52] epoch 2874, training loss: 8358.62, average training loss: 7903.71, base loss: 15349.07
[INFO 2017-07-01 22:03:05,795 main.py:52] epoch 2875, training loss: 8381.50, average training loss: 7903.78, base loss: 15349.66
[INFO 2017-07-01 22:03:09,931 main.py:52] epoch 2876, training loss: 8269.74, average training loss: 7903.86, base loss: 15349.21
[INFO 2017-07-01 22:03:14,070 main.py:52] epoch 2877, training loss: 8231.11, average training loss: 7904.78, base loss: 15348.79
[INFO 2017-07-01 22:03:18,161 main.py:52] epoch 2878, training loss: 7620.28, average training loss: 7905.38, base loss: 15349.29
[INFO 2017-07-01 22:03:22,310 main.py:52] epoch 2879, training loss: 7959.69, average training loss: 7905.87, base loss: 15349.67
[INFO 2017-07-01 22:03:26,565 main.py:52] epoch 2880, training loss: 7677.50, average training loss: 7905.08, base loss: 15349.39
[INFO 2017-07-01 22:03:30,776 main.py:52] epoch 2881, training loss: 8671.34, average training loss: 7906.43, base loss: 15350.16
[INFO 2017-07-01 22:03:34,928 main.py:52] epoch 2882, training loss: 7131.31, average training loss: 7906.13, base loss: 15349.65
[INFO 2017-07-01 22:03:39,063 main.py:52] epoch 2883, training loss: 6808.35, average training loss: 7905.40, base loss: 15348.57
[INFO 2017-07-01 22:03:43,153 main.py:52] epoch 2884, training loss: 7416.81, average training loss: 7904.20, base loss: 15348.08
[INFO 2017-07-01 22:03:47,323 main.py:52] epoch 2885, training loss: 8051.80, average training loss: 7904.83, base loss: 15348.50
[INFO 2017-07-01 22:03:51,485 main.py:52] epoch 2886, training loss: 7583.64, average training loss: 7904.61, base loss: 15348.51
[INFO 2017-07-01 22:03:55,671 main.py:52] epoch 2887, training loss: 7922.34, average training loss: 7904.15, base loss: 15348.48
[INFO 2017-07-01 22:03:59,842 main.py:52] epoch 2888, training loss: 7476.65, average training loss: 7904.12, base loss: 15347.98
[INFO 2017-07-01 22:04:04,002 main.py:52] epoch 2889, training loss: 7997.19, average training loss: 7903.97, base loss: 15348.67
[INFO 2017-07-01 22:04:08,212 main.py:52] epoch 2890, training loss: 7764.85, average training loss: 7905.02, base loss: 15349.08
[INFO 2017-07-01 22:04:12,354 main.py:52] epoch 2891, training loss: 7680.65, average training loss: 7904.35, base loss: 15349.07
[INFO 2017-07-01 22:04:16,462 main.py:52] epoch 2892, training loss: 7134.87, average training loss: 7903.62, base loss: 15348.28
[INFO 2017-07-01 22:04:20,660 main.py:52] epoch 2893, training loss: 7024.61, average training loss: 7903.09, base loss: 15347.50
[INFO 2017-07-01 22:04:24,877 main.py:52] epoch 2894, training loss: 7913.66, average training loss: 7903.67, base loss: 15347.79
[INFO 2017-07-01 22:04:29,106 main.py:52] epoch 2895, training loss: 7666.79, average training loss: 7903.25, base loss: 15347.38
[INFO 2017-07-01 22:04:33,311 main.py:52] epoch 2896, training loss: 8290.53, average training loss: 7903.53, base loss: 15346.69
[INFO 2017-07-01 22:04:37,428 main.py:52] epoch 2897, training loss: 7722.02, average training loss: 7903.33, base loss: 15344.90
[INFO 2017-07-01 22:04:41,671 main.py:52] epoch 2898, training loss: 8426.20, average training loss: 7904.28, base loss: 15345.84
[INFO 2017-07-01 22:04:45,883 main.py:52] epoch 2899, training loss: 7018.71, average training loss: 7903.46, base loss: 15344.58
[INFO 2017-07-01 22:04:45,883 main.py:54] epoch 2899, testing
[INFO 2017-07-01 22:04:45,883 main.py:61] model save to ./model/final.pth
[INFO 2017-07-01 22:04:50,190 main.py:52] epoch 2900, training loss: 7455.20, average training loss: 7903.11, base loss: 15344.32
[INFO 2017-07-01 22:04:54,327 main.py:52] epoch 2901, training loss: 7577.45, average training loss: 7903.02, base loss: 15344.72
[INFO 2017-07-01 22:04:58,523 main.py:52] epoch 2902, training loss: 8069.74, average training loss: 7903.35, base loss: 15344.15
[INFO 2017-07-01 22:05:02,681 main.py:52] epoch 2903, training loss: 7543.22, average training loss: 7902.99, base loss: 15343.04
[INFO 2017-07-01 22:05:06,831 main.py:52] epoch 2904, training loss: 9179.27, average training loss: 7903.86, base loss: 15344.10
[INFO 2017-07-01 22:05:11,012 main.py:52] epoch 2905, training loss: 8848.82, average training loss: 7905.21, base loss: 15345.03
[INFO 2017-07-01 22:05:15,172 main.py:52] epoch 2906, training loss: 8156.86, average training loss: 7904.30, base loss: 15345.61
[INFO 2017-07-01 22:05:19,412 main.py:52] epoch 2907, training loss: 8512.69, average training loss: 7906.08, base loss: 15346.69
[INFO 2017-07-01 22:05:23,531 main.py:52] epoch 2908, training loss: 7903.60, average training loss: 7905.69, base loss: 15347.66
[INFO 2017-07-01 22:05:27,679 main.py:52] epoch 2909, training loss: 7485.76, average training loss: 7905.48, base loss: 15347.62
[INFO 2017-07-01 22:05:31,824 main.py:52] epoch 2910, training loss: 8336.09, average training loss: 7905.11, base loss: 15348.63
[INFO 2017-07-01 22:05:36,058 main.py:52] epoch 2911, training loss: 7604.51, average training loss: 7904.84, base loss: 15348.69
[INFO 2017-07-01 22:05:40,240 main.py:52] epoch 2912, training loss: 7176.51, average training loss: 7904.59, base loss: 15348.54
[INFO 2017-07-01 22:05:44,349 main.py:52] epoch 2913, training loss: 7909.44, average training loss: 7905.00, base loss: 15349.37
[INFO 2017-07-01 22:05:48,539 main.py:52] epoch 2914, training loss: 7333.07, average training loss: 7902.61, base loss: 15348.48
[INFO 2017-07-01 22:05:52,682 main.py:52] epoch 2915, training loss: 7934.62, average training loss: 7902.06, base loss: 15348.58
[INFO 2017-07-01 22:05:56,849 main.py:52] epoch 2916, training loss: 8132.77, average training loss: 7902.03, base loss: 15348.87
[INFO 2017-07-01 22:06:01,138 main.py:52] epoch 2917, training loss: 7962.90, average training loss: 7902.64, base loss: 15348.24
[INFO 2017-07-01 22:06:05,290 main.py:52] epoch 2918, training loss: 7578.22, average training loss: 7902.74, base loss: 15347.04
[INFO 2017-07-01 22:06:09,410 main.py:52] epoch 2919, training loss: 9157.18, average training loss: 7903.60, base loss: 15346.82
[INFO 2017-07-01 22:06:13,552 main.py:52] epoch 2920, training loss: 8740.06, average training loss: 7904.76, base loss: 15347.58
[INFO 2017-07-01 22:06:17,647 main.py:52] epoch 2921, training loss: 7701.55, average training loss: 7904.35, base loss: 15347.40
[INFO 2017-07-01 22:06:21,833 main.py:52] epoch 2922, training loss: 8647.84, average training loss: 7905.03, base loss: 15347.87
[INFO 2017-07-01 22:06:25,989 main.py:52] epoch 2923, training loss: 8008.69, average training loss: 7904.44, base loss: 15347.89
[INFO 2017-07-01 22:06:30,198 main.py:52] epoch 2924, training loss: 7887.36, average training loss: 7904.13, base loss: 15347.97
[INFO 2017-07-01 22:06:34,365 main.py:52] epoch 2925, training loss: 8761.76, average training loss: 7904.30, base loss: 15348.87
[INFO 2017-07-01 22:06:38,588 main.py:52] epoch 2926, training loss: 8704.82, average training loss: 7904.96, base loss: 15349.78
[INFO 2017-07-01 22:06:42,710 main.py:52] epoch 2927, training loss: 7586.76, average training loss: 7904.81, base loss: 15349.81
[INFO 2017-07-01 22:06:46,828 main.py:52] epoch 2928, training loss: 7824.93, average training loss: 7904.69, base loss: 15349.14
[INFO 2017-07-01 22:06:51,095 main.py:52] epoch 2929, training loss: 8349.78, average training loss: 7904.72, base loss: 15349.21
[INFO 2017-07-01 22:06:55,277 main.py:52] epoch 2930, training loss: 8044.36, average training loss: 7904.76, base loss: 15349.18
[INFO 2017-07-01 22:06:59,428 main.py:52] epoch 2931, training loss: 7814.62, average training loss: 7904.63, base loss: 15348.80
[INFO 2017-07-01 22:07:03,594 main.py:52] epoch 2932, training loss: 8216.27, average training loss: 7905.25, base loss: 15349.18
[INFO 2017-07-01 22:07:07,791 main.py:52] epoch 2933, training loss: 7956.68, average training loss: 7904.54, base loss: 15349.47
[INFO 2017-07-01 22:07:11,960 main.py:52] epoch 2934, training loss: 8166.40, average training loss: 7904.69, base loss: 15350.58
[INFO 2017-07-01 22:07:16,134 main.py:52] epoch 2935, training loss: 7618.88, average training loss: 7904.68, base loss: 15351.04
[INFO 2017-07-01 22:07:20,312 main.py:52] epoch 2936, training loss: 7435.83, average training loss: 7904.48, base loss: 15350.71
[INFO 2017-07-01 22:07:24,481 main.py:52] epoch 2937, training loss: 7608.29, average training loss: 7904.68, base loss: 15350.94
[INFO 2017-07-01 22:07:28,643 main.py:52] epoch 2938, training loss: 7869.06, average training loss: 7905.01, base loss: 15350.89
[INFO 2017-07-01 22:07:32,819 main.py:52] epoch 2939, training loss: 9375.56, average training loss: 7906.81, base loss: 15352.16
[INFO 2017-07-01 22:07:37,054 main.py:52] epoch 2940, training loss: 7542.08, average training loss: 7906.62, base loss: 15351.00
[INFO 2017-07-01 22:07:41,364 main.py:52] epoch 2941, training loss: 7818.04, average training loss: 7905.87, base loss: 15350.59
[INFO 2017-07-01 22:07:45,552 main.py:52] epoch 2942, training loss: 8374.66, average training loss: 7905.19, base loss: 15351.50
[INFO 2017-07-01 22:07:49,709 main.py:52] epoch 2943, training loss: 8664.79, average training loss: 7905.56, base loss: 15352.79
[INFO 2017-07-01 22:07:53,850 main.py:52] epoch 2944, training loss: 8914.04, average training loss: 7907.06, base loss: 15353.72
[INFO 2017-07-01 22:07:58,053 main.py:52] epoch 2945, training loss: 8475.29, average training loss: 7906.55, base loss: 15353.78
[INFO 2017-07-01 22:08:02,263 main.py:52] epoch 2946, training loss: 7645.06, average training loss: 7905.22, base loss: 15353.37
[INFO 2017-07-01 22:08:06,372 main.py:52] epoch 2947, training loss: 8636.48, average training loss: 7905.27, base loss: 15354.50
[INFO 2017-07-01 22:08:10,542 main.py:52] epoch 2948, training loss: 8214.99, average training loss: 7906.06, base loss: 15354.20
[INFO 2017-07-01 22:08:14,783 main.py:52] epoch 2949, training loss: 8455.41, average training loss: 7905.29, base loss: 15354.56
[INFO 2017-07-01 22:08:18,909 main.py:52] epoch 2950, training loss: 6935.81, average training loss: 7904.15, base loss: 15353.56
[INFO 2017-07-01 22:08:23,074 main.py:52] epoch 2951, training loss: 8271.58, average training loss: 7904.81, base loss: 15353.56
[INFO 2017-07-01 22:08:27,226 main.py:52] epoch 2952, training loss: 7990.78, average training loss: 7904.89, base loss: 15352.80
[INFO 2017-07-01 22:08:31,365 main.py:52] epoch 2953, training loss: 9235.62, average training loss: 7905.84, base loss: 15353.74
[INFO 2017-07-01 22:08:35,532 main.py:52] epoch 2954, training loss: 7667.59, average training loss: 7905.07, base loss: 15352.41
[INFO 2017-07-01 22:08:39,629 main.py:52] epoch 2955, training loss: 7911.33, average training loss: 7904.13, base loss: 15351.64
[INFO 2017-07-01 22:08:43,819 main.py:52] epoch 2956, training loss: 8084.55, average training loss: 7904.79, base loss: 15350.89
[INFO 2017-07-01 22:08:47,920 main.py:52] epoch 2957, training loss: 7758.89, average training loss: 7904.24, base loss: 15349.62
[INFO 2017-07-01 22:08:52,087 main.py:52] epoch 2958, training loss: 8050.45, average training loss: 7904.85, base loss: 15350.09
[INFO 2017-07-01 22:08:56,320 main.py:52] epoch 2959, training loss: 7365.22, average training loss: 7904.58, base loss: 15349.12
[INFO 2017-07-01 22:09:00,494 main.py:52] epoch 2960, training loss: 7860.13, average training loss: 7905.66, base loss: 15348.96
[INFO 2017-07-01 22:09:04,666 main.py:52] epoch 2961, training loss: 8914.18, average training loss: 7907.17, base loss: 15350.63
[INFO 2017-07-01 22:09:08,787 main.py:52] epoch 2962, training loss: 7948.55, average training loss: 7907.26, base loss: 15351.11
[INFO 2017-07-01 22:09:12,940 main.py:52] epoch 2963, training loss: 8383.85, average training loss: 7908.47, base loss: 15350.78
[INFO 2017-07-01 22:09:17,150 main.py:52] epoch 2964, training loss: 8282.37, average training loss: 7907.90, base loss: 15351.39
[INFO 2017-07-01 22:09:21,271 main.py:52] epoch 2965, training loss: 7935.59, average training loss: 7908.06, base loss: 15352.01
[INFO 2017-07-01 22:09:25,466 main.py:52] epoch 2966, training loss: 8161.92, average training loss: 7908.73, base loss: 15352.06
[INFO 2017-07-01 22:09:29,605 main.py:52] epoch 2967, training loss: 7768.85, average training loss: 7907.81, base loss: 15352.64
[INFO 2017-07-01 22:09:33,748 main.py:52] epoch 2968, training loss: 7930.67, average training loss: 7907.31, base loss: 15352.30
[INFO 2017-07-01 22:09:37,949 main.py:52] epoch 2969, training loss: 8630.49, average training loss: 7907.78, base loss: 15352.02
[INFO 2017-07-01 22:09:42,149 main.py:52] epoch 2970, training loss: 7509.54, average training loss: 7908.75, base loss: 15352.54
[INFO 2017-07-01 22:09:46,327 main.py:52] epoch 2971, training loss: 7610.98, average training loss: 7908.13, base loss: 15352.84
[INFO 2017-07-01 22:09:50,543 main.py:52] epoch 2972, training loss: 8032.89, average training loss: 7908.50, base loss: 15353.13
[INFO 2017-07-01 22:09:54,659 main.py:52] epoch 2973, training loss: 7202.87, average training loss: 7908.32, base loss: 15352.98
[INFO 2017-07-01 22:09:58,796 main.py:52] epoch 2974, training loss: 8305.63, average training loss: 7909.23, base loss: 15352.99
[INFO 2017-07-01 22:10:03,020 main.py:52] epoch 2975, training loss: 8302.63, average training loss: 7909.93, base loss: 15352.77
[INFO 2017-07-01 22:10:07,185 main.py:52] epoch 2976, training loss: 7800.10, average training loss: 7910.14, base loss: 15351.90
[INFO 2017-07-01 22:10:11,380 main.py:52] epoch 2977, training loss: 8245.50, average training loss: 7910.26, base loss: 15351.76
[INFO 2017-07-01 22:10:15,516 main.py:52] epoch 2978, training loss: 8468.76, average training loss: 7910.23, base loss: 15352.34
[INFO 2017-07-01 22:10:19,693 main.py:52] epoch 2979, training loss: 8575.10, average training loss: 7911.26, base loss: 15352.81
[INFO 2017-07-01 22:10:23,824 main.py:52] epoch 2980, training loss: 8395.56, average training loss: 7911.94, base loss: 15353.46
[INFO 2017-07-01 22:10:27,937 main.py:52] epoch 2981, training loss: 8209.37, average training loss: 7911.67, base loss: 15353.34
[INFO 2017-07-01 22:10:32,065 main.py:52] epoch 2982, training loss: 7291.91, average training loss: 7911.18, base loss: 15352.06
[INFO 2017-07-01 22:10:36,171 main.py:52] epoch 2983, training loss: 8124.53, average training loss: 7911.24, base loss: 15351.80
[INFO 2017-07-01 22:10:40,369 main.py:52] epoch 2984, training loss: 8960.63, average training loss: 7911.99, base loss: 15352.58
[INFO 2017-07-01 22:10:44,454 main.py:52] epoch 2985, training loss: 7565.30, average training loss: 7911.30, base loss: 15351.87
[INFO 2017-07-01 22:10:48,580 main.py:52] epoch 2986, training loss: 6974.53, average training loss: 7910.68, base loss: 15350.37
[INFO 2017-07-01 22:10:52,733 main.py:52] epoch 2987, training loss: 8134.89, average training loss: 7911.41, base loss: 15350.25
[INFO 2017-07-01 22:10:56,903 main.py:52] epoch 2988, training loss: 8722.71, average training loss: 7912.90, base loss: 15350.54
[INFO 2017-07-01 22:11:01,158 main.py:52] epoch 2989, training loss: 8124.17, average training loss: 7912.19, base loss: 15350.89
[INFO 2017-07-01 22:11:05,294 main.py:52] epoch 2990, training loss: 7712.89, average training loss: 7911.98, base loss: 15350.40
[INFO 2017-07-01 22:11:09,542 main.py:52] epoch 2991, training loss: 8333.49, average training loss: 7912.65, base loss: 15351.04
[INFO 2017-07-01 22:11:13,742 main.py:52] epoch 2992, training loss: 7884.61, average training loss: 7912.92, base loss: 15351.29
[INFO 2017-07-01 22:11:17,871 main.py:52] epoch 2993, training loss: 8498.19, average training loss: 7910.62, base loss: 15352.34
[INFO 2017-07-01 22:11:22,054 main.py:52] epoch 2994, training loss: 7415.34, average training loss: 7911.05, base loss: 15352.94
[INFO 2017-07-01 22:11:26,174 main.py:52] epoch 2995, training loss: 7203.79, average training loss: 7910.54, base loss: 15352.60
[INFO 2017-07-01 22:11:30,335 main.py:52] epoch 2996, training loss: 7376.63, average training loss: 7910.57, base loss: 15352.16
[INFO 2017-07-01 22:11:34,499 main.py:52] epoch 2997, training loss: 8590.60, average training loss: 7911.57, base loss: 15352.52
[INFO 2017-07-01 22:11:38,666 main.py:52] epoch 2998, training loss: 7232.99, average training loss: 7910.98, base loss: 15351.74
[INFO 2017-07-01 22:11:42,834 main.py:52] epoch 2999, training loss: 8452.49, average training loss: 7910.85, base loss: 15352.21
[INFO 2017-07-01 22:11:42,834 main.py:54] epoch 2999, testing
[INFO 2017-07-01 22:11:42,835 main.py:61] model save to ./model/final.pth
[INFO 2017-07-01 22:11:47,136 main.py:52] epoch 3000, training loss: 7456.84, average training loss: 7910.42, base loss: 15351.69
[INFO 2017-07-01 22:11:51,354 main.py:52] epoch 3001, training loss: 8917.08, average training loss: 7911.21, base loss: 15353.30
[INFO 2017-07-01 22:11:55,482 main.py:52] epoch 3002, training loss: 7105.03, average training loss: 7910.54, base loss: 15352.19
[INFO 2017-07-01 22:11:59,636 main.py:52] epoch 3003, training loss: 7920.18, average training loss: 7910.15, base loss: 15352.10
[INFO 2017-07-01 22:12:03,775 main.py:52] epoch 3004, training loss: 8288.52, average training loss: 7911.48, base loss: 15351.95
[INFO 2017-07-01 22:12:07,961 main.py:52] epoch 3005, training loss: 8065.35, average training loss: 7911.63, base loss: 15351.87
[INFO 2017-07-01 22:12:12,127 main.py:52] epoch 3006, training loss: 6691.95, average training loss: 7910.46, base loss: 15351.46
[INFO 2017-07-01 22:12:16,425 main.py:52] epoch 3007, training loss: 8765.98, average training loss: 7910.34, base loss: 15352.31
[INFO 2017-07-01 22:12:20,566 main.py:52] epoch 3008, training loss: 7816.55, average training loss: 7910.29, base loss: 15351.66
[INFO 2017-07-01 22:12:24,756 main.py:52] epoch 3009, training loss: 7418.00, average training loss: 7910.27, base loss: 15351.61
[INFO 2017-07-01 22:12:28,892 main.py:52] epoch 3010, training loss: 7982.88, average training loss: 7910.92, base loss: 15351.36
[INFO 2017-07-01 22:12:33,107 main.py:52] epoch 3011, training loss: 8090.29, average training loss: 7911.35, base loss: 15351.24
[INFO 2017-07-01 22:12:37,293 main.py:52] epoch 3012, training loss: 7761.17, average training loss: 7910.46, base loss: 15350.43
[INFO 2017-07-01 22:12:41,469 main.py:52] epoch 3013, training loss: 8356.51, average training loss: 7910.61, base loss: 15351.01
[INFO 2017-07-01 22:12:45,666 main.py:52] epoch 3014, training loss: 7222.85, average training loss: 7910.66, base loss: 15350.85
[INFO 2017-07-01 22:12:49,931 main.py:52] epoch 3015, training loss: 8522.60, average training loss: 7911.27, base loss: 15351.07
[INFO 2017-07-01 22:12:54,042 main.py:52] epoch 3016, training loss: 7586.71, average training loss: 7911.63, base loss: 15350.73
[INFO 2017-07-01 22:12:58,255 main.py:52] epoch 3017, training loss: 7001.24, average training loss: 7911.04, base loss: 15349.68
[INFO 2017-07-01 22:13:02,443 main.py:52] epoch 3018, training loss: 7705.14, average training loss: 7911.37, base loss: 15348.89
[INFO 2017-07-01 22:13:06,605 main.py:52] epoch 3019, training loss: 8239.68, average training loss: 7910.93, base loss: 15348.45
[INFO 2017-07-01 22:13:10,791 main.py:52] epoch 3020, training loss: 7245.87, average training loss: 7911.02, base loss: 15346.82
[INFO 2017-07-01 22:13:14,952 main.py:52] epoch 3021, training loss: 7663.22, average training loss: 7910.32, base loss: 15345.72
[INFO 2017-07-01 22:13:19,157 main.py:52] epoch 3022, training loss: 7132.15, average training loss: 7909.58, base loss: 15344.84
[INFO 2017-07-01 22:13:23,374 main.py:52] epoch 3023, training loss: 7427.74, average training loss: 7909.69, base loss: 15344.61
[INFO 2017-07-01 22:13:27,518 main.py:52] epoch 3024, training loss: 7420.86, average training loss: 7909.06, base loss: 15344.67
[INFO 2017-07-01 22:13:31,729 main.py:52] epoch 3025, training loss: 7543.79, average training loss: 7908.12, base loss: 15344.84
[INFO 2017-07-01 22:13:35,912 main.py:52] epoch 3026, training loss: 7309.58, average training loss: 7906.57, base loss: 15344.28
[INFO 2017-07-01 22:13:40,129 main.py:52] epoch 3027, training loss: 8735.67, average training loss: 7907.69, base loss: 15344.78
[INFO 2017-07-01 22:13:44,279 main.py:52] epoch 3028, training loss: 8327.09, average training loss: 7907.55, base loss: 15345.39
[INFO 2017-07-01 22:13:48,401 main.py:52] epoch 3029, training loss: 7136.93, average training loss: 7905.78, base loss: 15344.68
[INFO 2017-07-01 22:13:52,576 main.py:52] epoch 3030, training loss: 8163.16, average training loss: 7906.36, base loss: 15344.77
[INFO 2017-07-01 22:13:56,722 main.py:52] epoch 3031, training loss: 8101.09, average training loss: 7905.75, base loss: 15344.83
[INFO 2017-07-01 22:14:00,922 main.py:52] epoch 3032, training loss: 7850.24, average training loss: 7905.00, base loss: 15345.14
[INFO 2017-07-01 22:14:05,141 main.py:52] epoch 3033, training loss: 7948.35, average training loss: 7904.21, base loss: 15345.74
[INFO 2017-07-01 22:14:09,258 main.py:52] epoch 3034, training loss: 7821.03, average training loss: 7902.93, base loss: 15345.81
[INFO 2017-07-01 22:14:13,357 main.py:52] epoch 3035, training loss: 8340.54, average training loss: 7903.67, base loss: 15346.31
[INFO 2017-07-01 22:14:17,569 main.py:52] epoch 3036, training loss: 8661.65, average training loss: 7904.16, base loss: 15346.50
[INFO 2017-07-01 22:14:21,740 main.py:52] epoch 3037, training loss: 8397.81, average training loss: 7904.78, base loss: 15346.98
[INFO 2017-07-01 22:14:25,863 main.py:52] epoch 3038, training loss: 8645.33, average training loss: 7906.25, base loss: 15348.09
[INFO 2017-07-01 22:14:29,968 main.py:52] epoch 3039, training loss: 7433.63, average training loss: 7906.10, base loss: 15347.24
[INFO 2017-07-01 22:14:34,131 main.py:52] epoch 3040, training loss: 8331.27, average training loss: 7905.99, base loss: 15347.44
[INFO 2017-07-01 22:14:38,295 main.py:52] epoch 3041, training loss: 8552.12, average training loss: 7905.83, base loss: 15348.11
[INFO 2017-07-01 22:14:42,457 main.py:52] epoch 3042, training loss: 9133.07, average training loss: 7907.65, base loss: 15349.45
[INFO 2017-07-01 22:14:46,648 main.py:52] epoch 3043, training loss: 7646.77, average training loss: 7906.49, base loss: 15349.58
[INFO 2017-07-01 22:14:50,773 main.py:52] epoch 3044, training loss: 7400.02, average training loss: 7906.10, base loss: 15349.63
[INFO 2017-07-01 22:14:55,008 main.py:52] epoch 3045, training loss: 8310.79, average training loss: 7907.03, base loss: 15349.66
[INFO 2017-07-01 22:14:59,182 main.py:52] epoch 3046, training loss: 7461.91, average training loss: 7906.84, base loss: 15349.39
[INFO 2017-07-01 22:15:03,335 main.py:52] epoch 3047, training loss: 7766.42, average training loss: 7906.41, base loss: 15349.70
[INFO 2017-07-01 22:15:07,549 main.py:52] epoch 3048, training loss: 6530.14, average training loss: 7904.90, base loss: 15349.26
[INFO 2017-07-01 22:15:11,620 main.py:52] epoch 3049, training loss: 7848.19, average training loss: 7904.37, base loss: 15349.36
[INFO 2017-07-01 22:15:15,765 main.py:52] epoch 3050, training loss: 8167.00, average training loss: 7905.04, base loss: 15349.25
[INFO 2017-07-01 22:15:19,929 main.py:52] epoch 3051, training loss: 7580.32, average training loss: 7904.75, base loss: 15349.42
[INFO 2017-07-01 22:15:24,038 main.py:52] epoch 3052, training loss: 8087.62, average training loss: 7905.32, base loss: 15348.99
[INFO 2017-07-01 22:15:28,192 main.py:52] epoch 3053, training loss: 8362.33, average training loss: 7905.77, base loss: 15350.08
[INFO 2017-07-01 22:15:32,362 main.py:52] epoch 3054, training loss: 7697.33, average training loss: 7905.21, base loss: 15350.48
[INFO 2017-07-01 22:15:36,512 main.py:52] epoch 3055, training loss: 9135.46, average training loss: 7907.18, base loss: 15352.73
[INFO 2017-07-01 22:15:40,717 main.py:52] epoch 3056, training loss: 8472.17, average training loss: 7907.66, base loss: 15352.86
[INFO 2017-07-01 22:15:44,919 main.py:52] epoch 3057, training loss: 7518.94, average training loss: 7907.49, base loss: 15351.90
[INFO 2017-07-01 22:15:49,135 main.py:52] epoch 3058, training loss: 7989.78, average training loss: 7907.53, base loss: 15352.45
[INFO 2017-07-01 22:15:53,344 main.py:52] epoch 3059, training loss: 8501.34, average training loss: 7908.42, base loss: 15352.61
[INFO 2017-07-01 22:15:57,527 main.py:52] epoch 3060, training loss: 7686.08, average training loss: 7908.89, base loss: 15351.49
[INFO 2017-07-01 22:16:01,646 main.py:52] epoch 3061, training loss: 9047.38, average training loss: 7910.23, base loss: 15352.96
[INFO 2017-07-01 22:16:05,771 main.py:52] epoch 3062, training loss: 6922.68, average training loss: 7910.17, base loss: 15352.90
[INFO 2017-07-01 22:16:09,906 main.py:52] epoch 3063, training loss: 7996.25, average training loss: 7909.87, base loss: 15353.42
[INFO 2017-07-01 22:16:14,019 main.py:52] epoch 3064, training loss: 7089.93, average training loss: 7909.79, base loss: 15353.53
[INFO 2017-07-01 22:16:18,232 main.py:52] epoch 3065, training loss: 7997.34, average training loss: 7910.39, base loss: 15353.98
[INFO 2017-07-01 22:16:22,389 main.py:52] epoch 3066, training loss: 7224.78, average training loss: 7909.87, base loss: 15352.62
[INFO 2017-07-01 22:16:26,633 main.py:52] epoch 3067, training loss: 7341.98, average training loss: 7908.78, base loss: 15351.42
[INFO 2017-07-01 22:16:30,818 main.py:52] epoch 3068, training loss: 7429.02, average training loss: 7908.19, base loss: 15351.36
[INFO 2017-07-01 22:16:34,933 main.py:52] epoch 3069, training loss: 7963.92, average training loss: 7907.94, base loss: 15352.04
[INFO 2017-07-01 22:16:39,106 main.py:52] epoch 3070, training loss: 7409.77, average training loss: 7906.31, base loss: 15351.28
[INFO 2017-07-01 22:16:43,287 main.py:52] epoch 3071, training loss: 7933.79, average training loss: 7907.22, base loss: 15351.23
[INFO 2017-07-01 22:16:47,429 main.py:52] epoch 3072, training loss: 7253.40, average training loss: 7906.48, base loss: 15350.76
[INFO 2017-07-01 22:16:51,611 main.py:52] epoch 3073, training loss: 7556.91, average training loss: 7905.99, base loss: 15350.13
[INFO 2017-07-01 22:16:55,774 main.py:52] epoch 3074, training loss: 7641.00, average training loss: 7905.19, base loss: 15349.59
[INFO 2017-07-01 22:16:59,901 main.py:52] epoch 3075, training loss: 7704.79, average training loss: 7904.50, base loss: 15349.16
[INFO 2017-07-01 22:17:04,136 main.py:52] epoch 3076, training loss: 7779.42, average training loss: 7905.17, base loss: 15348.51
[INFO 2017-07-01 22:17:08,263 main.py:52] epoch 3077, training loss: 7764.34, average training loss: 7904.75, base loss: 15347.42
[INFO 2017-07-01 22:17:12,377 main.py:52] epoch 3078, training loss: 8280.63, average training loss: 7904.56, base loss: 15348.00
[INFO 2017-07-01 22:17:16,595 main.py:52] epoch 3079, training loss: 7733.36, average training loss: 7904.62, base loss: 15348.14
[INFO 2017-07-01 22:17:20,753 main.py:52] epoch 3080, training loss: 8073.83, average training loss: 7904.93, base loss: 15348.14
[INFO 2017-07-01 22:17:24,904 main.py:52] epoch 3081, training loss: 7482.39, average training loss: 7904.59, base loss: 15348.23
[INFO 2017-07-01 22:17:29,043 main.py:52] epoch 3082, training loss: 8113.00, average training loss: 7905.44, base loss: 15348.50
[INFO 2017-07-01 22:17:33,231 main.py:52] epoch 3083, training loss: 7680.31, average training loss: 7905.17, base loss: 15347.69
[INFO 2017-07-01 22:17:37,396 main.py:52] epoch 3084, training loss: 7465.54, average training loss: 7905.51, base loss: 15346.98
[INFO 2017-07-01 22:17:41,591 main.py:52] epoch 3085, training loss: 7567.56, average training loss: 7904.85, base loss: 15346.11
[INFO 2017-07-01 22:17:45,718 main.py:52] epoch 3086, training loss: 8116.65, average training loss: 7905.52, base loss: 15346.75
[INFO 2017-07-01 22:17:49,879 main.py:52] epoch 3087, training loss: 8561.67, average training loss: 7906.07, base loss: 15347.62
[INFO 2017-07-01 22:17:54,018 main.py:52] epoch 3088, training loss: 7414.45, average training loss: 7905.03, base loss: 15346.75
[INFO 2017-07-01 22:17:58,222 main.py:52] epoch 3089, training loss: 7593.31, average training loss: 7903.44, base loss: 15347.36
[INFO 2017-07-01 22:18:02,453 main.py:52] epoch 3090, training loss: 7713.51, average training loss: 7902.40, base loss: 15346.96
[INFO 2017-07-01 22:18:06,647 main.py:52] epoch 3091, training loss: 9058.70, average training loss: 7903.39, base loss: 15346.67
[INFO 2017-07-01 22:18:10,799 main.py:52] epoch 3092, training loss: 8234.03, average training loss: 7903.51, base loss: 15346.65
[INFO 2017-07-01 22:18:15,013 main.py:52] epoch 3093, training loss: 8952.37, average training loss: 7904.82, base loss: 15346.40
[INFO 2017-07-01 22:18:19,243 main.py:52] epoch 3094, training loss: 7459.22, average training loss: 7905.21, base loss: 15346.63
[INFO 2017-07-01 22:18:23,391 main.py:52] epoch 3095, training loss: 7771.44, average training loss: 7905.85, base loss: 15346.94
[INFO 2017-07-01 22:18:27,576 main.py:52] epoch 3096, training loss: 7717.18, average training loss: 7905.47, base loss: 15346.51
[INFO 2017-07-01 22:18:31,767 main.py:52] epoch 3097, training loss: 7893.25, average training loss: 7906.14, base loss: 15345.83
[INFO 2017-07-01 22:18:35,899 main.py:52] epoch 3098, training loss: 7800.04, average training loss: 7906.38, base loss: 15345.86
[INFO 2017-07-01 22:18:40,118 main.py:52] epoch 3099, training loss: 8198.57, average training loss: 7906.86, base loss: 15345.84
[INFO 2017-07-01 22:18:40,118 main.py:54] epoch 3099, testing
[INFO 2017-07-01 22:18:40,118 main.py:61] model save to ./model/final.pth
[INFO 2017-07-01 22:18:44,408 main.py:52] epoch 3100, training loss: 8406.34, average training loss: 7906.88, base loss: 15346.83
[INFO 2017-07-01 22:18:48,587 main.py:52] epoch 3101, training loss: 9171.89, average training loss: 7907.19, base loss: 15348.14
[INFO 2017-07-01 22:18:52,701 main.py:52] epoch 3102, training loss: 8143.93, average training loss: 7907.44, base loss: 15348.52
[INFO 2017-07-01 22:18:56,967 main.py:52] epoch 3103, training loss: 8139.74, average training loss: 7907.05, base loss: 15349.07
[INFO 2017-07-01 22:19:01,161 main.py:52] epoch 3104, training loss: 7538.53, average training loss: 7906.65, base loss: 15348.32
[INFO 2017-07-01 22:19:05,290 main.py:52] epoch 3105, training loss: 7820.62, average training loss: 7906.57, base loss: 15347.74
[INFO 2017-07-01 22:19:09,505 main.py:52] epoch 3106, training loss: 7274.74, average training loss: 7905.96, base loss: 15347.24
[INFO 2017-07-01 22:19:13,641 main.py:52] epoch 3107, training loss: 8081.76, average training loss: 7905.92, base loss: 15347.57
[INFO 2017-07-01 22:19:17,813 main.py:52] epoch 3108, training loss: 8077.11, average training loss: 7906.28, base loss: 15347.52
[INFO 2017-07-01 22:19:21,986 main.py:52] epoch 3109, training loss: 8268.92, average training loss: 7907.04, base loss: 15347.52
[INFO 2017-07-01 22:19:26,128 main.py:52] epoch 3110, training loss: 7790.01, average training loss: 7906.39, base loss: 15347.17
[INFO 2017-07-01 22:19:30,274 main.py:52] epoch 3111, training loss: 8448.15, average training loss: 7907.18, base loss: 15347.22
[INFO 2017-07-01 22:19:34,566 main.py:52] epoch 3112, training loss: 8368.70, average training loss: 7906.84, base loss: 15348.33
[INFO 2017-07-01 22:19:38,753 main.py:52] epoch 3113, training loss: 8294.65, average training loss: 7906.64, base loss: 15347.80
[INFO 2017-07-01 22:19:42,910 main.py:52] epoch 3114, training loss: 7361.93, average training loss: 7905.32, base loss: 15347.29
[INFO 2017-07-01 22:19:47,029 main.py:52] epoch 3115, training loss: 8897.60, average training loss: 7906.43, base loss: 15347.76
[INFO 2017-07-01 22:19:51,203 main.py:52] epoch 3116, training loss: 7932.84, average training loss: 7905.52, base loss: 15347.02
[INFO 2017-07-01 22:19:55,376 main.py:52] epoch 3117, training loss: 9056.10, average training loss: 7905.18, base loss: 15347.14
[INFO 2017-07-01 22:19:59,557 main.py:52] epoch 3118, training loss: 7748.74, average training loss: 7905.72, base loss: 15347.10
[INFO 2017-07-01 22:20:03,759 main.py:52] epoch 3119, training loss: 7450.86, average training loss: 7905.77, base loss: 15346.08
[INFO 2017-07-01 22:20:07,883 main.py:52] epoch 3120, training loss: 7980.51, average training loss: 7906.59, base loss: 15346.36
[INFO 2017-07-01 22:20:12,048 main.py:52] epoch 3121, training loss: 7749.46, average training loss: 7906.71, base loss: 15346.85
[INFO 2017-07-01 22:20:16,329 main.py:52] epoch 3122, training loss: 8850.90, average training loss: 7908.22, base loss: 15347.96
[INFO 2017-07-01 22:20:20,494 main.py:52] epoch 3123, training loss: 7696.66, average training loss: 7907.41, base loss: 15348.13
[INFO 2017-07-01 22:20:24,600 main.py:52] epoch 3124, training loss: 8344.13, average training loss: 7908.20, base loss: 15348.88
[INFO 2017-07-01 22:20:28,799 main.py:52] epoch 3125, training loss: 7837.91, average training loss: 7908.09, base loss: 15349.04
[INFO 2017-07-01 22:20:33,034 main.py:52] epoch 3126, training loss: 8029.59, average training loss: 7907.89, base loss: 15348.81
[INFO 2017-07-01 22:20:37,183 main.py:52] epoch 3127, training loss: 7428.71, average training loss: 7907.76, base loss: 15347.80
[INFO 2017-07-01 22:20:41,406 main.py:52] epoch 3128, training loss: 7014.89, average training loss: 7905.85, base loss: 15346.34
[INFO 2017-07-01 22:20:45,577 main.py:52] epoch 3129, training loss: 8094.47, average training loss: 7905.68, base loss: 15346.24
[INFO 2017-07-01 22:20:49,776 main.py:52] epoch 3130, training loss: 6965.91, average training loss: 7904.67, base loss: 15345.58
[INFO 2017-07-01 22:20:53,910 main.py:52] epoch 3131, training loss: 7510.41, average training loss: 7904.19, base loss: 15344.53
[INFO 2017-07-01 22:20:58,063 main.py:52] epoch 3132, training loss: 8992.15, average training loss: 7905.15, base loss: 15344.55
[INFO 2017-07-01 22:21:02,183 main.py:52] epoch 3133, training loss: 8143.42, average training loss: 7905.82, base loss: 15343.99
[INFO 2017-07-01 22:21:06,296 main.py:52] epoch 3134, training loss: 8751.27, average training loss: 7907.10, base loss: 15345.87
[INFO 2017-07-01 22:21:10,460 main.py:52] epoch 3135, training loss: 8204.32, average training loss: 7907.54, base loss: 15346.49
[INFO 2017-07-01 22:21:14,667 main.py:52] epoch 3136, training loss: 7611.57, average training loss: 7906.45, base loss: 15346.65
[INFO 2017-07-01 22:21:18,799 main.py:52] epoch 3137, training loss: 7671.06, average training loss: 7906.79, base loss: 15346.86
[INFO 2017-07-01 22:21:22,945 main.py:52] epoch 3138, training loss: 8707.84, average training loss: 7907.53, base loss: 15347.47
[INFO 2017-07-01 22:21:27,144 main.py:52] epoch 3139, training loss: 8974.14, average training loss: 7908.57, base loss: 15347.18
[INFO 2017-07-01 22:21:31,299 main.py:52] epoch 3140, training loss: 8654.19, average training loss: 7909.80, base loss: 15347.68
[INFO 2017-07-01 22:21:35,483 main.py:52] epoch 3141, training loss: 7956.78, average training loss: 7910.03, base loss: 15348.38
[INFO 2017-07-01 22:21:39,705 main.py:52] epoch 3142, training loss: 7635.10, average training loss: 7910.27, base loss: 15348.82
[INFO 2017-07-01 22:21:43,879 main.py:52] epoch 3143, training loss: 7129.08, average training loss: 7910.45, base loss: 15347.82
[INFO 2017-07-01 22:21:48,024 main.py:52] epoch 3144, training loss: 7338.48, average training loss: 7910.02, base loss: 15347.53
[INFO 2017-07-01 22:21:52,223 main.py:52] epoch 3145, training loss: 7642.01, average training loss: 7909.84, base loss: 15347.49
[INFO 2017-07-01 22:21:56,356 main.py:52] epoch 3146, training loss: 8099.95, average training loss: 7910.44, base loss: 15347.23
[INFO 2017-07-01 22:22:00,509 main.py:52] epoch 3147, training loss: 7326.42, average training loss: 7909.46, base loss: 15346.71
[INFO 2017-07-01 22:22:04,745 main.py:52] epoch 3148, training loss: 7100.16, average training loss: 7909.27, base loss: 15346.88
[INFO 2017-07-01 22:22:08,863 main.py:52] epoch 3149, training loss: 8477.32, average training loss: 7910.44, base loss: 15347.76
[INFO 2017-07-01 22:22:13,017 main.py:52] epoch 3150, training loss: 7894.51, average training loss: 7911.18, base loss: 15348.40
[INFO 2017-07-01 22:22:17,143 main.py:52] epoch 3151, training loss: 7651.85, average training loss: 7910.78, base loss: 15348.58
[INFO 2017-07-01 22:22:21,372 main.py:52] epoch 3152, training loss: 7031.94, average training loss: 7909.87, base loss: 15347.48
[INFO 2017-07-01 22:22:25,515 main.py:52] epoch 3153, training loss: 7597.76, average training loss: 7909.02, base loss: 15346.89
[INFO 2017-07-01 22:22:29,636 main.py:52] epoch 3154, training loss: 8246.03, average training loss: 7909.08, base loss: 15346.98
[INFO 2017-07-01 22:22:33,776 main.py:52] epoch 3155, training loss: 7254.92, average training loss: 7907.17, base loss: 15345.64
[INFO 2017-07-01 22:22:37,940 main.py:52] epoch 3156, training loss: 7329.55, average training loss: 7906.56, base loss: 15346.19
[INFO 2017-07-01 22:22:42,162 main.py:52] epoch 3157, training loss: 8005.79, average training loss: 7906.72, base loss: 15347.37
[INFO 2017-07-01 22:22:46,356 main.py:52] epoch 3158, training loss: 8044.16, average training loss: 7907.52, base loss: 15346.63
[INFO 2017-07-01 22:22:50,457 main.py:52] epoch 3159, training loss: 7078.60, average training loss: 7906.33, base loss: 15345.73
[INFO 2017-07-01 22:22:54,626 main.py:52] epoch 3160, training loss: 8152.31, average training loss: 7907.04, base loss: 15346.57
[INFO 2017-07-01 22:22:58,774 main.py:52] epoch 3161, training loss: 7036.32, average training loss: 7905.24, base loss: 15345.64
[INFO 2017-07-01 22:23:02,934 main.py:52] epoch 3162, training loss: 9040.25, average training loss: 7905.98, base loss: 15347.21
[INFO 2017-07-01 22:23:07,136 main.py:52] epoch 3163, training loss: 8026.47, average training loss: 7906.03, base loss: 15347.26
[INFO 2017-07-01 22:23:11,330 main.py:52] epoch 3164, training loss: 7336.12, average training loss: 7905.81, base loss: 15346.48
[INFO 2017-07-01 22:23:15,500 main.py:52] epoch 3165, training loss: 6913.46, average training loss: 7904.48, base loss: 15344.96
[INFO 2017-07-01 22:23:19,682 main.py:52] epoch 3166, training loss: 7613.96, average training loss: 7903.71, base loss: 15344.25
[INFO 2017-07-01 22:23:23,880 main.py:52] epoch 3167, training loss: 7965.29, average training loss: 7904.09, base loss: 15344.30
[INFO 2017-07-01 22:23:27,989 main.py:52] epoch 3168, training loss: 8048.17, average training loss: 7903.57, base loss: 15344.12
[INFO 2017-07-01 22:23:32,151 main.py:52] epoch 3169, training loss: 8758.14, average training loss: 7904.54, base loss: 15345.46
[INFO 2017-07-01 22:23:36,309 main.py:52] epoch 3170, training loss: 8068.37, average training loss: 7904.82, base loss: 15345.88
[INFO 2017-07-01 22:23:40,589 main.py:52] epoch 3171, training loss: 7835.33, average training loss: 7903.55, base loss: 15345.65
[INFO 2017-07-01 22:23:44,816 main.py:52] epoch 3172, training loss: 8154.64, average training loss: 7904.14, base loss: 15345.42
[INFO 2017-07-01 22:23:49,016 main.py:52] epoch 3173, training loss: 7263.92, average training loss: 7903.70, base loss: 15344.86
[INFO 2017-07-01 22:23:53,253 main.py:52] epoch 3174, training loss: 8628.96, average training loss: 7904.40, base loss: 15345.76
[INFO 2017-07-01 22:23:57,410 main.py:52] epoch 3175, training loss: 7190.96, average training loss: 7904.15, base loss: 15345.04
[INFO 2017-07-01 22:24:01,549 main.py:52] epoch 3176, training loss: 8467.26, average training loss: 7903.66, base loss: 15345.08
[INFO 2017-07-01 22:24:05,714 main.py:52] epoch 3177, training loss: 8194.61, average training loss: 7904.49, base loss: 15345.73
[INFO 2017-07-01 22:24:09,870 main.py:52] epoch 3178, training loss: 8280.81, average training loss: 7905.53, base loss: 15345.97
[INFO 2017-07-01 22:24:14,130 main.py:52] epoch 3179, training loss: 7237.26, average training loss: 7905.13, base loss: 15345.74
[INFO 2017-07-01 22:24:18,275 main.py:52] epoch 3180, training loss: 7586.53, average training loss: 7904.54, base loss: 15346.22
[INFO 2017-07-01 22:24:22,416 main.py:52] epoch 3181, training loss: 8426.35, average training loss: 7905.57, base loss: 15348.16
[INFO 2017-07-01 22:24:26,458 main.py:52] epoch 3182, training loss: 8853.57, average training loss: 7907.10, base loss: 15348.94
[INFO 2017-07-01 22:24:30,626 main.py:52] epoch 3183, training loss: 7670.10, average training loss: 7906.08, base loss: 15348.51
[INFO 2017-07-01 22:24:34,849 main.py:52] epoch 3184, training loss: 6848.96, average training loss: 7904.72, base loss: 15348.80
[INFO 2017-07-01 22:24:38,971 main.py:52] epoch 3185, training loss: 8436.67, average training loss: 7905.75, base loss: 15349.08
[INFO 2017-07-01 22:24:43,123 main.py:52] epoch 3186, training loss: 8894.20, average training loss: 7907.20, base loss: 15349.67
[INFO 2017-07-01 22:24:47,362 main.py:52] epoch 3187, training loss: 7761.20, average training loss: 7907.13, base loss: 15348.98
[INFO 2017-07-01 22:24:51,497 main.py:52] epoch 3188, training loss: 8066.27, average training loss: 7908.13, base loss: 15348.96
[INFO 2017-07-01 22:24:55,654 main.py:52] epoch 3189, training loss: 7989.10, average training loss: 7908.70, base loss: 15349.45
[INFO 2017-07-01 22:24:59,908 main.py:52] epoch 3190, training loss: 7329.05, average training loss: 7907.66, base loss: 15348.41
[INFO 2017-07-01 22:25:04,086 main.py:52] epoch 3191, training loss: 7966.90, average training loss: 7907.57, base loss: 15348.48
[INFO 2017-07-01 22:25:08,241 main.py:52] epoch 3192, training loss: 8153.00, average training loss: 7907.94, base loss: 15349.08
[INFO 2017-07-01 22:25:12,410 main.py:52] epoch 3193, training loss: 7660.57, average training loss: 7907.86, base loss: 15348.72
[INFO 2017-07-01 22:25:16,524 main.py:52] epoch 3194, training loss: 7881.93, average training loss: 7908.36, base loss: 15348.20
[INFO 2017-07-01 22:25:20,723 main.py:52] epoch 3195, training loss: 8402.27, average training loss: 7908.60, base loss: 15348.71
[INFO 2017-07-01 22:25:24,939 main.py:52] epoch 3196, training loss: 7274.57, average training loss: 7907.18, base loss: 15348.31
[INFO 2017-07-01 22:25:29,090 main.py:52] epoch 3197, training loss: 8145.90, average training loss: 7907.04, base loss: 15348.83
[INFO 2017-07-01 22:25:33,271 main.py:52] epoch 3198, training loss: 8199.55, average training loss: 7907.11, base loss: 15348.72
[INFO 2017-07-01 22:25:37,459 main.py:52] epoch 3199, training loss: 8047.65, average training loss: 7907.32, base loss: 15347.87
[INFO 2017-07-01 22:25:37,459 main.py:54] epoch 3199, testing
[INFO 2017-07-01 22:25:37,459 main.py:61] model save to ./model/final.pth
[INFO 2017-07-01 22:25:41,737 main.py:52] epoch 3200, training loss: 6831.15, average training loss: 7905.55, base loss: 15347.27
[INFO 2017-07-01 22:25:45,893 main.py:52] epoch 3201, training loss: 7549.44, average training loss: 7905.18, base loss: 15346.95
[INFO 2017-07-01 22:25:50,061 main.py:52] epoch 3202, training loss: 7495.99, average training loss: 7903.70, base loss: 15346.75
[INFO 2017-07-01 22:25:54,179 main.py:52] epoch 3203, training loss: 7569.45, average training loss: 7903.77, base loss: 15346.61
[INFO 2017-07-01 22:25:58,368 main.py:52] epoch 3204, training loss: 8419.38, average training loss: 7904.49, base loss: 15347.79
[INFO 2017-07-01 22:26:02,541 main.py:52] epoch 3205, training loss: 7848.17, average training loss: 7904.80, base loss: 15347.98
[INFO 2017-07-01 22:26:06,699 main.py:52] epoch 3206, training loss: 7639.68, average training loss: 7905.43, base loss: 15347.23
[INFO 2017-07-01 22:26:10,952 main.py:52] epoch 3207, training loss: 6862.81, average training loss: 7904.57, base loss: 15345.53
[INFO 2017-07-01 22:26:15,133 main.py:52] epoch 3208, training loss: 7838.18, average training loss: 7904.98, base loss: 15345.39
[INFO 2017-07-01 22:26:19,338 main.py:52] epoch 3209, training loss: 7894.76, average training loss: 7904.88, base loss: 15345.80
[INFO 2017-07-01 22:26:23,453 main.py:52] epoch 3210, training loss: 8015.08, average training loss: 7905.01, base loss: 15346.10
[INFO 2017-07-01 22:26:27,644 main.py:52] epoch 3211, training loss: 7609.80, average training loss: 7904.01, base loss: 15345.00
[INFO 2017-07-01 22:26:31,811 main.py:52] epoch 3212, training loss: 7189.20, average training loss: 7902.71, base loss: 15343.98
[INFO 2017-07-01 22:26:35,963 main.py:52] epoch 3213, training loss: 8144.58, average training loss: 7902.47, base loss: 15344.83
[INFO 2017-07-01 22:26:40,124 main.py:52] epoch 3214, training loss: 7215.44, average training loss: 7902.33, base loss: 15344.22
[INFO 2017-07-01 22:26:44,289 main.py:52] epoch 3215, training loss: 7421.27, average training loss: 7901.42, base loss: 15344.19
[INFO 2017-07-01 22:26:48,509 main.py:52] epoch 3216, training loss: 6968.14, average training loss: 7901.54, base loss: 15343.72
[INFO 2017-07-01 22:26:52,668 main.py:52] epoch 3217, training loss: 7731.34, average training loss: 7902.02, base loss: 15343.08
[INFO 2017-07-01 22:26:56,917 main.py:52] epoch 3218, training loss: 8221.42, average training loss: 7902.76, base loss: 15344.00
[INFO 2017-07-01 22:27:01,116 main.py:52] epoch 3219, training loss: 8026.17, average training loss: 7902.48, base loss: 15343.89
[INFO 2017-07-01 22:27:05,304 main.py:52] epoch 3220, training loss: 7968.95, average training loss: 7902.28, base loss: 15343.53
[INFO 2017-07-01 22:27:09,514 main.py:52] epoch 3221, training loss: 7032.09, average training loss: 7901.12, base loss: 15342.15
[INFO 2017-07-01 22:27:13,675 main.py:52] epoch 3222, training loss: 9002.98, average training loss: 7900.91, base loss: 15343.53
[INFO 2017-07-01 22:27:17,843 main.py:52] epoch 3223, training loss: 8269.11, average training loss: 7901.27, base loss: 15343.63
[INFO 2017-07-01 22:27:22,016 main.py:52] epoch 3224, training loss: 9451.57, average training loss: 7901.64, base loss: 15345.61
[INFO 2017-07-01 22:27:26,101 main.py:52] epoch 3225, training loss: 8429.79, average training loss: 7902.25, base loss: 15346.43
[INFO 2017-07-01 22:27:30,281 main.py:52] epoch 3226, training loss: 8283.85, average training loss: 7902.87, base loss: 15346.19
[INFO 2017-07-01 22:27:34,459 main.py:52] epoch 3227, training loss: 7656.87, average training loss: 7903.74, base loss: 15345.24
[INFO 2017-07-01 22:27:38,686 main.py:52] epoch 3228, training loss: 8179.34, average training loss: 7904.03, base loss: 15345.13
[INFO 2017-07-01 22:27:42,854 main.py:52] epoch 3229, training loss: 7516.36, average training loss: 7904.10, base loss: 15344.40
[INFO 2017-07-01 22:27:47,087 main.py:52] epoch 3230, training loss: 7090.31, average training loss: 7902.48, base loss: 15343.22
[INFO 2017-07-01 22:27:51,313 main.py:52] epoch 3231, training loss: 7471.90, average training loss: 7903.21, base loss: 15343.04
[INFO 2017-07-01 22:27:55,504 main.py:52] epoch 3232, training loss: 8010.57, average training loss: 7903.09, base loss: 15342.37
[INFO 2017-07-01 22:27:59,702 main.py:52] epoch 3233, training loss: 6657.50, average training loss: 7901.82, base loss: 15341.36
[INFO 2017-07-01 22:28:03,899 main.py:52] epoch 3234, training loss: 8475.39, average training loss: 7902.54, base loss: 15342.33
[INFO 2017-07-01 22:28:08,115 main.py:52] epoch 3235, training loss: 7525.84, average training loss: 7902.05, base loss: 15342.37
[INFO 2017-07-01 22:28:12,243 main.py:52] epoch 3236, training loss: 7293.67, average training loss: 7901.72, base loss: 15341.10
[INFO 2017-07-01 22:28:16,369 main.py:52] epoch 3237, training loss: 7923.05, average training loss: 7901.66, base loss: 15340.41
[INFO 2017-07-01 22:28:20,520 main.py:52] epoch 3238, training loss: 7315.53, average training loss: 7900.48, base loss: 15339.99
[INFO 2017-07-01 22:28:24,729 main.py:52] epoch 3239, training loss: 8019.66, average training loss: 7899.75, base loss: 15340.63
[INFO 2017-07-01 22:28:28,923 main.py:52] epoch 3240, training loss: 7938.34, average training loss: 7899.99, base loss: 15340.90
[INFO 2017-07-01 22:28:33,084 main.py:52] epoch 3241, training loss: 8121.18, average training loss: 7900.98, base loss: 15342.00
[INFO 2017-07-01 22:28:37,280 main.py:52] epoch 3242, training loss: 7727.26, average training loss: 7900.83, base loss: 15341.85
[INFO 2017-07-01 22:28:41,401 main.py:52] epoch 3243, training loss: 7390.74, average training loss: 7900.17, base loss: 15341.48
[INFO 2017-07-01 22:28:45,685 main.py:52] epoch 3244, training loss: 7436.41, average training loss: 7899.52, base loss: 15341.46
[INFO 2017-07-01 22:28:49,800 main.py:52] epoch 3245, training loss: 7492.26, average training loss: 7900.09, base loss: 15341.01
[INFO 2017-07-01 22:28:53,936 main.py:52] epoch 3246, training loss: 7592.52, average training loss: 7899.58, base loss: 15341.20
[INFO 2017-07-01 22:28:58,164 main.py:52] epoch 3247, training loss: 7052.28, average training loss: 7898.19, base loss: 15341.20
[INFO 2017-07-01 22:29:02,332 main.py:52] epoch 3248, training loss: 7953.39, average training loss: 7897.04, base loss: 15341.24
[INFO 2017-07-01 22:29:06,449 main.py:52] epoch 3249, training loss: 8159.64, average training loss: 7897.28, base loss: 15341.45
[INFO 2017-07-01 22:29:10,727 main.py:52] epoch 3250, training loss: 7430.18, average training loss: 7896.25, base loss: 15340.50
[INFO 2017-07-01 22:29:14,866 main.py:52] epoch 3251, training loss: 7560.67, average training loss: 7895.76, base loss: 15340.35
[INFO 2017-07-01 22:29:19,038 main.py:52] epoch 3252, training loss: 8385.46, average training loss: 7896.30, base loss: 15340.16
[INFO 2017-07-01 22:29:23,184 main.py:52] epoch 3253, training loss: 8109.13, average training loss: 7896.77, base loss: 15340.06
[INFO 2017-07-01 22:29:27,414 main.py:52] epoch 3254, training loss: 7140.72, average training loss: 7896.35, base loss: 15340.10
[INFO 2017-07-01 22:29:31,559 main.py:52] epoch 3255, training loss: 7065.16, average training loss: 7895.21, base loss: 15339.98
[INFO 2017-07-01 22:29:35,708 main.py:52] epoch 3256, training loss: 7002.69, average training loss: 7893.90, base loss: 15338.99
[INFO 2017-07-01 22:29:39,815 main.py:52] epoch 3257, training loss: 8329.77, average training loss: 7894.18, base loss: 15338.92
[INFO 2017-07-01 22:29:43,929 main.py:52] epoch 3258, training loss: 7307.15, average training loss: 7893.96, base loss: 15338.64
[INFO 2017-07-01 22:29:48,144 main.py:52] epoch 3259, training loss: 7760.15, average training loss: 7893.95, base loss: 15339.00
[INFO 2017-07-01 22:29:52,333 main.py:52] epoch 3260, training loss: 8472.08, average training loss: 7894.12, base loss: 15339.62
[INFO 2017-07-01 22:29:56,546 main.py:52] epoch 3261, training loss: 8048.59, average training loss: 7893.91, base loss: 15340.03
[INFO 2017-07-01 22:30:00,725 main.py:52] epoch 3262, training loss: 8696.36, average training loss: 7894.48, base loss: 15340.54
[INFO 2017-07-01 22:30:04,934 main.py:52] epoch 3263, training loss: 7393.46, average training loss: 7893.49, base loss: 15340.03
[INFO 2017-07-01 22:30:09,097 main.py:52] epoch 3264, training loss: 7970.36, average training loss: 7894.41, base loss: 15340.40
[INFO 2017-07-01 22:30:13,266 main.py:52] epoch 3265, training loss: 7344.44, average training loss: 7893.77, base loss: 15339.82
[INFO 2017-07-01 22:30:17,427 main.py:52] epoch 3266, training loss: 9002.68, average training loss: 7895.45, base loss: 15341.34
[INFO 2017-07-01 22:30:21,598 main.py:52] epoch 3267, training loss: 7445.63, average training loss: 7895.02, base loss: 15341.18
[INFO 2017-07-01 22:30:25,864 main.py:52] epoch 3268, training loss: 9206.77, average training loss: 7896.24, base loss: 15342.02
[INFO 2017-07-01 22:30:30,088 main.py:52] epoch 3269, training loss: 7576.16, average training loss: 7896.80, base loss: 15341.84
[INFO 2017-07-01 22:30:34,254 main.py:52] epoch 3270, training loss: 7589.63, average training loss: 7895.95, base loss: 15342.42
[INFO 2017-07-01 22:30:38,416 main.py:52] epoch 3271, training loss: 7751.09, average training loss: 7896.12, base loss: 15342.49
[INFO 2017-07-01 22:30:42,556 main.py:52] epoch 3272, training loss: 7687.39, average training loss: 7895.94, base loss: 15343.33
[INFO 2017-07-01 22:30:46,704 main.py:52] epoch 3273, training loss: 8045.27, average training loss: 7896.04, base loss: 15344.22
[INFO 2017-07-01 22:30:50,901 main.py:52] epoch 3274, training loss: 8553.79, average training loss: 7897.15, base loss: 15344.80
[INFO 2017-07-01 22:30:54,985 main.py:52] epoch 3275, training loss: 8539.69, average training loss: 7896.69, base loss: 15345.33
[INFO 2017-07-01 22:30:59,248 main.py:52] epoch 3276, training loss: 7626.34, average training loss: 7896.20, base loss: 15345.57
[INFO 2017-07-01 22:31:03,416 main.py:52] epoch 3277, training loss: 7340.16, average training loss: 7894.88, base loss: 15345.84
[INFO 2017-07-01 22:31:07,561 main.py:52] epoch 3278, training loss: 7474.21, average training loss: 7894.05, base loss: 15345.79
[INFO 2017-07-01 22:31:11,758 main.py:52] epoch 3279, training loss: 8350.64, average training loss: 7894.19, base loss: 15347.00
[INFO 2017-07-01 22:31:15,970 main.py:52] epoch 3280, training loss: 8171.25, average training loss: 7895.09, base loss: 15347.89
[INFO 2017-07-01 22:31:20,063 main.py:52] epoch 3281, training loss: 7222.44, average training loss: 7893.91, base loss: 15347.36
[INFO 2017-07-01 22:31:24,301 main.py:52] epoch 3282, training loss: 7737.02, average training loss: 7893.30, base loss: 15348.11
[INFO 2017-07-01 22:31:28,393 main.py:52] epoch 3283, training loss: 7851.56, average training loss: 7894.10, base loss: 15348.46
[INFO 2017-07-01 22:31:32,502 main.py:52] epoch 3284, training loss: 8547.36, average training loss: 7895.03, base loss: 15349.88
[INFO 2017-07-01 22:31:36,640 main.py:52] epoch 3285, training loss: 8272.34, average training loss: 7895.97, base loss: 15351.50
[INFO 2017-07-01 22:31:40,805 main.py:52] epoch 3286, training loss: 7112.18, average training loss: 7894.95, base loss: 15351.04
[INFO 2017-07-01 22:31:44,948 main.py:52] epoch 3287, training loss: 8097.81, average training loss: 7895.76, base loss: 15351.46
[INFO 2017-07-01 22:31:49,067 main.py:52] epoch 3288, training loss: 7448.17, average training loss: 7896.06, base loss: 15350.70
[INFO 2017-07-01 22:31:53,366 main.py:52] epoch 3289, training loss: 8093.18, average training loss: 7896.42, base loss: 15351.75
[INFO 2017-07-01 22:31:57,557 main.py:52] epoch 3290, training loss: 7368.52, average training loss: 7895.95, base loss: 15351.03
[INFO 2017-07-01 22:32:01,777 main.py:52] epoch 3291, training loss: 7877.27, average training loss: 7895.51, base loss: 15350.26
[INFO 2017-07-01 22:32:05,912 main.py:52] epoch 3292, training loss: 8191.60, average training loss: 7895.52, base loss: 15350.13
[INFO 2017-07-01 22:32:10,069 main.py:52] epoch 3293, training loss: 8054.73, average training loss: 7894.79, base loss: 15350.69
[INFO 2017-07-01 22:32:14,203 main.py:52] epoch 3294, training loss: 7817.46, average training loss: 7895.19, base loss: 15350.50
[INFO 2017-07-01 22:32:18,418 main.py:52] epoch 3295, training loss: 7398.90, average training loss: 7895.35, base loss: 15350.53
[INFO 2017-07-01 22:32:22,548 main.py:52] epoch 3296, training loss: 8317.03, average training loss: 7895.48, base loss: 15351.09
[INFO 2017-07-01 22:32:26,743 main.py:52] epoch 3297, training loss: 7999.73, average training loss: 7894.92, base loss: 15351.83
[INFO 2017-07-01 22:32:30,882 main.py:52] epoch 3298, training loss: 7907.42, average training loss: 7895.87, base loss: 15352.73
[INFO 2017-07-01 22:32:35,058 main.py:52] epoch 3299, training loss: 7963.89, average training loss: 7895.62, base loss: 15353.02
[INFO 2017-07-01 22:32:35,059 main.py:54] epoch 3299, testing
[INFO 2017-07-01 22:32:35,059 main.py:61] model save to ./model/final.pth
[INFO 2017-07-01 22:32:39,280 main.py:52] epoch 3300, training loss: 7762.89, average training loss: 7895.70, base loss: 15353.14
[INFO 2017-07-01 22:32:43,401 main.py:52] epoch 3301, training loss: 7872.46, average training loss: 7895.77, base loss: 15353.41
[INFO 2017-07-01 22:32:47,646 main.py:52] epoch 3302, training loss: 7249.09, average training loss: 7893.84, base loss: 15353.13
[INFO 2017-07-01 22:32:51,821 main.py:52] epoch 3303, training loss: 7482.06, average training loss: 7893.93, base loss: 15351.70
[INFO 2017-07-01 22:32:55,940 main.py:52] epoch 3304, training loss: 7784.45, average training loss: 7893.67, base loss: 15351.56
[INFO 2017-07-01 22:33:00,080 main.py:52] epoch 3305, training loss: 7695.89, average training loss: 7893.49, base loss: 15352.01
[INFO 2017-07-01 22:33:04,223 main.py:52] epoch 3306, training loss: 7926.73, average training loss: 7893.10, base loss: 15352.02
[INFO 2017-07-01 22:33:08,362 main.py:52] epoch 3307, training loss: 8710.88, average training loss: 7892.98, base loss: 15353.22
[INFO 2017-07-01 22:33:12,519 main.py:52] epoch 3308, training loss: 8532.71, average training loss: 7893.73, base loss: 15353.29
[INFO 2017-07-01 22:33:16,777 main.py:52] epoch 3309, training loss: 8257.82, average training loss: 7894.35, base loss: 15354.64
[INFO 2017-07-01 22:33:20,979 main.py:52] epoch 3310, training loss: 9090.16, average training loss: 7895.21, base loss: 15355.56
[INFO 2017-07-01 22:33:25,208 main.py:52] epoch 3311, training loss: 7833.60, average training loss: 7894.14, base loss: 15355.46
[INFO 2017-07-01 22:33:29,379 main.py:52] epoch 3312, training loss: 7896.26, average training loss: 7893.67, base loss: 15355.21
[INFO 2017-07-01 22:33:33,504 main.py:52] epoch 3313, training loss: 7447.03, average training loss: 7892.96, base loss: 15355.02
[INFO 2017-07-01 22:33:37,677 main.py:52] epoch 3314, training loss: 7660.99, average training loss: 7892.82, base loss: 15354.86
[INFO 2017-07-01 22:33:41,798 main.py:52] epoch 3315, training loss: 8302.84, average training loss: 7893.10, base loss: 15354.26
[INFO 2017-07-01 22:33:45,964 main.py:52] epoch 3316, training loss: 6783.37, average training loss: 7891.52, base loss: 15353.79
[INFO 2017-07-01 22:33:50,116 main.py:52] epoch 3317, training loss: 7717.67, average training loss: 7891.47, base loss: 15354.33
[INFO 2017-07-01 22:33:54,256 main.py:52] epoch 3318, training loss: 7625.39, average training loss: 7890.83, base loss: 15353.80
[INFO 2017-07-01 22:33:58,446 main.py:52] epoch 3319, training loss: 7727.75, average training loss: 7891.17, base loss: 15353.12
[INFO 2017-07-01 22:34:02,631 main.py:52] epoch 3320, training loss: 9291.67, average training loss: 7892.49, base loss: 15354.49
[INFO 2017-07-01 22:34:06,828 main.py:52] epoch 3321, training loss: 7435.41, average training loss: 7891.87, base loss: 15353.56
[INFO 2017-07-01 22:34:11,018 main.py:52] epoch 3322, training loss: 7834.91, average training loss: 7891.21, base loss: 15353.37
[INFO 2017-07-01 22:34:15,140 main.py:52] epoch 3323, training loss: 7982.68, average training loss: 7890.63, base loss: 15353.54
[INFO 2017-07-01 22:34:19,244 main.py:52] epoch 3324, training loss: 8144.24, average training loss: 7891.70, base loss: 15353.93
[INFO 2017-07-01 22:34:23,400 main.py:52] epoch 3325, training loss: 7259.40, average training loss: 7891.29, base loss: 15354.20
[INFO 2017-07-01 22:34:27,599 main.py:52] epoch 3326, training loss: 7136.78, average training loss: 7890.47, base loss: 15353.63
[INFO 2017-07-01 22:34:31,749 main.py:52] epoch 3327, training loss: 7258.83, average training loss: 7889.74, base loss: 15353.31
[INFO 2017-07-01 22:34:35,931 main.py:52] epoch 3328, training loss: 7866.53, average training loss: 7889.73, base loss: 15353.30
[INFO 2017-07-01 22:34:40,080 main.py:52] epoch 3329, training loss: 7759.12, average training loss: 7889.80, base loss: 15353.70
[INFO 2017-07-01 22:34:44,273 main.py:52] epoch 3330, training loss: 8152.44, average training loss: 7890.18, base loss: 15353.86
[INFO 2017-07-01 22:34:48,436 main.py:52] epoch 3331, training loss: 7863.50, average training loss: 7890.57, base loss: 15354.21
[INFO 2017-07-01 22:34:52,586 main.py:52] epoch 3332, training loss: 7326.65, average training loss: 7889.88, base loss: 15353.69
[INFO 2017-07-01 22:34:56,714 main.py:52] epoch 3333, training loss: 7055.31, average training loss: 7889.11, base loss: 15352.67
[INFO 2017-07-01 22:35:00,843 main.py:52] epoch 3334, training loss: 7541.74, average training loss: 7888.28, base loss: 15351.93
[INFO 2017-07-01 22:35:05,046 main.py:52] epoch 3335, training loss: 8248.47, average training loss: 7889.65, base loss: 15351.86
[INFO 2017-07-01 22:35:09,152 main.py:52] epoch 3336, training loss: 8224.43, average training loss: 7888.90, base loss: 15352.39
[INFO 2017-07-01 22:35:13,338 main.py:52] epoch 3337, training loss: 7810.27, average training loss: 7888.87, base loss: 15352.89
[INFO 2017-07-01 22:35:17,421 main.py:52] epoch 3338, training loss: 6743.15, average training loss: 7887.48, base loss: 15352.14
[INFO 2017-07-01 22:35:21,673 main.py:52] epoch 3339, training loss: 7153.15, average training loss: 7886.94, base loss: 15351.77
[INFO 2017-07-01 22:35:25,850 main.py:52] epoch 3340, training loss: 7912.76, average training loss: 7886.33, base loss: 15351.12
[INFO 2017-07-01 22:35:30,079 main.py:52] epoch 3341, training loss: 9212.61, average training loss: 7888.23, base loss: 15351.55
[INFO 2017-07-01 22:35:34,233 main.py:52] epoch 3342, training loss: 7955.29, average training loss: 7888.47, base loss: 15350.76
[INFO 2017-07-01 22:35:38,359 main.py:52] epoch 3343, training loss: 7642.06, average training loss: 7888.59, base loss: 15350.22
[INFO 2017-07-01 22:35:42,454 main.py:52] epoch 3344, training loss: 8418.28, average training loss: 7888.26, base loss: 15351.09
[INFO 2017-07-01 22:35:46,617 main.py:52] epoch 3345, training loss: 6989.99, average training loss: 7887.98, base loss: 15349.52
[INFO 2017-07-01 22:35:50,880 main.py:52] epoch 3346, training loss: 8192.03, average training loss: 7887.50, base loss: 15349.71
[INFO 2017-07-01 22:35:55,020 main.py:52] epoch 3347, training loss: 8359.96, average training loss: 7887.93, base loss: 15350.25
[INFO 2017-07-01 22:35:59,131 main.py:52] epoch 3348, training loss: 7540.90, average training loss: 7887.24, base loss: 15350.07
[INFO 2017-07-01 22:36:03,265 main.py:52] epoch 3349, training loss: 7195.59, average training loss: 7887.02, base loss: 15349.35
[INFO 2017-07-01 22:36:07,397 main.py:52] epoch 3350, training loss: 6980.64, average training loss: 7884.99, base loss: 15349.30
[INFO 2017-07-01 22:36:11,520 main.py:52] epoch 3351, training loss: 7679.83, average training loss: 7885.23, base loss: 15350.04
[INFO 2017-07-01 22:36:15,704 main.py:52] epoch 3352, training loss: 8379.61, average training loss: 7886.17, base loss: 15350.46
[INFO 2017-07-01 22:36:19,831 main.py:52] epoch 3353, training loss: 8163.15, average training loss: 7885.90, base loss: 15350.08
[INFO 2017-07-01 22:36:23,992 main.py:52] epoch 3354, training loss: 8243.71, average training loss: 7886.43, base loss: 15349.69
[INFO 2017-07-01 22:36:28,104 main.py:52] epoch 3355, training loss: 7935.62, average training loss: 7886.56, base loss: 15350.14
[INFO 2017-07-01 22:36:32,265 main.py:52] epoch 3356, training loss: 7902.06, average training loss: 7887.58, base loss: 15349.55
[INFO 2017-07-01 22:36:36,457 main.py:52] epoch 3357, training loss: 7484.08, average training loss: 7887.04, base loss: 15349.21
[INFO 2017-07-01 22:36:40,673 main.py:52] epoch 3358, training loss: 8009.66, average training loss: 7887.61, base loss: 15349.85
[INFO 2017-07-01 22:36:44,834 main.py:52] epoch 3359, training loss: 6897.40, average training loss: 7886.65, base loss: 15349.72
[INFO 2017-07-01 22:36:48,980 main.py:52] epoch 3360, training loss: 8439.76, average training loss: 7886.26, base loss: 15349.97
[INFO 2017-07-01 22:36:53,078 main.py:52] epoch 3361, training loss: 7833.51, average training loss: 7886.13, base loss: 15350.67
[INFO 2017-07-01 22:36:57,195 main.py:52] epoch 3362, training loss: 8114.18, average training loss: 7886.23, base loss: 15350.97
[INFO 2017-07-01 22:37:01,360 main.py:52] epoch 3363, training loss: 7524.34, average training loss: 7884.66, base loss: 15351.19
[INFO 2017-07-01 22:37:05,556 main.py:52] epoch 3364, training loss: 7465.07, average training loss: 7884.28, base loss: 15350.41
[INFO 2017-07-01 22:37:09,693 main.py:52] epoch 3365, training loss: 7375.88, average training loss: 7883.92, base loss: 15349.10
[INFO 2017-07-01 22:37:13,781 main.py:52] epoch 3366, training loss: 7792.17, average training loss: 7883.32, base loss: 15349.10
[INFO 2017-07-01 22:37:17,952 main.py:52] epoch 3367, training loss: 7421.23, average training loss: 7882.07, base loss: 15348.43
[INFO 2017-07-01 22:37:22,124 main.py:52] epoch 3368, training loss: 7400.94, average training loss: 7882.70, base loss: 15347.79
[INFO 2017-07-01 22:37:26,289 main.py:52] epoch 3369, training loss: 7654.13, average training loss: 7882.07, base loss: 15347.46
[INFO 2017-07-01 22:37:30,490 main.py:52] epoch 3370, training loss: 8016.74, average training loss: 7882.62, base loss: 15347.70
[INFO 2017-07-01 22:37:34,609 main.py:52] epoch 3371, training loss: 7895.42, average training loss: 7882.54, base loss: 15348.25
[INFO 2017-07-01 22:37:38,776 main.py:52] epoch 3372, training loss: 7223.53, average training loss: 7881.61, base loss: 15348.10
[INFO 2017-07-01 22:37:42,916 main.py:52] epoch 3373, training loss: 8490.38, average training loss: 7881.91, base loss: 15348.28
[INFO 2017-07-01 22:37:47,107 main.py:52] epoch 3374, training loss: 8662.91, average training loss: 7883.83, base loss: 15348.70
[INFO 2017-07-01 22:37:51,236 main.py:52] epoch 3375, training loss: 8364.98, average training loss: 7884.22, base loss: 15349.21
[INFO 2017-07-01 22:37:55,459 main.py:52] epoch 3376, training loss: 6980.45, average training loss: 7883.19, base loss: 15347.93
[INFO 2017-07-01 22:37:59,604 main.py:52] epoch 3377, training loss: 8332.06, average training loss: 7883.83, base loss: 15348.79
[INFO 2017-07-01 22:38:03,822 main.py:52] epoch 3378, training loss: 7290.11, average training loss: 7883.84, base loss: 15349.69
[INFO 2017-07-01 22:38:07,993 main.py:52] epoch 3379, training loss: 8329.35, average training loss: 7884.43, base loss: 15350.50
[INFO 2017-07-01 22:38:12,234 main.py:52] epoch 3380, training loss: 7176.68, average training loss: 7883.74, base loss: 15349.87
[INFO 2017-07-01 22:38:16,398 main.py:52] epoch 3381, training loss: 7424.50, average training loss: 7882.84, base loss: 15349.22
[INFO 2017-07-01 22:38:20,568 main.py:52] epoch 3382, training loss: 8235.77, average training loss: 7882.92, base loss: 15349.05
[INFO 2017-07-01 22:38:24,666 main.py:52] epoch 3383, training loss: 7332.39, average training loss: 7883.01, base loss: 15348.59
[INFO 2017-07-01 22:38:28,859 main.py:52] epoch 3384, training loss: 6744.83, average training loss: 7881.73, base loss: 15348.63
[INFO 2017-07-01 22:38:33,008 main.py:52] epoch 3385, training loss: 6504.27, average training loss: 7881.20, base loss: 15347.82
[INFO 2017-07-01 22:38:37,193 main.py:52] epoch 3386, training loss: 7793.22, average training loss: 7881.30, base loss: 15347.74
[INFO 2017-07-01 22:38:41,416 main.py:52] epoch 3387, training loss: 8304.22, average training loss: 7882.18, base loss: 15348.39
[INFO 2017-07-01 22:38:45,615 main.py:52] epoch 3388, training loss: 7369.06, average training loss: 7882.75, base loss: 15347.86
[INFO 2017-07-01 22:38:49,830 main.py:52] epoch 3389, training loss: 6786.73, average training loss: 7880.81, base loss: 15346.91
[INFO 2017-07-01 22:38:53,962 main.py:52] epoch 3390, training loss: 7282.81, average training loss: 7880.99, base loss: 15345.90
[INFO 2017-07-01 22:38:58,155 main.py:52] epoch 3391, training loss: 7284.24, average training loss: 7879.61, base loss: 15345.14
[INFO 2017-07-01 22:39:02,269 main.py:52] epoch 3392, training loss: 7937.48, average training loss: 7878.68, base loss: 15344.75
[INFO 2017-07-01 22:39:06,454 main.py:52] epoch 3393, training loss: 7553.86, average training loss: 7877.50, base loss: 15343.93
[INFO 2017-07-01 22:39:10,636 main.py:52] epoch 3394, training loss: 7252.56, average training loss: 7876.82, base loss: 15343.34
[INFO 2017-07-01 22:39:14,781 main.py:52] epoch 3395, training loss: 6096.17, average training loss: 7874.92, base loss: 15340.81
[INFO 2017-07-01 22:39:18,890 main.py:52] epoch 3396, training loss: 8031.86, average training loss: 7875.11, base loss: 15341.56
[INFO 2017-07-01 22:39:23,125 main.py:52] epoch 3397, training loss: 7608.73, average training loss: 7874.64, base loss: 15341.63
[INFO 2017-07-01 22:39:27,335 main.py:52] epoch 3398, training loss: 8267.64, average training loss: 7875.68, base loss: 15343.06
[INFO 2017-07-01 22:39:31,466 main.py:52] epoch 3399, training loss: 7921.73, average training loss: 7875.20, base loss: 15344.06
[INFO 2017-07-01 22:39:31,466 main.py:54] epoch 3399, testing
[INFO 2017-07-01 22:39:31,466 main.py:61] model save to ./model/final.pth
[INFO 2017-07-01 22:39:35,717 main.py:52] epoch 3400, training loss: 8195.10, average training loss: 7874.11, base loss: 15343.55
[INFO 2017-07-01 22:39:39,861 main.py:52] epoch 3401, training loss: 7334.92, average training loss: 7874.56, base loss: 15342.69
[INFO 2017-07-01 22:39:44,041 main.py:52] epoch 3402, training loss: 9099.21, average training loss: 7875.38, base loss: 15343.22
[INFO 2017-07-01 22:39:48,180 main.py:52] epoch 3403, training loss: 7834.61, average training loss: 7875.01, base loss: 15342.88
[INFO 2017-07-01 22:39:52,329 main.py:52] epoch 3404, training loss: 8138.69, average training loss: 7874.95, base loss: 15342.25
[INFO 2017-07-01 22:39:56,484 main.py:52] epoch 3405, training loss: 7863.97, average training loss: 7875.25, base loss: 15342.19
[INFO 2017-07-01 22:40:00,632 main.py:52] epoch 3406, training loss: 7235.42, average training loss: 7875.27, base loss: 15342.00
[INFO 2017-07-01 22:40:04,857 main.py:52] epoch 3407, training loss: 8628.93, average training loss: 7876.64, base loss: 15343.39
[INFO 2017-07-01 22:40:08,967 main.py:52] epoch 3408, training loss: 6803.04, average training loss: 7875.66, base loss: 15342.88
[INFO 2017-07-01 22:40:13,162 main.py:52] epoch 3409, training loss: 8760.18, average training loss: 7875.51, base loss: 15343.61
[INFO 2017-07-01 22:40:17,382 main.py:52] epoch 3410, training loss: 8750.55, average training loss: 7877.06, base loss: 15344.91
[INFO 2017-07-01 22:40:21,514 main.py:52] epoch 3411, training loss: 8394.04, average training loss: 7877.44, base loss: 15346.21
[INFO 2017-07-01 22:40:25,762 main.py:52] epoch 3412, training loss: 8002.82, average training loss: 7878.56, base loss: 15346.57
[INFO 2017-07-01 22:40:29,893 main.py:52] epoch 3413, training loss: 8285.55, average training loss: 7878.84, base loss: 15347.37
[INFO 2017-07-01 22:40:34,094 main.py:52] epoch 3414, training loss: 8164.96, average training loss: 7879.41, base loss: 15347.29
[INFO 2017-07-01 22:40:38,218 main.py:52] epoch 3415, training loss: 7336.55, average training loss: 7878.60, base loss: 15346.80
[INFO 2017-07-01 22:40:42,422 main.py:52] epoch 3416, training loss: 7925.49, average training loss: 7878.68, base loss: 15346.85
[INFO 2017-07-01 22:40:46,525 main.py:52] epoch 3417, training loss: 8372.92, average training loss: 7879.08, base loss: 15347.96
[INFO 2017-07-01 22:40:50,731 main.py:52] epoch 3418, training loss: 7760.70, average training loss: 7878.66, base loss: 15348.04
[INFO 2017-07-01 22:40:54,891 main.py:52] epoch 3419, training loss: 7182.98, average training loss: 7876.53, base loss: 15347.43
[INFO 2017-07-01 22:40:59,070 main.py:52] epoch 3420, training loss: 7876.40, average training loss: 7877.06, base loss: 15347.60
[INFO 2017-07-01 22:41:03,258 main.py:52] epoch 3421, training loss: 6918.26, average training loss: 7874.62, base loss: 15346.45
[INFO 2017-07-01 22:41:07,461 main.py:52] epoch 3422, training loss: 6922.69, average training loss: 7874.13, base loss: 15345.22
[INFO 2017-07-01 22:41:11,604 main.py:52] epoch 3423, training loss: 7488.55, average training loss: 7873.29, base loss: 15344.87
[INFO 2017-07-01 22:41:15,787 main.py:52] epoch 3424, training loss: 7940.94, average training loss: 7873.22, base loss: 15344.60
[INFO 2017-07-01 22:41:19,889 main.py:52] epoch 3425, training loss: 8148.66, average training loss: 7872.54, base loss: 15344.58
[INFO 2017-07-01 22:41:24,015 main.py:52] epoch 3426, training loss: 7624.22, average training loss: 7870.42, base loss: 15344.60
[INFO 2017-07-01 22:41:28,170 main.py:52] epoch 3427, training loss: 8019.50, average training loss: 7871.21, base loss: 15344.27
[INFO 2017-07-01 22:41:32,329 main.py:52] epoch 3428, training loss: 7920.33, average training loss: 7872.18, base loss: 15344.09
[INFO 2017-07-01 22:41:36,495 main.py:52] epoch 3429, training loss: 8929.84, average training loss: 7873.65, base loss: 15344.28
[INFO 2017-07-01 22:41:40,698 main.py:52] epoch 3430, training loss: 8457.78, average training loss: 7874.70, base loss: 15344.59
[INFO 2017-07-01 22:41:44,820 main.py:52] epoch 3431, training loss: 7612.74, average training loss: 7874.39, base loss: 15344.84
[INFO 2017-07-01 22:41:48,930 main.py:52] epoch 3432, training loss: 8392.66, average training loss: 7875.00, base loss: 15345.73
[INFO 2017-07-01 22:41:53,111 main.py:52] epoch 3433, training loss: 7507.49, average training loss: 7874.03, base loss: 15345.48
[INFO 2017-07-01 22:41:57,332 main.py:52] epoch 3434, training loss: 6935.50, average training loss: 7873.43, base loss: 15344.51
[INFO 2017-07-01 22:42:01,470 main.py:52] epoch 3435, training loss: 7891.15, average training loss: 7873.90, base loss: 15344.43
[INFO 2017-07-01 22:42:05,664 main.py:52] epoch 3436, training loss: 7957.68, average training loss: 7874.07, base loss: 15345.30
[INFO 2017-07-01 22:42:09,857 main.py:52] epoch 3437, training loss: 7658.77, average training loss: 7873.92, base loss: 15345.57
[INFO 2017-07-01 22:42:13,972 main.py:52] epoch 3438, training loss: 8319.86, average training loss: 7874.22, base loss: 15345.72
[INFO 2017-07-01 22:42:18,101 main.py:52] epoch 3439, training loss: 8084.52, average training loss: 7874.01, base loss: 15345.56
[INFO 2017-07-01 22:42:22,327 main.py:52] epoch 3440, training loss: 7554.69, average training loss: 7873.39, base loss: 15345.95
[INFO 2017-07-01 22:42:26,485 main.py:52] epoch 3441, training loss: 7056.04, average training loss: 7872.40, base loss: 15345.77
[INFO 2017-07-01 22:42:30,687 main.py:52] epoch 3442, training loss: 7639.43, average training loss: 7872.06, base loss: 15345.18
[INFO 2017-07-01 22:42:34,901 main.py:52] epoch 3443, training loss: 6937.10, average training loss: 7871.35, base loss: 15344.41
[INFO 2017-07-01 22:42:39,064 main.py:52] epoch 3444, training loss: 8691.20, average training loss: 7873.00, base loss: 15346.09
[INFO 2017-07-01 22:42:43,194 main.py:52] epoch 3445, training loss: 7468.37, average training loss: 7872.26, base loss: 15346.72
[INFO 2017-07-01 22:42:47,363 main.py:52] epoch 3446, training loss: 7712.42, average training loss: 7871.06, base loss: 15346.82
[INFO 2017-07-01 22:42:51,510 main.py:52] epoch 3447, training loss: 7790.13, average training loss: 7871.26, base loss: 15347.20
[INFO 2017-07-01 22:42:55,663 main.py:52] epoch 3448, training loss: 7991.56, average training loss: 7871.73, base loss: 15348.01
[INFO 2017-07-01 22:42:59,807 main.py:52] epoch 3449, training loss: 7369.46, average training loss: 7871.16, base loss: 15347.05
[INFO 2017-07-01 22:43:04,007 main.py:52] epoch 3450, training loss: 7321.24, average training loss: 7871.40, base loss: 15346.83
[INFO 2017-07-01 22:43:08,153 main.py:52] epoch 3451, training loss: 7803.49, average training loss: 7870.98, base loss: 15346.92
[INFO 2017-07-01 22:43:12,399 main.py:52] epoch 3452, training loss: 8002.60, average training loss: 7870.47, base loss: 15346.98
[INFO 2017-07-01 22:43:16,546 main.py:52] epoch 3453, training loss: 7533.53, average training loss: 7870.51, base loss: 15346.90
[INFO 2017-07-01 22:43:20,755 main.py:52] epoch 3454, training loss: 8250.75, average training loss: 7870.74, base loss: 15347.13
[INFO 2017-07-01 22:43:24,979 main.py:52] epoch 3455, training loss: 7050.35, average training loss: 7869.78, base loss: 15346.31
[INFO 2017-07-01 22:43:29,091 main.py:52] epoch 3456, training loss: 7852.09, average training loss: 7869.71, base loss: 15346.22
[INFO 2017-07-01 22:43:33,325 main.py:52] epoch 3457, training loss: 7378.25, average training loss: 7869.25, base loss: 15345.03
[INFO 2017-07-01 22:43:37,481 main.py:52] epoch 3458, training loss: 7693.68, average training loss: 7869.38, base loss: 15344.03
[INFO 2017-07-01 22:43:41,632 main.py:52] epoch 3459, training loss: 7279.18, average training loss: 7869.49, base loss: 15343.20
[INFO 2017-07-01 22:43:45,823 main.py:52] epoch 3460, training loss: 6876.27, average training loss: 7868.67, base loss: 15341.96
[INFO 2017-07-01 22:43:50,003 main.py:52] epoch 3461, training loss: 8252.83, average training loss: 7868.81, base loss: 15342.27
[INFO 2017-07-01 22:43:54,085 main.py:52] epoch 3462, training loss: 7760.34, average training loss: 7867.62, base loss: 15341.78
[INFO 2017-07-01 22:43:58,205 main.py:52] epoch 3463, training loss: 7566.66, average training loss: 7868.07, base loss: 15341.07
[INFO 2017-07-01 22:44:02,352 main.py:52] epoch 3464, training loss: 8138.68, average training loss: 7867.90, base loss: 15341.56
[INFO 2017-07-01 22:44:06,550 main.py:52] epoch 3465, training loss: 8567.93, average training loss: 7868.18, base loss: 15342.65
[INFO 2017-07-01 22:44:10,773 main.py:52] epoch 3466, training loss: 8098.74, average training loss: 7867.89, base loss: 15343.32
[INFO 2017-07-01 22:44:14,895 main.py:52] epoch 3467, training loss: 8002.12, average training loss: 7868.41, base loss: 15343.68
[INFO 2017-07-01 22:44:19,103 main.py:52] epoch 3468, training loss: 7525.41, average training loss: 7868.19, base loss: 15343.65
[INFO 2017-07-01 22:44:23,220 main.py:52] epoch 3469, training loss: 7735.30, average training loss: 7867.78, base loss: 15343.92
[INFO 2017-07-01 22:44:27,390 main.py:52] epoch 3470, training loss: 7306.66, average training loss: 7866.11, base loss: 15343.22
[INFO 2017-07-01 22:44:31,547 main.py:52] epoch 3471, training loss: 7850.80, average training loss: 7865.33, base loss: 15342.67
[INFO 2017-07-01 22:44:35,762 main.py:52] epoch 3472, training loss: 8443.50, average training loss: 7865.62, base loss: 15342.49
[INFO 2017-07-01 22:44:39,963 main.py:52] epoch 3473, training loss: 7520.08, average training loss: 7865.62, base loss: 15341.73
[INFO 2017-07-01 22:44:44,124 main.py:52] epoch 3474, training loss: 8308.73, average training loss: 7866.02, base loss: 15342.06
[INFO 2017-07-01 22:44:48,271 main.py:52] epoch 3475, training loss: 7722.63, average training loss: 7865.54, base loss: 15342.24
[INFO 2017-07-01 22:44:52,486 main.py:52] epoch 3476, training loss: 8637.21, average training loss: 7866.18, base loss: 15342.50
[INFO 2017-07-01 22:44:56,610 main.py:52] epoch 3477, training loss: 8512.82, average training loss: 7867.37, base loss: 15342.65
[INFO 2017-07-01 22:45:00,792 main.py:52] epoch 3478, training loss: 7498.74, average training loss: 7867.38, base loss: 15342.10
[INFO 2017-07-01 22:45:04,944 main.py:52] epoch 3479, training loss: 7747.54, average training loss: 7867.76, base loss: 15341.60
[INFO 2017-07-01 22:45:09,164 main.py:52] epoch 3480, training loss: 7775.43, average training loss: 7867.12, base loss: 15342.28
[INFO 2017-07-01 22:45:13,286 main.py:52] epoch 3481, training loss: 7824.15, average training loss: 7867.14, base loss: 15342.53
[INFO 2017-07-01 22:45:17,448 main.py:52] epoch 3482, training loss: 8172.60, average training loss: 7867.81, base loss: 15342.49
[INFO 2017-07-01 22:45:21,660 main.py:52] epoch 3483, training loss: 7940.96, average training loss: 7868.20, base loss: 15341.65
[INFO 2017-07-01 22:45:25,830 main.py:52] epoch 3484, training loss: 7821.64, average training loss: 7868.46, base loss: 15341.53
[INFO 2017-07-01 22:45:29,978 main.py:52] epoch 3485, training loss: 7437.80, average training loss: 7868.45, base loss: 15340.57
[INFO 2017-07-01 22:45:34,121 main.py:52] epoch 3486, training loss: 8454.47, average training loss: 7868.90, base loss: 15341.52
[INFO 2017-07-01 22:45:38,300 main.py:52] epoch 3487, training loss: 7866.79, average training loss: 7868.29, base loss: 15341.20
[INFO 2017-07-01 22:45:42,430 main.py:52] epoch 3488, training loss: 8108.22, average training loss: 7868.84, base loss: 15342.06
[INFO 2017-07-01 22:45:46,536 main.py:52] epoch 3489, training loss: 7968.17, average training loss: 7869.50, base loss: 15342.39
[INFO 2017-07-01 22:45:50,763 main.py:52] epoch 3490, training loss: 8315.09, average training loss: 7870.43, base loss: 15343.05
[INFO 2017-07-01 22:45:54,962 main.py:52] epoch 3491, training loss: 7917.84, average training loss: 7870.27, base loss: 15342.67
[INFO 2017-07-01 22:45:59,119 main.py:52] epoch 3492, training loss: 8491.85, average training loss: 7871.71, base loss: 15343.46
[INFO 2017-07-01 22:46:03,336 main.py:52] epoch 3493, training loss: 7951.85, average training loss: 7871.40, base loss: 15343.52
[INFO 2017-07-01 22:46:07,549 main.py:52] epoch 3494, training loss: 8127.13, average training loss: 7871.11, base loss: 15343.54
[INFO 2017-07-01 22:46:11,717 main.py:52] epoch 3495, training loss: 7687.10, average training loss: 7870.60, base loss: 15343.13
[INFO 2017-07-01 22:46:15,916 main.py:52] epoch 3496, training loss: 7722.26, average training loss: 7870.91, base loss: 15344.02
[INFO 2017-07-01 22:46:20,084 main.py:52] epoch 3497, training loss: 7906.56, average training loss: 7871.83, base loss: 15344.69
[INFO 2017-07-01 22:46:24,228 main.py:52] epoch 3498, training loss: 7812.53, average training loss: 7871.17, base loss: 15344.50
[INFO 2017-07-01 22:46:28,427 main.py:52] epoch 3499, training loss: 8476.29, average training loss: 7872.09, base loss: 15344.67
[INFO 2017-07-01 22:46:28,427 main.py:54] epoch 3499, testing
[INFO 2017-07-01 22:46:28,427 main.py:61] model save to ./model/final.pth
[INFO 2017-07-01 22:46:32,707 main.py:52] epoch 3500, training loss: 7920.41, average training loss: 7871.78, base loss: 15344.67
[INFO 2017-07-01 22:46:36,887 main.py:52] epoch 3501, training loss: 7910.53, average training loss: 7871.78, base loss: 15345.06
[INFO 2017-07-01 22:46:41,032 main.py:52] epoch 3502, training loss: 8597.98, average training loss: 7873.10, base loss: 15345.10
[INFO 2017-07-01 22:46:45,234 main.py:52] epoch 3503, training loss: 7760.23, average training loss: 7872.82, base loss: 15344.82
[INFO 2017-07-01 22:46:49,348 main.py:52] epoch 3504, training loss: 6919.58, average training loss: 7871.09, base loss: 15343.81
[INFO 2017-07-01 22:46:53,489 main.py:52] epoch 3505, training loss: 8195.07, average training loss: 7871.19, base loss: 15344.07
[INFO 2017-07-01 22:46:57,687 main.py:52] epoch 3506, training loss: 7428.04, average training loss: 7870.34, base loss: 15343.24
[INFO 2017-07-01 22:47:01,889 main.py:52] epoch 3507, training loss: 8118.46, average training loss: 7870.34, base loss: 15342.73
[INFO 2017-07-01 22:47:06,052 main.py:52] epoch 3508, training loss: 8048.87, average training loss: 7870.29, base loss: 15342.52
[INFO 2017-07-01 22:47:10,204 main.py:52] epoch 3509, training loss: 7886.25, average training loss: 7869.68, base loss: 15342.54
[INFO 2017-07-01 22:47:14,431 main.py:52] epoch 3510, training loss: 6993.93, average training loss: 7868.87, base loss: 15341.75
[INFO 2017-07-01 22:47:18,615 main.py:52] epoch 3511, training loss: 8016.75, average training loss: 7869.15, base loss: 15341.11
[INFO 2017-07-01 22:47:22,751 main.py:52] epoch 3512, training loss: 8424.40, average training loss: 7870.41, base loss: 15341.38
[INFO 2017-07-01 22:47:26,922 main.py:52] epoch 3513, training loss: 7232.44, average training loss: 7870.46, base loss: 15341.10
[INFO 2017-07-01 22:47:31,073 main.py:52] epoch 3514, training loss: 7492.08, average training loss: 7869.78, base loss: 15340.86
[INFO 2017-07-01 22:47:35,232 main.py:52] epoch 3515, training loss: 7707.78, average training loss: 7870.01, base loss: 15341.19
[INFO 2017-07-01 22:47:39,383 main.py:52] epoch 3516, training loss: 7898.37, average training loss: 7869.48, base loss: 15341.47
[INFO 2017-07-01 22:47:43,525 main.py:52] epoch 3517, training loss: 8013.58, average training loss: 7870.16, base loss: 15341.17
[INFO 2017-07-01 22:47:47,705 main.py:52] epoch 3518, training loss: 8332.71, average training loss: 7869.78, base loss: 15341.16
[INFO 2017-07-01 22:47:51,884 main.py:52] epoch 3519, training loss: 6866.90, average training loss: 7869.61, base loss: 15340.00
[INFO 2017-07-01 22:47:56,054 main.py:52] epoch 3520, training loss: 7514.57, average training loss: 7869.20, base loss: 15340.11
[INFO 2017-07-01 22:48:00,242 main.py:52] epoch 3521, training loss: 8123.03, average training loss: 7869.64, base loss: 15340.80
[INFO 2017-07-01 22:48:04,418 main.py:52] epoch 3522, training loss: 7388.74, average training loss: 7869.64, base loss: 15340.58
[INFO 2017-07-01 22:48:08,573 main.py:52] epoch 3523, training loss: 8289.83, average training loss: 7869.13, base loss: 15341.26
[INFO 2017-07-01 22:48:12,785 main.py:52] epoch 3524, training loss: 7891.45, average training loss: 7868.39, base loss: 15340.93
[INFO 2017-07-01 22:48:16,897 main.py:52] epoch 3525, training loss: 7547.57, average training loss: 7868.74, base loss: 15340.60
[INFO 2017-07-01 22:48:21,080 main.py:52] epoch 3526, training loss: 7682.11, average training loss: 7868.84, base loss: 15340.56
[INFO 2017-07-01 22:48:25,274 main.py:52] epoch 3527, training loss: 7738.43, average training loss: 7869.12, base loss: 15340.59
[INFO 2017-07-01 22:48:29,439 main.py:52] epoch 3528, training loss: 7802.83, average training loss: 7867.03, base loss: 15339.98
[INFO 2017-07-01 22:48:33,644 main.py:52] epoch 3529, training loss: 8797.94, average training loss: 7868.80, base loss: 15340.54
[INFO 2017-07-01 22:48:37,874 main.py:52] epoch 3530, training loss: 7586.34, average training loss: 7869.03, base loss: 15339.61
[INFO 2017-07-01 22:48:42,068 main.py:52] epoch 3531, training loss: 7724.42, average training loss: 7869.04, base loss: 15339.20
[INFO 2017-07-01 22:48:46,209 main.py:52] epoch 3532, training loss: 7791.27, average training loss: 7868.36, base loss: 15338.60
[INFO 2017-07-01 22:48:50,359 main.py:52] epoch 3533, training loss: 8649.87, average training loss: 7868.89, base loss: 15338.34
[INFO 2017-07-01 22:48:54,528 main.py:52] epoch 3534, training loss: 8144.65, average training loss: 7869.31, base loss: 15339.00
[INFO 2017-07-01 22:48:58,761 main.py:52] epoch 3535, training loss: 8201.90, average training loss: 7869.33, base loss: 15339.76
[INFO 2017-07-01 22:49:02,931 main.py:52] epoch 3536, training loss: 8090.33, average training loss: 7869.41, base loss: 15339.39
[INFO 2017-07-01 22:49:07,173 main.py:52] epoch 3537, training loss: 8319.76, average training loss: 7870.05, base loss: 15338.52
[INFO 2017-07-01 22:49:11,345 main.py:52] epoch 3538, training loss: 7688.30, average training loss: 7869.64, base loss: 15338.72
[INFO 2017-07-01 22:49:15,553 main.py:52] epoch 3539, training loss: 8176.98, average training loss: 7870.03, base loss: 15338.59
[INFO 2017-07-01 22:49:19,747 main.py:52] epoch 3540, training loss: 8046.00, average training loss: 7869.59, base loss: 15339.13
[INFO 2017-07-01 22:49:23,925 main.py:52] epoch 3541, training loss: 8048.79, average training loss: 7869.15, base loss: 15339.93
[INFO 2017-07-01 22:49:28,058 main.py:52] epoch 3542, training loss: 7629.54, average training loss: 7868.98, base loss: 15340.13
[INFO 2017-07-01 22:49:32,228 main.py:52] epoch 3543, training loss: 7090.21, average training loss: 7867.56, base loss: 15339.21
[INFO 2017-07-01 22:49:36,327 main.py:52] epoch 3544, training loss: 8211.02, average training loss: 7868.34, base loss: 15339.39
[INFO 2017-07-01 22:49:40,489 main.py:52] epoch 3545, training loss: 7969.84, average training loss: 7868.16, base loss: 15339.71
[INFO 2017-07-01 22:49:44,720 main.py:52] epoch 3546, training loss: 8488.97, average training loss: 7868.56, base loss: 15340.12
[INFO 2017-07-01 22:49:48,881 main.py:52] epoch 3547, training loss: 8718.45, average training loss: 7870.11, base loss: 15340.65
[INFO 2017-07-01 22:49:53,084 main.py:52] epoch 3548, training loss: 7837.04, average training loss: 7869.91, base loss: 15339.79
[INFO 2017-07-01 22:49:57,272 main.py:52] epoch 3549, training loss: 7434.88, average training loss: 7869.97, base loss: 15338.98
[INFO 2017-07-01 22:50:01,466 main.py:52] epoch 3550, training loss: 7224.44, average training loss: 7869.64, base loss: 15338.70
[INFO 2017-07-01 22:50:05,643 main.py:52] epoch 3551, training loss: 8360.50, average training loss: 7870.36, base loss: 15339.39
[INFO 2017-07-01 22:50:09,810 main.py:52] epoch 3552, training loss: 8167.32, average training loss: 7870.44, base loss: 15339.44
[INFO 2017-07-01 22:50:13,970 main.py:52] epoch 3553, training loss: 7388.34, average training loss: 7869.58, base loss: 15339.18
[INFO 2017-07-01 22:50:18,129 main.py:52] epoch 3554, training loss: 7865.76, average training loss: 7869.90, base loss: 15339.10
[INFO 2017-07-01 22:50:22,259 main.py:52] epoch 3555, training loss: 6977.28, average training loss: 7868.30, base loss: 15338.48
[INFO 2017-07-01 22:50:26,362 main.py:52] epoch 3556, training loss: 7904.42, average training loss: 7869.30, base loss: 15338.74
[INFO 2017-07-01 22:50:30,535 main.py:52] epoch 3557, training loss: 7852.59, average training loss: 7869.43, base loss: 15339.25
[INFO 2017-07-01 22:50:34,699 main.py:52] epoch 3558, training loss: 7368.94, average training loss: 7869.32, base loss: 15338.99
[INFO 2017-07-01 22:50:38,838 main.py:52] epoch 3559, training loss: 7955.92, average training loss: 7869.64, base loss: 15338.58
[INFO 2017-07-01 22:50:42,997 main.py:52] epoch 3560, training loss: 7737.67, average training loss: 7869.30, base loss: 15338.38
[INFO 2017-07-01 22:50:47,140 main.py:52] epoch 3561, training loss: 7639.65, average training loss: 7868.42, base loss: 15338.61
[INFO 2017-07-01 22:50:51,310 main.py:52] epoch 3562, training loss: 7621.21, average training loss: 7868.45, base loss: 15339.07
[INFO 2017-07-01 22:50:55,480 main.py:52] epoch 3563, training loss: 7775.31, average training loss: 7868.37, base loss: 15339.46
[INFO 2017-07-01 22:50:59,680 main.py:52] epoch 3564, training loss: 7639.46, average training loss: 7868.16, base loss: 15338.53
[INFO 2017-07-01 22:51:03,849 main.py:52] epoch 3565, training loss: 7132.77, average training loss: 7867.98, base loss: 15336.40
[INFO 2017-07-01 22:51:08,016 main.py:52] epoch 3566, training loss: 7651.35, average training loss: 7867.03, base loss: 15335.74
[INFO 2017-07-01 22:51:12,194 main.py:52] epoch 3567, training loss: 7616.94, average training loss: 7866.28, base loss: 15335.81
[INFO 2017-07-01 22:51:16,455 main.py:52] epoch 3568, training loss: 7182.52, average training loss: 7865.30, base loss: 15335.52
[INFO 2017-07-01 22:51:20,598 main.py:52] epoch 3569, training loss: 8158.62, average training loss: 7864.94, base loss: 15335.88
[INFO 2017-07-01 22:51:24,780 main.py:52] epoch 3570, training loss: 7365.19, average training loss: 7864.72, base loss: 15335.04
[INFO 2017-07-01 22:51:28,937 main.py:52] epoch 3571, training loss: 7417.90, average training loss: 7863.41, base loss: 15334.45
[INFO 2017-07-01 22:51:33,085 main.py:52] epoch 3572, training loss: 7571.93, average training loss: 7863.58, base loss: 15334.23
[INFO 2017-07-01 22:51:37,299 main.py:52] epoch 3573, training loss: 8890.67, average training loss: 7865.12, base loss: 15335.05
[INFO 2017-07-01 22:51:41,525 main.py:52] epoch 3574, training loss: 8466.95, average training loss: 7865.77, base loss: 15335.72
[INFO 2017-07-01 22:51:45,646 main.py:52] epoch 3575, training loss: 8823.60, average training loss: 7866.79, base loss: 15336.26
[INFO 2017-07-01 22:51:49,749 main.py:52] epoch 3576, training loss: 8074.08, average training loss: 7867.25, base loss: 15336.84
[INFO 2017-07-01 22:51:53,910 main.py:52] epoch 3577, training loss: 8247.00, average training loss: 7867.59, base loss: 15336.73
[INFO 2017-07-01 22:51:58,098 main.py:52] epoch 3578, training loss: 7783.22, average training loss: 7867.76, base loss: 15335.42
[INFO 2017-07-01 22:52:02,268 main.py:52] epoch 3579, training loss: 7206.01, average training loss: 7866.38, base loss: 15334.57
[INFO 2017-07-01 22:52:06,463 main.py:52] epoch 3580, training loss: 7986.36, average training loss: 7866.50, base loss: 15334.71
[INFO 2017-07-01 22:52:10,669 main.py:52] epoch 3581, training loss: 7760.14, average training loss: 7866.56, base loss: 15334.73
[INFO 2017-07-01 22:52:14,850 main.py:52] epoch 3582, training loss: 7465.07, average training loss: 7866.39, base loss: 15334.19
[INFO 2017-07-01 22:52:19,051 main.py:52] epoch 3583, training loss: 7719.35, average training loss: 7866.16, base loss: 15333.80
[INFO 2017-07-01 22:52:23,185 main.py:52] epoch 3584, training loss: 8505.92, average training loss: 7866.83, base loss: 15334.90
[INFO 2017-07-01 22:52:27,315 main.py:52] epoch 3585, training loss: 7526.55, average training loss: 7866.34, base loss: 15334.99
[INFO 2017-07-01 22:52:31,561 main.py:52] epoch 3586, training loss: 7818.72, average training loss: 7865.57, base loss: 15335.42
[INFO 2017-07-01 22:52:35,733 main.py:52] epoch 3587, training loss: 8532.21, average training loss: 7866.14, base loss: 15335.31
[INFO 2017-07-01 22:52:39,881 main.py:52] epoch 3588, training loss: 8171.46, average training loss: 7866.70, base loss: 15334.98
[INFO 2017-07-01 22:52:43,983 main.py:52] epoch 3589, training loss: 7551.38, average training loss: 7866.09, base loss: 15334.89
[INFO 2017-07-01 22:52:48,119 main.py:52] epoch 3590, training loss: 7436.55, average training loss: 7865.75, base loss: 15334.29
[INFO 2017-07-01 22:52:52,268 main.py:52] epoch 3591, training loss: 7507.69, average training loss: 7866.23, base loss: 15334.77
[INFO 2017-07-01 22:52:56,393 main.py:52] epoch 3592, training loss: 7939.60, average training loss: 7866.18, base loss: 15334.72
[INFO 2017-07-01 22:53:00,541 main.py:52] epoch 3593, training loss: 6897.49, average training loss: 7864.58, base loss: 15333.73
[INFO 2017-07-01 22:53:04,717 main.py:52] epoch 3594, training loss: 7429.52, average training loss: 7864.06, base loss: 15333.42
[INFO 2017-07-01 22:53:08,851 main.py:52] epoch 3595, training loss: 7507.28, average training loss: 7863.84, base loss: 15333.51
[INFO 2017-07-01 22:53:13,057 main.py:52] epoch 3596, training loss: 7541.85, average training loss: 7863.07, base loss: 15332.45
[INFO 2017-07-01 22:53:17,303 main.py:52] epoch 3597, training loss: 7800.06, average training loss: 7863.39, base loss: 15332.10
[INFO 2017-07-01 22:53:21,516 main.py:52] epoch 3598, training loss: 7670.54, average training loss: 7863.26, base loss: 15331.86
[INFO 2017-07-01 22:53:25,674 main.py:52] epoch 3599, training loss: 7734.39, average training loss: 7864.08, base loss: 15332.27
[INFO 2017-07-01 22:53:25,674 main.py:54] epoch 3599, testing
[INFO 2017-07-01 22:53:25,674 main.py:61] model save to ./model/final.pth
[INFO 2017-07-01 22:53:29,949 main.py:52] epoch 3600, training loss: 6447.43, average training loss: 7862.67, base loss: 15330.73
[INFO 2017-07-01 22:53:34,084 main.py:52] epoch 3601, training loss: 8222.27, average training loss: 7863.20, base loss: 15330.43
[INFO 2017-07-01 22:53:38,219 main.py:52] epoch 3602, training loss: 7967.98, average training loss: 7863.41, base loss: 15330.34
[INFO 2017-07-01 22:53:42,323 main.py:52] epoch 3603, training loss: 6991.82, average training loss: 7861.86, base loss: 15328.88
[INFO 2017-07-01 22:53:46,528 main.py:52] epoch 3604, training loss: 8632.93, average training loss: 7862.85, base loss: 15329.70
[INFO 2017-07-01 22:53:50,708 main.py:52] epoch 3605, training loss: 8539.57, average training loss: 7863.75, base loss: 15330.59
[INFO 2017-07-01 22:53:54,840 main.py:52] epoch 3606, training loss: 8159.47, average training loss: 7863.35, base loss: 15331.06
[INFO 2017-07-01 22:53:59,019 main.py:52] epoch 3607, training loss: 7718.08, average training loss: 7863.34, base loss: 15331.09
[INFO 2017-07-01 22:54:03,190 main.py:52] epoch 3608, training loss: 7478.98, average training loss: 7862.97, base loss: 15330.75
[INFO 2017-07-01 22:54:07,360 main.py:52] epoch 3609, training loss: 7772.51, average training loss: 7862.57, base loss: 15330.32
[INFO 2017-07-01 22:54:11,555 main.py:52] epoch 3610, training loss: 9044.66, average training loss: 7863.91, base loss: 15330.59
[INFO 2017-07-01 22:54:15,680 main.py:52] epoch 3611, training loss: 7352.67, average training loss: 7863.69, base loss: 15330.67
[INFO 2017-07-01 22:54:19,829 main.py:52] epoch 3612, training loss: 7377.50, average training loss: 7863.66, base loss: 15330.82
[INFO 2017-07-01 22:54:24,008 main.py:52] epoch 3613, training loss: 7543.36, average training loss: 7863.73, base loss: 15331.04
[INFO 2017-07-01 22:54:28,182 main.py:52] epoch 3614, training loss: 8205.97, average training loss: 7863.85, base loss: 15331.77
[INFO 2017-07-01 22:54:32,428 main.py:52] epoch 3615, training loss: 7779.40, average training loss: 7862.79, base loss: 15331.36
[INFO 2017-07-01 22:54:36,628 main.py:52] epoch 3616, training loss: 8100.55, average training loss: 7863.33, base loss: 15331.44
[INFO 2017-07-01 22:54:40,934 main.py:52] epoch 3617, training loss: 8767.80, average training loss: 7864.49, base loss: 15332.75
[INFO 2017-07-01 22:54:45,099 main.py:52] epoch 3618, training loss: 7692.66, average training loss: 7864.07, base loss: 15332.49
[INFO 2017-07-01 22:54:49,266 main.py:52] epoch 3619, training loss: 9661.47, average training loss: 7866.23, base loss: 15334.34
[INFO 2017-07-01 22:54:53,355 main.py:52] epoch 3620, training loss: 8015.77, average training loss: 7866.02, base loss: 15333.68
[INFO 2017-07-01 22:54:57,511 main.py:52] epoch 3621, training loss: 8349.74, average training loss: 7866.43, base loss: 15333.19
[INFO 2017-07-01 22:55:01,673 main.py:52] epoch 3622, training loss: 8535.37, average training loss: 7867.04, base loss: 15332.91
[INFO 2017-07-01 22:55:05,828 main.py:52] epoch 3623, training loss: 8020.93, average training loss: 7868.28, base loss: 15333.53
[INFO 2017-07-01 22:55:10,025 main.py:52] epoch 3624, training loss: 7907.87, average training loss: 7867.33, base loss: 15333.01
[INFO 2017-07-01 22:55:14,198 main.py:52] epoch 3625, training loss: 7499.91, average training loss: 7867.27, base loss: 15332.88
[INFO 2017-07-01 22:55:18,404 main.py:52] epoch 3626, training loss: 7231.36, average training loss: 7865.88, base loss: 15333.09
[INFO 2017-07-01 22:55:22,589 main.py:52] epoch 3627, training loss: 6937.84, average training loss: 7864.29, base loss: 15332.47
[INFO 2017-07-01 22:55:26,827 main.py:52] epoch 3628, training loss: 7455.24, average training loss: 7863.14, base loss: 15332.25
[INFO 2017-07-01 22:55:31,078 main.py:52] epoch 3629, training loss: 6823.84, average training loss: 7862.86, base loss: 15331.35
[INFO 2017-07-01 22:55:35,261 main.py:52] epoch 3630, training loss: 6942.61, average training loss: 7862.08, base loss: 15330.86
[INFO 2017-07-01 22:55:39,455 main.py:52] epoch 3631, training loss: 8229.04, average training loss: 7861.77, base loss: 15331.24
[INFO 2017-07-01 22:55:43,671 main.py:52] epoch 3632, training loss: 7389.62, average training loss: 7861.92, base loss: 15330.85
[INFO 2017-07-01 22:55:47,887 main.py:52] epoch 3633, training loss: 8327.49, average training loss: 7862.57, base loss: 15331.39
[INFO 2017-07-01 22:55:52,075 main.py:52] epoch 3634, training loss: 7250.83, average training loss: 7861.13, base loss: 15331.47
[INFO 2017-07-01 22:55:56,205 main.py:52] epoch 3635, training loss: 7323.53, average training loss: 7860.25, base loss: 15331.34
[INFO 2017-07-01 22:56:00,363 main.py:52] epoch 3636, training loss: 7796.35, average training loss: 7859.71, base loss: 15331.16
[INFO 2017-07-01 22:56:04,518 main.py:52] epoch 3637, training loss: 7995.97, average training loss: 7859.52, base loss: 15330.92
[INFO 2017-07-01 22:56:08,710 main.py:52] epoch 3638, training loss: 7895.51, average training loss: 7859.00, base loss: 15330.70
[INFO 2017-07-01 22:56:12,896 main.py:52] epoch 3639, training loss: 8259.35, average training loss: 7858.56, base loss: 15331.57
[INFO 2017-07-01 22:56:17,082 main.py:52] epoch 3640, training loss: 6854.29, average training loss: 7857.17, base loss: 15331.29
[INFO 2017-07-01 22:56:21,246 main.py:52] epoch 3641, training loss: 8417.84, average training loss: 7858.12, base loss: 15332.52
[INFO 2017-07-01 22:56:25,476 main.py:52] epoch 3642, training loss: 8278.57, average training loss: 7858.57, base loss: 15333.02
[INFO 2017-07-01 22:56:29,673 main.py:52] epoch 3643, training loss: 8522.86, average training loss: 7859.75, base loss: 15333.78
[INFO 2017-07-01 22:56:33,858 main.py:52] epoch 3644, training loss: 7894.14, average training loss: 7860.21, base loss: 15333.63
[INFO 2017-07-01 22:56:37,965 main.py:52] epoch 3645, training loss: 8244.68, average training loss: 7860.01, base loss: 15333.95
[INFO 2017-07-01 22:56:42,082 main.py:52] epoch 3646, training loss: 7589.96, average training loss: 7860.15, base loss: 15333.45
[INFO 2017-07-01 22:56:46,264 main.py:52] epoch 3647, training loss: 7382.97, average training loss: 7859.01, base loss: 15333.21
[INFO 2017-07-01 22:56:50,377 main.py:52] epoch 3648, training loss: 6941.65, average training loss: 7857.86, base loss: 15331.52
[INFO 2017-07-01 22:56:54,562 main.py:52] epoch 3649, training loss: 7961.11, average training loss: 7858.49, base loss: 15330.78
[INFO 2017-07-01 22:56:58,740 main.py:52] epoch 3650, training loss: 8089.49, average training loss: 7858.43, base loss: 15330.88
[INFO 2017-07-01 22:57:02,826 main.py:52] epoch 3651, training loss: 7972.89, average training loss: 7858.49, base loss: 15331.34
[INFO 2017-07-01 22:57:06,963 main.py:52] epoch 3652, training loss: 7587.71, average training loss: 7857.49, base loss: 15331.77
[INFO 2017-07-01 22:57:11,229 main.py:52] epoch 3653, training loss: 7740.93, average training loss: 7857.03, base loss: 15331.71
[INFO 2017-07-01 22:57:15,351 main.py:52] epoch 3654, training loss: 7743.22, average training loss: 7857.68, base loss: 15331.76
[INFO 2017-07-01 22:57:19,470 main.py:52] epoch 3655, training loss: 7675.53, average training loss: 7857.33, base loss: 15332.24
[INFO 2017-07-01 22:57:23,593 main.py:52] epoch 3656, training loss: 7955.28, average training loss: 7857.56, base loss: 15332.40
[INFO 2017-07-01 22:57:27,756 main.py:52] epoch 3657, training loss: 8506.75, average training loss: 7857.76, base loss: 15332.91
[INFO 2017-07-01 22:57:31,864 main.py:52] epoch 3658, training loss: 8124.39, average training loss: 7858.03, base loss: 15333.26
[INFO 2017-07-01 22:57:35,986 main.py:52] epoch 3659, training loss: 8609.60, average training loss: 7858.73, base loss: 15334.22
[INFO 2017-07-01 22:57:40,151 main.py:52] epoch 3660, training loss: 7853.27, average training loss: 7859.46, base loss: 15334.05
[INFO 2017-07-01 22:57:44,343 main.py:52] epoch 3661, training loss: 8150.23, average training loss: 7860.20, base loss: 15333.77
[INFO 2017-07-01 22:57:48,458 main.py:52] epoch 3662, training loss: 7620.40, average training loss: 7860.20, base loss: 15333.51
[INFO 2017-07-01 22:57:52,580 main.py:52] epoch 3663, training loss: 7311.08, average training loss: 7859.80, base loss: 15332.89
[INFO 2017-07-01 22:57:56,760 main.py:52] epoch 3664, training loss: 7317.01, average training loss: 7859.32, base loss: 15332.43
[INFO 2017-07-01 22:58:00,959 main.py:52] epoch 3665, training loss: 7787.67, average training loss: 7859.51, base loss: 15332.19
[INFO 2017-07-01 22:58:05,129 main.py:52] epoch 3666, training loss: 8583.04, average training loss: 7860.69, base loss: 15332.83
[INFO 2017-07-01 22:58:09,273 main.py:52] epoch 3667, training loss: 7157.97, average training loss: 7860.29, base loss: 15331.99
[INFO 2017-07-01 22:58:13,495 main.py:52] epoch 3668, training loss: 7794.27, average training loss: 7860.51, base loss: 15332.01
[INFO 2017-07-01 22:58:17,619 main.py:52] epoch 3669, training loss: 8172.47, average training loss: 7860.36, base loss: 15332.61
[INFO 2017-07-01 22:58:21,770 main.py:52] epoch 3670, training loss: 7792.41, average training loss: 7859.94, base loss: 15332.53
[INFO 2017-07-01 22:58:25,980 main.py:52] epoch 3671, training loss: 7366.12, average training loss: 7860.07, base loss: 15332.24
[INFO 2017-07-01 22:58:30,135 main.py:52] epoch 3672, training loss: 8651.35, average training loss: 7860.51, base loss: 15333.27
[INFO 2017-07-01 22:58:34,386 main.py:52] epoch 3673, training loss: 7464.87, average training loss: 7860.12, base loss: 15332.88
[INFO 2017-07-01 22:58:38,572 main.py:52] epoch 3674, training loss: 8272.88, average training loss: 7860.24, base loss: 15333.03
[INFO 2017-07-01 22:58:42,731 main.py:52] epoch 3675, training loss: 7836.68, average training loss: 7860.52, base loss: 15332.31
[INFO 2017-07-01 22:58:46,970 main.py:52] epoch 3676, training loss: 7704.97, average training loss: 7859.61, base loss: 15331.57
[INFO 2017-07-01 22:58:51,088 main.py:52] epoch 3677, training loss: 7222.79, average training loss: 7859.58, base loss: 15330.71
[INFO 2017-07-01 22:58:55,263 main.py:52] epoch 3678, training loss: 7484.69, average training loss: 7860.04, base loss: 15331.19
[INFO 2017-07-01 22:58:59,436 main.py:52] epoch 3679, training loss: 7233.70, average training loss: 7859.24, base loss: 15330.77
[INFO 2017-07-01 22:59:03,595 main.py:52] epoch 3680, training loss: 7729.83, average training loss: 7858.69, base loss: 15330.37
[INFO 2017-07-01 22:59:07,727 main.py:52] epoch 3681, training loss: 7209.33, average training loss: 7858.97, base loss: 15329.27
[INFO 2017-07-01 22:59:11,805 main.py:52] epoch 3682, training loss: 7627.26, average training loss: 7858.83, base loss: 15330.10
[INFO 2017-07-01 22:59:15,914 main.py:52] epoch 3683, training loss: 8398.96, average training loss: 7859.49, base loss: 15330.47
[INFO 2017-07-01 22:59:20,160 main.py:52] epoch 3684, training loss: 8994.42, average training loss: 7861.36, base loss: 15330.88
[INFO 2017-07-01 22:59:24,272 main.py:52] epoch 3685, training loss: 8793.56, average training loss: 7862.70, base loss: 15331.49
[INFO 2017-07-01 22:59:28,431 main.py:52] epoch 3686, training loss: 8420.47, average training loss: 7862.25, base loss: 15331.45
[INFO 2017-07-01 22:59:32,618 main.py:52] epoch 3687, training loss: 7413.61, average training loss: 7861.90, base loss: 15330.80
[INFO 2017-07-01 22:59:36,823 main.py:52] epoch 3688, training loss: 7471.27, average training loss: 7861.01, base loss: 15331.05
[INFO 2017-07-01 22:59:40,971 main.py:52] epoch 3689, training loss: 7874.88, average training loss: 7860.48, base loss: 15330.81
[INFO 2017-07-01 22:59:45,153 main.py:52] epoch 3690, training loss: 7682.20, average training loss: 7860.33, base loss: 15330.79
[INFO 2017-07-01 22:59:49,302 main.py:52] epoch 3691, training loss: 8433.78, average training loss: 7860.13, base loss: 15331.03
[INFO 2017-07-01 22:59:53,474 main.py:52] epoch 3692, training loss: 8992.19, average training loss: 7861.50, base loss: 15332.00
[INFO 2017-07-01 22:59:57,613 main.py:52] epoch 3693, training loss: 6930.49, average training loss: 7861.07, base loss: 15331.95
[INFO 2017-07-01 23:00:01,762 main.py:52] epoch 3694, training loss: 7508.50, average training loss: 7861.30, base loss: 15331.05
[INFO 2017-07-01 23:00:05,934 main.py:52] epoch 3695, training loss: 8388.51, average training loss: 7861.60, base loss: 15330.83
[INFO 2017-07-01 23:00:10,050 main.py:52] epoch 3696, training loss: 7746.01, average training loss: 7861.52, base loss: 15331.29
[INFO 2017-07-01 23:00:14,250 main.py:52] epoch 3697, training loss: 7310.82, average training loss: 7861.99, base loss: 15330.91
[INFO 2017-07-01 23:00:18,420 main.py:52] epoch 3698, training loss: 7928.05, average training loss: 7862.31, base loss: 15330.60
[INFO 2017-07-01 23:00:22,500 main.py:52] epoch 3699, training loss: 7476.08, average training loss: 7862.65, base loss: 15330.27
[INFO 2017-07-01 23:00:22,501 main.py:54] epoch 3699, testing
[INFO 2017-07-01 23:00:22,501 main.py:61] model save to ./model/final.pth
[INFO 2017-07-01 23:00:26,786 main.py:52] epoch 3700, training loss: 7978.83, average training loss: 7862.08, base loss: 15330.36
[INFO 2017-07-01 23:00:30,903 main.py:52] epoch 3701, training loss: 8285.84, average training loss: 7862.96, base loss: 15330.48
[INFO 2017-07-01 23:00:35,066 main.py:52] epoch 3702, training loss: 7604.03, average training loss: 7861.44, base loss: 15329.77
[INFO 2017-07-01 23:00:39,297 main.py:52] epoch 3703, training loss: 7847.60, average training loss: 7861.77, base loss: 15329.25
[INFO 2017-07-01 23:00:43,493 main.py:52] epoch 3704, training loss: 7510.36, average training loss: 7861.19, base loss: 15329.42
[INFO 2017-07-01 23:00:47,623 main.py:52] epoch 3705, training loss: 7779.54, average training loss: 7860.89, base loss: 15330.39
[INFO 2017-07-01 23:00:51,782 main.py:52] epoch 3706, training loss: 8256.63, average training loss: 7862.32, base loss: 15331.30
[INFO 2017-07-01 23:00:55,934 main.py:52] epoch 3707, training loss: 7547.01, average training loss: 7862.08, base loss: 15331.94
[INFO 2017-07-01 23:01:00,100 main.py:52] epoch 3708, training loss: 7483.73, average training loss: 7862.00, base loss: 15331.56
[INFO 2017-07-01 23:01:04,297 main.py:52] epoch 3709, training loss: 8002.71, average training loss: 7862.38, base loss: 15331.65
[INFO 2017-07-01 23:01:08,495 main.py:52] epoch 3710, training loss: 8125.84, average training loss: 7862.93, base loss: 15332.44
[INFO 2017-07-01 23:01:12,726 main.py:52] epoch 3711, training loss: 7272.22, average training loss: 7862.36, base loss: 15332.38
[INFO 2017-07-01 23:01:16,901 main.py:52] epoch 3712, training loss: 7476.43, average training loss: 7861.16, base loss: 15332.57
[INFO 2017-07-01 23:01:21,127 main.py:52] epoch 3713, training loss: 8207.82, average training loss: 7861.23, base loss: 15333.41
[INFO 2017-07-01 23:01:25,250 main.py:52] epoch 3714, training loss: 7542.96, average training loss: 7861.58, base loss: 15332.82
[INFO 2017-07-01 23:01:29,408 main.py:52] epoch 3715, training loss: 7869.92, average training loss: 7861.62, base loss: 15333.18
[INFO 2017-07-01 23:01:33,564 main.py:52] epoch 3716, training loss: 8035.13, average training loss: 7860.05, base loss: 15333.94
[INFO 2017-07-01 23:01:37,709 main.py:52] epoch 3717, training loss: 8095.50, average training loss: 7860.97, base loss: 15334.32
[INFO 2017-07-01 23:01:41,876 main.py:52] epoch 3718, training loss: 8527.59, average training loss: 7861.05, base loss: 15335.22
[INFO 2017-07-01 23:01:46,051 main.py:52] epoch 3719, training loss: 7337.22, average training loss: 7860.75, base loss: 15334.85
[INFO 2017-07-01 23:01:50,196 main.py:52] epoch 3720, training loss: 7597.50, average training loss: 7860.97, base loss: 15334.64
[INFO 2017-07-01 23:01:54,333 main.py:52] epoch 3721, training loss: 7231.70, average training loss: 7860.96, base loss: 15333.54
[INFO 2017-07-01 23:01:58,563 main.py:52] epoch 3722, training loss: 8906.84, average training loss: 7862.46, base loss: 15334.06
[INFO 2017-07-01 23:02:02,906 main.py:52] epoch 3723, training loss: 8283.61, average training loss: 7862.36, base loss: 15334.38
[INFO 2017-07-01 23:02:07,116 main.py:52] epoch 3724, training loss: 7282.63, average training loss: 7862.69, base loss: 15333.56
[INFO 2017-07-01 23:02:11,276 main.py:52] epoch 3725, training loss: 7474.81, average training loss: 7862.93, base loss: 15333.01
[INFO 2017-07-01 23:02:15,462 main.py:52] epoch 3726, training loss: 8073.87, average training loss: 7863.45, base loss: 15333.30
[INFO 2017-07-01 23:02:19,618 main.py:52] epoch 3727, training loss: 7807.16, average training loss: 7863.63, base loss: 15333.15
[INFO 2017-07-01 23:02:23,770 main.py:52] epoch 3728, training loss: 8094.39, average training loss: 7864.50, base loss: 15333.38
[INFO 2017-07-01 23:02:27,989 main.py:52] epoch 3729, training loss: 7739.91, average training loss: 7863.36, base loss: 15334.18
[INFO 2017-07-01 23:02:32,191 main.py:52] epoch 3730, training loss: 8295.27, average training loss: 7864.65, base loss: 15333.93
[INFO 2017-07-01 23:02:36,376 main.py:52] epoch 3731, training loss: 8886.10, average training loss: 7864.94, base loss: 15334.02
[INFO 2017-07-01 23:02:40,488 main.py:52] epoch 3732, training loss: 7414.84, average training loss: 7864.63, base loss: 15333.43
[INFO 2017-07-01 23:02:44,695 main.py:52] epoch 3733, training loss: 7558.67, average training loss: 7862.89, base loss: 15333.58
[INFO 2017-07-01 23:02:48,850 main.py:52] epoch 3734, training loss: 7397.92, average training loss: 7862.96, base loss: 15332.92
[INFO 2017-07-01 23:02:52,964 main.py:52] epoch 3735, training loss: 7626.40, average training loss: 7862.04, base loss: 15332.41
[INFO 2017-07-01 23:02:57,212 main.py:52] epoch 3736, training loss: 7460.69, average training loss: 7861.89, base loss: 15332.71
[INFO 2017-07-01 23:03:01,362 main.py:52] epoch 3737, training loss: 7172.88, average training loss: 7861.41, base loss: 15332.57
[INFO 2017-07-01 23:03:05,588 main.py:52] epoch 3738, training loss: 7303.50, average training loss: 7861.60, base loss: 15332.38
[INFO 2017-07-01 23:03:09,748 main.py:52] epoch 3739, training loss: 7710.54, average training loss: 7861.51, base loss: 15332.21
[INFO 2017-07-01 23:03:13,951 main.py:52] epoch 3740, training loss: 7552.98, average training loss: 7860.85, base loss: 15332.33
[INFO 2017-07-01 23:03:18,162 main.py:52] epoch 3741, training loss: 8229.49, average training loss: 7862.36, base loss: 15333.25
[INFO 2017-07-01 23:03:22,349 main.py:52] epoch 3742, training loss: 8128.55, average training loss: 7861.38, base loss: 15333.81
[INFO 2017-07-01 23:03:26,492 main.py:52] epoch 3743, training loss: 8284.54, average training loss: 7862.63, base loss: 15334.49
[INFO 2017-07-01 23:03:30,689 main.py:52] epoch 3744, training loss: 7617.28, average training loss: 7861.72, base loss: 15334.36
[INFO 2017-07-01 23:03:34,838 main.py:52] epoch 3745, training loss: 7223.50, average training loss: 7861.40, base loss: 15334.01
[INFO 2017-07-01 23:03:39,046 main.py:52] epoch 3746, training loss: 7970.26, average training loss: 7861.51, base loss: 15334.88
[INFO 2017-07-01 23:03:43,203 main.py:52] epoch 3747, training loss: 7717.02, average training loss: 7860.83, base loss: 15335.40
[INFO 2017-07-01 23:03:47,323 main.py:52] epoch 3748, training loss: 8249.57, average training loss: 7861.24, base loss: 15334.53
[INFO 2017-07-01 23:03:51,464 main.py:52] epoch 3749, training loss: 7371.30, average training loss: 7860.84, base loss: 15333.77
[INFO 2017-07-01 23:03:55,571 main.py:52] epoch 3750, training loss: 8333.50, average training loss: 7861.91, base loss: 15334.80
[INFO 2017-07-01 23:03:59,734 main.py:52] epoch 3751, training loss: 7650.34, average training loss: 7861.73, base loss: 15335.31
[INFO 2017-07-01 23:04:03,906 main.py:52] epoch 3752, training loss: 7667.74, average training loss: 7861.78, base loss: 15335.23
[INFO 2017-07-01 23:04:08,159 main.py:52] epoch 3753, training loss: 7534.88, average training loss: 7861.75, base loss: 15334.99
[INFO 2017-07-01 23:04:12,388 main.py:52] epoch 3754, training loss: 8290.96, average training loss: 7862.40, base loss: 15335.59
[INFO 2017-07-01 23:04:16,543 main.py:52] epoch 3755, training loss: 8331.53, average training loss: 7862.66, base loss: 15336.28
[INFO 2017-07-01 23:04:20,729 main.py:52] epoch 3756, training loss: 7277.81, average training loss: 7861.52, base loss: 15335.96
[INFO 2017-07-01 23:04:24,925 main.py:52] epoch 3757, training loss: 7632.98, average training loss: 7862.17, base loss: 15335.68
[INFO 2017-07-01 23:04:29,093 main.py:52] epoch 3758, training loss: 7800.16, average training loss: 7862.70, base loss: 15335.61
[INFO 2017-07-01 23:04:33,240 main.py:52] epoch 3759, training loss: 6936.91, average training loss: 7860.72, base loss: 15335.37
[INFO 2017-07-01 23:04:37,367 main.py:52] epoch 3760, training loss: 7176.86, average training loss: 7859.75, base loss: 15335.03
[INFO 2017-07-01 23:04:41,571 main.py:52] epoch 3761, training loss: 7667.95, average training loss: 7858.38, base loss: 15334.51
[INFO 2017-07-01 23:04:45,690 main.py:52] epoch 3762, training loss: 7766.85, average training loss: 7857.44, base loss: 15334.73
[INFO 2017-07-01 23:04:49,821 main.py:52] epoch 3763, training loss: 7297.08, average training loss: 7857.62, base loss: 15334.92
[INFO 2017-07-01 23:04:53,964 main.py:52] epoch 3764, training loss: 7728.41, average training loss: 7857.96, base loss: 15334.37
[INFO 2017-07-01 23:04:58,096 main.py:52] epoch 3765, training loss: 7966.21, average training loss: 7858.57, base loss: 15334.18
[INFO 2017-07-01 23:05:02,255 main.py:52] epoch 3766, training loss: 8553.69, average training loss: 7859.75, base loss: 15334.77
[INFO 2017-07-01 23:05:06,434 main.py:52] epoch 3767, training loss: 6214.02, average training loss: 7857.49, base loss: 15333.44
[INFO 2017-07-01 23:05:10,566 main.py:52] epoch 3768, training loss: 7089.48, average training loss: 7856.43, base loss: 15332.59
[INFO 2017-07-01 23:05:14,755 main.py:52] epoch 3769, training loss: 7839.48, average training loss: 7856.83, base loss: 15332.16
[INFO 2017-07-01 23:05:18,918 main.py:52] epoch 3770, training loss: 7484.72, average training loss: 7856.44, base loss: 15332.41
[INFO 2017-07-01 23:05:23,067 main.py:52] epoch 3771, training loss: 7785.34, average training loss: 7855.59, base loss: 15332.88
[INFO 2017-07-01 23:05:27,252 main.py:52] epoch 3772, training loss: 7925.41, average training loss: 7855.38, base loss: 15333.25
[INFO 2017-07-01 23:05:31,474 main.py:52] epoch 3773, training loss: 7991.87, average training loss: 7855.49, base loss: 15333.15
[INFO 2017-07-01 23:05:35,616 main.py:52] epoch 3774, training loss: 9136.84, average training loss: 7857.10, base loss: 15333.72
[INFO 2017-07-01 23:05:39,795 main.py:52] epoch 3775, training loss: 7796.05, average training loss: 7856.32, base loss: 15333.73
[INFO 2017-07-01 23:05:43,988 main.py:52] epoch 3776, training loss: 7885.85, average training loss: 7856.16, base loss: 15333.78
[INFO 2017-07-01 23:05:48,178 main.py:52] epoch 3777, training loss: 7563.96, average training loss: 7855.98, base loss: 15333.42
[INFO 2017-07-01 23:05:52,292 main.py:52] epoch 3778, training loss: 7974.70, average training loss: 7856.20, base loss: 15333.40
[INFO 2017-07-01 23:05:56,436 main.py:52] epoch 3779, training loss: 8142.54, average training loss: 7856.70, base loss: 15333.37
[INFO 2017-07-01 23:06:00,559 main.py:52] epoch 3780, training loss: 7643.11, average training loss: 7855.66, base loss: 15332.76
[INFO 2017-07-01 23:06:04,669 main.py:52] epoch 3781, training loss: 8055.75, average training loss: 7856.67, base loss: 15332.81
[INFO 2017-07-01 23:06:08,850 main.py:52] epoch 3782, training loss: 7294.80, average training loss: 7856.77, base loss: 15332.45
[INFO 2017-07-01 23:06:12,991 main.py:52] epoch 3783, training loss: 8853.20, average training loss: 7858.05, base loss: 15333.68
[INFO 2017-07-01 23:06:17,196 main.py:52] epoch 3784, training loss: 7723.38, average training loss: 7857.88, base loss: 15333.27
[INFO 2017-07-01 23:06:21,339 main.py:52] epoch 3785, training loss: 7314.96, average training loss: 7856.48, base loss: 15333.15
[INFO 2017-07-01 23:06:25,526 main.py:52] epoch 3786, training loss: 7659.38, average training loss: 7856.75, base loss: 15332.87
[INFO 2017-07-01 23:06:29,692 main.py:52] epoch 3787, training loss: 8093.40, average training loss: 7857.27, base loss: 15333.50
[INFO 2017-07-01 23:06:33,860 main.py:52] epoch 3788, training loss: 7637.94, average training loss: 7856.02, base loss: 15333.01
[INFO 2017-07-01 23:06:38,055 main.py:52] epoch 3789, training loss: 7527.79, average training loss: 7855.21, base loss: 15332.78
[INFO 2017-07-01 23:06:42,215 main.py:52] epoch 3790, training loss: 7400.31, average training loss: 7854.82, base loss: 15331.59
[INFO 2017-07-01 23:06:46,341 main.py:52] epoch 3791, training loss: 7658.25, average training loss: 7854.96, base loss: 15331.18
[INFO 2017-07-01 23:06:50,583 main.py:52] epoch 3792, training loss: 7736.02, average training loss: 7854.02, base loss: 15330.71
[INFO 2017-07-01 23:06:54,731 main.py:52] epoch 3793, training loss: 7936.14, average training loss: 7854.15, base loss: 15330.37
[INFO 2017-07-01 23:06:58,980 main.py:52] epoch 3794, training loss: 9463.34, average training loss: 7856.13, base loss: 15331.47
[INFO 2017-07-01 23:07:03,165 main.py:52] epoch 3795, training loss: 7533.99, average training loss: 7856.39, base loss: 15331.08
[INFO 2017-07-01 23:07:07,375 main.py:52] epoch 3796, training loss: 8623.75, average training loss: 7856.25, base loss: 15331.50
[INFO 2017-07-01 23:07:11,547 main.py:52] epoch 3797, training loss: 7447.68, average training loss: 7856.15, base loss: 15331.21
[INFO 2017-07-01 23:07:15,789 main.py:52] epoch 3798, training loss: 6926.31, average training loss: 7855.16, base loss: 15330.53
[INFO 2017-07-01 23:07:19,950 main.py:52] epoch 3799, training loss: 8018.34, average training loss: 7855.77, base loss: 15330.67
[INFO 2017-07-01 23:07:19,950 main.py:54] epoch 3799, testing
[INFO 2017-07-01 23:07:19,950 main.py:61] model save to ./model/final.pth
[INFO 2017-07-01 23:07:24,142 main.py:52] epoch 3800, training loss: 7222.16, average training loss: 7854.27, base loss: 15330.13
[INFO 2017-07-01 23:07:28,320 main.py:52] epoch 3801, training loss: 7121.61, average training loss: 7853.60, base loss: 15329.31
[INFO 2017-07-01 23:07:32,501 main.py:52] epoch 3802, training loss: 8169.22, average training loss: 7854.14, base loss: 15329.17
[INFO 2017-07-01 23:07:36,726 main.py:52] epoch 3803, training loss: 7978.78, average training loss: 7852.79, base loss: 15328.95
[INFO 2017-07-01 23:07:40,873 main.py:52] epoch 3804, training loss: 7425.08, average training loss: 7852.23, base loss: 15328.65
[INFO 2017-07-01 23:07:45,018 main.py:52] epoch 3805, training loss: 8465.69, average training loss: 7853.13, base loss: 15328.79
[INFO 2017-07-01 23:07:49,211 main.py:52] epoch 3806, training loss: 8997.84, average training loss: 7855.21, base loss: 15329.12
[INFO 2017-07-01 23:07:53,421 main.py:52] epoch 3807, training loss: 7364.81, average training loss: 7854.29, base loss: 15328.49
[INFO 2017-07-01 23:07:57,582 main.py:52] epoch 3808, training loss: 7613.28, average training loss: 7854.22, base loss: 15328.61
[INFO 2017-07-01 23:08:01,730 main.py:52] epoch 3809, training loss: 7206.46, average training loss: 7853.88, base loss: 15328.27
[INFO 2017-07-01 23:08:05,862 main.py:52] epoch 3810, training loss: 7731.32, average training loss: 7854.50, base loss: 15328.26
[INFO 2017-07-01 23:08:10,060 main.py:52] epoch 3811, training loss: 6562.04, average training loss: 7853.44, base loss: 15327.66
[INFO 2017-07-01 23:08:14,312 main.py:52] epoch 3812, training loss: 8077.02, average training loss: 7854.48, base loss: 15327.55
[INFO 2017-07-01 23:08:18,567 main.py:52] epoch 3813, training loss: 7728.51, average training loss: 7855.43, base loss: 15326.88
[INFO 2017-07-01 23:08:22,751 main.py:52] epoch 3814, training loss: 8955.13, average training loss: 7856.02, base loss: 15328.33
[INFO 2017-07-01 23:08:26,896 main.py:52] epoch 3815, training loss: 8283.37, average training loss: 7855.26, base loss: 15329.41
[INFO 2017-07-01 23:08:31,097 main.py:52] epoch 3816, training loss: 8517.04, average training loss: 7855.82, base loss: 15330.84
[INFO 2017-07-01 23:08:35,311 main.py:52] epoch 3817, training loss: 7727.93, average training loss: 7856.25, base loss: 15330.60
[INFO 2017-07-01 23:08:39,464 main.py:52] epoch 3818, training loss: 7764.29, average training loss: 7856.68, base loss: 15331.01
[INFO 2017-07-01 23:08:43,600 main.py:52] epoch 3819, training loss: 7945.56, average training loss: 7857.58, base loss: 15331.50
[INFO 2017-07-01 23:08:47,780 main.py:52] epoch 3820, training loss: 6595.40, average training loss: 7856.91, base loss: 15330.91
[INFO 2017-07-01 23:08:51,957 main.py:52] epoch 3821, training loss: 7319.61, average training loss: 7856.33, base loss: 15330.99
[INFO 2017-07-01 23:08:56,130 main.py:52] epoch 3822, training loss: 7509.85, average training loss: 7856.84, base loss: 15331.03
[INFO 2017-07-01 23:09:00,337 main.py:52] epoch 3823, training loss: 6869.55, average training loss: 7857.02, base loss: 15330.17
[INFO 2017-07-01 23:09:04,489 main.py:52] epoch 3824, training loss: 8483.37, average training loss: 7857.87, base loss: 15330.78
[INFO 2017-07-01 23:09:08,642 main.py:52] epoch 3825, training loss: 8087.53, average training loss: 7858.43, base loss: 15330.30
[INFO 2017-07-01 23:09:12,838 main.py:52] epoch 3826, training loss: 7567.92, average training loss: 7858.10, base loss: 15330.18
[INFO 2017-07-01 23:09:16,984 main.py:52] epoch 3827, training loss: 7368.38, average training loss: 7857.49, base loss: 15329.81
[INFO 2017-07-01 23:09:21,220 main.py:52] epoch 3828, training loss: 8549.23, average training loss: 7859.12, base loss: 15331.76
[INFO 2017-07-01 23:09:25,418 main.py:52] epoch 3829, training loss: 7886.90, average training loss: 7859.71, base loss: 15332.02
[INFO 2017-07-01 23:09:29,533 main.py:52] epoch 3830, training loss: 8234.71, average training loss: 7860.26, base loss: 15332.17
[INFO 2017-07-01 23:09:33,793 main.py:52] epoch 3831, training loss: 8300.05, average training loss: 7860.85, base loss: 15332.66
[INFO 2017-07-01 23:09:37,960 main.py:52] epoch 3832, training loss: 8829.55, average training loss: 7861.69, base loss: 15334.04
[INFO 2017-07-01 23:09:42,212 main.py:52] epoch 3833, training loss: 7313.83, average training loss: 7861.31, base loss: 15333.78
[INFO 2017-07-01 23:09:46,346 main.py:52] epoch 3834, training loss: 8593.59, average training loss: 7861.79, base loss: 15334.51
[INFO 2017-07-01 23:09:50,516 main.py:52] epoch 3835, training loss: 8041.90, average training loss: 7863.17, base loss: 15334.55
[INFO 2017-07-01 23:09:54,725 main.py:52] epoch 3836, training loss: 7854.39, average training loss: 7862.39, base loss: 15334.53
[INFO 2017-07-01 23:09:58,924 main.py:52] epoch 3837, training loss: 6709.97, average training loss: 7861.53, base loss: 15334.28
[INFO 2017-07-01 23:10:03,088 main.py:52] epoch 3838, training loss: 7873.99, average training loss: 7861.74, base loss: 15334.42
[INFO 2017-07-01 23:10:07,196 main.py:52] epoch 3839, training loss: 6606.27, average training loss: 7859.05, base loss: 15333.35
[INFO 2017-07-01 23:10:11,362 main.py:52] epoch 3840, training loss: 8173.40, average training loss: 7859.26, base loss: 15333.97
[INFO 2017-07-01 23:10:15,589 main.py:52] epoch 3841, training loss: 7876.59, average training loss: 7859.55, base loss: 15334.68
[INFO 2017-07-01 23:10:19,763 main.py:52] epoch 3842, training loss: 7863.81, average training loss: 7859.68, base loss: 15334.49
[INFO 2017-07-01 23:10:23,986 main.py:52] epoch 3843, training loss: 7362.09, average training loss: 7859.46, base loss: 15334.40
[INFO 2017-07-01 23:10:28,126 main.py:52] epoch 3844, training loss: 8179.46, average training loss: 7858.20, base loss: 15334.06
[INFO 2017-07-01 23:10:32,314 main.py:52] epoch 3845, training loss: 7494.04, average training loss: 7858.49, base loss: 15333.22
[INFO 2017-07-01 23:10:36,483 main.py:52] epoch 3846, training loss: 7726.38, average training loss: 7858.37, base loss: 15333.71
[INFO 2017-07-01 23:10:40,722 main.py:52] epoch 3847, training loss: 8242.05, average training loss: 7858.79, base loss: 15334.02
[INFO 2017-07-01 23:10:44,825 main.py:52] epoch 3848, training loss: 7701.76, average training loss: 7857.41, base loss: 15334.28
[INFO 2017-07-01 23:10:48,979 main.py:52] epoch 3849, training loss: 8543.47, average training loss: 7858.45, base loss: 15334.94
[INFO 2017-07-01 23:10:53,219 main.py:52] epoch 3850, training loss: 7079.69, average training loss: 7858.25, base loss: 15333.79
[INFO 2017-07-01 23:10:57,446 main.py:52] epoch 3851, training loss: 8108.54, average training loss: 7858.67, base loss: 15333.83
[INFO 2017-07-01 23:11:01,550 main.py:52] epoch 3852, training loss: 7308.20, average training loss: 7858.47, base loss: 15333.73
[INFO 2017-07-01 23:11:05,772 main.py:52] epoch 3853, training loss: 9128.90, average training loss: 7859.06, base loss: 15334.39
[INFO 2017-07-01 23:11:09,968 main.py:52] epoch 3854, training loss: 7839.02, average training loss: 7859.49, base loss: 15334.60
[INFO 2017-07-01 23:11:14,171 main.py:52] epoch 3855, training loss: 7733.83, average training loss: 7858.71, base loss: 15334.22
[INFO 2017-07-01 23:11:18,263 main.py:52] epoch 3856, training loss: 7704.07, average training loss: 7858.68, base loss: 15333.65
[INFO 2017-07-01 23:11:22,386 main.py:52] epoch 3857, training loss: 7708.76, average training loss: 7858.04, base loss: 15334.18
[INFO 2017-07-01 23:11:26,523 main.py:52] epoch 3858, training loss: 8093.67, average training loss: 7858.00, base loss: 15334.81
[INFO 2017-07-01 23:11:30,725 main.py:52] epoch 3859, training loss: 8202.29, average training loss: 7858.53, base loss: 15335.48
[INFO 2017-07-01 23:11:34,924 main.py:52] epoch 3860, training loss: 7285.91, average training loss: 7857.41, base loss: 15335.10
[INFO 2017-07-01 23:11:39,132 main.py:52] epoch 3861, training loss: 7184.73, average training loss: 7856.81, base loss: 15334.67
[INFO 2017-07-01 23:11:43,267 main.py:52] epoch 3862, training loss: 7863.47, average training loss: 7856.71, base loss: 15335.28
[INFO 2017-07-01 23:11:47,427 main.py:52] epoch 3863, training loss: 7511.53, average training loss: 7856.53, base loss: 15335.59
[INFO 2017-07-01 23:11:51,598 main.py:52] epoch 3864, training loss: 7687.51, average training loss: 7856.06, base loss: 15334.95
[INFO 2017-07-01 23:11:55,750 main.py:52] epoch 3865, training loss: 8458.06, average training loss: 7855.86, base loss: 15335.40
[INFO 2017-07-01 23:11:59,904 main.py:52] epoch 3866, training loss: 8025.86, average training loss: 7856.32, base loss: 15334.96
[INFO 2017-07-01 23:12:04,159 main.py:52] epoch 3867, training loss: 7953.51, average training loss: 7856.97, base loss: 15334.74
[INFO 2017-07-01 23:12:08,426 main.py:52] epoch 3868, training loss: 7610.09, average training loss: 7855.63, base loss: 15334.41
[INFO 2017-07-01 23:12:12,683 main.py:52] epoch 3869, training loss: 8078.89, average training loss: 7855.69, base loss: 15334.58
[INFO 2017-07-01 23:12:16,894 main.py:52] epoch 3870, training loss: 8767.24, average training loss: 7856.77, base loss: 15335.57
[INFO 2017-07-01 23:12:21,126 main.py:52] epoch 3871, training loss: 7377.39, average training loss: 7856.58, base loss: 15335.77
[INFO 2017-07-01 23:12:25,311 main.py:52] epoch 3872, training loss: 7310.84, average training loss: 7855.68, base loss: 15335.79
[INFO 2017-07-01 23:12:29,415 main.py:52] epoch 3873, training loss: 6837.58, average training loss: 7854.21, base loss: 15335.34
[INFO 2017-07-01 23:12:33,583 main.py:52] epoch 3874, training loss: 7132.66, average training loss: 7852.99, base loss: 15335.29
[INFO 2017-07-01 23:12:37,760 main.py:52] epoch 3875, training loss: 7784.54, average training loss: 7852.39, base loss: 15334.67
[INFO 2017-07-01 23:12:41,877 main.py:52] epoch 3876, training loss: 7815.72, average training loss: 7851.94, base loss: 15335.36
[INFO 2017-07-01 23:12:46,048 main.py:52] epoch 3877, training loss: 8417.48, average training loss: 7852.12, base loss: 15336.56
[INFO 2017-07-01 23:12:50,269 main.py:52] epoch 3878, training loss: 7338.22, average training loss: 7851.84, base loss: 15336.23
[INFO 2017-07-01 23:12:54,403 main.py:52] epoch 3879, training loss: 7096.02, average training loss: 7850.98, base loss: 15335.83
[INFO 2017-07-01 23:12:58,526 main.py:52] epoch 3880, training loss: 8558.25, average training loss: 7851.86, base loss: 15336.46
[INFO 2017-07-01 23:13:02,657 main.py:52] epoch 3881, training loss: 8652.50, average training loss: 7851.84, base loss: 15337.40
[INFO 2017-07-01 23:13:06,844 main.py:52] epoch 3882, training loss: 7111.44, average training loss: 7851.82, base loss: 15337.53
[INFO 2017-07-01 23:13:10,997 main.py:52] epoch 3883, training loss: 7198.19, average training loss: 7852.21, base loss: 15337.57
[INFO 2017-07-01 23:13:15,215 main.py:52] epoch 3884, training loss: 7223.05, average training loss: 7852.01, base loss: 15337.37
[INFO 2017-07-01 23:13:19,417 main.py:52] epoch 3885, training loss: 7301.10, average training loss: 7851.26, base loss: 15337.31
[INFO 2017-07-01 23:13:23,612 main.py:52] epoch 3886, training loss: 8279.55, average training loss: 7851.96, base loss: 15338.76
[INFO 2017-07-01 23:13:27,789 main.py:52] epoch 3887, training loss: 7992.66, average training loss: 7852.03, base loss: 15339.68
[INFO 2017-07-01 23:13:31,969 main.py:52] epoch 3888, training loss: 8279.47, average training loss: 7852.83, base loss: 15339.94
[INFO 2017-07-01 23:13:36,134 main.py:52] epoch 3889, training loss: 6972.24, average training loss: 7851.81, base loss: 15338.51
[INFO 2017-07-01 23:13:40,284 main.py:52] epoch 3890, training loss: 7585.81, average training loss: 7851.63, base loss: 15338.30
[INFO 2017-07-01 23:13:44,435 main.py:52] epoch 3891, training loss: 7856.66, average training loss: 7851.80, base loss: 15337.97
[INFO 2017-07-01 23:13:48,528 main.py:52] epoch 3892, training loss: 8185.71, average training loss: 7852.86, base loss: 15338.43
[INFO 2017-07-01 23:13:52,745 main.py:52] epoch 3893, training loss: 8264.50, average training loss: 7854.10, base loss: 15338.91
[INFO 2017-07-01 23:13:56,881 main.py:52] epoch 3894, training loss: 7679.05, average training loss: 7853.86, base loss: 15338.76
[INFO 2017-07-01 23:14:01,086 main.py:52] epoch 3895, training loss: 8093.13, average training loss: 7854.29, base loss: 15339.13
[INFO 2017-07-01 23:14:05,228 main.py:52] epoch 3896, training loss: 7492.31, average training loss: 7853.49, base loss: 15338.45
[INFO 2017-07-01 23:14:09,437 main.py:52] epoch 3897, training loss: 7965.95, average training loss: 7853.73, base loss: 15338.72
[INFO 2017-07-01 23:14:13,674 main.py:52] epoch 3898, training loss: 7960.78, average training loss: 7853.27, base loss: 15338.27
[INFO 2017-07-01 23:14:17,851 main.py:52] epoch 3899, training loss: 7207.43, average training loss: 7853.46, base loss: 15337.44
[INFO 2017-07-01 23:14:17,851 main.py:54] epoch 3899, testing
[INFO 2017-07-01 23:14:17,851 main.py:61] model save to ./model/final.pth
[INFO 2017-07-01 23:14:22,134 main.py:52] epoch 3900, training loss: 8399.10, average training loss: 7854.40, base loss: 15337.79
[INFO 2017-07-01 23:14:26,321 main.py:52] epoch 3901, training loss: 7046.50, average training loss: 7853.87, base loss: 15336.93
[INFO 2017-07-01 23:14:30,591 main.py:52] epoch 3902, training loss: 7133.79, average training loss: 7852.93, base loss: 15336.59
[INFO 2017-07-01 23:14:34,712 main.py:52] epoch 3903, training loss: 8165.66, average training loss: 7853.56, base loss: 15336.88
[INFO 2017-07-01 23:14:38,877 main.py:52] epoch 3904, training loss: 7857.89, average training loss: 7852.23, base loss: 15337.01
[INFO 2017-07-01 23:14:43,067 main.py:52] epoch 3905, training loss: 7314.63, average training loss: 7850.70, base loss: 15337.25
[INFO 2017-07-01 23:14:47,332 main.py:52] epoch 3906, training loss: 8509.34, average training loss: 7851.05, base loss: 15337.28
[INFO 2017-07-01 23:14:51,529 main.py:52] epoch 3907, training loss: 7622.16, average training loss: 7850.16, base loss: 15337.60
[INFO 2017-07-01 23:14:55,741 main.py:52] epoch 3908, training loss: 7891.73, average training loss: 7850.15, base loss: 15338.26
[INFO 2017-07-01 23:15:00,023 main.py:52] epoch 3909, training loss: 7804.33, average training loss: 7850.47, base loss: 15338.47
[INFO 2017-07-01 23:15:04,163 main.py:52] epoch 3910, training loss: 7084.11, average training loss: 7849.22, base loss: 15337.70
[INFO 2017-07-01 23:15:08,289 main.py:52] epoch 3911, training loss: 8577.99, average training loss: 7850.19, base loss: 15337.83
[INFO 2017-07-01 23:15:12,503 main.py:52] epoch 3912, training loss: 7783.89, average training loss: 7850.80, base loss: 15337.89
[INFO 2017-07-01 23:15:16,639 main.py:52] epoch 3913, training loss: 7170.24, average training loss: 7850.06, base loss: 15337.65
[INFO 2017-07-01 23:15:20,828 main.py:52] epoch 3914, training loss: 6490.49, average training loss: 7849.22, base loss: 15337.51
[INFO 2017-07-01 23:15:25,049 main.py:52] epoch 3915, training loss: 8011.02, average training loss: 7849.29, base loss: 15337.93
[INFO 2017-07-01 23:15:29,232 main.py:52] epoch 3916, training loss: 7219.88, average training loss: 7848.38, base loss: 15337.28
[INFO 2017-07-01 23:15:33,385 main.py:52] epoch 3917, training loss: 7920.05, average training loss: 7848.34, base loss: 15337.14
[INFO 2017-07-01 23:15:37,584 main.py:52] epoch 3918, training loss: 7207.42, average training loss: 7847.97, base loss: 15335.95
[INFO 2017-07-01 23:15:41,715 main.py:52] epoch 3919, training loss: 8154.38, average training loss: 7846.96, base loss: 15335.77
[INFO 2017-07-01 23:15:45,888 main.py:52] epoch 3920, training loss: 7521.94, average training loss: 7845.74, base loss: 15335.39
[INFO 2017-07-01 23:15:50,081 main.py:52] epoch 3921, training loss: 8379.72, average training loss: 7846.42, base loss: 15335.59
[INFO 2017-07-01 23:15:54,217 main.py:52] epoch 3922, training loss: 7536.61, average training loss: 7845.31, base loss: 15335.79
[INFO 2017-07-01 23:15:58,363 main.py:52] epoch 3923, training loss: 7240.76, average training loss: 7844.54, base loss: 15335.62
[INFO 2017-07-01 23:16:02,560 main.py:52] epoch 3924, training loss: 8606.55, average training loss: 7845.26, base loss: 15336.40
[INFO 2017-07-01 23:16:06,717 main.py:52] epoch 3925, training loss: 7861.84, average training loss: 7844.36, base loss: 15336.47
[INFO 2017-07-01 23:16:10,936 main.py:52] epoch 3926, training loss: 8148.41, average training loss: 7843.81, base loss: 15336.57
[INFO 2017-07-01 23:16:15,086 main.py:52] epoch 3927, training loss: 7108.04, average training loss: 7843.33, base loss: 15336.15
[INFO 2017-07-01 23:16:19,288 main.py:52] epoch 3928, training loss: 7554.85, average training loss: 7843.06, base loss: 15336.32
[INFO 2017-07-01 23:16:23,484 main.py:52] epoch 3929, training loss: 7552.83, average training loss: 7842.26, base loss: 15336.81
[INFO 2017-07-01 23:16:27,667 main.py:52] epoch 3930, training loss: 7808.58, average training loss: 7842.03, base loss: 15336.90
[INFO 2017-07-01 23:16:31,870 main.py:52] epoch 3931, training loss: 7160.36, average training loss: 7841.37, base loss: 15336.35
[INFO 2017-07-01 23:16:35,986 main.py:52] epoch 3932, training loss: 7686.58, average training loss: 7840.84, base loss: 15336.70
[INFO 2017-07-01 23:16:40,127 main.py:52] epoch 3933, training loss: 8785.76, average training loss: 7841.67, base loss: 15336.97
[INFO 2017-07-01 23:16:44,306 main.py:52] epoch 3934, training loss: 7427.62, average training loss: 7840.93, base loss: 15336.84
[INFO 2017-07-01 23:16:48,429 main.py:52] epoch 3935, training loss: 7249.48, average training loss: 7840.56, base loss: 15336.46
[INFO 2017-07-01 23:16:52,623 main.py:52] epoch 3936, training loss: 8166.96, average training loss: 7841.29, base loss: 15336.47
[INFO 2017-07-01 23:16:56,861 main.py:52] epoch 3937, training loss: 8140.33, average training loss: 7841.83, base loss: 15336.69
[INFO 2017-07-01 23:17:00,990 main.py:52] epoch 3938, training loss: 7046.76, average training loss: 7841.00, base loss: 15336.28
[INFO 2017-07-01 23:17:05,182 main.py:52] epoch 3939, training loss: 7037.25, average training loss: 7838.66, base loss: 15335.40
[INFO 2017-07-01 23:17:09,325 main.py:52] epoch 3940, training loss: 7019.54, average training loss: 7838.14, base loss: 15334.42
[INFO 2017-07-01 23:17:13,501 main.py:52] epoch 3941, training loss: 8069.50, average training loss: 7838.39, base loss: 15334.50
[INFO 2017-07-01 23:17:17,697 main.py:52] epoch 3942, training loss: 7968.69, average training loss: 7837.99, base loss: 15334.60
[INFO 2017-07-01 23:17:21,797 main.py:52] epoch 3943, training loss: 8447.74, average training loss: 7837.77, base loss: 15334.26
[INFO 2017-07-01 23:17:25,939 main.py:52] epoch 3944, training loss: 7255.00, average training loss: 7836.11, base loss: 15333.43
[INFO 2017-07-01 23:17:30,035 main.py:52] epoch 3945, training loss: 7949.77, average training loss: 7835.59, base loss: 15333.68
[INFO 2017-07-01 23:17:34,161 main.py:52] epoch 3946, training loss: 8323.23, average training loss: 7836.26, base loss: 15334.13
[INFO 2017-07-01 23:17:38,348 main.py:52] epoch 3947, training loss: 8344.50, average training loss: 7835.97, base loss: 15334.59
[INFO 2017-07-01 23:17:42,523 main.py:52] epoch 3948, training loss: 6913.65, average training loss: 7834.67, base loss: 15333.61
[INFO 2017-07-01 23:17:46,656 main.py:52] epoch 3949, training loss: 8046.61, average training loss: 7834.26, base loss: 15334.16
[INFO 2017-07-01 23:17:50,755 main.py:52] epoch 3950, training loss: 8847.76, average training loss: 7836.17, base loss: 15334.38
[INFO 2017-07-01 23:17:54,949 main.py:52] epoch 3951, training loss: 6828.94, average training loss: 7834.73, base loss: 15333.81
[INFO 2017-07-01 23:17:59,151 main.py:52] epoch 3952, training loss: 7601.34, average training loss: 7834.34, base loss: 15333.60
[INFO 2017-07-01 23:18:03,328 main.py:52] epoch 3953, training loss: 8156.29, average training loss: 7833.26, base loss: 15333.42
[INFO 2017-07-01 23:18:07,574 main.py:52] epoch 3954, training loss: 7706.02, average training loss: 7833.30, base loss: 15333.57
[INFO 2017-07-01 23:18:11,840 main.py:52] epoch 3955, training loss: 7541.22, average training loss: 7832.93, base loss: 15333.61
[INFO 2017-07-01 23:18:16,018 main.py:52] epoch 3956, training loss: 6955.19, average training loss: 7831.80, base loss: 15332.41
[INFO 2017-07-01 23:18:20,199 main.py:52] epoch 3957, training loss: 7531.80, average training loss: 7831.57, base loss: 15332.54
[INFO 2017-07-01 23:18:24,318 main.py:52] epoch 3958, training loss: 8008.49, average training loss: 7831.53, base loss: 15333.03
[INFO 2017-07-01 23:18:28,460 main.py:52] epoch 3959, training loss: 7960.54, average training loss: 7832.13, base loss: 15332.94
[INFO 2017-07-01 23:18:32,623 main.py:52] epoch 3960, training loss: 8008.25, average training loss: 7832.28, base loss: 15332.65
[INFO 2017-07-01 23:18:36,881 main.py:52] epoch 3961, training loss: 8376.83, average training loss: 7831.74, base loss: 15333.36
[INFO 2017-07-01 23:18:41,038 main.py:52] epoch 3962, training loss: 7856.81, average training loss: 7831.65, base loss: 15333.02
[INFO 2017-07-01 23:18:45,231 main.py:52] epoch 3963, training loss: 7348.63, average training loss: 7830.61, base loss: 15332.75
[INFO 2017-07-01 23:18:49,486 main.py:52] epoch 3964, training loss: 7821.30, average training loss: 7830.15, base loss: 15332.68
[INFO 2017-07-01 23:18:53,640 main.py:52] epoch 3965, training loss: 8702.00, average training loss: 7830.92, base loss: 15333.40
[INFO 2017-07-01 23:18:57,889 main.py:52] epoch 3966, training loss: 7400.93, average training loss: 7830.16, base loss: 15333.00
[INFO 2017-07-01 23:19:02,146 main.py:52] epoch 3967, training loss: 8488.30, average training loss: 7830.88, base loss: 15333.34
[INFO 2017-07-01 23:19:06,306 main.py:52] epoch 3968, training loss: 8517.81, average training loss: 7831.46, base loss: 15334.36
[INFO 2017-07-01 23:19:10,480 main.py:52] epoch 3969, training loss: 7537.99, average training loss: 7830.37, base loss: 15334.41
[INFO 2017-07-01 23:19:14,669 main.py:52] epoch 3970, training loss: 7524.46, average training loss: 7830.38, base loss: 15334.44
[INFO 2017-07-01 23:19:18,797 main.py:52] epoch 3971, training loss: 7571.88, average training loss: 7830.35, base loss: 15333.84
[INFO 2017-07-01 23:19:22,887 main.py:52] epoch 3972, training loss: 8325.60, average training loss: 7830.64, base loss: 15334.21
[INFO 2017-07-01 23:19:27,064 main.py:52] epoch 3973, training loss: 7101.63, average training loss: 7830.54, base loss: 15333.44
[INFO 2017-07-01 23:19:31,220 main.py:52] epoch 3974, training loss: 8019.44, average training loss: 7830.25, base loss: 15334.45
[INFO 2017-07-01 23:19:35,354 main.py:52] epoch 3975, training loss: 7844.95, average training loss: 7829.79, base loss: 15334.70
[INFO 2017-07-01 23:19:39,497 main.py:52] epoch 3976, training loss: 7629.04, average training loss: 7829.62, base loss: 15334.28
[INFO 2017-07-01 23:19:43,668 main.py:52] epoch 3977, training loss: 7660.18, average training loss: 7829.04, base loss: 15333.91
[INFO 2017-07-01 23:19:47,843 main.py:52] epoch 3978, training loss: 7092.97, average training loss: 7827.66, base loss: 15333.00
[INFO 2017-07-01 23:19:52,077 main.py:52] epoch 3979, training loss: 8138.78, average training loss: 7827.22, base loss: 15333.12
[INFO 2017-07-01 23:19:56,277 main.py:52] epoch 3980, training loss: 7878.58, average training loss: 7826.71, base loss: 15332.27
[INFO 2017-07-01 23:20:00,427 main.py:52] epoch 3981, training loss: 8200.83, average training loss: 7826.70, base loss: 15332.36
[INFO 2017-07-01 23:20:04,656 main.py:52] epoch 3982, training loss: 7473.94, average training loss: 7826.88, base loss: 15332.48
[INFO 2017-07-01 23:20:08,838 main.py:52] epoch 3983, training loss: 8641.64, average training loss: 7827.40, base loss: 15333.46
[INFO 2017-07-01 23:20:12,989 main.py:52] epoch 3984, training loss: 7839.27, average training loss: 7826.28, base loss: 15333.71
[INFO 2017-07-01 23:20:17,164 main.py:52] epoch 3985, training loss: 8055.74, average training loss: 7826.77, base loss: 15334.87
[INFO 2017-07-01 23:20:21,265 main.py:52] epoch 3986, training loss: 7215.57, average training loss: 7827.01, base loss: 15334.45
[INFO 2017-07-01 23:20:25,459 main.py:52] epoch 3987, training loss: 7811.49, average training loss: 7826.69, base loss: 15334.58
[INFO 2017-07-01 23:20:29,625 main.py:52] epoch 3988, training loss: 6951.83, average training loss: 7824.91, base loss: 15333.86
[INFO 2017-07-01 23:20:33,894 main.py:52] epoch 3989, training loss: 7104.99, average training loss: 7823.90, base loss: 15332.88
[INFO 2017-07-01 23:20:38,007 main.py:52] epoch 3990, training loss: 7474.62, average training loss: 7823.66, base loss: 15333.02
[INFO 2017-07-01 23:20:42,136 main.py:52] epoch 3991, training loss: 7397.75, average training loss: 7822.72, base loss: 15332.79
[INFO 2017-07-01 23:20:46,327 main.py:52] epoch 3992, training loss: 7438.91, average training loss: 7822.28, base loss: 15332.58
[INFO 2017-07-01 23:20:50,523 main.py:52] epoch 3993, training loss: 7519.14, average training loss: 7821.30, base loss: 15332.14
[INFO 2017-07-01 23:20:54,739 main.py:52] epoch 3994, training loss: 7292.32, average training loss: 7821.17, base loss: 15331.85
[INFO 2017-07-01 23:20:58,983 main.py:52] epoch 3995, training loss: 8304.91, average training loss: 7822.27, base loss: 15332.33
[INFO 2017-07-01 23:21:03,206 main.py:52] epoch 3996, training loss: 7088.06, average training loss: 7821.99, base loss: 15331.61
[INFO 2017-07-01 23:21:07,379 main.py:52] epoch 3997, training loss: 7666.74, average training loss: 7821.06, base loss: 15331.15
[INFO 2017-07-01 23:21:11,574 main.py:52] epoch 3998, training loss: 6878.68, average training loss: 7820.71, base loss: 15330.34
[INFO 2017-07-01 23:21:15,709 main.py:52] epoch 3999, training loss: 7625.14, average training loss: 7819.88, base loss: 15330.65
[INFO 2017-07-01 23:21:15,710 main.py:54] epoch 3999, testing
[INFO 2017-07-01 23:21:15,710 main.py:61] model save to ./model/final.pth
[INFO 2017-07-01 23:21:20,004 main.py:52] epoch 4000, training loss: 7795.94, average training loss: 7820.22, base loss: 15330.38
[INFO 2017-07-01 23:21:24,243 main.py:52] epoch 4001, training loss: 8603.21, average training loss: 7819.91, base loss: 15330.56
[INFO 2017-07-01 23:21:28,403 main.py:52] epoch 4002, training loss: 8101.90, average training loss: 7820.90, base loss: 15331.29
[INFO 2017-07-01 23:21:32,537 main.py:52] epoch 4003, training loss: 7243.18, average training loss: 7820.23, base loss: 15331.13
[INFO 2017-07-01 23:21:36,730 main.py:52] epoch 4004, training loss: 7907.49, average training loss: 7819.84, base loss: 15331.16
[INFO 2017-07-01 23:21:40,891 main.py:52] epoch 4005, training loss: 7637.90, average training loss: 7819.42, base loss: 15330.25
[INFO 2017-07-01 23:21:45,094 main.py:52] epoch 4006, training loss: 8224.23, average training loss: 7820.95, base loss: 15330.54
[INFO 2017-07-01 23:21:49,329 main.py:52] epoch 4007, training loss: 8315.02, average training loss: 7820.50, base loss: 15330.30
[INFO 2017-07-01 23:21:53,557 main.py:52] epoch 4008, training loss: 7360.15, average training loss: 7820.04, base loss: 15329.80
[INFO 2017-07-01 23:21:57,749 main.py:52] epoch 4009, training loss: 7996.13, average training loss: 7820.62, base loss: 15329.34
[INFO 2017-07-01 23:22:01,874 main.py:52] epoch 4010, training loss: 8332.32, average training loss: 7820.97, base loss: 15329.00
[INFO 2017-07-01 23:22:06,067 main.py:52] epoch 4011, training loss: 7764.54, average training loss: 7820.64, base loss: 15328.94
[INFO 2017-07-01 23:22:10,283 main.py:52] epoch 4012, training loss: 8131.34, average training loss: 7821.01, base loss: 15329.09
[INFO 2017-07-01 23:22:14,492 main.py:52] epoch 4013, training loss: 7834.17, average training loss: 7820.49, base loss: 15329.68
[INFO 2017-07-01 23:22:18,614 main.py:52] epoch 4014, training loss: 7702.10, average training loss: 7820.97, base loss: 15329.39
[INFO 2017-07-01 23:22:22,768 main.py:52] epoch 4015, training loss: 8088.30, average training loss: 7820.54, base loss: 15329.69
[INFO 2017-07-01 23:22:26,933 main.py:52] epoch 4016, training loss: 8211.19, average training loss: 7821.16, base loss: 15330.07
[INFO 2017-07-01 23:22:31,134 main.py:52] epoch 4017, training loss: 8107.58, average training loss: 7822.27, base loss: 15330.16
[INFO 2017-07-01 23:22:35,325 main.py:52] epoch 4018, training loss: 7487.14, average training loss: 7822.05, base loss: 15329.57
[INFO 2017-07-01 23:22:39,484 main.py:52] epoch 4019, training loss: 8146.93, average training loss: 7821.96, base loss: 15329.29
[INFO 2017-07-01 23:22:43,635 main.py:52] epoch 4020, training loss: 7857.26, average training loss: 7822.57, base loss: 15329.52
[INFO 2017-07-01 23:22:47,883 main.py:52] epoch 4021, training loss: 7536.04, average training loss: 7822.44, base loss: 15329.39
[INFO 2017-07-01 23:22:52,074 main.py:52] epoch 4022, training loss: 8016.76, average training loss: 7823.33, base loss: 15329.83
[INFO 2017-07-01 23:22:56,202 main.py:52] epoch 4023, training loss: 7666.36, average training loss: 7823.56, base loss: 15329.62
[INFO 2017-07-01 23:23:00,360 main.py:52] epoch 4024, training loss: 8050.23, average training loss: 7824.19, base loss: 15329.23
[INFO 2017-07-01 23:23:04,516 main.py:52] epoch 4025, training loss: 7931.44, average training loss: 7824.58, base loss: 15329.05
[INFO 2017-07-01 23:23:08,696 main.py:52] epoch 4026, training loss: 8280.36, average training loss: 7825.55, base loss: 15329.32
[INFO 2017-07-01 23:23:12,816 main.py:52] epoch 4027, training loss: 7549.15, average training loss: 7824.37, base loss: 15329.39
[INFO 2017-07-01 23:23:16,965 main.py:52] epoch 4028, training loss: 7603.35, average training loss: 7823.64, base loss: 15329.44
[INFO 2017-07-01 23:23:21,159 main.py:52] epoch 4029, training loss: 7664.96, average training loss: 7824.17, base loss: 15329.01
[INFO 2017-07-01 23:23:25,300 main.py:52] epoch 4030, training loss: 7541.20, average training loss: 7823.55, base loss: 15329.21
[INFO 2017-07-01 23:23:29,429 main.py:52] epoch 4031, training loss: 8753.55, average training loss: 7824.20, base loss: 15329.67
[INFO 2017-07-01 23:23:33,628 main.py:52] epoch 4032, training loss: 8087.45, average training loss: 7824.44, base loss: 15329.61
[INFO 2017-07-01 23:23:37,809 main.py:52] epoch 4033, training loss: 7176.00, average training loss: 7823.67, base loss: 15329.33
[INFO 2017-07-01 23:23:41,987 main.py:52] epoch 4034, training loss: 7590.02, average training loss: 7823.43, base loss: 15329.16
[INFO 2017-07-01 23:23:46,194 main.py:52] epoch 4035, training loss: 6956.90, average training loss: 7822.05, base loss: 15328.24
[INFO 2017-07-01 23:23:50,290 main.py:52] epoch 4036, training loss: 7862.93, average training loss: 7821.25, base loss: 15328.45
[INFO 2017-07-01 23:23:54,430 main.py:52] epoch 4037, training loss: 7371.68, average training loss: 7820.23, base loss: 15327.95
[INFO 2017-07-01 23:23:58,644 main.py:52] epoch 4038, training loss: 8786.44, average training loss: 7820.37, base loss: 15328.91
[INFO 2017-07-01 23:24:02,810 main.py:52] epoch 4039, training loss: 7442.11, average training loss: 7820.38, base loss: 15328.34
[INFO 2017-07-01 23:24:07,028 main.py:52] epoch 4040, training loss: 7620.25, average training loss: 7819.66, base loss: 15328.34
[INFO 2017-07-01 23:24:11,196 main.py:52] epoch 4041, training loss: 7109.09, average training loss: 7818.22, base loss: 15327.69
[INFO 2017-07-01 23:24:15,348 main.py:52] epoch 4042, training loss: 7283.52, average training loss: 7816.37, base loss: 15327.75
[INFO 2017-07-01 23:24:19,487 main.py:52] epoch 4043, training loss: 7323.92, average training loss: 7816.05, base loss: 15327.12
[INFO 2017-07-01 23:24:23,751 main.py:52] epoch 4044, training loss: 7933.61, average training loss: 7816.58, base loss: 15327.11
[INFO 2017-07-01 23:24:27,865 main.py:52] epoch 4045, training loss: 8007.40, average training loss: 7816.28, base loss: 15327.61
[INFO 2017-07-01 23:24:32,010 main.py:52] epoch 4046, training loss: 8017.61, average training loss: 7816.83, base loss: 15327.09
[INFO 2017-07-01 23:24:36,201 main.py:52] epoch 4047, training loss: 8278.87, average training loss: 7817.35, base loss: 15327.17
[INFO 2017-07-01 23:24:40,296 main.py:52] epoch 4048, training loss: 7516.95, average training loss: 7818.33, base loss: 15327.09
[INFO 2017-07-01 23:24:44,455 main.py:52] epoch 4049, training loss: 8006.20, average training loss: 7818.49, base loss: 15327.47
[INFO 2017-07-01 23:24:48,680 main.py:52] epoch 4050, training loss: 7182.36, average training loss: 7817.51, base loss: 15327.15
[INFO 2017-07-01 23:24:52,828 main.py:52] epoch 4051, training loss: 7245.13, average training loss: 7817.17, base loss: 15327.14
[INFO 2017-07-01 23:24:57,047 main.py:52] epoch 4052, training loss: 7188.41, average training loss: 7816.27, base loss: 15325.87
[INFO 2017-07-01 23:25:01,193 main.py:52] epoch 4053, training loss: 7768.67, average training loss: 7815.68, base loss: 15325.37
[INFO 2017-07-01 23:25:05,309 main.py:52] epoch 4054, training loss: 8341.60, average training loss: 7816.32, base loss: 15325.99
[INFO 2017-07-01 23:25:09,534 main.py:52] epoch 4055, training loss: 8598.22, average training loss: 7815.79, base loss: 15326.39
[INFO 2017-07-01 23:25:13,701 main.py:52] epoch 4056, training loss: 7704.88, average training loss: 7815.02, base loss: 15326.54
[INFO 2017-07-01 23:25:17,848 main.py:52] epoch 4057, training loss: 7427.72, average training loss: 7814.93, base loss: 15326.13
[INFO 2017-07-01 23:25:21,956 main.py:52] epoch 4058, training loss: 8322.06, average training loss: 7815.26, base loss: 15325.81
[INFO 2017-07-01 23:25:26,058 main.py:52] epoch 4059, training loss: 7541.23, average training loss: 7814.30, base loss: 15324.50
[INFO 2017-07-01 23:25:30,139 main.py:52] epoch 4060, training loss: 8177.86, average training loss: 7814.79, base loss: 15324.90
[INFO 2017-07-01 23:25:34,282 main.py:52] epoch 4061, training loss: 7862.24, average training loss: 7813.61, base loss: 15324.60
[INFO 2017-07-01 23:25:38,441 main.py:52] epoch 4062, training loss: 8458.81, average training loss: 7815.14, base loss: 15325.16
[INFO 2017-07-01 23:25:42,586 main.py:52] epoch 4063, training loss: 8332.76, average training loss: 7815.48, base loss: 15324.92
[INFO 2017-07-01 23:25:46,801 main.py:52] epoch 4064, training loss: 6915.65, average training loss: 7815.30, base loss: 15325.29
[INFO 2017-07-01 23:25:50,921 main.py:52] epoch 4065, training loss: 7637.16, average training loss: 7814.94, base loss: 15324.82
[INFO 2017-07-01 23:25:55,069 main.py:52] epoch 4066, training loss: 8320.94, average training loss: 7816.04, base loss: 15325.69
[INFO 2017-07-01 23:25:59,227 main.py:52] epoch 4067, training loss: 7002.37, average training loss: 7815.70, base loss: 15325.64
[INFO 2017-07-01 23:26:03,436 main.py:52] epoch 4068, training loss: 7314.41, average training loss: 7815.59, base loss: 15325.68
[INFO 2017-07-01 23:26:07,627 main.py:52] epoch 4069, training loss: 7702.33, average training loss: 7815.33, base loss: 15325.16
[INFO 2017-07-01 23:26:11,696 main.py:52] epoch 4070, training loss: 7892.93, average training loss: 7815.81, base loss: 15325.67
[INFO 2017-07-01 23:26:15,915 main.py:52] epoch 4071, training loss: 7530.31, average training loss: 7815.40, base loss: 15325.38
[INFO 2017-07-01 23:26:20,111 main.py:52] epoch 4072, training loss: 8264.36, average training loss: 7816.42, base loss: 15325.86
[INFO 2017-07-01 23:26:24,314 main.py:52] epoch 4073, training loss: 6969.12, average training loss: 7815.83, base loss: 15325.07
[INFO 2017-07-01 23:26:28,463 main.py:52] epoch 4074, training loss: 7757.71, average training loss: 7815.94, base loss: 15324.98
[INFO 2017-07-01 23:26:32,588 main.py:52] epoch 4075, training loss: 8440.46, average training loss: 7816.68, base loss: 15325.43
[INFO 2017-07-01 23:26:36,805 main.py:52] epoch 4076, training loss: 7783.85, average training loss: 7816.68, base loss: 15325.70
[INFO 2017-07-01 23:26:40,989 main.py:52] epoch 4077, training loss: 7558.68, average training loss: 7816.48, base loss: 15325.63
[INFO 2017-07-01 23:26:45,082 main.py:52] epoch 4078, training loss: 8125.31, average training loss: 7816.32, base loss: 15325.94
[INFO 2017-07-01 23:26:49,256 main.py:52] epoch 4079, training loss: 8058.22, average training loss: 7816.65, base loss: 15326.23
[INFO 2017-07-01 23:26:53,462 main.py:52] epoch 4080, training loss: 7731.42, average training loss: 7816.31, base loss: 15326.38
[INFO 2017-07-01 23:26:57,619 main.py:52] epoch 4081, training loss: 7206.87, average training loss: 7816.03, base loss: 15326.20
[INFO 2017-07-01 23:27:01,721 main.py:52] epoch 4082, training loss: 7406.14, average training loss: 7815.32, base loss: 15325.02
[INFO 2017-07-01 23:27:05,927 main.py:52] epoch 4083, training loss: 7753.24, average training loss: 7815.40, base loss: 15324.46
[INFO 2017-07-01 23:27:10,106 main.py:52] epoch 4084, training loss: 8033.52, average training loss: 7815.96, base loss: 15324.54
[INFO 2017-07-01 23:27:14,257 main.py:52] epoch 4085, training loss: 7904.04, average training loss: 7816.30, base loss: 15324.54
[INFO 2017-07-01 23:27:18,355 main.py:52] epoch 4086, training loss: 7342.46, average training loss: 7815.53, base loss: 15324.18
[INFO 2017-07-01 23:27:22,576 main.py:52] epoch 4087, training loss: 7198.08, average training loss: 7814.16, base loss: 15323.87
[INFO 2017-07-01 23:27:26,706 main.py:52] epoch 4088, training loss: 8367.66, average training loss: 7815.12, base loss: 15324.30
[INFO 2017-07-01 23:27:30,920 main.py:52] epoch 4089, training loss: 7524.06, average training loss: 7815.05, base loss: 15323.92
[INFO 2017-07-01 23:27:35,142 main.py:52] epoch 4090, training loss: 7588.32, average training loss: 7814.92, base loss: 15323.58
[INFO 2017-07-01 23:27:39,324 main.py:52] epoch 4091, training loss: 7333.50, average training loss: 7813.20, base loss: 15323.21
[INFO 2017-07-01 23:27:43,477 main.py:52] epoch 4092, training loss: 8404.77, average training loss: 7813.37, base loss: 15323.88
[INFO 2017-07-01 23:27:47,635 main.py:52] epoch 4093, training loss: 7513.02, average training loss: 7811.93, base loss: 15323.52
[INFO 2017-07-01 23:27:51,830 main.py:52] epoch 4094, training loss: 8145.76, average training loss: 7812.61, base loss: 15323.61
[INFO 2017-07-01 23:27:56,041 main.py:52] epoch 4095, training loss: 7748.84, average training loss: 7812.59, base loss: 15324.04
[INFO 2017-07-01 23:28:00,198 main.py:52] epoch 4096, training loss: 7402.49, average training loss: 7812.28, base loss: 15324.18
[INFO 2017-07-01 23:28:04,410 main.py:52] epoch 4097, training loss: 7149.61, average training loss: 7811.53, base loss: 15324.73
[INFO 2017-07-01 23:28:08,563 main.py:52] epoch 4098, training loss: 7283.72, average training loss: 7811.02, base loss: 15324.41
[INFO 2017-07-01 23:28:12,751 main.py:52] epoch 4099, training loss: 7370.38, average training loss: 7810.19, base loss: 15324.41
[INFO 2017-07-01 23:28:12,752 main.py:54] epoch 4099, testing
[INFO 2017-07-01 23:28:12,752 main.py:61] model save to ./model/final.pth
[INFO 2017-07-01 23:28:16,992 main.py:52] epoch 4100, training loss: 7849.22, average training loss: 7809.63, base loss: 15324.20
[INFO 2017-07-01 23:28:21,101 main.py:52] epoch 4101, training loss: 7673.02, average training loss: 7808.13, base loss: 15324.09
[INFO 2017-07-01 23:28:25,289 main.py:52] epoch 4102, training loss: 7379.31, average training loss: 7807.37, base loss: 15323.43
[INFO 2017-07-01 23:28:29,490 main.py:52] epoch 4103, training loss: 7464.74, average training loss: 7806.69, base loss: 15322.98
[INFO 2017-07-01 23:28:33,621 main.py:52] epoch 4104, training loss: 7184.90, average training loss: 7806.34, base loss: 15322.88
[INFO 2017-07-01 23:28:37,783 main.py:52] epoch 4105, training loss: 7311.67, average training loss: 7805.83, base loss: 15323.43
[INFO 2017-07-01 23:28:41,919 main.py:52] epoch 4106, training loss: 8184.83, average training loss: 7806.74, base loss: 15323.72
[INFO 2017-07-01 23:28:46,171 main.py:52] epoch 4107, training loss: 7982.54, average training loss: 7806.64, base loss: 15324.14
[INFO 2017-07-01 23:28:50,386 main.py:52] epoch 4108, training loss: 7932.21, average training loss: 7806.50, base loss: 15323.98
[INFO 2017-07-01 23:28:54,574 main.py:52] epoch 4109, training loss: 7282.10, average training loss: 7805.51, base loss: 15324.00
[INFO 2017-07-01 23:28:58,803 main.py:52] epoch 4110, training loss: 7808.10, average training loss: 7805.53, base loss: 15323.87
[INFO 2017-07-01 23:29:02,954 main.py:52] epoch 4111, training loss: 7518.95, average training loss: 7804.60, base loss: 15323.90
[INFO 2017-07-01 23:29:07,137 main.py:52] epoch 4112, training loss: 7353.14, average training loss: 7803.58, base loss: 15323.55
[INFO 2017-07-01 23:29:11,309 main.py:52] epoch 4113, training loss: 8008.92, average training loss: 7803.30, base loss: 15324.08
[INFO 2017-07-01 23:29:15,498 main.py:52] epoch 4114, training loss: 7564.82, average training loss: 7803.50, base loss: 15323.73
[INFO 2017-07-01 23:29:19,613 main.py:52] epoch 4115, training loss: 7337.39, average training loss: 7801.94, base loss: 15323.09
[INFO 2017-07-01 23:29:23,787 main.py:52] epoch 4116, training loss: 7934.75, average training loss: 7801.94, base loss: 15322.74
[INFO 2017-07-01 23:29:27,886 main.py:52] epoch 4117, training loss: 7211.95, average training loss: 7800.10, base loss: 15322.37
[INFO 2017-07-01 23:29:32,028 main.py:52] epoch 4118, training loss: 9239.64, average training loss: 7801.59, base loss: 15323.49
[INFO 2017-07-01 23:29:36,229 main.py:52] epoch 4119, training loss: 8172.28, average training loss: 7802.31, base loss: 15323.61
[INFO 2017-07-01 23:29:40,313 main.py:52] epoch 4120, training loss: 7333.95, average training loss: 7801.66, base loss: 15323.73
[INFO 2017-07-01 23:29:44,436 main.py:52] epoch 4121, training loss: 7734.94, average training loss: 7801.65, base loss: 15324.23
[INFO 2017-07-01 23:29:48,612 main.py:52] epoch 4122, training loss: 8002.17, average training loss: 7800.80, base loss: 15324.73
[INFO 2017-07-01 23:29:52,831 main.py:52] epoch 4123, training loss: 7454.25, average training loss: 7800.56, base loss: 15324.97
[INFO 2017-07-01 23:29:57,115 main.py:52] epoch 4124, training loss: 7418.22, average training loss: 7799.63, base loss: 15324.22
[INFO 2017-07-01 23:30:01,334 main.py:52] epoch 4125, training loss: 7272.59, average training loss: 7799.07, base loss: 15323.74
[INFO 2017-07-01 23:30:05,442 main.py:52] epoch 4126, training loss: 7785.92, average training loss: 7798.82, base loss: 15324.11
[INFO 2017-07-01 23:30:09,583 main.py:52] epoch 4127, training loss: 7076.90, average training loss: 7798.47, base loss: 15323.99
[INFO 2017-07-01 23:30:13,889 main.py:52] epoch 4128, training loss: 7414.40, average training loss: 7798.87, base loss: 15324.06
[INFO 2017-07-01 23:30:18,068 main.py:52] epoch 4129, training loss: 7817.14, average training loss: 7798.59, base loss: 15324.58
[INFO 2017-07-01 23:30:22,178 main.py:52] epoch 4130, training loss: 8496.98, average training loss: 7800.13, base loss: 15325.38
[INFO 2017-07-01 23:30:26,449 main.py:52] epoch 4131, training loss: 8664.11, average training loss: 7801.28, base loss: 15325.59
[INFO 2017-07-01 23:30:30,579 main.py:52] epoch 4132, training loss: 7747.82, average training loss: 7800.03, base loss: 15324.87
[INFO 2017-07-01 23:30:34,723 main.py:52] epoch 4133, training loss: 7624.40, average training loss: 7799.52, base loss: 15324.51
[INFO 2017-07-01 23:30:38,923 main.py:52] epoch 4134, training loss: 7946.51, average training loss: 7798.71, base loss: 15325.03
[INFO 2017-07-01 23:30:43,117 main.py:52] epoch 4135, training loss: 7285.62, average training loss: 7797.79, base loss: 15324.71
[INFO 2017-07-01 23:30:47,315 main.py:52] epoch 4136, training loss: 6960.13, average training loss: 7797.14, base loss: 15324.29
[INFO 2017-07-01 23:30:51,504 main.py:52] epoch 4137, training loss: 7534.57, average training loss: 7797.00, base loss: 15323.99
[INFO 2017-07-01 23:30:55,728 main.py:52] epoch 4138, training loss: 7394.88, average training loss: 7795.69, base loss: 15323.43
[INFO 2017-07-01 23:30:59,865 main.py:52] epoch 4139, training loss: 8347.45, average training loss: 7795.06, base loss: 15323.14
[INFO 2017-07-01 23:31:04,151 main.py:52] epoch 4140, training loss: 7666.43, average training loss: 7794.08, base loss: 15323.72
[INFO 2017-07-01 23:31:08,359 main.py:52] epoch 4141, training loss: 7050.78, average training loss: 7793.17, base loss: 15323.30
[INFO 2017-07-01 23:31:12,539 main.py:52] epoch 4142, training loss: 7594.21, average training loss: 7793.13, base loss: 15323.31
[INFO 2017-07-01 23:31:16,735 main.py:52] epoch 4143, training loss: 7092.53, average training loss: 7793.09, base loss: 15323.11
[INFO 2017-07-01 23:31:21,014 main.py:52] epoch 4144, training loss: 7456.39, average training loss: 7793.21, base loss: 15322.39
[INFO 2017-07-01 23:31:25,164 main.py:52] epoch 4145, training loss: 7199.25, average training loss: 7792.77, base loss: 15321.33
[INFO 2017-07-01 23:31:29,354 main.py:52] epoch 4146, training loss: 8061.16, average training loss: 7792.73, base loss: 15321.19
[INFO 2017-07-01 23:31:33,501 main.py:52] epoch 4147, training loss: 6501.88, average training loss: 7791.90, base loss: 15319.83
[INFO 2017-07-01 23:31:37,646 main.py:52] epoch 4148, training loss: 7860.10, average training loss: 7792.66, base loss: 15320.08
[INFO 2017-07-01 23:31:41,753 main.py:52] epoch 4149, training loss: 7287.70, average training loss: 7791.48, base loss: 15319.51
[INFO 2017-07-01 23:31:46,007 main.py:52] epoch 4150, training loss: 8752.86, average training loss: 7792.33, base loss: 15320.59
[INFO 2017-07-01 23:31:50,226 main.py:52] epoch 4151, training loss: 7770.21, average training loss: 7792.45, base loss: 15320.87
[INFO 2017-07-01 23:31:54,455 main.py:52] epoch 4152, training loss: 7671.78, average training loss: 7793.09, base loss: 15320.70
[INFO 2017-07-01 23:31:58,610 main.py:52] epoch 4153, training loss: 7512.74, average training loss: 7793.01, base loss: 15320.23
[INFO 2017-07-01 23:32:02,710 main.py:52] epoch 4154, training loss: 6881.67, average training loss: 7791.64, base loss: 15319.72
[INFO 2017-07-01 23:32:06,908 main.py:52] epoch 4155, training loss: 8944.43, average training loss: 7793.33, base loss: 15320.63
[INFO 2017-07-01 23:32:11,066 main.py:52] epoch 4156, training loss: 7432.96, average training loss: 7793.44, base loss: 15320.47
[INFO 2017-07-01 23:32:15,234 main.py:52] epoch 4157, training loss: 7966.92, average training loss: 7793.40, base loss: 15320.41
[INFO 2017-07-01 23:32:19,399 main.py:52] epoch 4158, training loss: 8069.15, average training loss: 7793.42, base loss: 15320.97
[INFO 2017-07-01 23:32:23,593 main.py:52] epoch 4159, training loss: 7132.07, average training loss: 7793.48, base loss: 15321.45
[INFO 2017-07-01 23:32:27,771 main.py:52] epoch 4160, training loss: 7000.30, average training loss: 7792.32, base loss: 15320.75
[INFO 2017-07-01 23:32:32,099 main.py:52] epoch 4161, training loss: 7283.18, average training loss: 7792.57, base loss: 15320.81
[INFO 2017-07-01 23:32:36,263 main.py:52] epoch 4162, training loss: 7549.41, average training loss: 7791.08, base loss: 15320.39
[INFO 2017-07-01 23:32:40,471 main.py:52] epoch 4163, training loss: 8206.86, average training loss: 7791.26, base loss: 15321.04
[INFO 2017-07-01 23:32:44,625 main.py:52] epoch 4164, training loss: 7580.00, average training loss: 7791.50, base loss: 15321.46
[INFO 2017-07-01 23:32:48,745 main.py:52] epoch 4165, training loss: 6661.65, average training loss: 7791.25, base loss: 15321.09
[INFO 2017-07-01 23:32:52,894 main.py:52] epoch 4166, training loss: 8337.00, average training loss: 7791.97, base loss: 15321.40
[INFO 2017-07-01 23:32:57,084 main.py:52] epoch 4167, training loss: 8101.63, average training loss: 7792.11, base loss: 15321.27
[INFO 2017-07-01 23:33:01,217 main.py:52] epoch 4168, training loss: 7033.86, average training loss: 7791.10, base loss: 15321.05
[INFO 2017-07-01 23:33:05,422 main.py:52] epoch 4169, training loss: 7089.21, average training loss: 7789.43, base loss: 15320.46
[INFO 2017-07-01 23:33:09,588 main.py:52] epoch 4170, training loss: 7202.37, average training loss: 7788.56, base loss: 15320.16
[INFO 2017-07-01 23:33:13,729 main.py:52] epoch 4171, training loss: 7400.44, average training loss: 7788.13, base loss: 15319.78
[INFO 2017-07-01 23:33:17,943 main.py:52] epoch 4172, training loss: 8480.99, average training loss: 7788.45, base loss: 15320.49
[INFO 2017-07-01 23:33:22,051 main.py:52] epoch 4173, training loss: 7857.10, average training loss: 7789.05, base loss: 15320.27
[INFO 2017-07-01 23:33:26,256 main.py:52] epoch 4174, training loss: 7301.12, average training loss: 7787.72, base loss: 15319.78
[INFO 2017-07-01 23:33:30,501 main.py:52] epoch 4175, training loss: 7523.19, average training loss: 7788.05, base loss: 15319.72
[INFO 2017-07-01 23:33:34,671 main.py:52] epoch 4176, training loss: 7859.56, average training loss: 7787.44, base loss: 15319.98
[INFO 2017-07-01 23:33:38,784 main.py:52] epoch 4177, training loss: 8492.16, average training loss: 7787.74, base loss: 15320.59
[INFO 2017-07-01 23:33:42,910 main.py:52] epoch 4178, training loss: 7514.39, average training loss: 7786.97, base loss: 15320.12
[INFO 2017-07-01 23:33:47,175 main.py:52] epoch 4179, training loss: 7417.21, average training loss: 7787.15, base loss: 15320.16
[INFO 2017-07-01 23:33:51,325 main.py:52] epoch 4180, training loss: 6898.10, average training loss: 7786.47, base loss: 15319.51
[INFO 2017-07-01 23:33:55,466 main.py:52] epoch 4181, training loss: 8305.09, average training loss: 7786.34, base loss: 15320.08
[INFO 2017-07-01 23:33:59,678 main.py:52] epoch 4182, training loss: 7120.77, average training loss: 7784.61, base loss: 15319.57
[INFO 2017-07-01 23:34:03,773 main.py:52] epoch 4183, training loss: 7498.13, average training loss: 7784.44, base loss: 15319.15
[INFO 2017-07-01 23:34:07,980 main.py:52] epoch 4184, training loss: 7992.83, average training loss: 7785.58, base loss: 15319.56
[INFO 2017-07-01 23:34:12,107 main.py:52] epoch 4185, training loss: 7421.68, average training loss: 7784.57, base loss: 15319.08
[INFO 2017-07-01 23:34:16,278 main.py:52] epoch 4186, training loss: 7687.13, average training loss: 7783.36, base loss: 15319.61
[INFO 2017-07-01 23:34:20,447 main.py:52] epoch 4187, training loss: 7326.64, average training loss: 7782.93, base loss: 15319.42
[INFO 2017-07-01 23:34:24,657 main.py:52] epoch 4188, training loss: 8840.46, average training loss: 7783.70, base loss: 15320.24
[INFO 2017-07-01 23:34:28,863 main.py:52] epoch 4189, training loss: 6711.52, average training loss: 7782.42, base loss: 15320.17
[INFO 2017-07-01 23:34:32,997 main.py:52] epoch 4190, training loss: 7187.22, average training loss: 7782.28, base loss: 15319.42
[INFO 2017-07-01 23:34:37,196 main.py:52] epoch 4191, training loss: 7795.92, average training loss: 7782.11, base loss: 15319.70
[INFO 2017-07-01 23:34:41,375 main.py:52] epoch 4192, training loss: 8206.00, average training loss: 7782.16, base loss: 15319.54
[INFO 2017-07-01 23:34:45,502 main.py:52] epoch 4193, training loss: 7598.93, average training loss: 7782.10, base loss: 15319.02
[INFO 2017-07-01 23:34:49,739 main.py:52] epoch 4194, training loss: 7867.71, average training loss: 7782.09, base loss: 15319.24
[INFO 2017-07-01 23:34:53,974 main.py:52] epoch 4195, training loss: 8823.11, average training loss: 7782.51, base loss: 15320.67
[INFO 2017-07-01 23:34:58,185 main.py:52] epoch 4196, training loss: 8291.78, average training loss: 7783.53, base loss: 15321.02
[INFO 2017-07-01 23:35:02,355 main.py:52] epoch 4197, training loss: 7608.99, average training loss: 7782.99, base loss: 15321.10
[INFO 2017-07-01 23:35:06,557 main.py:52] epoch 4198, training loss: 7079.32, average training loss: 7781.87, base loss: 15320.47
[INFO 2017-07-01 23:35:10,682 main.py:52] epoch 4199, training loss: 8434.99, average training loss: 7782.26, base loss: 15320.92
[INFO 2017-07-01 23:35:10,683 main.py:54] epoch 4199, testing
[INFO 2017-07-01 23:35:10,683 main.py:61] model save to ./model/final.pth
[INFO 2017-07-01 23:35:14,945 main.py:52] epoch 4200, training loss: 7181.32, average training loss: 7782.61, base loss: 15320.35
[INFO 2017-07-01 23:35:19,144 main.py:52] epoch 4201, training loss: 7484.73, average training loss: 7782.54, base loss: 15319.85
[INFO 2017-07-01 23:35:23,406 main.py:52] epoch 4202, training loss: 7801.74, average training loss: 7782.85, base loss: 15319.42
[INFO 2017-07-01 23:35:27,619 main.py:52] epoch 4203, training loss: 7392.94, average training loss: 7782.67, base loss: 15318.69
[INFO 2017-07-01 23:35:31,815 main.py:52] epoch 4204, training loss: 7147.94, average training loss: 7781.40, base loss: 15318.75
[INFO 2017-07-01 23:35:36,029 main.py:52] epoch 4205, training loss: 8072.12, average training loss: 7781.62, base loss: 15319.26
[INFO 2017-07-01 23:35:40,309 main.py:52] epoch 4206, training loss: 8598.69, average training loss: 7782.58, base loss: 15319.66
[INFO 2017-07-01 23:35:44,557 main.py:52] epoch 4207, training loss: 7555.43, average training loss: 7783.27, base loss: 15319.36
[INFO 2017-07-01 23:35:48,699 main.py:52] epoch 4208, training loss: 7100.03, average training loss: 7782.54, base loss: 15318.75
[INFO 2017-07-01 23:35:52,889 main.py:52] epoch 4209, training loss: 7411.17, average training loss: 7782.05, base loss: 15318.15
[INFO 2017-07-01 23:35:56,995 main.py:52] epoch 4210, training loss: 7757.20, average training loss: 7781.80, base loss: 15317.49
[INFO 2017-07-01 23:36:01,156 main.py:52] epoch 4211, training loss: 8077.93, average training loss: 7782.26, base loss: 15317.21
[INFO 2017-07-01 23:36:05,367 main.py:52] epoch 4212, training loss: 7812.45, average training loss: 7782.89, base loss: 15316.93
[INFO 2017-07-01 23:36:09,546 main.py:52] epoch 4213, training loss: 8668.16, average training loss: 7783.41, base loss: 15318.02
[INFO 2017-07-01 23:36:13,765 main.py:52] epoch 4214, training loss: 8185.73, average training loss: 7784.38, base loss: 15319.00
[INFO 2017-07-01 23:36:17,884 main.py:52] epoch 4215, training loss: 6866.01, average training loss: 7783.83, base loss: 15317.72
[INFO 2017-07-01 23:36:22,017 main.py:52] epoch 4216, training loss: 8702.45, average training loss: 7785.56, base loss: 15318.13
[INFO 2017-07-01 23:36:26,178 main.py:52] epoch 4217, training loss: 7487.47, average training loss: 7785.32, base loss: 15317.13
[INFO 2017-07-01 23:36:30,369 main.py:52] epoch 4218, training loss: 8352.57, average training loss: 7785.45, base loss: 15317.50
[INFO 2017-07-01 23:36:34,512 main.py:52] epoch 4219, training loss: 8539.07, average training loss: 7785.96, base loss: 15317.60
[INFO 2017-07-01 23:36:38,622 main.py:52] epoch 4220, training loss: 7711.06, average training loss: 7785.70, base loss: 15317.24
[INFO 2017-07-01 23:36:42,825 main.py:52] epoch 4221, training loss: 7575.81, average training loss: 7786.25, base loss: 15316.58
[INFO 2017-07-01 23:36:47,004 main.py:52] epoch 4222, training loss: 8435.67, average training loss: 7785.68, base loss: 15317.61
[INFO 2017-07-01 23:36:51,192 main.py:52] epoch 4223, training loss: 7980.94, average training loss: 7785.39, base loss: 15317.97
[INFO 2017-07-01 23:36:55,346 main.py:52] epoch 4224, training loss: 8596.82, average training loss: 7784.54, base loss: 15318.65
[INFO 2017-07-01 23:36:59,474 main.py:52] epoch 4225, training loss: 8190.44, average training loss: 7784.30, base loss: 15319.15
[INFO 2017-07-01 23:37:03,639 main.py:52] epoch 4226, training loss: 7038.74, average training loss: 7783.05, base loss: 15318.98
[INFO 2017-07-01 23:37:07,836 main.py:52] epoch 4227, training loss: 7143.98, average training loss: 7782.54, base loss: 15318.85
[INFO 2017-07-01 23:37:12,067 main.py:52] epoch 4228, training loss: 7181.95, average training loss: 7781.54, base loss: 15318.54
[INFO 2017-07-01 23:37:16,330 main.py:52] epoch 4229, training loss: 7522.75, average training loss: 7781.55, base loss: 15318.32
[INFO 2017-07-01 23:37:20,475 main.py:52] epoch 4230, training loss: 7406.07, average training loss: 7781.86, base loss: 15318.45
[INFO 2017-07-01 23:37:24,778 main.py:52] epoch 4231, training loss: 6468.03, average training loss: 7780.86, base loss: 15317.87
[INFO 2017-07-01 23:37:28,912 main.py:52] epoch 4232, training loss: 7694.29, average training loss: 7780.54, base loss: 15318.51
[INFO 2017-07-01 23:37:33,079 main.py:52] epoch 4233, training loss: 7370.86, average training loss: 7781.26, base loss: 15318.14
[INFO 2017-07-01 23:37:37,254 main.py:52] epoch 4234, training loss: 9111.95, average training loss: 7781.89, base loss: 15319.75
[INFO 2017-07-01 23:37:41,449 main.py:52] epoch 4235, training loss: 8085.80, average training loss: 7782.45, base loss: 15319.97
[INFO 2017-07-01 23:37:45,625 main.py:52] epoch 4236, training loss: 7727.02, average training loss: 7782.89, base loss: 15319.41
[INFO 2017-07-01 23:37:49,827 main.py:52] epoch 4237, training loss: 8012.92, average training loss: 7782.98, base loss: 15319.09
[INFO 2017-07-01 23:37:54,063 main.py:52] epoch 4238, training loss: 8127.95, average training loss: 7783.79, base loss: 15319.43
[INFO 2017-07-01 23:37:58,219 main.py:52] epoch 4239, training loss: 7947.10, average training loss: 7783.72, base loss: 15319.98
[INFO 2017-07-01 23:38:02,417 main.py:52] epoch 4240, training loss: 7374.70, average training loss: 7783.15, base loss: 15319.31
[INFO 2017-07-01 23:38:06,552 main.py:52] epoch 4241, training loss: 7523.91, average training loss: 7782.55, base loss: 15318.95
[INFO 2017-07-01 23:38:10,788 main.py:52] epoch 4242, training loss: 7534.29, average training loss: 7782.36, base loss: 15318.81
[INFO 2017-07-01 23:38:15,050 main.py:52] epoch 4243, training loss: 8658.30, average training loss: 7783.63, base loss: 15319.21
[INFO 2017-07-01 23:38:19,235 main.py:52] epoch 4244, training loss: 8033.00, average training loss: 7784.23, base loss: 15319.01
[INFO 2017-07-01 23:38:23,428 main.py:52] epoch 4245, training loss: 8756.90, average training loss: 7785.49, base loss: 15319.70
[INFO 2017-07-01 23:38:27,610 main.py:52] epoch 4246, training loss: 7175.26, average training loss: 7785.07, base loss: 15319.30
[INFO 2017-07-01 23:38:31,803 main.py:52] epoch 4247, training loss: 7927.17, average training loss: 7785.95, base loss: 15319.26
[INFO 2017-07-01 23:38:36,005 main.py:52] epoch 4248, training loss: 7697.78, average training loss: 7785.69, base loss: 15319.57
[INFO 2017-07-01 23:38:40,138 main.py:52] epoch 4249, training loss: 7850.16, average training loss: 7785.38, base loss: 15319.54
[INFO 2017-07-01 23:38:44,259 main.py:52] epoch 4250, training loss: 8115.52, average training loss: 7786.07, base loss: 15319.96
[INFO 2017-07-01 23:38:48,509 main.py:52] epoch 4251, training loss: 7581.89, average training loss: 7786.09, base loss: 15319.67
[INFO 2017-07-01 23:38:52,733 main.py:52] epoch 4252, training loss: 7184.85, average training loss: 7784.89, base loss: 15318.91
[INFO 2017-07-01 23:38:56,906 main.py:52] epoch 4253, training loss: 7485.10, average training loss: 7784.26, base loss: 15318.43
[INFO 2017-07-01 23:39:01,087 main.py:52] epoch 4254, training loss: 8947.70, average training loss: 7786.07, base loss: 15320.05
[INFO 2017-07-01 23:39:05,252 main.py:52] epoch 4255, training loss: 7985.61, average training loss: 7786.99, base loss: 15320.67
[INFO 2017-07-01 23:39:09,421 main.py:52] epoch 4256, training loss: 7891.89, average training loss: 7787.88, base loss: 15321.00
[INFO 2017-07-01 23:39:13,574 main.py:52] epoch 4257, training loss: 7883.01, average training loss: 7787.43, base loss: 15321.08
[INFO 2017-07-01 23:39:17,756 main.py:52] epoch 4258, training loss: 8112.75, average training loss: 7788.24, base loss: 15321.71
