[INFO 2017-06-27 16:47:30,134 main.py:174] Namespace(batch_size=32, display=False, flow_dir='flow', flow_video_dir='flow-video', image_size=64, init_model_path='', learning_rate=0.001, method='unsupervised', motion_range=2, num_channel=3, num_inputs=3, save_dir='./model', test=False, test_dir='/home/yi/Downloads/youtube-64', test_epoch=10, test_interval=100, test_video=False, train=True, train_dir='/home/yi/Downloads/youtube-64', train_epoch=10000)
[INFO 2017-06-27 16:47:32,534 main.py:51] epoch 0, training loss: 25494.08, average training loss: 25494.08, base loss: 4190.96
[INFO 2017-06-27 16:47:32,856 main.py:51] epoch 1, training loss: 20195.86, average training loss: 22844.97, base loss: 4730.70
[INFO 2017-06-27 16:47:33,167 main.py:51] epoch 2, training loss: 16945.55, average training loss: 20878.50, base loss: 4520.42
[INFO 2017-06-27 16:47:33,472 main.py:51] epoch 3, training loss: 13997.00, average training loss: 19158.12, base loss: 4406.06
[INFO 2017-06-27 16:47:33,782 main.py:51] epoch 4, training loss: 12590.40, average training loss: 17844.58, base loss: 4405.20
[INFO 2017-06-27 16:47:34,098 main.py:51] epoch 5, training loss: 11838.96, average training loss: 16843.64, base loss: 4347.21
[INFO 2017-06-27 16:47:34,406 main.py:51] epoch 6, training loss: 10689.05, average training loss: 15964.41, base loss: 4386.60
[INFO 2017-06-27 16:47:34,717 main.py:51] epoch 7, training loss: 9491.42, average training loss: 15155.29, base loss: 4325.41
[INFO 2017-06-27 16:47:35,028 main.py:51] epoch 8, training loss: 9176.54, average training loss: 14490.99, base loss: 4355.84
[INFO 2017-06-27 16:47:35,341 main.py:51] epoch 9, training loss: 7803.10, average training loss: 13822.20, base loss: 4271.61
[INFO 2017-06-27 16:47:35,655 main.py:51] epoch 10, training loss: 8996.45, average training loss: 13383.49, base loss: 4337.92
[INFO 2017-06-27 16:47:35,973 main.py:51] epoch 11, training loss: 7755.98, average training loss: 12914.53, base loss: 4337.31
[INFO 2017-06-27 16:47:36,286 main.py:51] epoch 12, training loss: 10694.02, average training loss: 12743.72, base loss: 4627.98
[INFO 2017-06-27 16:47:36,600 main.py:51] epoch 13, training loss: 7382.15, average training loss: 12360.75, base loss: 4622.06
[INFO 2017-06-27 16:47:36,914 main.py:51] epoch 14, training loss: 6265.36, average training loss: 11954.40, base loss: 4571.07
[INFO 2017-06-27 16:47:37,225 main.py:51] epoch 15, training loss: 5742.94, average training loss: 11566.18, base loss: 4506.63
[INFO 2017-06-27 16:47:37,539 main.py:51] epoch 16, training loss: 6951.35, average training loss: 11294.72, base loss: 4523.81
[INFO 2017-06-27 16:47:37,854 main.py:51] epoch 17, training loss: 6489.50, average training loss: 11027.76, base loss: 4532.75
[INFO 2017-06-27 16:47:38,172 main.py:51] epoch 18, training loss: 6649.11, average training loss: 10797.31, base loss: 4554.36
[INFO 2017-06-27 16:47:38,484 main.py:51] epoch 19, training loss: 5750.22, average training loss: 10544.95, base loss: 4541.13
[INFO 2017-06-27 16:47:38,798 main.py:51] epoch 20, training loss: 6090.81, average training loss: 10332.85, base loss: 4551.13
[INFO 2017-06-27 16:47:39,103 main.py:51] epoch 21, training loss: 5546.59, average training loss: 10115.29, base loss: 4540.26
[INFO 2017-06-27 16:47:39,425 main.py:51] epoch 22, training loss: 5144.77, average training loss: 9899.18, base loss: 4523.19
[INFO 2017-06-27 16:47:39,733 main.py:51] epoch 23, training loss: 5469.89, average training loss: 9714.63, base loss: 4520.24
[INFO 2017-06-27 16:47:40,045 main.py:51] epoch 24, training loss: 5403.28, average training loss: 9542.18, base loss: 4519.10
[INFO 2017-06-27 16:47:40,360 main.py:51] epoch 25, training loss: 5251.24, average training loss: 9377.14, base loss: 4513.55
[INFO 2017-06-27 16:47:40,673 main.py:51] epoch 26, training loss: 12167.12, average training loss: 9480.47, base loss: 4771.76
[INFO 2017-06-27 16:47:40,985 main.py:51] epoch 27, training loss: 5217.41, average training loss: 9328.22, base loss: 4760.77
[INFO 2017-06-27 16:47:41,301 main.py:51] epoch 28, training loss: 4766.58, average training loss: 9170.92, base loss: 4734.07
[INFO 2017-06-27 16:47:41,608 main.py:51] epoch 29, training loss: 4793.46, average training loss: 9025.01, base loss: 4714.54
[INFO 2017-06-27 16:47:41,921 main.py:51] epoch 30, training loss: 5680.34, average training loss: 8917.11, base loss: 4728.02
[INFO 2017-06-27 16:47:42,224 main.py:51] epoch 31, training loss: 4582.66, average training loss: 8781.66, base loss: 4703.47
[INFO 2017-06-27 16:47:42,530 main.py:51] epoch 32, training loss: 4202.43, average training loss: 8642.90, base loss: 4672.43
[INFO 2017-06-27 16:47:42,865 main.py:51] epoch 33, training loss: 5253.61, average training loss: 8543.21, base loss: 4676.18
[INFO 2017-06-27 16:47:43,178 main.py:51] epoch 34, training loss: 4081.02, average training loss: 8415.72, base loss: 4646.53
[INFO 2017-06-27 16:47:43,484 main.py:51] epoch 35, training loss: 5380.67, average training loss: 8331.41, base loss: 4656.86
[INFO 2017-06-27 16:47:43,793 main.py:51] epoch 36, training loss: 4893.29, average training loss: 8238.49, base loss: 4655.33
[INFO 2017-06-27 16:47:44,098 main.py:51] epoch 37, training loss: 4673.19, average training loss: 8144.67, base loss: 4648.21
[INFO 2017-06-27 16:47:44,407 main.py:51] epoch 38, training loss: 3924.63, average training loss: 8036.46, base loss: 4621.90
[INFO 2017-06-27 16:47:44,713 main.py:51] epoch 39, training loss: 4671.96, average training loss: 7952.35, base loss: 4617.33
[INFO 2017-06-27 16:47:45,027 main.py:51] epoch 40, training loss: 4153.19, average training loss: 7859.69, base loss: 4600.10
[INFO 2017-06-27 16:47:45,344 main.py:51] epoch 41, training loss: 4650.10, average training loss: 7783.27, base loss: 4596.75
[INFO 2017-06-27 16:47:45,659 main.py:51] epoch 42, training loss: 4458.97, average training loss: 7705.96, base loss: 4589.32
[INFO 2017-06-27 16:47:45,986 main.py:51] epoch 43, training loss: 5027.06, average training loss: 7645.07, base loss: 4595.30
[INFO 2017-06-27 16:47:46,316 main.py:51] epoch 44, training loss: 4431.90, average training loss: 7573.67, base loss: 4587.95
[INFO 2017-06-27 16:47:46,646 main.py:51] epoch 45, training loss: 4925.43, average training loss: 7516.10, base loss: 4592.16
[INFO 2017-06-27 16:47:46,986 main.py:51] epoch 46, training loss: 4786.09, average training loss: 7458.02, base loss: 4594.06
[INFO 2017-06-27 16:47:47,327 main.py:51] epoch 47, training loss: 4333.44, average training loss: 7392.92, base loss: 4585.98
[INFO 2017-06-27 16:47:47,666 main.py:51] epoch 48, training loss: 5448.42, average training loss: 7353.24, base loss: 4602.29
[INFO 2017-06-27 16:47:48,002 main.py:51] epoch 49, training loss: 4443.25, average training loss: 7295.04, base loss: 4596.81
[INFO 2017-06-27 16:47:48,344 main.py:51] epoch 50, training loss: 4211.93, average training loss: 7234.58, base loss: 4587.04
[INFO 2017-06-27 16:47:48,694 main.py:51] epoch 51, training loss: 4061.38, average training loss: 7173.56, base loss: 4574.94
[INFO 2017-06-27 16:47:49,040 main.py:51] epoch 52, training loss: 4651.52, average training loss: 7125.97, base loss: 4575.18
[INFO 2017-06-27 16:47:49,402 main.py:51] epoch 53, training loss: 4576.05, average training loss: 7078.75, base loss: 4573.87
[INFO 2017-06-27 16:47:49,753 main.py:51] epoch 54, training loss: 3654.16, average training loss: 7016.49, base loss: 4555.83
[INFO 2017-06-27 16:47:50,107 main.py:51] epoch 55, training loss: 3563.96, average training loss: 6954.84, base loss: 4536.25
[INFO 2017-06-27 16:47:50,472 main.py:51] epoch 56, training loss: 4370.05, average training loss: 6909.49, base loss: 4532.57
[INFO 2017-06-27 16:47:50,838 main.py:51] epoch 57, training loss: 3801.46, average training loss: 6855.90, base loss: 4518.92
[INFO 2017-06-27 16:47:51,207 main.py:51] epoch 58, training loss: 4339.54, average training loss: 6813.25, base loss: 4515.31
[INFO 2017-06-27 16:47:51,578 main.py:51] epoch 59, training loss: 4917.29, average training loss: 6781.65, base loss: 4521.69
[INFO 2017-06-27 16:47:51,948 main.py:51] epoch 60, training loss: 3843.95, average training loss: 6733.49, base loss: 4509.58
[INFO 2017-06-27 16:47:52,318 main.py:51] epoch 61, training loss: 4455.33, average training loss: 6696.75, base loss: 4508.36
[INFO 2017-06-27 16:47:52,689 main.py:51] epoch 62, training loss: 4816.84, average training loss: 6666.91, base loss: 4513.56
[INFO 2017-06-27 16:47:53,059 main.py:51] epoch 63, training loss: 4377.00, average training loss: 6631.13, base loss: 4511.19
[INFO 2017-06-27 16:47:53,431 main.py:51] epoch 64, training loss: 4289.39, average training loss: 6595.10, base loss: 4507.26
[INFO 2017-06-27 16:47:53,801 main.py:51] epoch 65, training loss: 3900.79, average training loss: 6554.28, base loss: 4497.37
[INFO 2017-06-27 16:47:54,175 main.py:51] epoch 66, training loss: 5640.25, average training loss: 6540.64, base loss: 4514.90
[INFO 2017-06-27 16:47:54,544 main.py:51] epoch 67, training loss: 4642.16, average training loss: 6512.72, base loss: 4516.98
[INFO 2017-06-27 16:47:54,915 main.py:51] epoch 68, training loss: 4138.55, average training loss: 6478.31, base loss: 4511.13
[INFO 2017-06-27 16:47:55,284 main.py:51] epoch 69, training loss: 3723.65, average training loss: 6438.96, base loss: 4499.34
[INFO 2017-06-27 16:47:55,659 main.py:51] epoch 70, training loss: 5194.95, average training loss: 6421.44, base loss: 4509.81
[INFO 2017-06-27 16:47:56,028 main.py:51] epoch 71, training loss: 3376.74, average training loss: 6379.15, base loss: 4492.48
[INFO 2017-06-27 16:47:56,399 main.py:51] epoch 72, training loss: 4759.98, average training loss: 6356.97, base loss: 4496.61
[INFO 2017-06-27 16:47:56,771 main.py:51] epoch 73, training loss: 4169.97, average training loss: 6327.42, base loss: 4492.50
[INFO 2017-06-27 16:47:57,143 main.py:51] epoch 74, training loss: 5180.78, average training loss: 6312.13, base loss: 4502.63
[INFO 2017-06-27 16:47:57,512 main.py:51] epoch 75, training loss: 4425.06, average training loss: 6287.30, base loss: 4502.06
[INFO 2017-06-27 16:47:57,886 main.py:51] epoch 76, training loss: 4428.12, average training loss: 6263.15, base loss: 4501.57
[INFO 2017-06-27 16:47:58,255 main.py:51] epoch 77, training loss: 3388.76, average training loss: 6226.30, base loss: 4487.12
[INFO 2017-06-27 16:47:58,628 main.py:51] epoch 78, training loss: 4968.51, average training loss: 6210.38, base loss: 4494.12
[INFO 2017-06-27 16:47:58,998 main.py:51] epoch 79, training loss: 4371.06, average training loss: 6187.39, base loss: 4493.25
[INFO 2017-06-27 16:47:59,367 main.py:51] epoch 80, training loss: 4230.14, average training loss: 6163.22, base loss: 4489.84
[INFO 2017-06-27 16:47:59,737 main.py:51] epoch 81, training loss: 3593.24, average training loss: 6131.88, base loss: 4478.51
[INFO 2017-06-27 16:48:00,106 main.py:51] epoch 82, training loss: 4035.90, average training loss: 6106.63, base loss: 4473.69
[INFO 2017-06-27 16:48:00,481 main.py:51] epoch 83, training loss: 3895.93, average training loss: 6080.31, base loss: 4467.09
[INFO 2017-06-27 16:48:00,850 main.py:51] epoch 84, training loss: 7928.04, average training loss: 6102.05, base loss: 4508.11
[INFO 2017-06-27 16:48:01,227 main.py:51] epoch 85, training loss: 4525.69, average training loss: 6083.72, base loss: 4508.87
[INFO 2017-06-27 16:48:01,597 main.py:51] epoch 86, training loss: 4124.38, average training loss: 6061.20, base loss: 4504.50
[INFO 2017-06-27 16:48:01,974 main.py:51] epoch 87, training loss: 3942.73, average training loss: 6037.13, base loss: 4498.20
[INFO 2017-06-27 16:48:02,345 main.py:51] epoch 88, training loss: 4541.40, average training loss: 6020.32, base loss: 4499.54
[INFO 2017-06-27 16:48:02,715 main.py:51] epoch 89, training loss: 3708.75, average training loss: 5994.64, base loss: 4490.95
[INFO 2017-06-27 16:48:03,085 main.py:51] epoch 90, training loss: 4334.75, average training loss: 5976.40, base loss: 4490.11
[INFO 2017-06-27 16:48:03,457 main.py:51] epoch 91, training loss: 5380.36, average training loss: 5969.92, base loss: 4500.94
[INFO 2017-06-27 16:48:03,828 main.py:51] epoch 92, training loss: 5251.98, average training loss: 5962.20, base loss: 4510.37
[INFO 2017-06-27 16:48:04,200 main.py:51] epoch 93, training loss: 7485.89, average training loss: 5978.41, base loss: 4542.40
[INFO 2017-06-27 16:48:04,570 main.py:51] epoch 94, training loss: 7847.29, average training loss: 5998.08, base loss: 4577.34
[INFO 2017-06-27 16:48:04,939 main.py:51] epoch 95, training loss: 4481.28, average training loss: 5982.28, base loss: 4577.15
[INFO 2017-06-27 16:48:05,310 main.py:51] epoch 96, training loss: 3929.00, average training loss: 5961.11, base loss: 4570.84
[INFO 2017-06-27 16:48:05,679 main.py:51] epoch 97, training loss: 4222.43, average training loss: 5943.37, base loss: 4568.12
[INFO 2017-06-27 16:48:06,053 main.py:51] epoch 98, training loss: 4485.32, average training loss: 5928.64, base loss: 4567.94
[INFO 2017-06-27 16:48:06,422 main.py:51] epoch 99, training loss: 4219.62, average training loss: 5911.55, base loss: 4564.81
[INFO 2017-06-27 16:48:06,423 main.py:53] epoch 99, testing
[INFO 2017-06-27 16:48:08,026 main.py:105] average testing loss: 4451.87, base loss: 4527.19
[INFO 2017-06-27 16:48:08,027 main.py:106] improve_loss: 75.33, improve_percent: 0.02
[INFO 2017-06-27 16:48:08,027 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 16:48:08,039 main.py:76] current best improved percent: 0.02
[INFO 2017-06-27 16:48:08,410 main.py:51] epoch 100, training loss: 4679.76, average training loss: 5899.36, base loss: 4566.93
[INFO 2017-06-27 16:48:08,778 main.py:51] epoch 101, training loss: 4102.79, average training loss: 5881.74, base loss: 4563.21
[INFO 2017-06-27 16:48:09,155 main.py:51] epoch 102, training loss: 4381.09, average training loss: 5867.17, base loss: 4562.33
[INFO 2017-06-27 16:48:09,535 main.py:51] epoch 103, training loss: 4524.08, average training loss: 5854.26, base loss: 4562.89
[INFO 2017-06-27 16:48:09,908 main.py:51] epoch 104, training loss: 4085.61, average training loss: 5837.41, base loss: 4559.08
[INFO 2017-06-27 16:48:10,276 main.py:51] epoch 105, training loss: 5134.25, average training loss: 5830.78, base loss: 4565.71
[INFO 2017-06-27 16:48:10,650 main.py:51] epoch 106, training loss: 4670.16, average training loss: 5819.93, base loss: 4567.58
[INFO 2017-06-27 16:48:11,026 main.py:51] epoch 107, training loss: 4770.45, average training loss: 5810.22, base loss: 4570.32
[INFO 2017-06-27 16:48:11,401 main.py:51] epoch 108, training loss: 4096.02, average training loss: 5794.49, base loss: 4566.77
[INFO 2017-06-27 16:48:11,770 main.py:51] epoch 109, training loss: 3609.68, average training loss: 5774.63, base loss: 4558.71
[INFO 2017-06-27 16:48:12,150 main.py:51] epoch 110, training loss: 3758.36, average training loss: 5756.46, base loss: 4552.04
[INFO 2017-06-27 16:48:12,521 main.py:51] epoch 111, training loss: 7877.55, average training loss: 5775.40, base loss: 4582.51
[INFO 2017-06-27 16:48:12,897 main.py:51] epoch 112, training loss: 4103.42, average training loss: 5760.60, base loss: 4578.99
[INFO 2017-06-27 16:48:13,268 main.py:51] epoch 113, training loss: 3784.69, average training loss: 5743.27, base loss: 4572.66
[INFO 2017-06-27 16:48:13,645 main.py:51] epoch 114, training loss: 3621.28, average training loss: 5724.82, base loss: 4564.94
[INFO 2017-06-27 16:48:14,018 main.py:51] epoch 115, training loss: 4666.33, average training loss: 5715.70, base loss: 4566.71
[INFO 2017-06-27 16:48:14,394 main.py:51] epoch 116, training loss: 4187.46, average training loss: 5702.63, base loss: 4564.18
[INFO 2017-06-27 16:48:14,771 main.py:51] epoch 117, training loss: 4411.39, average training loss: 5691.69, base loss: 4563.74
[INFO 2017-06-27 16:48:15,144 main.py:51] epoch 118, training loss: 4738.27, average training loss: 5683.68, base loss: 4566.09
[INFO 2017-06-27 16:48:15,523 main.py:51] epoch 119, training loss: 4428.80, average training loss: 5673.22, base loss: 4566.23
[INFO 2017-06-27 16:48:15,900 main.py:51] epoch 120, training loss: 4183.31, average training loss: 5660.91, base loss: 4563.99
[INFO 2017-06-27 16:48:16,276 main.py:51] epoch 121, training loss: 3789.95, average training loss: 5645.57, base loss: 4558.05
[INFO 2017-06-27 16:48:16,655 main.py:51] epoch 122, training loss: 4096.45, average training loss: 5632.98, base loss: 4555.23
[INFO 2017-06-27 16:48:17,030 main.py:51] epoch 123, training loss: 4587.69, average training loss: 5624.55, base loss: 4556.73
[INFO 2017-06-27 16:48:17,405 main.py:51] epoch 124, training loss: 4364.51, average training loss: 5614.47, base loss: 4555.94
[INFO 2017-06-27 16:48:17,781 main.py:51] epoch 125, training loss: 4649.72, average training loss: 5606.81, base loss: 4557.80
[INFO 2017-06-27 16:48:18,156 main.py:51] epoch 126, training loss: 3290.51, average training loss: 5588.57, base loss: 4547.89
[INFO 2017-06-27 16:48:18,531 main.py:51] epoch 127, training loss: 3909.21, average training loss: 5575.45, base loss: 4543.63
[INFO 2017-06-27 16:48:18,906 main.py:51] epoch 128, training loss: 3959.53, average training loss: 5562.93, base loss: 4539.54
[INFO 2017-06-27 16:48:19,282 main.py:51] epoch 129, training loss: 4286.75, average training loss: 5553.11, base loss: 4538.44
[INFO 2017-06-27 16:48:19,658 main.py:51] epoch 130, training loss: 4799.34, average training loss: 5547.36, base loss: 4541.82
[INFO 2017-06-27 16:48:20,039 main.py:51] epoch 131, training loss: 4687.94, average training loss: 5540.84, base loss: 4544.16
[INFO 2017-06-27 16:48:20,415 main.py:51] epoch 132, training loss: 4045.36, average training loss: 5529.60, base loss: 4541.22
[INFO 2017-06-27 16:48:20,791 main.py:51] epoch 133, training loss: 5042.47, average training loss: 5525.97, base loss: 4546.20
[INFO 2017-06-27 16:48:21,167 main.py:51] epoch 134, training loss: 4026.55, average training loss: 5514.86, base loss: 4543.16
[INFO 2017-06-27 16:48:21,547 main.py:51] epoch 135, training loss: 4529.69, average training loss: 5507.61, base loss: 4544.40
[INFO 2017-06-27 16:48:21,923 main.py:51] epoch 136, training loss: 4408.81, average training loss: 5499.59, base loss: 4543.47
[INFO 2017-06-27 16:48:22,301 main.py:51] epoch 137, training loss: 4418.19, average training loss: 5491.76, base loss: 4543.14
[INFO 2017-06-27 16:48:22,676 main.py:51] epoch 138, training loss: 3828.40, average training loss: 5479.79, base loss: 4538.32
[INFO 2017-06-27 16:48:23,056 main.py:51] epoch 139, training loss: 4444.05, average training loss: 5472.39, base loss: 4538.54
[INFO 2017-06-27 16:48:23,436 main.py:51] epoch 140, training loss: 3921.18, average training loss: 5461.39, base loss: 4534.64
[INFO 2017-06-27 16:48:23,811 main.py:51] epoch 141, training loss: 4741.68, average training loss: 5456.32, base loss: 4537.25
[INFO 2017-06-27 16:48:24,190 main.py:51] epoch 142, training loss: 5205.87, average training loss: 5454.57, base loss: 4542.90
[INFO 2017-06-27 16:48:24,566 main.py:51] epoch 143, training loss: 4326.88, average training loss: 5446.74, base loss: 4542.12
[INFO 2017-06-27 16:48:24,941 main.py:51] epoch 144, training loss: 3955.35, average training loss: 5436.45, base loss: 4538.58
[INFO 2017-06-27 16:48:25,316 main.py:51] epoch 145, training loss: 4161.88, average training loss: 5427.72, base loss: 4536.62
[INFO 2017-06-27 16:48:25,697 main.py:51] epoch 146, training loss: 4663.50, average training loss: 5422.53, base loss: 4538.71
[INFO 2017-06-27 16:48:26,078 main.py:51] epoch 147, training loss: 4228.36, average training loss: 5414.46, base loss: 4537.80
[INFO 2017-06-27 16:48:26,453 main.py:51] epoch 148, training loss: 3908.20, average training loss: 5404.35, base loss: 4534.36
[INFO 2017-06-27 16:48:26,830 main.py:51] epoch 149, training loss: 3942.41, average training loss: 5394.60, base loss: 4531.32
[INFO 2017-06-27 16:48:27,215 main.py:51] epoch 150, training loss: 4685.87, average training loss: 5389.91, base loss: 4533.35
[INFO 2017-06-27 16:48:27,597 main.py:51] epoch 151, training loss: 3766.60, average training loss: 5379.23, base loss: 4528.79
[INFO 2017-06-27 16:48:27,978 main.py:51] epoch 152, training loss: 4091.46, average training loss: 5370.81, base loss: 4526.70
[INFO 2017-06-27 16:48:28,356 main.py:51] epoch 153, training loss: 4391.93, average training loss: 5364.46, base loss: 4527.03
[INFO 2017-06-27 16:48:28,736 main.py:51] epoch 154, training loss: 4018.63, average training loss: 5355.77, base loss: 4524.74
[INFO 2017-06-27 16:48:29,116 main.py:51] epoch 155, training loss: 4646.32, average training loss: 5351.23, base loss: 4526.47
[INFO 2017-06-27 16:48:29,490 main.py:51] epoch 156, training loss: 4412.89, average training loss: 5345.25, base loss: 4526.99
[INFO 2017-06-27 16:48:29,870 main.py:51] epoch 157, training loss: 4648.72, average training loss: 5340.84, base loss: 4528.88
[INFO 2017-06-27 16:48:30,249 main.py:51] epoch 158, training loss: 4641.40, average training loss: 5336.44, base loss: 4530.44
[INFO 2017-06-27 16:48:30,624 main.py:51] epoch 159, training loss: 4101.88, average training loss: 5328.73, base loss: 4528.75
[INFO 2017-06-27 16:48:31,004 main.py:51] epoch 160, training loss: 4104.32, average training loss: 5321.12, base loss: 4527.14
[INFO 2017-06-27 16:48:31,388 main.py:51] epoch 161, training loss: 4515.33, average training loss: 5316.15, base loss: 4528.25
[INFO 2017-06-27 16:48:31,764 main.py:51] epoch 162, training loss: 4516.89, average training loss: 5311.24, base loss: 4529.40
[INFO 2017-06-27 16:48:32,144 main.py:51] epoch 163, training loss: 4148.37, average training loss: 5304.15, base loss: 4528.22
[INFO 2017-06-27 16:48:32,518 main.py:51] epoch 164, training loss: 4514.61, average training loss: 5299.37, base loss: 4529.31
[INFO 2017-06-27 16:48:32,894 main.py:51] epoch 165, training loss: 4136.55, average training loss: 5292.36, base loss: 4528.13
[INFO 2017-06-27 16:48:33,273 main.py:51] epoch 166, training loss: 4307.91, average training loss: 5286.47, base loss: 4528.21
[INFO 2017-06-27 16:48:33,652 main.py:51] epoch 167, training loss: 4557.70, average training loss: 5282.13, base loss: 4529.22
[INFO 2017-06-27 16:48:34,027 main.py:51] epoch 168, training loss: 4084.20, average training loss: 5275.04, base loss: 4527.67
[INFO 2017-06-27 16:48:34,407 main.py:51] epoch 169, training loss: 3475.39, average training loss: 5264.45, base loss: 4522.31
[INFO 2017-06-27 16:48:34,788 main.py:51] epoch 170, training loss: 4003.27, average training loss: 5257.08, base loss: 4520.06
[INFO 2017-06-27 16:48:35,167 main.py:51] epoch 171, training loss: 4451.42, average training loss: 5252.40, base loss: 4520.85
[INFO 2017-06-27 16:48:35,542 main.py:51] epoch 172, training loss: 4300.39, average training loss: 5246.89, base loss: 4520.85
[INFO 2017-06-27 16:48:35,916 main.py:51] epoch 173, training loss: 4400.80, average training loss: 5242.03, base loss: 4521.54
[INFO 2017-06-27 16:48:36,296 main.py:51] epoch 174, training loss: 7823.66, average training loss: 5256.78, base loss: 4541.49
[INFO 2017-06-27 16:48:36,677 main.py:51] epoch 175, training loss: 3474.66, average training loss: 5246.66, base loss: 4536.03
[INFO 2017-06-27 16:48:37,053 main.py:51] epoch 176, training loss: 5294.05, average training loss: 5246.92, base loss: 4541.95
[INFO 2017-06-27 16:48:37,428 main.py:51] epoch 177, training loss: 4193.71, average training loss: 5241.01, base loss: 4541.27
[INFO 2017-06-27 16:48:37,801 main.py:51] epoch 178, training loss: 3980.75, average training loss: 5233.97, base loss: 4538.95
[INFO 2017-06-27 16:48:38,177 main.py:51] epoch 179, training loss: 3446.96, average training loss: 5224.04, base loss: 4533.51
[INFO 2017-06-27 16:48:38,559 main.py:51] epoch 180, training loss: 4141.88, average training loss: 5218.06, base loss: 4532.50
[INFO 2017-06-27 16:48:38,940 main.py:51] epoch 181, training loss: 3931.87, average training loss: 5210.99, base loss: 4529.84
[INFO 2017-06-27 16:48:39,314 main.py:51] epoch 182, training loss: 3838.98, average training loss: 5203.50, base loss: 4526.92
[INFO 2017-06-27 16:48:39,695 main.py:51] epoch 183, training loss: 3727.43, average training loss: 5195.47, base loss: 4523.17
[INFO 2017-06-27 16:48:40,070 main.py:51] epoch 184, training loss: 5290.17, average training loss: 5195.99, base loss: 4528.72
[INFO 2017-06-27 16:48:40,447 main.py:51] epoch 185, training loss: 4447.92, average training loss: 5191.96, base loss: 4529.48
[INFO 2017-06-27 16:48:40,827 main.py:51] epoch 186, training loss: 4835.32, average training loss: 5190.06, base loss: 4532.52
[INFO 2017-06-27 16:48:41,206 main.py:51] epoch 187, training loss: 4291.18, average training loss: 5185.27, base loss: 4531.92
[INFO 2017-06-27 16:48:41,582 main.py:51] epoch 188, training loss: 3892.34, average training loss: 5178.43, base loss: 4529.57
[INFO 2017-06-27 16:48:41,956 main.py:51] epoch 189, training loss: 4090.17, average training loss: 5172.71, base loss: 4528.64
[INFO 2017-06-27 16:48:42,335 main.py:51] epoch 190, training loss: 3726.91, average training loss: 5165.14, base loss: 4525.19
[INFO 2017-06-27 16:48:42,710 main.py:51] epoch 191, training loss: 4149.37, average training loss: 5159.85, base loss: 4524.13
[INFO 2017-06-27 16:48:43,089 main.py:51] epoch 192, training loss: 4510.49, average training loss: 5156.48, base loss: 4525.27
[INFO 2017-06-27 16:48:43,469 main.py:51] epoch 193, training loss: 4001.23, average training loss: 5150.53, base loss: 4523.55
[INFO 2017-06-27 16:48:43,844 main.py:51] epoch 194, training loss: 4074.25, average training loss: 5145.01, base loss: 4522.15
[INFO 2017-06-27 16:48:44,218 main.py:51] epoch 195, training loss: 4009.19, average training loss: 5139.21, base loss: 4520.70
[INFO 2017-06-27 16:48:44,600 main.py:51] epoch 196, training loss: 3818.40, average training loss: 5132.51, base loss: 4518.15
[INFO 2017-06-27 16:48:44,980 main.py:51] epoch 197, training loss: 3895.53, average training loss: 5126.26, base loss: 4515.86
[INFO 2017-06-27 16:48:45,359 main.py:51] epoch 198, training loss: 4169.08, average training loss: 5121.45, base loss: 4515.42
[INFO 2017-06-27 16:48:45,740 main.py:51] epoch 199, training loss: 4036.07, average training loss: 5116.02, base loss: 4514.24
[INFO 2017-06-27 16:48:45,740 main.py:53] epoch 199, testing
[INFO 2017-06-27 16:48:47,326 main.py:105] average testing loss: 4494.32, base loss: 4752.83
[INFO 2017-06-27 16:48:47,326 main.py:106] improve_loss: 258.52, improve_percent: 0.05
[INFO 2017-06-27 16:48:47,327 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 16:48:47,339 main.py:76] current best improved percent: 0.05
[INFO 2017-06-27 16:48:47,714 main.py:51] epoch 200, training loss: 4349.41, average training loss: 5112.21, base loss: 4514.48
[INFO 2017-06-27 16:48:48,090 main.py:51] epoch 201, training loss: 4824.91, average training loss: 5110.79, base loss: 4517.48
[INFO 2017-06-27 16:48:48,469 main.py:51] epoch 202, training loss: 4096.83, average training loss: 5105.79, base loss: 4516.09
[INFO 2017-06-27 16:48:48,845 main.py:51] epoch 203, training loss: 4787.87, average training loss: 5104.23, base loss: 4518.71
[INFO 2017-06-27 16:48:49,224 main.py:51] epoch 204, training loss: 4366.53, average training loss: 5100.64, base loss: 4519.06
[INFO 2017-06-27 16:48:49,605 main.py:51] epoch 205, training loss: 4265.61, average training loss: 5096.58, base loss: 4519.07
[INFO 2017-06-27 16:48:49,980 main.py:51] epoch 206, training loss: 4741.60, average training loss: 5094.87, base loss: 4521.84
[INFO 2017-06-27 16:48:50,359 main.py:51] epoch 207, training loss: 4238.02, average training loss: 5090.75, base loss: 4521.67
[INFO 2017-06-27 16:48:50,733 main.py:51] epoch 208, training loss: 4157.51, average training loss: 5086.28, base loss: 4521.11
[INFO 2017-06-27 16:48:51,107 main.py:51] epoch 209, training loss: 3329.51, average training loss: 5077.92, base loss: 4515.68
[INFO 2017-06-27 16:48:51,487 main.py:51] epoch 210, training loss: 4376.30, average training loss: 5074.59, base loss: 4516.13
[INFO 2017-06-27 16:48:51,863 main.py:51] epoch 211, training loss: 3989.96, average training loss: 5069.48, base loss: 4514.63
[INFO 2017-06-27 16:48:52,237 main.py:51] epoch 212, training loss: 4149.88, average training loss: 5065.16, base loss: 4514.11
[INFO 2017-06-27 16:48:52,613 main.py:51] epoch 213, training loss: 5329.40, average training loss: 5066.39, base loss: 4519.48
[INFO 2017-06-27 16:48:52,989 main.py:51] epoch 214, training loss: 4197.66, average training loss: 5062.35, base loss: 4519.30
[INFO 2017-06-27 16:48:53,365 main.py:51] epoch 215, training loss: 3735.37, average training loss: 5056.21, base loss: 4516.35
[INFO 2017-06-27 16:48:53,743 main.py:51] epoch 216, training loss: 4321.00, average training loss: 5052.82, base loss: 4516.68
[INFO 2017-06-27 16:48:54,119 main.py:51] epoch 217, training loss: 4254.55, average training loss: 5049.16, base loss: 4516.70
[INFO 2017-06-27 16:48:54,494 main.py:51] epoch 218, training loss: 3696.88, average training loss: 5042.98, base loss: 4514.08
[INFO 2017-06-27 16:48:54,869 main.py:51] epoch 219, training loss: 4195.03, average training loss: 5039.13, base loss: 4513.48
[INFO 2017-06-27 16:48:55,250 main.py:51] epoch 220, training loss: 7580.86, average training loss: 5050.63, base loss: 4528.59
[INFO 2017-06-27 16:48:55,624 main.py:51] epoch 221, training loss: 3799.79, average training loss: 5045.00, base loss: 4526.34
[INFO 2017-06-27 16:48:56,003 main.py:51] epoch 222, training loss: 7456.35, average training loss: 5055.81, base loss: 4540.63
[INFO 2017-06-27 16:48:56,381 main.py:51] epoch 223, training loss: 3458.22, average training loss: 5048.68, base loss: 4536.80
[INFO 2017-06-27 16:48:56,756 main.py:51] epoch 224, training loss: 4014.21, average training loss: 5044.08, base loss: 4535.94
[INFO 2017-06-27 16:48:57,137 main.py:51] epoch 225, training loss: 3570.59, average training loss: 5037.56, base loss: 4532.37
[INFO 2017-06-27 16:48:57,518 main.py:51] epoch 226, training loss: 3825.58, average training loss: 5032.22, base loss: 4530.39
[INFO 2017-06-27 16:48:57,892 main.py:51] epoch 227, training loss: 3998.63, average training loss: 5027.69, base loss: 4529.27
[INFO 2017-06-27 16:48:58,267 main.py:51] epoch 228, training loss: 3572.38, average training loss: 5021.33, base loss: 4525.53
[INFO 2017-06-27 16:48:58,641 main.py:51] epoch 229, training loss: 4841.23, average training loss: 5020.55, base loss: 4528.05
[INFO 2017-06-27 16:48:59,016 main.py:51] epoch 230, training loss: 4298.93, average training loss: 5017.43, base loss: 4527.78
[INFO 2017-06-27 16:48:59,390 main.py:51] epoch 231, training loss: 4107.25, average training loss: 5013.50, base loss: 4527.21
[INFO 2017-06-27 16:48:59,762 main.py:51] epoch 232, training loss: 3869.82, average training loss: 5008.59, base loss: 4525.48
[INFO 2017-06-27 16:49:00,136 main.py:51] epoch 233, training loss: 4188.27, average training loss: 5005.09, base loss: 4525.02
[INFO 2017-06-27 16:49:00,506 main.py:51] epoch 234, training loss: 3689.04, average training loss: 4999.49, base loss: 4522.79
[INFO 2017-06-27 16:49:00,883 main.py:51] epoch 235, training loss: 4338.48, average training loss: 4996.69, base loss: 4523.49
[INFO 2017-06-27 16:49:01,258 main.py:51] epoch 236, training loss: 4024.94, average training loss: 4992.59, base loss: 4522.31
[INFO 2017-06-27 16:49:01,628 main.py:51] epoch 237, training loss: 4374.25, average training loss: 4989.99, base loss: 4523.54
[INFO 2017-06-27 16:49:02,010 main.py:51] epoch 238, training loss: 7114.52, average training loss: 4998.88, base loss: 4535.32
[INFO 2017-06-27 16:49:02,388 main.py:51] epoch 239, training loss: 4639.43, average training loss: 4997.38, base loss: 4537.32
[INFO 2017-06-27 16:49:02,762 main.py:51] epoch 240, training loss: 4318.05, average training loss: 4994.56, base loss: 4537.70
[INFO 2017-06-27 16:49:03,138 main.py:51] epoch 241, training loss: 4201.56, average training loss: 4991.28, base loss: 4537.35
[INFO 2017-06-27 16:49:03,513 main.py:51] epoch 242, training loss: 3968.02, average training loss: 4987.07, base loss: 4536.10
[INFO 2017-06-27 16:49:03,886 main.py:51] epoch 243, training loss: 4039.90, average training loss: 4983.19, base loss: 4535.45
[INFO 2017-06-27 16:49:04,265 main.py:51] epoch 244, training loss: 3855.03, average training loss: 4978.59, base loss: 4533.79
[INFO 2017-06-27 16:49:04,638 main.py:51] epoch 245, training loss: 4164.10, average training loss: 4975.28, base loss: 4533.71
[INFO 2017-06-27 16:49:05,014 main.py:51] epoch 246, training loss: 3738.44, average training loss: 4970.27, base loss: 4531.69
[INFO 2017-06-27 16:49:05,388 main.py:51] epoch 247, training loss: 3639.52, average training loss: 4964.90, base loss: 4528.72
[INFO 2017-06-27 16:49:05,763 main.py:51] epoch 248, training loss: 3993.87, average training loss: 4961.00, base loss: 4527.58
[INFO 2017-06-27 16:49:06,136 main.py:51] epoch 249, training loss: 4185.81, average training loss: 4957.90, base loss: 4527.66
[INFO 2017-06-27 16:49:06,517 main.py:51] epoch 250, training loss: 4650.26, average training loss: 4956.68, base loss: 4528.96
[INFO 2017-06-27 16:49:06,893 main.py:51] epoch 251, training loss: 4220.24, average training loss: 4953.75, base loss: 4529.36
[INFO 2017-06-27 16:49:07,264 main.py:51] epoch 252, training loss: 3967.60, average training loss: 4949.86, base loss: 4528.37
[INFO 2017-06-27 16:49:07,637 main.py:51] epoch 253, training loss: 3852.81, average training loss: 4945.54, base loss: 4526.56
[INFO 2017-06-27 16:49:08,010 main.py:51] epoch 254, training loss: 3441.80, average training loss: 4939.64, base loss: 4522.66
[INFO 2017-06-27 16:49:08,383 main.py:51] epoch 255, training loss: 3813.40, average training loss: 4935.24, base loss: 4521.14
[INFO 2017-06-27 16:49:08,753 main.py:51] epoch 256, training loss: 7818.75, average training loss: 4946.46, base loss: 4535.33
[INFO 2017-06-27 16:49:09,125 main.py:51] epoch 257, training loss: 4260.54, average training loss: 4943.80, base loss: 4535.16
[INFO 2017-06-27 16:49:09,500 main.py:51] epoch 258, training loss: 3981.09, average training loss: 4940.09, base loss: 4533.65
[INFO 2017-06-27 16:49:09,875 main.py:51] epoch 259, training loss: 3684.53, average training loss: 4935.26, base loss: 4531.45
[INFO 2017-06-27 16:49:10,250 main.py:51] epoch 260, training loss: 4102.80, average training loss: 4932.07, base loss: 4531.07
[INFO 2017-06-27 16:49:10,628 main.py:51] epoch 261, training loss: 4753.77, average training loss: 4931.39, base loss: 4533.73
[INFO 2017-06-27 16:49:11,003 main.py:51] epoch 262, training loss: 4005.68, average training loss: 4927.87, base loss: 4532.74
[INFO 2017-06-27 16:49:11,380 main.py:51] epoch 263, training loss: 3789.62, average training loss: 4923.55, base loss: 4530.90
[INFO 2017-06-27 16:49:11,755 main.py:51] epoch 264, training loss: 4284.85, average training loss: 4921.14, base loss: 4531.52
[INFO 2017-06-27 16:49:12,126 main.py:51] epoch 265, training loss: 4484.27, average training loss: 4919.50, base loss: 4532.49
[INFO 2017-06-27 16:49:12,502 main.py:51] epoch 266, training loss: 4717.16, average training loss: 4918.74, base loss: 4535.12
[INFO 2017-06-27 16:49:12,883 main.py:51] epoch 267, training loss: 4017.72, average training loss: 4915.38, base loss: 4534.44
[INFO 2017-06-27 16:49:13,254 main.py:51] epoch 268, training loss: 3592.52, average training loss: 4910.46, base loss: 4531.93
[INFO 2017-06-27 16:49:13,628 main.py:51] epoch 269, training loss: 3711.47, average training loss: 4906.02, base loss: 4529.98
[INFO 2017-06-27 16:49:14,006 main.py:51] epoch 270, training loss: 3647.32, average training loss: 4901.38, base loss: 4527.71
[INFO 2017-06-27 16:49:14,380 main.py:51] epoch 271, training loss: 4061.79, average training loss: 4898.29, base loss: 4527.19
[INFO 2017-06-27 16:49:14,750 main.py:51] epoch 272, training loss: 4513.38, average training loss: 4896.88, base loss: 4528.82
[INFO 2017-06-27 16:49:15,120 main.py:51] epoch 273, training loss: 3939.56, average training loss: 4893.39, base loss: 4527.87
[INFO 2017-06-27 16:49:15,489 main.py:51] epoch 274, training loss: 4062.73, average training loss: 4890.37, base loss: 4527.72
[INFO 2017-06-27 16:49:15,860 main.py:51] epoch 275, training loss: 3786.33, average training loss: 4886.37, base loss: 4526.16
[INFO 2017-06-27 16:49:16,234 main.py:51] epoch 276, training loss: 4592.55, average training loss: 4885.31, base loss: 4527.93
[INFO 2017-06-27 16:49:16,604 main.py:51] epoch 277, training loss: 3896.62, average training loss: 4881.75, base loss: 4526.84
[INFO 2017-06-27 16:49:16,974 main.py:51] epoch 278, training loss: 4334.56, average training loss: 4879.79, base loss: 4527.57
[INFO 2017-06-27 16:49:17,348 main.py:51] epoch 279, training loss: 5368.40, average training loss: 4881.53, base loss: 4532.79
[INFO 2017-06-27 16:49:17,717 main.py:51] epoch 280, training loss: 3924.77, average training loss: 4878.13, base loss: 4531.74
[INFO 2017-06-27 16:49:18,091 main.py:51] epoch 281, training loss: 3992.99, average training loss: 4874.99, base loss: 4531.26
[INFO 2017-06-27 16:49:18,461 main.py:51] epoch 282, training loss: 3370.22, average training loss: 4869.67, base loss: 4528.65
[INFO 2017-06-27 16:49:18,837 main.py:51] epoch 283, training loss: 3855.82, average training loss: 4866.10, base loss: 4527.09
[INFO 2017-06-27 16:49:19,208 main.py:51] epoch 284, training loss: 4541.29, average training loss: 4864.96, base loss: 4528.91
[INFO 2017-06-27 16:49:19,585 main.py:51] epoch 285, training loss: 3804.17, average training loss: 4861.26, base loss: 4527.42
[INFO 2017-06-27 16:49:19,960 main.py:51] epoch 286, training loss: 4044.51, average training loss: 4858.41, base loss: 4527.00
[INFO 2017-06-27 16:49:20,336 main.py:51] epoch 287, training loss: 4644.96, average training loss: 4857.67, base loss: 4528.58
[INFO 2017-06-27 16:49:20,714 main.py:51] epoch 288, training loss: 4157.79, average training loss: 4855.25, base loss: 4528.31
[INFO 2017-06-27 16:49:21,090 main.py:51] epoch 289, training loss: 4210.75, average training loss: 4853.02, base loss: 4528.72
[INFO 2017-06-27 16:49:21,464 main.py:51] epoch 290, training loss: 3891.02, average training loss: 4849.72, base loss: 4527.88
[INFO 2017-06-27 16:49:21,837 main.py:51] epoch 291, training loss: 4475.06, average training loss: 4848.44, base loss: 4529.49
[INFO 2017-06-27 16:49:22,209 main.py:51] epoch 292, training loss: 3981.48, average training loss: 4845.48, base loss: 4529.21
[INFO 2017-06-27 16:49:22,582 main.py:51] epoch 293, training loss: 3968.08, average training loss: 4842.49, base loss: 4528.82
[INFO 2017-06-27 16:49:22,957 main.py:51] epoch 294, training loss: 4274.59, average training loss: 4840.57, base loss: 4529.48
[INFO 2017-06-27 16:49:23,342 main.py:51] epoch 295, training loss: 4012.76, average training loss: 4837.77, base loss: 4529.38
[INFO 2017-06-27 16:49:23,723 main.py:51] epoch 296, training loss: 4368.83, average training loss: 4836.19, base loss: 4530.93
[INFO 2017-06-27 16:49:24,103 main.py:51] epoch 297, training loss: 4114.04, average training loss: 4833.77, base loss: 4531.05
[INFO 2017-06-27 16:49:24,478 main.py:51] epoch 298, training loss: 4065.28, average training loss: 4831.20, base loss: 4531.05
[INFO 2017-06-27 16:49:24,856 main.py:51] epoch 299, training loss: 4075.62, average training loss: 4828.68, base loss: 4531.10
[INFO 2017-06-27 16:49:24,856 main.py:53] epoch 299, testing
[INFO 2017-06-27 16:49:26,475 main.py:105] average testing loss: 4090.63, base loss: 4524.38
[INFO 2017-06-27 16:49:26,475 main.py:106] improve_loss: 433.75, improve_percent: 0.10
[INFO 2017-06-27 16:49:26,476 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 16:49:26,488 main.py:76] current best improved percent: 0.10
[INFO 2017-06-27 16:49:26,860 main.py:51] epoch 300, training loss: 3892.42, average training loss: 4825.57, base loss: 4530.59
[INFO 2017-06-27 16:49:27,234 main.py:51] epoch 301, training loss: 3938.96, average training loss: 4822.63, base loss: 4530.10
[INFO 2017-06-27 16:49:27,609 main.py:51] epoch 302, training loss: 4076.70, average training loss: 4820.17, base loss: 4530.18
[INFO 2017-06-27 16:49:27,980 main.py:51] epoch 303, training loss: 4483.46, average training loss: 4819.06, base loss: 4531.77
[INFO 2017-06-27 16:49:28,354 main.py:51] epoch 304, training loss: 4449.19, average training loss: 4817.85, base loss: 4532.87
[INFO 2017-06-27 16:49:28,726 main.py:51] epoch 305, training loss: 4868.50, average training loss: 4818.02, base loss: 4535.47
[INFO 2017-06-27 16:49:29,100 main.py:51] epoch 306, training loss: 4036.77, average training loss: 4815.47, base loss: 4535.67
[INFO 2017-06-27 16:49:29,471 main.py:51] epoch 307, training loss: 3641.12, average training loss: 4811.66, base loss: 4533.33
[INFO 2017-06-27 16:49:29,851 main.py:51] epoch 308, training loss: 4275.90, average training loss: 4809.92, base loss: 4534.59
[INFO 2017-06-27 16:49:30,229 main.py:51] epoch 309, training loss: 4771.22, average training loss: 4809.80, base loss: 4537.50
[INFO 2017-06-27 16:49:30,611 main.py:51] epoch 310, training loss: 7634.63, average training loss: 4818.88, base loss: 4549.16
[INFO 2017-06-27 16:49:30,991 main.py:51] epoch 311, training loss: 4096.58, average training loss: 4816.57, base loss: 4549.54
[INFO 2017-06-27 16:49:31,371 main.py:51] epoch 312, training loss: 3504.28, average training loss: 4812.38, base loss: 4547.14
[INFO 2017-06-27 16:49:31,750 main.py:51] epoch 313, training loss: 3887.95, average training loss: 4809.43, base loss: 4546.65
[INFO 2017-06-27 16:49:32,125 main.py:51] epoch 314, training loss: 3339.46, average training loss: 4804.76, base loss: 4543.71
[INFO 2017-06-27 16:49:32,497 main.py:51] epoch 315, training loss: 4600.64, average training loss: 4804.12, base loss: 4545.64
[INFO 2017-06-27 16:49:32,872 main.py:51] epoch 316, training loss: 4535.64, average training loss: 4803.27, base loss: 4547.57
[INFO 2017-06-27 16:49:33,246 main.py:51] epoch 317, training loss: 7600.93, average training loss: 4812.07, base loss: 4558.81
[INFO 2017-06-27 16:49:33,620 main.py:51] epoch 318, training loss: 3577.48, average training loss: 4808.20, base loss: 4557.14
[INFO 2017-06-27 16:49:33,996 main.py:51] epoch 319, training loss: 4607.80, average training loss: 4807.57, base loss: 4558.95
[INFO 2017-06-27 16:49:34,370 main.py:51] epoch 320, training loss: 3821.77, average training loss: 4804.50, base loss: 4557.72
[INFO 2017-06-27 16:49:34,744 main.py:51] epoch 321, training loss: 4251.08, average training loss: 4802.78, base loss: 4558.67
[INFO 2017-06-27 16:49:35,122 main.py:51] epoch 322, training loss: 3793.52, average training loss: 4799.66, base loss: 4557.34
[INFO 2017-06-27 16:49:35,507 main.py:51] epoch 323, training loss: 4307.68, average training loss: 4798.14, base loss: 4558.34
[INFO 2017-06-27 16:49:35,882 main.py:51] epoch 324, training loss: 3808.00, average training loss: 4795.09, base loss: 4557.35
[INFO 2017-06-27 16:49:36,256 main.py:51] epoch 325, training loss: 3987.16, average training loss: 4792.62, base loss: 4556.84
[INFO 2017-06-27 16:49:36,629 main.py:51] epoch 326, training loss: 3965.77, average training loss: 4790.09, base loss: 4556.72
[INFO 2017-06-27 16:49:37,001 main.py:51] epoch 327, training loss: 3865.96, average training loss: 4787.27, base loss: 4555.87
[INFO 2017-06-27 16:49:37,379 main.py:51] epoch 328, training loss: 3933.97, average training loss: 4784.68, base loss: 4554.97
[INFO 2017-06-27 16:49:37,753 main.py:51] epoch 329, training loss: 7519.58, average training loss: 4792.96, base loss: 4565.30
[INFO 2017-06-27 16:49:38,128 main.py:51] epoch 330, training loss: 4330.34, average training loss: 4791.57, base loss: 4566.00
[INFO 2017-06-27 16:49:38,498 main.py:51] epoch 331, training loss: 3686.15, average training loss: 4788.24, base loss: 4564.38
[INFO 2017-06-27 16:49:38,873 main.py:51] epoch 332, training loss: 4517.42, average training loss: 4787.42, base loss: 4565.89
[INFO 2017-06-27 16:49:39,246 main.py:51] epoch 333, training loss: 3998.41, average training loss: 4785.06, base loss: 4565.64
[INFO 2017-06-27 16:49:39,625 main.py:51] epoch 334, training loss: 4197.21, average training loss: 4783.31, base loss: 4566.45
[INFO 2017-06-27 16:49:40,001 main.py:51] epoch 335, training loss: 3893.49, average training loss: 4780.66, base loss: 4566.13
[INFO 2017-06-27 16:49:40,377 main.py:51] epoch 336, training loss: 3290.03, average training loss: 4776.23, base loss: 4563.42
[INFO 2017-06-27 16:49:40,758 main.py:51] epoch 337, training loss: 4020.94, average training loss: 4774.00, base loss: 4562.57
[INFO 2017-06-27 16:49:41,133 main.py:51] epoch 338, training loss: 4330.35, average training loss: 4772.69, base loss: 4563.39
[INFO 2017-06-27 16:49:41,503 main.py:51] epoch 339, training loss: 3944.98, average training loss: 4770.26, base loss: 4562.79
[INFO 2017-06-27 16:49:41,881 main.py:51] epoch 340, training loss: 4182.49, average training loss: 4768.53, base loss: 4563.28
[INFO 2017-06-27 16:49:42,251 main.py:51] epoch 341, training loss: 4145.68, average training loss: 4766.71, base loss: 4563.42
[INFO 2017-06-27 16:49:42,628 main.py:51] epoch 342, training loss: 3770.40, average training loss: 4763.81, base loss: 4562.45
[INFO 2017-06-27 16:49:43,010 main.py:51] epoch 343, training loss: 3764.09, average training loss: 4760.90, base loss: 4561.34
[INFO 2017-06-27 16:49:43,393 main.py:51] epoch 344, training loss: 4275.42, average training loss: 4759.49, base loss: 4562.01
[INFO 2017-06-27 16:49:43,770 main.py:51] epoch 345, training loss: 4687.74, average training loss: 4759.29, base loss: 4564.15
[INFO 2017-06-27 16:49:44,143 main.py:51] epoch 346, training loss: 4060.58, average training loss: 4757.27, base loss: 4564.25
[INFO 2017-06-27 16:49:44,516 main.py:51] epoch 347, training loss: 4115.28, average training loss: 4755.43, base loss: 4564.52
[INFO 2017-06-27 16:49:44,888 main.py:51] epoch 348, training loss: 3706.22, average training loss: 4752.42, base loss: 4562.33
[INFO 2017-06-27 16:49:45,271 main.py:51] epoch 349, training loss: 4567.83, average training loss: 4751.89, base loss: 4564.06
[INFO 2017-06-27 16:49:45,647 main.py:51] epoch 350, training loss: 4345.42, average training loss: 4750.74, base loss: 4564.10
[INFO 2017-06-27 16:49:46,024 main.py:51] epoch 351, training loss: 4119.78, average training loss: 4748.94, base loss: 4564.62
[INFO 2017-06-27 16:49:46,399 main.py:51] epoch 352, training loss: 3719.50, average training loss: 4746.03, base loss: 4563.63
[INFO 2017-06-27 16:49:46,776 main.py:51] epoch 353, training loss: 4352.11, average training loss: 4744.91, base loss: 4564.96
[INFO 2017-06-27 16:49:47,153 main.py:51] epoch 354, training loss: 3811.85, average training loss: 4742.29, base loss: 4563.51
[INFO 2017-06-27 16:49:47,526 main.py:51] epoch 355, training loss: 4012.66, average training loss: 4740.24, base loss: 4563.64
[INFO 2017-06-27 16:49:47,901 main.py:51] epoch 356, training loss: 3600.72, average training loss: 4737.04, base loss: 4562.31
[INFO 2017-06-27 16:49:48,282 main.py:51] epoch 357, training loss: 3541.81, average training loss: 4733.71, base loss: 4560.43
[INFO 2017-06-27 16:49:48,658 main.py:51] epoch 358, training loss: 3958.93, average training loss: 4731.55, base loss: 4560.05
[INFO 2017-06-27 16:49:49,039 main.py:51] epoch 359, training loss: 3550.18, average training loss: 4728.27, base loss: 4558.18
[INFO 2017-06-27 16:49:49,415 main.py:51] epoch 360, training loss: 4387.36, average training loss: 4727.32, base loss: 4558.92
[INFO 2017-06-27 16:49:49,789 main.py:51] epoch 361, training loss: 3834.15, average training loss: 4724.85, base loss: 4558.45
[INFO 2017-06-27 16:49:50,171 main.py:51] epoch 362, training loss: 4043.28, average training loss: 4722.98, base loss: 4558.57
[INFO 2017-06-27 16:49:50,557 main.py:51] epoch 363, training loss: 3839.33, average training loss: 4720.55, base loss: 4557.92
[INFO 2017-06-27 16:49:50,934 main.py:51] epoch 364, training loss: 3726.55, average training loss: 4717.83, base loss: 4556.33
[INFO 2017-06-27 16:49:51,308 main.py:51] epoch 365, training loss: 4500.29, average training loss: 4717.23, base loss: 4557.96
[INFO 2017-06-27 16:49:51,678 main.py:51] epoch 366, training loss: 4120.76, average training loss: 4715.61, base loss: 4558.50
[INFO 2017-06-27 16:49:52,054 main.py:51] epoch 367, training loss: 3634.85, average training loss: 4712.67, base loss: 4557.28
[INFO 2017-06-27 16:49:52,435 main.py:51] epoch 368, training loss: 4547.45, average training loss: 4712.22, base loss: 4559.04
[INFO 2017-06-27 16:49:52,811 main.py:51] epoch 369, training loss: 3862.95, average training loss: 4709.93, base loss: 4558.32
[INFO 2017-06-27 16:49:53,187 main.py:51] epoch 370, training loss: 3777.04, average training loss: 4707.41, base loss: 4557.43
[INFO 2017-06-27 16:49:53,560 main.py:51] epoch 371, training loss: 3860.69, average training loss: 4705.14, base loss: 4557.00
[INFO 2017-06-27 16:49:53,942 main.py:51] epoch 372, training loss: 3373.86, average training loss: 4701.57, base loss: 4554.92
[INFO 2017-06-27 16:49:54,317 main.py:51] epoch 373, training loss: 4298.84, average training loss: 4700.49, base loss: 4555.83
[INFO 2017-06-27 16:49:54,689 main.py:51] epoch 374, training loss: 3687.50, average training loss: 4697.79, base loss: 4554.75
[INFO 2017-06-27 16:49:55,063 main.py:51] epoch 375, training loss: 3747.24, average training loss: 4695.26, base loss: 4554.15
[INFO 2017-06-27 16:49:55,448 main.py:51] epoch 376, training loss: 3949.24, average training loss: 4693.28, base loss: 4553.78
[INFO 2017-06-27 16:49:55,818 main.py:51] epoch 377, training loss: 4593.80, average training loss: 4693.02, base loss: 4555.78
[INFO 2017-06-27 16:49:56,194 main.py:51] epoch 378, training loss: 3870.12, average training loss: 4690.85, base loss: 4554.84
[INFO 2017-06-27 16:49:56,568 main.py:51] epoch 379, training loss: 4155.77, average training loss: 4689.44, base loss: 4555.65
[INFO 2017-06-27 16:49:56,938 main.py:51] epoch 380, training loss: 3353.63, average training loss: 4685.93, base loss: 4553.38
[INFO 2017-06-27 16:49:57,308 main.py:51] epoch 381, training loss: 3723.70, average training loss: 4683.41, base loss: 4552.45
[INFO 2017-06-27 16:49:57,680 main.py:51] epoch 382, training loss: 3634.07, average training loss: 4680.67, base loss: 4551.19
[INFO 2017-06-27 16:49:58,052 main.py:51] epoch 383, training loss: 4095.41, average training loss: 4679.15, base loss: 4551.22
[INFO 2017-06-27 16:49:58,435 main.py:51] epoch 384, training loss: 4022.53, average training loss: 4677.44, base loss: 4551.20
[INFO 2017-06-27 16:49:58,814 main.py:51] epoch 385, training loss: 3886.90, average training loss: 4675.40, base loss: 4550.73
[INFO 2017-06-27 16:49:59,195 main.py:51] epoch 386, training loss: 3362.78, average training loss: 4672.00, base loss: 4548.58
[INFO 2017-06-27 16:49:59,573 main.py:51] epoch 387, training loss: 3514.58, average training loss: 4669.02, base loss: 4546.96
[INFO 2017-06-27 16:49:59,962 main.py:51] epoch 388, training loss: 4107.71, average training loss: 4667.58, base loss: 4547.31
[INFO 2017-06-27 16:50:00,345 main.py:51] epoch 389, training loss: 4260.22, average training loss: 4666.53, base loss: 4548.01
[INFO 2017-06-27 16:50:00,722 main.py:51] epoch 390, training loss: 4141.44, average training loss: 4665.19, base loss: 4548.26
[INFO 2017-06-27 16:50:01,102 main.py:51] epoch 391, training loss: 3546.33, average training loss: 4662.34, base loss: 4546.24
[INFO 2017-06-27 16:50:01,478 main.py:51] epoch 392, training loss: 4219.20, average training loss: 4661.21, base loss: 4546.56
[INFO 2017-06-27 16:50:01,853 main.py:51] epoch 393, training loss: 4065.97, average training loss: 4659.70, base loss: 4546.50
[INFO 2017-06-27 16:50:02,230 main.py:51] epoch 394, training loss: 3691.23, average training loss: 4657.25, base loss: 4545.40
[INFO 2017-06-27 16:50:02,605 main.py:51] epoch 395, training loss: 4572.78, average training loss: 4657.03, base loss: 4546.68
[INFO 2017-06-27 16:50:02,977 main.py:51] epoch 396, training loss: 7744.52, average training loss: 4664.81, base loss: 4556.11
[INFO 2017-06-27 16:50:03,349 main.py:51] epoch 397, training loss: 4412.03, average training loss: 4664.18, base loss: 4556.83
[INFO 2017-06-27 16:50:03,721 main.py:51] epoch 398, training loss: 7894.91, average training loss: 4672.27, base loss: 4567.06
[INFO 2017-06-27 16:50:04,092 main.py:51] epoch 399, training loss: 4164.06, average training loss: 4671.00, base loss: 4567.43
[INFO 2017-06-27 16:50:04,092 main.py:53] epoch 399, testing
[INFO 2017-06-27 16:50:05,711 main.py:105] average testing loss: 4844.06, base loss: 5408.90
[INFO 2017-06-27 16:50:05,711 main.py:106] improve_loss: 564.84, improve_percent: 0.10
[INFO 2017-06-27 16:50:05,712 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 16:50:05,724 main.py:76] current best improved percent: 0.10
[INFO 2017-06-27 16:50:06,103 main.py:51] epoch 400, training loss: 3563.36, average training loss: 4668.24, base loss: 4566.19
[INFO 2017-06-27 16:50:06,479 main.py:51] epoch 401, training loss: 3526.58, average training loss: 4665.40, base loss: 4564.53
[INFO 2017-06-27 16:50:06,856 main.py:51] epoch 402, training loss: 4141.80, average training loss: 4664.10, base loss: 4565.07
[INFO 2017-06-27 16:50:07,230 main.py:51] epoch 403, training loss: 3996.17, average training loss: 4662.45, base loss: 4565.06
[INFO 2017-06-27 16:50:07,602 main.py:51] epoch 404, training loss: 4057.75, average training loss: 4660.95, base loss: 4565.85
[INFO 2017-06-27 16:50:07,973 main.py:51] epoch 405, training loss: 4108.34, average training loss: 4659.59, base loss: 4565.87
[INFO 2017-06-27 16:50:08,348 main.py:51] epoch 406, training loss: 4298.08, average training loss: 4658.71, base loss: 4566.65
[INFO 2017-06-27 16:50:08,733 main.py:51] epoch 407, training loss: 3985.35, average training loss: 4657.05, base loss: 4566.08
[INFO 2017-06-27 16:50:09,111 main.py:51] epoch 408, training loss: 6981.62, average training loss: 4662.74, base loss: 4572.68
[INFO 2017-06-27 16:50:09,486 main.py:51] epoch 409, training loss: 4031.57, average training loss: 4661.20, base loss: 4572.92
[INFO 2017-06-27 16:50:09,860 main.py:51] epoch 410, training loss: 3535.56, average training loss: 4658.46, base loss: 4571.43
[INFO 2017-06-27 16:50:10,239 main.py:51] epoch 411, training loss: 3611.16, average training loss: 4655.92, base loss: 4570.12
[INFO 2017-06-27 16:50:10,612 main.py:51] epoch 412, training loss: 3487.74, average training loss: 4653.09, base loss: 4568.16
[INFO 2017-06-27 16:50:10,993 main.py:51] epoch 413, training loss: 3595.74, average training loss: 4650.54, base loss: 4566.56
[INFO 2017-06-27 16:50:11,364 main.py:51] epoch 414, training loss: 3961.84, average training loss: 4648.88, base loss: 4566.44
[INFO 2017-06-27 16:50:11,740 main.py:51] epoch 415, training loss: 3970.82, average training loss: 4647.25, base loss: 4566.03
[INFO 2017-06-27 16:50:12,130 main.py:51] epoch 416, training loss: 3298.93, average training loss: 4644.01, base loss: 4563.33
[INFO 2017-06-27 16:50:12,505 main.py:51] epoch 417, training loss: 4228.91, average training loss: 4643.02, base loss: 4564.03
[INFO 2017-06-27 16:50:12,886 main.py:51] epoch 418, training loss: 3785.78, average training loss: 4640.97, base loss: 4563.47
[INFO 2017-06-27 16:50:13,261 main.py:51] epoch 419, training loss: 4819.99, average training loss: 4641.40, base loss: 4566.03
[INFO 2017-06-27 16:50:13,638 main.py:51] epoch 420, training loss: 4247.89, average training loss: 4640.47, base loss: 4566.97
[INFO 2017-06-27 16:50:14,014 main.py:51] epoch 421, training loss: 3897.57, average training loss: 4638.70, base loss: 4567.28
[INFO 2017-06-27 16:50:14,389 main.py:51] epoch 422, training loss: 3907.82, average training loss: 4636.98, base loss: 4567.11
[INFO 2017-06-27 16:50:14,771 main.py:51] epoch 423, training loss: 3589.35, average training loss: 4634.51, base loss: 4565.78
[INFO 2017-06-27 16:50:15,145 main.py:51] epoch 424, training loss: 3791.77, average training loss: 4632.52, base loss: 4565.58
[INFO 2017-06-27 16:50:15,516 main.py:51] epoch 425, training loss: 4166.01, average training loss: 4631.43, base loss: 4565.74
[INFO 2017-06-27 16:50:15,888 main.py:51] epoch 426, training loss: 4378.23, average training loss: 4630.84, base loss: 4566.89
[INFO 2017-06-27 16:50:16,260 main.py:51] epoch 427, training loss: 4256.65, average training loss: 4629.96, base loss: 4567.56
[INFO 2017-06-27 16:50:16,643 main.py:51] epoch 428, training loss: 3758.96, average training loss: 4627.93, base loss: 4566.76
[INFO 2017-06-27 16:50:17,019 main.py:51] epoch 429, training loss: 3979.60, average training loss: 4626.42, base loss: 4566.29
[INFO 2017-06-27 16:50:17,399 main.py:51] epoch 430, training loss: 7907.99, average training loss: 4634.04, base loss: 4575.80
[INFO 2017-06-27 16:50:17,777 main.py:51] epoch 431, training loss: 4550.12, average training loss: 4633.84, base loss: 4576.84
[INFO 2017-06-27 16:50:18,155 main.py:51] epoch 432, training loss: 7455.19, average training loss: 4640.36, base loss: 4584.88
[INFO 2017-06-27 16:50:18,529 main.py:51] epoch 433, training loss: 3559.31, average training loss: 4637.87, base loss: 4583.60
[INFO 2017-06-27 16:50:18,905 main.py:51] epoch 434, training loss: 3957.64, average training loss: 4636.30, base loss: 4583.60
[INFO 2017-06-27 16:50:19,281 main.py:51] epoch 435, training loss: 3891.09, average training loss: 4634.59, base loss: 4583.49
[INFO 2017-06-27 16:50:19,660 main.py:51] epoch 436, training loss: 4149.66, average training loss: 4633.48, base loss: 4584.07
[INFO 2017-06-27 16:50:20,036 main.py:51] epoch 437, training loss: 3330.77, average training loss: 4630.51, base loss: 4582.12
[INFO 2017-06-27 16:50:20,414 main.py:51] epoch 438, training loss: 4488.15, average training loss: 4630.19, base loss: 4583.58
[INFO 2017-06-27 16:50:20,793 main.py:51] epoch 439, training loss: 3938.31, average training loss: 4628.61, base loss: 4583.07
[INFO 2017-06-27 16:50:21,168 main.py:51] epoch 440, training loss: 4054.66, average training loss: 4627.31, base loss: 4583.21
[INFO 2017-06-27 16:50:21,546 main.py:51] epoch 441, training loss: 3946.55, average training loss: 4625.77, base loss: 4582.94
[INFO 2017-06-27 16:50:21,923 main.py:51] epoch 442, training loss: 3849.49, average training loss: 4624.02, base loss: 4582.39
[INFO 2017-06-27 16:50:22,297 main.py:51] epoch 443, training loss: 7739.47, average training loss: 4631.04, base loss: 4590.63
[INFO 2017-06-27 16:50:22,668 main.py:51] epoch 444, training loss: 3392.44, average training loss: 4628.25, base loss: 4589.09
[INFO 2017-06-27 16:50:23,040 main.py:51] epoch 445, training loss: 4538.94, average training loss: 4628.05, base loss: 4590.75
[INFO 2017-06-27 16:50:23,421 main.py:51] epoch 446, training loss: 3678.23, average training loss: 4625.93, base loss: 4589.38
[INFO 2017-06-27 16:50:23,801 main.py:51] epoch 447, training loss: 3782.14, average training loss: 4624.04, base loss: 4588.99
[INFO 2017-06-27 16:50:24,179 main.py:51] epoch 448, training loss: 3430.47, average training loss: 4621.39, base loss: 4587.39
[INFO 2017-06-27 16:50:24,552 main.py:51] epoch 449, training loss: 3380.59, average training loss: 4618.63, base loss: 4585.73
[INFO 2017-06-27 16:50:24,928 main.py:51] epoch 450, training loss: 3426.06, average training loss: 4615.98, base loss: 4584.46
[INFO 2017-06-27 16:50:25,301 main.py:51] epoch 451, training loss: 3414.33, average training loss: 4613.33, base loss: 4582.66
[INFO 2017-06-27 16:50:25,680 main.py:51] epoch 452, training loss: 3405.03, average training loss: 4610.66, base loss: 4580.84
[INFO 2017-06-27 16:50:26,055 main.py:51] epoch 453, training loss: 4335.50, average training loss: 4610.05, base loss: 4581.80
[INFO 2017-06-27 16:50:26,431 main.py:51] epoch 454, training loss: 3676.80, average training loss: 4608.00, base loss: 4580.71
[INFO 2017-06-27 16:50:26,809 main.py:51] epoch 455, training loss: 3423.89, average training loss: 4605.41, base loss: 4579.26
[INFO 2017-06-27 16:50:27,189 main.py:51] epoch 456, training loss: 3500.11, average training loss: 4602.99, base loss: 4577.99
[INFO 2017-06-27 16:50:27,569 main.py:51] epoch 457, training loss: 7092.80, average training loss: 4608.42, base loss: 4584.70
[INFO 2017-06-27 16:50:27,949 main.py:51] epoch 458, training loss: 3229.89, average training loss: 4605.42, base loss: 4582.25
[INFO 2017-06-27 16:50:28,327 main.py:51] epoch 459, training loss: 3591.71, average training loss: 4603.22, base loss: 4581.21
[INFO 2017-06-27 16:50:28,702 main.py:51] epoch 460, training loss: 4145.47, average training loss: 4602.22, base loss: 4581.32
[INFO 2017-06-27 16:50:29,075 main.py:51] epoch 461, training loss: 4061.55, average training loss: 4601.05, base loss: 4581.39
[INFO 2017-06-27 16:50:29,446 main.py:51] epoch 462, training loss: 3405.48, average training loss: 4598.47, base loss: 4579.74
[INFO 2017-06-27 16:50:29,826 main.py:51] epoch 463, training loss: 4014.36, average training loss: 4597.21, base loss: 4580.17
[INFO 2017-06-27 16:50:30,201 main.py:51] epoch 464, training loss: 3139.64, average training loss: 4594.08, base loss: 4577.64
[INFO 2017-06-27 16:50:30,575 main.py:51] epoch 465, training loss: 3441.53, average training loss: 4591.60, base loss: 4576.09
[INFO 2017-06-27 16:50:30,951 main.py:51] epoch 466, training loss: 3865.79, average training loss: 4590.05, base loss: 4575.75
[INFO 2017-06-27 16:50:31,332 main.py:51] epoch 467, training loss: 3911.65, average training loss: 4588.60, base loss: 4575.46
[INFO 2017-06-27 16:50:31,722 main.py:51] epoch 468, training loss: 3628.81, average training loss: 4586.55, base loss: 4574.48
[INFO 2017-06-27 16:50:32,103 main.py:51] epoch 469, training loss: 3935.65, average training loss: 4585.17, base loss: 4574.36
[INFO 2017-06-27 16:50:32,486 main.py:51] epoch 470, training loss: 3658.21, average training loss: 4583.20, base loss: 4573.29
[INFO 2017-06-27 16:50:32,862 main.py:51] epoch 471, training loss: 4080.09, average training loss: 4582.13, base loss: 4573.63
[INFO 2017-06-27 16:50:33,233 main.py:51] epoch 472, training loss: 3284.64, average training loss: 4579.39, base loss: 4571.48
[INFO 2017-06-27 16:50:33,608 main.py:51] epoch 473, training loss: 3294.00, average training loss: 4576.68, base loss: 4569.66
[INFO 2017-06-27 16:50:33,984 main.py:51] epoch 474, training loss: 4032.16, average training loss: 4575.53, base loss: 4570.05
[INFO 2017-06-27 16:50:34,362 main.py:51] epoch 475, training loss: 3173.40, average training loss: 4572.59, base loss: 4567.85
[INFO 2017-06-27 16:50:34,741 main.py:51] epoch 476, training loss: 3898.81, average training loss: 4571.17, base loss: 4567.66
[INFO 2017-06-27 16:50:35,119 main.py:51] epoch 477, training loss: 4477.38, average training loss: 4570.98, base loss: 4568.88
[INFO 2017-06-27 16:50:35,495 main.py:51] epoch 478, training loss: 3474.96, average training loss: 4568.69, base loss: 4567.29
[INFO 2017-06-27 16:50:35,868 main.py:51] epoch 479, training loss: 4485.47, average training loss: 4568.52, base loss: 4568.28
[INFO 2017-06-27 16:50:36,245 main.py:51] epoch 480, training loss: 4006.92, average training loss: 4567.35, base loss: 4568.60
[INFO 2017-06-27 16:50:36,622 main.py:51] epoch 481, training loss: 4540.91, average training loss: 4567.29, base loss: 4570.24
[INFO 2017-06-27 16:50:36,996 main.py:51] epoch 482, training loss: 3681.52, average training loss: 4565.46, base loss: 4569.32
[INFO 2017-06-27 16:50:37,371 main.py:51] epoch 483, training loss: 3479.21, average training loss: 4563.22, base loss: 4567.57
[INFO 2017-06-27 16:50:37,741 main.py:51] epoch 484, training loss: 3474.43, average training loss: 4560.97, base loss: 4566.25
[INFO 2017-06-27 16:50:38,112 main.py:51] epoch 485, training loss: 8106.86, average training loss: 4568.27, base loss: 4575.07
[INFO 2017-06-27 16:50:38,486 main.py:51] epoch 486, training loss: 3559.98, average training loss: 4566.20, base loss: 4573.76
[INFO 2017-06-27 16:50:38,856 main.py:51] epoch 487, training loss: 3711.93, average training loss: 4564.45, base loss: 4573.14
[INFO 2017-06-27 16:50:39,231 main.py:51] epoch 488, training loss: 3595.53, average training loss: 4562.47, base loss: 4572.32
[INFO 2017-06-27 16:50:39,606 main.py:51] epoch 489, training loss: 4031.82, average training loss: 4561.38, base loss: 4572.24
[INFO 2017-06-27 16:50:39,993 main.py:51] epoch 490, training loss: 3944.50, average training loss: 4560.13, base loss: 4572.45
[INFO 2017-06-27 16:50:40,388 main.py:51] epoch 491, training loss: 3901.71, average training loss: 4558.79, base loss: 4572.53
[INFO 2017-06-27 16:50:40,768 main.py:51] epoch 492, training loss: 4020.06, average training loss: 4557.69, base loss: 4573.10
[INFO 2017-06-27 16:50:41,148 main.py:51] epoch 493, training loss: 3771.67, average training loss: 4556.10, base loss: 4572.68
[INFO 2017-06-27 16:50:41,543 main.py:51] epoch 494, training loss: 3814.59, average training loss: 4554.61, base loss: 4572.76
[INFO 2017-06-27 16:50:41,921 main.py:51] epoch 495, training loss: 3641.79, average training loss: 4552.77, base loss: 4571.66
[INFO 2017-06-27 16:50:42,309 main.py:51] epoch 496, training loss: 3852.86, average training loss: 4551.36, base loss: 4571.71
[INFO 2017-06-27 16:50:42,694 main.py:51] epoch 497, training loss: 4122.91, average training loss: 4550.50, base loss: 4571.84
[INFO 2017-06-27 16:50:43,073 main.py:51] epoch 498, training loss: 3933.40, average training loss: 4549.26, base loss: 4571.61
[INFO 2017-06-27 16:50:43,451 main.py:51] epoch 499, training loss: 4241.29, average training loss: 4548.64, base loss: 4572.75
[INFO 2017-06-27 16:50:43,451 main.py:53] epoch 499, testing
[INFO 2017-06-27 16:50:45,095 main.py:105] average testing loss: 4535.80, base loss: 5182.16
[INFO 2017-06-27 16:50:45,095 main.py:106] improve_loss: 646.36, improve_percent: 0.12
[INFO 2017-06-27 16:50:45,096 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 16:50:45,109 main.py:76] current best improved percent: 0.12
[INFO 2017-06-27 16:50:45,487 main.py:51] epoch 500, training loss: 3423.09, average training loss: 4546.40, base loss: 4571.50
[INFO 2017-06-27 16:50:45,861 main.py:51] epoch 501, training loss: 3956.59, average training loss: 4545.22, base loss: 4572.02
[INFO 2017-06-27 16:50:46,237 main.py:51] epoch 502, training loss: 4147.86, average training loss: 4544.43, base loss: 4573.29
[INFO 2017-06-27 16:50:46,607 main.py:51] epoch 503, training loss: 3843.95, average training loss: 4543.04, base loss: 4573.23
[INFO 2017-06-27 16:50:46,988 main.py:51] epoch 504, training loss: 3953.99, average training loss: 4541.88, base loss: 4573.32
[INFO 2017-06-27 16:50:47,372 main.py:51] epoch 505, training loss: 3858.31, average training loss: 4540.53, base loss: 4572.84
[INFO 2017-06-27 16:50:47,750 main.py:51] epoch 506, training loss: 3731.29, average training loss: 4538.93, base loss: 4572.54
[INFO 2017-06-27 16:50:48,130 main.py:51] epoch 507, training loss: 3837.28, average training loss: 4537.55, base loss: 4572.11
[INFO 2017-06-27 16:50:48,520 main.py:51] epoch 508, training loss: 3995.54, average training loss: 4536.48, base loss: 4572.29
[INFO 2017-06-27 16:50:48,902 main.py:51] epoch 509, training loss: 3393.65, average training loss: 4534.24, base loss: 4570.75
[INFO 2017-06-27 16:50:49,289 main.py:51] epoch 510, training loss: 3505.84, average training loss: 4532.23, base loss: 4569.70
[INFO 2017-06-27 16:50:49,675 main.py:51] epoch 511, training loss: 4583.74, average training loss: 4532.33, base loss: 4571.12
[INFO 2017-06-27 16:50:50,060 main.py:51] epoch 512, training loss: 3471.71, average training loss: 4530.26, base loss: 4569.96
[INFO 2017-06-27 16:50:50,441 main.py:51] epoch 513, training loss: 3943.98, average training loss: 4529.12, base loss: 4570.22
[INFO 2017-06-27 16:50:50,843 main.py:51] epoch 514, training loss: 4095.01, average training loss: 4528.28, base loss: 4570.58
[INFO 2017-06-27 16:50:51,288 main.py:51] epoch 515, training loss: 3419.63, average training loss: 4526.13, base loss: 4568.95
[INFO 2017-06-27 16:50:51,737 main.py:51] epoch 516, training loss: 3644.56, average training loss: 4524.43, base loss: 4568.38
[INFO 2017-06-27 16:50:52,185 main.py:51] epoch 517, training loss: 4069.00, average training loss: 4523.55, base loss: 4569.04
[INFO 2017-06-27 16:50:52,609 main.py:51] epoch 518, training loss: 3514.65, average training loss: 4521.60, base loss: 4568.50
[INFO 2017-06-27 16:50:53,046 main.py:51] epoch 519, training loss: 3968.77, average training loss: 4520.54, base loss: 4568.97
[INFO 2017-06-27 16:50:53,446 main.py:51] epoch 520, training loss: 3657.80, average training loss: 4518.88, base loss: 4567.88
[INFO 2017-06-27 16:50:53,898 main.py:51] epoch 521, training loss: 3955.67, average training loss: 4517.80, base loss: 4568.39
[INFO 2017-06-27 16:50:54,361 main.py:51] epoch 522, training loss: 3996.77, average training loss: 4516.81, base loss: 4568.91
[INFO 2017-06-27 16:50:54,754 main.py:51] epoch 523, training loss: 3902.10, average training loss: 4515.64, base loss: 4568.81
[INFO 2017-06-27 16:50:55,129 main.py:51] epoch 524, training loss: 3728.86, average training loss: 4514.14, base loss: 4568.28
[INFO 2017-06-27 16:50:55,503 main.py:51] epoch 525, training loss: 3844.55, average training loss: 4512.86, base loss: 4567.58
[INFO 2017-06-27 16:50:55,878 main.py:51] epoch 526, training loss: 3472.53, average training loss: 4510.89, base loss: 4566.64
[INFO 2017-06-27 16:50:56,253 main.py:51] epoch 527, training loss: 4227.41, average training loss: 4510.35, base loss: 4567.42
[INFO 2017-06-27 16:50:56,628 main.py:51] epoch 528, training loss: 4073.25, average training loss: 4509.53, base loss: 4567.90
[INFO 2017-06-27 16:50:57,006 main.py:51] epoch 529, training loss: 4341.25, average training loss: 4509.21, base loss: 4569.26
[INFO 2017-06-27 16:50:57,384 main.py:51] epoch 530, training loss: 3421.82, average training loss: 4507.16, base loss: 4568.10
[INFO 2017-06-27 16:50:57,758 main.py:51] epoch 531, training loss: 3895.34, average training loss: 4506.01, base loss: 4567.90
[INFO 2017-06-27 16:50:58,135 main.py:51] epoch 532, training loss: 3742.41, average training loss: 4504.58, base loss: 4567.54
[INFO 2017-06-27 16:50:58,510 main.py:51] epoch 533, training loss: 3874.65, average training loss: 4503.40, base loss: 4567.66
[INFO 2017-06-27 16:50:58,886 main.py:51] epoch 534, training loss: 3517.11, average training loss: 4501.56, base loss: 4566.76
[INFO 2017-06-27 16:50:59,263 main.py:51] epoch 535, training loss: 4106.97, average training loss: 4500.82, base loss: 4567.54
[INFO 2017-06-27 16:50:59,676 main.py:51] epoch 536, training loss: 3905.44, average training loss: 4499.71, base loss: 4567.53
[INFO 2017-06-27 16:51:00,057 main.py:51] epoch 537, training loss: 7096.85, average training loss: 4504.54, base loss: 4573.09
[INFO 2017-06-27 16:51:00,434 main.py:51] epoch 538, training loss: 3668.57, average training loss: 4502.99, base loss: 4572.48
[INFO 2017-06-27 16:51:00,884 main.py:51] epoch 539, training loss: 3833.74, average training loss: 4501.75, base loss: 4572.26
[INFO 2017-06-27 16:51:01,293 main.py:51] epoch 540, training loss: 3760.29, average training loss: 4500.38, base loss: 4571.68
[INFO 2017-06-27 16:51:01,726 main.py:51] epoch 541, training loss: 4152.92, average training loss: 4499.74, base loss: 4572.25
[INFO 2017-06-27 16:51:02,138 main.py:51] epoch 542, training loss: 3886.71, average training loss: 4498.61, base loss: 4572.50
[INFO 2017-06-27 16:51:02,516 main.py:51] epoch 543, training loss: 4571.33, average training loss: 4498.74, base loss: 4574.17
[INFO 2017-06-27 16:51:02,895 main.py:51] epoch 544, training loss: 4077.48, average training loss: 4497.97, base loss: 4574.68
[INFO 2017-06-27 16:51:03,312 main.py:51] epoch 545, training loss: 4458.31, average training loss: 4497.90, base loss: 4576.06
[INFO 2017-06-27 16:51:03,695 main.py:51] epoch 546, training loss: 3481.85, average training loss: 4496.04, base loss: 4574.29
[INFO 2017-06-27 16:51:04,094 main.py:51] epoch 547, training loss: 4271.99, average training loss: 4495.63, base loss: 4575.19
[INFO 2017-06-27 16:51:04,544 main.py:51] epoch 548, training loss: 3851.93, average training loss: 4494.46, base loss: 4575.41
[INFO 2017-06-27 16:51:04,925 main.py:51] epoch 549, training loss: 3505.57, average training loss: 4492.66, base loss: 4574.56
[INFO 2017-06-27 16:51:05,391 main.py:51] epoch 550, training loss: 3713.88, average training loss: 4491.24, base loss: 4574.05
[INFO 2017-06-27 16:51:05,806 main.py:51] epoch 551, training loss: 3682.67, average training loss: 4489.78, base loss: 4573.53
[INFO 2017-06-27 16:51:06,215 main.py:51] epoch 552, training loss: 3692.93, average training loss: 4488.34, base loss: 4572.85
[INFO 2017-06-27 16:51:06,617 main.py:51] epoch 553, training loss: 4634.20, average training loss: 4488.60, base loss: 4574.57
[INFO 2017-06-27 16:51:06,994 main.py:51] epoch 554, training loss: 4543.70, average training loss: 4488.70, base loss: 4576.21
[INFO 2017-06-27 16:51:07,473 main.py:51] epoch 555, training loss: 3387.13, average training loss: 4486.72, base loss: 4574.32
[INFO 2017-06-27 16:51:07,860 main.py:51] epoch 556, training loss: 3914.08, average training loss: 4485.69, base loss: 4574.35
[INFO 2017-06-27 16:51:08,349 main.py:51] epoch 557, training loss: 4304.22, average training loss: 4485.37, base loss: 4574.69
[INFO 2017-06-27 16:51:08,747 main.py:51] epoch 558, training loss: 3994.61, average training loss: 4484.49, base loss: 4575.12
[INFO 2017-06-27 16:51:09,230 main.py:51] epoch 559, training loss: 5363.50, average training loss: 4486.06, base loss: 4578.33
[INFO 2017-06-27 16:51:09,614 main.py:51] epoch 560, training loss: 4193.92, average training loss: 4485.54, base loss: 4579.02
[INFO 2017-06-27 16:51:10,006 main.py:51] epoch 561, training loss: 3962.07, average training loss: 4484.61, base loss: 4579.25
[INFO 2017-06-27 16:51:10,442 main.py:51] epoch 562, training loss: 3745.74, average training loss: 4483.29, base loss: 4578.98
[INFO 2017-06-27 16:51:10,823 main.py:51] epoch 563, training loss: 3481.15, average training loss: 4481.52, base loss: 4577.58
[INFO 2017-06-27 16:51:11,283 main.py:51] epoch 564, training loss: 3659.78, average training loss: 4480.06, base loss: 4576.69
[INFO 2017-06-27 16:51:11,692 main.py:51] epoch 565, training loss: 3796.81, average training loss: 4478.86, base loss: 4576.51
[INFO 2017-06-27 16:51:12,074 main.py:51] epoch 566, training loss: 3946.23, average training loss: 4477.92, base loss: 4576.75
[INFO 2017-06-27 16:51:12,564 main.py:51] epoch 567, training loss: 3922.89, average training loss: 4476.94, base loss: 4576.70
[INFO 2017-06-27 16:51:12,980 main.py:51] epoch 568, training loss: 3744.06, average training loss: 4475.65, base loss: 4576.51
[INFO 2017-06-27 16:51:13,426 main.py:51] epoch 569, training loss: 4028.25, average training loss: 4474.87, base loss: 4576.94
[INFO 2017-06-27 16:51:13,826 main.py:51] epoch 570, training loss: 3708.15, average training loss: 4473.52, base loss: 4576.40
[INFO 2017-06-27 16:51:14,205 main.py:51] epoch 571, training loss: 3745.71, average training loss: 4472.25, base loss: 4575.55
[INFO 2017-06-27 16:51:14,602 main.py:51] epoch 572, training loss: 3884.48, average training loss: 4471.23, base loss: 4575.62
[INFO 2017-06-27 16:51:14,995 main.py:51] epoch 573, training loss: 4199.51, average training loss: 4470.75, base loss: 4576.13
[INFO 2017-06-27 16:51:15,374 main.py:51] epoch 574, training loss: 4117.41, average training loss: 4470.14, base loss: 4576.48
[INFO 2017-06-27 16:51:15,760 main.py:51] epoch 575, training loss: 4365.24, average training loss: 4469.96, base loss: 4577.60
[INFO 2017-06-27 16:51:16,134 main.py:51] epoch 576, training loss: 3302.31, average training loss: 4467.93, base loss: 4576.38
[INFO 2017-06-27 16:51:16,505 main.py:51] epoch 577, training loss: 4137.03, average training loss: 4467.36, base loss: 4576.65
[INFO 2017-06-27 16:51:16,882 main.py:51] epoch 578, training loss: 3986.78, average training loss: 4466.53, base loss: 4576.75
[INFO 2017-06-27 16:51:17,254 main.py:51] epoch 579, training loss: 3116.90, average training loss: 4464.20, base loss: 4574.76
[INFO 2017-06-27 16:51:17,632 main.py:51] epoch 580, training loss: 3500.14, average training loss: 4462.54, base loss: 4573.75
[INFO 2017-06-27 16:51:18,006 main.py:51] epoch 581, training loss: 4078.70, average training loss: 4461.88, base loss: 4574.23
[INFO 2017-06-27 16:51:18,381 main.py:51] epoch 582, training loss: 3917.46, average training loss: 4460.95, base loss: 4574.58
[INFO 2017-06-27 16:51:18,754 main.py:51] epoch 583, training loss: 3907.78, average training loss: 4460.00, base loss: 4574.72
[INFO 2017-06-27 16:51:19,128 main.py:51] epoch 584, training loss: 3617.25, average training loss: 4458.56, base loss: 4574.12
[INFO 2017-06-27 16:51:19,503 main.py:51] epoch 585, training loss: 3951.79, average training loss: 4457.70, base loss: 4574.03
[INFO 2017-06-27 16:51:19,878 main.py:51] epoch 586, training loss: 3725.15, average training loss: 4456.45, base loss: 4573.68
[INFO 2017-06-27 16:51:20,251 main.py:51] epoch 587, training loss: 3340.75, average training loss: 4454.55, base loss: 4572.54
[INFO 2017-06-27 16:51:20,621 main.py:51] epoch 588, training loss: 4217.23, average training loss: 4454.15, base loss: 4573.38
[INFO 2017-06-27 16:51:20,997 main.py:51] epoch 589, training loss: 4343.59, average training loss: 4453.96, base loss: 4574.35
[INFO 2017-06-27 16:51:21,378 main.py:51] epoch 590, training loss: 3607.54, average training loss: 4452.53, base loss: 4573.49
[INFO 2017-06-27 16:51:21,753 main.py:51] epoch 591, training loss: 3781.21, average training loss: 4451.40, base loss: 4573.33
[INFO 2017-06-27 16:51:22,127 main.py:51] epoch 592, training loss: 3538.42, average training loss: 4449.86, base loss: 4572.62
[INFO 2017-06-27 16:51:22,505 main.py:51] epoch 593, training loss: 3519.79, average training loss: 4448.29, base loss: 4571.83
[INFO 2017-06-27 16:51:22,879 main.py:51] epoch 594, training loss: 3994.77, average training loss: 4447.53, base loss: 4572.06
[INFO 2017-06-27 16:51:23,249 main.py:51] epoch 595, training loss: 4228.46, average training loss: 4447.16, base loss: 4572.83
[INFO 2017-06-27 16:51:23,620 main.py:51] epoch 596, training loss: 3524.21, average training loss: 4445.61, base loss: 4571.83
[INFO 2017-06-27 16:51:23,994 main.py:51] epoch 597, training loss: 4865.03, average training loss: 4446.32, base loss: 4574.47
[INFO 2017-06-27 16:51:24,366 main.py:51] epoch 598, training loss: 3398.22, average training loss: 4444.57, base loss: 4572.94
[INFO 2017-06-27 16:51:24,738 main.py:51] epoch 599, training loss: 3569.50, average training loss: 4443.11, base loss: 4572.24
[INFO 2017-06-27 16:51:24,739 main.py:53] epoch 599, testing
[INFO 2017-06-27 16:51:26,328 main.py:105] average testing loss: 3619.98, base loss: 4208.32
[INFO 2017-06-27 16:51:26,328 main.py:106] improve_loss: 588.34, improve_percent: 0.14
[INFO 2017-06-27 16:51:26,328 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 16:51:26,343 main.py:76] current best improved percent: 0.14
[INFO 2017-06-27 16:51:26,715 main.py:51] epoch 600, training loss: 3266.89, average training loss: 4441.15, base loss: 4570.46
[INFO 2017-06-27 16:51:27,084 main.py:51] epoch 601, training loss: 3410.16, average training loss: 4439.44, base loss: 4569.39
[INFO 2017-06-27 16:51:27,453 main.py:51] epoch 602, training loss: 3953.56, average training loss: 4438.63, base loss: 4569.29
[INFO 2017-06-27 16:51:27,836 main.py:51] epoch 603, training loss: 4035.36, average training loss: 4437.96, base loss: 4569.39
[INFO 2017-06-27 16:51:28,207 main.py:51] epoch 604, training loss: 3638.59, average training loss: 4436.64, base loss: 4568.84
[INFO 2017-06-27 16:51:28,594 main.py:51] epoch 605, training loss: 3618.30, average training loss: 4435.29, base loss: 4568.26
[INFO 2017-06-27 16:51:28,967 main.py:51] epoch 606, training loss: 3797.41, average training loss: 4434.24, base loss: 4568.16
[INFO 2017-06-27 16:51:29,341 main.py:51] epoch 607, training loss: 3601.46, average training loss: 4432.87, base loss: 4567.35
[INFO 2017-06-27 16:51:29,718 main.py:51] epoch 608, training loss: 3915.98, average training loss: 4432.02, base loss: 4567.20
[INFO 2017-06-27 16:51:30,096 main.py:51] epoch 609, training loss: 4099.42, average training loss: 4431.48, base loss: 4567.99
[INFO 2017-06-27 16:51:30,529 main.py:51] epoch 610, training loss: 3679.84, average training loss: 4430.25, base loss: 4567.49
[INFO 2017-06-27 16:51:30,904 main.py:51] epoch 611, training loss: 4312.85, average training loss: 4430.06, base loss: 4568.53
[INFO 2017-06-27 16:51:31,286 main.py:51] epoch 612, training loss: 3896.25, average training loss: 4429.19, base loss: 4568.40
[INFO 2017-06-27 16:51:31,673 main.py:51] epoch 613, training loss: 3757.75, average training loss: 4428.09, base loss: 4568.39
[INFO 2017-06-27 16:51:32,055 main.py:51] epoch 614, training loss: 3742.68, average training loss: 4426.98, base loss: 4568.07
[INFO 2017-06-27 16:51:32,432 main.py:51] epoch 615, training loss: 3926.61, average training loss: 4426.16, base loss: 4568.39
[INFO 2017-06-27 16:51:32,808 main.py:51] epoch 616, training loss: 4080.79, average training loss: 4425.60, base loss: 4569.10
[INFO 2017-06-27 16:51:33,191 main.py:51] epoch 617, training loss: 3818.37, average training loss: 4424.62, base loss: 4569.22
[INFO 2017-06-27 16:51:33,577 main.py:51] epoch 618, training loss: 3503.49, average training loss: 4423.13, base loss: 4568.57
[INFO 2017-06-27 16:51:33,961 main.py:51] epoch 619, training loss: 3903.93, average training loss: 4422.30, base loss: 4568.75
[INFO 2017-06-27 16:51:34,352 main.py:51] epoch 620, training loss: 3466.37, average training loss: 4420.76, base loss: 4567.91
[INFO 2017-06-27 16:51:34,764 main.py:51] epoch 621, training loss: 4254.02, average training loss: 4420.49, base loss: 4568.84
[INFO 2017-06-27 16:51:35,217 main.py:51] epoch 622, training loss: 3517.81, average training loss: 4419.04, base loss: 4568.05
[INFO 2017-06-27 16:51:35,663 main.py:51] epoch 623, training loss: 4506.92, average training loss: 4419.18, base loss: 4569.73
[INFO 2017-06-27 16:51:36,101 main.py:51] epoch 624, training loss: 3241.04, average training loss: 4417.30, base loss: 4568.36
[INFO 2017-06-27 16:51:36,556 main.py:51] epoch 625, training loss: 3504.10, average training loss: 4415.84, base loss: 4567.90
[INFO 2017-06-27 16:51:36,968 main.py:51] epoch 626, training loss: 3389.74, average training loss: 4414.20, base loss: 4567.00
[INFO 2017-06-27 16:51:37,399 main.py:51] epoch 627, training loss: 3989.38, average training loss: 4413.52, base loss: 4567.38
[INFO 2017-06-27 16:51:37,857 main.py:51] epoch 628, training loss: 3932.21, average training loss: 4412.76, base loss: 4567.78
[INFO 2017-06-27 16:51:38,262 main.py:51] epoch 629, training loss: 3632.18, average training loss: 4411.52, base loss: 4567.37
[INFO 2017-06-27 16:51:38,641 main.py:51] epoch 630, training loss: 4198.27, average training loss: 4411.18, base loss: 4568.05
[INFO 2017-06-27 16:51:39,016 main.py:51] epoch 631, training loss: 4219.19, average training loss: 4410.88, base loss: 4568.84
[INFO 2017-06-27 16:51:39,392 main.py:51] epoch 632, training loss: 3356.53, average training loss: 4409.21, base loss: 4567.66
[INFO 2017-06-27 16:51:39,769 main.py:51] epoch 633, training loss: 4027.68, average training loss: 4408.61, base loss: 4567.95
[INFO 2017-06-27 16:51:40,156 main.py:51] epoch 634, training loss: 3567.67, average training loss: 4407.29, base loss: 4567.09
[INFO 2017-06-27 16:51:40,529 main.py:51] epoch 635, training loss: 3266.65, average training loss: 4405.49, base loss: 4565.97
[INFO 2017-06-27 16:51:40,903 main.py:51] epoch 636, training loss: 4112.03, average training loss: 4405.03, base loss: 4565.99
[INFO 2017-06-27 16:51:41,275 main.py:51] epoch 637, training loss: 6841.30, average training loss: 4408.85, base loss: 4570.36
[INFO 2017-06-27 16:51:41,650 main.py:51] epoch 638, training loss: 3460.41, average training loss: 4407.37, base loss: 4569.54
[INFO 2017-06-27 16:51:42,021 main.py:51] epoch 639, training loss: 3832.93, average training loss: 4406.47, base loss: 4569.49
[INFO 2017-06-27 16:51:42,399 main.py:51] epoch 640, training loss: 3983.63, average training loss: 4405.81, base loss: 4569.74
[INFO 2017-06-27 16:51:42,777 main.py:51] epoch 641, training loss: 3480.24, average training loss: 4404.37, base loss: 4568.92
[INFO 2017-06-27 16:51:43,207 main.py:51] epoch 642, training loss: 3654.24, average training loss: 4403.20, base loss: 4568.80
[INFO 2017-06-27 16:51:43,618 main.py:51] epoch 643, training loss: 3160.11, average training loss: 4401.27, base loss: 4567.50
[INFO 2017-06-27 16:51:43,997 main.py:51] epoch 644, training loss: 3470.20, average training loss: 4399.83, base loss: 4566.91
[INFO 2017-06-27 16:51:44,402 main.py:51] epoch 645, training loss: 3951.32, average training loss: 4399.13, base loss: 4567.28
[INFO 2017-06-27 16:51:44,816 main.py:51] epoch 646, training loss: 3395.87, average training loss: 4397.58, base loss: 4566.37
[INFO 2017-06-27 16:51:45,197 main.py:51] epoch 647, training loss: 4183.43, average training loss: 4397.25, base loss: 4567.50
[INFO 2017-06-27 16:51:45,595 main.py:51] epoch 648, training loss: 3328.84, average training loss: 4395.61, base loss: 4566.46
[INFO 2017-06-27 16:51:45,988 main.py:51] epoch 649, training loss: 3646.64, average training loss: 4394.45, base loss: 4566.40
[INFO 2017-06-27 16:51:46,362 main.py:51] epoch 650, training loss: 3170.70, average training loss: 4392.57, base loss: 4565.05
[INFO 2017-06-27 16:51:46,738 main.py:51] epoch 651, training loss: 3846.34, average training loss: 4391.74, base loss: 4565.12
[INFO 2017-06-27 16:51:47,113 main.py:51] epoch 652, training loss: 3768.04, average training loss: 4390.78, base loss: 4565.14
[INFO 2017-06-27 16:51:47,489 main.py:51] epoch 653, training loss: 3937.60, average training loss: 4390.09, base loss: 4565.38
[INFO 2017-06-27 16:51:47,861 main.py:51] epoch 654, training loss: 3671.38, average training loss: 4388.99, base loss: 4565.27
[INFO 2017-06-27 16:51:48,237 main.py:51] epoch 655, training loss: 4057.52, average training loss: 4388.49, base loss: 4565.84
[INFO 2017-06-27 16:51:48,612 main.py:51] epoch 656, training loss: 3695.79, average training loss: 4387.43, base loss: 4565.67
[INFO 2017-06-27 16:51:48,987 main.py:51] epoch 657, training loss: 4000.32, average training loss: 4386.84, base loss: 4566.06
[INFO 2017-06-27 16:51:49,363 main.py:51] epoch 658, training loss: 4470.95, average training loss: 4386.97, base loss: 4567.66
[INFO 2017-06-27 16:51:49,738 main.py:51] epoch 659, training loss: 3987.80, average training loss: 4386.37, base loss: 4567.70
[INFO 2017-06-27 16:51:50,110 main.py:51] epoch 660, training loss: 4358.13, average training loss: 4386.32, base loss: 4568.76
[INFO 2017-06-27 16:51:50,491 main.py:51] epoch 661, training loss: 3608.94, average training loss: 4385.15, base loss: 4568.41
[INFO 2017-06-27 16:51:50,866 main.py:51] epoch 662, training loss: 3512.26, average training loss: 4383.83, base loss: 4567.41
[INFO 2017-06-27 16:51:51,237 main.py:51] epoch 663, training loss: 3818.37, average training loss: 4382.98, base loss: 4567.25
[INFO 2017-06-27 16:51:51,610 main.py:51] epoch 664, training loss: 3848.75, average training loss: 4382.18, base loss: 4567.47
[INFO 2017-06-27 16:51:51,985 main.py:51] epoch 665, training loss: 3870.57, average training loss: 4381.41, base loss: 4567.07
[INFO 2017-06-27 16:51:52,357 main.py:51] epoch 666, training loss: 3387.38, average training loss: 4379.92, base loss: 4566.39
[INFO 2017-06-27 16:51:52,729 main.py:51] epoch 667, training loss: 3253.32, average training loss: 4378.23, base loss: 4565.21
[INFO 2017-06-27 16:51:53,102 main.py:51] epoch 668, training loss: 3562.31, average training loss: 4377.01, base loss: 4564.72
[INFO 2017-06-27 16:51:53,474 main.py:51] epoch 669, training loss: 3694.91, average training loss: 4375.99, base loss: 4564.53
[INFO 2017-06-27 16:51:53,850 main.py:51] epoch 670, training loss: 6957.96, average training loss: 4379.84, base loss: 4568.80
[INFO 2017-06-27 16:51:54,224 main.py:51] epoch 671, training loss: 3923.72, average training loss: 4379.16, base loss: 4569.06
[INFO 2017-06-27 16:51:54,597 main.py:51] epoch 672, training loss: 3693.34, average training loss: 4378.14, base loss: 4568.79
[INFO 2017-06-27 16:51:54,973 main.py:51] epoch 673, training loss: 3621.83, average training loss: 4377.02, base loss: 4568.49
[INFO 2017-06-27 16:51:55,351 main.py:51] epoch 674, training loss: 3893.20, average training loss: 4376.31, base loss: 4568.85
[INFO 2017-06-27 16:51:55,726 main.py:51] epoch 675, training loss: 3098.36, average training loss: 4374.42, base loss: 4567.08
[INFO 2017-06-27 16:51:56,096 main.py:51] epoch 676, training loss: 4000.45, average training loss: 4373.86, base loss: 4567.51
[INFO 2017-06-27 16:51:56,470 main.py:51] epoch 677, training loss: 3623.57, average training loss: 4372.76, base loss: 4567.06
[INFO 2017-06-27 16:51:56,843 main.py:51] epoch 678, training loss: 4177.92, average training loss: 4372.47, base loss: 4567.81
[INFO 2017-06-27 16:51:57,224 main.py:51] epoch 679, training loss: 4401.51, average training loss: 4372.51, base loss: 4569.22
[INFO 2017-06-27 16:51:57,598 main.py:51] epoch 680, training loss: 4042.82, average training loss: 4372.03, base loss: 4569.50
[INFO 2017-06-27 16:51:57,973 main.py:51] epoch 681, training loss: 3817.62, average training loss: 4371.22, base loss: 4569.62
[INFO 2017-06-27 16:51:58,345 main.py:51] epoch 682, training loss: 3810.03, average training loss: 4370.39, base loss: 4569.73
[INFO 2017-06-27 16:51:58,717 main.py:51] epoch 683, training loss: 3604.36, average training loss: 4369.27, base loss: 4569.41
[INFO 2017-06-27 16:51:59,087 main.py:51] epoch 684, training loss: 3291.85, average training loss: 4367.70, base loss: 4568.17
[INFO 2017-06-27 16:51:59,464 main.py:51] epoch 685, training loss: 3160.11, average training loss: 4365.94, base loss: 4566.68
[INFO 2017-06-27 16:51:59,840 main.py:51] epoch 686, training loss: 3804.56, average training loss: 4365.12, base loss: 4566.71
[INFO 2017-06-27 16:52:00,214 main.py:51] epoch 687, training loss: 4113.33, average training loss: 4364.76, base loss: 4567.09
[INFO 2017-06-27 16:52:00,589 main.py:51] epoch 688, training loss: 4621.21, average training loss: 4365.13, base loss: 4568.63
[INFO 2017-06-27 16:52:00,965 main.py:51] epoch 689, training loss: 3770.89, average training loss: 4364.27, base loss: 4568.51
[INFO 2017-06-27 16:52:01,340 main.py:51] epoch 690, training loss: 3581.54, average training loss: 4363.14, base loss: 4568.27
[INFO 2017-06-27 16:52:01,720 main.py:51] epoch 691, training loss: 4322.96, average training loss: 4363.08, base loss: 4568.95
[INFO 2017-06-27 16:52:02,092 main.py:51] epoch 692, training loss: 3593.23, average training loss: 4361.97, base loss: 4568.46
[INFO 2017-06-27 16:52:02,468 main.py:51] epoch 693, training loss: 3970.55, average training loss: 4361.40, base loss: 4569.06
[INFO 2017-06-27 16:52:02,850 main.py:51] epoch 694, training loss: 3442.35, average training loss: 4360.08, base loss: 4568.31
[INFO 2017-06-27 16:52:03,227 main.py:51] epoch 695, training loss: 4002.92, average training loss: 4359.57, base loss: 4568.57
[INFO 2017-06-27 16:52:03,602 main.py:51] epoch 696, training loss: 3268.99, average training loss: 4358.00, base loss: 4567.41
[INFO 2017-06-27 16:52:03,976 main.py:51] epoch 697, training loss: 3697.26, average training loss: 4357.06, base loss: 4567.13
[INFO 2017-06-27 16:52:04,366 main.py:51] epoch 698, training loss: 3610.02, average training loss: 4355.99, base loss: 4566.55
[INFO 2017-06-27 16:52:04,746 main.py:51] epoch 699, training loss: 4102.41, average training loss: 4355.62, base loss: 4567.02
[INFO 2017-06-27 16:52:04,747 main.py:53] epoch 699, testing
[INFO 2017-06-27 16:52:06,360 main.py:105] average testing loss: 3720.96, base loss: 4419.94
[INFO 2017-06-27 16:52:06,360 main.py:106] improve_loss: 698.98, improve_percent: 0.16
[INFO 2017-06-27 16:52:06,361 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 16:52:06,373 main.py:76] current best improved percent: 0.16
[INFO 2017-06-27 16:52:06,749 main.py:51] epoch 700, training loss: 3834.10, average training loss: 4354.88, base loss: 4567.13
[INFO 2017-06-27 16:52:07,127 main.py:51] epoch 701, training loss: 3939.67, average training loss: 4354.29, base loss: 4567.47
[INFO 2017-06-27 16:52:07,513 main.py:51] epoch 702, training loss: 3188.03, average training loss: 4352.63, base loss: 4566.26
[INFO 2017-06-27 16:52:07,886 main.py:51] epoch 703, training loss: 3838.40, average training loss: 4351.90, base loss: 4566.23
[INFO 2017-06-27 16:52:08,261 main.py:51] epoch 704, training loss: 3269.80, average training loss: 4350.36, base loss: 4565.07
[INFO 2017-06-27 16:52:08,637 main.py:51] epoch 705, training loss: 3583.25, average training loss: 4349.28, base loss: 4564.77
[INFO 2017-06-27 16:52:09,012 main.py:51] epoch 706, training loss: 3631.13, average training loss: 4348.26, base loss: 4564.33
[INFO 2017-06-27 16:52:09,385 main.py:51] epoch 707, training loss: 3806.93, average training loss: 4347.50, base loss: 4564.20
[INFO 2017-06-27 16:52:09,763 main.py:51] epoch 708, training loss: 3791.06, average training loss: 4346.71, base loss: 4564.21
[INFO 2017-06-27 16:52:10,140 main.py:51] epoch 709, training loss: 7365.23, average training loss: 4350.96, base loss: 4569.16
[INFO 2017-06-27 16:52:10,515 main.py:51] epoch 710, training loss: 3771.51, average training loss: 4350.15, base loss: 4569.29
[INFO 2017-06-27 16:52:10,891 main.py:51] epoch 711, training loss: 3784.01, average training loss: 4349.35, base loss: 4569.13
[INFO 2017-06-27 16:52:11,267 main.py:51] epoch 712, training loss: 3614.06, average training loss: 4348.32, base loss: 4569.04
[INFO 2017-06-27 16:52:11,646 main.py:51] epoch 713, training loss: 4223.56, average training loss: 4348.15, base loss: 4569.99
[INFO 2017-06-27 16:52:12,022 main.py:51] epoch 714, training loss: 3703.62, average training loss: 4347.25, base loss: 4569.89
[INFO 2017-06-27 16:52:12,400 main.py:51] epoch 715, training loss: 4060.41, average training loss: 4346.85, base loss: 4570.07
[INFO 2017-06-27 16:52:12,774 main.py:51] epoch 716, training loss: 4835.80, average training loss: 4347.53, base loss: 4572.02
[INFO 2017-06-27 16:52:13,148 main.py:51] epoch 717, training loss: 4067.84, average training loss: 4347.14, base loss: 4572.38
[INFO 2017-06-27 16:52:13,521 main.py:51] epoch 718, training loss: 3984.87, average training loss: 4346.63, base loss: 4572.73
[INFO 2017-06-27 16:52:13,898 main.py:51] epoch 719, training loss: 3577.95, average training loss: 4345.57, base loss: 4572.00
[INFO 2017-06-27 16:52:14,288 main.py:51] epoch 720, training loss: 3605.69, average training loss: 4344.54, base loss: 4571.97
[INFO 2017-06-27 16:52:14,659 main.py:51] epoch 721, training loss: 3614.99, average training loss: 4343.53, base loss: 4571.56
[INFO 2017-06-27 16:52:15,033 main.py:51] epoch 722, training loss: 3711.42, average training loss: 4342.66, base loss: 4571.27
[INFO 2017-06-27 16:52:15,412 main.py:51] epoch 723, training loss: 3474.63, average training loss: 4341.46, base loss: 4570.55
[INFO 2017-06-27 16:52:15,791 main.py:51] epoch 724, training loss: 3728.16, average training loss: 4340.61, base loss: 4570.46
[INFO 2017-06-27 16:52:16,161 main.py:51] epoch 725, training loss: 4522.28, average training loss: 4340.86, base loss: 4571.73
[INFO 2017-06-27 16:52:16,536 main.py:51] epoch 726, training loss: 3900.24, average training loss: 4340.26, base loss: 4571.61
[INFO 2017-06-27 16:52:16,914 main.py:51] epoch 727, training loss: 3645.08, average training loss: 4339.30, base loss: 4571.43
[INFO 2017-06-27 16:52:17,294 main.py:51] epoch 728, training loss: 3232.89, average training loss: 4337.78, base loss: 4570.36
[INFO 2017-06-27 16:52:17,672 main.py:51] epoch 729, training loss: 3671.21, average training loss: 4336.87, base loss: 4570.31
[INFO 2017-06-27 16:52:18,047 main.py:51] epoch 730, training loss: 3814.93, average training loss: 4336.16, base loss: 4570.34
[INFO 2017-06-27 16:52:18,423 main.py:51] epoch 731, training loss: 3654.00, average training loss: 4335.22, base loss: 4569.91
[INFO 2017-06-27 16:52:18,801 main.py:51] epoch 732, training loss: 3795.98, average training loss: 4334.49, base loss: 4570.01
[INFO 2017-06-27 16:52:19,179 main.py:51] epoch 733, training loss: 3812.75, average training loss: 4333.78, base loss: 4570.16
[INFO 2017-06-27 16:52:19,558 main.py:51] epoch 734, training loss: 3196.57, average training loss: 4332.23, base loss: 4569.05
[INFO 2017-06-27 16:52:19,933 main.py:51] epoch 735, training loss: 3469.86, average training loss: 4331.06, base loss: 4568.51
[INFO 2017-06-27 16:52:20,316 main.py:51] epoch 736, training loss: 7095.46, average training loss: 4334.81, base loss: 4572.93
[INFO 2017-06-27 16:52:20,704 main.py:51] epoch 737, training loss: 3685.16, average training loss: 4333.93, base loss: 4572.54
[INFO 2017-06-27 16:52:21,084 main.py:51] epoch 738, training loss: 3399.02, average training loss: 4332.66, base loss: 4571.93
[INFO 2017-06-27 16:52:21,466 main.py:51] epoch 739, training loss: 3936.85, average training loss: 4332.13, base loss: 4572.16
[INFO 2017-06-27 16:52:21,872 main.py:51] epoch 740, training loss: 3258.57, average training loss: 4330.68, base loss: 4571.37
[INFO 2017-06-27 16:52:22,319 main.py:51] epoch 741, training loss: 3837.17, average training loss: 4330.02, base loss: 4571.36
[INFO 2017-06-27 16:52:22,761 main.py:51] epoch 742, training loss: 3646.24, average training loss: 4329.10, base loss: 4571.22
[INFO 2017-06-27 16:52:23,199 main.py:51] epoch 743, training loss: 3530.11, average training loss: 4328.02, base loss: 4570.72
[INFO 2017-06-27 16:52:23,649 main.py:51] epoch 744, training loss: 3562.58, average training loss: 4326.99, base loss: 4570.16
[INFO 2017-06-27 16:52:24,068 main.py:51] epoch 745, training loss: 3902.85, average training loss: 4326.43, base loss: 4570.63
[INFO 2017-06-27 16:52:24,490 main.py:51] epoch 746, training loss: 3955.96, average training loss: 4325.93, base loss: 4570.96
[INFO 2017-06-27 16:52:24,952 main.py:51] epoch 747, training loss: 3326.13, average training loss: 4324.59, base loss: 4570.21
[INFO 2017-06-27 16:52:25,400 main.py:51] epoch 748, training loss: 3802.71, average training loss: 4323.90, base loss: 4570.32
[INFO 2017-06-27 16:52:25,780 main.py:51] epoch 749, training loss: 3944.20, average training loss: 4323.39, base loss: 4570.89
[INFO 2017-06-27 16:52:26,157 main.py:51] epoch 750, training loss: 3862.07, average training loss: 4322.78, base loss: 4571.22
[INFO 2017-06-27 16:52:26,531 main.py:51] epoch 751, training loss: 4059.05, average training loss: 4322.42, base loss: 4571.74
[INFO 2017-06-27 16:52:26,905 main.py:51] epoch 752, training loss: 3063.23, average training loss: 4320.75, base loss: 4570.19
[INFO 2017-06-27 16:52:27,281 main.py:51] epoch 753, training loss: 3935.92, average training loss: 4320.24, base loss: 4570.66
[INFO 2017-06-27 16:52:27,657 main.py:51] epoch 754, training loss: 3805.25, average training loss: 4319.56, base loss: 4570.62
[INFO 2017-06-27 16:52:28,029 main.py:51] epoch 755, training loss: 3593.90, average training loss: 4318.60, base loss: 4570.17
[INFO 2017-06-27 16:52:28,405 main.py:51] epoch 756, training loss: 3514.04, average training loss: 4317.54, base loss: 4569.79
[INFO 2017-06-27 16:52:28,787 main.py:51] epoch 757, training loss: 3697.97, average training loss: 4316.72, base loss: 4569.59
[INFO 2017-06-27 16:52:29,161 main.py:51] epoch 758, training loss: 3474.82, average training loss: 4315.61, base loss: 4568.92
[INFO 2017-06-27 16:52:29,606 main.py:51] epoch 759, training loss: 3614.47, average training loss: 4314.69, base loss: 4568.47
[INFO 2017-06-27 16:52:30,015 main.py:51] epoch 760, training loss: 3862.58, average training loss: 4314.09, base loss: 4568.40
[INFO 2017-06-27 16:52:30,394 main.py:51] epoch 761, training loss: 3030.08, average training loss: 4312.41, base loss: 4567.03
[INFO 2017-06-27 16:52:30,776 main.py:51] epoch 762, training loss: 3613.79, average training loss: 4311.49, base loss: 4566.62
[INFO 2017-06-27 16:52:31,152 main.py:51] epoch 763, training loss: 4017.38, average training loss: 4311.11, base loss: 4566.75
[INFO 2017-06-27 16:52:31,534 main.py:51] epoch 764, training loss: 3825.32, average training loss: 4310.47, base loss: 4567.00
[INFO 2017-06-27 16:52:31,928 main.py:51] epoch 765, training loss: 3471.99, average training loss: 4309.38, base loss: 4566.53
[INFO 2017-06-27 16:52:32,305 main.py:51] epoch 766, training loss: 4025.97, average training loss: 4309.01, base loss: 4566.87
[INFO 2017-06-27 16:52:32,681 main.py:51] epoch 767, training loss: 3671.08, average training loss: 4308.18, base loss: 4566.64
[INFO 2017-06-27 16:52:33,061 main.py:51] epoch 768, training loss: 3903.79, average training loss: 4307.65, base loss: 4566.90
[INFO 2017-06-27 16:52:33,442 main.py:51] epoch 769, training loss: 3547.38, average training loss: 4306.67, base loss: 4566.51
[INFO 2017-06-27 16:52:33,907 main.py:51] epoch 770, training loss: 3363.81, average training loss: 4305.44, base loss: 4565.68
[INFO 2017-06-27 16:52:34,309 main.py:51] epoch 771, training loss: 3620.14, average training loss: 4304.55, base loss: 4565.65
[INFO 2017-06-27 16:52:34,691 main.py:51] epoch 772, training loss: 3277.74, average training loss: 4303.23, base loss: 4564.52
[INFO 2017-06-27 16:52:35,085 main.py:51] epoch 773, training loss: 3592.26, average training loss: 4302.31, base loss: 4564.25
[INFO 2017-06-27 16:52:35,461 main.py:51] epoch 774, training loss: 6659.76, average training loss: 4305.35, base loss: 4568.03
[INFO 2017-06-27 16:52:35,876 main.py:51] epoch 775, training loss: 3271.99, average training loss: 4304.02, base loss: 4567.06
[INFO 2017-06-27 16:52:36,285 main.py:51] epoch 776, training loss: 3731.80, average training loss: 4303.28, base loss: 4567.22
[INFO 2017-06-27 16:52:36,669 main.py:51] epoch 777, training loss: 3428.35, average training loss: 4302.16, base loss: 4566.74
[INFO 2017-06-27 16:52:37,045 main.py:51] epoch 778, training loss: 3933.80, average training loss: 4301.68, base loss: 4567.12
[INFO 2017-06-27 16:52:37,522 main.py:51] epoch 779, training loss: 3811.65, average training loss: 4301.06, base loss: 4567.00
[INFO 2017-06-27 16:52:37,909 main.py:51] epoch 780, training loss: 4359.55, average training loss: 4301.13, base loss: 4568.34
[INFO 2017-06-27 16:52:38,330 main.py:51] epoch 781, training loss: 7058.22, average training loss: 4304.66, base loss: 4572.46
[INFO 2017-06-27 16:52:38,742 main.py:51] epoch 782, training loss: 3747.35, average training loss: 4303.94, base loss: 4572.64
[INFO 2017-06-27 16:52:39,122 main.py:51] epoch 783, training loss: 3749.72, average training loss: 4303.24, base loss: 4572.51
[INFO 2017-06-27 16:52:39,499 main.py:51] epoch 784, training loss: 3958.54, average training loss: 4302.80, base loss: 4572.90
[INFO 2017-06-27 16:52:39,960 main.py:51] epoch 785, training loss: 3823.58, average training loss: 4302.19, base loss: 4573.28
[INFO 2017-06-27 16:52:40,357 main.py:51] epoch 786, training loss: 3485.13, average training loss: 4301.15, base loss: 4572.70
[INFO 2017-06-27 16:52:40,744 main.py:51] epoch 787, training loss: 3358.00, average training loss: 4299.95, base loss: 4572.03
[INFO 2017-06-27 16:52:41,225 main.py:51] epoch 788, training loss: 4009.53, average training loss: 4299.59, base loss: 4572.51
[INFO 2017-06-27 16:52:41,621 main.py:51] epoch 789, training loss: 3530.01, average training loss: 4298.61, base loss: 4572.14
[INFO 2017-06-27 16:52:42,069 main.py:51] epoch 790, training loss: 3968.99, average training loss: 4298.19, base loss: 4572.43
[INFO 2017-06-27 16:52:42,492 main.py:51] epoch 791, training loss: 3344.10, average training loss: 4296.99, base loss: 4571.66
[INFO 2017-06-27 16:52:42,871 main.py:51] epoch 792, training loss: 3712.85, average training loss: 4296.25, base loss: 4571.45
[INFO 2017-06-27 16:52:43,351 main.py:51] epoch 793, training loss: 3456.35, average training loss: 4295.20, base loss: 4570.74
[INFO 2017-06-27 16:52:43,746 main.py:51] epoch 794, training loss: 3305.40, average training loss: 4293.95, base loss: 4569.81
[INFO 2017-06-27 16:52:44,124 main.py:51] epoch 795, training loss: 3644.24, average training loss: 4293.13, base loss: 4569.67
[INFO 2017-06-27 16:52:44,508 main.py:51] epoch 796, training loss: 3571.40, average training loss: 4292.23, base loss: 4569.25
[INFO 2017-06-27 16:52:44,966 main.py:51] epoch 797, training loss: 4343.80, average training loss: 4292.29, base loss: 4570.17
[INFO 2017-06-27 16:52:45,408 main.py:51] epoch 798, training loss: 4047.98, average training loss: 4291.99, base loss: 4570.79
[INFO 2017-06-27 16:52:45,786 main.py:51] epoch 799, training loss: 3897.30, average training loss: 4291.49, base loss: 4571.04
[INFO 2017-06-27 16:52:45,786 main.py:53] epoch 799, testing
[INFO 2017-06-27 16:52:47,501 main.py:105] average testing loss: 3778.13, base loss: 4413.59
[INFO 2017-06-27 16:52:47,501 main.py:106] improve_loss: 635.46, improve_percent: 0.14
[INFO 2017-06-27 16:52:47,501 main.py:76] current best improved percent: 0.16
[INFO 2017-06-27 16:52:47,877 main.py:51] epoch 800, training loss: 3621.37, average training loss: 4290.66, base loss: 4570.82
[INFO 2017-06-27 16:52:48,252 main.py:51] epoch 801, training loss: 3542.02, average training loss: 4289.72, base loss: 4570.40
[INFO 2017-06-27 16:52:48,632 main.py:51] epoch 802, training loss: 4045.97, average training loss: 4289.42, base loss: 4571.22
[INFO 2017-06-27 16:52:49,104 main.py:51] epoch 803, training loss: 3440.70, average training loss: 4288.37, base loss: 4570.39
[INFO 2017-06-27 16:52:49,479 main.py:51] epoch 804, training loss: 3389.85, average training loss: 4287.25, base loss: 4569.49
[INFO 2017-06-27 16:52:49,869 main.py:51] epoch 805, training loss: 3428.50, average training loss: 4286.18, base loss: 4569.09
[INFO 2017-06-27 16:52:50,268 main.py:51] epoch 806, training loss: 4379.08, average training loss: 4286.30, base loss: 4570.06
[INFO 2017-06-27 16:52:50,649 main.py:51] epoch 807, training loss: 4006.68, average training loss: 4285.95, base loss: 4570.38
[INFO 2017-06-27 16:52:51,111 main.py:51] epoch 808, training loss: 3398.32, average training loss: 4284.86, base loss: 4569.75
[INFO 2017-06-27 16:52:51,511 main.py:51] epoch 809, training loss: 4068.33, average training loss: 4284.59, base loss: 4570.12
[INFO 2017-06-27 16:52:51,894 main.py:51] epoch 810, training loss: 3855.81, average training loss: 4284.06, base loss: 4570.36
[INFO 2017-06-27 16:52:52,272 main.py:51] epoch 811, training loss: 3853.11, average training loss: 4283.53, base loss: 4570.85
[INFO 2017-06-27 16:52:52,652 main.py:51] epoch 812, training loss: 3484.69, average training loss: 4282.55, base loss: 4570.60
[INFO 2017-06-27 16:52:53,026 main.py:51] epoch 813, training loss: 3759.44, average training loss: 4281.90, base loss: 4570.81
[INFO 2017-06-27 16:52:53,498 main.py:51] epoch 814, training loss: 3492.43, average training loss: 4280.93, base loss: 4570.17
[INFO 2017-06-27 16:52:53,876 main.py:51] epoch 815, training loss: 3543.61, average training loss: 4280.03, base loss: 4569.96
[INFO 2017-06-27 16:52:54,253 main.py:51] epoch 816, training loss: 3871.25, average training loss: 4279.53, base loss: 4569.98
[INFO 2017-06-27 16:52:54,634 main.py:51] epoch 817, training loss: 7038.88, average training loss: 4282.90, base loss: 4573.62
[INFO 2017-06-27 16:52:55,019 main.py:51] epoch 818, training loss: 3942.19, average training loss: 4282.49, base loss: 4574.20
[INFO 2017-06-27 16:52:55,422 main.py:51] epoch 819, training loss: 4043.05, average training loss: 4282.20, base loss: 4575.03
[INFO 2017-06-27 16:52:55,801 main.py:51] epoch 820, training loss: 3559.86, average training loss: 4281.32, base loss: 4574.80
[INFO 2017-06-27 16:52:56,178 main.py:51] epoch 821, training loss: 3284.84, average training loss: 4280.10, base loss: 4573.80
[INFO 2017-06-27 16:52:56,556 main.py:51] epoch 822, training loss: 3299.30, average training loss: 4278.91, base loss: 4573.21
[INFO 2017-06-27 16:52:56,945 main.py:51] epoch 823, training loss: 3681.65, average training loss: 4278.19, base loss: 4573.04
[INFO 2017-06-27 16:52:57,352 main.py:51] epoch 824, training loss: 4303.23, average training loss: 4278.22, base loss: 4573.84
[INFO 2017-06-27 16:52:57,729 main.py:51] epoch 825, training loss: 4122.09, average training loss: 4278.03, base loss: 4574.34
[INFO 2017-06-27 16:52:58,105 main.py:51] epoch 826, training loss: 3477.09, average training loss: 4277.06, base loss: 4574.27
[INFO 2017-06-27 16:52:58,480 main.py:51] epoch 827, training loss: 3850.64, average training loss: 4276.55, base loss: 4574.31
[INFO 2017-06-27 16:52:58,850 main.py:51] epoch 828, training loss: 3541.66, average training loss: 4275.66, base loss: 4573.70
[INFO 2017-06-27 16:52:59,223 main.py:51] epoch 829, training loss: 3790.71, average training loss: 4275.07, base loss: 4573.61
[INFO 2017-06-27 16:52:59,600 main.py:51] epoch 830, training loss: 3417.88, average training loss: 4274.04, base loss: 4573.14
[INFO 2017-06-27 16:52:59,974 main.py:51] epoch 831, training loss: 3520.65, average training loss: 4273.14, base loss: 4572.87
[INFO 2017-06-27 16:53:00,346 main.py:51] epoch 832, training loss: 3475.78, average training loss: 4272.18, base loss: 4572.40
[INFO 2017-06-27 16:53:00,721 main.py:51] epoch 833, training loss: 3814.37, average training loss: 4271.63, base loss: 4572.55
[INFO 2017-06-27 16:53:01,099 main.py:51] epoch 834, training loss: 3887.73, average training loss: 4271.17, base loss: 4572.97
[INFO 2017-06-27 16:53:01,477 main.py:51] epoch 835, training loss: 3777.06, average training loss: 4270.58, base loss: 4572.79
[INFO 2017-06-27 16:53:01,847 main.py:51] epoch 836, training loss: 3609.53, average training loss: 4269.79, base loss: 4572.50
[INFO 2017-06-27 16:53:02,225 main.py:51] epoch 837, training loss: 3642.40, average training loss: 4269.04, base loss: 4572.11
[INFO 2017-06-27 16:53:02,610 main.py:51] epoch 838, training loss: 3195.57, average training loss: 4267.76, base loss: 4570.94
[INFO 2017-06-27 16:53:02,987 main.py:51] epoch 839, training loss: 3771.67, average training loss: 4267.17, base loss: 4571.15
[INFO 2017-06-27 16:53:03,365 main.py:51] epoch 840, training loss: 4313.85, average training loss: 4267.23, base loss: 4572.02
[INFO 2017-06-27 16:53:03,741 main.py:51] epoch 841, training loss: 3661.93, average training loss: 4266.51, base loss: 4571.96
[INFO 2017-06-27 16:53:04,218 main.py:51] epoch 842, training loss: 3426.85, average training loss: 4265.51, base loss: 4571.38
[INFO 2017-06-27 16:53:04,620 main.py:51] epoch 843, training loss: 3531.43, average training loss: 4264.64, base loss: 4571.09
[INFO 2017-06-27 16:53:04,998 main.py:51] epoch 844, training loss: 3430.76, average training loss: 4263.66, base loss: 4570.66
[INFO 2017-06-27 16:53:05,375 main.py:51] epoch 845, training loss: 3494.25, average training loss: 4262.75, base loss: 4570.29
[INFO 2017-06-27 16:53:05,752 main.py:51] epoch 846, training loss: 3332.60, average training loss: 4261.65, base loss: 4569.69
[INFO 2017-06-27 16:53:06,129 main.py:51] epoch 847, training loss: 4260.83, average training loss: 4261.65, base loss: 4570.63
[INFO 2017-06-27 16:53:06,510 main.py:51] epoch 848, training loss: 3339.96, average training loss: 4260.56, base loss: 4570.07
[INFO 2017-06-27 16:53:06,959 main.py:51] epoch 849, training loss: 4087.57, average training loss: 4260.36, base loss: 4570.74
[INFO 2017-06-27 16:53:07,357 main.py:51] epoch 850, training loss: 6909.67, average training loss: 4263.47, base loss: 4574.30
[INFO 2017-06-27 16:53:07,753 main.py:51] epoch 851, training loss: 3762.53, average training loss: 4262.88, base loss: 4574.34
[INFO 2017-06-27 16:53:08,158 main.py:51] epoch 852, training loss: 3947.03, average training loss: 4262.51, base loss: 4574.74
[INFO 2017-06-27 16:53:08,536 main.py:51] epoch 853, training loss: 3857.32, average training loss: 4262.04, base loss: 4574.97
[INFO 2017-06-27 16:53:08,915 main.py:51] epoch 854, training loss: 3707.06, average training loss: 4261.39, base loss: 4575.26
[INFO 2017-06-27 16:53:09,293 main.py:51] epoch 855, training loss: 3776.36, average training loss: 4260.82, base loss: 4575.28
[INFO 2017-06-27 16:53:09,667 main.py:51] epoch 856, training loss: 3650.92, average training loss: 4260.11, base loss: 4575.51
[INFO 2017-06-27 16:53:10,052 main.py:51] epoch 857, training loss: 3439.10, average training loss: 4259.15, base loss: 4574.81
[INFO 2017-06-27 16:53:10,427 main.py:51] epoch 858, training loss: 4343.58, average training loss: 4259.25, base loss: 4575.83
[INFO 2017-06-27 16:53:10,803 main.py:51] epoch 859, training loss: 3805.35, average training loss: 4258.72, base loss: 4576.09
[INFO 2017-06-27 16:53:11,178 main.py:51] epoch 860, training loss: 3887.62, average training loss: 4258.29, base loss: 4576.43
[INFO 2017-06-27 16:53:11,548 main.py:51] epoch 861, training loss: 3820.97, average training loss: 4257.79, base loss: 4576.74
[INFO 2017-06-27 16:53:11,920 main.py:51] epoch 862, training loss: 7340.88, average training loss: 4261.36, base loss: 4580.98
[INFO 2017-06-27 16:53:12,290 main.py:51] epoch 863, training loss: 3920.08, average training loss: 4260.96, base loss: 4581.52
[INFO 2017-06-27 16:53:12,660 main.py:51] epoch 864, training loss: 3398.46, average training loss: 4259.97, base loss: 4581.31
[INFO 2017-06-27 16:53:13,030 main.py:51] epoch 865, training loss: 3796.05, average training loss: 4259.43, base loss: 4581.42
[INFO 2017-06-27 16:53:13,406 main.py:51] epoch 866, training loss: 3631.15, average training loss: 4258.71, base loss: 4581.20
[INFO 2017-06-27 16:53:13,778 main.py:51] epoch 867, training loss: 4132.34, average training loss: 4258.56, base loss: 4582.08
[INFO 2017-06-27 16:53:14,153 main.py:51] epoch 868, training loss: 3845.37, average training loss: 4258.09, base loss: 4582.32
[INFO 2017-06-27 16:53:14,537 main.py:51] epoch 869, training loss: 3730.65, average training loss: 4257.48, base loss: 4582.46
[INFO 2017-06-27 16:53:14,916 main.py:51] epoch 870, training loss: 3418.45, average training loss: 4256.52, base loss: 4581.87
[INFO 2017-06-27 16:53:15,292 main.py:51] epoch 871, training loss: 3179.89, average training loss: 4255.28, base loss: 4580.95
[INFO 2017-06-27 16:53:15,760 main.py:51] epoch 872, training loss: 3655.02, average training loss: 4254.59, base loss: 4580.60
[INFO 2017-06-27 16:53:16,145 main.py:51] epoch 873, training loss: 3938.66, average training loss: 4254.23, base loss: 4580.94
[INFO 2017-06-27 16:53:16,542 main.py:51] epoch 874, training loss: 7134.81, average training loss: 4257.52, base loss: 4584.56
[INFO 2017-06-27 16:53:16,980 main.py:51] epoch 875, training loss: 4002.50, average training loss: 4257.23, base loss: 4585.11
[INFO 2017-06-27 16:53:17,361 main.py:51] epoch 876, training loss: 3653.20, average training loss: 4256.54, base loss: 4585.24
[INFO 2017-06-27 16:53:17,754 main.py:51] epoch 877, training loss: 3847.28, average training loss: 4256.08, base loss: 4585.57
[INFO 2017-06-27 16:53:18,229 main.py:51] epoch 878, training loss: 3825.58, average training loss: 4255.59, base loss: 4585.86
[INFO 2017-06-27 16:53:18,624 main.py:51] epoch 879, training loss: 3847.37, average training loss: 4255.12, base loss: 4586.18
[INFO 2017-06-27 16:53:19,006 main.py:51] epoch 880, training loss: 3372.92, average training loss: 4254.12, base loss: 4585.23
[INFO 2017-06-27 16:53:19,456 main.py:51] epoch 881, training loss: 3784.76, average training loss: 4253.59, base loss: 4585.36
[INFO 2017-06-27 16:53:19,887 main.py:51] epoch 882, training loss: 3772.84, average training loss: 4253.05, base loss: 4585.27
[INFO 2017-06-27 16:53:20,303 main.py:51] epoch 883, training loss: 3863.24, average training loss: 4252.61, base loss: 4585.38
[INFO 2017-06-27 16:53:20,717 main.py:51] epoch 884, training loss: 7460.66, average training loss: 4256.23, base loss: 4589.57
[INFO 2017-06-27 16:53:21,095 main.py:51] epoch 885, training loss: 4335.87, average training loss: 4256.32, base loss: 4590.36
[INFO 2017-06-27 16:53:21,475 main.py:51] epoch 886, training loss: 3398.99, average training loss: 4255.35, base loss: 4589.46
[INFO 2017-06-27 16:53:21,853 main.py:51] epoch 887, training loss: 7238.35, average training loss: 4258.71, base loss: 4593.75
[INFO 2017-06-27 16:53:22,317 main.py:51] epoch 888, training loss: 3546.22, average training loss: 4257.91, base loss: 4593.48
[INFO 2017-06-27 16:53:22,712 main.py:51] epoch 889, training loss: 3788.53, average training loss: 4257.38, base loss: 4593.66
[INFO 2017-06-27 16:53:23,093 main.py:51] epoch 890, training loss: 6828.19, average training loss: 4260.27, base loss: 4597.01
[INFO 2017-06-27 16:53:23,533 main.py:51] epoch 891, training loss: 3782.66, average training loss: 4259.73, base loss: 4597.16
[INFO 2017-06-27 16:53:23,930 main.py:51] epoch 892, training loss: 3770.67, average training loss: 4259.19, base loss: 4597.30
[INFO 2017-06-27 16:53:24,308 main.py:51] epoch 893, training loss: 3513.12, average training loss: 4258.35, base loss: 4596.66
[INFO 2017-06-27 16:53:24,690 main.py:51] epoch 894, training loss: 7663.45, average training loss: 4262.16, base loss: 4601.21
[INFO 2017-06-27 16:53:25,123 main.py:51] epoch 895, training loss: 3578.46, average training loss: 4261.39, base loss: 4600.76
[INFO 2017-06-27 16:53:25,537 main.py:51] epoch 896, training loss: 3773.82, average training loss: 4260.85, base loss: 4600.96
[INFO 2017-06-27 16:53:25,917 main.py:51] epoch 897, training loss: 3205.46, average training loss: 4259.67, base loss: 4600.14
[INFO 2017-06-27 16:53:26,330 main.py:51] epoch 898, training loss: 3612.18, average training loss: 4258.95, base loss: 4600.24
[INFO 2017-06-27 16:53:26,713 main.py:51] epoch 899, training loss: 3705.66, average training loss: 4258.34, base loss: 4600.06
[INFO 2017-06-27 16:53:26,713 main.py:53] epoch 899, testing
[INFO 2017-06-27 16:53:28,515 main.py:105] average testing loss: 3743.70, base loss: 4621.91
[INFO 2017-06-27 16:53:28,515 main.py:106] improve_loss: 878.21, improve_percent: 0.19
[INFO 2017-06-27 16:53:28,517 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 16:53:28,547 main.py:76] current best improved percent: 0.19
[INFO 2017-06-27 16:53:28,955 main.py:51] epoch 900, training loss: 3209.93, average training loss: 4257.18, base loss: 4599.10
[INFO 2017-06-27 16:53:29,332 main.py:51] epoch 901, training loss: 3705.45, average training loss: 4256.56, base loss: 4599.17
[INFO 2017-06-27 16:53:29,709 main.py:51] epoch 902, training loss: 3513.16, average training loss: 4255.74, base loss: 4598.84
[INFO 2017-06-27 16:53:30,091 main.py:51] epoch 903, training loss: 3801.45, average training loss: 4255.24, base loss: 4599.03
[INFO 2017-06-27 16:53:30,464 main.py:51] epoch 904, training loss: 3744.23, average training loss: 4254.67, base loss: 4599.02
[INFO 2017-06-27 16:53:30,840 main.py:51] epoch 905, training loss: 3515.83, average training loss: 4253.86, base loss: 4598.63
[INFO 2017-06-27 16:53:31,218 main.py:51] epoch 906, training loss: 3528.17, average training loss: 4253.06, base loss: 4598.36
[INFO 2017-06-27 16:53:31,594 main.py:51] epoch 907, training loss: 3581.85, average training loss: 4252.32, base loss: 4598.14
[INFO 2017-06-27 16:53:31,974 main.py:51] epoch 908, training loss: 3611.25, average training loss: 4251.61, base loss: 4598.13
[INFO 2017-06-27 16:53:32,353 main.py:51] epoch 909, training loss: 3938.14, average training loss: 4251.27, base loss: 4598.31
[INFO 2017-06-27 16:53:32,734 main.py:51] epoch 910, training loss: 3647.42, average training loss: 4250.61, base loss: 4598.42
[INFO 2017-06-27 16:53:33,116 main.py:51] epoch 911, training loss: 4271.23, average training loss: 4250.63, base loss: 4599.38
[INFO 2017-06-27 16:53:33,497 main.py:51] epoch 912, training loss: 4008.28, average training loss: 4250.36, base loss: 4599.59
[INFO 2017-06-27 16:53:33,877 main.py:51] epoch 913, training loss: 3358.45, average training loss: 4249.39, base loss: 4599.07
[INFO 2017-06-27 16:53:34,258 main.py:51] epoch 914, training loss: 3425.36, average training loss: 4248.49, base loss: 4598.88
[INFO 2017-06-27 16:53:34,647 main.py:51] epoch 915, training loss: 3112.19, average training loss: 4247.25, base loss: 4597.90
[INFO 2017-06-27 16:53:35,036 main.py:51] epoch 916, training loss: 4040.23, average training loss: 4247.02, base loss: 4598.40
[INFO 2017-06-27 16:53:35,423 main.py:51] epoch 917, training loss: 4333.38, average training loss: 4247.12, base loss: 4599.08
[INFO 2017-06-27 16:53:35,806 main.py:51] epoch 918, training loss: 3709.63, average training loss: 4246.53, base loss: 4599.18
[INFO 2017-06-27 16:53:36,207 main.py:51] epoch 919, training loss: 3898.79, average training loss: 4246.15, base loss: 4599.33
[INFO 2017-06-27 16:53:36,659 main.py:51] epoch 920, training loss: 3807.48, average training loss: 4245.68, base loss: 4599.52
[INFO 2017-06-27 16:53:37,096 main.py:51] epoch 921, training loss: 3464.02, average training loss: 4244.83, base loss: 4598.97
[INFO 2017-06-27 16:53:37,546 main.py:51] epoch 922, training loss: 3469.55, average training loss: 4243.99, base loss: 4598.79
[INFO 2017-06-27 16:53:37,986 main.py:51] epoch 923, training loss: 4487.56, average training loss: 4244.25, base loss: 4600.18
[INFO 2017-06-27 16:53:38,397 main.py:51] epoch 924, training loss: 3813.34, average training loss: 4243.79, base loss: 4600.38
[INFO 2017-06-27 16:53:38,794 main.py:51] epoch 925, training loss: 3568.72, average training loss: 4243.06, base loss: 4599.71
[INFO 2017-06-27 16:53:39,258 main.py:51] epoch 926, training loss: 3279.46, average training loss: 4242.02, base loss: 4599.06
[INFO 2017-06-27 16:53:39,718 main.py:51] epoch 927, training loss: 3404.20, average training loss: 4241.11, base loss: 4598.34
[INFO 2017-06-27 16:53:40,099 main.py:51] epoch 928, training loss: 3392.95, average training loss: 4240.20, base loss: 4598.04
[INFO 2017-06-27 16:53:40,492 main.py:51] epoch 929, training loss: 7159.78, average training loss: 4243.34, base loss: 4601.71
[INFO 2017-06-27 16:53:40,873 main.py:51] epoch 930, training loss: 3948.25, average training loss: 4243.02, base loss: 4602.02
[INFO 2017-06-27 16:53:41,251 main.py:51] epoch 931, training loss: 3813.30, average training loss: 4242.56, base loss: 4602.30
[INFO 2017-06-27 16:53:41,626 main.py:51] epoch 932, training loss: 3940.87, average training loss: 4242.24, base loss: 4602.71
[INFO 2017-06-27 16:53:41,998 main.py:51] epoch 933, training loss: 6708.58, average training loss: 4244.88, base loss: 4605.64
[INFO 2017-06-27 16:53:42,434 main.py:51] epoch 934, training loss: 3697.46, average training loss: 4244.29, base loss: 4605.59
[INFO 2017-06-27 16:53:42,813 main.py:51] epoch 935, training loss: 4132.18, average training loss: 4244.18, base loss: 4606.50
[INFO 2017-06-27 16:53:43,213 main.py:51] epoch 936, training loss: 3773.50, average training loss: 4243.67, base loss: 4606.54
[INFO 2017-06-27 16:53:43,665 main.py:51] epoch 937, training loss: 3876.86, average training loss: 4243.28, base loss: 4606.81
[INFO 2017-06-27 16:53:44,063 main.py:51] epoch 938, training loss: 4035.58, average training loss: 4243.06, base loss: 4607.27
[INFO 2017-06-27 16:53:44,445 main.py:51] epoch 939, training loss: 3668.78, average training loss: 4242.45, base loss: 4607.27
[INFO 2017-06-27 16:53:44,824 main.py:51] epoch 940, training loss: 3796.74, average training loss: 4241.98, base loss: 4607.02
[INFO 2017-06-27 16:53:45,199 main.py:51] epoch 941, training loss: 3265.78, average training loss: 4240.94, base loss: 4606.08
[INFO 2017-06-27 16:53:45,575 main.py:51] epoch 942, training loss: 3746.01, average training loss: 4240.41, base loss: 4606.01
[INFO 2017-06-27 16:53:46,023 main.py:51] epoch 943, training loss: 3842.25, average training loss: 4239.99, base loss: 4606.27
[INFO 2017-06-27 16:53:46,407 main.py:51] epoch 944, training loss: 3578.38, average training loss: 4239.29, base loss: 4606.10
[INFO 2017-06-27 16:53:46,804 main.py:51] epoch 945, training loss: 4016.75, average training loss: 4239.06, base loss: 4606.45
[INFO 2017-06-27 16:53:47,204 main.py:51] epoch 946, training loss: 3709.42, average training loss: 4238.50, base loss: 4606.42
[INFO 2017-06-27 16:53:47,581 main.py:51] epoch 947, training loss: 3591.45, average training loss: 4237.82, base loss: 4605.92
[INFO 2017-06-27 16:53:47,967 main.py:51] epoch 948, training loss: 4047.83, average training loss: 4237.62, base loss: 4606.30
[INFO 2017-06-27 16:53:48,343 main.py:51] epoch 949, training loss: 3239.89, average training loss: 4236.57, base loss: 4605.54
[INFO 2017-06-27 16:53:48,731 main.py:51] epoch 950, training loss: 3382.10, average training loss: 4235.67, base loss: 4604.89
[INFO 2017-06-27 16:53:49,107 main.py:51] epoch 951, training loss: 3643.87, average training loss: 4235.05, base loss: 4604.72
[INFO 2017-06-27 16:53:49,484 main.py:51] epoch 952, training loss: 4067.53, average training loss: 4234.87, base loss: 4605.21
[INFO 2017-06-27 16:53:49,855 main.py:51] epoch 953, training loss: 3375.52, average training loss: 4233.97, base loss: 4604.87
[INFO 2017-06-27 16:53:50,234 main.py:51] epoch 954, training loss: 3748.89, average training loss: 4233.46, base loss: 4604.68
[INFO 2017-06-27 16:53:50,621 main.py:51] epoch 955, training loss: 3365.76, average training loss: 4232.55, base loss: 4604.05
[INFO 2017-06-27 16:53:51,004 main.py:51] epoch 956, training loss: 3699.99, average training loss: 4232.00, base loss: 4604.03
[INFO 2017-06-27 16:53:51,381 main.py:51] epoch 957, training loss: 4035.30, average training loss: 4231.79, base loss: 4604.50
[INFO 2017-06-27 16:53:51,757 main.py:51] epoch 958, training loss: 4088.89, average training loss: 4231.64, base loss: 4605.17
[INFO 2017-06-27 16:53:52,130 main.py:51] epoch 959, training loss: 3741.21, average training loss: 4231.13, base loss: 4605.29
[INFO 2017-06-27 16:53:52,522 main.py:51] epoch 960, training loss: 3618.17, average training loss: 4230.49, base loss: 4605.36
[INFO 2017-06-27 16:53:52,898 main.py:51] epoch 961, training loss: 4448.59, average training loss: 4230.72, base loss: 4606.47
[INFO 2017-06-27 16:53:53,270 main.py:51] epoch 962, training loss: 3198.36, average training loss: 4229.65, base loss: 4605.47
[INFO 2017-06-27 16:53:53,645 main.py:51] epoch 963, training loss: 3406.69, average training loss: 4228.79, base loss: 4605.14
[INFO 2017-06-27 16:53:54,018 main.py:51] epoch 964, training loss: 3278.93, average training loss: 4227.81, base loss: 4604.39
[INFO 2017-06-27 16:53:54,395 main.py:51] epoch 965, training loss: 3620.86, average training loss: 4227.18, base loss: 4604.43
[INFO 2017-06-27 16:53:54,773 main.py:51] epoch 966, training loss: 3526.35, average training loss: 4226.46, base loss: 4604.07
[INFO 2017-06-27 16:53:55,148 main.py:51] epoch 967, training loss: 3761.91, average training loss: 4225.98, base loss: 4604.33
[INFO 2017-06-27 16:53:55,525 main.py:51] epoch 968, training loss: 3456.92, average training loss: 4225.18, base loss: 4603.89
[INFO 2017-06-27 16:53:55,900 main.py:51] epoch 969, training loss: 3921.35, average training loss: 4224.87, base loss: 4604.57
[INFO 2017-06-27 16:53:56,278 main.py:51] epoch 970, training loss: 3459.74, average training loss: 4224.08, base loss: 4604.27
[INFO 2017-06-27 16:53:56,650 main.py:51] epoch 971, training loss: 3759.18, average training loss: 4223.60, base loss: 4604.46
[INFO 2017-06-27 16:53:57,025 main.py:51] epoch 972, training loss: 3620.96, average training loss: 4222.98, base loss: 4604.50
[INFO 2017-06-27 16:53:57,400 main.py:51] epoch 973, training loss: 3700.20, average training loss: 4222.45, base loss: 4604.36
[INFO 2017-06-27 16:53:57,774 main.py:51] epoch 974, training loss: 6815.68, average training loss: 4225.11, base loss: 4607.32
[INFO 2017-06-27 16:53:58,147 main.py:51] epoch 975, training loss: 3469.82, average training loss: 4224.33, base loss: 4606.89
[INFO 2017-06-27 16:53:58,523 main.py:51] epoch 976, training loss: 3099.75, average training loss: 4223.18, base loss: 4605.89
[INFO 2017-06-27 16:53:58,898 main.py:51] epoch 977, training loss: 3354.49, average training loss: 4222.29, base loss: 4605.21
[INFO 2017-06-27 16:53:59,278 main.py:51] epoch 978, training loss: 3398.27, average training loss: 4221.45, base loss: 4604.69
[INFO 2017-06-27 16:53:59,653 main.py:51] epoch 979, training loss: 3586.26, average training loss: 4220.80, base loss: 4604.16
[INFO 2017-06-27 16:54:00,029 main.py:51] epoch 980, training loss: 3455.01, average training loss: 4220.02, base loss: 4603.63
[INFO 2017-06-27 16:54:00,405 main.py:51] epoch 981, training loss: 3402.43, average training loss: 4219.19, base loss: 4603.25
[INFO 2017-06-27 16:54:00,779 main.py:51] epoch 982, training loss: 3472.22, average training loss: 4218.43, base loss: 4602.85
[INFO 2017-06-27 16:54:01,154 main.py:51] epoch 983, training loss: 3582.87, average training loss: 4217.79, base loss: 4602.73
[INFO 2017-06-27 16:54:01,530 main.py:51] epoch 984, training loss: 3807.19, average training loss: 4217.37, base loss: 4603.09
[INFO 2017-06-27 16:54:01,907 main.py:51] epoch 985, training loss: 3456.68, average training loss: 4216.60, base loss: 4602.83
[INFO 2017-06-27 16:54:02,284 main.py:51] epoch 986, training loss: 3656.23, average training loss: 4216.03, base loss: 4602.61
[INFO 2017-06-27 16:54:02,660 main.py:51] epoch 987, training loss: 3790.78, average training loss: 4215.60, base loss: 4602.88
[INFO 2017-06-27 16:54:03,034 main.py:51] epoch 988, training loss: 3917.62, average training loss: 4215.30, base loss: 4603.44
[INFO 2017-06-27 16:54:03,412 main.py:51] epoch 989, training loss: 3363.75, average training loss: 4214.44, base loss: 4602.94
[INFO 2017-06-27 16:54:03,782 main.py:51] epoch 990, training loss: 3481.00, average training loss: 4213.70, base loss: 4602.71
[INFO 2017-06-27 16:54:04,239 main.py:51] epoch 991, training loss: 3442.71, average training loss: 4212.92, base loss: 4602.19
[INFO 2017-06-27 16:54:04,646 main.py:51] epoch 992, training loss: 3753.70, average training loss: 4212.46, base loss: 4602.20
[INFO 2017-06-27 16:54:05,032 main.py:51] epoch 993, training loss: 3629.74, average training loss: 4211.87, base loss: 4602.10
[INFO 2017-06-27 16:54:05,432 main.py:51] epoch 994, training loss: 3347.98, average training loss: 4211.00, base loss: 4601.46
[INFO 2017-06-27 16:54:05,813 main.py:51] epoch 995, training loss: 3889.13, average training loss: 4210.68, base loss: 4601.57
[INFO 2017-06-27 16:54:06,197 main.py:51] epoch 996, training loss: 3734.50, average training loss: 4210.20, base loss: 4601.76
[INFO 2017-06-27 16:54:06,582 main.py:51] epoch 997, training loss: 3856.10, average training loss: 4209.85, base loss: 4601.96
[INFO 2017-06-27 16:54:06,968 main.py:51] epoch 998, training loss: 3764.30, average training loss: 4209.40, base loss: 4602.26
[INFO 2017-06-27 16:54:07,346 main.py:51] epoch 999, training loss: 3931.98, average training loss: 4209.12, base loss: 4602.37
[INFO 2017-06-27 16:54:07,346 main.py:53] epoch 999, testing
[INFO 2017-06-27 16:54:08,939 main.py:105] average testing loss: 3497.52, base loss: 4257.82
[INFO 2017-06-27 16:54:08,940 main.py:106] improve_loss: 760.31, improve_percent: 0.18
[INFO 2017-06-27 16:54:08,940 main.py:76] current best improved percent: 0.19
[INFO 2017-06-27 16:54:09,319 main.py:51] epoch 1000, training loss: 3308.26, average training loss: 4186.94, base loss: 4602.14
[INFO 2017-06-27 16:54:09,701 main.py:51] epoch 1001, training loss: 3783.39, average training loss: 4170.53, base loss: 4601.53
[INFO 2017-06-27 16:54:10,076 main.py:51] epoch 1002, training loss: 3383.85, average training loss: 4156.96, base loss: 4601.63
[INFO 2017-06-27 16:54:10,456 main.py:51] epoch 1003, training loss: 3796.75, average training loss: 4146.76, base loss: 4602.14
[INFO 2017-06-27 16:54:10,831 main.py:51] epoch 1004, training loss: 3739.14, average training loss: 4137.91, base loss: 4602.42
[INFO 2017-06-27 16:54:11,206 main.py:51] epoch 1005, training loss: 4593.99, average training loss: 4130.67, base loss: 4604.19
[INFO 2017-06-27 16:54:11,583 main.py:51] epoch 1006, training loss: 3596.34, average training loss: 4123.58, base loss: 4604.24
[INFO 2017-06-27 16:54:11,962 main.py:51] epoch 1007, training loss: 3732.59, average training loss: 4117.82, base loss: 4604.99
[INFO 2017-06-27 16:54:12,338 main.py:51] epoch 1008, training loss: 3708.63, average training loss: 4112.35, base loss: 4605.12
[INFO 2017-06-27 16:54:12,810 main.py:51] epoch 1009, training loss: 3645.26, average training loss: 4108.19, base loss: 4606.00
[INFO 2017-06-27 16:54:13,205 main.py:51] epoch 1010, training loss: 3580.99, average training loss: 4102.78, base loss: 4605.36
[INFO 2017-06-27 16:54:13,584 main.py:51] epoch 1011, training loss: 3545.15, average training loss: 4098.56, base loss: 4605.33
[INFO 2017-06-27 16:54:14,050 main.py:51] epoch 1012, training loss: 3958.21, average training loss: 4091.83, base loss: 4602.28
[INFO 2017-06-27 16:54:14,437 main.py:51] epoch 1013, training loss: 3359.39, average training loss: 4087.81, base loss: 4602.00
[INFO 2017-06-27 16:54:14,910 main.py:51] epoch 1014, training loss: 3474.70, average training loss: 4085.02, base loss: 4602.38
[INFO 2017-06-27 16:54:15,305 main.py:51] epoch 1015, training loss: 3660.67, average training loss: 4082.93, base loss: 4603.33
[INFO 2017-06-27 16:54:15,685 main.py:51] epoch 1016, training loss: 3913.62, average training loss: 4079.90, base loss: 4603.53
[INFO 2017-06-27 16:54:16,064 main.py:51] epoch 1017, training loss: 3414.30, average training loss: 4076.82, base loss: 4602.84
[INFO 2017-06-27 16:54:16,447 main.py:51] epoch 1018, training loss: 3515.57, average training loss: 4073.69, base loss: 4602.28
[INFO 2017-06-27 16:54:16,821 main.py:51] epoch 1019, training loss: 3397.58, average training loss: 4071.33, base loss: 4602.28
[INFO 2017-06-27 16:54:17,212 main.py:51] epoch 1020, training loss: 3923.21, average training loss: 4069.17, base loss: 4602.57
[INFO 2017-06-27 16:54:17,591 main.py:51] epoch 1021, training loss: 3703.83, average training loss: 4067.32, base loss: 4602.87
[INFO 2017-06-27 16:54:17,967 main.py:51] epoch 1022, training loss: 3704.53, average training loss: 4065.88, base loss: 4603.42
[INFO 2017-06-27 16:54:18,338 main.py:51] epoch 1023, training loss: 3565.78, average training loss: 4063.98, base loss: 4603.52
[INFO 2017-06-27 16:54:18,713 main.py:51] epoch 1024, training loss: 3500.63, average training loss: 4062.08, base loss: 4603.52
[INFO 2017-06-27 16:54:19,085 main.py:51] epoch 1025, training loss: 3608.55, average training loss: 4060.43, base loss: 4603.72
[INFO 2017-06-27 16:54:19,457 main.py:51] epoch 1026, training loss: 3581.05, average training loss: 4051.85, base loss: 4596.60
[INFO 2017-06-27 16:54:19,832 main.py:51] epoch 1027, training loss: 3130.07, average training loss: 4049.76, base loss: 4595.89
[INFO 2017-06-27 16:54:20,207 main.py:51] epoch 1028, training loss: 3627.94, average training loss: 4048.62, base loss: 4596.60
[INFO 2017-06-27 16:54:20,585 main.py:51] epoch 1029, training loss: 7190.32, average training loss: 4051.02, base loss: 4600.55
[INFO 2017-06-27 16:54:20,957 main.py:51] epoch 1030, training loss: 3508.27, average training loss: 4048.85, base loss: 4599.87
[INFO 2017-06-27 16:54:21,329 main.py:51] epoch 1031, training loss: 3787.61, average training loss: 4048.05, base loss: 4600.71
[INFO 2017-06-27 16:54:21,701 main.py:51] epoch 1032, training loss: 4031.55, average training loss: 4047.88, base loss: 4601.99
[INFO 2017-06-27 16:54:22,074 main.py:51] epoch 1033, training loss: 3268.12, average training loss: 4045.90, base loss: 4601.20
[INFO 2017-06-27 16:54:22,450 main.py:51] epoch 1034, training loss: 6898.87, average training loss: 4048.71, base loss: 4605.34
[INFO 2017-06-27 16:54:22,828 main.py:51] epoch 1035, training loss: 4195.94, average training loss: 4047.53, base loss: 4605.83
[INFO 2017-06-27 16:54:23,278 main.py:51] epoch 1036, training loss: 3231.55, average training loss: 4045.87, base loss: 4605.13
[INFO 2017-06-27 16:54:23,682 main.py:51] epoch 1037, training loss: 3766.62, average training loss: 4044.96, base loss: 4605.32
[INFO 2017-06-27 16:54:24,077 main.py:51] epoch 1038, training loss: 3371.67, average training loss: 4044.41, base loss: 4605.91
[INFO 2017-06-27 16:54:24,456 main.py:51] epoch 1039, training loss: 3498.93, average training loss: 4043.23, base loss: 4605.63
[INFO 2017-06-27 16:54:24,842 main.py:51] epoch 1040, training loss: 4063.91, average training loss: 4043.14, base loss: 4606.75
[INFO 2017-06-27 16:54:25,218 main.py:51] epoch 1041, training loss: 3620.94, average training loss: 4042.12, base loss: 4606.73
[INFO 2017-06-27 16:54:25,590 main.py:51] epoch 1042, training loss: 3896.42, average training loss: 4041.55, base loss: 4607.18
[INFO 2017-06-27 16:54:25,969 main.py:51] epoch 1043, training loss: 3528.75, average training loss: 4040.05, base loss: 4606.73
[INFO 2017-06-27 16:54:26,348 main.py:51] epoch 1044, training loss: 4356.12, average training loss: 4039.98, base loss: 4608.02
[INFO 2017-06-27 16:54:26,722 main.py:51] epoch 1045, training loss: 3410.20, average training loss: 4038.46, base loss: 4607.20
[INFO 2017-06-27 16:54:27,098 main.py:51] epoch 1046, training loss: 3376.11, average training loss: 4037.05, base loss: 4606.60
[INFO 2017-06-27 16:54:27,472 main.py:51] epoch 1047, training loss: 3900.25, average training loss: 4036.62, base loss: 4607.33
[INFO 2017-06-27 16:54:27,873 main.py:51] epoch 1048, training loss: 3677.92, average training loss: 4034.85, base loss: 4606.60
[INFO 2017-06-27 16:54:28,270 main.py:51] epoch 1049, training loss: 3959.17, average training loss: 4034.37, base loss: 4607.29
[INFO 2017-06-27 16:54:28,647 main.py:51] epoch 1050, training loss: 3228.36, average training loss: 4033.38, base loss: 4607.13
[INFO 2017-06-27 16:54:29,025 main.py:51] epoch 1051, training loss: 3347.31, average training loss: 4032.67, base loss: 4607.13
[INFO 2017-06-27 16:54:29,413 main.py:51] epoch 1052, training loss: 3698.54, average training loss: 4031.72, base loss: 4607.06
[INFO 2017-06-27 16:54:29,790 main.py:51] epoch 1053, training loss: 3553.83, average training loss: 4030.69, base loss: 4606.98
[INFO 2017-06-27 16:54:30,164 main.py:51] epoch 1054, training loss: 3250.08, average training loss: 4030.29, base loss: 4607.31
[INFO 2017-06-27 16:54:30,536 main.py:51] epoch 1055, training loss: 3213.97, average training loss: 4029.94, base loss: 4607.67
[INFO 2017-06-27 16:54:30,908 main.py:51] epoch 1056, training loss: 3278.51, average training loss: 4028.85, base loss: 4607.23
[INFO 2017-06-27 16:54:31,278 main.py:51] epoch 1057, training loss: 3507.81, average training loss: 4028.55, base loss: 4607.65
[INFO 2017-06-27 16:54:31,655 main.py:51] epoch 1058, training loss: 3482.89, average training loss: 4027.70, base loss: 4607.61
[INFO 2017-06-27 16:54:32,029 main.py:51] epoch 1059, training loss: 3702.05, average training loss: 4026.48, base loss: 4607.16
[INFO 2017-06-27 16:54:32,401 main.py:51] epoch 1060, training loss: 6902.61, average training loss: 4029.54, base loss: 4611.11
[INFO 2017-06-27 16:54:32,778 main.py:51] epoch 1061, training loss: 3742.60, average training loss: 4028.83, base loss: 4611.55
[INFO 2017-06-27 16:54:33,153 main.py:51] epoch 1062, training loss: 3407.32, average training loss: 4027.42, base loss: 4610.88
[INFO 2017-06-27 16:54:33,527 main.py:51] epoch 1063, training loss: 3436.97, average training loss: 4026.48, base loss: 4610.54
[INFO 2017-06-27 16:54:33,901 main.py:51] epoch 1064, training loss: 3638.31, average training loss: 4025.83, base loss: 4610.70
[INFO 2017-06-27 16:54:34,273 main.py:51] epoch 1065, training loss: 3908.26, average training loss: 4025.83, base loss: 4611.78
[INFO 2017-06-27 16:54:34,647 main.py:51] epoch 1066, training loss: 3541.43, average training loss: 4023.74, base loss: 4610.32
[INFO 2017-06-27 16:54:35,020 main.py:51] epoch 1067, training loss: 3492.66, average training loss: 4022.59, base loss: 4609.93
[INFO 2017-06-27 16:54:35,395 main.py:51] epoch 1068, training loss: 4200.31, average training loss: 4022.65, base loss: 4611.28
[INFO 2017-06-27 16:54:35,770 main.py:51] epoch 1069, training loss: 3486.31, average training loss: 4022.41, base loss: 4612.20
[INFO 2017-06-27 16:54:36,145 main.py:51] epoch 1070, training loss: 3373.62, average training loss: 4020.59, base loss: 4611.26
[INFO 2017-06-27 16:54:36,519 main.py:51] epoch 1071, training loss: 7190.88, average training loss: 4024.40, base loss: 4615.96
[INFO 2017-06-27 16:54:36,899 main.py:51] epoch 1072, training loss: 3131.07, average training loss: 4022.77, base loss: 4614.90
[INFO 2017-06-27 16:54:37,287 main.py:51] epoch 1073, training loss: 3711.55, average training loss: 4022.32, base loss: 4615.38
[INFO 2017-06-27 16:54:37,663 main.py:51] epoch 1074, training loss: 2947.03, average training loss: 4020.08, base loss: 4613.52
[INFO 2017-06-27 16:54:38,039 main.py:51] epoch 1075, training loss: 3614.91, average training loss: 4019.27, base loss: 4613.90
[INFO 2017-06-27 16:54:38,420 main.py:51] epoch 1076, training loss: 3325.32, average training loss: 4018.17, base loss: 4613.68
[INFO 2017-06-27 16:54:38,797 main.py:51] epoch 1077, training loss: 3303.21, average training loss: 4018.08, base loss: 4614.55
[INFO 2017-06-27 16:54:39,172 main.py:51] epoch 1078, training loss: 3565.77, average training loss: 4016.68, base loss: 4613.94
[INFO 2017-06-27 16:54:39,552 main.py:51] epoch 1079, training loss: 3480.29, average training loss: 4015.79, base loss: 4613.86
[INFO 2017-06-27 16:54:39,928 main.py:51] epoch 1080, training loss: 3390.09, average training loss: 4014.95, base loss: 4613.78
[INFO 2017-06-27 16:54:40,303 main.py:51] epoch 1081, training loss: 3457.08, average training loss: 4014.81, base loss: 4614.48
[INFO 2017-06-27 16:54:40,676 main.py:51] epoch 1082, training loss: 7036.40, average training loss: 4017.81, base loss: 4618.20
[INFO 2017-06-27 16:54:41,049 main.py:51] epoch 1083, training loss: 3646.01, average training loss: 4017.57, base loss: 4618.90
[INFO 2017-06-27 16:54:41,424 main.py:51] epoch 1084, training loss: 3115.62, average training loss: 4012.75, base loss: 4614.50
[INFO 2017-06-27 16:54:41,795 main.py:51] epoch 1085, training loss: 3550.38, average training loss: 4011.78, base loss: 4614.35
[INFO 2017-06-27 16:54:42,172 main.py:51] epoch 1086, training loss: 3507.78, average training loss: 4011.16, base loss: 4614.63
[INFO 2017-06-27 16:54:42,547 main.py:51] epoch 1087, training loss: 3746.22, average training loss: 4010.96, base loss: 4615.35
[INFO 2017-06-27 16:54:42,919 main.py:51] epoch 1088, training loss: 4228.54, average training loss: 4010.65, base loss: 4616.07
[INFO 2017-06-27 16:54:43,291 main.py:51] epoch 1089, training loss: 3087.97, average training loss: 4010.03, base loss: 4616.01
[INFO 2017-06-27 16:54:43,662 main.py:51] epoch 1090, training loss: 3229.31, average training loss: 4008.93, base loss: 4615.86
[INFO 2017-06-27 16:54:44,034 main.py:51] epoch 1091, training loss: 3973.73, average training loss: 4007.52, base loss: 4615.58
[INFO 2017-06-27 16:54:44,411 main.py:51] epoch 1092, training loss: 3798.46, average training loss: 4006.06, base loss: 4614.96
[INFO 2017-06-27 16:54:44,783 main.py:51] epoch 1093, training loss: 3592.01, average training loss: 4002.17, base loss: 4611.81
[INFO 2017-06-27 16:54:45,159 main.py:51] epoch 1094, training loss: 3072.15, average training loss: 3997.40, base loss: 4607.59
[INFO 2017-06-27 16:54:45,534 main.py:51] epoch 1095, training loss: 3728.17, average training loss: 3996.64, base loss: 4607.84
[INFO 2017-06-27 16:54:45,908 main.py:51] epoch 1096, training loss: 3679.85, average training loss: 3996.39, base loss: 4608.68
[INFO 2017-06-27 16:54:46,280 main.py:51] epoch 1097, training loss: 4108.68, average training loss: 3996.28, base loss: 4609.64
[INFO 2017-06-27 16:54:46,651 main.py:51] epoch 1098, training loss: 3913.44, average training loss: 3995.71, base loss: 4610.32
[INFO 2017-06-27 16:54:47,025 main.py:51] epoch 1099, training loss: 3328.23, average training loss: 3994.82, base loss: 4610.11
[INFO 2017-06-27 16:54:47,025 main.py:53] epoch 1099, testing
[INFO 2017-06-27 16:54:48,638 main.py:105] average testing loss: 3653.78, base loss: 4555.62
[INFO 2017-06-27 16:54:48,638 main.py:106] improve_loss: 901.83, improve_percent: 0.20
[INFO 2017-06-27 16:54:48,638 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 16:54:48,651 main.py:76] current best improved percent: 0.20
[INFO 2017-06-27 16:54:49,026 main.py:51] epoch 1100, training loss: 3409.86, average training loss: 3993.55, base loss: 4609.83
[INFO 2017-06-27 16:54:49,401 main.py:51] epoch 1101, training loss: 3659.75, average training loss: 3993.10, base loss: 4610.02
[INFO 2017-06-27 16:54:49,779 main.py:51] epoch 1102, training loss: 3467.26, average training loss: 3992.19, base loss: 4609.94
[INFO 2017-06-27 16:54:50,151 main.py:51] epoch 1103, training loss: 3880.35, average training loss: 3991.55, base loss: 4610.22
[INFO 2017-06-27 16:54:50,528 main.py:51] epoch 1104, training loss: 3181.05, average training loss: 3990.64, base loss: 4609.69
[INFO 2017-06-27 16:54:50,904 main.py:51] epoch 1105, training loss: 3218.41, average training loss: 3988.73, base loss: 4608.40
[INFO 2017-06-27 16:54:51,279 main.py:51] epoch 1106, training loss: 3181.38, average training loss: 3987.24, base loss: 4607.66
[INFO 2017-06-27 16:54:51,660 main.py:51] epoch 1107, training loss: 3548.73, average training loss: 3986.02, base loss: 4607.34
[INFO 2017-06-27 16:54:52,033 main.py:51] epoch 1108, training loss: 3545.91, average training loss: 3985.47, base loss: 4607.66
[INFO 2017-06-27 16:54:52,418 main.py:51] epoch 1109, training loss: 3224.81, average training loss: 3985.08, base loss: 4608.12
[INFO 2017-06-27 16:54:52,795 main.py:51] epoch 1110, training loss: 3607.49, average training loss: 3984.93, base loss: 4609.04
[INFO 2017-06-27 16:54:53,174 main.py:51] epoch 1111, training loss: 3495.77, average training loss: 3980.55, base loss: 4605.58
[INFO 2017-06-27 16:54:53,552 main.py:51] epoch 1112, training loss: 3524.99, average training loss: 3979.97, base loss: 4605.76
[INFO 2017-06-27 16:54:53,939 main.py:51] epoch 1113, training loss: 3426.59, average training loss: 3979.61, base loss: 4605.92
[INFO 2017-06-27 16:54:54,322 main.py:51] epoch 1114, training loss: 3217.08, average training loss: 3979.21, base loss: 4606.09
[INFO 2017-06-27 16:54:54,705 main.py:51] epoch 1115, training loss: 3128.48, average training loss: 3977.67, base loss: 4605.12
[INFO 2017-06-27 16:54:55,089 main.py:51] epoch 1116, training loss: 3344.91, average training loss: 3976.83, base loss: 4604.70
[INFO 2017-06-27 16:54:55,471 main.py:51] epoch 1117, training loss: 3476.97, average training loss: 3975.89, base loss: 4604.39
[INFO 2017-06-27 16:54:55,848 main.py:51] epoch 1118, training loss: 3583.72, average training loss: 3974.74, base loss: 4603.95
[INFO 2017-06-27 16:54:56,227 main.py:51] epoch 1119, training loss: 3833.84, average training loss: 3974.14, base loss: 4604.31
[INFO 2017-06-27 16:54:56,620 main.py:51] epoch 1120, training loss: 3368.14, average training loss: 3973.33, base loss: 4604.18
[INFO 2017-06-27 16:54:57,060 main.py:51] epoch 1121, training loss: 3292.90, average training loss: 3972.83, base loss: 4604.42
[INFO 2017-06-27 16:54:57,522 main.py:51] epoch 1122, training loss: 3372.48, average training loss: 3972.11, base loss: 4604.25
[INFO 2017-06-27 16:54:57,966 main.py:51] epoch 1123, training loss: 4495.98, average training loss: 3972.01, base loss: 4605.49
[INFO 2017-06-27 16:54:58,407 main.py:51] epoch 1124, training loss: 4212.62, average training loss: 3971.86, base loss: 4606.44
[INFO 2017-06-27 16:54:58,830 main.py:51] epoch 1125, training loss: 3266.81, average training loss: 3970.48, base loss: 4605.80
[INFO 2017-06-27 16:54:59,224 main.py:51] epoch 1126, training loss: 3345.66, average training loss: 3970.54, base loss: 4606.63
[INFO 2017-06-27 16:54:59,670 main.py:51] epoch 1127, training loss: 3117.51, average training loss: 3969.74, base loss: 4606.52
[INFO 2017-06-27 16:55:00,149 main.py:51] epoch 1128, training loss: 3805.87, average training loss: 3969.59, base loss: 4607.25
[INFO 2017-06-27 16:55:00,545 main.py:51] epoch 1129, training loss: 3879.47, average training loss: 3969.18, base loss: 4607.73
[INFO 2017-06-27 16:55:00,918 main.py:51] epoch 1130, training loss: 3201.29, average training loss: 3967.58, base loss: 4606.67
[INFO 2017-06-27 16:55:01,296 main.py:51] epoch 1131, training loss: 3415.68, average training loss: 3966.31, base loss: 4605.87
[INFO 2017-06-27 16:55:01,669 main.py:51] epoch 1132, training loss: 3225.23, average training loss: 3965.49, base loss: 4605.68
[INFO 2017-06-27 16:55:02,063 main.py:51] epoch 1133, training loss: 4062.83, average training loss: 3964.51, base loss: 4605.61
[INFO 2017-06-27 16:55:02,456 main.py:51] epoch 1134, training loss: 3658.88, average training loss: 3964.14, base loss: 4605.90
[INFO 2017-06-27 16:55:02,834 main.py:51] epoch 1135, training loss: 3676.21, average training loss: 3963.29, base loss: 4606.01
[INFO 2017-06-27 16:55:03,279 main.py:51] epoch 1136, training loss: 3684.52, average training loss: 3962.57, base loss: 4606.25
[INFO 2017-06-27 16:55:03,697 main.py:51] epoch 1137, training loss: 3378.45, average training loss: 3961.53, base loss: 4605.86
[INFO 2017-06-27 16:55:04,080 main.py:51] epoch 1138, training loss: 3455.63, average training loss: 3961.15, base loss: 4606.05
[INFO 2017-06-27 16:55:04,460 main.py:51] epoch 1139, training loss: 3552.46, average training loss: 3960.26, base loss: 4606.11
[INFO 2017-06-27 16:55:04,835 main.py:51] epoch 1140, training loss: 3436.44, average training loss: 3959.78, base loss: 4606.40
[INFO 2017-06-27 16:55:05,210 main.py:51] epoch 1141, training loss: 3528.75, average training loss: 3958.57, base loss: 4606.10
[INFO 2017-06-27 16:55:05,583 main.py:51] epoch 1142, training loss: 3106.53, average training loss: 3956.47, base loss: 4604.40
[INFO 2017-06-27 16:55:05,957 main.py:51] epoch 1143, training loss: 3398.71, average training loss: 3955.54, base loss: 4604.04
[INFO 2017-06-27 16:55:06,363 main.py:51] epoch 1144, training loss: 3825.13, average training loss: 3955.41, base loss: 4604.89
[INFO 2017-06-27 16:55:06,739 main.py:51] epoch 1145, training loss: 4140.12, average training loss: 3955.39, base loss: 4606.02
[INFO 2017-06-27 16:55:07,117 main.py:51] epoch 1146, training loss: 3268.39, average training loss: 3953.99, base loss: 4605.24
[INFO 2017-06-27 16:55:07,495 main.py:51] epoch 1147, training loss: 3297.98, average training loss: 3953.06, base loss: 4605.23
[INFO 2017-06-27 16:55:07,871 main.py:51] epoch 1148, training loss: 4117.11, average training loss: 3953.27, base loss: 4606.81
[INFO 2017-06-27 16:55:08,246 main.py:51] epoch 1149, training loss: 3805.16, average training loss: 3953.13, base loss: 4607.65
[INFO 2017-06-27 16:55:08,619 main.py:51] epoch 1150, training loss: 3566.81, average training loss: 3952.01, base loss: 4607.44
[INFO 2017-06-27 16:55:09,001 main.py:51] epoch 1151, training loss: 3233.20, average training loss: 3951.48, base loss: 4607.57
[INFO 2017-06-27 16:55:09,372 main.py:51] epoch 1152, training loss: 3530.44, average training loss: 3950.92, base loss: 4607.89
[INFO 2017-06-27 16:55:09,748 main.py:51] epoch 1153, training loss: 3649.61, average training loss: 3950.18, base loss: 4607.81
[INFO 2017-06-27 16:55:10,123 main.py:51] epoch 1154, training loss: 3210.76, average training loss: 3949.37, base loss: 4607.81
[INFO 2017-06-27 16:55:10,498 main.py:51] epoch 1155, training loss: 3679.71, average training loss: 3948.40, base loss: 4607.38
[INFO 2017-06-27 16:55:10,873 main.py:51] epoch 1156, training loss: 3146.63, average training loss: 3947.14, base loss: 4606.57
[INFO 2017-06-27 16:55:11,249 main.py:51] epoch 1157, training loss: 3322.94, average training loss: 3945.81, base loss: 4605.95
[INFO 2017-06-27 16:55:11,629 main.py:51] epoch 1158, training loss: 6764.32, average training loss: 3947.93, base loss: 4608.86
[INFO 2017-06-27 16:55:12,002 main.py:51] epoch 1159, training loss: 3648.52, average training loss: 3947.48, base loss: 4609.52
[INFO 2017-06-27 16:55:12,373 main.py:51] epoch 1160, training loss: 3713.54, average training loss: 3947.09, base loss: 4610.05
[INFO 2017-06-27 16:55:12,745 main.py:51] epoch 1161, training loss: 3422.33, average training loss: 3946.00, base loss: 4609.35
[INFO 2017-06-27 16:55:13,118 main.py:51] epoch 1162, training loss: 4080.68, average training loss: 3945.56, base loss: 4609.92
[INFO 2017-06-27 16:55:13,494 main.py:51] epoch 1163, training loss: 3454.78, average training loss: 3944.87, base loss: 4610.07
[INFO 2017-06-27 16:55:13,869 main.py:51] epoch 1164, training loss: 3500.93, average training loss: 3943.85, base loss: 4609.70
[INFO 2017-06-27 16:55:14,248 main.py:51] epoch 1165, training loss: 3407.43, average training loss: 3943.12, base loss: 4609.28
[INFO 2017-06-27 16:55:14,623 main.py:51] epoch 1166, training loss: 3366.88, average training loss: 3942.18, base loss: 4608.87
[INFO 2017-06-27 16:55:14,999 main.py:51] epoch 1167, training loss: 3265.21, average training loss: 3940.89, base loss: 4608.34
[INFO 2017-06-27 16:55:15,375 main.py:51] epoch 1168, training loss: 3728.76, average training loss: 3940.53, base loss: 4608.71
[INFO 2017-06-27 16:55:15,752 main.py:51] epoch 1169, training loss: 3564.24, average training loss: 3940.62, base loss: 4609.61
[INFO 2017-06-27 16:55:16,129 main.py:51] epoch 1170, training loss: 3350.95, average training loss: 3939.97, base loss: 4609.53
[INFO 2017-06-27 16:55:16,504 main.py:51] epoch 1171, training loss: 7612.65, average training loss: 3943.13, base loss: 4613.55
[INFO 2017-06-27 16:55:16,880 main.py:51] epoch 1172, training loss: 7457.39, average training loss: 3946.29, base loss: 4617.59
[INFO 2017-06-27 16:55:17,251 main.py:51] epoch 1173, training loss: 3354.06, average training loss: 3945.24, base loss: 4616.99
[INFO 2017-06-27 16:55:17,626 main.py:51] epoch 1174, training loss: 3543.07, average training loss: 3940.96, base loss: 4613.52
[INFO 2017-06-27 16:55:17,997 main.py:51] epoch 1175, training loss: 3726.75, average training loss: 3941.21, base loss: 4614.45
[INFO 2017-06-27 16:55:18,374 main.py:51] epoch 1176, training loss: 3682.49, average training loss: 3939.60, base loss: 4613.43
[INFO 2017-06-27 16:55:18,750 main.py:51] epoch 1177, training loss: 3657.75, average training loss: 3939.07, base loss: 4613.74
[INFO 2017-06-27 16:55:19,123 main.py:51] epoch 1178, training loss: 3096.00, average training loss: 3938.18, base loss: 4613.25
[INFO 2017-06-27 16:55:19,498 main.py:51] epoch 1179, training loss: 3588.21, average training loss: 3938.32, base loss: 4613.87
[INFO 2017-06-27 16:55:19,870 main.py:51] epoch 1180, training loss: 3233.02, average training loss: 3937.41, base loss: 4613.59
[INFO 2017-06-27 16:55:20,244 main.py:51] epoch 1181, training loss: 3808.75, average training loss: 3937.29, base loss: 4614.27
[INFO 2017-06-27 16:55:20,622 main.py:51] epoch 1182, training loss: 3301.23, average training loss: 3936.75, base loss: 4614.45
[INFO 2017-06-27 16:55:20,998 main.py:51] epoch 1183, training loss: 3443.89, average training loss: 3936.47, base loss: 4615.09
[INFO 2017-06-27 16:55:21,376 main.py:51] epoch 1184, training loss: 7088.93, average training loss: 3938.27, base loss: 4617.53
[INFO 2017-06-27 16:55:21,843 main.py:51] epoch 1185, training loss: 4352.61, average training loss: 3938.17, base loss: 4618.71
[INFO 2017-06-27 16:55:22,235 main.py:51] epoch 1186, training loss: 3673.54, average training loss: 3937.01, base loss: 4618.08
[INFO 2017-06-27 16:55:22,669 main.py:51] epoch 1187, training loss: 3238.25, average training loss: 3935.96, base loss: 4617.65
[INFO 2017-06-27 16:55:23,082 main.py:51] epoch 1188, training loss: 3954.83, average training loss: 3936.02, base loss: 4618.38
[INFO 2017-06-27 16:55:23,461 main.py:51] epoch 1189, training loss: 3885.84, average training loss: 3935.82, base loss: 4619.15
[INFO 2017-06-27 16:55:23,854 main.py:51] epoch 1190, training loss: 3346.52, average training loss: 3935.44, base loss: 4619.33
[INFO 2017-06-27 16:55:24,316 main.py:51] epoch 1191, training loss: 3190.58, average training loss: 3934.48, base loss: 4618.89
[INFO 2017-06-27 16:55:24,716 main.py:51] epoch 1192, training loss: 4153.13, average training loss: 3934.12, base loss: 4619.71
[INFO 2017-06-27 16:55:25,129 main.py:51] epoch 1193, training loss: 3514.59, average training loss: 3933.63, base loss: 4619.93
[INFO 2017-06-27 16:55:25,608 main.py:51] epoch 1194, training loss: 3357.05, average training loss: 3932.92, base loss: 4619.81
[INFO 2017-06-27 16:55:26,066 main.py:51] epoch 1195, training loss: 3028.50, average training loss: 3931.93, base loss: 4619.04
[INFO 2017-06-27 16:55:26,479 main.py:51] epoch 1196, training loss: 3571.74, average training loss: 3931.69, base loss: 4619.62
[INFO 2017-06-27 16:55:26,859 main.py:51] epoch 1197, training loss: 3912.65, average training loss: 3931.71, base loss: 4620.40
[INFO 2017-06-27 16:55:27,324 main.py:51] epoch 1198, training loss: 3294.21, average training loss: 3930.83, base loss: 4620.10
[INFO 2017-06-27 16:55:27,707 main.py:51] epoch 1199, training loss: 3316.90, average training loss: 3930.11, base loss: 4620.00
[INFO 2017-06-27 16:55:27,708 main.py:53] epoch 1199, testing
[INFO 2017-06-27 16:55:29,557 main.py:105] average testing loss: 3607.84, base loss: 4567.29
[INFO 2017-06-27 16:55:29,557 main.py:106] improve_loss: 959.46, improve_percent: 0.21
[INFO 2017-06-27 16:55:29,558 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 16:55:29,571 main.py:76] current best improved percent: 0.21
[INFO 2017-06-27 16:55:30,044 main.py:51] epoch 1200, training loss: 6776.96, average training loss: 3932.54, base loss: 4623.07
[INFO 2017-06-27 16:55:30,428 main.py:51] epoch 1201, training loss: 3119.58, average training loss: 3930.83, base loss: 4621.95
[INFO 2017-06-27 16:55:30,819 main.py:51] epoch 1202, training loss: 3987.74, average training loss: 3930.72, base loss: 4622.83
[INFO 2017-06-27 16:55:31,197 main.py:51] epoch 1203, training loss: 6967.14, average training loss: 3932.90, base loss: 4625.42
[INFO 2017-06-27 16:55:31,572 main.py:51] epoch 1204, training loss: 3190.95, average training loss: 3931.73, base loss: 4624.91
[INFO 2017-06-27 16:55:31,952 main.py:51] epoch 1205, training loss: 2996.89, average training loss: 3930.46, base loss: 4624.02
[INFO 2017-06-27 16:55:32,329 main.py:51] epoch 1206, training loss: 3729.83, average training loss: 3929.45, base loss: 4623.76
[INFO 2017-06-27 16:55:32,705 main.py:51] epoch 1207, training loss: 3317.81, average training loss: 3928.53, base loss: 4623.44
[INFO 2017-06-27 16:55:33,087 main.py:51] epoch 1208, training loss: 3138.07, average training loss: 3927.51, base loss: 4622.72
[INFO 2017-06-27 16:55:33,505 main.py:51] epoch 1209, training loss: 3820.89, average training loss: 3928.00, base loss: 4624.34
[INFO 2017-06-27 16:55:33,884 main.py:51] epoch 1210, training loss: 3956.80, average training loss: 3927.58, base loss: 4624.96
[INFO 2017-06-27 16:55:34,262 main.py:51] epoch 1211, training loss: 3835.42, average training loss: 3927.43, base loss: 4625.87
[INFO 2017-06-27 16:55:34,642 main.py:51] epoch 1212, training loss: 3276.68, average training loss: 3926.55, base loss: 4625.28
[INFO 2017-06-27 16:55:35,030 main.py:51] epoch 1213, training loss: 4198.22, average training loss: 3925.42, base loss: 4625.25
[INFO 2017-06-27 16:55:35,411 main.py:51] epoch 1214, training loss: 3471.96, average training loss: 3924.70, base loss: 4625.18
[INFO 2017-06-27 16:55:35,798 main.py:51] epoch 1215, training loss: 3690.01, average training loss: 3924.65, base loss: 4626.09
[INFO 2017-06-27 16:55:36,190 main.py:51] epoch 1216, training loss: 3439.18, average training loss: 3923.77, base loss: 4625.65
[INFO 2017-06-27 16:55:36,587 main.py:51] epoch 1217, training loss: 2995.67, average training loss: 3922.51, base loss: 4624.77
[INFO 2017-06-27 16:55:36,977 main.py:51] epoch 1218, training loss: 3886.55, average training loss: 3922.70, base loss: 4625.83
[INFO 2017-06-27 16:55:37,374 main.py:51] epoch 1219, training loss: 3814.00, average training loss: 3922.32, base loss: 4626.44
[INFO 2017-06-27 16:55:37,819 main.py:51] epoch 1220, training loss: 3490.86, average training loss: 3918.23, base loss: 4623.29
[INFO 2017-06-27 16:55:38,257 main.py:51] epoch 1221, training loss: 3382.84, average training loss: 3917.81, base loss: 4623.40
[INFO 2017-06-27 16:55:38,726 main.py:51] epoch 1222, training loss: 3405.30, average training loss: 3913.76, base loss: 4619.77
[INFO 2017-06-27 16:55:39,180 main.py:51] epoch 1223, training loss: 3890.88, average training loss: 3914.19, base loss: 4620.89
[INFO 2017-06-27 16:55:39,585 main.py:51] epoch 1224, training loss: 3582.51, average training loss: 3913.76, base loss: 4621.06
[INFO 2017-06-27 16:55:40,009 main.py:51] epoch 1225, training loss: 3592.39, average training loss: 3913.78, base loss: 4621.75
[INFO 2017-06-27 16:55:40,464 main.py:51] epoch 1226, training loss: 3257.41, average training loss: 3913.21, base loss: 4621.80
[INFO 2017-06-27 16:55:40,914 main.py:51] epoch 1227, training loss: 3453.77, average training loss: 3912.67, base loss: 4621.84
[INFO 2017-06-27 16:55:41,301 main.py:51] epoch 1228, training loss: 3116.79, average training loss: 3912.21, base loss: 4621.87
[INFO 2017-06-27 16:55:41,680 main.py:51] epoch 1229, training loss: 3000.92, average training loss: 3910.37, base loss: 4620.37
[INFO 2017-06-27 16:55:42,056 main.py:51] epoch 1230, training loss: 3616.45, average training loss: 3909.69, base loss: 4620.45
[INFO 2017-06-27 16:55:42,429 main.py:51] epoch 1231, training loss: 3571.76, average training loss: 3909.16, base loss: 4620.56
[INFO 2017-06-27 16:55:42,802 main.py:51] epoch 1232, training loss: 3514.46, average training loss: 3908.80, base loss: 4620.91
[INFO 2017-06-27 16:55:43,178 main.py:51] epoch 1233, training loss: 3638.04, average training loss: 3908.25, base loss: 4621.21
[INFO 2017-06-27 16:55:43,577 main.py:51] epoch 1234, training loss: 3139.08, average training loss: 3907.70, base loss: 4621.10
[INFO 2017-06-27 16:55:43,960 main.py:51] epoch 1235, training loss: 3525.13, average training loss: 3906.89, base loss: 4620.68
[INFO 2017-06-27 16:55:44,336 main.py:51] epoch 1236, training loss: 6477.93, average training loss: 3909.34, base loss: 4623.37
[INFO 2017-06-27 16:55:44,713 main.py:51] epoch 1237, training loss: 3267.72, average training loss: 3908.23, base loss: 4622.50
[INFO 2017-06-27 16:55:45,086 main.py:51] epoch 1238, training loss: 3589.48, average training loss: 3904.71, base loss: 4619.61
[INFO 2017-06-27 16:55:45,461 main.py:51] epoch 1239, training loss: 3766.76, average training loss: 3903.84, base loss: 4619.38
[INFO 2017-06-27 16:55:45,836 main.py:51] epoch 1240, training loss: 3354.71, average training loss: 3902.87, base loss: 4618.92
[INFO 2017-06-27 16:55:46,208 main.py:51] epoch 1241, training loss: 3090.91, average training loss: 3901.76, base loss: 4618.35
[INFO 2017-06-27 16:55:46,587 main.py:51] epoch 1242, training loss: 3096.33, average training loss: 3900.89, base loss: 4617.76
[INFO 2017-06-27 16:55:46,962 main.py:51] epoch 1243, training loss: 3520.97, average training loss: 3900.37, base loss: 4617.93
[INFO 2017-06-27 16:55:47,343 main.py:51] epoch 1244, training loss: 3516.17, average training loss: 3900.03, base loss: 4618.04
[INFO 2017-06-27 16:55:47,717 main.py:51] epoch 1245, training loss: 3963.65, average training loss: 3899.83, base loss: 4618.81
[INFO 2017-06-27 16:55:48,089 main.py:51] epoch 1246, training loss: 3187.58, average training loss: 3899.28, base loss: 4618.72
[INFO 2017-06-27 16:55:48,461 main.py:51] epoch 1247, training loss: 3653.74, average training loss: 3899.29, base loss: 4619.57
[INFO 2017-06-27 16:55:48,830 main.py:51] epoch 1248, training loss: 3335.89, average training loss: 3898.64, base loss: 4619.59
[INFO 2017-06-27 16:55:49,203 main.py:51] epoch 1249, training loss: 3223.30, average training loss: 3897.67, base loss: 4619.02
[INFO 2017-06-27 16:55:49,577 main.py:51] epoch 1250, training loss: 3393.70, average training loss: 3896.42, base loss: 4618.61
[INFO 2017-06-27 16:55:49,948 main.py:51] epoch 1251, training loss: 3395.90, average training loss: 3895.59, base loss: 4618.24
[INFO 2017-06-27 16:55:50,319 main.py:51] epoch 1252, training loss: 3758.68, average training loss: 3895.38, base loss: 4618.52
[INFO 2017-06-27 16:55:50,693 main.py:51] epoch 1253, training loss: 3567.84, average training loss: 3895.10, base loss: 4618.90
[INFO 2017-06-27 16:55:51,065 main.py:51] epoch 1254, training loss: 3688.90, average training loss: 3895.35, base loss: 4619.88
[INFO 2017-06-27 16:55:51,435 main.py:51] epoch 1255, training loss: 3400.21, average training loss: 3894.93, base loss: 4619.79
[INFO 2017-06-27 16:55:51,812 main.py:51] epoch 1256, training loss: 3699.41, average training loss: 3890.81, base loss: 4616.65
[INFO 2017-06-27 16:55:52,192 main.py:51] epoch 1257, training loss: 3427.89, average training loss: 3889.98, base loss: 4616.74
[INFO 2017-06-27 16:55:52,573 main.py:51] epoch 1258, training loss: 3718.76, average training loss: 3889.72, base loss: 4617.32
[INFO 2017-06-27 16:55:52,948 main.py:51] epoch 1259, training loss: 3304.01, average training loss: 3889.34, base loss: 4617.19
[INFO 2017-06-27 16:55:53,323 main.py:51] epoch 1260, training loss: 3450.20, average training loss: 3888.69, base loss: 4617.05
[INFO 2017-06-27 16:55:53,777 main.py:51] epoch 1261, training loss: 3597.25, average training loss: 3887.53, base loss: 4616.35
[INFO 2017-06-27 16:55:54,154 main.py:51] epoch 1262, training loss: 3776.06, average training loss: 3887.30, base loss: 4617.11
[INFO 2017-06-27 16:55:54,538 main.py:51] epoch 1263, training loss: 3717.98, average training loss: 3887.23, base loss: 4617.91
[INFO 2017-06-27 16:55:54,914 main.py:51] epoch 1264, training loss: 3346.54, average training loss: 3886.29, base loss: 4617.55
[INFO 2017-06-27 16:55:55,388 main.py:51] epoch 1265, training loss: 3533.45, average training loss: 3885.34, base loss: 4617.29
[INFO 2017-06-27 16:55:55,771 main.py:51] epoch 1266, training loss: 3314.70, average training loss: 3883.94, base loss: 4615.97
[INFO 2017-06-27 16:55:56,221 main.py:51] epoch 1267, training loss: 3198.97, average training loss: 3883.12, base loss: 4615.62
[INFO 2017-06-27 16:55:56,631 main.py:51] epoch 1268, training loss: 3727.05, average training loss: 3883.25, base loss: 4616.54
[INFO 2017-06-27 16:55:57,044 main.py:51] epoch 1269, training loss: 3040.21, average training loss: 3882.58, base loss: 4616.31
[INFO 2017-06-27 16:55:57,451 main.py:51] epoch 1270, training loss: 3490.06, average training loss: 3882.42, base loss: 4616.88
[INFO 2017-06-27 16:55:57,829 main.py:51] epoch 1271, training loss: 3852.50, average training loss: 3882.21, base loss: 4617.55
[INFO 2017-06-27 16:55:58,205 main.py:51] epoch 1272, training loss: 7142.31, average training loss: 3884.84, base loss: 4620.51
[INFO 2017-06-27 16:55:58,586 main.py:51] epoch 1273, training loss: 3739.14, average training loss: 3884.64, base loss: 4620.81
[INFO 2017-06-27 16:55:58,958 main.py:51] epoch 1274, training loss: 3537.53, average training loss: 3884.12, base loss: 4620.78
[INFO 2017-06-27 16:55:59,358 main.py:51] epoch 1275, training loss: 3240.68, average training loss: 3883.57, base loss: 4620.85
[INFO 2017-06-27 16:55:59,757 main.py:51] epoch 1276, training loss: 3238.61, average training loss: 3882.22, base loss: 4619.90
[INFO 2017-06-27 16:56:00,138 main.py:51] epoch 1277, training loss: 3936.00, average training loss: 3882.26, base loss: 4620.71
[INFO 2017-06-27 16:56:00,576 main.py:51] epoch 1278, training loss: 3590.39, average training loss: 3881.51, base loss: 4620.68
[INFO 2017-06-27 16:56:01,029 main.py:51] epoch 1279, training loss: 3805.96, average training loss: 3879.95, base loss: 4619.69
[INFO 2017-06-27 16:56:01,409 main.py:51] epoch 1280, training loss: 3183.63, average training loss: 3879.21, base loss: 4619.31
[INFO 2017-06-27 16:56:01,783 main.py:51] epoch 1281, training loss: 3351.38, average training loss: 3878.57, base loss: 4619.28
[INFO 2017-06-27 16:56:02,251 main.py:51] epoch 1282, training loss: 3324.61, average training loss: 3878.52, base loss: 4619.61
[INFO 2017-06-27 16:56:02,636 main.py:51] epoch 1283, training loss: 3071.92, average training loss: 3877.74, base loss: 4619.29
[INFO 2017-06-27 16:56:03,013 main.py:51] epoch 1284, training loss: 3429.29, average training loss: 3876.63, base loss: 4618.46
[INFO 2017-06-27 16:56:03,389 main.py:51] epoch 1285, training loss: 3151.26, average training loss: 3875.97, base loss: 4618.15
[INFO 2017-06-27 16:56:03,762 main.py:51] epoch 1286, training loss: 3802.87, average training loss: 3875.73, base loss: 4618.75
[INFO 2017-06-27 16:56:04,142 main.py:51] epoch 1287, training loss: 3620.03, average training loss: 3874.71, base loss: 4618.39
[INFO 2017-06-27 16:56:04,520 main.py:51] epoch 1288, training loss: 3358.77, average training loss: 3873.91, base loss: 4618.16
[INFO 2017-06-27 16:56:04,895 main.py:51] epoch 1289, training loss: 2987.20, average training loss: 3872.68, base loss: 4617.23
[INFO 2017-06-27 16:56:05,271 main.py:51] epoch 1290, training loss: 7200.42, average training loss: 3875.99, base loss: 4621.20
[INFO 2017-06-27 16:56:05,647 main.py:51] epoch 1291, training loss: 3509.45, average training loss: 3875.03, base loss: 4620.36
[INFO 2017-06-27 16:56:06,111 main.py:51] epoch 1292, training loss: 3916.82, average training loss: 3874.96, base loss: 4621.09
[INFO 2017-06-27 16:56:06,490 main.py:51] epoch 1293, training loss: 6502.77, average training loss: 3877.50, base loss: 4623.89
[INFO 2017-06-27 16:56:06,873 main.py:51] epoch 1294, training loss: 3630.16, average training loss: 3876.85, base loss: 4624.26
[INFO 2017-06-27 16:56:07,251 main.py:51] epoch 1295, training loss: 3270.37, average training loss: 3876.11, base loss: 4623.90
[INFO 2017-06-27 16:56:07,626 main.py:51] epoch 1296, training loss: 3518.75, average training loss: 3875.26, base loss: 4623.71
[INFO 2017-06-27 16:56:08,001 main.py:51] epoch 1297, training loss: 3144.76, average training loss: 3874.29, base loss: 4623.23
[INFO 2017-06-27 16:56:08,377 main.py:51] epoch 1298, training loss: 3424.49, average training loss: 3873.65, base loss: 4623.04
[INFO 2017-06-27 16:56:08,753 main.py:51] epoch 1299, training loss: 3485.62, average training loss: 3873.06, base loss: 4623.02
[INFO 2017-06-27 16:56:08,753 main.py:53] epoch 1299, testing
[INFO 2017-06-27 16:56:10,368 main.py:105] average testing loss: 3459.06, base loss: 4309.95
[INFO 2017-06-27 16:56:10,369 main.py:106] improve_loss: 850.89, improve_percent: 0.20
[INFO 2017-06-27 16:56:10,369 main.py:76] current best improved percent: 0.21
[INFO 2017-06-27 16:56:10,742 main.py:51] epoch 1300, training loss: 3967.82, average training loss: 3873.14, base loss: 4623.80
[INFO 2017-06-27 16:56:11,118 main.py:51] epoch 1301, training loss: 3364.05, average training loss: 3872.56, base loss: 4623.59
[INFO 2017-06-27 16:56:11,493 main.py:51] epoch 1302, training loss: 3005.71, average training loss: 3871.49, base loss: 4622.78
[INFO 2017-06-27 16:56:11,870 main.py:51] epoch 1303, training loss: 3475.19, average training loss: 3870.48, base loss: 4622.14
[INFO 2017-06-27 16:56:12,246 main.py:51] epoch 1304, training loss: 2972.87, average training loss: 3869.01, base loss: 4620.89
[INFO 2017-06-27 16:56:12,715 main.py:51] epoch 1305, training loss: 3187.92, average training loss: 3867.33, base loss: 4619.42
[INFO 2017-06-27 16:56:13,101 main.py:51] epoch 1306, training loss: 3559.04, average training loss: 3866.85, base loss: 4619.19
[INFO 2017-06-27 16:56:13,480 main.py:51] epoch 1307, training loss: 3497.92, average training loss: 3866.71, base loss: 4620.05
[INFO 2017-06-27 16:56:13,862 main.py:51] epoch 1308, training loss: 3029.91, average training loss: 3865.46, base loss: 4618.97
[INFO 2017-06-27 16:56:14,244 main.py:51] epoch 1309, training loss: 3301.58, average training loss: 3863.99, base loss: 4617.79
[INFO 2017-06-27 16:56:14,631 main.py:51] epoch 1310, training loss: 3377.15, average training loss: 3859.73, base loss: 4613.84
[INFO 2017-06-27 16:56:15,003 main.py:51] epoch 1311, training loss: 3410.31, average training loss: 3859.05, base loss: 4613.57
[INFO 2017-06-27 16:56:15,379 main.py:51] epoch 1312, training loss: 3229.16, average training loss: 3858.77, base loss: 4613.73
[INFO 2017-06-27 16:56:15,749 main.py:51] epoch 1313, training loss: 3500.63, average training loss: 3858.38, base loss: 4613.90
[INFO 2017-06-27 16:56:16,124 main.py:51] epoch 1314, training loss: 3286.38, average training loss: 3858.33, base loss: 4614.33
[INFO 2017-06-27 16:56:16,495 main.py:51] epoch 1315, training loss: 3211.41, average training loss: 3856.94, base loss: 4613.28
[INFO 2017-06-27 16:56:16,873 main.py:51] epoch 1316, training loss: 3784.97, average training loss: 3856.19, base loss: 4613.32
[INFO 2017-06-27 16:56:17,243 main.py:51] epoch 1317, training loss: 3447.14, average training loss: 3852.04, base loss: 4609.80
[INFO 2017-06-27 16:56:17,618 main.py:51] epoch 1318, training loss: 7017.54, average training loss: 3855.48, base loss: 4614.14
[INFO 2017-06-27 16:56:17,992 main.py:51] epoch 1319, training loss: 3615.64, average training loss: 3854.48, base loss: 4613.55
[INFO 2017-06-27 16:56:18,367 main.py:51] epoch 1320, training loss: 3448.14, average training loss: 3854.11, base loss: 4613.80
[INFO 2017-06-27 16:56:18,747 main.py:51] epoch 1321, training loss: 4026.81, average training loss: 3853.89, base loss: 4614.16
[INFO 2017-06-27 16:56:19,116 main.py:51] epoch 1322, training loss: 3267.51, average training loss: 3853.36, base loss: 4614.07
[INFO 2017-06-27 16:56:19,489 main.py:51] epoch 1323, training loss: 3603.33, average training loss: 3852.66, base loss: 4613.80
[INFO 2017-06-27 16:56:19,864 main.py:51] epoch 1324, training loss: 3109.07, average training loss: 3851.96, base loss: 4613.38
[INFO 2017-06-27 16:56:20,240 main.py:51] epoch 1325, training loss: 3626.09, average training loss: 3851.60, base loss: 4613.81
[INFO 2017-06-27 16:56:20,611 main.py:51] epoch 1326, training loss: 3328.46, average training loss: 3850.96, base loss: 4613.66
[INFO 2017-06-27 16:56:20,986 main.py:51] epoch 1327, training loss: 3420.94, average training loss: 3850.51, base loss: 4613.59
[INFO 2017-06-27 16:56:21,362 main.py:51] epoch 1328, training loss: 3734.90, average training loss: 3850.31, base loss: 4614.17
[INFO 2017-06-27 16:56:21,735 main.py:51] epoch 1329, training loss: 3125.10, average training loss: 3845.92, base loss: 4610.00
[INFO 2017-06-27 16:56:22,108 main.py:51] epoch 1330, training loss: 2983.54, average training loss: 3844.57, base loss: 4608.60
[INFO 2017-06-27 16:56:22,480 main.py:51] epoch 1331, training loss: 3147.42, average training loss: 3844.03, base loss: 4608.61
[INFO 2017-06-27 16:56:22,851 main.py:51] epoch 1332, training loss: 3569.41, average training loss: 3843.09, base loss: 4608.11
[INFO 2017-06-27 16:56:23,222 main.py:51] epoch 1333, training loss: 2748.64, average training loss: 3841.84, base loss: 4607.04
[INFO 2017-06-27 16:56:23,597 main.py:51] epoch 1334, training loss: 3511.48, average training loss: 3841.15, base loss: 4606.60
[INFO 2017-06-27 16:56:23,968 main.py:51] epoch 1335, training loss: 3701.88, average training loss: 3840.96, base loss: 4607.00
[INFO 2017-06-27 16:56:24,342 main.py:51] epoch 1336, training loss: 3463.93, average training loss: 3841.13, base loss: 4607.85
[INFO 2017-06-27 16:56:24,716 main.py:51] epoch 1337, training loss: 3636.45, average training loss: 3840.75, base loss: 4608.38
[INFO 2017-06-27 16:56:25,091 main.py:51] epoch 1338, training loss: 3231.15, average training loss: 3839.65, base loss: 4607.75
[INFO 2017-06-27 16:56:25,468 main.py:51] epoch 1339, training loss: 3983.17, average training loss: 3839.69, base loss: 4608.84
[INFO 2017-06-27 16:56:25,843 main.py:51] epoch 1340, training loss: 3465.46, average training loss: 3838.97, base loss: 4608.54
[INFO 2017-06-27 16:56:26,224 main.py:51] epoch 1341, training loss: 7290.54, average training loss: 3842.12, base loss: 4612.19
[INFO 2017-06-27 16:56:26,599 main.py:51] epoch 1342, training loss: 3669.20, average training loss: 3842.01, base loss: 4612.69
[INFO 2017-06-27 16:56:26,977 main.py:51] epoch 1343, training loss: 3483.73, average training loss: 3841.73, base loss: 4613.18
[INFO 2017-06-27 16:56:27,352 main.py:51] epoch 1344, training loss: 3662.27, average training loss: 3841.12, base loss: 4613.27
[INFO 2017-06-27 16:56:27,724 main.py:51] epoch 1345, training loss: 3281.77, average training loss: 3839.72, base loss: 4612.21
[INFO 2017-06-27 16:56:28,094 main.py:51] epoch 1346, training loss: 3447.00, average training loss: 3839.10, base loss: 4611.73
[INFO 2017-06-27 16:56:28,469 main.py:51] epoch 1347, training loss: 3918.34, average training loss: 3838.90, base loss: 4612.03
[INFO 2017-06-27 16:56:28,845 main.py:51] epoch 1348, training loss: 3184.29, average training loss: 3838.38, base loss: 4612.24
[INFO 2017-06-27 16:56:29,238 main.py:51] epoch 1349, training loss: 3591.83, average training loss: 3837.41, base loss: 4611.50
[INFO 2017-06-27 16:56:29,646 main.py:51] epoch 1350, training loss: 4038.89, average training loss: 3837.10, base loss: 4612.41
[INFO 2017-06-27 16:56:30,026 main.py:51] epoch 1351, training loss: 3293.49, average training loss: 3836.27, base loss: 4611.74
[INFO 2017-06-27 16:56:30,410 main.py:51] epoch 1352, training loss: 4083.30, average training loss: 3836.64, base loss: 4612.94
[INFO 2017-06-27 16:56:30,789 main.py:51] epoch 1353, training loss: 6710.24, average training loss: 3839.00, base loss: 4615.61
[INFO 2017-06-27 16:56:31,180 main.py:51] epoch 1354, training loss: 3380.45, average training loss: 3838.56, base loss: 4615.79
[INFO 2017-06-27 16:56:31,582 main.py:51] epoch 1355, training loss: 3401.31, average training loss: 3837.95, base loss: 4615.57
[INFO 2017-06-27 16:56:31,968 main.py:51] epoch 1356, training loss: 3527.45, average training loss: 3837.88, base loss: 4616.02
[INFO 2017-06-27 16:56:32,359 main.py:51] epoch 1357, training loss: 3710.02, average training loss: 3838.05, base loss: 4616.90
[INFO 2017-06-27 16:56:32,811 main.py:51] epoch 1358, training loss: 3552.71, average training loss: 3837.64, base loss: 4617.10
[INFO 2017-06-27 16:56:33,218 main.py:51] epoch 1359, training loss: 4027.14, average training loss: 3838.12, base loss: 4618.44
[INFO 2017-06-27 16:56:33,597 main.py:51] epoch 1360, training loss: 3738.89, average training loss: 3837.47, base loss: 4619.07
[INFO 2017-06-27 16:56:34,069 main.py:51] epoch 1361, training loss: 3559.42, average training loss: 3837.20, base loss: 4619.41
[INFO 2017-06-27 16:56:34,462 main.py:51] epoch 1362, training loss: 3415.32, average training loss: 3836.57, base loss: 4619.33
[INFO 2017-06-27 16:56:34,845 main.py:51] epoch 1363, training loss: 3807.98, average training loss: 3836.54, base loss: 4620.40
[INFO 2017-06-27 16:56:35,227 main.py:51] epoch 1364, training loss: 3701.22, average training loss: 3836.51, base loss: 4621.63
[INFO 2017-06-27 16:56:35,598 main.py:51] epoch 1365, training loss: 3598.06, average training loss: 3835.61, base loss: 4621.00
[INFO 2017-06-27 16:56:36,025 main.py:51] epoch 1366, training loss: 3673.34, average training loss: 3835.16, base loss: 4621.05
[INFO 2017-06-27 16:56:36,436 main.py:51] epoch 1367, training loss: 3882.97, average training loss: 3835.41, base loss: 4622.19
[INFO 2017-06-27 16:56:36,813 main.py:51] epoch 1368, training loss: 3738.47, average training loss: 3834.60, base loss: 4621.88
[INFO 2017-06-27 16:56:37,197 main.py:51] epoch 1369, training loss: 3643.03, average training loss: 3834.38, base loss: 4622.19
[INFO 2017-06-27 16:56:37,581 main.py:51] epoch 1370, training loss: 3324.68, average training loss: 3833.93, base loss: 4622.12
[INFO 2017-06-27 16:56:38,013 main.py:51] epoch 1371, training loss: 3186.47, average training loss: 3833.25, base loss: 4621.81
[INFO 2017-06-27 16:56:38,395 main.py:51] epoch 1372, training loss: 3299.71, average training loss: 3833.18, base loss: 4622.44
[INFO 2017-06-27 16:56:38,778 main.py:51] epoch 1373, training loss: 7183.15, average training loss: 3836.06, base loss: 4625.89
[INFO 2017-06-27 16:56:39,155 main.py:51] epoch 1374, training loss: 3017.55, average training loss: 3835.39, base loss: 4625.47
[INFO 2017-06-27 16:56:39,529 main.py:51] epoch 1375, training loss: 4012.63, average training loss: 3835.66, base loss: 4626.73
[INFO 2017-06-27 16:56:39,905 main.py:51] epoch 1376, training loss: 3597.83, average training loss: 3835.31, base loss: 4626.75
[INFO 2017-06-27 16:56:40,300 main.py:51] epoch 1377, training loss: 3656.05, average training loss: 3834.37, base loss: 4626.14
[INFO 2017-06-27 16:56:40,673 main.py:51] epoch 1378, training loss: 3588.33, average training loss: 3834.09, base loss: 4626.51
[INFO 2017-06-27 16:56:41,047 main.py:51] epoch 1379, training loss: 3568.89, average training loss: 3833.50, base loss: 4626.37
[INFO 2017-06-27 16:56:41,420 main.py:51] epoch 1380, training loss: 3471.10, average training loss: 3833.62, base loss: 4627.29
[INFO 2017-06-27 16:56:41,796 main.py:51] epoch 1381, training loss: 3506.64, average training loss: 3833.40, base loss: 4627.66
[INFO 2017-06-27 16:56:42,173 main.py:51] epoch 1382, training loss: 3338.31, average training loss: 3833.11, base loss: 4627.76
[INFO 2017-06-27 16:56:42,555 main.py:51] epoch 1383, training loss: 3520.30, average training loss: 3832.53, base loss: 4627.93
[INFO 2017-06-27 16:56:42,930 main.py:51] epoch 1384, training loss: 3452.21, average training loss: 3831.96, base loss: 4627.59
[INFO 2017-06-27 16:56:43,307 main.py:51] epoch 1385, training loss: 4041.15, average training loss: 3832.12, base loss: 4628.52
[INFO 2017-06-27 16:56:43,678 main.py:51] epoch 1386, training loss: 3473.29, average training loss: 3832.23, base loss: 4629.34
[INFO 2017-06-27 16:56:44,049 main.py:51] epoch 1387, training loss: 3273.03, average training loss: 3831.98, base loss: 4629.49
[INFO 2017-06-27 16:56:44,421 main.py:51] epoch 1388, training loss: 3347.61, average training loss: 3831.22, base loss: 4629.22
[INFO 2017-06-27 16:56:44,821 main.py:51] epoch 1389, training loss: 3340.26, average training loss: 3830.30, base loss: 4628.89
[INFO 2017-06-27 16:56:45,198 main.py:51] epoch 1390, training loss: 2818.62, average training loss: 3828.98, base loss: 4627.41
[INFO 2017-06-27 16:56:45,569 main.py:51] epoch 1391, training loss: 3970.96, average training loss: 3829.41, base loss: 4629.18
[INFO 2017-06-27 16:56:45,948 main.py:51] epoch 1392, training loss: 3448.12, average training loss: 3828.63, base loss: 4629.04
[INFO 2017-06-27 16:56:46,320 main.py:51] epoch 1393, training loss: 3364.10, average training loss: 3827.93, base loss: 4628.92
[INFO 2017-06-27 16:56:46,697 main.py:51] epoch 1394, training loss: 3118.11, average training loss: 3827.36, base loss: 4628.84
[INFO 2017-06-27 16:56:47,100 main.py:51] epoch 1395, training loss: 3315.12, average training loss: 3826.10, base loss: 4628.06
[INFO 2017-06-27 16:56:47,499 main.py:51] epoch 1396, training loss: 3658.14, average training loss: 3822.02, base loss: 4624.46
[INFO 2017-06-27 16:56:47,876 main.py:51] epoch 1397, training loss: 3576.91, average training loss: 3821.18, base loss: 4624.21
[INFO 2017-06-27 16:56:48,267 main.py:51] epoch 1398, training loss: 3565.37, average training loss: 3816.85, base loss: 4619.99
[INFO 2017-06-27 16:56:48,641 main.py:51] epoch 1399, training loss: 3926.20, average training loss: 3816.61, base loss: 4620.47
[INFO 2017-06-27 16:56:48,642 main.py:53] epoch 1399, testing
[INFO 2017-06-27 16:56:50,258 main.py:105] average testing loss: 3373.77, base loss: 4310.52
[INFO 2017-06-27 16:56:50,259 main.py:106] improve_loss: 936.74, improve_percent: 0.22
[INFO 2017-06-27 16:56:50,259 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 16:56:50,271 main.py:76] current best improved percent: 0.22
[INFO 2017-06-27 16:56:50,650 main.py:51] epoch 1400, training loss: 7067.74, average training loss: 3820.12, base loss: 4624.46
[INFO 2017-06-27 16:56:51,031 main.py:51] epoch 1401, training loss: 2932.49, average training loss: 3819.52, base loss: 4623.97
[INFO 2017-06-27 16:56:51,406 main.py:51] epoch 1402, training loss: 3116.93, average training loss: 3818.50, base loss: 4623.10
[INFO 2017-06-27 16:56:51,779 main.py:51] epoch 1403, training loss: 3316.97, average training loss: 3817.82, base loss: 4622.94
[INFO 2017-06-27 16:56:52,153 main.py:51] epoch 1404, training loss: 3749.01, average training loss: 3817.51, base loss: 4622.90
[INFO 2017-06-27 16:56:52,528 main.py:51] epoch 1405, training loss: 3236.82, average training loss: 3816.64, base loss: 4622.42
[INFO 2017-06-27 16:56:52,907 main.py:51] epoch 1406, training loss: 3725.89, average training loss: 3816.07, base loss: 4622.57
[INFO 2017-06-27 16:56:53,285 main.py:51] epoch 1407, training loss: 3267.85, average training loss: 3815.35, base loss: 4622.47
[INFO 2017-06-27 16:56:53,660 main.py:51] epoch 1408, training loss: 4115.90, average training loss: 3812.48, base loss: 4620.69
[INFO 2017-06-27 16:56:54,034 main.py:51] epoch 1409, training loss: 3442.69, average training loss: 3811.90, base loss: 4620.65
[INFO 2017-06-27 16:56:54,427 main.py:51] epoch 1410, training loss: 3015.23, average training loss: 3811.37, base loss: 4620.22
[INFO 2017-06-27 16:56:54,811 main.py:51] epoch 1411, training loss: 3471.23, average training loss: 3811.23, base loss: 4620.52
[INFO 2017-06-27 16:56:55,194 main.py:51] epoch 1412, training loss: 3879.49, average training loss: 3811.63, base loss: 4621.72
[INFO 2017-06-27 16:56:55,571 main.py:51] epoch 1413, training loss: 3115.40, average training loss: 3811.15, base loss: 4621.67
[INFO 2017-06-27 16:56:55,950 main.py:51] epoch 1414, training loss: 3442.39, average training loss: 3810.63, base loss: 4621.56
[INFO 2017-06-27 16:56:56,329 main.py:51] epoch 1415, training loss: 3167.90, average training loss: 3809.82, base loss: 4621.10
[INFO 2017-06-27 16:56:56,709 main.py:51] epoch 1416, training loss: 3533.94, average training loss: 3810.06, base loss: 4622.45
[INFO 2017-06-27 16:56:57,098 main.py:51] epoch 1417, training loss: 3406.49, average training loss: 3809.24, base loss: 4622.12
[INFO 2017-06-27 16:56:57,478 main.py:51] epoch 1418, training loss: 3491.09, average training loss: 3808.94, base loss: 4622.35
[INFO 2017-06-27 16:56:57,859 main.py:51] epoch 1419, training loss: 3257.40, average training loss: 3807.38, base loss: 4620.82
[INFO 2017-06-27 16:56:58,244 main.py:51] epoch 1420, training loss: 3107.35, average training loss: 3806.24, base loss: 4619.74
[INFO 2017-06-27 16:56:58,634 main.py:51] epoch 1421, training loss: 3553.80, average training loss: 3805.89, base loss: 4619.76
[INFO 2017-06-27 16:56:59,016 main.py:51] epoch 1422, training loss: 3685.63, average training loss: 3805.67, base loss: 4619.99
[INFO 2017-06-27 16:56:59,415 main.py:51] epoch 1423, training loss: 3741.75, average training loss: 3805.83, base loss: 4620.78
[INFO 2017-06-27 16:56:59,801 main.py:51] epoch 1424, training loss: 3195.55, average training loss: 3805.23, base loss: 4620.29
[INFO 2017-06-27 16:57:00,190 main.py:51] epoch 1425, training loss: 3535.37, average training loss: 3804.60, base loss: 4620.22
[INFO 2017-06-27 16:57:00,573 main.py:51] epoch 1426, training loss: 3431.08, average training loss: 3803.65, base loss: 4619.42
[INFO 2017-06-27 16:57:00,970 main.py:51] epoch 1427, training loss: 3091.68, average training loss: 3802.49, base loss: 4618.25
[INFO 2017-06-27 16:57:01,415 main.py:51] epoch 1428, training loss: 3196.89, average training loss: 3801.92, base loss: 4618.31
[INFO 2017-06-27 16:57:01,852 main.py:51] epoch 1429, training loss: 3141.55, average training loss: 3801.09, base loss: 4617.88
[INFO 2017-06-27 16:57:02,293 main.py:51] epoch 1430, training loss: 3300.17, average training loss: 3796.48, base loss: 4613.38
[INFO 2017-06-27 16:57:02,750 main.py:51] epoch 1431, training loss: 6753.93, average training loss: 3798.68, base loss: 4616.09
[INFO 2017-06-27 16:57:03,165 main.py:51] epoch 1432, training loss: 7451.49, average training loss: 3798.68, base loss: 4616.52
[INFO 2017-06-27 16:57:03,581 main.py:51] epoch 1433, training loss: 4033.11, average training loss: 3799.15, base loss: 4617.73
[INFO 2017-06-27 16:57:04,045 main.py:51] epoch 1434, training loss: 6963.76, average training loss: 3802.16, base loss: 4620.94
[INFO 2017-06-27 16:57:04,507 main.py:51] epoch 1435, training loss: 3540.10, average training loss: 3801.81, base loss: 4620.99
[INFO 2017-06-27 16:57:04,890 main.py:51] epoch 1436, training loss: 6682.86, average training loss: 3804.34, base loss: 4623.80
[INFO 2017-06-27 16:57:05,269 main.py:51] epoch 1437, training loss: 3268.69, average training loss: 3804.28, base loss: 4624.10
[INFO 2017-06-27 16:57:05,651 main.py:51] epoch 1438, training loss: 3457.62, average training loss: 3803.25, base loss: 4623.19
[INFO 2017-06-27 16:57:06,045 main.py:51] epoch 1439, training loss: 3612.46, average training loss: 3802.92, base loss: 4623.85
[INFO 2017-06-27 16:57:06,422 main.py:51] epoch 1440, training loss: 3953.09, average training loss: 3802.82, base loss: 4624.39
[INFO 2017-06-27 16:57:06,803 main.py:51] epoch 1441, training loss: 7456.28, average training loss: 3806.33, base loss: 4629.06
[INFO 2017-06-27 16:57:07,182 main.py:51] epoch 1442, training loss: 3812.68, average training loss: 3806.29, base loss: 4629.58
[INFO 2017-06-27 16:57:07,558 main.py:51] epoch 1443, training loss: 3715.52, average training loss: 3802.27, base loss: 4625.99
[INFO 2017-06-27 16:57:07,936 main.py:51] epoch 1444, training loss: 3287.12, average training loss: 3802.16, base loss: 4626.26
[INFO 2017-06-27 16:57:08,312 main.py:51] epoch 1445, training loss: 3291.85, average training loss: 3800.92, base loss: 4625.10
[INFO 2017-06-27 16:57:08,687 main.py:51] epoch 1446, training loss: 3646.21, average training loss: 3800.88, base loss: 4625.85
[INFO 2017-06-27 16:57:09,067 main.py:51] epoch 1447, training loss: 3128.71, average training loss: 3800.23, base loss: 4625.33
[INFO 2017-06-27 16:57:09,447 main.py:51] epoch 1448, training loss: 3563.35, average training loss: 3800.36, base loss: 4626.12
[INFO 2017-06-27 16:57:09,824 main.py:51] epoch 1449, training loss: 3541.23, average training loss: 3800.52, base loss: 4626.80
[INFO 2017-06-27 16:57:10,205 main.py:51] epoch 1450, training loss: 3392.06, average training loss: 3800.49, base loss: 4627.04
[INFO 2017-06-27 16:57:10,585 main.py:51] epoch 1451, training loss: 2973.39, average training loss: 3800.05, base loss: 4626.81
[INFO 2017-06-27 16:57:10,961 main.py:51] epoch 1452, training loss: 3197.25, average training loss: 3799.84, base loss: 4627.01
[INFO 2017-06-27 16:57:11,342 main.py:51] epoch 1453, training loss: 3468.17, average training loss: 3798.97, base loss: 4626.51
[INFO 2017-06-27 16:57:11,718 main.py:51] epoch 1454, training loss: 3178.67, average training loss: 3798.48, base loss: 4626.33
[INFO 2017-06-27 16:57:12,094 main.py:51] epoch 1455, training loss: 3645.65, average training loss: 3798.70, base loss: 4626.98
[INFO 2017-06-27 16:57:12,470 main.py:51] epoch 1456, training loss: 3502.43, average training loss: 3798.70, base loss: 4627.70
[INFO 2017-06-27 16:57:12,850 main.py:51] epoch 1457, training loss: 3117.94, average training loss: 3794.73, base loss: 4624.06
[INFO 2017-06-27 16:57:13,226 main.py:51] epoch 1458, training loss: 3598.29, average training loss: 3795.09, base loss: 4625.11
[INFO 2017-06-27 16:57:13,602 main.py:51] epoch 1459, training loss: 3507.70, average training loss: 3795.01, base loss: 4625.76
[INFO 2017-06-27 16:57:13,978 main.py:51] epoch 1460, training loss: 3308.30, average training loss: 3794.17, base loss: 4625.44
[INFO 2017-06-27 16:57:14,352 main.py:51] epoch 1461, training loss: 3359.84, average training loss: 3793.47, base loss: 4625.33
[INFO 2017-06-27 16:57:14,731 main.py:51] epoch 1462, training loss: 3553.39, average training loss: 3793.62, base loss: 4626.18
[INFO 2017-06-27 16:57:15,109 main.py:51] epoch 1463, training loss: 7067.26, average training loss: 3796.67, base loss: 4629.15
[INFO 2017-06-27 16:57:15,488 main.py:51] epoch 1464, training loss: 3758.51, average training loss: 3797.29, base loss: 4630.86
[INFO 2017-06-27 16:57:15,944 main.py:51] epoch 1465, training loss: 3256.45, average training loss: 3797.11, base loss: 4631.35
[INFO 2017-06-27 16:57:16,344 main.py:51] epoch 1466, training loss: 6803.47, average training loss: 3800.04, base loss: 4634.53
[INFO 2017-06-27 16:57:16,721 main.py:51] epoch 1467, training loss: 3413.43, average training loss: 3799.55, base loss: 4634.48
[INFO 2017-06-27 16:57:17,097 main.py:51] epoch 1468, training loss: 3714.75, average training loss: 3799.63, base loss: 4635.27
[INFO 2017-06-27 16:57:17,545 main.py:51] epoch 1469, training loss: 3536.11, average training loss: 3799.23, base loss: 4635.42
[INFO 2017-06-27 16:57:17,946 main.py:51] epoch 1470, training loss: 3331.77, average training loss: 3798.91, base loss: 4635.70
[INFO 2017-06-27 16:57:18,323 main.py:51] epoch 1471, training loss: 3709.63, average training loss: 3798.54, base loss: 4636.04
[INFO 2017-06-27 16:57:18,771 main.py:51] epoch 1472, training loss: 3284.43, average training loss: 3798.53, base loss: 4636.41
[INFO 2017-06-27 16:57:19,147 main.py:51] epoch 1473, training loss: 3478.68, average training loss: 3798.72, base loss: 4637.21
[INFO 2017-06-27 16:57:19,529 main.py:51] epoch 1474, training loss: 3315.22, average training loss: 3798.00, base loss: 4636.72
[INFO 2017-06-27 16:57:19,906 main.py:51] epoch 1475, training loss: 3322.29, average training loss: 3798.15, base loss: 4637.71
[INFO 2017-06-27 16:57:20,278 main.py:51] epoch 1476, training loss: 4047.50, average training loss: 3798.30, base loss: 4638.72
[INFO 2017-06-27 16:57:20,673 main.py:51] epoch 1477, training loss: 3492.05, average training loss: 3797.31, base loss: 4637.88
[INFO 2017-06-27 16:57:21,053 main.py:51] epoch 1478, training loss: 3591.45, average training loss: 3797.43, base loss: 4638.91
[INFO 2017-06-27 16:57:21,432 main.py:51] epoch 1479, training loss: 3205.05, average training loss: 3796.15, base loss: 4638.08
[INFO 2017-06-27 16:57:21,809 main.py:51] epoch 1480, training loss: 3470.22, average training loss: 3795.61, base loss: 4637.63
[INFO 2017-06-27 16:57:22,185 main.py:51] epoch 1481, training loss: 3367.90, average training loss: 3794.44, base loss: 4636.83
[INFO 2017-06-27 16:57:22,563 main.py:51] epoch 1482, training loss: 3200.88, average training loss: 3793.96, base loss: 4636.77
[INFO 2017-06-27 16:57:22,940 main.py:51] epoch 1483, training loss: 3836.94, average training loss: 3794.32, base loss: 4637.82
[INFO 2017-06-27 16:57:23,313 main.py:51] epoch 1484, training loss: 3332.40, average training loss: 3794.18, base loss: 4638.09
[INFO 2017-06-27 16:57:23,683 main.py:51] epoch 1485, training loss: 3409.46, average training loss: 3789.48, base loss: 4633.81
[INFO 2017-06-27 16:57:24,059 main.py:51] epoch 1486, training loss: 3505.82, average training loss: 3789.42, base loss: 4634.42
[INFO 2017-06-27 16:57:24,436 main.py:51] epoch 1487, training loss: 3306.77, average training loss: 3789.02, base loss: 4634.04
[INFO 2017-06-27 16:57:24,812 main.py:51] epoch 1488, training loss: 3226.90, average training loss: 3788.65, base loss: 4634.08
[INFO 2017-06-27 16:57:25,187 main.py:51] epoch 1489, training loss: 3233.43, average training loss: 3787.85, base loss: 4633.48
[INFO 2017-06-27 16:57:25,569 main.py:51] epoch 1490, training loss: 3069.73, average training loss: 3786.98, base loss: 4632.71
[INFO 2017-06-27 16:57:25,945 main.py:51] epoch 1491, training loss: 3693.95, average training loss: 3786.77, base loss: 4632.96
[INFO 2017-06-27 16:57:26,321 main.py:51] epoch 1492, training loss: 6945.25, average training loss: 3789.70, base loss: 4636.29
[INFO 2017-06-27 16:57:26,695 main.py:51] epoch 1493, training loss: 3500.14, average training loss: 3789.42, base loss: 4636.35
[INFO 2017-06-27 16:57:27,070 main.py:51] epoch 1494, training loss: 3733.01, average training loss: 3789.34, base loss: 4636.83
[INFO 2017-06-27 16:57:27,446 main.py:51] epoch 1495, training loss: 3468.03, average training loss: 3789.17, base loss: 4637.53
[INFO 2017-06-27 16:57:27,822 main.py:51] epoch 1496, training loss: 2956.99, average training loss: 3788.27, base loss: 4636.72
[INFO 2017-06-27 16:57:28,220 main.py:51] epoch 1497, training loss: 3658.08, average training loss: 3787.81, base loss: 4637.03
[INFO 2017-06-27 16:57:28,618 main.py:51] epoch 1498, training loss: 7008.30, average training loss: 3790.88, base loss: 4640.77
[INFO 2017-06-27 16:57:28,999 main.py:51] epoch 1499, training loss: 3711.47, average training loss: 3790.35, base loss: 4640.42
[INFO 2017-06-27 16:57:29,000 main.py:53] epoch 1499, testing
[INFO 2017-06-27 16:57:30,624 main.py:105] average testing loss: 3674.47, base loss: 4639.09
[INFO 2017-06-27 16:57:30,624 main.py:106] improve_loss: 964.62, improve_percent: 0.21
[INFO 2017-06-27 16:57:30,625 main.py:76] current best improved percent: 0.22
[INFO 2017-06-27 16:57:31,001 main.py:51] epoch 1500, training loss: 3125.61, average training loss: 3790.06, base loss: 4640.53
[INFO 2017-06-27 16:57:31,380 main.py:51] epoch 1501, training loss: 3326.73, average training loss: 3789.43, base loss: 4639.96
[INFO 2017-06-27 16:57:31,754 main.py:51] epoch 1502, training loss: 2946.23, average training loss: 3788.22, base loss: 4638.21
[INFO 2017-06-27 16:57:32,131 main.py:51] epoch 1503, training loss: 4014.47, average training loss: 3788.39, base loss: 4638.92
[INFO 2017-06-27 16:57:32,508 main.py:51] epoch 1504, training loss: 3854.93, average training loss: 3788.30, base loss: 4639.91
[INFO 2017-06-27 16:57:32,880 main.py:51] epoch 1505, training loss: 6445.21, average training loss: 3790.88, base loss: 4642.72
[INFO 2017-06-27 16:57:33,254 main.py:51] epoch 1506, training loss: 3700.71, average training loss: 3790.85, base loss: 4643.41
[INFO 2017-06-27 16:57:33,627 main.py:51] epoch 1507, training loss: 3130.45, average training loss: 3790.14, base loss: 4642.80
[INFO 2017-06-27 16:57:34,002 main.py:51] epoch 1508, training loss: 3206.86, average training loss: 3789.36, base loss: 4642.54
[INFO 2017-06-27 16:57:34,375 main.py:51] epoch 1509, training loss: 3068.70, average training loss: 3789.03, base loss: 4642.87
[INFO 2017-06-27 16:57:34,747 main.py:51] epoch 1510, training loss: 3938.06, average training loss: 3789.46, base loss: 4644.05
[INFO 2017-06-27 16:57:35,125 main.py:51] epoch 1511, training loss: 3302.20, average training loss: 3788.18, base loss: 4642.91
[INFO 2017-06-27 16:57:35,501 main.py:51] epoch 1512, training loss: 3703.70, average training loss: 3788.41, base loss: 4643.64
[INFO 2017-06-27 16:57:35,878 main.py:51] epoch 1513, training loss: 3756.12, average training loss: 3788.23, base loss: 4644.02
[INFO 2017-06-27 16:57:36,253 main.py:51] epoch 1514, training loss: 3125.83, average training loss: 3787.26, base loss: 4643.17
[INFO 2017-06-27 16:57:36,632 main.py:51] epoch 1515, training loss: 3527.91, average training loss: 3787.36, base loss: 4644.13
[INFO 2017-06-27 16:57:37,007 main.py:51] epoch 1516, training loss: 3070.25, average training loss: 3786.79, base loss: 4643.58
[INFO 2017-06-27 16:57:37,425 main.py:51] epoch 1517, training loss: 3261.91, average training loss: 3785.98, base loss: 4642.95
[INFO 2017-06-27 16:57:37,841 main.py:51] epoch 1518, training loss: 3954.60, average training loss: 3786.42, base loss: 4643.83
[INFO 2017-06-27 16:57:38,224 main.py:51] epoch 1519, training loss: 3615.02, average training loss: 3786.07, base loss: 4643.62
[INFO 2017-06-27 16:57:38,605 main.py:51] epoch 1520, training loss: 3624.29, average training loss: 3786.04, base loss: 4644.55
[INFO 2017-06-27 16:57:39,077 main.py:51] epoch 1521, training loss: 3708.79, average training loss: 3785.79, base loss: 4644.76
[INFO 2017-06-27 16:57:39,488 main.py:51] epoch 1522, training loss: 3401.82, average training loss: 3785.19, base loss: 4644.26
[INFO 2017-06-27 16:57:39,893 main.py:51] epoch 1523, training loss: 3299.87, average training loss: 3784.59, base loss: 4644.04
[INFO 2017-06-27 16:57:40,314 main.py:51] epoch 1524, training loss: 3648.79, average training loss: 3784.51, base loss: 4644.59
[INFO 2017-06-27 16:57:40,698 main.py:51] epoch 1525, training loss: 3294.83, average training loss: 3783.96, base loss: 4644.62
[INFO 2017-06-27 16:57:41,125 main.py:51] epoch 1526, training loss: 2751.08, average training loss: 3783.24, base loss: 4643.95
[INFO 2017-06-27 16:57:41,538 main.py:51] epoch 1527, training loss: 3660.01, average training loss: 3782.67, base loss: 4643.62
[INFO 2017-06-27 16:57:41,928 main.py:51] epoch 1528, training loss: 3092.33, average training loss: 3781.69, base loss: 4642.65
[INFO 2017-06-27 16:57:42,305 main.py:51] epoch 1529, training loss: 7500.18, average training loss: 3784.85, base loss: 4646.70
[INFO 2017-06-27 16:57:42,682 main.py:51] epoch 1530, training loss: 2975.69, average training loss: 3784.41, base loss: 4646.31
[INFO 2017-06-27 16:57:43,071 main.py:51] epoch 1531, training loss: 3300.71, average training loss: 3783.81, base loss: 4646.18
[INFO 2017-06-27 16:57:43,448 main.py:51] epoch 1532, training loss: 2977.42, average training loss: 3783.05, base loss: 4645.77
[INFO 2017-06-27 16:57:43,825 main.py:51] epoch 1533, training loss: 3282.97, average training loss: 3782.45, base loss: 4645.41
[INFO 2017-06-27 16:57:44,210 main.py:51] epoch 1534, training loss: 3352.63, average training loss: 3782.29, base loss: 4645.69
[INFO 2017-06-27 16:57:44,594 main.py:51] epoch 1535, training loss: 3322.31, average training loss: 3781.50, base loss: 4645.02
[INFO 2017-06-27 16:57:45,024 main.py:51] epoch 1536, training loss: 3187.07, average training loss: 3780.79, base loss: 4644.33
[INFO 2017-06-27 16:57:45,437 main.py:51] epoch 1537, training loss: 3324.74, average training loss: 3777.01, base loss: 4641.19
[INFO 2017-06-27 16:57:45,819 main.py:51] epoch 1538, training loss: 3393.03, average training loss: 3776.74, base loss: 4641.47
[INFO 2017-06-27 16:57:46,275 main.py:51] epoch 1539, training loss: 3240.33, average training loss: 3776.15, base loss: 4641.14
[INFO 2017-06-27 16:57:46,680 main.py:51] epoch 1540, training loss: 3370.34, average training loss: 3775.76, base loss: 4641.32
[INFO 2017-06-27 16:57:47,123 main.py:51] epoch 1541, training loss: 3013.08, average training loss: 3774.62, base loss: 4640.14
[INFO 2017-06-27 16:57:47,547 main.py:51] epoch 1542, training loss: 3420.23, average training loss: 3774.15, base loss: 4640.12
[INFO 2017-06-27 16:57:47,934 main.py:51] epoch 1543, training loss: 3282.37, average training loss: 3772.86, base loss: 4638.85
[INFO 2017-06-27 16:57:48,386 main.py:51] epoch 1544, training loss: 2991.51, average training loss: 3771.77, base loss: 4637.68
[INFO 2017-06-27 16:57:48,764 main.py:51] epoch 1545, training loss: 3540.49, average training loss: 3770.86, base loss: 4637.06
[INFO 2017-06-27 16:57:49,205 main.py:51] epoch 1546, training loss: 3082.89, average training loss: 3770.46, base loss: 4637.18
[INFO 2017-06-27 16:57:49,605 main.py:51] epoch 1547, training loss: 3288.00, average training loss: 3769.47, base loss: 4636.65
[INFO 2017-06-27 16:57:49,986 main.py:51] epoch 1548, training loss: 3076.35, average training loss: 3768.70, base loss: 4636.09
[INFO 2017-06-27 16:57:50,366 main.py:51] epoch 1549, training loss: 3452.23, average training loss: 3768.64, base loss: 4636.56
[INFO 2017-06-27 16:57:50,775 main.py:51] epoch 1550, training loss: 4013.15, average training loss: 3768.94, base loss: 4637.89
[INFO 2017-06-27 16:57:51,156 main.py:51] epoch 1551, training loss: 3142.89, average training loss: 3768.40, base loss: 4637.40
[INFO 2017-06-27 16:57:51,540 main.py:51] epoch 1552, training loss: 3240.88, average training loss: 3767.95, base loss: 4637.39
[INFO 2017-06-27 16:57:51,914 main.py:51] epoch 1553, training loss: 3366.58, average training loss: 3766.68, base loss: 4636.40
[INFO 2017-06-27 16:57:52,292 main.py:51] epoch 1554, training loss: 3485.53, average training loss: 3765.63, base loss: 4635.32
[INFO 2017-06-27 16:57:52,670 main.py:51] epoch 1555, training loss: 3658.03, average training loss: 3765.90, base loss: 4636.88
[INFO 2017-06-27 16:57:53,045 main.py:51] epoch 1556, training loss: 3203.65, average training loss: 3765.19, base loss: 4636.54
[INFO 2017-06-27 16:57:53,430 main.py:51] epoch 1557, training loss: 4007.52, average training loss: 3764.89, base loss: 4637.49
[INFO 2017-06-27 16:57:53,807 main.py:51] epoch 1558, training loss: 2946.43, average training loss: 3763.84, base loss: 4636.34
[INFO 2017-06-27 16:57:54,179 main.py:51] epoch 1559, training loss: 3368.74, average training loss: 3761.85, base loss: 4634.47
[INFO 2017-06-27 16:57:54,554 main.py:51] epoch 1560, training loss: 3494.69, average training loss: 3761.15, base loss: 4634.15
[INFO 2017-06-27 16:57:54,933 main.py:51] epoch 1561, training loss: 3243.77, average training loss: 3760.43, base loss: 4633.75
[INFO 2017-06-27 16:57:55,307 main.py:51] epoch 1562, training loss: 3401.21, average training loss: 3760.09, base loss: 4633.89
[INFO 2017-06-27 16:57:55,681 main.py:51] epoch 1563, training loss: 3420.06, average training loss: 3760.02, base loss: 4634.40
[INFO 2017-06-27 16:57:56,054 main.py:51] epoch 1564, training loss: 3298.59, average training loss: 3759.66, base loss: 4634.62
[INFO 2017-06-27 16:57:56,427 main.py:51] epoch 1565, training loss: 3067.04, average training loss: 3758.93, base loss: 4634.21
[INFO 2017-06-27 16:57:56,799 main.py:51] epoch 1566, training loss: 6864.71, average training loss: 3761.85, base loss: 4637.48
[INFO 2017-06-27 16:57:57,171 main.py:51] epoch 1567, training loss: 3814.41, average training loss: 3761.74, base loss: 4637.96
[INFO 2017-06-27 16:57:57,548 main.py:51] epoch 1568, training loss: 6703.21, average training loss: 3764.70, base loss: 4641.28
[INFO 2017-06-27 16:57:57,923 main.py:51] epoch 1569, training loss: 3443.50, average training loss: 3764.12, base loss: 4641.15
[INFO 2017-06-27 16:57:58,293 main.py:51] epoch 1570, training loss: 3294.92, average training loss: 3763.70, base loss: 4641.30
[INFO 2017-06-27 16:57:58,665 main.py:51] epoch 1571, training loss: 3213.53, average training loss: 3763.17, base loss: 4640.98
[INFO 2017-06-27 16:57:59,037 main.py:51] epoch 1572, training loss: 3870.31, average training loss: 3763.16, base loss: 4641.34
[INFO 2017-06-27 16:57:59,416 main.py:51] epoch 1573, training loss: 3475.11, average training loss: 3762.43, base loss: 4640.90
[INFO 2017-06-27 16:57:59,796 main.py:51] epoch 1574, training loss: 3591.38, average training loss: 3761.91, base loss: 4640.86
[INFO 2017-06-27 16:58:00,174 main.py:51] epoch 1575, training loss: 3412.69, average training loss: 3760.95, base loss: 4640.23
[INFO 2017-06-27 16:58:00,547 main.py:51] epoch 1576, training loss: 3548.31, average training loss: 3761.20, base loss: 4641.32
[INFO 2017-06-27 16:58:00,922 main.py:51] epoch 1577, training loss: 3502.94, average training loss: 3760.57, base loss: 4641.17
[INFO 2017-06-27 16:58:01,396 main.py:51] epoch 1578, training loss: 3276.44, average training loss: 3759.86, base loss: 4640.79
[INFO 2017-06-27 16:58:01,791 main.py:51] epoch 1579, training loss: 3353.58, average training loss: 3760.09, base loss: 4641.74
[INFO 2017-06-27 16:58:02,207 main.py:51] epoch 1580, training loss: 3383.05, average training loss: 3759.98, base loss: 4642.42
[INFO 2017-06-27 16:58:02,623 main.py:51] epoch 1581, training loss: 4128.38, average training loss: 3760.03, base loss: 4643.29
[INFO 2017-06-27 16:58:03,055 main.py:51] epoch 1582, training loss: 3570.53, average training loss: 3759.68, base loss: 4643.01
[INFO 2017-06-27 16:58:03,465 main.py:51] epoch 1583, training loss: 2866.03, average training loss: 3758.64, base loss: 4642.01
[INFO 2017-06-27 16:58:03,847 main.py:51] epoch 1584, training loss: 3740.28, average training loss: 3758.76, base loss: 4642.73
[INFO 2017-06-27 16:58:04,234 main.py:51] epoch 1585, training loss: 3647.15, average training loss: 3758.46, base loss: 4642.97
[INFO 2017-06-27 16:58:04,619 main.py:51] epoch 1586, training loss: 3062.20, average training loss: 3757.79, base loss: 4642.50
[INFO 2017-06-27 16:58:05,001 main.py:51] epoch 1587, training loss: 3449.74, average training loss: 3757.90, base loss: 4643.32
[INFO 2017-06-27 16:58:05,384 main.py:51] epoch 1588, training loss: 3887.81, average training loss: 3757.57, base loss: 4643.55
[INFO 2017-06-27 16:58:05,761 main.py:51] epoch 1589, training loss: 3394.82, average training loss: 3756.62, base loss: 4642.70
[INFO 2017-06-27 16:58:06,136 main.py:51] epoch 1590, training loss: 3292.57, average training loss: 3756.31, base loss: 4643.03
[INFO 2017-06-27 16:58:06,509 main.py:51] epoch 1591, training loss: 3618.28, average training loss: 3756.15, base loss: 4643.50
[INFO 2017-06-27 16:58:06,882 main.py:51] epoch 1592, training loss: 3344.14, average training loss: 3755.95, base loss: 4643.59
[INFO 2017-06-27 16:58:07,257 main.py:51] epoch 1593, training loss: 3417.53, average training loss: 3755.85, base loss: 4644.02
[INFO 2017-06-27 16:58:07,633 main.py:51] epoch 1594, training loss: 3249.13, average training loss: 3755.10, base loss: 4643.55
[INFO 2017-06-27 16:58:08,032 main.py:51] epoch 1595, training loss: 3556.61, average training loss: 3754.43, base loss: 4643.23
[INFO 2017-06-27 16:58:08,411 main.py:51] epoch 1596, training loss: 3579.07, average training loss: 3754.49, base loss: 4644.27
[INFO 2017-06-27 16:58:08,788 main.py:51] epoch 1597, training loss: 2841.41, average training loss: 3752.46, base loss: 4641.69
[INFO 2017-06-27 16:58:09,164 main.py:51] epoch 1598, training loss: 2955.65, average training loss: 3752.02, base loss: 4641.92
[INFO 2017-06-27 16:58:09,539 main.py:51] epoch 1599, training loss: 3201.69, average training loss: 3751.65, base loss: 4641.86
[INFO 2017-06-27 16:58:09,539 main.py:53] epoch 1599, testing
[INFO 2017-06-27 16:58:11,147 main.py:105] average testing loss: 3528.99, base loss: 4740.53
[INFO 2017-06-27 16:58:11,147 main.py:106] improve_loss: 1211.54, improve_percent: 0.26
[INFO 2017-06-27 16:58:11,147 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 16:58:11,160 main.py:76] current best improved percent: 0.26
[INFO 2017-06-27 16:58:11,539 main.py:51] epoch 1600, training loss: 3488.32, average training loss: 3751.87, base loss: 4643.12
[INFO 2017-06-27 16:58:11,914 main.py:51] epoch 1601, training loss: 3138.94, average training loss: 3751.60, base loss: 4643.07
[INFO 2017-06-27 16:58:12,290 main.py:51] epoch 1602, training loss: 3556.45, average training loss: 3751.21, base loss: 4643.33
[INFO 2017-06-27 16:58:12,664 main.py:51] epoch 1603, training loss: 3447.89, average training loss: 3750.62, base loss: 4643.31
[INFO 2017-06-27 16:58:13,041 main.py:51] epoch 1604, training loss: 3027.96, average training loss: 3750.01, base loss: 4642.97
[INFO 2017-06-27 16:58:13,415 main.py:51] epoch 1605, training loss: 6974.74, average training loss: 3753.36, base loss: 4646.71
[INFO 2017-06-27 16:58:13,792 main.py:51] epoch 1606, training loss: 3472.38, average training loss: 3753.04, base loss: 4646.90
[INFO 2017-06-27 16:58:14,170 main.py:51] epoch 1607, training loss: 6841.84, average training loss: 3756.28, base loss: 4650.66
[INFO 2017-06-27 16:58:14,547 main.py:51] epoch 1608, training loss: 3376.58, average training loss: 3755.74, base loss: 4650.60
[INFO 2017-06-27 16:58:14,925 main.py:51] epoch 1609, training loss: 3341.09, average training loss: 3754.98, base loss: 4649.61
[INFO 2017-06-27 16:58:15,308 main.py:51] epoch 1610, training loss: 3120.87, average training loss: 3754.42, base loss: 4649.29
[INFO 2017-06-27 16:58:15,690 main.py:51] epoch 1611, training loss: 3339.19, average training loss: 3753.45, base loss: 4648.48
[INFO 2017-06-27 16:58:16,071 main.py:51] epoch 1612, training loss: 3509.05, average training loss: 3753.06, base loss: 4648.66
[INFO 2017-06-27 16:58:16,458 main.py:51] epoch 1613, training loss: 3460.46, average training loss: 3752.76, base loss: 4648.74
[INFO 2017-06-27 16:58:16,835 main.py:51] epoch 1614, training loss: 2922.28, average training loss: 3751.94, base loss: 4647.97
[INFO 2017-06-27 16:58:17,222 main.py:51] epoch 1615, training loss: 3327.90, average training loss: 3751.34, base loss: 4647.91
[INFO 2017-06-27 16:58:17,607 main.py:51] epoch 1616, training loss: 3387.62, average training loss: 3750.65, base loss: 4647.23
[INFO 2017-06-27 16:58:17,991 main.py:51] epoch 1617, training loss: 3972.75, average training loss: 3750.81, base loss: 4648.06
[INFO 2017-06-27 16:58:18,376 main.py:51] epoch 1618, training loss: 6988.94, average training loss: 3754.29, base loss: 4652.25
[INFO 2017-06-27 16:58:18,785 main.py:51] epoch 1619, training loss: 3202.29, average training loss: 3753.59, base loss: 4651.37
[INFO 2017-06-27 16:58:19,230 main.py:51] epoch 1620, training loss: 3253.00, average training loss: 3753.38, base loss: 4651.44
[INFO 2017-06-27 16:58:19,671 main.py:51] epoch 1621, training loss: 3571.97, average training loss: 3752.69, base loss: 4650.98
[INFO 2017-06-27 16:58:20,117 main.py:51] epoch 1622, training loss: 3619.96, average training loss: 3752.80, base loss: 4651.86
[INFO 2017-06-27 16:58:20,568 main.py:51] epoch 1623, training loss: 3172.15, average training loss: 3751.46, base loss: 4650.30
[INFO 2017-06-27 16:58:20,980 main.py:51] epoch 1624, training loss: 3128.40, average training loss: 3751.35, base loss: 4650.94
[INFO 2017-06-27 16:58:21,398 main.py:51] epoch 1625, training loss: 3682.88, average training loss: 3751.53, base loss: 4651.46
[INFO 2017-06-27 16:58:21,859 main.py:51] epoch 1626, training loss: 3084.98, average training loss: 3751.22, base loss: 4651.46
[INFO 2017-06-27 16:58:22,310 main.py:51] epoch 1627, training loss: 3406.06, average training loss: 3750.64, base loss: 4651.29
[INFO 2017-06-27 16:58:22,697 main.py:51] epoch 1628, training loss: 3567.64, average training loss: 3750.28, base loss: 4651.36
[INFO 2017-06-27 16:58:23,074 main.py:51] epoch 1629, training loss: 3136.34, average training loss: 3749.78, base loss: 4651.25
[INFO 2017-06-27 16:58:23,455 main.py:51] epoch 1630, training loss: 3257.84, average training loss: 3748.84, base loss: 4650.42
[INFO 2017-06-27 16:58:23,837 main.py:51] epoch 1631, training loss: 3572.65, average training loss: 3748.19, base loss: 4650.27
[INFO 2017-06-27 16:58:24,226 main.py:51] epoch 1632, training loss: 3199.87, average training loss: 3748.04, base loss: 4650.61
[INFO 2017-06-27 16:58:24,602 main.py:51] epoch 1633, training loss: 6892.88, average training loss: 3750.90, base loss: 4654.18
[INFO 2017-06-27 16:58:24,976 main.py:51] epoch 1634, training loss: 3169.53, average training loss: 3750.50, base loss: 4654.17
[INFO 2017-06-27 16:58:25,352 main.py:51] epoch 1635, training loss: 3681.51, average training loss: 3750.92, base loss: 4655.30
[INFO 2017-06-27 16:58:25,726 main.py:51] epoch 1636, training loss: 3421.03, average training loss: 3750.23, base loss: 4655.28
[INFO 2017-06-27 16:58:26,100 main.py:51] epoch 1637, training loss: 3159.31, average training loss: 3746.54, base loss: 4651.95
[INFO 2017-06-27 16:58:26,475 main.py:51] epoch 1638, training loss: 6234.24, average training loss: 3749.32, base loss: 4654.78
[INFO 2017-06-27 16:58:26,851 main.py:51] epoch 1639, training loss: 3468.61, average training loss: 3748.95, base loss: 4654.83
[INFO 2017-06-27 16:58:27,230 main.py:51] epoch 1640, training loss: 3065.97, average training loss: 3748.04, base loss: 4653.79
[INFO 2017-06-27 16:58:27,610 main.py:51] epoch 1641, training loss: 3836.34, average training loss: 3748.39, base loss: 4655.21
[INFO 2017-06-27 16:58:27,986 main.py:51] epoch 1642, training loss: 3782.24, average training loss: 3748.52, base loss: 4656.05
[INFO 2017-06-27 16:58:28,371 main.py:51] epoch 1643, training loss: 6836.72, average training loss: 3752.20, base loss: 4660.03
[INFO 2017-06-27 16:58:28,746 main.py:51] epoch 1644, training loss: 3175.87, average training loss: 3751.90, base loss: 4659.92
[INFO 2017-06-27 16:58:29,139 main.py:51] epoch 1645, training loss: 3697.36, average training loss: 3751.65, base loss: 4660.01
[INFO 2017-06-27 16:58:29,513 main.py:51] epoch 1646, training loss: 3734.24, average training loss: 3751.99, base loss: 4661.31
[INFO 2017-06-27 16:58:29,980 main.py:51] epoch 1647, training loss: 3639.49, average training loss: 3751.44, base loss: 4661.16
[INFO 2017-06-27 16:58:30,373 main.py:51] epoch 1648, training loss: 3446.70, average training loss: 3751.56, base loss: 4661.87
[INFO 2017-06-27 16:58:30,791 main.py:51] epoch 1649, training loss: 3416.13, average training loss: 3751.33, base loss: 4662.01
[INFO 2017-06-27 16:58:31,204 main.py:51] epoch 1650, training loss: 3291.37, average training loss: 3751.45, base loss: 4662.41
[INFO 2017-06-27 16:58:31,583 main.py:51] epoch 1651, training loss: 3872.19, average training loss: 3751.48, base loss: 4663.32
[INFO 2017-06-27 16:58:32,012 main.py:51] epoch 1652, training loss: 3223.71, average training loss: 3750.93, base loss: 4662.92
[INFO 2017-06-27 16:58:32,422 main.py:51] epoch 1653, training loss: 3300.32, average training loss: 3750.30, base loss: 4662.49
[INFO 2017-06-27 16:58:32,804 main.py:51] epoch 1654, training loss: 3261.97, average training loss: 3749.89, base loss: 4662.21
[INFO 2017-06-27 16:58:33,289 main.py:51] epoch 1655, training loss: 3406.69, average training loss: 3749.24, base loss: 4661.99
[INFO 2017-06-27 16:58:33,692 main.py:51] epoch 1656, training loss: 3024.39, average training loss: 3748.56, base loss: 4661.58
[INFO 2017-06-27 16:58:34,155 main.py:51] epoch 1657, training loss: 3326.44, average training loss: 3747.89, base loss: 4661.01
[INFO 2017-06-27 16:58:34,588 main.py:51] epoch 1658, training loss: 3283.25, average training loss: 3746.70, base loss: 4659.61
[INFO 2017-06-27 16:58:35,035 main.py:51] epoch 1659, training loss: 6773.38, average training loss: 3749.49, base loss: 4662.73
[INFO 2017-06-27 16:58:35,471 main.py:51] epoch 1660, training loss: 3831.07, average training loss: 3748.96, base loss: 4662.48
[INFO 2017-06-27 16:58:35,852 main.py:51] epoch 1661, training loss: 3524.50, average training loss: 3748.88, base loss: 4662.88
[INFO 2017-06-27 16:58:36,232 main.py:51] epoch 1662, training loss: 3401.55, average training loss: 3748.77, base loss: 4663.41
[INFO 2017-06-27 16:58:36,609 main.py:51] epoch 1663, training loss: 3682.46, average training loss: 3748.63, base loss: 4663.84
[INFO 2017-06-27 16:58:37,045 main.py:51] epoch 1664, training loss: 3176.26, average training loss: 3747.96, base loss: 4663.43
[INFO 2017-06-27 16:58:37,424 main.py:51] epoch 1665, training loss: 3927.48, average training loss: 3748.01, base loss: 4664.58
[INFO 2017-06-27 16:58:37,807 main.py:51] epoch 1666, training loss: 3320.90, average training loss: 3747.95, base loss: 4664.76
[INFO 2017-06-27 16:58:38,193 main.py:51] epoch 1667, training loss: 3104.47, average training loss: 3747.80, base loss: 4664.90
[INFO 2017-06-27 16:58:38,578 main.py:51] epoch 1668, training loss: 3545.14, average training loss: 3747.78, base loss: 4665.43
[INFO 2017-06-27 16:58:38,966 main.py:51] epoch 1669, training loss: 3843.44, average training loss: 3747.93, base loss: 4666.31
[INFO 2017-06-27 16:58:39,343 main.py:51] epoch 1670, training loss: 3537.87, average training loss: 3744.51, base loss: 4663.70
[INFO 2017-06-27 16:58:39,715 main.py:51] epoch 1671, training loss: 3482.76, average training loss: 3744.07, base loss: 4663.52
[INFO 2017-06-27 16:58:40,093 main.py:51] epoch 1672, training loss: 3743.66, average training loss: 3744.12, base loss: 4664.35
[INFO 2017-06-27 16:58:40,471 main.py:51] epoch 1673, training loss: 3462.74, average training loss: 3743.96, base loss: 4664.39
[INFO 2017-06-27 16:58:40,845 main.py:51] epoch 1674, training loss: 3457.60, average training loss: 3743.53, base loss: 4664.47
[INFO 2017-06-27 16:58:41,222 main.py:51] epoch 1675, training loss: 3276.55, average training loss: 3743.70, base loss: 4665.18
[INFO 2017-06-27 16:58:41,600 main.py:51] epoch 1676, training loss: 3032.99, average training loss: 3742.74, base loss: 4664.28
[INFO 2017-06-27 16:58:41,979 main.py:51] epoch 1677, training loss: 3182.62, average training loss: 3742.29, base loss: 4664.22
[INFO 2017-06-27 16:58:42,353 main.py:51] epoch 1678, training loss: 3398.41, average training loss: 3741.52, base loss: 4663.40
[INFO 2017-06-27 16:58:42,729 main.py:51] epoch 1679, training loss: 7196.59, average training loss: 3744.31, base loss: 4666.25
[INFO 2017-06-27 16:58:43,103 main.py:51] epoch 1680, training loss: 3165.05, average training loss: 3743.43, base loss: 4665.56
[INFO 2017-06-27 16:58:43,477 main.py:51] epoch 1681, training loss: 3696.92, average training loss: 3743.31, base loss: 4665.84
[INFO 2017-06-27 16:58:43,852 main.py:51] epoch 1682, training loss: 3146.62, average training loss: 3742.65, base loss: 4665.14
[INFO 2017-06-27 16:58:44,231 main.py:51] epoch 1683, training loss: 2987.03, average training loss: 3742.03, base loss: 4664.64
[INFO 2017-06-27 16:58:44,604 main.py:51] epoch 1684, training loss: 3432.51, average training loss: 3742.17, base loss: 4665.60
[INFO 2017-06-27 16:58:44,980 main.py:51] epoch 1685, training loss: 3428.86, average training loss: 3742.44, base loss: 4666.79
[INFO 2017-06-27 16:58:45,361 main.py:51] epoch 1686, training loss: 3578.93, average training loss: 3742.22, base loss: 4667.40
[INFO 2017-06-27 16:58:45,787 main.py:51] epoch 1687, training loss: 3125.35, average training loss: 3741.23, base loss: 4666.64
[INFO 2017-06-27 16:58:46,192 main.py:51] epoch 1688, training loss: 3260.26, average training loss: 3739.87, base loss: 4665.35
[INFO 2017-06-27 16:58:46,573 main.py:51] epoch 1689, training loss: 3425.19, average training loss: 3739.52, base loss: 4665.50
[INFO 2017-06-27 16:58:46,960 main.py:51] epoch 1690, training loss: 3177.15, average training loss: 3739.12, base loss: 4665.21
[INFO 2017-06-27 16:58:47,435 main.py:51] epoch 1691, training loss: 3338.26, average training loss: 3738.13, base loss: 4664.79
[INFO 2017-06-27 16:58:47,829 main.py:51] epoch 1692, training loss: 3415.24, average training loss: 3737.95, base loss: 4665.04
[INFO 2017-06-27 16:58:48,207 main.py:51] epoch 1693, training loss: 3408.80, average training loss: 3737.39, base loss: 4664.43
[INFO 2017-06-27 16:58:48,608 main.py:51] epoch 1694, training loss: 7503.62, average training loss: 3741.45, base loss: 4669.53
[INFO 2017-06-27 16:58:49,010 main.py:51] epoch 1695, training loss: 3394.73, average training loss: 3740.84, base loss: 4669.35
[INFO 2017-06-27 16:58:49,395 main.py:51] epoch 1696, training loss: 3053.39, average training loss: 3740.63, base loss: 4669.48
[INFO 2017-06-27 16:58:49,780 main.py:51] epoch 1697, training loss: 3167.61, average training loss: 3740.10, base loss: 4669.04
[INFO 2017-06-27 16:58:50,175 main.py:51] epoch 1698, training loss: 3155.71, average training loss: 3739.65, base loss: 4668.83
[INFO 2017-06-27 16:58:50,566 main.py:51] epoch 1699, training loss: 3623.83, average training loss: 3739.17, base loss: 4668.67
[INFO 2017-06-27 16:58:50,566 main.py:53] epoch 1699, testing
[INFO 2017-06-27 16:58:52,163 main.py:105] average testing loss: 3675.26, base loss: 4619.50
[INFO 2017-06-27 16:58:52,163 main.py:106] improve_loss: 944.24, improve_percent: 0.20
[INFO 2017-06-27 16:58:52,164 main.py:76] current best improved percent: 0.26
[INFO 2017-06-27 16:58:52,536 main.py:51] epoch 1700, training loss: 3573.25, average training loss: 3738.91, base loss: 4668.83
[INFO 2017-06-27 16:58:52,913 main.py:51] epoch 1701, training loss: 3037.66, average training loss: 3738.00, base loss: 4668.00
[INFO 2017-06-27 16:58:53,288 main.py:51] epoch 1702, training loss: 3650.31, average training loss: 3738.47, base loss: 4669.08
[INFO 2017-06-27 16:58:53,663 main.py:51] epoch 1703, training loss: 3782.47, average training loss: 3738.41, base loss: 4669.70
[INFO 2017-06-27 16:58:54,040 main.py:51] epoch 1704, training loss: 3399.13, average training loss: 3738.54, base loss: 4670.48
[INFO 2017-06-27 16:58:54,414 main.py:51] epoch 1705, training loss: 2892.54, average training loss: 3737.85, base loss: 4669.34
[INFO 2017-06-27 16:58:54,790 main.py:51] epoch 1706, training loss: 8078.45, average training loss: 3742.30, base loss: 4675.04
[INFO 2017-06-27 16:58:55,163 main.py:51] epoch 1707, training loss: 3376.70, average training loss: 3741.87, base loss: 4675.06
[INFO 2017-06-27 16:58:55,624 main.py:51] epoch 1708, training loss: 3555.20, average training loss: 3741.63, base loss: 4675.45
[INFO 2017-06-27 16:58:56,019 main.py:51] epoch 1709, training loss: 3440.68, average training loss: 3737.71, base loss: 4671.96
[INFO 2017-06-27 16:58:56,414 main.py:51] epoch 1710, training loss: 3557.82, average training loss: 3737.49, base loss: 4672.42
[INFO 2017-06-27 16:58:56,798 main.py:51] epoch 1711, training loss: 3541.75, average training loss: 3737.25, base loss: 4672.61
[INFO 2017-06-27 16:58:57,178 main.py:51] epoch 1712, training loss: 3378.92, average training loss: 3737.01, base loss: 4672.28
[INFO 2017-06-27 16:58:57,554 main.py:51] epoch 1713, training loss: 3317.33, average training loss: 3736.11, base loss: 4671.43
[INFO 2017-06-27 16:58:57,929 main.py:51] epoch 1714, training loss: 2983.65, average training loss: 3735.39, base loss: 4670.71
[INFO 2017-06-27 16:58:58,303 main.py:51] epoch 1715, training loss: 2735.48, average training loss: 3734.06, base loss: 4669.30
[INFO 2017-06-27 16:58:58,680 main.py:51] epoch 1716, training loss: 7530.62, average training loss: 3736.76, base loss: 4672.36
[INFO 2017-06-27 16:58:59,056 main.py:51] epoch 1717, training loss: 6739.73, average training loss: 3739.43, base loss: 4675.44
[INFO 2017-06-27 16:58:59,431 main.py:51] epoch 1718, training loss: 3759.63, average training loss: 3739.20, base loss: 4675.21
[INFO 2017-06-27 16:58:59,806 main.py:51] epoch 1719, training loss: 3291.11, average training loss: 3738.92, base loss: 4675.56
[INFO 2017-06-27 16:59:00,180 main.py:51] epoch 1720, training loss: 3660.41, average training loss: 3738.97, base loss: 4676.14
[INFO 2017-06-27 16:59:00,555 main.py:51] epoch 1721, training loss: 3534.23, average training loss: 3738.89, base loss: 4676.50
[INFO 2017-06-27 16:59:00,932 main.py:51] epoch 1722, training loss: 3497.34, average training loss: 3738.68, base loss: 4677.02
[INFO 2017-06-27 16:59:01,308 main.py:51] epoch 1723, training loss: 3680.05, average training loss: 3738.88, base loss: 4677.83
[INFO 2017-06-27 16:59:01,680 main.py:51] epoch 1724, training loss: 3554.62, average training loss: 3738.71, base loss: 4678.20
[INFO 2017-06-27 16:59:02,056 main.py:51] epoch 1725, training loss: 3499.32, average training loss: 3737.69, base loss: 4677.36
[INFO 2017-06-27 16:59:02,433 main.py:51] epoch 1726, training loss: 3256.58, average training loss: 3737.04, base loss: 4677.14
[INFO 2017-06-27 16:59:02,809 main.py:51] epoch 1727, training loss: 3978.91, average training loss: 3737.38, base loss: 4678.05
[INFO 2017-06-27 16:59:03,182 main.py:51] epoch 1728, training loss: 3423.80, average training loss: 3737.57, base loss: 4678.77
[INFO 2017-06-27 16:59:03,559 main.py:51] epoch 1729, training loss: 3171.29, average training loss: 3737.07, base loss: 4678.37
[INFO 2017-06-27 16:59:03,936 main.py:51] epoch 1730, training loss: 3195.16, average training loss: 3736.45, base loss: 4677.86
[INFO 2017-06-27 16:59:04,311 main.py:51] epoch 1731, training loss: 3206.70, average training loss: 3736.00, base loss: 4677.72
[INFO 2017-06-27 16:59:04,693 main.py:51] epoch 1732, training loss: 3466.99, average training loss: 3735.67, base loss: 4677.64
[INFO 2017-06-27 16:59:05,068 main.py:51] epoch 1733, training loss: 2919.80, average training loss: 3734.78, base loss: 4676.82
[INFO 2017-06-27 16:59:05,445 main.py:51] epoch 1734, training loss: 2835.72, average training loss: 3734.42, base loss: 4676.41
[INFO 2017-06-27 16:59:05,819 main.py:51] epoch 1735, training loss: 3446.09, average training loss: 3734.39, base loss: 4676.90
[INFO 2017-06-27 16:59:06,193 main.py:51] epoch 1736, training loss: 3273.92, average training loss: 3730.57, base loss: 4673.54
[INFO 2017-06-27 16:59:06,608 main.py:51] epoch 1737, training loss: 3219.11, average training loss: 3730.11, base loss: 4673.32
[INFO 2017-06-27 16:59:07,024 main.py:51] epoch 1738, training loss: 3255.95, average training loss: 3729.96, base loss: 4673.63
[INFO 2017-06-27 16:59:07,405 main.py:51] epoch 1739, training loss: 3624.57, average training loss: 3729.65, base loss: 4673.78
[INFO 2017-06-27 16:59:07,847 main.py:51] epoch 1740, training loss: 3687.53, average training loss: 3730.08, base loss: 4674.78
[INFO 2017-06-27 16:59:08,254 main.py:51] epoch 1741, training loss: 3494.77, average training loss: 3729.74, base loss: 4675.11
[INFO 2017-06-27 16:59:08,634 main.py:51] epoch 1742, training loss: 3079.10, average training loss: 3729.17, base loss: 4674.41
[INFO 2017-06-27 16:59:09,011 main.py:51] epoch 1743, training loss: 3280.52, average training loss: 3728.92, base loss: 4674.76
[INFO 2017-06-27 16:59:09,411 main.py:51] epoch 1744, training loss: 3267.27, average training loss: 3728.63, base loss: 4674.84
[INFO 2017-06-27 16:59:09,787 main.py:51] epoch 1745, training loss: 3652.19, average training loss: 3728.38, base loss: 4675.22
[INFO 2017-06-27 16:59:10,160 main.py:51] epoch 1746, training loss: 3202.25, average training loss: 3727.62, base loss: 4674.46
[INFO 2017-06-27 16:59:10,534 main.py:51] epoch 1747, training loss: 3637.09, average training loss: 3727.93, base loss: 4675.46
[INFO 2017-06-27 16:59:10,908 main.py:51] epoch 1748, training loss: 3080.56, average training loss: 3727.21, base loss: 4674.67
[INFO 2017-06-27 16:59:11,284 main.py:51] epoch 1749, training loss: 2760.43, average training loss: 3726.03, base loss: 4673.14
[INFO 2017-06-27 16:59:11,658 main.py:51] epoch 1750, training loss: 3471.63, average training loss: 3725.64, base loss: 4672.97
[INFO 2017-06-27 16:59:12,031 main.py:51] epoch 1751, training loss: 3254.00, average training loss: 3724.83, base loss: 4672.37
[INFO 2017-06-27 16:59:12,403 main.py:51] epoch 1752, training loss: 3675.07, average training loss: 3725.44, base loss: 4673.95
[INFO 2017-06-27 16:59:12,780 main.py:51] epoch 1753, training loss: 3077.19, average training loss: 3724.58, base loss: 4672.98
[INFO 2017-06-27 16:59:13,154 main.py:51] epoch 1754, training loss: 3451.06, average training loss: 3724.23, base loss: 4673.32
[INFO 2017-06-27 16:59:13,526 main.py:51] epoch 1755, training loss: 3131.76, average training loss: 3723.77, base loss: 4673.15
[INFO 2017-06-27 16:59:13,904 main.py:51] epoch 1756, training loss: 3393.52, average training loss: 3723.65, base loss: 4673.69
[INFO 2017-06-27 16:59:14,277 main.py:51] epoch 1757, training loss: 3127.55, average training loss: 3723.08, base loss: 4673.51
[INFO 2017-06-27 16:59:14,657 main.py:51] epoch 1758, training loss: 3388.95, average training loss: 3722.99, base loss: 4674.20
[INFO 2017-06-27 16:59:15,034 main.py:51] epoch 1759, training loss: 3130.60, average training loss: 3722.51, base loss: 4673.90
[INFO 2017-06-27 16:59:15,408 main.py:51] epoch 1760, training loss: 3159.98, average training loss: 3721.80, base loss: 4673.51
[INFO 2017-06-27 16:59:15,789 main.py:51] epoch 1761, training loss: 3363.61, average training loss: 3722.14, base loss: 4674.62
[INFO 2017-06-27 16:59:16,165 main.py:51] epoch 1762, training loss: 3558.31, average training loss: 3722.08, base loss: 4675.24
[INFO 2017-06-27 16:59:16,568 main.py:51] epoch 1763, training loss: 6725.21, average training loss: 3724.79, base loss: 4678.63
[INFO 2017-06-27 16:59:16,966 main.py:51] epoch 1764, training loss: 2861.94, average training loss: 3723.83, base loss: 4677.44
[INFO 2017-06-27 16:59:17,348 main.py:51] epoch 1765, training loss: 3468.43, average training loss: 3723.82, base loss: 4677.87
[INFO 2017-06-27 16:59:17,736 main.py:51] epoch 1766, training loss: 3118.02, average training loss: 3722.92, base loss: 4677.24
[INFO 2017-06-27 16:59:18,115 main.py:51] epoch 1767, training loss: 2722.22, average training loss: 3721.97, base loss: 4676.18
[INFO 2017-06-27 16:59:18,490 main.py:51] epoch 1768, training loss: 3176.62, average training loss: 3721.24, base loss: 4675.25
[INFO 2017-06-27 16:59:18,883 main.py:51] epoch 1769, training loss: 3574.14, average training loss: 3721.27, base loss: 4675.79
[INFO 2017-06-27 16:59:19,257 main.py:51] epoch 1770, training loss: 3506.48, average training loss: 3721.41, base loss: 4676.49
[INFO 2017-06-27 16:59:19,625 main.py:51] epoch 1771, training loss: 3703.59, average training loss: 3721.49, base loss: 4676.94
[INFO 2017-06-27 16:59:20,004 main.py:51] epoch 1772, training loss: 3285.10, average training loss: 3721.50, base loss: 4677.58
[INFO 2017-06-27 16:59:20,381 main.py:51] epoch 1773, training loss: 3284.96, average training loss: 3721.19, base loss: 4677.54
[INFO 2017-06-27 16:59:20,756 main.py:51] epoch 1774, training loss: 3424.17, average training loss: 3717.96, base loss: 4674.89
[INFO 2017-06-27 16:59:21,130 main.py:51] epoch 1775, training loss: 3130.69, average training loss: 3717.82, base loss: 4675.08
[INFO 2017-06-27 16:59:21,503 main.py:51] epoch 1776, training loss: 2976.88, average training loss: 3717.06, base loss: 4674.49
[INFO 2017-06-27 16:59:21,879 main.py:51] epoch 1777, training loss: 3687.02, average training loss: 3717.32, base loss: 4674.97
[INFO 2017-06-27 16:59:22,253 main.py:51] epoch 1778, training loss: 3842.72, average training loss: 3717.23, base loss: 4674.89
[INFO 2017-06-27 16:59:22,636 main.py:51] epoch 1779, training loss: 3447.18, average training loss: 3716.86, base loss: 4675.02
[INFO 2017-06-27 16:59:23,010 main.py:51] epoch 1780, training loss: 3620.24, average training loss: 3716.12, base loss: 4674.26
[INFO 2017-06-27 16:59:23,389 main.py:51] epoch 1781, training loss: 3209.49, average training loss: 3712.28, base loss: 4670.64
[INFO 2017-06-27 16:59:23,766 main.py:51] epoch 1782, training loss: 3097.77, average training loss: 3711.63, base loss: 4669.85
[INFO 2017-06-27 16:59:24,242 main.py:51] epoch 1783, training loss: 3331.98, average training loss: 3711.21, base loss: 4669.69
[INFO 2017-06-27 16:59:24,638 main.py:51] epoch 1784, training loss: 3077.10, average training loss: 3710.33, base loss: 4668.86
[INFO 2017-06-27 16:59:25,093 main.py:51] epoch 1785, training loss: 3496.18, average training loss: 3710.00, base loss: 4668.60
[INFO 2017-06-27 16:59:25,517 main.py:51] epoch 1786, training loss: 3225.61, average training loss: 3709.74, base loss: 4668.57
[INFO 2017-06-27 16:59:25,895 main.py:51] epoch 1787, training loss: 2936.86, average training loss: 3709.32, base loss: 4668.17
[INFO 2017-06-27 16:59:26,356 main.py:51] epoch 1788, training loss: 3306.37, average training loss: 3708.62, base loss: 4667.40
[INFO 2017-06-27 16:59:26,734 main.py:51] epoch 1789, training loss: 3009.55, average training loss: 3708.10, base loss: 4666.99
[INFO 2017-06-27 16:59:27,130 main.py:51] epoch 1790, training loss: 3630.62, average training loss: 3707.76, base loss: 4667.32
[INFO 2017-06-27 16:59:27,505 main.py:51] epoch 1791, training loss: 3084.74, average training loss: 3707.50, base loss: 4667.19
[INFO 2017-06-27 16:59:27,911 main.py:51] epoch 1792, training loss: 3351.26, average training loss: 3707.14, base loss: 4667.11
[INFO 2017-06-27 16:59:28,286 main.py:51] epoch 1793, training loss: 3802.16, average training loss: 3707.48, base loss: 4668.30
[INFO 2017-06-27 16:59:28,664 main.py:51] epoch 1794, training loss: 3444.22, average training loss: 3707.62, base loss: 4669.17
[INFO 2017-06-27 16:59:29,037 main.py:51] epoch 1795, training loss: 3372.25, average training loss: 3707.35, base loss: 4669.27
[INFO 2017-06-27 16:59:29,415 main.py:51] epoch 1796, training loss: 3652.99, average training loss: 3707.43, base loss: 4669.97
[INFO 2017-06-27 16:59:29,789 main.py:51] epoch 1797, training loss: 3376.16, average training loss: 3706.46, base loss: 4669.04
[INFO 2017-06-27 16:59:30,165 main.py:51] epoch 1798, training loss: 3158.40, average training loss: 3705.57, base loss: 4667.94
[INFO 2017-06-27 16:59:30,542 main.py:51] epoch 1799, training loss: 3315.08, average training loss: 3704.99, base loss: 4667.54
[INFO 2017-06-27 16:59:30,542 main.py:53] epoch 1799, testing
[INFO 2017-06-27 16:59:32,150 main.py:105] average testing loss: 3320.55, base loss: 4444.48
[INFO 2017-06-27 16:59:32,151 main.py:106] improve_loss: 1123.93, improve_percent: 0.25
[INFO 2017-06-27 16:59:32,151 main.py:76] current best improved percent: 0.26
[INFO 2017-06-27 16:59:32,527 main.py:51] epoch 1800, training loss: 3063.81, average training loss: 3704.43, base loss: 4667.17
[INFO 2017-06-27 16:59:32,902 main.py:51] epoch 1801, training loss: 3010.46, average training loss: 3703.90, base loss: 4666.70
[INFO 2017-06-27 16:59:33,276 main.py:51] epoch 1802, training loss: 3418.64, average training loss: 3703.27, base loss: 4666.06
[INFO 2017-06-27 16:59:33,653 main.py:51] epoch 1803, training loss: 2958.76, average training loss: 3702.79, base loss: 4665.99
[INFO 2017-06-27 16:59:34,023 main.py:51] epoch 1804, training loss: 3045.07, average training loss: 3702.45, base loss: 4666.09
[INFO 2017-06-27 16:59:34,397 main.py:51] epoch 1805, training loss: 3245.44, average training loss: 3702.26, base loss: 4665.85
[INFO 2017-06-27 16:59:34,769 main.py:51] epoch 1806, training loss: 3220.33, average training loss: 3701.11, base loss: 4665.07
[INFO 2017-06-27 16:59:35,141 main.py:51] epoch 1807, training loss: 3777.50, average training loss: 3700.88, base loss: 4665.21
[INFO 2017-06-27 16:59:35,523 main.py:51] epoch 1808, training loss: 3091.25, average training loss: 3700.57, base loss: 4665.05
[INFO 2017-06-27 16:59:35,901 main.py:51] epoch 1809, training loss: 3129.81, average training loss: 3699.63, base loss: 4664.41
[INFO 2017-06-27 16:59:36,278 main.py:51] epoch 1810, training loss: 3799.58, average training loss: 3699.57, base loss: 4664.77
[INFO 2017-06-27 16:59:36,762 main.py:51] epoch 1811, training loss: 3355.81, average training loss: 3699.08, base loss: 4664.35
[INFO 2017-06-27 16:59:37,145 main.py:51] epoch 1812, training loss: 3436.37, average training loss: 3699.03, base loss: 4664.55
[INFO 2017-06-27 16:59:37,610 main.py:51] epoch 1813, training loss: 3522.31, average training loss: 3698.79, base loss: 4664.63
[INFO 2017-06-27 16:59:38,019 main.py:51] epoch 1814, training loss: 3189.36, average training loss: 3698.49, base loss: 4664.85
[INFO 2017-06-27 16:59:38,428 main.py:51] epoch 1815, training loss: 3284.52, average training loss: 3698.23, base loss: 4664.81
[INFO 2017-06-27 16:59:38,853 main.py:51] epoch 1816, training loss: 3105.11, average training loss: 3697.46, base loss: 4664.36
[INFO 2017-06-27 16:59:39,237 main.py:51] epoch 1817, training loss: 2850.82, average training loss: 3693.28, base loss: 4660.45
[INFO 2017-06-27 16:59:39,614 main.py:51] epoch 1818, training loss: 3375.29, average training loss: 3692.71, base loss: 4659.71
[INFO 2017-06-27 16:59:40,006 main.py:51] epoch 1819, training loss: 3252.28, average training loss: 3691.92, base loss: 4658.62
[INFO 2017-06-27 16:59:40,399 main.py:51] epoch 1820, training loss: 3333.51, average training loss: 3691.69, base loss: 4658.68
[INFO 2017-06-27 16:59:40,779 main.py:51] epoch 1821, training loss: 3152.91, average training loss: 3691.56, base loss: 4658.67
[INFO 2017-06-27 16:59:41,240 main.py:51] epoch 1822, training loss: 3553.83, average training loss: 3691.81, base loss: 4659.37
[INFO 2017-06-27 16:59:41,655 main.py:51] epoch 1823, training loss: 3153.93, average training loss: 3691.29, base loss: 4659.25
[INFO 2017-06-27 16:59:42,044 main.py:51] epoch 1824, training loss: 2953.17, average training loss: 3689.94, base loss: 4657.76
[INFO 2017-06-27 16:59:42,422 main.py:51] epoch 1825, training loss: 4057.42, average training loss: 3689.87, base loss: 4658.43
[INFO 2017-06-27 16:59:42,801 main.py:51] epoch 1826, training loss: 3201.39, average training loss: 3689.60, base loss: 4658.24
[INFO 2017-06-27 16:59:43,175 main.py:51] epoch 1827, training loss: 3099.85, average training loss: 3688.85, base loss: 4657.88
[INFO 2017-06-27 16:59:43,551 main.py:51] epoch 1828, training loss: 3648.64, average training loss: 3688.95, base loss: 4658.93
[INFO 2017-06-27 16:59:43,922 main.py:51] epoch 1829, training loss: 3114.91, average training loss: 3688.28, base loss: 4658.59
[INFO 2017-06-27 16:59:44,298 main.py:51] epoch 1830, training loss: 3167.85, average training loss: 3688.03, base loss: 4658.79
[INFO 2017-06-27 16:59:44,750 main.py:51] epoch 1831, training loss: 3275.72, average training loss: 3687.78, base loss: 4658.63
[INFO 2017-06-27 16:59:45,159 main.py:51] epoch 1832, training loss: 3407.75, average training loss: 3687.71, base loss: 4658.85
[INFO 2017-06-27 16:59:45,559 main.py:51] epoch 1833, training loss: 3243.20, average training loss: 3687.14, base loss: 4658.88
[INFO 2017-06-27 16:59:45,948 main.py:51] epoch 1834, training loss: 3592.52, average training loss: 3686.85, base loss: 4658.59
[INFO 2017-06-27 16:59:46,322 main.py:51] epoch 1835, training loss: 3532.88, average training loss: 3686.60, base loss: 4659.11
[INFO 2017-06-27 16:59:46,705 main.py:51] epoch 1836, training loss: 3763.58, average training loss: 3686.76, base loss: 4659.87
[INFO 2017-06-27 16:59:47,081 main.py:51] epoch 1837, training loss: 3444.93, average training loss: 3686.56, base loss: 4660.22
[INFO 2017-06-27 16:59:47,541 main.py:51] epoch 1838, training loss: 3317.78, average training loss: 3686.68, base loss: 4661.05
[INFO 2017-06-27 16:59:47,920 main.py:51] epoch 1839, training loss: 3239.82, average training loss: 3686.15, base loss: 4660.70
[INFO 2017-06-27 16:59:48,300 main.py:51] epoch 1840, training loss: 3004.24, average training loss: 3684.84, base loss: 4659.29
[INFO 2017-06-27 16:59:48,679 main.py:51] epoch 1841, training loss: 3385.01, average training loss: 3684.56, base loss: 4659.33
[INFO 2017-06-27 16:59:49,050 main.py:51] epoch 1842, training loss: 3700.59, average training loss: 3684.84, base loss: 4660.27
[INFO 2017-06-27 16:59:49,520 main.py:51] epoch 1843, training loss: 3339.37, average training loss: 3684.64, base loss: 4660.34
[INFO 2017-06-27 16:59:49,897 main.py:51] epoch 1844, training loss: 3322.67, average training loss: 3684.54, base loss: 4660.66
[INFO 2017-06-27 16:59:50,298 main.py:51] epoch 1845, training loss: 3281.32, average training loss: 3684.32, base loss: 4660.88
[INFO 2017-06-27 16:59:50,685 main.py:51] epoch 1846, training loss: 3094.60, average training loss: 3684.09, base loss: 4660.76
[INFO 2017-06-27 16:59:51,068 main.py:51] epoch 1847, training loss: 3234.01, average training loss: 3683.06, base loss: 4659.54
[INFO 2017-06-27 16:59:51,446 main.py:51] epoch 1848, training loss: 3482.77, average training loss: 3683.20, base loss: 4660.06
[INFO 2017-06-27 16:59:51,823 main.py:51] epoch 1849, training loss: 3436.83, average training loss: 3682.55, base loss: 4659.66
[INFO 2017-06-27 16:59:52,210 main.py:51] epoch 1850, training loss: 2908.16, average training loss: 3678.55, base loss: 4655.90
[INFO 2017-06-27 16:59:52,590 main.py:51] epoch 1851, training loss: 3433.41, average training loss: 3678.22, base loss: 4655.98
[INFO 2017-06-27 16:59:52,974 main.py:51] epoch 1852, training loss: 3286.14, average training loss: 3677.56, base loss: 4655.48
[INFO 2017-06-27 16:59:53,353 main.py:51] epoch 1853, training loss: 3537.43, average training loss: 3677.24, base loss: 4655.66
[INFO 2017-06-27 16:59:53,735 main.py:51] epoch 1854, training loss: 3258.80, average training loss: 3676.79, base loss: 4655.26
[INFO 2017-06-27 16:59:54,109 main.py:51] epoch 1855, training loss: 2854.51, average training loss: 3675.87, base loss: 4654.17
[INFO 2017-06-27 16:59:54,490 main.py:51] epoch 1856, training loss: 3654.48, average training loss: 3675.87, base loss: 4654.23
[INFO 2017-06-27 16:59:54,963 main.py:51] epoch 1857, training loss: 3488.52, average training loss: 3675.92, base loss: 4655.20
[INFO 2017-06-27 16:59:55,350 main.py:51] epoch 1858, training loss: 3361.66, average training loss: 3674.94, base loss: 4654.23
[INFO 2017-06-27 16:59:55,730 main.py:51] epoch 1859, training loss: 3284.20, average training loss: 3674.42, base loss: 4653.65
[INFO 2017-06-27 16:59:56,104 main.py:51] epoch 1860, training loss: 3409.40, average training loss: 3673.94, base loss: 4653.56
[INFO 2017-06-27 16:59:56,479 main.py:51] epoch 1861, training loss: 3022.28, average training loss: 3673.14, base loss: 4652.69
[INFO 2017-06-27 16:59:56,877 main.py:51] epoch 1862, training loss: 3623.58, average training loss: 3669.43, base loss: 4649.54
[INFO 2017-06-27 16:59:57,257 main.py:51] epoch 1863, training loss: 3517.49, average training loss: 3669.02, base loss: 4649.29
[INFO 2017-06-27 16:59:57,631 main.py:51] epoch 1864, training loss: 3065.37, average training loss: 3668.69, base loss: 4648.68
[INFO 2017-06-27 16:59:58,008 main.py:51] epoch 1865, training loss: 3702.58, average training loss: 3668.60, base loss: 4649.16
[INFO 2017-06-27 16:59:58,383 main.py:51] epoch 1866, training loss: 3539.31, average training loss: 3668.50, base loss: 4649.76
[INFO 2017-06-27 16:59:58,758 main.py:51] epoch 1867, training loss: 3371.03, average training loss: 3667.74, base loss: 4648.71
[INFO 2017-06-27 16:59:59,132 main.py:51] epoch 1868, training loss: 3806.85, average training loss: 3667.70, base loss: 4649.26
[INFO 2017-06-27 16:59:59,505 main.py:51] epoch 1869, training loss: 3598.39, average training loss: 3667.57, base loss: 4649.44
[INFO 2017-06-27 16:59:59,883 main.py:51] epoch 1870, training loss: 3279.24, average training loss: 3667.43, base loss: 4649.73
[INFO 2017-06-27 17:00:00,263 main.py:51] epoch 1871, training loss: 3683.53, average training loss: 3667.94, base loss: 4650.99
[INFO 2017-06-27 17:00:00,637 main.py:51] epoch 1872, training loss: 3430.91, average training loss: 3667.71, base loss: 4651.44
[INFO 2017-06-27 17:00:01,095 main.py:51] epoch 1873, training loss: 3229.20, average training loss: 3667.00, base loss: 4650.64
[INFO 2017-06-27 17:00:01,493 main.py:51] epoch 1874, training loss: 3450.99, average training loss: 3663.32, base loss: 4647.43
[INFO 2017-06-27 17:00:01,895 main.py:51] epoch 1875, training loss: 3164.21, average training loss: 3662.48, base loss: 4646.29
[INFO 2017-06-27 17:00:02,319 main.py:51] epoch 1876, training loss: 3399.73, average training loss: 3662.23, base loss: 4646.34
[INFO 2017-06-27 17:00:02,701 main.py:51] epoch 1877, training loss: 3031.72, average training loss: 3661.41, base loss: 4645.52
[INFO 2017-06-27 17:00:03,078 main.py:51] epoch 1878, training loss: 2923.18, average training loss: 3660.51, base loss: 4644.36
[INFO 2017-06-27 17:00:03,456 main.py:51] epoch 1879, training loss: 3376.26, average training loss: 3660.04, base loss: 4644.01
[INFO 2017-06-27 17:00:03,831 main.py:51] epoch 1880, training loss: 3431.54, average training loss: 3660.10, base loss: 4645.15
[INFO 2017-06-27 17:00:04,213 main.py:51] epoch 1881, training loss: 3473.70, average training loss: 3659.79, base loss: 4645.17
[INFO 2017-06-27 17:00:04,594 main.py:51] epoch 1882, training loss: 3150.66, average training loss: 3659.16, base loss: 4644.97
[INFO 2017-06-27 17:00:04,971 main.py:51] epoch 1883, training loss: 3153.70, average training loss: 3658.45, base loss: 4644.45
[INFO 2017-06-27 17:00:05,346 main.py:51] epoch 1884, training loss: 3494.23, average training loss: 3654.49, base loss: 4640.69
[INFO 2017-06-27 17:00:05,721 main.py:51] epoch 1885, training loss: 3469.82, average training loss: 3653.62, base loss: 4640.08
[INFO 2017-06-27 17:00:06,093 main.py:51] epoch 1886, training loss: 3564.81, average training loss: 3653.79, base loss: 4641.29
[INFO 2017-06-27 17:00:06,465 main.py:51] epoch 1887, training loss: 3539.47, average training loss: 3650.09, base loss: 4637.79
[INFO 2017-06-27 17:00:06,835 main.py:51] epoch 1888, training loss: 3179.41, average training loss: 3649.72, base loss: 4637.98
[INFO 2017-06-27 17:00:07,209 main.py:51] epoch 1889, training loss: 3060.60, average training loss: 3648.99, base loss: 4637.28
[INFO 2017-06-27 17:00:07,584 main.py:51] epoch 1890, training loss: 3208.60, average training loss: 3645.37, base loss: 4634.18
[INFO 2017-06-27 17:00:07,959 main.py:51] epoch 1891, training loss: 3181.92, average training loss: 3644.77, base loss: 4633.60
[INFO 2017-06-27 17:00:08,343 main.py:51] epoch 1892, training loss: 3537.81, average training loss: 3644.54, base loss: 4633.61
[INFO 2017-06-27 17:00:08,719 main.py:51] epoch 1893, training loss: 3214.83, average training loss: 3644.24, base loss: 4633.72
[INFO 2017-06-27 17:00:09,115 main.py:51] epoch 1894, training loss: 2671.44, average training loss: 3639.25, base loss: 4628.34
[INFO 2017-06-27 17:00:09,516 main.py:51] epoch 1895, training loss: 3134.26, average training loss: 3638.81, base loss: 4628.14
[INFO 2017-06-27 17:00:09,893 main.py:51] epoch 1896, training loss: 3267.19, average training loss: 3638.30, base loss: 4627.80
[INFO 2017-06-27 17:00:10,285 main.py:51] epoch 1897, training loss: 3243.86, average training loss: 3638.34, base loss: 4628.04
[INFO 2017-06-27 17:00:10,684 main.py:51] epoch 1898, training loss: 4046.19, average training loss: 3638.77, base loss: 4629.03
[INFO 2017-06-27 17:00:11,060 main.py:51] epoch 1899, training loss: 3480.29, average training loss: 3638.55, base loss: 4629.24
[INFO 2017-06-27 17:00:11,060 main.py:53] epoch 1899, testing
[INFO 2017-06-27 17:00:12,658 main.py:105] average testing loss: 3284.74, base loss: 4395.99
[INFO 2017-06-27 17:00:12,658 main.py:106] improve_loss: 1111.25, improve_percent: 0.25
[INFO 2017-06-27 17:00:12,659 main.py:76] current best improved percent: 0.26
[INFO 2017-06-27 17:00:13,033 main.py:51] epoch 1900, training loss: 3621.02, average training loss: 3638.96, base loss: 4630.45
[INFO 2017-06-27 17:00:13,408 main.py:51] epoch 1901, training loss: 3191.36, average training loss: 3638.44, base loss: 4629.60
[INFO 2017-06-27 17:00:13,800 main.py:51] epoch 1902, training loss: 3218.71, average training loss: 3638.15, base loss: 4629.51
[INFO 2017-06-27 17:00:14,196 main.py:51] epoch 1903, training loss: 3426.87, average training loss: 3637.77, base loss: 4629.37
[INFO 2017-06-27 17:00:14,590 main.py:51] epoch 1904, training loss: 3223.13, average training loss: 3637.25, base loss: 4629.19
[INFO 2017-06-27 17:00:14,969 main.py:51] epoch 1905, training loss: 3239.36, average training loss: 3636.98, base loss: 4629.32
[INFO 2017-06-27 17:00:15,347 main.py:51] epoch 1906, training loss: 3218.94, average training loss: 3636.67, base loss: 4629.11
[INFO 2017-06-27 17:00:15,754 main.py:51] epoch 1907, training loss: 3297.55, average training loss: 3636.38, base loss: 4629.20
[INFO 2017-06-27 17:00:16,131 main.py:51] epoch 1908, training loss: 3250.18, average training loss: 3636.02, base loss: 4629.33
[INFO 2017-06-27 17:00:16,510 main.py:51] epoch 1909, training loss: 2851.05, average training loss: 3634.94, base loss: 4628.01
[INFO 2017-06-27 17:00:16,890 main.py:51] epoch 1910, training loss: 3715.35, average training loss: 3635.00, base loss: 4628.37
[INFO 2017-06-27 17:00:17,272 main.py:51] epoch 1911, training loss: 3212.50, average training loss: 3633.94, base loss: 4627.25
[INFO 2017-06-27 17:00:17,648 main.py:51] epoch 1912, training loss: 3560.42, average training loss: 3633.50, base loss: 4627.19
[INFO 2017-06-27 17:00:18,024 main.py:51] epoch 1913, training loss: 2905.98, average training loss: 3633.04, base loss: 4626.77
[INFO 2017-06-27 17:00:18,406 main.py:51] epoch 1914, training loss: 3198.17, average training loss: 3632.82, base loss: 4626.69
[INFO 2017-06-27 17:00:18,783 main.py:51] epoch 1915, training loss: 3227.43, average training loss: 3632.93, base loss: 4627.33
[INFO 2017-06-27 17:00:19,164 main.py:51] epoch 1916, training loss: 3549.12, average training loss: 3632.44, base loss: 4627.10
[INFO 2017-06-27 17:00:19,541 main.py:51] epoch 1917, training loss: 3230.54, average training loss: 3631.34, base loss: 4626.21
[INFO 2017-06-27 17:00:20,012 main.py:51] epoch 1918, training loss: 3228.71, average training loss: 3630.86, base loss: 4625.82
[INFO 2017-06-27 17:00:20,401 main.py:51] epoch 1919, training loss: 2954.75, average training loss: 3629.91, base loss: 4624.99
[INFO 2017-06-27 17:00:20,785 main.py:51] epoch 1920, training loss: 3501.78, average training loss: 3629.61, base loss: 4624.96
[INFO 2017-06-27 17:00:21,255 main.py:51] epoch 1921, training loss: 3432.69, average training loss: 3629.58, base loss: 4625.61
[INFO 2017-06-27 17:00:21,639 main.py:51] epoch 1922, training loss: 3019.30, average training loss: 3629.13, base loss: 4624.94
[INFO 2017-06-27 17:00:22,016 main.py:51] epoch 1923, training loss: 3061.62, average training loss: 3627.70, base loss: 4623.37
[INFO 2017-06-27 17:00:22,467 main.py:51] epoch 1924, training loss: 3496.56, average training loss: 3627.38, base loss: 4623.26
[INFO 2017-06-27 17:00:22,853 main.py:51] epoch 1925, training loss: 3229.25, average training loss: 3627.04, base loss: 4623.98
[INFO 2017-06-27 17:00:23,233 main.py:51] epoch 1926, training loss: 3231.73, average training loss: 3627.00, base loss: 4624.32
[INFO 2017-06-27 17:00:23,614 main.py:51] epoch 1927, training loss: 3630.09, average training loss: 3627.22, base loss: 4625.41
[INFO 2017-06-27 17:00:23,998 main.py:51] epoch 1928, training loss: 3428.54, average training loss: 3627.26, base loss: 4625.92
[INFO 2017-06-27 17:00:24,387 main.py:51] epoch 1929, training loss: 3125.57, average training loss: 3623.22, base loss: 4621.79
[INFO 2017-06-27 17:00:24,764 main.py:51] epoch 1930, training loss: 3335.95, average training loss: 3622.61, base loss: 4621.25
[INFO 2017-06-27 17:00:25,140 main.py:51] epoch 1931, training loss: 3685.96, average training loss: 3622.48, base loss: 4621.58
[INFO 2017-06-27 17:00:25,516 main.py:51] epoch 1932, training loss: 3063.58, average training loss: 3621.61, base loss: 4620.32
[INFO 2017-06-27 17:00:25,895 main.py:51] epoch 1933, training loss: 3648.59, average training loss: 3618.55, base loss: 4618.20
[INFO 2017-06-27 17:00:26,271 main.py:51] epoch 1934, training loss: 3675.22, average training loss: 3618.52, base loss: 4618.83
[INFO 2017-06-27 17:00:26,647 main.py:51] epoch 1935, training loss: 3286.37, average training loss: 3617.68, base loss: 4617.84
[INFO 2017-06-27 17:00:27,023 main.py:51] epoch 1936, training loss: 3257.95, average training loss: 3617.16, base loss: 4617.78
[INFO 2017-06-27 17:00:27,399 main.py:51] epoch 1937, training loss: 3480.28, average training loss: 3616.77, base loss: 4617.69
[INFO 2017-06-27 17:00:27,776 main.py:51] epoch 1938, training loss: 3566.58, average training loss: 3616.30, base loss: 4617.48
[INFO 2017-06-27 17:00:28,163 main.py:51] epoch 1939, training loss: 3633.90, average training loss: 3616.26, base loss: 4618.21
[INFO 2017-06-27 17:00:28,540 main.py:51] epoch 1940, training loss: 3478.18, average training loss: 3615.94, base loss: 4618.60
[INFO 2017-06-27 17:00:28,917 main.py:51] epoch 1941, training loss: 3182.14, average training loss: 3615.86, base loss: 4619.21
[INFO 2017-06-27 17:00:29,298 main.py:51] epoch 1942, training loss: 3526.65, average training loss: 3615.64, base loss: 4619.20
[INFO 2017-06-27 17:00:29,678 main.py:51] epoch 1943, training loss: 3178.92, average training loss: 3614.98, base loss: 4618.51
[INFO 2017-06-27 17:00:30,073 main.py:51] epoch 1944, training loss: 3003.80, average training loss: 3614.40, base loss: 4618.21
[INFO 2017-06-27 17:00:30,455 main.py:51] epoch 1945, training loss: 3579.70, average training loss: 3613.97, base loss: 4618.28
[INFO 2017-06-27 17:00:30,831 main.py:51] epoch 1946, training loss: 3348.46, average training loss: 3613.61, base loss: 4618.33
[INFO 2017-06-27 17:00:31,209 main.py:51] epoch 1947, training loss: 3398.62, average training loss: 3613.41, base loss: 4618.68
[INFO 2017-06-27 17:00:31,586 main.py:51] epoch 1948, training loss: 3234.94, average training loss: 3612.60, base loss: 4618.20
[INFO 2017-06-27 17:00:31,962 main.py:51] epoch 1949, training loss: 2971.63, average training loss: 3612.33, base loss: 4618.02
[INFO 2017-06-27 17:00:32,339 main.py:51] epoch 1950, training loss: 2966.44, average training loss: 3611.92, base loss: 4617.78
[INFO 2017-06-27 17:00:32,715 main.py:51] epoch 1951, training loss: 3033.56, average training loss: 3611.31, base loss: 4617.28
[INFO 2017-06-27 17:00:33,093 main.py:51] epoch 1952, training loss: 4081.98, average training loss: 3611.32, base loss: 4617.63
[INFO 2017-06-27 17:00:33,470 main.py:51] epoch 1953, training loss: 3113.17, average training loss: 3611.06, base loss: 4617.33
[INFO 2017-06-27 17:00:33,895 main.py:51] epoch 1954, training loss: 3490.29, average training loss: 3610.80, base loss: 4617.55
[INFO 2017-06-27 17:00:34,309 main.py:51] epoch 1955, training loss: 3185.55, average training loss: 3610.62, base loss: 4617.83
[INFO 2017-06-27 17:00:34,693 main.py:51] epoch 1956, training loss: 3396.79, average training loss: 3610.32, base loss: 4618.05
[INFO 2017-06-27 17:00:35,070 main.py:51] epoch 1957, training loss: 3614.53, average training loss: 3609.89, base loss: 4618.18
[INFO 2017-06-27 17:00:35,446 main.py:51] epoch 1958, training loss: 2851.89, average training loss: 3608.66, base loss: 4616.21
[INFO 2017-06-27 17:00:35,819 main.py:51] epoch 1959, training loss: 3709.07, average training loss: 3608.63, base loss: 4616.23
[INFO 2017-06-27 17:00:36,195 main.py:51] epoch 1960, training loss: 3601.78, average training loss: 3608.61, base loss: 4616.43
[INFO 2017-06-27 17:00:36,586 main.py:51] epoch 1961, training loss: 3513.15, average training loss: 3607.67, base loss: 4615.58
[INFO 2017-06-27 17:00:37,016 main.py:51] epoch 1962, training loss: 3345.48, average training loss: 3607.82, base loss: 4616.52
[INFO 2017-06-27 17:00:37,399 main.py:51] epoch 1963, training loss: 3247.03, average training loss: 3607.66, base loss: 4616.66
[INFO 2017-06-27 17:00:37,861 main.py:51] epoch 1964, training loss: 3020.33, average training loss: 3607.40, base loss: 4616.40
[INFO 2017-06-27 17:00:38,261 main.py:51] epoch 1965, training loss: 3676.51, average training loss: 3607.46, base loss: 4616.62
[INFO 2017-06-27 17:00:38,711 main.py:51] epoch 1966, training loss: 3229.33, average training loss: 3607.16, base loss: 4616.55
[INFO 2017-06-27 17:00:39,128 main.py:51] epoch 1967, training loss: 3551.58, average training loss: 3606.95, base loss: 4616.58
[INFO 2017-06-27 17:00:39,571 main.py:51] epoch 1968, training loss: 3728.86, average training loss: 3607.22, base loss: 4617.39
[INFO 2017-06-27 17:00:39,983 main.py:51] epoch 1969, training loss: 3055.39, average training loss: 3606.36, base loss: 4616.15
[INFO 2017-06-27 17:00:40,431 main.py:51] epoch 1970, training loss: 3026.80, average training loss: 3605.92, base loss: 4615.56
[INFO 2017-06-27 17:00:40,849 main.py:51] epoch 1971, training loss: 3053.26, average training loss: 3605.22, base loss: 4615.04
[INFO 2017-06-27 17:00:41,233 main.py:51] epoch 1972, training loss: 3021.00, average training loss: 3604.62, base loss: 4614.33
[INFO 2017-06-27 17:00:41,612 main.py:51] epoch 1973, training loss: 3088.14, average training loss: 3604.01, base loss: 4613.89
[INFO 2017-06-27 17:00:41,991 main.py:51] epoch 1974, training loss: 3926.42, average training loss: 3601.12, base loss: 4611.92
[INFO 2017-06-27 17:00:42,376 main.py:51] epoch 1975, training loss: 3012.51, average training loss: 3600.66, base loss: 4611.82
[INFO 2017-06-27 17:00:42,754 main.py:51] epoch 1976, training loss: 3669.79, average training loss: 3601.23, base loss: 4613.23
[INFO 2017-06-27 17:00:43,138 main.py:51] epoch 1977, training loss: 3291.39, average training loss: 3601.17, base loss: 4613.74
[INFO 2017-06-27 17:00:43,533 main.py:51] epoch 1978, training loss: 3556.23, average training loss: 3601.32, base loss: 4614.50
[INFO 2017-06-27 17:00:43,927 main.py:51] epoch 1979, training loss: 3180.62, average training loss: 3600.92, base loss: 4614.71
[INFO 2017-06-27 17:00:44,305 main.py:51] epoch 1980, training loss: 3349.39, average training loss: 3600.81, base loss: 4615.27
[INFO 2017-06-27 17:00:44,682 main.py:51] epoch 1981, training loss: 3347.90, average training loss: 3600.76, base loss: 4615.44
[INFO 2017-06-27 17:00:45,059 main.py:51] epoch 1982, training loss: 3144.67, average training loss: 3600.43, base loss: 4615.30
[INFO 2017-06-27 17:00:45,436 main.py:51] epoch 1983, training loss: 3075.84, average training loss: 3599.92, base loss: 4614.70
[INFO 2017-06-27 17:00:45,817 main.py:51] epoch 1984, training loss: 3621.10, average training loss: 3599.74, base loss: 4614.66
[INFO 2017-06-27 17:00:46,195 main.py:51] epoch 1985, training loss: 6721.74, average training loss: 3603.00, base loss: 4618.15
[INFO 2017-06-27 17:00:46,580 main.py:51] epoch 1986, training loss: 3371.44, average training loss: 3602.72, base loss: 4618.34
[INFO 2017-06-27 17:00:46,957 main.py:51] epoch 1987, training loss: 3295.20, average training loss: 3602.22, base loss: 4618.03
[INFO 2017-06-27 17:00:47,335 main.py:51] epoch 1988, training loss: 6658.62, average training loss: 3604.96, base loss: 4620.43
[INFO 2017-06-27 17:00:47,713 main.py:51] epoch 1989, training loss: 3303.90, average training loss: 3604.90, base loss: 4620.81
[INFO 2017-06-27 17:00:48,089 main.py:51] epoch 1990, training loss: 3471.31, average training loss: 3604.89, base loss: 4621.03
[INFO 2017-06-27 17:00:48,466 main.py:51] epoch 1991, training loss: 3267.51, average training loss: 3604.72, base loss: 4621.31
[INFO 2017-06-27 17:00:48,837 main.py:51] epoch 1992, training loss: 3209.29, average training loss: 3604.17, base loss: 4620.92
[INFO 2017-06-27 17:00:49,213 main.py:51] epoch 1993, training loss: 3220.58, average training loss: 3603.76, base loss: 4620.42
[INFO 2017-06-27 17:00:49,584 main.py:51] epoch 1994, training loss: 3448.20, average training loss: 3603.87, base loss: 4621.31
[INFO 2017-06-27 17:00:49,957 main.py:51] epoch 1995, training loss: 3129.49, average training loss: 3603.11, base loss: 4620.74
[INFO 2017-06-27 17:00:50,333 main.py:51] epoch 1996, training loss: 3215.96, average training loss: 3602.59, base loss: 4620.16
[INFO 2017-06-27 17:00:50,711 main.py:51] epoch 1997, training loss: 3243.88, average training loss: 3601.97, base loss: 4619.97
[INFO 2017-06-27 17:00:51,083 main.py:51] epoch 1998, training loss: 3224.97, average training loss: 3601.44, base loss: 4619.39
[INFO 2017-06-27 17:00:51,465 main.py:51] epoch 1999, training loss: 3026.47, average training loss: 3600.53, base loss: 4618.83
[INFO 2017-06-27 17:00:51,466 main.py:53] epoch 1999, testing
[INFO 2017-06-27 17:00:53,069 main.py:105] average testing loss: 3334.07, base loss: 4529.98
[INFO 2017-06-27 17:00:53,069 main.py:106] improve_loss: 1195.91, improve_percent: 0.26
[INFO 2017-06-27 17:00:53,069 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 17:00:53,081 main.py:76] current best improved percent: 0.26
[INFO 2017-06-27 17:00:53,456 main.py:51] epoch 2000, training loss: 3331.83, average training loss: 3600.55, base loss: 4619.44
[INFO 2017-06-27 17:00:53,830 main.py:51] epoch 2001, training loss: 3383.15, average training loss: 3600.15, base loss: 4619.32
[INFO 2017-06-27 17:00:54,201 main.py:51] epoch 2002, training loss: 3217.16, average training loss: 3599.99, base loss: 4619.54
[INFO 2017-06-27 17:00:54,581 main.py:51] epoch 2003, training loss: 3452.73, average training loss: 3599.64, base loss: 4619.84
[INFO 2017-06-27 17:00:54,952 main.py:51] epoch 2004, training loss: 2864.31, average training loss: 3598.77, base loss: 4618.85
[INFO 2017-06-27 17:00:55,327 main.py:51] epoch 2005, training loss: 3205.54, average training loss: 3597.38, base loss: 4617.20
[INFO 2017-06-27 17:00:55,696 main.py:51] epoch 2006, training loss: 3451.27, average training loss: 3597.23, base loss: 4617.48
[INFO 2017-06-27 17:00:56,071 main.py:51] epoch 2007, training loss: 3141.53, average training loss: 3596.64, base loss: 4617.07
[INFO 2017-06-27 17:00:56,454 main.py:51] epoch 2008, training loss: 3355.13, average training loss: 3596.29, base loss: 4616.96
[INFO 2017-06-27 17:00:56,837 main.py:51] epoch 2009, training loss: 3522.83, average training loss: 3596.17, base loss: 4617.49
