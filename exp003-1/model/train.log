[INFO 2017-06-27 17:01:24,007 main.py:174] Namespace(batch_size=64, display=False, flow_dir='flow', flow_video_dir='flow-video', image_size=64, init_model_path='', learning_rate=0.001, method='unsupervised', motion_range=3, num_channel=3, num_inputs=3, save_dir='./model', test=False, test_dir='/home/yi/Downloads/youtube-64', test_epoch=10, test_interval=100, test_video=False, train=True, train_dir='/home/yi/Downloads/youtube-64', train_epoch=10000)
[INFO 2017-06-27 17:01:27,074 main.py:51] epoch 0, training loss: 67737.20, average training loss: 67737.20, base loss: 9466.82
[INFO 2017-06-27 17:01:28,026 main.py:51] epoch 1, training loss: 55745.06, average training loss: 61741.13, base loss: 9509.73
[INFO 2017-06-27 17:01:28,992 main.py:51] epoch 2, training loss: 46663.83, average training loss: 56715.36, base loss: 8815.03
[INFO 2017-06-27 17:01:29,956 main.py:51] epoch 3, training loss: 41408.11, average training loss: 52888.55, base loss: 8955.58
[INFO 2017-06-27 17:01:30,909 main.py:51] epoch 4, training loss: 39837.61, average training loss: 50278.36, base loss: 9721.34
[INFO 2017-06-27 17:01:31,888 main.py:51] epoch 5, training loss: 31924.88, average training loss: 47219.45, base loss: 9637.40
[INFO 2017-06-27 17:01:32,970 main.py:51] epoch 6, training loss: 27873.87, average training loss: 44455.79, base loss: 9517.98
[INFO 2017-06-27 17:01:34,176 main.py:51] epoch 7, training loss: 27984.78, average training loss: 42396.92, base loss: 9622.08
[INFO 2017-06-27 17:01:35,388 main.py:51] epoch 8, training loss: 22437.69, average training loss: 40179.22, base loss: 9429.31
[INFO 2017-06-27 17:01:36,597 main.py:51] epoch 9, training loss: 22263.66, average training loss: 38387.67, base loss: 9440.25
[INFO 2017-06-27 17:01:37,804 main.py:51] epoch 10, training loss: 20768.99, average training loss: 36785.97, base loss: 9408.27
[INFO 2017-06-27 17:01:39,013 main.py:51] epoch 11, training loss: 18907.82, average training loss: 35296.12, base loss: 9334.61
[INFO 2017-06-27 17:01:40,224 main.py:51] epoch 12, training loss: 17971.59, average training loss: 33963.47, base loss: 9303.32
[INFO 2017-06-27 17:01:41,433 main.py:51] epoch 13, training loss: 17686.91, average training loss: 32800.86, base loss: 9231.85
[INFO 2017-06-27 17:01:42,640 main.py:51] epoch 14, training loss: 16528.83, average training loss: 31716.05, base loss: 9192.19
[INFO 2017-06-27 17:01:43,849 main.py:51] epoch 15, training loss: 15003.78, average training loss: 30671.54, base loss: 9083.17
[INFO 2017-06-27 17:01:45,064 main.py:51] epoch 16, training loss: 15348.34, average training loss: 29770.17, base loss: 9049.43
[INFO 2017-06-27 17:01:46,275 main.py:51] epoch 17, training loss: 14382.87, average training loss: 28915.32, base loss: 8997.93
[INFO 2017-06-27 17:01:47,486 main.py:51] epoch 18, training loss: 16303.13, average training loss: 28251.52, base loss: 9097.62
[INFO 2017-06-27 17:01:48,694 main.py:51] epoch 19, training loss: 13425.66, average training loss: 27510.23, base loss: 9054.65
[INFO 2017-06-27 17:01:49,904 main.py:51] epoch 20, training loss: 13880.69, average training loss: 26861.20, base loss: 9065.65
[INFO 2017-06-27 17:01:51,114 main.py:51] epoch 21, training loss: 13036.26, average training loss: 26232.80, base loss: 9045.73
[INFO 2017-06-27 17:01:52,323 main.py:51] epoch 22, training loss: 13185.18, average training loss: 25665.51, base loss: 9037.82
[INFO 2017-06-27 17:01:53,532 main.py:51] epoch 23, training loss: 11812.73, average training loss: 25088.31, base loss: 8997.58
[INFO 2017-06-27 17:01:54,739 main.py:51] epoch 24, training loss: 12430.43, average training loss: 24582.00, base loss: 8995.15
[INFO 2017-06-27 17:01:55,951 main.py:51] epoch 25, training loss: 12416.30, average training loss: 24114.08, base loss: 8988.46
[INFO 2017-06-27 17:01:57,160 main.py:51] epoch 26, training loss: 14753.90, average training loss: 23767.41, base loss: 9083.43
[INFO 2017-06-27 17:01:58,370 main.py:51] epoch 27, training loss: 12009.24, average training loss: 23347.48, base loss: 9091.54
[INFO 2017-06-27 17:01:59,576 main.py:51] epoch 28, training loss: 11305.67, average training loss: 22932.24, base loss: 9069.41
[INFO 2017-06-27 17:02:00,784 main.py:51] epoch 29, training loss: 11802.20, average training loss: 22561.24, base loss: 9073.73
[INFO 2017-06-27 17:02:01,994 main.py:51] epoch 30, training loss: 11691.17, average training loss: 22210.59, base loss: 9084.42
[INFO 2017-06-27 17:02:03,201 main.py:51] epoch 31, training loss: 11033.16, average training loss: 21861.30, base loss: 9073.73
[INFO 2017-06-27 17:02:04,414 main.py:51] epoch 32, training loss: 11475.71, average training loss: 21546.58, base loss: 9082.45
[INFO 2017-06-27 17:02:05,625 main.py:51] epoch 33, training loss: 10436.07, average training loss: 21219.80, base loss: 9055.34
[INFO 2017-06-27 17:02:06,838 main.py:51] epoch 34, training loss: 10967.84, average training loss: 20926.89, base loss: 9055.77
[INFO 2017-06-27 17:02:08,046 main.py:51] epoch 35, training loss: 10796.57, average training loss: 20645.49, base loss: 9054.90
[INFO 2017-06-27 17:02:09,249 main.py:51] epoch 36, training loss: 10419.21, average training loss: 20369.11, base loss: 9040.55
[INFO 2017-06-27 17:02:10,462 main.py:51] epoch 37, training loss: 10207.37, average training loss: 20101.69, base loss: 9023.62
[INFO 2017-06-27 17:02:11,668 main.py:51] epoch 38, training loss: 10936.23, average training loss: 19866.68, base loss: 9033.53
[INFO 2017-06-27 17:02:12,881 main.py:51] epoch 39, training loss: 10147.46, average training loss: 19623.70, base loss: 9023.43
[INFO 2017-06-27 17:02:14,088 main.py:51] epoch 40, training loss: 10293.71, average training loss: 19396.14, base loss: 9020.76
[INFO 2017-06-27 17:02:15,310 main.py:51] epoch 41, training loss: 10578.64, average training loss: 19186.20, base loss: 9028.20
[INFO 2017-06-27 17:02:16,522 main.py:51] epoch 42, training loss: 10328.98, average training loss: 18980.22, base loss: 9029.16
[INFO 2017-06-27 17:02:17,733 main.py:51] epoch 43, training loss: 8836.88, average training loss: 18749.69, base loss: 8995.51
[INFO 2017-06-27 17:02:18,944 main.py:51] epoch 44, training loss: 9493.13, average training loss: 18543.99, base loss: 8980.49
[INFO 2017-06-27 17:02:20,153 main.py:51] epoch 45, training loss: 11464.06, average training loss: 18390.07, base loss: 9015.81
[INFO 2017-06-27 17:02:21,368 main.py:51] epoch 46, training loss: 10349.47, average training loss: 18219.00, base loss: 9022.96
[INFO 2017-06-27 17:02:22,573 main.py:51] epoch 47, training loss: 10085.76, average training loss: 18049.55, base loss: 9029.76
[INFO 2017-06-27 17:02:23,781 main.py:51] epoch 48, training loss: 9280.06, average training loss: 17870.59, base loss: 9014.20
[INFO 2017-06-27 17:02:24,990 main.py:51] epoch 49, training loss: 10243.38, average training loss: 17718.04, base loss: 9023.03
[INFO 2017-06-27 17:02:26,203 main.py:51] epoch 50, training loss: 13136.17, average training loss: 17628.20, base loss: 9088.86
[INFO 2017-06-27 17:02:27,414 main.py:51] epoch 51, training loss: 8743.16, average training loss: 17457.33, base loss: 9067.35
[INFO 2017-06-27 17:02:28,621 main.py:51] epoch 52, training loss: 9849.43, average training loss: 17313.79, base loss: 9069.73
[INFO 2017-06-27 17:02:29,832 main.py:51] epoch 53, training loss: 9194.21, average training loss: 17163.43, base loss: 9059.21
[INFO 2017-06-27 17:02:31,042 main.py:51] epoch 54, training loss: 10569.57, average training loss: 17043.54, base loss: 9076.79
[INFO 2017-06-27 17:02:32,253 main.py:51] epoch 55, training loss: 12940.14, average training loss: 16970.26, base loss: 9134.64
[INFO 2017-06-27 17:02:33,464 main.py:51] epoch 56, training loss: 14880.65, average training loss: 16933.60, base loss: 9227.62
[INFO 2017-06-27 17:02:34,671 main.py:51] epoch 57, training loss: 10731.93, average training loss: 16826.68, base loss: 9244.47
[INFO 2017-06-27 17:02:35,882 main.py:51] epoch 58, training loss: 9357.96, average training loss: 16700.09, base loss: 9238.27
[INFO 2017-06-27 17:02:37,093 main.py:51] epoch 59, training loss: 9513.83, average training loss: 16580.32, base loss: 9235.16
[INFO 2017-06-27 17:02:38,298 main.py:51] epoch 60, training loss: 9468.02, average training loss: 16463.72, base loss: 9230.38
[INFO 2017-06-27 17:02:39,509 main.py:51] epoch 61, training loss: 9894.08, average training loss: 16357.76, base loss: 9234.77
[INFO 2017-06-27 17:02:40,723 main.py:51] epoch 62, training loss: 9167.26, average training loss: 16243.63, base loss: 9226.94
[INFO 2017-06-27 17:02:41,934 main.py:51] epoch 63, training loss: 9666.10, average training loss: 16140.85, base loss: 9228.25
[INFO 2017-06-27 17:02:43,146 main.py:51] epoch 64, training loss: 9147.91, average training loss: 16033.27, base loss: 9219.86
[INFO 2017-06-27 17:02:44,354 main.py:51] epoch 65, training loss: 9560.32, average training loss: 15935.19, base loss: 9220.08
[INFO 2017-06-27 17:02:45,562 main.py:51] epoch 66, training loss: 9709.95, average training loss: 15842.28, base loss: 9221.92
[INFO 2017-06-27 17:02:46,774 main.py:51] epoch 67, training loss: 9285.27, average training loss: 15745.85, base loss: 9218.21
[INFO 2017-06-27 17:02:47,985 main.py:51] epoch 68, training loss: 8654.39, average training loss: 15643.08, base loss: 9204.92
[INFO 2017-06-27 17:02:49,199 main.py:51] epoch 69, training loss: 8888.50, average training loss: 15546.58, base loss: 9195.40
[INFO 2017-06-27 17:02:50,408 main.py:51] epoch 70, training loss: 9182.48, average training loss: 15456.95, base loss: 9191.45
[INFO 2017-06-27 17:02:51,613 main.py:51] epoch 71, training loss: 12183.92, average training loss: 15411.49, base loss: 9228.72
[INFO 2017-06-27 17:02:52,822 main.py:51] epoch 72, training loss: 10218.80, average training loss: 15340.36, base loss: 9239.60
[INFO 2017-06-27 17:02:54,034 main.py:51] epoch 73, training loss: 9146.59, average training loss: 15256.66, base loss: 9235.67
[INFO 2017-06-27 17:02:55,244 main.py:51] epoch 74, training loss: 8655.48, average training loss: 15168.64, base loss: 9224.91
[INFO 2017-06-27 17:02:56,453 main.py:51] epoch 75, training loss: 12459.73, average training loss: 15133.00, base loss: 9264.76
[INFO 2017-06-27 17:02:57,666 main.py:51] epoch 76, training loss: 9689.97, average training loss: 15062.31, base loss: 9267.83
[INFO 2017-06-27 17:02:58,876 main.py:51] epoch 77, training loss: 8767.31, average training loss: 14981.60, base loss: 9259.01
[INFO 2017-06-27 17:03:00,084 main.py:51] epoch 78, training loss: 9592.24, average training loss: 14913.38, base loss: 9260.84
[INFO 2017-06-27 17:03:01,296 main.py:51] epoch 79, training loss: 8698.00, average training loss: 14835.69, base loss: 9251.91
[INFO 2017-06-27 17:03:02,511 main.py:51] epoch 80, training loss: 8627.47, average training loss: 14759.05, base loss: 9242.39
[INFO 2017-06-27 17:03:03,723 main.py:51] epoch 81, training loss: 8873.78, average training loss: 14687.28, base loss: 9236.68
[INFO 2017-06-27 17:03:04,932 main.py:51] epoch 82, training loss: 9771.23, average training loss: 14628.05, base loss: 9242.29
[INFO 2017-06-27 17:03:06,147 main.py:51] epoch 83, training loss: 8990.21, average training loss: 14560.93, base loss: 9237.85
[INFO 2017-06-27 17:03:07,355 main.py:51] epoch 84, training loss: 8998.57, average training loss: 14495.49, base loss: 9234.15
[INFO 2017-06-27 17:03:08,566 main.py:51] epoch 85, training loss: 9006.33, average training loss: 14431.66, base loss: 9230.83
[INFO 2017-06-27 17:03:09,776 main.py:51] epoch 86, training loss: 8062.46, average training loss: 14358.45, base loss: 9215.67
[INFO 2017-06-27 17:03:10,991 main.py:51] epoch 87, training loss: 9150.44, average training loss: 14299.27, base loss: 9214.55
[INFO 2017-06-27 17:03:12,202 main.py:51] epoch 88, training loss: 9830.07, average training loss: 14249.06, base loss: 9221.87
[INFO 2017-06-27 17:03:13,413 main.py:51] epoch 89, training loss: 9752.90, average training loss: 14199.10, base loss: 9227.72
[INFO 2017-06-27 17:03:14,622 main.py:51] epoch 90, training loss: 15233.09, average training loss: 14210.46, base loss: 9292.50
[INFO 2017-06-27 17:03:15,833 main.py:51] epoch 91, training loss: 8817.31, average training loss: 14151.84, base loss: 9286.83
[INFO 2017-06-27 17:03:17,048 main.py:51] epoch 92, training loss: 8719.78, average training loss: 14093.43, base loss: 9280.21
[INFO 2017-06-27 17:03:18,257 main.py:51] epoch 93, training loss: 8933.71, average training loss: 14038.54, base loss: 9275.67
[INFO 2017-06-27 17:03:19,469 main.py:51] epoch 94, training loss: 9107.75, average training loss: 13986.64, base loss: 9274.21
[INFO 2017-06-27 17:03:20,681 main.py:51] epoch 95, training loss: 9282.97, average training loss: 13937.64, base loss: 9274.26
[INFO 2017-06-27 17:03:21,891 main.py:51] epoch 96, training loss: 8073.66, average training loss: 13877.19, base loss: 9261.58
[INFO 2017-06-27 17:03:23,104 main.py:51] epoch 97, training loss: 8559.71, average training loss: 13822.93, base loss: 9254.32
[INFO 2017-06-27 17:03:24,318 main.py:51] epoch 98, training loss: 8618.17, average training loss: 13770.35, base loss: 9247.67
[INFO 2017-06-27 17:03:25,532 main.py:51] epoch 99, training loss: 8778.60, average training loss: 13720.44, base loss: 9243.06
[INFO 2017-06-27 17:03:25,532 main.py:53] epoch 99, testing
[INFO 2017-06-27 17:03:30,226 main.py:105] average testing loss: 10136.62, base loss: 10115.73
[INFO 2017-06-27 17:03:30,226 main.py:106] improve_loss: -20.89, improve_percent: -0.00
[INFO 2017-06-27 17:03:30,227 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 17:03:30,238 main.py:76] current best improved percent: -0.00
[INFO 2017-06-27 17:03:31,452 main.py:51] epoch 100, training loss: 8770.03, average training loss: 13671.42, base loss: 9238.51
[INFO 2017-06-27 17:03:32,665 main.py:51] epoch 101, training loss: 9486.04, average training loss: 13630.39, base loss: 9241.68
[INFO 2017-06-27 17:03:33,874 main.py:51] epoch 102, training loss: 8037.98, average training loss: 13576.09, base loss: 9229.80
[INFO 2017-06-27 17:03:35,082 main.py:51] epoch 103, training loss: 7962.40, average training loss: 13522.12, base loss: 9217.41
[INFO 2017-06-27 17:03:36,292 main.py:51] epoch 104, training loss: 8179.49, average training loss: 13471.23, base loss: 9207.38
[INFO 2017-06-27 17:03:37,501 main.py:51] epoch 105, training loss: 7868.29, average training loss: 13418.38, base loss: 9194.31
[INFO 2017-06-27 17:03:38,715 main.py:51] epoch 106, training loss: 9660.91, average training loss: 13383.26, base loss: 9199.68
[INFO 2017-06-27 17:03:39,924 main.py:51] epoch 107, training loss: 10328.34, average training loss: 13354.97, base loss: 9211.31
[INFO 2017-06-27 17:03:41,136 main.py:51] epoch 108, training loss: 9037.80, average training loss: 13315.37, base loss: 9210.82
[INFO 2017-06-27 17:03:42,353 main.py:51] epoch 109, training loss: 8096.67, average training loss: 13267.92, base loss: 9200.22
[INFO 2017-06-27 17:03:43,565 main.py:51] epoch 110, training loss: 9091.11, average training loss: 13230.29, base loss: 9200.28
[INFO 2017-06-27 17:03:44,774 main.py:51] epoch 111, training loss: 9587.49, average training loss: 13197.77, base loss: 9204.69
[INFO 2017-06-27 17:03:45,984 main.py:51] epoch 112, training loss: 9442.29, average training loss: 13164.54, base loss: 9207.46
[INFO 2017-06-27 17:03:47,195 main.py:51] epoch 113, training loss: 8877.61, average training loss: 13126.93, base loss: 9205.64
[INFO 2017-06-27 17:03:48,408 main.py:51] epoch 114, training loss: 8295.80, average training loss: 13084.92, base loss: 9197.82
[INFO 2017-06-27 17:03:49,618 main.py:51] epoch 115, training loss: 9010.44, average training loss: 13049.80, base loss: 9197.06
[INFO 2017-06-27 17:03:50,824 main.py:51] epoch 116, training loss: 9263.47, average training loss: 13017.43, base loss: 9198.74
[INFO 2017-06-27 17:03:52,036 main.py:51] epoch 117, training loss: 9422.51, average training loss: 12986.97, base loss: 9201.65
[INFO 2017-06-27 17:03:53,248 main.py:51] epoch 118, training loss: 8787.17, average training loss: 12951.68, base loss: 9199.15
[INFO 2017-06-27 17:03:54,456 main.py:51] epoch 119, training loss: 7828.98, average training loss: 12908.99, base loss: 9188.32
[INFO 2017-06-27 17:03:55,667 main.py:51] epoch 120, training loss: 9644.43, average training loss: 12882.01, base loss: 9194.02
[INFO 2017-06-27 17:03:56,879 main.py:51] epoch 121, training loss: 8979.24, average training loss: 12850.02, base loss: 9193.56
[INFO 2017-06-27 17:03:58,088 main.py:51] epoch 122, training loss: 8632.92, average training loss: 12815.73, base loss: 9189.53
[INFO 2017-06-27 17:03:59,297 main.py:51] epoch 123, training loss: 8961.46, average training loss: 12784.65, base loss: 9188.15
[INFO 2017-06-27 17:04:00,506 main.py:51] epoch 124, training loss: 8452.49, average training loss: 12749.99, base loss: 9183.32
[INFO 2017-06-27 17:04:01,716 main.py:51] epoch 125, training loss: 12137.74, average training loss: 12745.13, base loss: 9208.09
[INFO 2017-06-27 17:04:02,924 main.py:51] epoch 126, training loss: 8383.86, average training loss: 12710.79, base loss: 9203.07
[INFO 2017-06-27 17:04:04,134 main.py:51] epoch 127, training loss: 8564.75, average training loss: 12678.40, base loss: 9199.31
[INFO 2017-06-27 17:04:05,346 main.py:51] epoch 128, training loss: 9441.97, average training loss: 12653.31, base loss: 9202.66
[INFO 2017-06-27 17:04:06,560 main.py:51] epoch 129, training loss: 8724.03, average training loss: 12623.09, base loss: 9200.26
[INFO 2017-06-27 17:04:07,776 main.py:51] epoch 130, training loss: 9053.97, average training loss: 12595.84, base loss: 9200.89
[INFO 2017-06-27 17:04:08,985 main.py:51] epoch 131, training loss: 7840.20, average training loss: 12559.81, base loss: 9191.24
[INFO 2017-06-27 17:04:10,197 main.py:51] epoch 132, training loss: 7802.27, average training loss: 12524.04, base loss: 9181.29
[INFO 2017-06-27 17:04:11,409 main.py:51] epoch 133, training loss: 8407.47, average training loss: 12493.32, base loss: 9176.80
[INFO 2017-06-27 17:04:12,619 main.py:51] epoch 134, training loss: 7285.12, average training loss: 12454.74, base loss: 9163.29
[INFO 2017-06-27 17:04:13,829 main.py:51] epoch 135, training loss: 9164.68, average training loss: 12430.55, base loss: 9164.66
[INFO 2017-06-27 17:04:15,042 main.py:51] epoch 136, training loss: 8519.08, average training loss: 12402.00, base loss: 9161.66
[INFO 2017-06-27 17:04:16,250 main.py:51] epoch 137, training loss: 8460.19, average training loss: 12373.44, base loss: 9157.38
[INFO 2017-06-27 17:04:17,458 main.py:51] epoch 138, training loss: 9798.55, average training loss: 12354.91, base loss: 9164.44
[INFO 2017-06-27 17:04:18,671 main.py:51] epoch 139, training loss: 8265.32, average training loss: 12325.70, base loss: 9159.37
[INFO 2017-06-27 17:04:19,882 main.py:51] epoch 140, training loss: 8608.21, average training loss: 12299.34, base loss: 9157.05
[INFO 2017-06-27 17:04:21,093 main.py:51] epoch 141, training loss: 9540.32, average training loss: 12279.91, base loss: 9161.94
[INFO 2017-06-27 17:04:22,303 main.py:51] epoch 142, training loss: 9221.00, average training loss: 12258.52, base loss: 9164.40
[INFO 2017-06-27 17:04:23,512 main.py:51] epoch 143, training loss: 8456.52, average training loss: 12232.11, base loss: 9161.23
[INFO 2017-06-27 17:04:24,722 main.py:51] epoch 144, training loss: 9463.55, average training loss: 12213.02, base loss: 9165.22
[INFO 2017-06-27 17:04:25,933 main.py:51] epoch 145, training loss: 12896.29, average training loss: 12217.70, base loss: 9192.92
[INFO 2017-06-27 17:04:27,145 main.py:51] epoch 146, training loss: 8546.72, average training loss: 12192.73, base loss: 9190.05
[INFO 2017-06-27 17:04:28,354 main.py:51] epoch 147, training loss: 9654.05, average training loss: 12175.57, base loss: 9195.16
[INFO 2017-06-27 17:04:29,566 main.py:51] epoch 148, training loss: 9136.02, average training loss: 12155.17, base loss: 9196.58
[INFO 2017-06-27 17:04:30,776 main.py:51] epoch 149, training loss: 9937.19, average training loss: 12140.39, base loss: 9203.80
[INFO 2017-06-27 17:04:31,991 main.py:51] epoch 150, training loss: 8651.25, average training loss: 12117.28, base loss: 9201.87
[INFO 2017-06-27 17:04:33,202 main.py:51] epoch 151, training loss: 8662.16, average training loss: 12094.55, base loss: 9199.81
[INFO 2017-06-27 17:04:34,416 main.py:51] epoch 152, training loss: 8502.49, average training loss: 12071.07, base loss: 9196.86
[INFO 2017-06-27 17:04:35,630 main.py:51] epoch 153, training loss: 9354.98, average training loss: 12053.43, base loss: 9199.94
[INFO 2017-06-27 17:04:36,840 main.py:51] epoch 154, training loss: 9083.18, average training loss: 12034.27, base loss: 9200.92
[INFO 2017-06-27 17:04:38,054 main.py:51] epoch 155, training loss: 8628.51, average training loss: 12012.44, base loss: 9198.60
[INFO 2017-06-27 17:04:39,264 main.py:51] epoch 156, training loss: 12957.68, average training loss: 12018.46, base loss: 9224.80
[INFO 2017-06-27 17:04:40,474 main.py:51] epoch 157, training loss: 8742.33, average training loss: 11997.73, base loss: 9223.16
[INFO 2017-06-27 17:04:41,687 main.py:51] epoch 158, training loss: 8225.55, average training loss: 11974.00, base loss: 9218.62
[INFO 2017-06-27 17:04:42,898 main.py:51] epoch 159, training loss: 8627.29, average training loss: 11953.08, base loss: 9216.39
[INFO 2017-06-27 17:04:44,110 main.py:51] epoch 160, training loss: 9307.66, average training loss: 11936.65, base loss: 9219.18
[INFO 2017-06-27 17:04:45,323 main.py:51] epoch 161, training loss: 9181.02, average training loss: 11919.64, base loss: 9221.45
[INFO 2017-06-27 17:04:46,532 main.py:51] epoch 162, training loss: 9314.86, average training loss: 11903.66, base loss: 9224.62
[INFO 2017-06-27 17:04:47,745 main.py:51] epoch 163, training loss: 8806.96, average training loss: 11884.78, base loss: 9223.87
[INFO 2017-06-27 17:04:48,959 main.py:51] epoch 164, training loss: 8410.69, average training loss: 11863.73, base loss: 9220.18
[INFO 2017-06-27 17:04:50,169 main.py:51] epoch 165, training loss: 9260.15, average training loss: 11848.04, base loss: 9222.51
[INFO 2017-06-27 17:04:51,378 main.py:51] epoch 166, training loss: 8211.48, average training loss: 11826.27, base loss: 9218.42
[INFO 2017-06-27 17:04:52,590 main.py:51] epoch 167, training loss: 8878.07, average training loss: 11808.72, base loss: 9218.81
[INFO 2017-06-27 17:04:53,801 main.py:51] epoch 168, training loss: 8351.49, average training loss: 11788.26, base loss: 9215.06
[INFO 2017-06-27 17:04:55,011 main.py:51] epoch 169, training loss: 8609.29, average training loss: 11769.56, base loss: 9213.38
[INFO 2017-06-27 17:04:56,222 main.py:51] epoch 170, training loss: 9261.21, average training loss: 11754.89, base loss: 9215.82
[INFO 2017-06-27 17:04:57,432 main.py:51] epoch 171, training loss: 9576.53, average training loss: 11742.23, base loss: 9220.58
[INFO 2017-06-27 17:04:58,644 main.py:51] epoch 172, training loss: 7623.40, average training loss: 11718.42, base loss: 9212.51
[INFO 2017-06-27 17:04:59,856 main.py:51] epoch 173, training loss: 8421.81, average training loss: 11699.47, base loss: 9209.37
[INFO 2017-06-27 17:05:01,070 main.py:51] epoch 174, training loss: 8614.61, average training loss: 11681.84, base loss: 9207.74
[INFO 2017-06-27 17:05:02,282 main.py:51] epoch 175, training loss: 7491.95, average training loss: 11658.04, base loss: 9199.25
[INFO 2017-06-27 17:05:03,490 main.py:51] epoch 176, training loss: 9161.72, average training loss: 11643.93, base loss: 9200.91
[INFO 2017-06-27 17:05:04,699 main.py:51] epoch 177, training loss: 7902.78, average training loss: 11622.92, base loss: 9195.36
[INFO 2017-06-27 17:05:05,912 main.py:51] epoch 178, training loss: 8936.27, average training loss: 11607.91, base loss: 9195.76
[INFO 2017-06-27 17:05:07,126 main.py:51] epoch 179, training loss: 8509.06, average training loss: 11590.69, base loss: 9194.05
[INFO 2017-06-27 17:05:08,339 main.py:51] epoch 180, training loss: 9670.35, average training loss: 11580.08, base loss: 9198.74
[INFO 2017-06-27 17:05:09,550 main.py:51] epoch 181, training loss: 8989.53, average training loss: 11565.85, base loss: 9199.92
[INFO 2017-06-27 17:05:10,760 main.py:51] epoch 182, training loss: 8844.17, average training loss: 11550.98, base loss: 9199.89
[INFO 2017-06-27 17:05:11,968 main.py:51] epoch 183, training loss: 8311.22, average training loss: 11533.37, base loss: 9196.45
[INFO 2017-06-27 17:05:13,177 main.py:51] epoch 184, training loss: 8835.11, average training loss: 11518.78, base loss: 9196.51
[INFO 2017-06-27 17:05:14,388 main.py:51] epoch 185, training loss: 7571.22, average training loss: 11497.56, base loss: 9188.59
[INFO 2017-06-27 17:05:15,599 main.py:51] epoch 186, training loss: 9351.19, average training loss: 11486.08, base loss: 9191.59
[INFO 2017-06-27 17:05:16,813 main.py:51] epoch 187, training loss: 8329.85, average training loss: 11469.29, base loss: 9188.47
[INFO 2017-06-27 17:05:18,028 main.py:51] epoch 188, training loss: 8273.68, average training loss: 11452.39, base loss: 9185.36
[INFO 2017-06-27 17:05:19,244 main.py:51] epoch 189, training loss: 7941.68, average training loss: 11433.91, base loss: 9180.61
[INFO 2017-06-27 17:05:20,455 main.py:51] epoch 190, training loss: 8770.24, average training loss: 11419.96, base loss: 9180.76
[INFO 2017-06-27 17:05:21,667 main.py:51] epoch 191, training loss: 10157.27, average training loss: 11413.39, base loss: 9188.11
[INFO 2017-06-27 17:05:22,876 main.py:51] epoch 192, training loss: 8010.90, average training loss: 11395.76, base loss: 9183.65
[INFO 2017-06-27 17:05:24,090 main.py:51] epoch 193, training loss: 8646.45, average training loss: 11381.58, base loss: 9183.31
[INFO 2017-06-27 17:05:25,298 main.py:51] epoch 194, training loss: 12036.84, average training loss: 11384.94, base loss: 9200.14
[INFO 2017-06-27 17:05:26,508 main.py:51] epoch 195, training loss: 8112.80, average training loss: 11368.25, base loss: 9196.27
[INFO 2017-06-27 17:05:27,720 main.py:51] epoch 196, training loss: 8156.82, average training loss: 11351.95, base loss: 9192.72
[INFO 2017-06-27 17:05:28,931 main.py:51] epoch 197, training loss: 8248.58, average training loss: 11336.27, base loss: 9190.00
[INFO 2017-06-27 17:05:30,144 main.py:51] epoch 198, training loss: 8133.49, average training loss: 11320.18, base loss: 9186.26
[INFO 2017-06-27 17:05:31,356 main.py:51] epoch 199, training loss: 8623.53, average training loss: 11306.70, base loss: 9185.46
[INFO 2017-06-27 17:05:31,356 main.py:53] epoch 199, testing
[INFO 2017-06-27 17:05:36,040 main.py:105] average testing loss: 8693.65, base loss: 9116.62
[INFO 2017-06-27 17:05:36,040 main.py:106] improve_loss: 422.97, improve_percent: 0.05
[INFO 2017-06-27 17:05:36,042 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 17:05:36,054 main.py:76] current best improved percent: 0.05
[INFO 2017-06-27 17:05:37,265 main.py:51] epoch 200, training loss: 7795.93, average training loss: 11289.23, base loss: 9180.00
[INFO 2017-06-27 17:05:38,480 main.py:51] epoch 201, training loss: 8911.62, average training loss: 11277.46, base loss: 9180.73
[INFO 2017-06-27 17:05:39,693 main.py:51] epoch 202, training loss: 9288.37, average training loss: 11267.66, base loss: 9184.38
[INFO 2017-06-27 17:05:40,911 main.py:51] epoch 203, training loss: 11939.34, average training loss: 11270.95, base loss: 9199.64
[INFO 2017-06-27 17:05:42,132 main.py:51] epoch 204, training loss: 7566.34, average training loss: 11252.88, base loss: 9193.09
[INFO 2017-06-27 17:05:43,356 main.py:51] epoch 205, training loss: 9000.94, average training loss: 11241.95, base loss: 9194.36
[INFO 2017-06-27 17:05:44,567 main.py:51] epoch 206, training loss: 8794.34, average training loss: 11230.13, base loss: 9194.69
[INFO 2017-06-27 17:05:45,776 main.py:51] epoch 207, training loss: 8587.36, average training loss: 11217.42, base loss: 9193.92
[INFO 2017-06-27 17:05:46,985 main.py:51] epoch 208, training loss: 8163.03, average training loss: 11202.81, base loss: 9190.51
[INFO 2017-06-27 17:05:48,194 main.py:51] epoch 209, training loss: 7831.47, average training loss: 11186.75, base loss: 9185.39
[INFO 2017-06-27 17:05:49,406 main.py:51] epoch 210, training loss: 9165.70, average training loss: 11177.17, base loss: 9187.87
[INFO 2017-06-27 17:05:50,612 main.py:51] epoch 211, training loss: 11738.28, average training loss: 11179.82, base loss: 9201.95
[INFO 2017-06-27 17:05:51,826 main.py:51] epoch 212, training loss: 7523.30, average training loss: 11162.65, base loss: 9195.78
[INFO 2017-06-27 17:05:53,036 main.py:51] epoch 213, training loss: 7706.01, average training loss: 11146.50, base loss: 9190.39
[INFO 2017-06-27 17:05:54,246 main.py:51] epoch 214, training loss: 8348.81, average training loss: 11133.49, base loss: 9188.97
[INFO 2017-06-27 17:05:55,460 main.py:51] epoch 215, training loss: 8048.46, average training loss: 11119.21, base loss: 9185.92
[INFO 2017-06-27 17:05:56,673 main.py:51] epoch 216, training loss: 8560.51, average training loss: 11107.42, base loss: 9185.06
[INFO 2017-06-27 17:05:57,882 main.py:51] epoch 217, training loss: 8346.28, average training loss: 11094.75, base loss: 9183.02
[INFO 2017-06-27 17:05:59,094 main.py:51] epoch 218, training loss: 8952.34, average training loss: 11084.97, base loss: 9184.79
[INFO 2017-06-27 17:06:00,305 main.py:51] epoch 219, training loss: 8638.90, average training loss: 11073.85, base loss: 9184.49
[INFO 2017-06-27 17:06:01,516 main.py:51] epoch 220, training loss: 7629.84, average training loss: 11058.27, base loss: 9179.04
[INFO 2017-06-27 17:06:02,724 main.py:51] epoch 221, training loss: 8893.40, average training loss: 11048.51, base loss: 9180.27
[INFO 2017-06-27 17:06:03,934 main.py:51] epoch 222, training loss: 9256.41, average training loss: 11040.48, base loss: 9183.42
[INFO 2017-06-27 17:06:05,143 main.py:51] epoch 223, training loss: 7940.98, average training loss: 11026.64, base loss: 9179.59
[INFO 2017-06-27 17:06:06,353 main.py:51] epoch 224, training loss: 8408.05, average training loss: 11015.00, base loss: 9178.48
[INFO 2017-06-27 17:06:07,568 main.py:51] epoch 225, training loss: 8022.72, average training loss: 11001.76, base loss: 9175.32
[INFO 2017-06-27 17:06:08,783 main.py:51] epoch 226, training loss: 8337.94, average training loss: 10990.03, base loss: 9173.66
[INFO 2017-06-27 17:06:09,998 main.py:51] epoch 227, training loss: 8674.03, average training loss: 10979.87, base loss: 9173.89
[INFO 2017-06-27 17:06:11,216 main.py:51] epoch 228, training loss: 10032.94, average training loss: 10975.73, base loss: 9180.76
[INFO 2017-06-27 17:06:12,433 main.py:51] epoch 229, training loss: 8589.33, average training loss: 10965.36, base loss: 9180.27
[INFO 2017-06-27 17:06:13,644 main.py:51] epoch 230, training loss: 9021.77, average training loss: 10956.94, base loss: 9181.86
[INFO 2017-06-27 17:06:14,858 main.py:51] epoch 231, training loss: 8050.53, average training loss: 10944.42, base loss: 9178.54
[INFO 2017-06-27 17:06:16,071 main.py:51] epoch 232, training loss: 7743.48, average training loss: 10930.68, base loss: 9174.52
[INFO 2017-06-27 17:06:17,285 main.py:51] epoch 233, training loss: 8197.35, average training loss: 10919.00, base loss: 9172.18
[INFO 2017-06-27 17:06:18,503 main.py:51] epoch 234, training loss: 12069.81, average training loss: 10923.90, base loss: 9186.55
[INFO 2017-06-27 17:06:19,719 main.py:51] epoch 235, training loss: 9177.32, average training loss: 10916.49, base loss: 9188.94
[INFO 2017-06-27 17:06:20,933 main.py:51] epoch 236, training loss: 8445.35, average training loss: 10906.07, base loss: 9187.79
[INFO 2017-06-27 17:06:22,149 main.py:51] epoch 237, training loss: 9279.74, average training loss: 10899.23, base loss: 9191.27
[INFO 2017-06-27 17:06:23,364 main.py:51] epoch 238, training loss: 7570.52, average training loss: 10885.31, base loss: 9186.18
[INFO 2017-06-27 17:06:24,577 main.py:51] epoch 239, training loss: 12572.68, average training loss: 10892.34, base loss: 9202.82
[INFO 2017-06-27 17:06:25,794 main.py:51] epoch 240, training loss: 8331.22, average training loss: 10881.71, base loss: 9201.27
[INFO 2017-06-27 17:06:27,007 main.py:51] epoch 241, training loss: 8657.56, average training loss: 10872.52, base loss: 9201.36
[INFO 2017-06-27 17:06:28,221 main.py:51] epoch 242, training loss: 8336.24, average training loss: 10862.08, base loss: 9200.14
[INFO 2017-06-27 17:06:29,436 main.py:51] epoch 243, training loss: 8767.09, average training loss: 10853.50, base loss: 9200.96
[INFO 2017-06-27 17:06:30,651 main.py:51] epoch 244, training loss: 8161.69, average training loss: 10842.51, base loss: 9198.57
[INFO 2017-06-27 17:06:31,866 main.py:51] epoch 245, training loss: 8724.38, average training loss: 10833.90, base loss: 9199.46
[INFO 2017-06-27 17:06:33,081 main.py:51] epoch 246, training loss: 8242.85, average training loss: 10823.41, base loss: 9197.31
[INFO 2017-06-27 17:06:34,294 main.py:51] epoch 247, training loss: 8573.17, average training loss: 10814.34, base loss: 9197.45
[INFO 2017-06-27 17:06:35,508 main.py:51] epoch 248, training loss: 9404.22, average training loss: 10808.67, base loss: 9201.45
[INFO 2017-06-27 17:06:36,718 main.py:51] epoch 249, training loss: 8395.06, average training loss: 10799.02, base loss: 9201.05
[INFO 2017-06-27 17:06:37,929 main.py:51] epoch 250, training loss: 9408.01, average training loss: 10793.48, base loss: 9205.15
[INFO 2017-06-27 17:06:39,142 main.py:51] epoch 251, training loss: 9365.83, average training loss: 10787.81, base loss: 9208.41
[INFO 2017-06-27 17:06:40,353 main.py:51] epoch 252, training loss: 7893.30, average training loss: 10776.37, base loss: 9205.03
[INFO 2017-06-27 17:06:41,563 main.py:51] epoch 253, training loss: 8229.11, average training loss: 10766.34, base loss: 9203.45
[INFO 2017-06-27 17:06:42,771 main.py:51] epoch 254, training loss: 12114.16, average training loss: 10771.63, base loss: 9217.21
[INFO 2017-06-27 17:06:43,980 main.py:51] epoch 255, training loss: 15975.47, average training loss: 10791.95, base loss: 9246.58
[INFO 2017-06-27 17:06:45,189 main.py:51] epoch 256, training loss: 9052.70, average training loss: 10785.19, base loss: 9249.04
[INFO 2017-06-27 17:06:46,404 main.py:51] epoch 257, training loss: 7815.11, average training loss: 10773.67, base loss: 9245.32
[INFO 2017-06-27 17:06:47,615 main.py:51] epoch 258, training loss: 8070.52, average training loss: 10763.24, base loss: 9242.90
[INFO 2017-06-27 17:06:48,827 main.py:51] epoch 259, training loss: 7714.91, average training loss: 10751.51, base loss: 9239.31
[INFO 2017-06-27 17:06:50,038 main.py:51] epoch 260, training loss: 8117.65, average training loss: 10741.42, base loss: 9236.91
[INFO 2017-06-27 17:06:51,249 main.py:51] epoch 261, training loss: 8323.78, average training loss: 10732.19, base loss: 9235.54
[INFO 2017-06-27 17:06:52,463 main.py:51] epoch 262, training loss: 8247.67, average training loss: 10722.75, base loss: 9234.38
[INFO 2017-06-27 17:06:53,673 main.py:51] epoch 263, training loss: 8203.32, average training loss: 10713.20, base loss: 9233.01
[INFO 2017-06-27 17:06:54,882 main.py:51] epoch 264, training loss: 9022.75, average training loss: 10706.83, base loss: 9234.97
[INFO 2017-06-27 17:06:56,095 main.py:51] epoch 265, training loss: 7729.90, average training loss: 10695.63, base loss: 9232.14
[INFO 2017-06-27 17:06:57,304 main.py:51] epoch 266, training loss: 8001.72, average training loss: 10685.54, base loss: 9229.53
[INFO 2017-06-27 17:06:58,518 main.py:51] epoch 267, training loss: 9005.44, average training loss: 10679.28, base loss: 9232.08
[INFO 2017-06-27 17:06:59,728 main.py:51] epoch 268, training loss: 8747.10, average training loss: 10672.09, base loss: 9232.67
[INFO 2017-06-27 17:07:00,936 main.py:51] epoch 269, training loss: 11908.40, average training loss: 10676.67, base loss: 9245.14
[INFO 2017-06-27 17:07:02,153 main.py:51] epoch 270, training loss: 7119.38, average training loss: 10663.54, base loss: 9238.70
[INFO 2017-06-27 17:07:03,364 main.py:51] epoch 271, training loss: 8681.78, average training loss: 10656.26, base loss: 9239.71
[INFO 2017-06-27 17:07:04,576 main.py:51] epoch 272, training loss: 8438.23, average training loss: 10648.13, base loss: 9239.13
[INFO 2017-06-27 17:07:05,789 main.py:51] epoch 273, training loss: 8304.07, average training loss: 10639.58, base loss: 9238.19
[INFO 2017-06-27 17:07:07,002 main.py:51] epoch 274, training loss: 7797.98, average training loss: 10629.25, base loss: 9235.52
[INFO 2017-06-27 17:07:08,214 main.py:51] epoch 275, training loss: 8159.32, average training loss: 10620.30, base loss: 9233.93
[INFO 2017-06-27 17:07:09,425 main.py:51] epoch 276, training loss: 8688.61, average training loss: 10613.32, base loss: 9234.25
[INFO 2017-06-27 17:07:10,632 main.py:51] epoch 277, training loss: 9180.69, average training loss: 10608.17, base loss: 9237.37
[INFO 2017-06-27 17:07:11,843 main.py:51] epoch 278, training loss: 10994.49, average training loss: 10609.55, base loss: 9246.05
[INFO 2017-06-27 17:07:13,055 main.py:51] epoch 279, training loss: 7780.61, average training loss: 10599.45, base loss: 9243.24
[INFO 2017-06-27 17:07:14,269 main.py:51] epoch 280, training loss: 8460.70, average training loss: 10591.84, base loss: 9243.09
[INFO 2017-06-27 17:07:15,479 main.py:51] epoch 281, training loss: 7613.06, average training loss: 10581.28, base loss: 9239.63
[INFO 2017-06-27 17:07:16,686 main.py:51] epoch 282, training loss: 8857.73, average training loss: 10575.19, base loss: 9241.46
[INFO 2017-06-27 17:07:17,898 main.py:51] epoch 283, training loss: 7685.73, average training loss: 10565.01, base loss: 9238.28
[INFO 2017-06-27 17:07:19,112 main.py:51] epoch 284, training loss: 7744.56, average training loss: 10555.12, base loss: 9235.17
[INFO 2017-06-27 17:07:20,322 main.py:51] epoch 285, training loss: 7560.76, average training loss: 10544.65, base loss: 9231.03
[INFO 2017-06-27 17:07:21,530 main.py:51] epoch 286, training loss: 6974.60, average training loss: 10532.21, base loss: 9224.68
[INFO 2017-06-27 17:07:22,742 main.py:51] epoch 287, training loss: 8692.03, average training loss: 10525.82, base loss: 9225.20
[INFO 2017-06-27 17:07:23,953 main.py:51] epoch 288, training loss: 6997.32, average training loss: 10513.61, base loss: 9219.38
[INFO 2017-06-27 17:07:25,168 main.py:51] epoch 289, training loss: 7831.93, average training loss: 10504.36, base loss: 9217.11
[INFO 2017-06-27 17:07:26,382 main.py:51] epoch 290, training loss: 7783.24, average training loss: 10495.01, base loss: 9214.21
[INFO 2017-06-27 17:07:27,590 main.py:51] epoch 291, training loss: 7939.57, average training loss: 10486.26, base loss: 9211.87
[INFO 2017-06-27 17:07:28,802 main.py:51] epoch 292, training loss: 8729.13, average training loss: 10480.26, base loss: 9213.06
[INFO 2017-06-27 17:07:30,011 main.py:51] epoch 293, training loss: 9272.01, average training loss: 10476.15, base loss: 9216.09
[INFO 2017-06-27 17:07:31,227 main.py:51] epoch 294, training loss: 7759.22, average training loss: 10466.94, base loss: 9213.50
[INFO 2017-06-27 17:07:32,437 main.py:51] epoch 295, training loss: 8197.86, average training loss: 10459.28, base loss: 9212.79
[INFO 2017-06-27 17:07:33,647 main.py:51] epoch 296, training loss: 7894.76, average training loss: 10450.64, base loss: 9211.12
[INFO 2017-06-27 17:07:34,855 main.py:51] epoch 297, training loss: 8156.00, average training loss: 10442.94, base loss: 9210.19
[INFO 2017-06-27 17:07:36,063 main.py:51] epoch 298, training loss: 8723.68, average training loss: 10437.19, base loss: 9211.12
[INFO 2017-06-27 17:07:37,272 main.py:51] epoch 299, training loss: 8119.07, average training loss: 10429.46, base loss: 9209.31
[INFO 2017-06-27 17:07:37,273 main.py:53] epoch 299, testing
[INFO 2017-06-27 17:07:41,973 main.py:105] average testing loss: 8378.10, base loss: 9115.15
[INFO 2017-06-27 17:07:41,974 main.py:106] improve_loss: 737.05, improve_percent: 0.08
[INFO 2017-06-27 17:07:41,975 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 17:07:41,987 main.py:76] current best improved percent: 0.08
[INFO 2017-06-27 17:07:43,193 main.py:51] epoch 300, training loss: 7118.90, average training loss: 10418.47, base loss: 9204.24
[INFO 2017-06-27 17:07:44,406 main.py:51] epoch 301, training loss: 10789.28, average training loss: 10419.69, base loss: 9211.21
[INFO 2017-06-27 17:07:45,618 main.py:51] epoch 302, training loss: 7807.16, average training loss: 10411.07, base loss: 9208.37
[INFO 2017-06-27 17:07:46,832 main.py:51] epoch 303, training loss: 7664.25, average training loss: 10402.04, base loss: 9205.63
[INFO 2017-06-27 17:07:48,050 main.py:51] epoch 304, training loss: 11072.67, average training loss: 10404.23, base loss: 9213.94
[INFO 2017-06-27 17:07:49,266 main.py:51] epoch 305, training loss: 8100.90, average training loss: 10396.71, base loss: 9213.36
[INFO 2017-06-27 17:07:50,484 main.py:51] epoch 306, training loss: 8110.04, average training loss: 10389.26, base loss: 9212.90
[INFO 2017-06-27 17:07:51,693 main.py:51] epoch 307, training loss: 7234.48, average training loss: 10379.02, base loss: 9208.30
[INFO 2017-06-27 17:07:52,904 main.py:51] epoch 308, training loss: 7399.85, average training loss: 10369.38, base loss: 9204.68
[INFO 2017-06-27 17:07:54,118 main.py:51] epoch 309, training loss: 8421.58, average training loss: 10363.09, base loss: 9205.52
[INFO 2017-06-27 17:07:55,328 main.py:51] epoch 310, training loss: 8457.45, average training loss: 10356.96, base loss: 9206.43
[INFO 2017-06-27 17:07:56,538 main.py:51] epoch 311, training loss: 8117.74, average training loss: 10349.79, base loss: 9205.14
[INFO 2017-06-27 17:07:57,762 main.py:51] epoch 312, training loss: 8301.20, average training loss: 10343.24, base loss: 9204.73
[INFO 2017-06-27 17:07:58,976 main.py:51] epoch 313, training loss: 7815.81, average training loss: 10335.19, base loss: 9202.40
[INFO 2017-06-27 17:08:00,194 main.py:51] epoch 314, training loss: 11284.38, average training loss: 10338.21, base loss: 9211.54
[INFO 2017-06-27 17:08:01,416 main.py:51] epoch 315, training loss: 7261.13, average training loss: 10328.47, base loss: 9207.68
[INFO 2017-06-27 17:08:02,625 main.py:51] epoch 316, training loss: 8120.40, average training loss: 10321.50, base loss: 9207.07
[INFO 2017-06-27 17:08:03,836 main.py:51] epoch 317, training loss: 11437.57, average training loss: 10325.01, base loss: 9216.48
[INFO 2017-06-27 17:08:05,050 main.py:51] epoch 318, training loss: 8469.29, average training loss: 10319.20, base loss: 9216.46
[INFO 2017-06-27 17:08:06,261 main.py:51] epoch 319, training loss: 7742.21, average training loss: 10311.14, base loss: 9214.58
[INFO 2017-06-27 17:08:07,473 main.py:51] epoch 320, training loss: 7506.60, average training loss: 10302.41, base loss: 9211.51
[INFO 2017-06-27 17:08:08,681 main.py:51] epoch 321, training loss: 8218.78, average training loss: 10295.93, base loss: 9211.05
[INFO 2017-06-27 17:08:09,887 main.py:51] epoch 322, training loss: 8914.33, average training loss: 10291.66, base loss: 9213.33
[INFO 2017-06-27 17:08:11,101 main.py:51] epoch 323, training loss: 7817.80, average training loss: 10284.02, base loss: 9211.73
[INFO 2017-06-27 17:08:12,311 main.py:51] epoch 324, training loss: 7185.92, average training loss: 10274.49, base loss: 9207.38
[INFO 2017-06-27 17:08:13,522 main.py:51] epoch 325, training loss: 7457.71, average training loss: 10265.85, base loss: 9204.82
[INFO 2017-06-27 17:08:14,735 main.py:51] epoch 326, training loss: 8563.30, average training loss: 10260.64, base loss: 9206.21
[INFO 2017-06-27 17:08:15,944 main.py:51] epoch 327, training loss: 8073.29, average training loss: 10253.97, base loss: 9205.99
[INFO 2017-06-27 17:08:17,153 main.py:51] epoch 328, training loss: 8059.66, average training loss: 10247.30, base loss: 9204.87
[INFO 2017-06-27 17:08:18,364 main.py:51] epoch 329, training loss: 8238.50, average training loss: 10241.22, base loss: 9204.13
[INFO 2017-06-27 17:08:19,576 main.py:51] epoch 330, training loss: 8749.56, average training loss: 10236.71, base loss: 9206.06
[INFO 2017-06-27 17:08:20,788 main.py:51] epoch 331, training loss: 11748.70, average training loss: 10241.26, base loss: 9216.78
[INFO 2017-06-27 17:08:22,001 main.py:51] epoch 332, training loss: 8645.55, average training loss: 10236.47, base loss: 9218.59
[INFO 2017-06-27 17:08:23,213 main.py:51] epoch 333, training loss: 8988.38, average training loss: 10232.74, base loss: 9222.05
[INFO 2017-06-27 17:08:24,425 main.py:51] epoch 334, training loss: 7555.51, average training loss: 10224.74, base loss: 9218.71
[INFO 2017-06-27 17:08:25,631 main.py:51] epoch 335, training loss: 9071.96, average training loss: 10221.31, base loss: 9221.68
[INFO 2017-06-27 17:08:26,843 main.py:51] epoch 336, training loss: 8347.60, average training loss: 10215.75, base loss: 9222.44
[INFO 2017-06-27 17:08:28,056 main.py:51] epoch 337, training loss: 11791.71, average training loss: 10220.42, base loss: 9232.89
[INFO 2017-06-27 17:08:29,266 main.py:51] epoch 338, training loss: 7759.71, average training loss: 10213.16, base loss: 9231.34
[INFO 2017-06-27 17:08:30,472 main.py:51] epoch 339, training loss: 7921.75, average training loss: 10206.42, base loss: 9229.38
[INFO 2017-06-27 17:08:31,679 main.py:51] epoch 340, training loss: 8683.24, average training loss: 10201.95, base loss: 9231.50
[INFO 2017-06-27 17:08:32,886 main.py:51] epoch 341, training loss: 8391.10, average training loss: 10196.66, base loss: 9231.83
[INFO 2017-06-27 17:08:34,095 main.py:51] epoch 342, training loss: 8642.76, average training loss: 10192.13, base loss: 9233.19
[INFO 2017-06-27 17:08:35,304 main.py:51] epoch 343, training loss: 11746.38, average training loss: 10196.64, base loss: 9243.85
[INFO 2017-06-27 17:08:36,512 main.py:51] epoch 344, training loss: 8976.29, average training loss: 10193.11, base loss: 9246.71
[INFO 2017-06-27 17:08:37,723 main.py:51] epoch 345, training loss: 7151.49, average training loss: 10184.32, base loss: 9242.54
[INFO 2017-06-27 17:08:38,938 main.py:51] epoch 346, training loss: 8586.57, average training loss: 10179.71, base loss: 9244.52
[INFO 2017-06-27 17:08:40,149 main.py:51] epoch 347, training loss: 11016.64, average training loss: 10182.12, base loss: 9252.25
[INFO 2017-06-27 17:08:41,357 main.py:51] epoch 348, training loss: 7633.03, average training loss: 10174.81, base loss: 9248.95
[INFO 2017-06-27 17:08:42,565 main.py:51] epoch 349, training loss: 7649.11, average training loss: 10167.60, base loss: 9246.45
[INFO 2017-06-27 17:08:43,775 main.py:51] epoch 350, training loss: 8216.19, average training loss: 10162.04, base loss: 9246.51
[INFO 2017-06-27 17:08:44,985 main.py:51] epoch 351, training loss: 8704.05, average training loss: 10157.89, base loss: 9248.70
[INFO 2017-06-27 17:08:46,196 main.py:51] epoch 352, training loss: 8039.73, average training loss: 10151.89, base loss: 9248.64
[INFO 2017-06-27 17:08:47,404 main.py:51] epoch 353, training loss: 8441.46, average training loss: 10147.06, base loss: 9249.97
[INFO 2017-06-27 17:08:48,615 main.py:51] epoch 354, training loss: 7899.14, average training loss: 10140.73, base loss: 9248.70
[INFO 2017-06-27 17:08:49,825 main.py:51] epoch 355, training loss: 7132.16, average training loss: 10132.28, base loss: 9244.14
[INFO 2017-06-27 17:08:51,039 main.py:51] epoch 356, training loss: 8636.84, average training loss: 10128.09, base loss: 9245.67
[INFO 2017-06-27 17:08:52,249 main.py:51] epoch 357, training loss: 8289.64, average training loss: 10122.95, base loss: 9245.54
[INFO 2017-06-27 17:08:53,463 main.py:51] epoch 358, training loss: 7140.80, average training loss: 10114.65, base loss: 9241.87
[INFO 2017-06-27 17:08:54,673 main.py:51] epoch 359, training loss: 7566.59, average training loss: 10107.57, base loss: 9239.18
[INFO 2017-06-27 17:08:55,882 main.py:51] epoch 360, training loss: 7283.83, average training loss: 10099.75, base loss: 9235.84
[INFO 2017-06-27 17:08:57,094 main.py:51] epoch 361, training loss: 7242.99, average training loss: 10091.86, base loss: 9232.25
[INFO 2017-06-27 17:08:58,303 main.py:51] epoch 362, training loss: 9101.97, average training loss: 10089.13, base loss: 9235.12
[INFO 2017-06-27 17:08:59,512 main.py:51] epoch 363, training loss: 7513.18, average training loss: 10082.05, base loss: 9232.97
[INFO 2017-06-27 17:09:00,722 main.py:51] epoch 364, training loss: 7591.32, average training loss: 10075.23, base loss: 9230.64
[INFO 2017-06-27 17:09:01,932 main.py:51] epoch 365, training loss: 8224.51, average training loss: 10070.17, base loss: 9231.12
[INFO 2017-06-27 17:09:03,141 main.py:51] epoch 366, training loss: 7493.14, average training loss: 10063.15, base loss: 9228.47
[INFO 2017-06-27 17:09:04,351 main.py:51] epoch 367, training loss: 10673.35, average training loss: 10064.81, base loss: 9234.46
[INFO 2017-06-27 17:09:05,562 main.py:51] epoch 368, training loss: 7440.53, average training loss: 10057.70, base loss: 9232.41
[INFO 2017-06-27 17:09:06,773 main.py:51] epoch 369, training loss: 8208.61, average training loss: 10052.70, base loss: 9232.03
[INFO 2017-06-27 17:09:07,984 main.py:51] epoch 370, training loss: 7590.30, average training loss: 10046.06, base loss: 9230.00
[INFO 2017-06-27 17:09:09,194 main.py:51] epoch 371, training loss: 8232.22, average training loss: 10041.19, base loss: 9230.09
[INFO 2017-06-27 17:09:10,405 main.py:51] epoch 372, training loss: 8517.11, average training loss: 10037.10, base loss: 9231.19
[INFO 2017-06-27 17:09:11,614 main.py:51] epoch 373, training loss: 7981.19, average training loss: 10031.60, base loss: 9230.69
[INFO 2017-06-27 17:09:12,825 main.py:51] epoch 374, training loss: 7436.22, average training loss: 10024.68, base loss: 9228.22
[INFO 2017-06-27 17:09:14,032 main.py:51] epoch 375, training loss: 8171.21, average training loss: 10019.75, base loss: 9228.43
[INFO 2017-06-27 17:09:15,238 main.py:51] epoch 376, training loss: 8583.13, average training loss: 10015.94, base loss: 9230.63
[INFO 2017-06-27 17:09:16,449 main.py:51] epoch 377, training loss: 8320.96, average training loss: 10011.46, base loss: 9231.43
[INFO 2017-06-27 17:09:17,661 main.py:51] epoch 378, training loss: 7490.82, average training loss: 10004.81, base loss: 9229.09
[INFO 2017-06-27 17:09:18,873 main.py:51] epoch 379, training loss: 8294.25, average training loss: 10000.31, base loss: 9229.85
[INFO 2017-06-27 17:09:20,086 main.py:51] epoch 380, training loss: 7821.79, average training loss: 9994.59, base loss: 9229.28
[INFO 2017-06-27 17:09:21,294 main.py:51] epoch 381, training loss: 7599.41, average training loss: 9988.32, base loss: 9227.87
[INFO 2017-06-27 17:09:22,510 main.py:51] epoch 382, training loss: 8732.57, average training loss: 9985.04, base loss: 9229.49
[INFO 2017-06-27 17:09:23,723 main.py:51] epoch 383, training loss: 7570.80, average training loss: 9978.75, base loss: 9227.34
[INFO 2017-06-27 17:09:24,935 main.py:51] epoch 384, training loss: 7908.43, average training loss: 9973.37, base loss: 9226.38
[INFO 2017-06-27 17:09:26,149 main.py:51] epoch 385, training loss: 7531.29, average training loss: 9967.05, base loss: 9224.58
[INFO 2017-06-27 17:09:27,359 main.py:51] epoch 386, training loss: 8152.80, average training loss: 9962.36, base loss: 9225.29
[INFO 2017-06-27 17:09:28,569 main.py:51] epoch 387, training loss: 8734.46, average training loss: 9959.19, base loss: 9228.23
[INFO 2017-06-27 17:09:29,782 main.py:51] epoch 388, training loss: 6971.75, average training loss: 9951.52, base loss: 9224.48
[INFO 2017-06-27 17:09:30,992 main.py:51] epoch 389, training loss: 7596.40, average training loss: 9945.48, base loss: 9222.30
[INFO 2017-06-27 17:09:32,201 main.py:51] epoch 390, training loss: 7152.76, average training loss: 9938.33, base loss: 9219.42
[INFO 2017-06-27 17:09:33,412 main.py:51] epoch 391, training loss: 8410.25, average training loss: 9934.44, base loss: 9220.18
[INFO 2017-06-27 17:09:34,623 main.py:51] epoch 392, training loss: 9329.96, average training loss: 9932.90, base loss: 9224.12
[INFO 2017-06-27 17:09:35,833 main.py:51] epoch 393, training loss: 8214.87, average training loss: 9928.54, base loss: 9224.79
[INFO 2017-06-27 17:09:37,047 main.py:51] epoch 394, training loss: 7627.63, average training loss: 9922.71, base loss: 9223.46
[INFO 2017-06-27 17:09:38,253 main.py:51] epoch 395, training loss: 7993.06, average training loss: 9917.84, base loss: 9223.81
[INFO 2017-06-27 17:09:39,461 main.py:51] epoch 396, training loss: 7850.88, average training loss: 9912.63, base loss: 9223.77
[INFO 2017-06-27 17:09:40,673 main.py:51] epoch 397, training loss: 8224.49, average training loss: 9908.39, base loss: 9223.95
[INFO 2017-06-27 17:09:41,886 main.py:51] epoch 398, training loss: 7982.87, average training loss: 9903.57, base loss: 9223.90
[INFO 2017-06-27 17:09:43,095 main.py:51] epoch 399, training loss: 11150.81, average training loss: 9906.68, base loss: 9232.29
[INFO 2017-06-27 17:09:43,095 main.py:53] epoch 399, testing
[INFO 2017-06-27 17:09:47,806 main.py:105] average testing loss: 7851.59, base loss: 8983.16
[INFO 2017-06-27 17:09:47,806 main.py:106] improve_loss: 1131.57, improve_percent: 0.13
[INFO 2017-06-27 17:09:47,807 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 17:09:47,819 main.py:76] current best improved percent: 0.13
[INFO 2017-06-27 17:09:49,032 main.py:51] epoch 400, training loss: 7439.63, average training loss: 9900.53, base loss: 9231.12
[INFO 2017-06-27 17:09:50,244 main.py:51] epoch 401, training loss: 12109.71, average training loss: 9906.03, base loss: 9241.44
[INFO 2017-06-27 17:09:51,456 main.py:51] epoch 402, training loss: 8092.53, average training loss: 9901.53, base loss: 9241.50
[INFO 2017-06-27 17:09:52,669 main.py:51] epoch 403, training loss: 7911.22, average training loss: 9896.60, base loss: 9241.37
[INFO 2017-06-27 17:09:53,886 main.py:51] epoch 404, training loss: 9507.98, average training loss: 9895.64, base loss: 9245.51
[INFO 2017-06-27 17:09:55,099 main.py:51] epoch 405, training loss: 7749.80, average training loss: 9890.36, base loss: 9244.62
[INFO 2017-06-27 17:09:56,308 main.py:51] epoch 406, training loss: 8715.05, average training loss: 9887.47, base loss: 9246.36
[INFO 2017-06-27 17:09:57,524 main.py:51] epoch 407, training loss: 8459.07, average training loss: 9883.97, base loss: 9247.49
[INFO 2017-06-27 17:09:58,739 main.py:51] epoch 408, training loss: 7580.02, average training loss: 9878.33, base loss: 9246.65
[INFO 2017-06-27 17:09:59,954 main.py:51] epoch 409, training loss: 11229.77, average training loss: 9881.63, base loss: 9253.80
[INFO 2017-06-27 17:10:01,171 main.py:51] epoch 410, training loss: 11247.28, average training loss: 9884.95, base loss: 9261.21
[INFO 2017-06-27 17:10:02,388 main.py:51] epoch 411, training loss: 7563.50, average training loss: 9879.32, base loss: 9259.74
[INFO 2017-06-27 17:10:03,602 main.py:51] epoch 412, training loss: 7997.27, average training loss: 9874.76, base loss: 9259.59
[INFO 2017-06-27 17:10:04,814 main.py:51] epoch 413, training loss: 7356.78, average training loss: 9868.68, base loss: 9256.90
[INFO 2017-06-27 17:10:06,026 main.py:51] epoch 414, training loss: 8509.06, average training loss: 9865.40, base loss: 9258.30
[INFO 2017-06-27 17:10:07,243 main.py:51] epoch 415, training loss: 7228.20, average training loss: 9859.06, base loss: 9255.66
[INFO 2017-06-27 17:10:08,455 main.py:51] epoch 416, training loss: 7535.24, average training loss: 9853.49, base loss: 9254.38
[INFO 2017-06-27 17:10:09,668 main.py:51] epoch 417, training loss: 8301.17, average training loss: 9849.78, base loss: 9255.38
[INFO 2017-06-27 17:10:10,882 main.py:51] epoch 418, training loss: 6722.46, average training loss: 9842.31, base loss: 9251.42
[INFO 2017-06-27 17:10:12,097 main.py:51] epoch 419, training loss: 7318.48, average training loss: 9836.30, base loss: 9249.39
[INFO 2017-06-27 17:10:13,313 main.py:51] epoch 420, training loss: 7992.07, average training loss: 9831.92, base loss: 9249.45
[INFO 2017-06-27 17:10:14,524 main.py:51] epoch 421, training loss: 8048.73, average training loss: 9827.70, base loss: 9249.78
[INFO 2017-06-27 17:10:15,740 main.py:51] epoch 422, training loss: 8084.31, average training loss: 9823.58, base loss: 9249.72
[INFO 2017-06-27 17:10:16,953 main.py:51] epoch 423, training loss: 7355.92, average training loss: 9817.76, base loss: 9247.98
[INFO 2017-06-27 17:10:18,168 main.py:51] epoch 424, training loss: 7594.79, average training loss: 9812.53, base loss: 9246.60
[INFO 2017-06-27 17:10:19,383 main.py:51] epoch 425, training loss: 7853.29, average training loss: 9807.93, base loss: 9245.93
[INFO 2017-06-27 17:10:20,597 main.py:51] epoch 426, training loss: 7843.27, average training loss: 9803.33, base loss: 9245.84
[INFO 2017-06-27 17:10:21,809 main.py:51] epoch 427, training loss: 7301.24, average training loss: 9797.48, base loss: 9243.46
[INFO 2017-06-27 17:10:23,020 main.py:51] epoch 428, training loss: 7265.06, average training loss: 9791.58, base loss: 9241.13
[INFO 2017-06-27 17:10:24,234 main.py:51] epoch 429, training loss: 7133.66, average training loss: 9785.40, base loss: 9238.47
[INFO 2017-06-27 17:10:25,448 main.py:51] epoch 430, training loss: 7711.99, average training loss: 9780.58, base loss: 9237.54
[INFO 2017-06-27 17:10:26,660 main.py:51] epoch 431, training loss: 7812.70, average training loss: 9776.03, base loss: 9236.49
[INFO 2017-06-27 17:10:27,873 main.py:51] epoch 432, training loss: 8260.01, average training loss: 9772.53, base loss: 9237.12
[INFO 2017-06-27 17:10:29,087 main.py:51] epoch 433, training loss: 7100.70, average training loss: 9766.37, base loss: 9233.80
[INFO 2017-06-27 17:10:30,300 main.py:51] epoch 434, training loss: 8163.36, average training loss: 9762.69, base loss: 9235.11
[INFO 2017-06-27 17:10:31,511 main.py:51] epoch 435, training loss: 7071.92, average training loss: 9756.52, base loss: 9232.64
[INFO 2017-06-27 17:10:32,721 main.py:51] epoch 436, training loss: 7288.43, average training loss: 9750.87, base loss: 9230.88
[INFO 2017-06-27 17:10:33,935 main.py:51] epoch 437, training loss: 7333.16, average training loss: 9745.35, base loss: 9228.55
[INFO 2017-06-27 17:10:35,147 main.py:51] epoch 438, training loss: 7912.22, average training loss: 9741.17, base loss: 9228.08
[INFO 2017-06-27 17:10:36,359 main.py:51] epoch 439, training loss: 7736.26, average training loss: 9736.62, base loss: 9227.18
[INFO 2017-06-27 17:10:37,574 main.py:51] epoch 440, training loss: 11015.50, average training loss: 9739.52, base loss: 9234.00
[INFO 2017-06-27 17:10:38,787 main.py:51] epoch 441, training loss: 7620.39, average training loss: 9734.72, base loss: 9233.78
[INFO 2017-06-27 17:10:40,002 main.py:51] epoch 442, training loss: 7863.97, average training loss: 9730.50, base loss: 9232.87
[INFO 2017-06-27 17:10:41,216 main.py:51] epoch 443, training loss: 7070.67, average training loss: 9724.51, base loss: 9229.88
[INFO 2017-06-27 17:10:42,426 main.py:51] epoch 444, training loss: 7315.48, average training loss: 9719.09, base loss: 9228.90
[INFO 2017-06-27 17:10:43,642 main.py:51] epoch 445, training loss: 8152.58, average training loss: 9715.58, base loss: 9229.29
[INFO 2017-06-27 17:10:44,855 main.py:51] epoch 446, training loss: 7057.21, average training loss: 9709.63, base loss: 9226.75
[INFO 2017-06-27 17:10:46,069 main.py:51] epoch 447, training loss: 6855.07, average training loss: 9703.26, base loss: 9223.65
[INFO 2017-06-27 17:10:47,285 main.py:51] epoch 448, training loss: 8087.18, average training loss: 9699.66, base loss: 9224.08
[INFO 2017-06-27 17:10:48,499 main.py:51] epoch 449, training loss: 7575.98, average training loss: 9694.94, base loss: 9223.34
[INFO 2017-06-27 17:10:49,715 main.py:51] epoch 450, training loss: 7283.05, average training loss: 9689.60, base loss: 9221.95
[INFO 2017-06-27 17:10:50,929 main.py:51] epoch 451, training loss: 8682.18, average training loss: 9687.37, base loss: 9224.88
[INFO 2017-06-27 17:10:52,144 main.py:51] epoch 452, training loss: 8071.53, average training loss: 9683.80, base loss: 9225.06
[INFO 2017-06-27 17:10:53,357 main.py:51] epoch 453, training loss: 7986.52, average training loss: 9680.06, base loss: 9225.45
[INFO 2017-06-27 17:10:54,568 main.py:51] epoch 454, training loss: 8232.64, average training loss: 9676.88, base loss: 9226.91
[INFO 2017-06-27 17:10:55,778 main.py:51] epoch 455, training loss: 7850.41, average training loss: 9672.87, base loss: 9226.84
[INFO 2017-06-27 17:10:56,993 main.py:51] epoch 456, training loss: 7205.66, average training loss: 9667.48, base loss: 9225.39
[INFO 2017-06-27 17:10:58,210 main.py:51] epoch 457, training loss: 7482.02, average training loss: 9662.70, base loss: 9223.70
[INFO 2017-06-27 17:10:59,422 main.py:51] epoch 458, training loss: 8143.12, average training loss: 9659.39, base loss: 9224.63
[INFO 2017-06-27 17:11:00,633 main.py:51] epoch 459, training loss: 8472.91, average training loss: 9656.81, base loss: 9226.26
[INFO 2017-06-27 17:11:01,845 main.py:51] epoch 460, training loss: 7783.74, average training loss: 9652.75, base loss: 9227.33
[INFO 2017-06-27 17:11:03,056 main.py:51] epoch 461, training loss: 6562.57, average training loss: 9646.06, base loss: 9223.39
[INFO 2017-06-27 17:11:04,266 main.py:51] epoch 462, training loss: 7316.60, average training loss: 9641.03, base loss: 9221.39
[INFO 2017-06-27 17:11:05,479 main.py:51] epoch 463, training loss: 7883.93, average training loss: 9637.24, base loss: 9221.69
[INFO 2017-06-27 17:11:06,694 main.py:51] epoch 464, training loss: 7918.43, average training loss: 9633.55, base loss: 9221.86
[INFO 2017-06-27 17:11:07,905 main.py:51] epoch 465, training loss: 7196.83, average training loss: 9628.32, base loss: 9220.19
[INFO 2017-06-27 17:11:09,116 main.py:51] epoch 466, training loss: 7145.52, average training loss: 9623.00, base loss: 9218.71
[INFO 2017-06-27 17:11:10,328 main.py:51] epoch 467, training loss: 7778.64, average training loss: 9619.06, base loss: 9218.75
[INFO 2017-06-27 17:11:11,543 main.py:51] epoch 468, training loss: 11450.04, average training loss: 9622.97, base loss: 9226.65
[INFO 2017-06-27 17:11:12,753 main.py:51] epoch 469, training loss: 7636.33, average training loss: 9618.74, base loss: 9226.27
[INFO 2017-06-27 17:11:13,964 main.py:51] epoch 470, training loss: 7357.26, average training loss: 9613.94, base loss: 9224.85
[INFO 2017-06-27 17:11:15,176 main.py:51] epoch 471, training loss: 7706.30, average training loss: 9609.90, base loss: 9224.38
[INFO 2017-06-27 17:11:16,386 main.py:51] epoch 472, training loss: 7752.06, average training loss: 9605.97, base loss: 9223.42
[INFO 2017-06-27 17:11:17,596 main.py:51] epoch 473, training loss: 7226.26, average training loss: 9600.95, base loss: 9220.93
[INFO 2017-06-27 17:11:18,807 main.py:51] epoch 474, training loss: 7396.95, average training loss: 9596.31, base loss: 9219.69
[INFO 2017-06-27 17:11:20,018 main.py:51] epoch 475, training loss: 7095.72, average training loss: 9591.05, base loss: 9217.77
[INFO 2017-06-27 17:11:21,231 main.py:51] epoch 476, training loss: 7319.10, average training loss: 9586.29, base loss: 9216.68
[INFO 2017-06-27 17:11:22,445 main.py:51] epoch 477, training loss: 7393.28, average training loss: 9581.70, base loss: 9215.22
[INFO 2017-06-27 17:11:23,657 main.py:51] epoch 478, training loss: 7236.78, average training loss: 9576.81, base loss: 9214.02
[INFO 2017-06-27 17:11:24,870 main.py:51] epoch 479, training loss: 6684.26, average training loss: 9570.78, base loss: 9211.43
[INFO 2017-06-27 17:11:26,081 main.py:51] epoch 480, training loss: 8096.35, average training loss: 9567.72, base loss: 9212.45
[INFO 2017-06-27 17:11:27,299 main.py:51] epoch 481, training loss: 8451.56, average training loss: 9565.40, base loss: 9214.96
[INFO 2017-06-27 17:11:28,513 main.py:51] epoch 482, training loss: 7377.76, average training loss: 9560.87, base loss: 9213.66
[INFO 2017-06-27 17:11:29,728 main.py:51] epoch 483, training loss: 7542.18, average training loss: 9556.70, base loss: 9212.11
[INFO 2017-06-27 17:11:30,946 main.py:51] epoch 484, training loss: 7292.39, average training loss: 9552.03, base loss: 9210.56
[INFO 2017-06-27 17:11:32,160 main.py:51] epoch 485, training loss: 7502.90, average training loss: 9547.82, base loss: 9209.40
[INFO 2017-06-27 17:11:33,371 main.py:51] epoch 486, training loss: 7164.71, average training loss: 9542.92, base loss: 9207.07
[INFO 2017-06-27 17:11:34,585 main.py:51] epoch 487, training loss: 8542.36, average training loss: 9540.87, base loss: 9208.55
[INFO 2017-06-27 17:11:35,797 main.py:51] epoch 488, training loss: 7238.26, average training loss: 9536.16, base loss: 9206.75
[INFO 2017-06-27 17:11:37,008 main.py:51] epoch 489, training loss: 7134.18, average training loss: 9531.26, base loss: 9204.81
[INFO 2017-06-27 17:11:38,219 main.py:51] epoch 490, training loss: 7247.26, average training loss: 9526.61, base loss: 9202.77
[INFO 2017-06-27 17:11:39,433 main.py:51] epoch 491, training loss: 8328.37, average training loss: 9524.17, base loss: 9205.07
[INFO 2017-06-27 17:11:40,649 main.py:51] epoch 492, training loss: 7813.88, average training loss: 9520.70, base loss: 9204.63
[INFO 2017-06-27 17:11:41,862 main.py:51] epoch 493, training loss: 6942.92, average training loss: 9515.49, base loss: 9202.41
[INFO 2017-06-27 17:11:43,074 main.py:51] epoch 494, training loss: 8051.00, average training loss: 9512.53, base loss: 9203.01
[INFO 2017-06-27 17:11:44,291 main.py:51] epoch 495, training loss: 11456.04, average training loss: 9516.45, base loss: 9210.55
[INFO 2017-06-27 17:11:45,502 main.py:51] epoch 496, training loss: 6990.68, average training loss: 9511.36, base loss: 9208.74
[INFO 2017-06-27 17:11:46,714 main.py:51] epoch 497, training loss: 7674.10, average training loss: 9507.68, base loss: 9208.89
[INFO 2017-06-27 17:11:47,925 main.py:51] epoch 498, training loss: 7554.92, average training loss: 9503.76, base loss: 9208.52
[INFO 2017-06-27 17:11:49,134 main.py:51] epoch 499, training loss: 7768.44, average training loss: 9500.29, base loss: 9208.85
[INFO 2017-06-27 17:11:49,134 main.py:53] epoch 499, testing
[INFO 2017-06-27 17:11:53,837 main.py:105] average testing loss: 8026.35, base loss: 9390.74
[INFO 2017-06-27 17:11:53,837 main.py:106] improve_loss: 1364.38, improve_percent: 0.15
[INFO 2017-06-27 17:11:53,838 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 17:11:53,850 main.py:76] current best improved percent: 0.15
[INFO 2017-06-27 17:11:55,057 main.py:51] epoch 500, training loss: 7267.27, average training loss: 9495.83, base loss: 9207.50
[INFO 2017-06-27 17:11:56,270 main.py:51] epoch 501, training loss: 6916.08, average training loss: 9490.70, base loss: 9205.08
[INFO 2017-06-27 17:11:57,481 main.py:51] epoch 502, training loss: 8672.94, average training loss: 9489.07, base loss: 9207.28
[INFO 2017-06-27 17:11:58,694 main.py:51] epoch 503, training loss: 7745.70, average training loss: 9485.61, base loss: 9206.59
[INFO 2017-06-27 17:11:59,904 main.py:51] epoch 504, training loss: 11066.17, average training loss: 9488.74, base loss: 9212.78
[INFO 2017-06-27 17:12:01,116 main.py:51] epoch 505, training loss: 8167.81, average training loss: 9486.13, base loss: 9213.71
[INFO 2017-06-27 17:12:02,326 main.py:51] epoch 506, training loss: 10839.93, average training loss: 9488.80, base loss: 9219.74
[INFO 2017-06-27 17:12:03,538 main.py:51] epoch 507, training loss: 8304.19, average training loss: 9486.47, base loss: 9220.73
[INFO 2017-06-27 17:12:04,754 main.py:51] epoch 508, training loss: 7081.97, average training loss: 9481.74, base loss: 9218.63
[INFO 2017-06-27 17:12:05,964 main.py:51] epoch 509, training loss: 8674.72, average training loss: 9480.16, base loss: 9221.29
[INFO 2017-06-27 17:12:07,171 main.py:51] epoch 510, training loss: 7173.76, average training loss: 9475.65, base loss: 9219.47
[INFO 2017-06-27 17:12:08,384 main.py:51] epoch 511, training loss: 7021.07, average training loss: 9470.85, base loss: 9217.22
[INFO 2017-06-27 17:12:09,598 main.py:51] epoch 512, training loss: 7956.09, average training loss: 9467.90, base loss: 9217.96
[INFO 2017-06-27 17:12:10,808 main.py:51] epoch 513, training loss: 7999.87, average training loss: 9465.05, base loss: 9218.35
[INFO 2017-06-27 17:12:12,020 main.py:51] epoch 514, training loss: 7194.70, average training loss: 9460.64, base loss: 9215.99
[INFO 2017-06-27 17:12:13,233 main.py:51] epoch 515, training loss: 6999.47, average training loss: 9455.87, base loss: 9214.52
[INFO 2017-06-27 17:12:14,446 main.py:51] epoch 516, training loss: 7356.02, average training loss: 9451.81, base loss: 9213.58
[INFO 2017-06-27 17:12:15,658 main.py:51] epoch 517, training loss: 10459.69, average training loss: 9453.75, base loss: 9218.11
[INFO 2017-06-27 17:12:16,873 main.py:51] epoch 518, training loss: 7592.47, average training loss: 9450.16, base loss: 9217.12
[INFO 2017-06-27 17:12:18,089 main.py:51] epoch 519, training loss: 7067.36, average training loss: 9445.58, base loss: 9215.53
[INFO 2017-06-27 17:12:19,304 main.py:51] epoch 520, training loss: 7757.72, average training loss: 9442.34, base loss: 9215.36
[INFO 2017-06-27 17:12:20,516 main.py:51] epoch 521, training loss: 8618.78, average training loss: 9440.77, base loss: 9217.67
[INFO 2017-06-27 17:12:21,731 main.py:51] epoch 522, training loss: 7218.30, average training loss: 9436.52, base loss: 9215.90
[INFO 2017-06-27 17:12:22,941 main.py:51] epoch 523, training loss: 6741.21, average training loss: 9431.37, base loss: 9213.70
[INFO 2017-06-27 17:12:24,153 main.py:51] epoch 524, training loss: 7456.82, average training loss: 9427.61, base loss: 9212.72
[INFO 2017-06-27 17:12:25,366 main.py:51] epoch 525, training loss: 7478.51, average training loss: 9423.91, base loss: 9211.79
[INFO 2017-06-27 17:12:26,580 main.py:51] epoch 526, training loss: 7477.94, average training loss: 9420.21, base loss: 9210.94
[INFO 2017-06-27 17:12:27,792 main.py:51] epoch 527, training loss: 7119.16, average training loss: 9415.85, base loss: 9208.95
[INFO 2017-06-27 17:12:29,003 main.py:51] epoch 528, training loss: 7621.20, average training loss: 9412.46, base loss: 9208.76
[INFO 2017-06-27 17:12:30,218 main.py:51] epoch 529, training loss: 7312.80, average training loss: 9408.50, base loss: 9207.73
[INFO 2017-06-27 17:12:31,433 main.py:51] epoch 530, training loss: 7030.06, average training loss: 9404.02, base loss: 9205.54
[INFO 2017-06-27 17:12:32,645 main.py:51] epoch 531, training loss: 7539.81, average training loss: 9400.52, base loss: 9205.20
[INFO 2017-06-27 17:12:33,858 main.py:51] epoch 532, training loss: 7906.47, average training loss: 9397.71, base loss: 9205.78
[INFO 2017-06-27 17:12:35,071 main.py:51] epoch 533, training loss: 7329.84, average training loss: 9393.84, base loss: 9205.46
[INFO 2017-06-27 17:12:36,286 main.py:51] epoch 534, training loss: 7553.22, average training loss: 9390.40, base loss: 9205.27
[INFO 2017-06-27 17:12:37,496 main.py:51] epoch 535, training loss: 7646.50, average training loss: 9387.15, base loss: 9205.21
[INFO 2017-06-27 17:12:38,711 main.py:51] epoch 536, training loss: 6923.72, average training loss: 9382.56, base loss: 9203.15
[INFO 2017-06-27 17:12:39,923 main.py:51] epoch 537, training loss: 7451.58, average training loss: 9378.97, base loss: 9202.87
[INFO 2017-06-27 17:12:41,136 main.py:51] epoch 538, training loss: 7704.86, average training loss: 9375.87, base loss: 9203.14
[INFO 2017-06-27 17:12:42,348 main.py:51] epoch 539, training loss: 7152.17, average training loss: 9371.75, base loss: 9201.26
[INFO 2017-06-27 17:12:43,563 main.py:51] epoch 540, training loss: 10666.80, average training loss: 9374.14, base loss: 9206.41
[INFO 2017-06-27 17:12:44,773 main.py:51] epoch 541, training loss: 6581.01, average training loss: 9368.99, base loss: 9203.66
[INFO 2017-06-27 17:12:45,988 main.py:51] epoch 542, training loss: 11575.12, average training loss: 9373.05, base loss: 9210.82
[INFO 2017-06-27 17:12:47,202 main.py:51] epoch 543, training loss: 6854.19, average training loss: 9368.42, base loss: 9208.98
[INFO 2017-06-27 17:12:48,417 main.py:51] epoch 544, training loss: 7970.01, average training loss: 9365.85, base loss: 9210.19
[INFO 2017-06-27 17:12:49,633 main.py:51] epoch 545, training loss: 7395.11, average training loss: 9362.25, base loss: 9209.84
[INFO 2017-06-27 17:12:50,849 main.py:51] epoch 546, training loss: 7459.52, average training loss: 9358.77, base loss: 9209.76
[INFO 2017-06-27 17:12:52,063 main.py:51] epoch 547, training loss: 7103.62, average training loss: 9354.65, base loss: 9208.76
[INFO 2017-06-27 17:12:53,277 main.py:51] epoch 548, training loss: 6941.09, average training loss: 9350.26, base loss: 9207.49
[INFO 2017-06-27 17:12:54,494 main.py:51] epoch 549, training loss: 8212.36, average training loss: 9348.19, base loss: 9209.03
[INFO 2017-06-27 17:12:55,712 main.py:51] epoch 550, training loss: 7161.00, average training loss: 9344.22, base loss: 9207.57
[INFO 2017-06-27 17:12:56,928 main.py:51] epoch 551, training loss: 7505.84, average training loss: 9340.89, base loss: 9207.50
[INFO 2017-06-27 17:12:58,143 main.py:51] epoch 552, training loss: 7176.81, average training loss: 9336.97, base loss: 9206.10
[INFO 2017-06-27 17:12:59,355 main.py:51] epoch 553, training loss: 7961.42, average training loss: 9334.49, base loss: 9206.69
[INFO 2017-06-27 17:13:00,568 main.py:51] epoch 554, training loss: 8762.69, average training loss: 9333.46, base loss: 9209.95
[INFO 2017-06-27 17:13:01,783 main.py:51] epoch 555, training loss: 7709.54, average training loss: 9330.54, base loss: 9210.16
[INFO 2017-06-27 17:13:02,999 main.py:51] epoch 556, training loss: 6432.02, average training loss: 9325.34, base loss: 9206.60
[INFO 2017-06-27 17:13:04,210 main.py:51] epoch 557, training loss: 6314.16, average training loss: 9319.94, base loss: 9203.52
[INFO 2017-06-27 17:13:05,424 main.py:51] epoch 558, training loss: 7355.00, average training loss: 9316.42, base loss: 9203.19
[INFO 2017-06-27 17:13:06,636 main.py:51] epoch 559, training loss: 7900.35, average training loss: 9313.90, base loss: 9203.40
[INFO 2017-06-27 17:13:07,847 main.py:51] epoch 560, training loss: 6879.83, average training loss: 9309.56, base loss: 9201.42
[INFO 2017-06-27 17:13:09,062 main.py:51] epoch 561, training loss: 8087.27, average training loss: 9307.38, base loss: 9202.87
[INFO 2017-06-27 17:13:10,274 main.py:51] epoch 562, training loss: 6880.26, average training loss: 9303.07, base loss: 9200.44
[INFO 2017-06-27 17:13:11,485 main.py:51] epoch 563, training loss: 7350.99, average training loss: 9299.61, base loss: 9199.52
[INFO 2017-06-27 17:13:12,697 main.py:51] epoch 564, training loss: 7796.79, average training loss: 9296.95, base loss: 9200.29
[INFO 2017-06-27 17:13:13,913 main.py:51] epoch 565, training loss: 7551.47, average training loss: 9293.87, base loss: 9200.08
[INFO 2017-06-27 17:13:15,124 main.py:51] epoch 566, training loss: 7343.25, average training loss: 9290.43, base loss: 9199.61
[INFO 2017-06-27 17:13:16,337 main.py:51] epoch 567, training loss: 7194.70, average training loss: 9286.74, base loss: 9198.41
[INFO 2017-06-27 17:13:17,551 main.py:51] epoch 568, training loss: 7887.36, average training loss: 9284.28, base loss: 9199.03
[INFO 2017-06-27 17:13:18,764 main.py:51] epoch 569, training loss: 6865.06, average training loss: 9280.03, base loss: 9197.02
[INFO 2017-06-27 17:13:19,979 main.py:51] epoch 570, training loss: 7205.76, average training loss: 9276.40, base loss: 9195.46
[INFO 2017-06-27 17:13:21,190 main.py:51] epoch 571, training loss: 7333.91, average training loss: 9273.00, base loss: 9195.08
[INFO 2017-06-27 17:13:22,402 main.py:51] epoch 572, training loss: 12007.91, average training loss: 9277.78, base loss: 9203.81
[INFO 2017-06-27 17:13:23,615 main.py:51] epoch 573, training loss: 7973.78, average training loss: 9275.50, base loss: 9204.29
[INFO 2017-06-27 17:13:24,829 main.py:51] epoch 574, training loss: 7094.13, average training loss: 9271.71, base loss: 9202.63
[INFO 2017-06-27 17:13:26,041 main.py:51] epoch 575, training loss: 7497.87, average training loss: 9268.63, base loss: 9202.06
[INFO 2017-06-27 17:13:27,255 main.py:51] epoch 576, training loss: 7444.27, average training loss: 9265.47, base loss: 9201.99
[INFO 2017-06-27 17:13:28,465 main.py:51] epoch 577, training loss: 6712.10, average training loss: 9261.05, base loss: 9199.44
[INFO 2017-06-27 17:13:29,680 main.py:51] epoch 578, training loss: 9996.52, average training loss: 9262.32, base loss: 9202.26
[INFO 2017-06-27 17:13:30,893 main.py:51] epoch 579, training loss: 7268.95, average training loss: 9258.89, base loss: 9201.64
[INFO 2017-06-27 17:13:32,107 main.py:51] epoch 580, training loss: 7211.75, average training loss: 9255.36, base loss: 9200.96
[INFO 2017-06-27 17:13:33,324 main.py:51] epoch 581, training loss: 8159.39, average training loss: 9253.48, base loss: 9201.94
[INFO 2017-06-27 17:13:34,536 main.py:51] epoch 582, training loss: 7140.07, average training loss: 9249.85, base loss: 9200.48
[INFO 2017-06-27 17:13:35,749 main.py:51] epoch 583, training loss: 7172.73, average training loss: 9246.30, base loss: 9199.23
[INFO 2017-06-27 17:13:36,962 main.py:51] epoch 584, training loss: 7914.43, average training loss: 9244.02, base loss: 9200.06
[INFO 2017-06-27 17:13:38,173 main.py:51] epoch 585, training loss: 8139.08, average training loss: 9242.13, base loss: 9200.96
[INFO 2017-06-27 17:13:39,382 main.py:51] epoch 586, training loss: 7280.28, average training loss: 9238.79, base loss: 9200.54
[INFO 2017-06-27 17:13:40,594 main.py:51] epoch 587, training loss: 7865.50, average training loss: 9236.46, base loss: 9201.40
[INFO 2017-06-27 17:13:41,807 main.py:51] epoch 588, training loss: 7402.45, average training loss: 9233.34, base loss: 9201.24
[INFO 2017-06-27 17:13:43,020 main.py:51] epoch 589, training loss: 6641.92, average training loss: 9228.95, base loss: 9199.20
[INFO 2017-06-27 17:13:44,232 main.py:51] epoch 590, training loss: 6923.48, average training loss: 9225.05, base loss: 9198.19
[INFO 2017-06-27 17:13:45,441 main.py:51] epoch 591, training loss: 6972.04, average training loss: 9221.24, base loss: 9197.08
[INFO 2017-06-27 17:13:46,655 main.py:51] epoch 592, training loss: 6900.83, average training loss: 9217.33, base loss: 9195.45
[INFO 2017-06-27 17:13:47,870 main.py:51] epoch 593, training loss: 10754.92, average training loss: 9219.92, base loss: 9200.17
[INFO 2017-06-27 17:13:49,083 main.py:51] epoch 594, training loss: 8002.30, average training loss: 9217.87, base loss: 9201.01
[INFO 2017-06-27 17:13:50,294 main.py:51] epoch 595, training loss: 6760.21, average training loss: 9213.75, base loss: 9198.54
[INFO 2017-06-27 17:13:51,506 main.py:51] epoch 596, training loss: 6443.83, average training loss: 9209.11, base loss: 9195.66
[INFO 2017-06-27 17:13:52,716 main.py:51] epoch 597, training loss: 7685.47, average training loss: 9206.56, base loss: 9196.01
[INFO 2017-06-27 17:13:53,928 main.py:51] epoch 598, training loss: 7436.43, average training loss: 9203.61, base loss: 9195.74
[INFO 2017-06-27 17:13:55,137 main.py:51] epoch 599, training loss: 7088.90, average training loss: 9200.08, base loss: 9194.99
[INFO 2017-06-27 17:13:55,137 main.py:53] epoch 599, testing
[INFO 2017-06-27 17:13:59,849 main.py:105] average testing loss: 7294.75, base loss: 8968.81
[INFO 2017-06-27 17:13:59,849 main.py:106] improve_loss: 1674.06, improve_percent: 0.19
[INFO 2017-06-27 17:13:59,850 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 17:13:59,863 main.py:76] current best improved percent: 0.19
[INFO 2017-06-27 17:14:01,073 main.py:51] epoch 600, training loss: 7699.39, average training loss: 9197.59, base loss: 9195.85
[INFO 2017-06-27 17:14:02,277 main.py:51] epoch 601, training loss: 7840.58, average training loss: 9195.33, base loss: 9196.64
[INFO 2017-06-27 17:14:03,490 main.py:51] epoch 602, training loss: 7472.52, average training loss: 9192.47, base loss: 9196.57
[INFO 2017-06-27 17:14:04,700 main.py:51] epoch 603, training loss: 6756.97, average training loss: 9188.44, base loss: 9194.27
[INFO 2017-06-27 17:14:05,914 main.py:51] epoch 604, training loss: 7910.58, average training loss: 9186.33, base loss: 9195.32
[INFO 2017-06-27 17:14:07,121 main.py:51] epoch 605, training loss: 7439.04, average training loss: 9183.45, base loss: 9195.54
[INFO 2017-06-27 17:14:08,332 main.py:51] epoch 606, training loss: 8218.00, average training loss: 9181.86, base loss: 9197.14
[INFO 2017-06-27 17:14:09,546 main.py:51] epoch 607, training loss: 8200.78, average training loss: 9180.24, base loss: 9199.57
[INFO 2017-06-27 17:14:10,758 main.py:51] epoch 608, training loss: 7166.83, average training loss: 9176.94, base loss: 9198.29
[INFO 2017-06-27 17:14:11,970 main.py:51] epoch 609, training loss: 7184.90, average training loss: 9173.67, base loss: 9197.62
[INFO 2017-06-27 17:14:13,180 main.py:51] epoch 610, training loss: 7501.03, average training loss: 9170.93, base loss: 9197.27
[INFO 2017-06-27 17:14:14,394 main.py:51] epoch 611, training loss: 7638.81, average training loss: 9168.43, base loss: 9197.85
[INFO 2017-06-27 17:14:15,604 main.py:51] epoch 612, training loss: 8040.82, average training loss: 9166.59, base loss: 9198.80
[INFO 2017-06-27 17:14:16,812 main.py:51] epoch 613, training loss: 7953.90, average training loss: 9164.61, base loss: 9199.76
[INFO 2017-06-27 17:14:18,024 main.py:51] epoch 614, training loss: 7823.91, average training loss: 9162.43, base loss: 9200.65
[INFO 2017-06-27 17:14:19,238 main.py:51] epoch 615, training loss: 7324.14, average training loss: 9159.45, base loss: 9200.35
[INFO 2017-06-27 17:14:20,450 main.py:51] epoch 616, training loss: 7173.48, average training loss: 9156.23, base loss: 9198.80
[INFO 2017-06-27 17:14:21,663 main.py:51] epoch 617, training loss: 7310.66, average training loss: 9153.25, base loss: 9198.30
[INFO 2017-06-27 17:14:22,877 main.py:51] epoch 618, training loss: 6696.12, average training loss: 9149.28, base loss: 9196.39
[INFO 2017-06-27 17:14:24,088 main.py:51] epoch 619, training loss: 7728.15, average training loss: 9146.98, base loss: 9196.91
[INFO 2017-06-27 17:14:25,296 main.py:51] epoch 620, training loss: 7717.13, average training loss: 9144.68, base loss: 9197.42
[INFO 2017-06-27 17:14:26,504 main.py:51] epoch 621, training loss: 6685.71, average training loss: 9140.73, base loss: 9195.30
[INFO 2017-06-27 17:14:27,715 main.py:51] epoch 622, training loss: 10740.78, average training loss: 9143.30, base loss: 9201.04
[INFO 2017-06-27 17:14:28,922 main.py:51] epoch 623, training loss: 7521.02, average training loss: 9140.70, base loss: 9201.31
[INFO 2017-06-27 17:14:30,132 main.py:51] epoch 624, training loss: 10952.33, average training loss: 9143.60, base loss: 9207.01
[INFO 2017-06-27 17:14:31,342 main.py:51] epoch 625, training loss: 7026.29, average training loss: 9140.21, base loss: 9205.77
[INFO 2017-06-27 17:14:32,556 main.py:51] epoch 626, training loss: 6953.76, average training loss: 9136.73, base loss: 9204.78
[INFO 2017-06-27 17:14:33,765 main.py:51] epoch 627, training loss: 7107.91, average training loss: 9133.50, base loss: 9203.67
[INFO 2017-06-27 17:14:34,978 main.py:51] epoch 628, training loss: 6462.07, average training loss: 9129.25, base loss: 9201.25
[INFO 2017-06-27 17:14:36,188 main.py:51] epoch 629, training loss: 7140.90, average training loss: 9126.09, base loss: 9199.75
[INFO 2017-06-27 17:14:37,402 main.py:51] epoch 630, training loss: 7377.57, average training loss: 9123.32, base loss: 9199.06
[INFO 2017-06-27 17:14:38,611 main.py:51] epoch 631, training loss: 7503.60, average training loss: 9120.76, base loss: 9199.34
[INFO 2017-06-27 17:14:39,824 main.py:51] epoch 632, training loss: 6721.32, average training loss: 9116.97, base loss: 9197.67
[INFO 2017-06-27 17:14:41,055 main.py:51] epoch 633, training loss: 6594.50, average training loss: 9112.99, base loss: 9194.86
[INFO 2017-06-27 17:14:42,266 main.py:51] epoch 634, training loss: 7696.38, average training loss: 9110.76, base loss: 9195.38
[INFO 2017-06-27 17:14:43,472 main.py:51] epoch 635, training loss: 6591.67, average training loss: 9106.80, base loss: 9193.73
[INFO 2017-06-27 17:14:44,686 main.py:51] epoch 636, training loss: 7966.45, average training loss: 9105.01, base loss: 9194.54
[INFO 2017-06-27 17:14:45,895 main.py:51] epoch 637, training loss: 10746.65, average training loss: 9107.58, base loss: 9199.07
[INFO 2017-06-27 17:14:47,105 main.py:51] epoch 638, training loss: 8380.13, average training loss: 9106.44, base loss: 9201.40
[INFO 2017-06-27 17:14:48,316 main.py:51] epoch 639, training loss: 10574.94, average training loss: 9108.74, base loss: 9205.92
[INFO 2017-06-27 17:14:49,525 main.py:51] epoch 640, training loss: 7766.02, average training loss: 9106.64, base loss: 9206.98
[INFO 2017-06-27 17:14:50,736 main.py:51] epoch 641, training loss: 8170.47, average training loss: 9105.18, base loss: 9209.35
[INFO 2017-06-27 17:14:51,951 main.py:51] epoch 642, training loss: 8555.32, average training loss: 9104.33, base loss: 9212.08
[INFO 2017-06-27 17:14:53,160 main.py:51] epoch 643, training loss: 7282.35, average training loss: 9101.50, base loss: 9211.70
[INFO 2017-06-27 17:14:54,372 main.py:51] epoch 644, training loss: 7420.69, average training loss: 9098.89, base loss: 9212.02
[INFO 2017-06-27 17:14:55,585 main.py:51] epoch 645, training loss: 10611.96, average training loss: 9101.24, base loss: 9217.09
[INFO 2017-06-27 17:14:56,792 main.py:51] epoch 646, training loss: 6549.17, average training loss: 9097.29, base loss: 9215.36
[INFO 2017-06-27 17:14:58,005 main.py:51] epoch 647, training loss: 8159.58, average training loss: 9095.84, base loss: 9216.85
[INFO 2017-06-27 17:14:59,214 main.py:51] epoch 648, training loss: 6854.47, average training loss: 9092.39, base loss: 9215.72
[INFO 2017-06-27 17:15:00,423 main.py:51] epoch 649, training loss: 6957.24, average training loss: 9089.11, base loss: 9214.75
[INFO 2017-06-27 17:15:01,633 main.py:51] epoch 650, training loss: 7469.77, average training loss: 9086.62, base loss: 9214.45
[INFO 2017-06-27 17:15:02,839 main.py:51] epoch 651, training loss: 7301.94, average training loss: 9083.88, base loss: 9214.44
[INFO 2017-06-27 17:15:04,050 main.py:51] epoch 652, training loss: 10956.63, average training loss: 9086.75, base loss: 9219.71
[INFO 2017-06-27 17:15:05,259 main.py:51] epoch 653, training loss: 6980.91, average training loss: 9083.53, base loss: 9219.04
[INFO 2017-06-27 17:15:06,471 main.py:51] epoch 654, training loss: 6934.00, average training loss: 9080.25, base loss: 9217.50
[INFO 2017-06-27 17:15:07,680 main.py:51] epoch 655, training loss: 7473.33, average training loss: 9077.80, base loss: 9217.76
[INFO 2017-06-27 17:15:08,890 main.py:51] epoch 656, training loss: 7339.34, average training loss: 9075.15, base loss: 9217.54
[INFO 2017-06-27 17:15:10,098 main.py:51] epoch 657, training loss: 6472.28, average training loss: 9071.20, base loss: 9215.85
[INFO 2017-06-27 17:15:11,305 main.py:51] epoch 658, training loss: 7071.92, average training loss: 9068.16, base loss: 9215.41
[INFO 2017-06-27 17:15:12,515 main.py:51] epoch 659, training loss: 6675.07, average training loss: 9064.54, base loss: 9213.78
[INFO 2017-06-27 17:15:13,727 main.py:51] epoch 660, training loss: 7213.24, average training loss: 9061.74, base loss: 9213.08
[INFO 2017-06-27 17:15:14,937 main.py:51] epoch 661, training loss: 7059.81, average training loss: 9058.71, base loss: 9212.42
[INFO 2017-06-27 17:15:16,149 main.py:51] epoch 662, training loss: 7093.04, average training loss: 9055.75, base loss: 9211.65
[INFO 2017-06-27 17:15:17,403 main.py:51] epoch 663, training loss: 8189.79, average training loss: 9054.44, base loss: 9212.92
[INFO 2017-06-27 17:15:18,614 main.py:51] epoch 664, training loss: 7017.19, average training loss: 9051.38, base loss: 9212.07
[INFO 2017-06-27 17:15:19,818 main.py:51] epoch 665, training loss: 6698.25, average training loss: 9047.85, base loss: 9210.76
[INFO 2017-06-27 17:15:21,031 main.py:51] epoch 666, training loss: 7277.48, average training loss: 9045.19, base loss: 9210.59
[INFO 2017-06-27 17:15:22,242 main.py:51] epoch 667, training loss: 7674.20, average training loss: 9043.14, base loss: 9210.72
[INFO 2017-06-27 17:15:23,454 main.py:51] epoch 668, training loss: 7397.69, average training loss: 9040.68, base loss: 9210.73
[INFO 2017-06-27 17:15:24,661 main.py:51] epoch 669, training loss: 6955.34, average training loss: 9037.57, base loss: 9210.01
[INFO 2017-06-27 17:15:25,869 main.py:51] epoch 670, training loss: 10516.26, average training loss: 9039.77, base loss: 9214.85
[INFO 2017-06-27 17:15:27,082 main.py:51] epoch 671, training loss: 7014.81, average training loss: 9036.76, base loss: 9214.24
[INFO 2017-06-27 17:15:28,295 main.py:51] epoch 672, training loss: 6866.30, average training loss: 9033.53, base loss: 9213.19
[INFO 2017-06-27 17:15:29,508 main.py:51] epoch 673, training loss: 7438.03, average training loss: 9031.16, base loss: 9213.32
[INFO 2017-06-27 17:15:30,722 main.py:51] epoch 674, training loss: 7561.62, average training loss: 9028.99, base loss: 9213.61
[INFO 2017-06-27 17:15:31,937 main.py:51] epoch 675, training loss: 6999.42, average training loss: 9025.99, base loss: 9212.12
[INFO 2017-06-27 17:15:33,147 main.py:51] epoch 676, training loss: 7458.83, average training loss: 9023.67, base loss: 9211.77
[INFO 2017-06-27 17:15:34,358 main.py:51] epoch 677, training loss: 6404.29, average training loss: 9019.81, base loss: 9208.78
[INFO 2017-06-27 17:15:35,566 main.py:51] epoch 678, training loss: 7385.65, average training loss: 9017.40, base loss: 9208.60
[INFO 2017-06-27 17:15:36,777 main.py:51] epoch 679, training loss: 7844.17, average training loss: 9015.68, base loss: 9209.36
[INFO 2017-06-27 17:15:37,989 main.py:51] epoch 680, training loss: 6672.41, average training loss: 9012.23, base loss: 9207.25
[INFO 2017-06-27 17:15:39,200 main.py:51] epoch 681, training loss: 7075.57, average training loss: 9009.39, base loss: 9206.99
[INFO 2017-06-27 17:15:40,409 main.py:51] epoch 682, training loss: 7222.18, average training loss: 9006.78, base loss: 9207.05
[INFO 2017-06-27 17:15:41,618 main.py:51] epoch 683, training loss: 10708.78, average training loss: 9009.27, base loss: 9211.83
[INFO 2017-06-27 17:15:42,829 main.py:51] epoch 684, training loss: 6921.22, average training loss: 9006.22, base loss: 9210.74
[INFO 2017-06-27 17:15:44,043 main.py:51] epoch 685, training loss: 6956.19, average training loss: 9003.23, base loss: 9209.60
[INFO 2017-06-27 17:15:45,257 main.py:51] epoch 686, training loss: 7574.75, average training loss: 9001.15, base loss: 9210.67
[INFO 2017-06-27 17:15:46,469 main.py:51] epoch 687, training loss: 8228.26, average training loss: 9000.03, base loss: 9212.81
[INFO 2017-06-27 17:15:47,680 main.py:51] epoch 688, training loss: 6708.39, average training loss: 8996.70, base loss: 9210.64
[INFO 2017-06-27 17:15:48,892 main.py:51] epoch 689, training loss: 10505.10, average training loss: 8998.89, base loss: 9214.86
[INFO 2017-06-27 17:15:50,106 main.py:51] epoch 690, training loss: 7700.44, average training loss: 8997.01, base loss: 9215.55
[INFO 2017-06-27 17:15:51,319 main.py:51] epoch 691, training loss: 7094.23, average training loss: 8994.26, base loss: 9215.27
[INFO 2017-06-27 17:15:52,531 main.py:51] epoch 692, training loss: 6806.33, average training loss: 8991.10, base loss: 9214.44
[INFO 2017-06-27 17:15:53,740 main.py:51] epoch 693, training loss: 7473.52, average training loss: 8988.91, base loss: 9215.44
[INFO 2017-06-27 17:15:54,953 main.py:51] epoch 694, training loss: 6765.82, average training loss: 8985.72, base loss: 9213.91
[INFO 2017-06-27 17:15:56,165 main.py:51] epoch 695, training loss: 6882.78, average training loss: 8982.69, base loss: 9213.02
[INFO 2017-06-27 17:15:57,376 main.py:51] epoch 696, training loss: 6562.15, average training loss: 8979.22, base loss: 9211.36
[INFO 2017-06-27 17:15:58,590 main.py:51] epoch 697, training loss: 7440.66, average training loss: 8977.02, base loss: 9211.81
[INFO 2017-06-27 17:15:59,802 main.py:51] epoch 698, training loss: 6590.45, average training loss: 8973.60, base loss: 9210.13
[INFO 2017-06-27 17:16:01,013 main.py:51] epoch 699, training loss: 7038.25, average training loss: 8970.84, base loss: 9209.54
[INFO 2017-06-27 17:16:01,014 main.py:53] epoch 699, testing
[INFO 2017-06-27 17:16:05,720 main.py:105] average testing loss: 6975.97, base loss: 8589.18
[INFO 2017-06-27 17:16:05,720 main.py:106] improve_loss: 1613.21, improve_percent: 0.19
[INFO 2017-06-27 17:16:05,721 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 17:16:05,734 main.py:76] current best improved percent: 0.19
[INFO 2017-06-27 17:16:06,942 main.py:51] epoch 700, training loss: 10774.18, average training loss: 8973.41, base loss: 9213.90
[INFO 2017-06-27 17:16:08,155 main.py:51] epoch 701, training loss: 6992.70, average training loss: 8970.59, base loss: 9214.18
[INFO 2017-06-27 17:16:09,371 main.py:51] epoch 702, training loss: 7515.87, average training loss: 8968.52, base loss: 9215.11
[INFO 2017-06-27 17:16:10,577 main.py:51] epoch 703, training loss: 7486.04, average training loss: 8966.41, base loss: 9215.78
[INFO 2017-06-27 17:16:11,791 main.py:51] epoch 704, training loss: 6935.13, average training loss: 8963.53, base loss: 9214.56
[INFO 2017-06-27 17:16:13,000 main.py:51] epoch 705, training loss: 7767.99, average training loss: 8961.84, base loss: 9215.44
[INFO 2017-06-27 17:16:14,209 main.py:51] epoch 706, training loss: 7436.33, average training loss: 8959.68, base loss: 9215.09
[INFO 2017-06-27 17:16:15,416 main.py:51] epoch 707, training loss: 6939.61, average training loss: 8956.83, base loss: 9214.46
[INFO 2017-06-27 17:16:16,628 main.py:51] epoch 708, training loss: 7608.60, average training loss: 8954.93, base loss: 9215.20
[INFO 2017-06-27 17:16:17,841 main.py:51] epoch 709, training loss: 10216.13, average training loss: 8956.70, base loss: 9219.18
[INFO 2017-06-27 17:16:19,054 main.py:51] epoch 710, training loss: 6785.63, average training loss: 8953.65, base loss: 9217.94
[INFO 2017-06-27 17:16:20,265 main.py:51] epoch 711, training loss: 7055.94, average training loss: 8950.98, base loss: 9217.06
[INFO 2017-06-27 17:16:21,476 main.py:51] epoch 712, training loss: 6662.48, average training loss: 8947.77, base loss: 9215.74
[INFO 2017-06-27 17:16:22,689 main.py:51] epoch 713, training loss: 6825.48, average training loss: 8944.80, base loss: 9214.71
[INFO 2017-06-27 17:16:23,895 main.py:51] epoch 714, training loss: 6310.63, average training loss: 8941.12, base loss: 9212.86
[INFO 2017-06-27 17:16:25,107 main.py:51] epoch 715, training loss: 10836.15, average training loss: 8943.76, base loss: 9217.15
[INFO 2017-06-27 17:16:26,319 main.py:51] epoch 716, training loss: 7297.15, average training loss: 8941.47, base loss: 9216.92
[INFO 2017-06-27 17:16:27,534 main.py:51] epoch 717, training loss: 7271.82, average training loss: 8939.14, base loss: 9216.71
[INFO 2017-06-27 17:16:28,745 main.py:51] epoch 718, training loss: 7067.49, average training loss: 8936.54, base loss: 9215.86
[INFO 2017-06-27 17:16:29,958 main.py:51] epoch 719, training loss: 6528.53, average training loss: 8933.19, base loss: 9213.87
[INFO 2017-06-27 17:16:31,170 main.py:51] epoch 720, training loss: 7872.05, average training loss: 8931.72, base loss: 9215.66
[INFO 2017-06-27 17:16:32,384 main.py:51] epoch 721, training loss: 6700.87, average training loss: 8928.63, base loss: 9214.50
[INFO 2017-06-27 17:16:33,599 main.py:51] epoch 722, training loss: 6778.66, average training loss: 8925.66, base loss: 9213.36
[INFO 2017-06-27 17:16:34,811 main.py:51] epoch 723, training loss: 6183.90, average training loss: 8921.87, base loss: 9210.54
[INFO 2017-06-27 17:16:36,021 main.py:51] epoch 724, training loss: 6838.29, average training loss: 8919.00, base loss: 9209.22
[INFO 2017-06-27 17:16:37,235 main.py:51] epoch 725, training loss: 6784.00, average training loss: 8916.06, base loss: 9207.90
[INFO 2017-06-27 17:16:38,444 main.py:51] epoch 726, training loss: 10657.27, average training loss: 8918.45, base loss: 9211.93
[INFO 2017-06-27 17:16:39,658 main.py:51] epoch 727, training loss: 6344.35, average training loss: 8914.92, base loss: 9209.43
[INFO 2017-06-27 17:16:40,873 main.py:51] epoch 728, training loss: 7733.50, average training loss: 8913.30, base loss: 9210.67
[INFO 2017-06-27 17:16:42,084 main.py:51] epoch 729, training loss: 7253.36, average training loss: 8911.02, base loss: 9210.01
[INFO 2017-06-27 17:16:43,295 main.py:51] epoch 730, training loss: 8215.81, average training loss: 8910.07, base loss: 9212.37
[INFO 2017-06-27 17:16:44,512 main.py:51] epoch 731, training loss: 7088.38, average training loss: 8907.58, base loss: 9211.31
[INFO 2017-06-27 17:16:45,727 main.py:51] epoch 732, training loss: 7564.37, average training loss: 8905.75, base loss: 9211.82
[INFO 2017-06-27 17:16:46,939 main.py:51] epoch 733, training loss: 6558.73, average training loss: 8902.55, base loss: 9210.08
[INFO 2017-06-27 17:16:48,151 main.py:51] epoch 734, training loss: 6792.42, average training loss: 8899.68, base loss: 9208.95
[INFO 2017-06-27 17:16:49,364 main.py:51] epoch 735, training loss: 7871.51, average training loss: 8898.29, base loss: 9209.88
[INFO 2017-06-27 17:16:50,577 main.py:51] epoch 736, training loss: 7894.43, average training loss: 8896.92, base loss: 9210.48
[INFO 2017-06-27 17:16:51,788 main.py:51] epoch 737, training loss: 7239.78, average training loss: 8894.68, base loss: 9210.14
[INFO 2017-06-27 17:16:53,003 main.py:51] epoch 738, training loss: 6906.92, average training loss: 8891.99, base loss: 9209.13
[INFO 2017-06-27 17:16:54,212 main.py:51] epoch 739, training loss: 7959.41, average training loss: 8890.73, base loss: 9210.36
[INFO 2017-06-27 17:16:55,426 main.py:51] epoch 740, training loss: 7495.08, average training loss: 8888.84, base loss: 9210.65
[INFO 2017-06-27 17:16:56,637 main.py:51] epoch 741, training loss: 7592.30, average training loss: 8887.10, base loss: 9211.05
[INFO 2017-06-27 17:16:57,849 main.py:51] epoch 742, training loss: 6524.30, average training loss: 8883.92, base loss: 9209.22
[INFO 2017-06-27 17:16:59,058 main.py:51] epoch 743, training loss: 6904.23, average training loss: 8881.26, base loss: 9208.31
[INFO 2017-06-27 17:17:00,270 main.py:51] epoch 744, training loss: 8050.07, average training loss: 8880.14, base loss: 9209.23
[INFO 2017-06-27 17:17:01,485 main.py:51] epoch 745, training loss: 7135.48, average training loss: 8877.80, base loss: 9208.86
[INFO 2017-06-27 17:17:02,699 main.py:51] epoch 746, training loss: 7309.48, average training loss: 8875.70, base loss: 9208.61
[INFO 2017-06-27 17:17:03,904 main.py:51] epoch 747, training loss: 7256.78, average training loss: 8873.54, base loss: 9208.41
[INFO 2017-06-27 17:17:05,114 main.py:51] epoch 748, training loss: 6688.43, average training loss: 8870.62, base loss: 9206.35
[INFO 2017-06-27 17:17:06,330 main.py:51] epoch 749, training loss: 6907.27, average training loss: 8868.00, base loss: 9205.65
[INFO 2017-06-27 17:17:07,544 main.py:51] epoch 750, training loss: 7733.43, average training loss: 8866.49, base loss: 9206.13
[INFO 2017-06-27 17:17:08,756 main.py:51] epoch 751, training loss: 7680.95, average training loss: 8864.92, base loss: 9206.84
[INFO 2017-06-27 17:17:09,970 main.py:51] epoch 752, training loss: 7086.45, average training loss: 8862.55, base loss: 9206.35
[INFO 2017-06-27 17:17:11,182 main.py:51] epoch 753, training loss: 7418.92, average training loss: 8860.64, base loss: 9206.12
[INFO 2017-06-27 17:17:12,394 main.py:51] epoch 754, training loss: 7377.23, average training loss: 8858.67, base loss: 9206.09
[INFO 2017-06-27 17:17:13,605 main.py:51] epoch 755, training loss: 6891.91, average training loss: 8856.07, base loss: 9204.93
[INFO 2017-06-27 17:17:14,816 main.py:51] epoch 756, training loss: 8005.32, average training loss: 8854.95, base loss: 9206.09
[INFO 2017-06-27 17:17:16,026 main.py:51] epoch 757, training loss: 7631.41, average training loss: 8853.33, base loss: 9206.25
[INFO 2017-06-27 17:17:17,238 main.py:51] epoch 758, training loss: 7565.88, average training loss: 8851.64, base loss: 9207.19
[INFO 2017-06-27 17:17:18,452 main.py:51] epoch 759, training loss: 7023.86, average training loss: 8849.23, base loss: 9206.11
[INFO 2017-06-27 17:17:19,660 main.py:51] epoch 760, training loss: 6857.15, average training loss: 8846.62, base loss: 9205.27
[INFO 2017-06-27 17:17:20,872 main.py:51] epoch 761, training loss: 7342.79, average training loss: 8844.64, base loss: 9205.06
[INFO 2017-06-27 17:17:22,082 main.py:51] epoch 762, training loss: 7514.47, average training loss: 8842.90, base loss: 9205.12
[INFO 2017-06-27 17:17:23,294 main.py:51] epoch 763, training loss: 5777.55, average training loss: 8838.89, base loss: 9202.02
[INFO 2017-06-27 17:17:24,505 main.py:51] epoch 764, training loss: 7735.56, average training loss: 8837.44, base loss: 9202.82
[INFO 2017-06-27 17:17:25,713 main.py:51] epoch 765, training loss: 6471.89, average training loss: 8834.36, base loss: 9201.20
[INFO 2017-06-27 17:17:26,923 main.py:51] epoch 766, training loss: 7159.84, average training loss: 8832.17, base loss: 9200.73
[INFO 2017-06-27 17:17:28,136 main.py:51] epoch 767, training loss: 7179.93, average training loss: 8830.02, base loss: 9200.10
[INFO 2017-06-27 17:17:29,345 main.py:51] epoch 768, training loss: 7706.18, average training loss: 8828.56, base loss: 9201.28
[INFO 2017-06-27 17:17:30,559 main.py:51] epoch 769, training loss: 7417.49, average training loss: 8826.73, base loss: 9201.21
[INFO 2017-06-27 17:17:31,770 main.py:51] epoch 770, training loss: 6826.17, average training loss: 8824.13, base loss: 9200.32
[INFO 2017-06-27 17:17:32,980 main.py:51] epoch 771, training loss: 7317.36, average training loss: 8822.18, base loss: 9200.82
[INFO 2017-06-27 17:17:34,186 main.py:51] epoch 772, training loss: 7800.38, average training loss: 8820.86, base loss: 9201.50
[INFO 2017-06-27 17:17:35,396 main.py:51] epoch 773, training loss: 11421.22, average training loss: 8824.22, base loss: 9207.52
[INFO 2017-06-27 17:17:36,605 main.py:51] epoch 774, training loss: 7494.78, average training loss: 8822.50, base loss: 9208.61
[INFO 2017-06-27 17:17:37,814 main.py:51] epoch 775, training loss: 7207.89, average training loss: 8820.42, base loss: 9208.27
[INFO 2017-06-27 17:17:39,023 main.py:51] epoch 776, training loss: 7371.48, average training loss: 8818.56, base loss: 9208.14
[INFO 2017-06-27 17:17:40,235 main.py:51] epoch 777, training loss: 6934.49, average training loss: 8816.14, base loss: 9207.48
[INFO 2017-06-27 17:17:41,448 main.py:51] epoch 778, training loss: 6964.72, average training loss: 8813.76, base loss: 9206.85
[INFO 2017-06-27 17:17:42,661 main.py:51] epoch 779, training loss: 6449.46, average training loss: 8810.73, base loss: 9205.07
[INFO 2017-06-27 17:17:43,873 main.py:51] epoch 780, training loss: 7297.22, average training loss: 8808.79, base loss: 9204.80
[INFO 2017-06-27 17:17:45,084 main.py:51] epoch 781, training loss: 10378.10, average training loss: 8810.80, base loss: 9208.44
[INFO 2017-06-27 17:17:46,296 main.py:51] epoch 782, training loss: 7295.31, average training loss: 8808.86, base loss: 9208.20
[INFO 2017-06-27 17:17:47,508 main.py:51] epoch 783, training loss: 7174.18, average training loss: 8806.78, base loss: 9208.63
[INFO 2017-06-27 17:17:48,720 main.py:51] epoch 784, training loss: 7354.34, average training loss: 8804.93, base loss: 9209.03
[INFO 2017-06-27 17:17:49,931 main.py:51] epoch 785, training loss: 7054.29, average training loss: 8802.70, base loss: 9208.13
[INFO 2017-06-27 17:17:51,145 main.py:51] epoch 786, training loss: 7088.88, average training loss: 8800.52, base loss: 9208.15
[INFO 2017-06-27 17:17:52,355 main.py:51] epoch 787, training loss: 6877.40, average training loss: 8798.08, base loss: 9207.42
[INFO 2017-06-27 17:17:53,563 main.py:51] epoch 788, training loss: 6994.46, average training loss: 8795.79, base loss: 9206.82
[INFO 2017-06-27 17:17:54,768 main.py:51] epoch 789, training loss: 7881.17, average training loss: 8794.64, base loss: 9207.56
[INFO 2017-06-27 17:17:55,976 main.py:51] epoch 790, training loss: 6892.85, average training loss: 8792.23, base loss: 9206.64
[INFO 2017-06-27 17:17:57,186 main.py:51] epoch 791, training loss: 6717.92, average training loss: 8789.61, base loss: 9205.39
[INFO 2017-06-27 17:17:58,397 main.py:51] epoch 792, training loss: 7053.53, average training loss: 8787.42, base loss: 9204.58
[INFO 2017-06-27 17:17:59,604 main.py:51] epoch 793, training loss: 7198.11, average training loss: 8785.42, base loss: 9204.05
[INFO 2017-06-27 17:18:00,813 main.py:51] epoch 794, training loss: 10690.92, average training loss: 8787.82, base loss: 9208.09
[INFO 2017-06-27 17:18:02,026 main.py:51] epoch 795, training loss: 6884.94, average training loss: 8785.43, base loss: 9207.41
[INFO 2017-06-27 17:18:03,239 main.py:51] epoch 796, training loss: 7480.85, average training loss: 8783.79, base loss: 9207.74
[INFO 2017-06-27 17:18:04,448 main.py:51] epoch 797, training loss: 7343.71, average training loss: 8781.99, base loss: 9207.64
[INFO 2017-06-27 17:18:05,661 main.py:51] epoch 798, training loss: 6817.11, average training loss: 8779.53, base loss: 9206.65
[INFO 2017-06-27 17:18:06,869 main.py:51] epoch 799, training loss: 7396.60, average training loss: 8777.80, base loss: 9207.02
[INFO 2017-06-27 17:18:06,869 main.py:53] epoch 799, testing
[INFO 2017-06-27 17:18:11,553 main.py:105] average testing loss: 7031.12, base loss: 8779.47
[INFO 2017-06-27 17:18:11,553 main.py:106] improve_loss: 1748.35, improve_percent: 0.20
[INFO 2017-06-27 17:18:11,554 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 17:18:11,567 main.py:76] current best improved percent: 0.20
[INFO 2017-06-27 17:18:12,777 main.py:51] epoch 800, training loss: 7839.29, average training loss: 8776.63, base loss: 9208.39
[INFO 2017-06-27 17:18:13,987 main.py:51] epoch 801, training loss: 6741.35, average training loss: 8774.09, base loss: 9206.97
[INFO 2017-06-27 17:18:15,197 main.py:51] epoch 802, training loss: 6750.10, average training loss: 8771.57, base loss: 9205.74
[INFO 2017-06-27 17:18:16,408 main.py:51] epoch 803, training loss: 7190.09, average training loss: 8769.60, base loss: 9205.84
[INFO 2017-06-27 17:18:17,614 main.py:51] epoch 804, training loss: 7710.34, average training loss: 8768.29, base loss: 9206.78
[INFO 2017-06-27 17:18:18,825 main.py:51] epoch 805, training loss: 7100.12, average training loss: 8766.22, base loss: 9206.69
[INFO 2017-06-27 17:18:20,037 main.py:51] epoch 806, training loss: 6919.61, average training loss: 8763.93, base loss: 9205.85
[INFO 2017-06-27 17:18:21,249 main.py:51] epoch 807, training loss: 6866.69, average training loss: 8761.58, base loss: 9205.96
[INFO 2017-06-27 17:18:22,457 main.py:51] epoch 808, training loss: 6923.15, average training loss: 8759.31, base loss: 9205.15
[INFO 2017-06-27 17:18:23,664 main.py:51] epoch 809, training loss: 7394.29, average training loss: 8757.62, base loss: 9204.74
[INFO 2017-06-27 17:18:24,875 main.py:51] epoch 810, training loss: 7311.05, average training loss: 8755.84, base loss: 9204.39
[INFO 2017-06-27 17:18:26,083 main.py:51] epoch 811, training loss: 7151.79, average training loss: 8753.86, base loss: 9204.40
[INFO 2017-06-27 17:18:27,301 main.py:51] epoch 812, training loss: 6875.34, average training loss: 8751.55, base loss: 9203.45
[INFO 2017-06-27 17:18:28,512 main.py:51] epoch 813, training loss: 7734.35, average training loss: 8750.30, base loss: 9204.44
[INFO 2017-06-27 17:18:29,723 main.py:51] epoch 814, training loss: 7312.89, average training loss: 8748.54, base loss: 9205.02
[INFO 2017-06-27 17:18:30,928 main.py:51] epoch 815, training loss: 6710.28, average training loss: 8746.04, base loss: 9203.52
[INFO 2017-06-27 17:18:32,135 main.py:51] epoch 816, training loss: 6793.97, average training loss: 8743.65, base loss: 9202.73
[INFO 2017-06-27 17:18:33,345 main.py:51] epoch 817, training loss: 6309.38, average training loss: 8740.68, base loss: 9200.80
[INFO 2017-06-27 17:18:34,554 main.py:51] epoch 818, training loss: 6489.51, average training loss: 8737.93, base loss: 9199.25
[INFO 2017-06-27 17:18:35,761 main.py:51] epoch 819, training loss: 8345.08, average training loss: 8737.45, base loss: 9201.26
[INFO 2017-06-27 17:18:36,968 main.py:51] epoch 820, training loss: 6768.86, average training loss: 8735.05, base loss: 9200.12
[INFO 2017-06-27 17:18:38,178 main.py:51] epoch 821, training loss: 6444.54, average training loss: 8732.27, base loss: 9198.81
[INFO 2017-06-27 17:18:39,387 main.py:51] epoch 822, training loss: 7000.60, average training loss: 8730.16, base loss: 9198.29
[INFO 2017-06-27 17:18:40,595 main.py:51] epoch 823, training loss: 7315.84, average training loss: 8728.44, base loss: 9198.78
[INFO 2017-06-27 17:18:41,809 main.py:51] epoch 824, training loss: 6765.00, average training loss: 8726.06, base loss: 9197.58
[INFO 2017-06-27 17:18:43,019 main.py:51] epoch 825, training loss: 6923.07, average training loss: 8723.88, base loss: 9197.18
[INFO 2017-06-27 17:18:44,228 main.py:51] epoch 826, training loss: 6625.28, average training loss: 8721.34, base loss: 9195.68
[INFO 2017-06-27 17:18:45,439 main.py:51] epoch 827, training loss: 6913.94, average training loss: 8719.16, base loss: 9195.35
[INFO 2017-06-27 17:18:46,648 main.py:51] epoch 828, training loss: 6559.49, average training loss: 8716.56, base loss: 9194.41
[INFO 2017-06-27 17:18:47,860 main.py:51] epoch 829, training loss: 6766.52, average training loss: 8714.21, base loss: 9193.55
[INFO 2017-06-27 17:18:49,070 main.py:51] epoch 830, training loss: 6498.52, average training loss: 8711.54, base loss: 9191.98
[INFO 2017-06-27 17:18:50,284 main.py:51] epoch 831, training loss: 7573.89, average training loss: 8710.17, base loss: 9192.57
[INFO 2017-06-27 17:18:51,492 main.py:51] epoch 832, training loss: 6881.52, average training loss: 8707.98, base loss: 9191.79
[INFO 2017-06-27 17:18:52,702 main.py:51] epoch 833, training loss: 10625.16, average training loss: 8710.28, base loss: 9196.00
[INFO 2017-06-27 17:18:53,916 main.py:51] epoch 834, training loss: 7038.25, average training loss: 8708.27, base loss: 9195.38
[INFO 2017-06-27 17:18:55,127 main.py:51] epoch 835, training loss: 6484.13, average training loss: 8705.61, base loss: 9193.63
[INFO 2017-06-27 17:18:56,344 main.py:51] epoch 836, training loss: 6445.76, average training loss: 8702.91, base loss: 9192.08
[INFO 2017-06-27 17:18:57,560 main.py:51] epoch 837, training loss: 6477.44, average training loss: 8700.26, base loss: 9191.09
[INFO 2017-06-27 17:18:58,772 main.py:51] epoch 838, training loss: 7025.72, average training loss: 8698.26, base loss: 9190.81
[INFO 2017-06-27 17:18:59,983 main.py:51] epoch 839, training loss: 6957.51, average training loss: 8696.19, base loss: 9190.44
[INFO 2017-06-27 17:19:01,199 main.py:51] epoch 840, training loss: 6698.89, average training loss: 8693.82, base loss: 9189.44
[INFO 2017-06-27 17:19:02,427 main.py:51] epoch 841, training loss: 7778.52, average training loss: 8692.73, base loss: 9190.80
[INFO 2017-06-27 17:19:03,642 main.py:51] epoch 842, training loss: 10466.38, average training loss: 8694.83, base loss: 9193.96
[INFO 2017-06-27 17:19:04,853 main.py:51] epoch 843, training loss: 10285.14, average training loss: 8696.72, base loss: 9197.41
[INFO 2017-06-27 17:19:06,068 main.py:51] epoch 844, training loss: 7323.01, average training loss: 8695.09, base loss: 9197.78
[INFO 2017-06-27 17:19:07,278 main.py:51] epoch 845, training loss: 6703.20, average training loss: 8692.74, base loss: 9196.92
[INFO 2017-06-27 17:19:08,489 main.py:51] epoch 846, training loss: 7453.11, average training loss: 8691.27, base loss: 9197.74
[INFO 2017-06-27 17:19:09,704 main.py:51] epoch 847, training loss: 7201.66, average training loss: 8689.52, base loss: 9197.98
[INFO 2017-06-27 17:19:10,911 main.py:51] epoch 848, training loss: 6489.36, average training loss: 8686.92, base loss: 9196.62
[INFO 2017-06-27 17:19:12,121 main.py:51] epoch 849, training loss: 7754.61, average training loss: 8685.83, base loss: 9198.03
[INFO 2017-06-27 17:19:13,336 main.py:51] epoch 850, training loss: 10023.62, average training loss: 8687.40, base loss: 9200.91
[INFO 2017-06-27 17:19:14,543 main.py:51] epoch 851, training loss: 6546.00, average training loss: 8684.89, base loss: 9200.09
[INFO 2017-06-27 17:19:15,757 main.py:51] epoch 852, training loss: 7101.31, average training loss: 8683.03, base loss: 9199.72
[INFO 2017-06-27 17:19:16,968 main.py:51] epoch 853, training loss: 6741.23, average training loss: 8680.76, base loss: 9198.78
[INFO 2017-06-27 17:19:18,182 main.py:51] epoch 854, training loss: 6630.15, average training loss: 8678.36, base loss: 9197.61
[INFO 2017-06-27 17:19:19,393 main.py:51] epoch 855, training loss: 6971.08, average training loss: 8676.36, base loss: 9197.34
[INFO 2017-06-27 17:19:20,602 main.py:51] epoch 856, training loss: 7141.62, average training loss: 8674.57, base loss: 9197.68
[INFO 2017-06-27 17:19:21,812 main.py:51] epoch 857, training loss: 7373.46, average training loss: 8673.06, base loss: 9198.48
[INFO 2017-06-27 17:19:23,023 main.py:51] epoch 858, training loss: 10772.78, average training loss: 8675.50, base loss: 9202.56
[INFO 2017-06-27 17:19:24,237 main.py:51] epoch 859, training loss: 7123.23, average training loss: 8673.70, base loss: 9202.35
[INFO 2017-06-27 17:19:25,450 main.py:51] epoch 860, training loss: 7358.25, average training loss: 8672.17, base loss: 9202.64
[INFO 2017-06-27 17:19:26,659 main.py:51] epoch 861, training loss: 7183.87, average training loss: 8670.44, base loss: 9203.01
[INFO 2017-06-27 17:19:27,870 main.py:51] epoch 862, training loss: 7321.26, average training loss: 8668.88, base loss: 9203.03
[INFO 2017-06-27 17:19:29,077 main.py:51] epoch 863, training loss: 6621.55, average training loss: 8666.51, base loss: 9202.22
[INFO 2017-06-27 17:19:30,290 main.py:51] epoch 864, training loss: 6989.62, average training loss: 8664.57, base loss: 9202.07
[INFO 2017-06-27 17:19:31,501 main.py:51] epoch 865, training loss: 7031.04, average training loss: 8662.68, base loss: 9201.70
[INFO 2017-06-27 17:19:32,712 main.py:51] epoch 866, training loss: 10719.81, average training loss: 8665.06, base loss: 9206.41
[INFO 2017-06-27 17:19:33,926 main.py:51] epoch 867, training loss: 6488.51, average training loss: 8662.55, base loss: 9205.22
[INFO 2017-06-27 17:19:35,136 main.py:51] epoch 868, training loss: 7364.61, average training loss: 8661.05, base loss: 9205.51
[INFO 2017-06-27 17:19:36,350 main.py:51] epoch 869, training loss: 7255.63, average training loss: 8659.44, base loss: 9205.00
[INFO 2017-06-27 17:19:37,563 main.py:51] epoch 870, training loss: 6616.91, average training loss: 8657.09, base loss: 9203.90
[INFO 2017-06-27 17:19:38,774 main.py:51] epoch 871, training loss: 7294.95, average training loss: 8655.53, base loss: 9203.77
[INFO 2017-06-27 17:19:39,983 main.py:51] epoch 872, training loss: 7637.82, average training loss: 8654.37, base loss: 9204.49
[INFO 2017-06-27 17:19:41,192 main.py:51] epoch 873, training loss: 6828.76, average training loss: 8652.28, base loss: 9203.72
[INFO 2017-06-27 17:19:42,404 main.py:51] epoch 874, training loss: 7809.98, average training loss: 8651.31, base loss: 9205.36
[INFO 2017-06-27 17:19:43,614 main.py:51] epoch 875, training loss: 7055.75, average training loss: 8649.49, base loss: 9205.37
[INFO 2017-06-27 17:19:44,827 main.py:51] epoch 876, training loss: 7439.55, average training loss: 8648.11, base loss: 9205.58
[INFO 2017-06-27 17:19:46,037 main.py:51] epoch 877, training loss: 7382.30, average training loss: 8646.67, base loss: 9206.35
[INFO 2017-06-27 17:19:47,252 main.py:51] epoch 878, training loss: 11060.46, average training loss: 8649.42, base loss: 9210.64
[INFO 2017-06-27 17:19:48,465 main.py:51] epoch 879, training loss: 6869.03, average training loss: 8647.39, base loss: 9210.41
[INFO 2017-06-27 17:19:49,676 main.py:51] epoch 880, training loss: 10544.92, average training loss: 8649.55, base loss: 9214.08
[INFO 2017-06-27 17:19:50,884 main.py:51] epoch 881, training loss: 6930.86, average training loss: 8647.60, base loss: 9214.16
[INFO 2017-06-27 17:19:52,092 main.py:51] epoch 882, training loss: 10353.50, average training loss: 8649.53, base loss: 9217.88
[INFO 2017-06-27 17:19:53,301 main.py:51] epoch 883, training loss: 6751.35, average training loss: 8647.38, base loss: 9217.29
[INFO 2017-06-27 17:19:54,514 main.py:51] epoch 884, training loss: 7340.38, average training loss: 8645.91, base loss: 9217.71
[INFO 2017-06-27 17:19:55,727 main.py:51] epoch 885, training loss: 10222.89, average training loss: 8647.69, base loss: 9220.78
[INFO 2017-06-27 17:19:56,938 main.py:51] epoch 886, training loss: 6483.61, average training loss: 8645.25, base loss: 9219.50
[INFO 2017-06-27 17:19:58,152 main.py:51] epoch 887, training loss: 7497.68, average training loss: 8643.96, base loss: 9220.37
[INFO 2017-06-27 17:19:59,366 main.py:51] epoch 888, training loss: 6952.96, average training loss: 8642.05, base loss: 9220.10
[INFO 2017-06-27 17:20:00,579 main.py:51] epoch 889, training loss: 6762.74, average training loss: 8639.94, base loss: 9219.72
[INFO 2017-06-27 17:20:01,790 main.py:51] epoch 890, training loss: 6750.49, average training loss: 8637.82, base loss: 9219.14
[INFO 2017-06-27 17:20:03,003 main.py:51] epoch 891, training loss: 6667.65, average training loss: 8635.61, base loss: 9218.52
[INFO 2017-06-27 17:20:04,220 main.py:51] epoch 892, training loss: 6722.41, average training loss: 8633.47, base loss: 9217.76
[INFO 2017-06-27 17:20:05,434 main.py:51] epoch 893, training loss: 6859.16, average training loss: 8631.49, base loss: 9217.70
[INFO 2017-06-27 17:20:06,648 main.py:51] epoch 894, training loss: 6913.23, average training loss: 8629.57, base loss: 9217.57
[INFO 2017-06-27 17:20:07,857 main.py:51] epoch 895, training loss: 7195.00, average training loss: 8627.96, base loss: 9218.16
[INFO 2017-06-27 17:20:09,066 main.py:51] epoch 896, training loss: 6450.87, average training loss: 8625.54, base loss: 9216.44
[INFO 2017-06-27 17:20:10,281 main.py:51] epoch 897, training loss: 7412.81, average training loss: 8624.19, base loss: 9216.94
[INFO 2017-06-27 17:20:11,499 main.py:51] epoch 898, training loss: 10602.85, average training loss: 8626.39, base loss: 9220.81
[INFO 2017-06-27 17:20:12,708 main.py:51] epoch 899, training loss: 7024.05, average training loss: 8624.61, base loss: 9220.24
[INFO 2017-06-27 17:20:12,708 main.py:53] epoch 899, testing
[INFO 2017-06-27 17:20:17,435 main.py:105] average testing loss: 6737.96, base loss: 8627.74
[INFO 2017-06-27 17:20:17,435 main.py:106] improve_loss: 1889.78, improve_percent: 0.22
[INFO 2017-06-27 17:20:17,436 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 17:20:17,449 main.py:76] current best improved percent: 0.22
[INFO 2017-06-27 17:20:18,658 main.py:51] epoch 900, training loss: 7048.49, average training loss: 8622.86, base loss: 9220.44
[INFO 2017-06-27 17:20:19,868 main.py:51] epoch 901, training loss: 7059.62, average training loss: 8621.13, base loss: 9220.32
[INFO 2017-06-27 17:20:21,078 main.py:51] epoch 902, training loss: 7092.50, average training loss: 8619.43, base loss: 9220.45
[INFO 2017-06-27 17:20:22,295 main.py:51] epoch 903, training loss: 6463.00, average training loss: 8617.05, base loss: 9219.14
[INFO 2017-06-27 17:20:23,510 main.py:51] epoch 904, training loss: 7032.63, average training loss: 8615.30, base loss: 9218.86
[INFO 2017-06-27 17:20:24,726 main.py:51] epoch 905, training loss: 7021.78, average training loss: 8613.54, base loss: 9218.68
[INFO 2017-06-27 17:20:25,935 main.py:51] epoch 906, training loss: 6606.48, average training loss: 8611.32, base loss: 9217.95
[INFO 2017-06-27 17:20:27,146 main.py:51] epoch 907, training loss: 6971.56, average training loss: 8609.52, base loss: 9217.48
[INFO 2017-06-27 17:20:28,362 main.py:51] epoch 908, training loss: 6634.12, average training loss: 8607.35, base loss: 9216.88
[INFO 2017-06-27 17:20:29,571 main.py:51] epoch 909, training loss: 6776.46, average training loss: 8605.33, base loss: 9216.66
[INFO 2017-06-27 17:20:30,788 main.py:51] epoch 910, training loss: 6705.52, average training loss: 8603.25, base loss: 9215.99
[INFO 2017-06-27 17:20:31,994 main.py:51] epoch 911, training loss: 6742.23, average training loss: 8601.21, base loss: 9215.77
[INFO 2017-06-27 17:20:33,204 main.py:51] epoch 912, training loss: 7277.30, average training loss: 8599.76, base loss: 9215.70
[INFO 2017-06-27 17:20:34,417 main.py:51] epoch 913, training loss: 7560.51, average training loss: 8598.62, base loss: 9216.27
[INFO 2017-06-27 17:20:35,627 main.py:51] epoch 914, training loss: 6469.13, average training loss: 8596.29, base loss: 9215.10
[INFO 2017-06-27 17:20:36,842 main.py:51] epoch 915, training loss: 6779.81, average training loss: 8594.31, base loss: 9214.45
[INFO 2017-06-27 17:20:38,057 main.py:51] epoch 916, training loss: 6910.91, average training loss: 8592.47, base loss: 9213.85
[INFO 2017-06-27 17:20:39,267 main.py:51] epoch 917, training loss: 6503.25, average training loss: 8590.20, base loss: 9212.77
[INFO 2017-06-27 17:20:40,480 main.py:51] epoch 918, training loss: 10593.81, average training loss: 8592.38, base loss: 9216.30
[INFO 2017-06-27 17:20:41,692 main.py:51] epoch 919, training loss: 7612.17, average training loss: 8591.31, base loss: 9217.40
[INFO 2017-06-27 17:20:42,905 main.py:51] epoch 920, training loss: 7203.14, average training loss: 8589.81, base loss: 9217.43
[INFO 2017-06-27 17:20:44,118 main.py:51] epoch 921, training loss: 7422.73, average training loss: 8588.54, base loss: 9217.96
[INFO 2017-06-27 17:20:45,332 main.py:51] epoch 922, training loss: 6686.23, average training loss: 8586.48, base loss: 9217.31
[INFO 2017-06-27 17:20:46,545 main.py:51] epoch 923, training loss: 6725.78, average training loss: 8584.47, base loss: 9216.72
[INFO 2017-06-27 17:20:47,758 main.py:51] epoch 924, training loss: 7106.37, average training loss: 8582.87, base loss: 9216.77
[INFO 2017-06-27 17:20:48,970 main.py:51] epoch 925, training loss: 6393.50, average training loss: 8580.50, base loss: 9215.30
[INFO 2017-06-27 17:20:50,182 main.py:51] epoch 926, training loss: 6843.33, average training loss: 8578.63, base loss: 9214.75
[INFO 2017-06-27 17:20:51,393 main.py:51] epoch 927, training loss: 6317.28, average training loss: 8576.19, base loss: 9213.70
[INFO 2017-06-27 17:20:52,607 main.py:51] epoch 928, training loss: 6631.99, average training loss: 8574.10, base loss: 9212.63
[INFO 2017-06-27 17:20:53,818 main.py:51] epoch 929, training loss: 7007.19, average training loss: 8572.41, base loss: 9212.71
[INFO 2017-06-27 17:20:55,027 main.py:51] epoch 930, training loss: 6665.73, average training loss: 8570.37, base loss: 9212.27
[INFO 2017-06-27 17:20:56,241 main.py:51] epoch 931, training loss: 7055.53, average training loss: 8568.74, base loss: 9212.41
[INFO 2017-06-27 17:20:57,451 main.py:51] epoch 932, training loss: 10217.99, average training loss: 8570.51, base loss: 9215.22
[INFO 2017-06-27 17:20:58,660 main.py:51] epoch 933, training loss: 7118.51, average training loss: 8568.95, base loss: 9215.82
[INFO 2017-06-27 17:20:59,873 main.py:51] epoch 934, training loss: 6700.37, average training loss: 8566.96, base loss: 9215.25
[INFO 2017-06-27 17:21:01,088 main.py:51] epoch 935, training loss: 7189.84, average training loss: 8565.48, base loss: 9215.78
[INFO 2017-06-27 17:21:02,299 main.py:51] epoch 936, training loss: 7559.68, average training loss: 8564.41, base loss: 9217.01
[INFO 2017-06-27 17:21:03,513 main.py:51] epoch 937, training loss: 7128.00, average training loss: 8562.88, base loss: 9217.43
[INFO 2017-06-27 17:21:04,726 main.py:51] epoch 938, training loss: 6426.55, average training loss: 8560.60, base loss: 9216.35
[INFO 2017-06-27 17:21:05,940 main.py:51] epoch 939, training loss: 6348.08, average training loss: 8558.25, base loss: 9214.62
[INFO 2017-06-27 17:21:07,152 main.py:51] epoch 940, training loss: 6826.33, average training loss: 8556.41, base loss: 9213.82
[INFO 2017-06-27 17:21:08,365 main.py:51] epoch 941, training loss: 6612.20, average training loss: 8554.35, base loss: 9212.72
[INFO 2017-06-27 17:21:09,572 main.py:51] epoch 942, training loss: 7566.54, average training loss: 8553.30, base loss: 9213.09
[INFO 2017-06-27 17:21:10,779 main.py:51] epoch 943, training loss: 6725.53, average training loss: 8551.36, base loss: 9212.40
[INFO 2017-06-27 17:21:11,989 main.py:51] epoch 944, training loss: 6621.17, average training loss: 8549.32, base loss: 9211.46
[INFO 2017-06-27 17:21:13,202 main.py:51] epoch 945, training loss: 6702.10, average training loss: 8547.37, base loss: 9210.83
[INFO 2017-06-27 17:21:14,418 main.py:51] epoch 946, training loss: 6818.99, average training loss: 8545.54, base loss: 9210.35
[INFO 2017-06-27 17:21:15,628 main.py:51] epoch 947, training loss: 7348.89, average training loss: 8544.28, base loss: 9210.89
[INFO 2017-06-27 17:21:16,837 main.py:51] epoch 948, training loss: 6644.40, average training loss: 8542.28, base loss: 9210.37
[INFO 2017-06-27 17:21:18,050 main.py:51] epoch 949, training loss: 6464.85, average training loss: 8540.09, base loss: 9209.23
[INFO 2017-06-27 17:21:19,264 main.py:51] epoch 950, training loss: 7583.94, average training loss: 8539.09, base loss: 9209.82
[INFO 2017-06-27 17:21:20,471 main.py:51] epoch 951, training loss: 6762.92, average training loss: 8537.22, base loss: 9209.66
[INFO 2017-06-27 17:21:21,679 main.py:51] epoch 952, training loss: 7020.34, average training loss: 8535.63, base loss: 9209.73
[INFO 2017-06-27 17:21:22,892 main.py:51] epoch 953, training loss: 10049.98, average training loss: 8537.22, base loss: 9212.24
[INFO 2017-06-27 17:21:24,102 main.py:51] epoch 954, training loss: 10806.41, average training loss: 8539.59, base loss: 9216.16
[INFO 2017-06-27 17:21:25,315 main.py:51] epoch 955, training loss: 10369.15, average training loss: 8541.51, base loss: 9219.47
[INFO 2017-06-27 17:21:26,526 main.py:51] epoch 956, training loss: 7222.66, average training loss: 8540.13, base loss: 9219.98
[INFO 2017-06-27 17:21:27,736 main.py:51] epoch 957, training loss: 10657.29, average training loss: 8542.34, base loss: 9223.87
[INFO 2017-06-27 17:21:28,948 main.py:51] epoch 958, training loss: 6620.33, average training loss: 8540.33, base loss: 9222.91
[INFO 2017-06-27 17:21:30,160 main.py:51] epoch 959, training loss: 7023.48, average training loss: 8538.75, base loss: 9222.78
[INFO 2017-06-27 17:21:31,371 main.py:51] epoch 960, training loss: 7260.09, average training loss: 8537.42, base loss: 9223.25
[INFO 2017-06-27 17:21:32,579 main.py:51] epoch 961, training loss: 6959.74, average training loss: 8535.78, base loss: 9222.86
[INFO 2017-06-27 17:21:33,794 main.py:51] epoch 962, training loss: 6358.11, average training loss: 8533.52, base loss: 9221.28
[INFO 2017-06-27 17:21:35,002 main.py:51] epoch 963, training loss: 6850.58, average training loss: 8531.78, base loss: 9220.75
[INFO 2017-06-27 17:21:36,212 main.py:51] epoch 964, training loss: 11232.67, average training loss: 8534.57, base loss: 9225.58
[INFO 2017-06-27 17:21:37,421 main.py:51] epoch 965, training loss: 6577.89, average training loss: 8532.55, base loss: 9224.41
[INFO 2017-06-27 17:21:38,661 main.py:51] epoch 966, training loss: 7084.74, average training loss: 8531.05, base loss: 9224.54
[INFO 2017-06-27 17:21:39,874 main.py:51] epoch 967, training loss: 6752.78, average training loss: 8529.22, base loss: 9224.25
[INFO 2017-06-27 17:21:41,082 main.py:51] epoch 968, training loss: 6982.18, average training loss: 8527.62, base loss: 9224.62
[INFO 2017-06-27 17:21:42,293 main.py:51] epoch 969, training loss: 7106.25, average training loss: 8526.15, base loss: 9224.78
[INFO 2017-06-27 17:21:43,505 main.py:51] epoch 970, training loss: 6765.83, average training loss: 8524.34, base loss: 9224.18
[INFO 2017-06-27 17:21:44,714 main.py:51] epoch 971, training loss: 7343.50, average training loss: 8523.13, base loss: 9224.48
[INFO 2017-06-27 17:21:45,922 main.py:51] epoch 972, training loss: 6928.22, average training loss: 8521.49, base loss: 9224.31
[INFO 2017-06-27 17:21:47,131 main.py:51] epoch 973, training loss: 6472.53, average training loss: 8519.38, base loss: 9223.35
[INFO 2017-06-27 17:21:48,347 main.py:51] epoch 974, training loss: 6679.83, average training loss: 8517.50, base loss: 9223.28
[INFO 2017-06-27 17:21:49,558 main.py:51] epoch 975, training loss: 5899.19, average training loss: 8514.81, base loss: 9221.37
[INFO 2017-06-27 17:21:50,765 main.py:51] epoch 976, training loss: 7035.89, average training loss: 8513.30, base loss: 9221.54
[INFO 2017-06-27 17:21:51,975 main.py:51] epoch 977, training loss: 6672.55, average training loss: 8511.42, base loss: 9220.82
[INFO 2017-06-27 17:21:53,187 main.py:51] epoch 978, training loss: 7306.44, average training loss: 8510.19, base loss: 9221.14
[INFO 2017-06-27 17:21:54,402 main.py:51] epoch 979, training loss: 6799.59, average training loss: 8508.44, base loss: 9220.80
[INFO 2017-06-27 17:21:55,615 main.py:51] epoch 980, training loss: 7122.35, average training loss: 8507.03, base loss: 9221.05
[INFO 2017-06-27 17:21:56,831 main.py:51] epoch 981, training loss: 6418.14, average training loss: 8504.90, base loss: 9220.11
[INFO 2017-06-27 17:21:58,043 main.py:51] epoch 982, training loss: 7272.79, average training loss: 8503.65, base loss: 9220.79
[INFO 2017-06-27 17:21:59,257 main.py:51] epoch 983, training loss: 7421.24, average training loss: 8502.55, base loss: 9221.59
[INFO 2017-06-27 17:22:00,464 main.py:51] epoch 984, training loss: 7216.35, average training loss: 8501.24, base loss: 9222.15
[INFO 2017-06-27 17:22:01,680 main.py:51] epoch 985, training loss: 6781.42, average training loss: 8499.50, base loss: 9222.17
[INFO 2017-06-27 17:22:02,896 main.py:51] epoch 986, training loss: 7182.95, average training loss: 8498.16, base loss: 9222.41
[INFO 2017-06-27 17:22:04,105 main.py:51] epoch 987, training loss: 7299.56, average training loss: 8496.95, base loss: 9222.37
[INFO 2017-06-27 17:22:05,314 main.py:51] epoch 988, training loss: 6540.55, average training loss: 8494.97, base loss: 9221.28
[INFO 2017-06-27 17:22:06,529 main.py:51] epoch 989, training loss: 7013.59, average training loss: 8493.48, base loss: 9221.07
[INFO 2017-06-27 17:22:07,737 main.py:51] epoch 990, training loss: 6800.97, average training loss: 8491.77, base loss: 9220.81
[INFO 2017-06-27 17:22:08,952 main.py:51] epoch 991, training loss: 6530.21, average training loss: 8489.79, base loss: 9219.81
[INFO 2017-06-27 17:22:10,168 main.py:51] epoch 992, training loss: 7192.69, average training loss: 8488.48, base loss: 9220.10
[INFO 2017-06-27 17:22:11,379 main.py:51] epoch 993, training loss: 6814.94, average training loss: 8486.80, base loss: 9220.33
[INFO 2017-06-27 17:22:12,588 main.py:51] epoch 994, training loss: 7039.05, average training loss: 8485.35, base loss: 9220.49
[INFO 2017-06-27 17:22:13,799 main.py:51] epoch 995, training loss: 6778.06, average training loss: 8483.63, base loss: 9219.86
[INFO 2017-06-27 17:22:15,011 main.py:51] epoch 996, training loss: 6975.02, average training loss: 8482.12, base loss: 9220.16
[INFO 2017-06-27 17:22:16,223 main.py:51] epoch 997, training loss: 6780.60, average training loss: 8480.41, base loss: 9219.90
[INFO 2017-06-27 17:22:17,434 main.py:51] epoch 998, training loss: 6447.73, average training loss: 8478.38, base loss: 9219.11
[INFO 2017-06-27 17:22:18,648 main.py:51] epoch 999, training loss: 6516.38, average training loss: 8476.42, base loss: 9218.51
[INFO 2017-06-27 17:22:18,648 main.py:53] epoch 999, testing
[INFO 2017-06-27 17:22:23,363 main.py:105] average testing loss: 6951.94, base loss: 8911.90
[INFO 2017-06-27 17:22:23,363 main.py:106] improve_loss: 1959.95, improve_percent: 0.22
[INFO 2017-06-27 17:22:23,364 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 17:22:23,377 main.py:76] current best improved percent: 0.22
[INFO 2017-06-27 17:22:24,586 main.py:51] epoch 1000, training loss: 6725.48, average training loss: 8415.41, base loss: 9217.80
[INFO 2017-06-27 17:22:25,802 main.py:51] epoch 1001, training loss: 7047.78, average training loss: 8366.71, base loss: 9217.45
[INFO 2017-06-27 17:22:27,014 main.py:51] epoch 1002, training loss: 7070.65, average training loss: 8327.11, base loss: 9219.79
[INFO 2017-06-27 17:22:28,225 main.py:51] epoch 1003, training loss: 6862.28, average training loss: 8292.57, base loss: 9219.38
[INFO 2017-06-27 17:22:29,438 main.py:51] epoch 1004, training loss: 6801.02, average training loss: 8259.53, base loss: 9215.87
[INFO 2017-06-27 17:22:30,649 main.py:51] epoch 1005, training loss: 6575.80, average training loss: 8234.18, base loss: 9214.89
[INFO 2017-06-27 17:22:31,863 main.py:51] epoch 1006, training loss: 7220.28, average training loss: 8213.53, base loss: 9215.54
[INFO 2017-06-27 17:22:33,076 main.py:51] epoch 1007, training loss: 6329.64, average training loss: 8191.87, base loss: 9213.13
[INFO 2017-06-27 17:22:34,292 main.py:51] epoch 1008, training loss: 7109.82, average training loss: 8176.55, base loss: 9214.45
[INFO 2017-06-27 17:22:35,506 main.py:51] epoch 1009, training loss: 6876.68, average training loss: 8161.16, base loss: 9214.30
[INFO 2017-06-27 17:22:36,718 main.py:51] epoch 1010, training loss: 7050.62, average training loss: 8147.44, base loss: 9213.71
[INFO 2017-06-27 17:22:37,926 main.py:51] epoch 1011, training loss: 6508.60, average training loss: 8135.04, base loss: 9213.04
[INFO 2017-06-27 17:22:39,139 main.py:51] epoch 1012, training loss: 7031.17, average training loss: 8124.10, base loss: 9213.11
[INFO 2017-06-27 17:22:40,348 main.py:51] epoch 1013, training loss: 6639.12, average training loss: 8113.05, base loss: 9212.66
[INFO 2017-06-27 17:22:41,562 main.py:51] epoch 1014, training loss: 7128.03, average training loss: 8103.65, base loss: 9213.14
[INFO 2017-06-27 17:22:42,771 main.py:51] epoch 1015, training loss: 6612.15, average training loss: 8095.26, base loss: 9213.97
[INFO 2017-06-27 17:22:43,983 main.py:51] epoch 1016, training loss: 6779.53, average training loss: 8086.69, base loss: 9214.35
[INFO 2017-06-27 17:22:45,195 main.py:51] epoch 1017, training loss: 6977.59, average training loss: 8079.29, base loss: 9215.77
[INFO 2017-06-27 17:22:46,405 main.py:51] epoch 1018, training loss: 7273.05, average training loss: 8070.26, base loss: 9214.58
[INFO 2017-06-27 17:22:47,618 main.py:51] epoch 1019, training loss: 6471.79, average training loss: 8063.30, base loss: 9214.10
[INFO 2017-06-27 17:22:48,831 main.py:51] epoch 1020, training loss: 7210.26, average training loss: 8056.63, base loss: 9214.02
[INFO 2017-06-27 17:22:50,044 main.py:51] epoch 1021, training loss: 6516.00, average training loss: 8050.11, base loss: 9213.61
[INFO 2017-06-27 17:22:51,257 main.py:51] epoch 1022, training loss: 7169.27, average training loss: 8044.10, base loss: 9214.05
[INFO 2017-06-27 17:22:52,469 main.py:51] epoch 1023, training loss: 6778.21, average training loss: 8039.06, base loss: 9214.83
[INFO 2017-06-27 17:22:53,677 main.py:51] epoch 1024, training loss: 6848.73, average training loss: 8033.48, base loss: 9214.75
[INFO 2017-06-27 17:22:54,890 main.py:51] epoch 1025, training loss: 6676.81, average training loss: 8027.74, base loss: 9214.62
[INFO 2017-06-27 17:22:56,103 main.py:51] epoch 1026, training loss: 6679.27, average training loss: 8019.67, base loss: 9211.86
[INFO 2017-06-27 17:22:57,312 main.py:51] epoch 1027, training loss: 7367.22, average training loss: 8015.02, base loss: 9212.17
[INFO 2017-06-27 17:22:58,529 main.py:51] epoch 1028, training loss: 7215.19, average training loss: 8010.93, base loss: 9213.34
[INFO 2017-06-27 17:22:59,736 main.py:51] epoch 1029, training loss: 6435.25, average training loss: 8005.57, base loss: 9211.91
[INFO 2017-06-27 17:23:00,942 main.py:51] epoch 1030, training loss: 6458.56, average training loss: 8000.33, base loss: 9210.45
[INFO 2017-06-27 17:23:02,151 main.py:51] epoch 1031, training loss: 7702.07, average training loss: 7997.00, base loss: 9211.46
[INFO 2017-06-27 17:23:03,362 main.py:51] epoch 1032, training loss: 6876.96, average training loss: 7992.40, base loss: 9210.75
[INFO 2017-06-27 17:23:04,576 main.py:51] epoch 1033, training loss: 6894.85, average training loss: 7988.86, base loss: 9211.77
[INFO 2017-06-27 17:23:05,790 main.py:51] epoch 1034, training loss: 6487.70, average training loss: 7984.38, base loss: 9211.16
[INFO 2017-06-27 17:23:07,006 main.py:51] epoch 1035, training loss: 6655.29, average training loss: 7980.24, base loss: 9210.88
[INFO 2017-06-27 17:23:08,220 main.py:51] epoch 1036, training loss: 6750.73, average training loss: 7976.57, base loss: 9211.20
[INFO 2017-06-27 17:23:09,433 main.py:51] epoch 1037, training loss: 6535.94, average training loss: 7972.90, base loss: 9211.54
[INFO 2017-06-27 17:23:10,647 main.py:51] epoch 1038, training loss: 6738.72, average training loss: 7968.70, base loss: 9211.33
[INFO 2017-06-27 17:23:11,851 main.py:51] epoch 1039, training loss: 5872.81, average training loss: 7964.43, base loss: 9210.12
[INFO 2017-06-27 17:23:13,067 main.py:51] epoch 1040, training loss: 5936.65, average training loss: 7960.07, base loss: 9208.92
[INFO 2017-06-27 17:23:14,274 main.py:51] epoch 1041, training loss: 6644.30, average training loss: 7956.14, base loss: 9208.18
[INFO 2017-06-27 17:23:15,483 main.py:51] epoch 1042, training loss: 6555.02, average training loss: 7952.36, base loss: 9207.51
[INFO 2017-06-27 17:23:16,696 main.py:51] epoch 1043, training loss: 6462.58, average training loss: 7949.99, base loss: 9208.07
[INFO 2017-06-27 17:23:17,906 main.py:51] epoch 1044, training loss: 6300.28, average training loss: 7946.80, base loss: 9208.31
[INFO 2017-06-27 17:23:19,118 main.py:51] epoch 1045, training loss: 6996.69, average training loss: 7942.33, base loss: 9207.30
[INFO 2017-06-27 17:23:20,327 main.py:51] epoch 1046, training loss: 7014.49, average training loss: 7938.99, base loss: 9207.61
[INFO 2017-06-27 17:23:21,538 main.py:51] epoch 1047, training loss: 9682.62, average training loss: 7938.59, base loss: 9209.46
[INFO 2017-06-27 17:23:22,752 main.py:51] epoch 1048, training loss: 6247.39, average training loss: 7935.56, base loss: 9209.02
[INFO 2017-06-27 17:23:23,966 main.py:51] epoch 1049, training loss: 10312.60, average training loss: 7935.63, base loss: 9212.12
[INFO 2017-06-27 17:23:25,177 main.py:51] epoch 1050, training loss: 6443.98, average training loss: 7928.94, base loss: 9208.21
[INFO 2017-06-27 17:23:26,394 main.py:51] epoch 1051, training loss: 7161.90, average training loss: 7927.35, base loss: 9209.89
[INFO 2017-06-27 17:23:27,604 main.py:51] epoch 1052, training loss: 9761.63, average training loss: 7927.27, base loss: 9212.38
[INFO 2017-06-27 17:23:28,820 main.py:51] epoch 1053, training loss: 7199.73, average training loss: 7925.27, base loss: 9213.35
[INFO 2017-06-27 17:23:30,035 main.py:51] epoch 1054, training loss: 10181.15, average training loss: 7924.88, base loss: 9215.49
[INFO 2017-06-27 17:23:31,246 main.py:51] epoch 1055, training loss: 13641.38, average training loss: 7925.59, base loss: 9218.86
[INFO 2017-06-27 17:23:32,456 main.py:51] epoch 1056, training loss: 10922.16, average training loss: 7921.63, base loss: 9217.83
[INFO 2017-06-27 17:23:33,667 main.py:51] epoch 1057, training loss: 6551.86, average training loss: 7917.45, base loss: 9216.60
[INFO 2017-06-27 17:23:34,882 main.py:51] epoch 1058, training loss: 7173.84, average training loss: 7915.26, base loss: 9216.74
[INFO 2017-06-27 17:23:36,098 main.py:51] epoch 1059, training loss: 6537.47, average training loss: 7912.29, base loss: 9216.47
[INFO 2017-06-27 17:23:37,331 main.py:51] epoch 1060, training loss: 7260.56, average training loss: 7910.08, base loss: 9217.28
[INFO 2017-06-27 17:23:38,547 main.py:51] epoch 1061, training loss: 6254.98, average training loss: 7906.44, base loss: 9216.08
[INFO 2017-06-27 17:23:39,761 main.py:51] epoch 1062, training loss: 7036.57, average training loss: 7904.31, base loss: 9216.83
[INFO 2017-06-27 17:23:40,975 main.py:51] epoch 1063, training loss: 6589.91, average training loss: 7901.23, base loss: 9216.20
[INFO 2017-06-27 17:23:42,193 main.py:51] epoch 1064, training loss: 6685.12, average training loss: 7898.77, base loss: 9216.29
[INFO 2017-06-27 17:23:43,409 main.py:51] epoch 1065, training loss: 6795.57, average training loss: 7896.01, base loss: 9216.41
[INFO 2017-06-27 17:23:44,618 main.py:51] epoch 1066, training loss: 6665.90, average training loss: 7892.96, base loss: 9215.67
[INFO 2017-06-27 17:23:45,828 main.py:51] epoch 1067, training loss: 6460.89, average training loss: 7890.14, base loss: 9215.27
[INFO 2017-06-27 17:23:47,044 main.py:51] epoch 1068, training loss: 6930.69, average training loss: 7888.41, base loss: 9216.47
[INFO 2017-06-27 17:23:48,262 main.py:51] epoch 1069, training loss: 6934.43, average training loss: 7886.46, base loss: 9217.15
[INFO 2017-06-27 17:23:49,475 main.py:51] epoch 1070, training loss: 6757.99, average training loss: 7884.03, base loss: 9217.93
[INFO 2017-06-27 17:23:50,687 main.py:51] epoch 1071, training loss: 6402.43, average training loss: 7878.25, base loss: 9213.97
[INFO 2017-06-27 17:23:51,903 main.py:51] epoch 1072, training loss: 5967.76, average training loss: 7874.00, base loss: 9211.63
[INFO 2017-06-27 17:23:53,131 main.py:51] epoch 1073, training loss: 6356.09, average training loss: 7871.21, base loss: 9210.55
[INFO 2017-06-27 17:23:54,344 main.py:51] epoch 1074, training loss: 6897.34, average training loss: 7869.45, base loss: 9210.92
[INFO 2017-06-27 17:23:55,551 main.py:51] epoch 1075, training loss: 10507.09, average training loss: 7867.50, base loss: 9211.32
[INFO 2017-06-27 17:23:56,768 main.py:51] epoch 1076, training loss: 7602.30, average training loss: 7865.41, base loss: 9212.05
[INFO 2017-06-27 17:23:57,975 main.py:51] epoch 1077, training loss: 7168.18, average training loss: 7863.81, base loss: 9213.18
[INFO 2017-06-27 17:23:59,192 main.py:51] epoch 1078, training loss: 6891.17, average training loss: 7861.11, base loss: 9212.43
[INFO 2017-06-27 17:24:00,408 main.py:51] epoch 1079, training loss: 7153.37, average training loss: 7859.57, base loss: 9213.83
[INFO 2017-06-27 17:24:01,618 main.py:51] epoch 1080, training loss: 6520.75, average training loss: 7857.46, base loss: 9213.75
[INFO 2017-06-27 17:24:02,826 main.py:51] epoch 1081, training loss: 6351.52, average training loss: 7854.94, base loss: 9212.82
[INFO 2017-06-27 17:24:04,040 main.py:51] epoch 1082, training loss: 7183.78, average training loss: 7852.35, base loss: 9212.10
[INFO 2017-06-27 17:24:05,256 main.py:51] epoch 1083, training loss: 7007.75, average training loss: 7850.37, base loss: 9212.09
[INFO 2017-06-27 17:24:06,474 main.py:51] epoch 1084, training loss: 7070.66, average training loss: 7848.44, base loss: 9212.69
[INFO 2017-06-27 17:24:07,688 main.py:51] epoch 1085, training loss: 7012.96, average training loss: 7846.45, base loss: 9213.10
[INFO 2017-06-27 17:24:08,902 main.py:51] epoch 1086, training loss: 6222.07, average training loss: 7844.61, base loss: 9213.22
[INFO 2017-06-27 17:24:10,114 main.py:51] epoch 1087, training loss: 7123.09, average training loss: 7842.58, base loss: 9213.62
[INFO 2017-06-27 17:24:11,332 main.py:51] epoch 1088, training loss: 6655.09, average training loss: 7839.41, base loss: 9212.03
[INFO 2017-06-27 17:24:12,541 main.py:51] epoch 1089, training loss: 6358.05, average training loss: 7836.01, base loss: 9210.28
[INFO 2017-06-27 17:24:13,748 main.py:51] epoch 1090, training loss: 6521.97, average training loss: 7827.30, base loss: 9203.72
[INFO 2017-06-27 17:24:14,961 main.py:51] epoch 1091, training loss: 6790.82, average training loss: 7825.27, base loss: 9204.22
[INFO 2017-06-27 17:24:16,177 main.py:51] epoch 1092, training loss: 6943.31, average training loss: 7823.50, base loss: 9204.70
[INFO 2017-06-27 17:24:17,389 main.py:51] epoch 1093, training loss: 6282.50, average training loss: 7820.85, base loss: 9204.32
[INFO 2017-06-27 17:24:18,603 main.py:51] epoch 1094, training loss: 6682.99, average training loss: 7818.42, base loss: 9203.80
[INFO 2017-06-27 17:24:19,814 main.py:51] epoch 1095, training loss: 6766.66, average training loss: 7815.90, base loss: 9203.58
[INFO 2017-06-27 17:24:21,026 main.py:51] epoch 1096, training loss: 6324.61, average training loss: 7814.16, base loss: 9203.73
[INFO 2017-06-27 17:24:22,242 main.py:51] epoch 1097, training loss: 6824.12, average training loss: 7812.42, base loss: 9204.05
[INFO 2017-06-27 17:24:23,457 main.py:51] epoch 1098, training loss: 7084.20, average training loss: 7810.89, base loss: 9204.93
[INFO 2017-06-27 17:24:24,672 main.py:51] epoch 1099, training loss: 7063.26, average training loss: 7809.17, base loss: 9205.67
[INFO 2017-06-27 17:24:24,672 main.py:53] epoch 1099, testing
[INFO 2017-06-27 17:24:29,394 main.py:105] average testing loss: 6784.20, base loss: 8915.13
[INFO 2017-06-27 17:24:29,394 main.py:106] improve_loss: 2130.92, improve_percent: 0.24
[INFO 2017-06-27 17:24:29,395 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 17:24:29,408 main.py:76] current best improved percent: 0.24
[INFO 2017-06-27 17:24:30,621 main.py:51] epoch 1100, training loss: 6514.18, average training loss: 7806.91, base loss: 9205.38
[INFO 2017-06-27 17:24:31,834 main.py:51] epoch 1101, training loss: 7069.90, average training loss: 7804.50, base loss: 9205.28
[INFO 2017-06-27 17:24:33,049 main.py:51] epoch 1102, training loss: 6658.27, average training loss: 7803.12, base loss: 9205.79
[INFO 2017-06-27 17:24:34,259 main.py:51] epoch 1103, training loss: 7513.89, average training loss: 7802.67, base loss: 9208.03
[INFO 2017-06-27 17:24:35,473 main.py:51] epoch 1104, training loss: 8100.39, average training loss: 7802.59, base loss: 9210.81
[INFO 2017-06-27 17:24:36,682 main.py:51] epoch 1105, training loss: 6886.34, average training loss: 7801.61, base loss: 9212.49
[INFO 2017-06-27 17:24:37,899 main.py:51] epoch 1106, training loss: 6712.20, average training loss: 7798.66, base loss: 9211.85
[INFO 2017-06-27 17:24:39,111 main.py:51] epoch 1107, training loss: 6324.79, average training loss: 7794.66, base loss: 9209.56
[INFO 2017-06-27 17:24:40,327 main.py:51] epoch 1108, training loss: 10751.95, average training loss: 7796.37, base loss: 9213.95
[INFO 2017-06-27 17:24:41,543 main.py:51] epoch 1109, training loss: 6288.40, average training loss: 7794.56, base loss: 9213.95
[INFO 2017-06-27 17:24:42,755 main.py:51] epoch 1110, training loss: 6576.05, average training loss: 7792.05, base loss: 9213.36
[INFO 2017-06-27 17:24:43,971 main.py:51] epoch 1111, training loss: 6769.13, average training loss: 7789.23, base loss: 9212.49
[INFO 2017-06-27 17:24:45,183 main.py:51] epoch 1112, training loss: 6682.65, average training loss: 7786.47, base loss: 9211.72
[INFO 2017-06-27 17:24:46,399 main.py:51] epoch 1113, training loss: 7640.26, average training loss: 7785.23, base loss: 9213.20
[INFO 2017-06-27 17:24:47,615 main.py:51] epoch 1114, training loss: 7297.18, average training loss: 7784.23, base loss: 9214.68
[INFO 2017-06-27 17:24:48,831 main.py:51] epoch 1115, training loss: 7099.20, average training loss: 7782.32, base loss: 9215.67
[INFO 2017-06-27 17:24:50,040 main.py:51] epoch 1116, training loss: 6530.45, average training loss: 7779.59, base loss: 9214.85
[INFO 2017-06-27 17:24:51,258 main.py:51] epoch 1117, training loss: 6255.14, average training loss: 7776.42, base loss: 9213.86
[INFO 2017-06-27 17:24:52,467 main.py:51] epoch 1118, training loss: 6359.71, average training loss: 7773.99, base loss: 9212.66
[INFO 2017-06-27 17:24:53,686 main.py:51] epoch 1119, training loss: 6595.04, average training loss: 7772.76, base loss: 9213.23
[INFO 2017-06-27 17:24:54,904 main.py:51] epoch 1120, training loss: 6603.38, average training loss: 7769.72, base loss: 9211.84
[INFO 2017-06-27 17:24:56,114 main.py:51] epoch 1121, training loss: 6582.14, average training loss: 7767.32, base loss: 9211.35
[INFO 2017-06-27 17:24:57,328 main.py:51] epoch 1122, training loss: 6459.26, average training loss: 7765.15, base loss: 9210.88
[INFO 2017-06-27 17:24:58,543 main.py:51] epoch 1123, training loss: 7003.04, average training loss: 7763.19, base loss: 9211.39
[INFO 2017-06-27 17:24:59,758 main.py:51] epoch 1124, training loss: 7403.48, average training loss: 7762.14, base loss: 9213.20
[INFO 2017-06-27 17:25:00,973 main.py:51] epoch 1125, training loss: 7520.80, average training loss: 7757.52, base loss: 9211.70
[INFO 2017-06-27 17:25:02,190 main.py:51] epoch 1126, training loss: 7015.67, average training loss: 7756.16, base loss: 9211.94
[INFO 2017-06-27 17:25:03,406 main.py:51] epoch 1127, training loss: 6773.20, average training loss: 7754.36, base loss: 9211.65
[INFO 2017-06-27 17:25:04,619 main.py:51] epoch 1128, training loss: 7408.90, average training loss: 7752.33, base loss: 9211.73
[INFO 2017-06-27 17:25:05,835 main.py:51] epoch 1129, training loss: 6237.99, average training loss: 7749.85, base loss: 9210.94
[INFO 2017-06-27 17:25:07,046 main.py:51] epoch 1130, training loss: 7200.33, average training loss: 7747.99, base loss: 9210.97
[INFO 2017-06-27 17:25:08,252 main.py:51] epoch 1131, training loss: 6918.82, average training loss: 7747.07, base loss: 9212.21
[INFO 2017-06-27 17:25:09,469 main.py:51] epoch 1132, training loss: 7059.30, average training loss: 7746.33, base loss: 9213.83
[INFO 2017-06-27 17:25:10,684 main.py:51] epoch 1133, training loss: 6482.99, average training loss: 7744.40, base loss: 9213.44
[INFO 2017-06-27 17:25:11,896 main.py:51] epoch 1134, training loss: 7007.34, average training loss: 7744.13, base loss: 9215.39
[INFO 2017-06-27 17:25:13,111 main.py:51] epoch 1135, training loss: 6533.40, average training loss: 7741.49, base loss: 9214.37
[INFO 2017-06-27 17:25:14,326 main.py:51] epoch 1136, training loss: 7252.00, average training loss: 7740.23, base loss: 9215.37
[INFO 2017-06-27 17:25:15,543 main.py:51] epoch 1137, training loss: 6131.52, average training loss: 7737.90, base loss: 9214.76
[INFO 2017-06-27 17:25:16,758 main.py:51] epoch 1138, training loss: 6161.28, average training loss: 7734.26, base loss: 9212.36
[INFO 2017-06-27 17:25:17,973 main.py:51] epoch 1139, training loss: 6375.86, average training loss: 7732.37, base loss: 9211.79
[INFO 2017-06-27 17:25:19,187 main.py:51] epoch 1140, training loss: 6271.20, average training loss: 7730.03, base loss: 9211.33
[INFO 2017-06-27 17:25:20,402 main.py:51] epoch 1141, training loss: 7784.62, average training loss: 7728.28, base loss: 9212.26
[INFO 2017-06-27 17:25:21,616 main.py:51] epoch 1142, training loss: 7901.65, average training loss: 7726.96, base loss: 9213.31
[INFO 2017-06-27 17:25:22,826 main.py:51] epoch 1143, training loss: 6675.71, average training loss: 7725.18, base loss: 9212.70
[INFO 2017-06-27 17:25:24,042 main.py:51] epoch 1144, training loss: 6643.17, average training loss: 7722.36, base loss: 9211.40
[INFO 2017-06-27 17:25:25,258 main.py:51] epoch 1145, training loss: 6609.10, average training loss: 7716.07, base loss: 9207.21
[INFO 2017-06-27 17:25:26,470 main.py:51] epoch 1146, training loss: 7094.24, average training loss: 7714.62, base loss: 9208.03
[INFO 2017-06-27 17:25:27,685 main.py:51] epoch 1147, training loss: 6911.58, average training loss: 7711.88, base loss: 9207.23
[INFO 2017-06-27 17:25:28,897 main.py:51] epoch 1148, training loss: 6793.11, average training loss: 7709.53, base loss: 9206.95
[INFO 2017-06-27 17:25:30,113 main.py:51] epoch 1149, training loss: 7161.10, average training loss: 7706.76, base loss: 9206.82
[INFO 2017-06-27 17:25:31,326 main.py:51] epoch 1150, training loss: 7193.12, average training loss: 7705.30, base loss: 9207.68
[INFO 2017-06-27 17:25:32,540 main.py:51] epoch 1151, training loss: 7131.24, average training loss: 7703.77, base loss: 9208.32
[INFO 2017-06-27 17:25:33,751 main.py:51] epoch 1152, training loss: 6437.23, average training loss: 7701.70, base loss: 9208.22
[INFO 2017-06-27 17:25:34,963 main.py:51] epoch 1153, training loss: 6751.36, average training loss: 7699.10, base loss: 9207.23
[INFO 2017-06-27 17:25:36,170 main.py:51] epoch 1154, training loss: 5666.80, average training loss: 7695.68, base loss: 9204.83
[INFO 2017-06-27 17:25:37,385 main.py:51] epoch 1155, training loss: 6256.64, average training loss: 7693.31, base loss: 9203.99
[INFO 2017-06-27 17:25:38,600 main.py:51] epoch 1156, training loss: 6708.52, average training loss: 7687.06, base loss: 9199.46
[INFO 2017-06-27 17:25:39,807 main.py:51] epoch 1157, training loss: 6187.05, average training loss: 7684.51, base loss: 9198.29
[INFO 2017-06-27 17:25:41,011 main.py:51] epoch 1158, training loss: 6979.70, average training loss: 7683.26, base loss: 9198.94
[INFO 2017-06-27 17:25:42,221 main.py:51] epoch 1159, training loss: 6731.77, average training loss: 7681.37, base loss: 9198.89
[INFO 2017-06-27 17:25:43,435 main.py:51] epoch 1160, training loss: 6885.78, average training loss: 7678.94, base loss: 9198.49
[INFO 2017-06-27 17:25:44,648 main.py:51] epoch 1161, training loss: 6510.56, average training loss: 7676.27, base loss: 9197.69
[INFO 2017-06-27 17:25:45,860 main.py:51] epoch 1162, training loss: 6637.33, average training loss: 7673.60, base loss: 9196.67
[INFO 2017-06-27 17:25:47,070 main.py:51] epoch 1163, training loss: 6124.30, average training loss: 7670.91, base loss: 9195.58
[INFO 2017-06-27 17:25:48,285 main.py:51] epoch 1164, training loss: 6335.88, average training loss: 7668.84, base loss: 9195.12
[INFO 2017-06-27 17:25:49,498 main.py:51] epoch 1165, training loss: 6387.30, average training loss: 7665.96, base loss: 9193.94
[INFO 2017-06-27 17:25:50,710 main.py:51] epoch 1166, training loss: 6342.62, average training loss: 7664.10, base loss: 9193.57
[INFO 2017-06-27 17:25:51,918 main.py:51] epoch 1167, training loss: 7497.39, average training loss: 7662.72, base loss: 9194.40
[INFO 2017-06-27 17:25:53,129 main.py:51] epoch 1168, training loss: 6460.52, average training loss: 7660.82, base loss: 9194.61
[INFO 2017-06-27 17:25:54,342 main.py:51] epoch 1169, training loss: 9552.65, average training loss: 7661.77, base loss: 9196.77
[INFO 2017-06-27 17:25:55,558 main.py:51] epoch 1170, training loss: 6584.91, average training loss: 7659.09, base loss: 9195.76
[INFO 2017-06-27 17:25:56,766 main.py:51] epoch 1171, training loss: 6644.70, average training loss: 7656.16, base loss: 9194.37
[INFO 2017-06-27 17:25:57,976 main.py:51] epoch 1172, training loss: 6534.36, average training loss: 7655.07, base loss: 9194.96
[INFO 2017-06-27 17:25:59,191 main.py:51] epoch 1173, training loss: 6506.11, average training loss: 7653.15, base loss: 9194.80
[INFO 2017-06-27 17:26:00,404 main.py:51] epoch 1174, training loss: 6551.30, average training loss: 7651.09, base loss: 9194.45
[INFO 2017-06-27 17:26:01,619 main.py:51] epoch 1175, training loss: 6215.33, average training loss: 7649.82, base loss: 9194.73
[INFO 2017-06-27 17:26:02,829 main.py:51] epoch 1176, training loss: 6514.38, average training loss: 7647.17, base loss: 9193.96
[INFO 2017-06-27 17:26:04,038 main.py:51] epoch 1177, training loss: 7308.51, average training loss: 7646.57, base loss: 9195.67
[INFO 2017-06-27 17:26:05,252 main.py:51] epoch 1178, training loss: 7252.57, average training loss: 7644.89, base loss: 9196.52
[INFO 2017-06-27 17:26:06,466 main.py:51] epoch 1179, training loss: 6523.36, average training loss: 7642.90, base loss: 9196.59
[INFO 2017-06-27 17:26:07,679 main.py:51] epoch 1180, training loss: 6524.74, average training loss: 7639.76, base loss: 9195.10
[INFO 2017-06-27 17:26:08,893 main.py:51] epoch 1181, training loss: 6751.52, average training loss: 7637.52, base loss: 9194.82
[INFO 2017-06-27 17:26:10,106 main.py:51] epoch 1182, training loss: 6249.76, average training loss: 7634.93, base loss: 9193.55
[INFO 2017-06-27 17:26:11,316 main.py:51] epoch 1183, training loss: 6805.19, average training loss: 7633.42, base loss: 9194.17
[INFO 2017-06-27 17:26:12,523 main.py:51] epoch 1184, training loss: 6670.68, average training loss: 7631.26, base loss: 9194.04
[INFO 2017-06-27 17:26:13,734 main.py:51] epoch 1185, training loss: 13235.35, average training loss: 7636.92, base loss: 9201.49
[INFO 2017-06-27 17:26:14,945 main.py:51] epoch 1186, training loss: 6799.59, average training loss: 7634.37, base loss: 9200.78
[INFO 2017-06-27 17:26:16,160 main.py:51] epoch 1187, training loss: 6698.39, average training loss: 7632.74, base loss: 9201.05
[INFO 2017-06-27 17:26:17,374 main.py:51] epoch 1188, training loss: 7176.03, average training loss: 7631.64, base loss: 9202.30
[INFO 2017-06-27 17:26:18,588 main.py:51] epoch 1189, training loss: 9658.23, average training loss: 7633.36, base loss: 9205.63
[INFO 2017-06-27 17:26:19,803 main.py:51] epoch 1190, training loss: 6855.22, average training loss: 7631.44, base loss: 9205.86
[INFO 2017-06-27 17:26:21,016 main.py:51] epoch 1191, training loss: 6776.33, average training loss: 7628.06, base loss: 9204.55
[INFO 2017-06-27 17:26:22,229 main.py:51] epoch 1192, training loss: 6578.03, average training loss: 7626.63, base loss: 9204.99
[INFO 2017-06-27 17:26:23,440 main.py:51] epoch 1193, training loss: 7276.97, average training loss: 7625.26, base loss: 9206.43
[INFO 2017-06-27 17:26:24,657 main.py:51] epoch 1194, training loss: 6676.73, average training loss: 7619.90, base loss: 9203.04
[INFO 2017-06-27 17:26:25,866 main.py:51] epoch 1195, training loss: 6085.14, average training loss: 7617.87, base loss: 9202.33
[INFO 2017-06-27 17:26:27,081 main.py:51] epoch 1196, training loss: 6623.51, average training loss: 7616.34, base loss: 9202.39
[INFO 2017-06-27 17:26:28,299 main.py:51] epoch 1197, training loss: 7337.08, average training loss: 7615.42, base loss: 9203.83
[INFO 2017-06-27 17:26:29,515 main.py:51] epoch 1198, training loss: 6656.96, average training loss: 7613.95, base loss: 9204.05
[INFO 2017-06-27 17:26:30,730 main.py:51] epoch 1199, training loss: 6607.68, average training loss: 7611.93, base loss: 9203.72
[INFO 2017-06-27 17:26:30,730 main.py:53] epoch 1199, testing
[INFO 2017-06-27 17:26:35,446 main.py:105] average testing loss: 6743.31, base loss: 8927.56
[INFO 2017-06-27 17:26:35,447 main.py:106] improve_loss: 2184.24, improve_percent: 0.24
[INFO 2017-06-27 17:26:35,447 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 17:26:35,460 main.py:76] current best improved percent: 0.24
[INFO 2017-06-27 17:26:36,672 main.py:51] epoch 1200, training loss: 7017.70, average training loss: 7611.15, base loss: 9205.39
[INFO 2017-06-27 17:26:37,888 main.py:51] epoch 1201, training loss: 6764.58, average training loss: 7609.01, base loss: 9204.76
[INFO 2017-06-27 17:26:39,103 main.py:51] epoch 1202, training loss: 6285.54, average training loss: 7606.00, base loss: 9202.92
[INFO 2017-06-27 17:26:40,320 main.py:51] epoch 1203, training loss: 6906.07, average training loss: 7600.97, base loss: 9200.23
[INFO 2017-06-27 17:26:41,529 main.py:51] epoch 1204, training loss: 10155.67, average training loss: 7603.56, base loss: 9205.10
[INFO 2017-06-27 17:26:42,741 main.py:51] epoch 1205, training loss: 6681.52, average training loss: 7601.24, base loss: 9204.68
[INFO 2017-06-27 17:26:43,951 main.py:51] epoch 1206, training loss: 6361.69, average training loss: 7598.81, base loss: 9203.71
[INFO 2017-06-27 17:26:45,167 main.py:51] epoch 1207, training loss: 6048.36, average training loss: 7596.27, base loss: 9202.34
[INFO 2017-06-27 17:26:46,380 main.py:51] epoch 1208, training loss: 6535.21, average training loss: 7594.64, base loss: 9202.38
[INFO 2017-06-27 17:26:47,593 main.py:51] epoch 1209, training loss: 7305.82, average training loss: 7594.12, base loss: 9204.31
[INFO 2017-06-27 17:26:48,807 main.py:51] epoch 1210, training loss: 6809.22, average training loss: 7591.76, base loss: 9203.84
[INFO 2017-06-27 17:26:50,021 main.py:51] epoch 1211, training loss: 6532.94, average training loss: 7586.55, base loss: 9200.48
[INFO 2017-06-27 17:26:51,229 main.py:51] epoch 1212, training loss: 6396.90, average training loss: 7585.43, base loss: 9200.84
[INFO 2017-06-27 17:26:52,441 main.py:51] epoch 1213, training loss: 7285.14, average training loss: 7585.01, base loss: 9203.23
[INFO 2017-06-27 17:26:53,656 main.py:51] epoch 1214, training loss: 6853.91, average training loss: 7583.51, base loss: 9203.39
[INFO 2017-06-27 17:26:54,867 main.py:51] epoch 1215, training loss: 6421.69, average training loss: 7581.88, base loss: 9203.13
[INFO 2017-06-27 17:26:56,078 main.py:51] epoch 1216, training loss: 6710.57, average training loss: 7580.03, base loss: 9202.95
[INFO 2017-06-27 17:26:57,292 main.py:51] epoch 1217, training loss: 6336.49, average training loss: 7578.03, base loss: 9202.59
[INFO 2017-06-27 17:26:58,499 main.py:51] epoch 1218, training loss: 9582.75, average training loss: 7578.66, base loss: 9204.81
[INFO 2017-06-27 17:26:59,708 main.py:51] epoch 1219, training loss: 7227.59, average training loss: 7577.24, base loss: 9205.29
[INFO 2017-06-27 17:27:00,925 main.py:51] epoch 1220, training loss: 9811.24, average training loss: 7579.43, base loss: 9209.43
[INFO 2017-06-27 17:27:02,137 main.py:51] epoch 1221, training loss: 6674.04, average training loss: 7577.21, base loss: 9209.19
[INFO 2017-06-27 17:27:03,351 main.py:51] epoch 1222, training loss: 10581.38, average training loss: 7578.53, base loss: 9212.29
[INFO 2017-06-27 17:27:04,562 main.py:51] epoch 1223, training loss: 7022.70, average training loss: 7577.61, base loss: 9212.76
[INFO 2017-06-27 17:27:05,776 main.py:51] epoch 1224, training loss: 6587.01, average training loss: 7575.79, base loss: 9212.58
[INFO 2017-06-27 17:27:06,990 main.py:51] epoch 1225, training loss: 6638.35, average training loss: 7574.41, base loss: 9212.99
[INFO 2017-06-27 17:27:08,200 main.py:51] epoch 1226, training loss: 6756.41, average training loss: 7572.83, base loss: 9213.84
[INFO 2017-06-27 17:27:09,412 main.py:51] epoch 1227, training loss: 6715.43, average training loss: 7570.87, base loss: 9213.46
[INFO 2017-06-27 17:27:10,629 main.py:51] epoch 1228, training loss: 6619.29, average training loss: 7567.45, base loss: 9211.17
[INFO 2017-06-27 17:27:11,842 main.py:51] epoch 1229, training loss: 6636.71, average training loss: 7565.50, base loss: 9210.80
[INFO 2017-06-27 17:27:13,057 main.py:51] epoch 1230, training loss: 6385.27, average training loss: 7562.86, base loss: 9209.90
[INFO 2017-06-27 17:27:14,264 main.py:51] epoch 1231, training loss: 9680.95, average training loss: 7564.50, base loss: 9213.33
[INFO 2017-06-27 17:27:15,481 main.py:51] epoch 1232, training loss: 7078.57, average training loss: 7563.83, base loss: 9214.91
[INFO 2017-06-27 17:27:16,693 main.py:51] epoch 1233, training loss: 6947.47, average training loss: 7562.58, base loss: 9215.85
[INFO 2017-06-27 17:27:17,902 main.py:51] epoch 1234, training loss: 7011.10, average training loss: 7557.52, base loss: 9213.12
[INFO 2017-06-27 17:27:19,111 main.py:51] epoch 1235, training loss: 6514.66, average training loss: 7554.86, base loss: 9212.18
[INFO 2017-06-27 17:27:20,323 main.py:51] epoch 1236, training loss: 6998.21, average training loss: 7553.41, base loss: 9212.65
[INFO 2017-06-27 17:27:21,530 main.py:51] epoch 1237, training loss: 6831.43, average training loss: 7550.96, base loss: 9211.97
[INFO 2017-06-27 17:27:22,743 main.py:51] epoch 1238, training loss: 6408.55, average training loss: 7549.80, base loss: 9212.33
[INFO 2017-06-27 17:27:23,958 main.py:51] epoch 1239, training loss: 6417.50, average training loss: 7543.65, base loss: 9207.72
[INFO 2017-06-27 17:27:25,171 main.py:51] epoch 1240, training loss: 6428.00, average training loss: 7541.74, base loss: 9207.33
[INFO 2017-06-27 17:27:26,387 main.py:51] epoch 1241, training loss: 10558.72, average training loss: 7543.64, base loss: 9211.43
[INFO 2017-06-27 17:27:27,605 main.py:51] epoch 1242, training loss: 7439.23, average training loss: 7542.75, base loss: 9213.07
[INFO 2017-06-27 17:27:28,817 main.py:51] epoch 1243, training loss: 6755.81, average training loss: 7540.74, base loss: 9212.85
[INFO 2017-06-27 17:27:30,028 main.py:51] epoch 1244, training loss: 6534.98, average training loss: 7539.11, base loss: 9212.60
[INFO 2017-06-27 17:27:31,240 main.py:51] epoch 1245, training loss: 6126.17, average training loss: 7536.51, base loss: 9211.10
[INFO 2017-06-27 17:27:32,453 main.py:51] epoch 1246, training loss: 6399.77, average training loss: 7534.67, base loss: 9210.71
[INFO 2017-06-27 17:27:33,663 main.py:51] epoch 1247, training loss: 6345.13, average training loss: 7532.44, base loss: 9209.67
[INFO 2017-06-27 17:27:34,875 main.py:51] epoch 1248, training loss: 5988.50, average training loss: 7529.02, base loss: 9206.89
[INFO 2017-06-27 17:27:36,086 main.py:51] epoch 1249, training loss: 6109.23, average training loss: 7526.74, base loss: 9205.42
[INFO 2017-06-27 17:27:37,297 main.py:51] epoch 1250, training loss: 6387.02, average training loss: 7523.72, base loss: 9203.09
[INFO 2017-06-27 17:27:38,508 main.py:51] epoch 1251, training loss: 6948.36, average training loss: 7521.30, base loss: 9202.12
[INFO 2017-06-27 17:27:39,719 main.py:51] epoch 1252, training loss: 6901.33, average training loss: 7520.31, base loss: 9202.75
[INFO 2017-06-27 17:27:40,929 main.py:51] epoch 1253, training loss: 7131.93, average training loss: 7519.21, base loss: 9203.53
[INFO 2017-06-27 17:27:42,138 main.py:51] epoch 1254, training loss: 6630.62, average training loss: 7513.73, base loss: 9199.80
[INFO 2017-06-27 17:27:43,351 main.py:51] epoch 1255, training loss: 6198.70, average training loss: 7503.95, base loss: 9191.28
[INFO 2017-06-27 17:27:44,562 main.py:51] epoch 1256, training loss: 6501.45, average training loss: 7501.40, base loss: 9190.41
[INFO 2017-06-27 17:27:45,774 main.py:51] epoch 1257, training loss: 6580.15, average training loss: 7500.16, base loss: 9190.59
[INFO 2017-06-27 17:27:46,985 main.py:51] epoch 1258, training loss: 13496.08, average training loss: 7505.59, base loss: 9197.86
[INFO 2017-06-27 17:27:48,197 main.py:51] epoch 1259, training loss: 7451.56, average training loss: 7505.33, base loss: 9199.46
[INFO 2017-06-27 17:27:49,413 main.py:51] epoch 1260, training loss: 6425.99, average training loss: 7503.63, base loss: 9199.32
[INFO 2017-06-27 17:27:50,625 main.py:51] epoch 1261, training loss: 6789.39, average training loss: 7502.10, base loss: 9199.11
[INFO 2017-06-27 17:27:51,833 main.py:51] epoch 1262, training loss: 7009.89, average training loss: 7500.86, base loss: 9199.45
[INFO 2017-06-27 17:27:53,048 main.py:51] epoch 1263, training loss: 10520.63, average training loss: 7503.18, base loss: 9203.75
[INFO 2017-06-27 17:27:54,259 main.py:51] epoch 1264, training loss: 6693.46, average training loss: 7500.85, base loss: 9203.01
[INFO 2017-06-27 17:27:55,473 main.py:51] epoch 1265, training loss: 6427.67, average training loss: 7499.55, base loss: 9203.24
[INFO 2017-06-27 17:27:56,684 main.py:51] epoch 1266, training loss: 6931.47, average training loss: 7498.48, base loss: 9203.92
[INFO 2017-06-27 17:27:57,894 main.py:51] epoch 1267, training loss: 10247.31, average training loss: 7499.72, base loss: 9206.80
[INFO 2017-06-27 17:27:59,104 main.py:51] epoch 1268, training loss: 6429.86, average training loss: 7497.40, base loss: 9206.23
[INFO 2017-06-27 17:28:00,313 main.py:51] epoch 1269, training loss: 6789.25, average training loss: 7492.28, base loss: 9203.33
[INFO 2017-06-27 17:28:01,525 main.py:51] epoch 1270, training loss: 6466.10, average training loss: 7491.63, base loss: 9204.40
[INFO 2017-06-27 17:28:02,733 main.py:51] epoch 1271, training loss: 6306.61, average training loss: 7489.26, base loss: 9203.62
[INFO 2017-06-27 17:28:03,945 main.py:51] epoch 1272, training loss: 6955.07, average training loss: 7487.77, base loss: 9204.22
[INFO 2017-06-27 17:28:05,158 main.py:51] epoch 1273, training loss: 6480.62, average training loss: 7485.95, base loss: 9203.48
[INFO 2017-06-27 17:28:06,371 main.py:51] epoch 1274, training loss: 6389.92, average training loss: 7484.54, base loss: 9203.33
[INFO 2017-06-27 17:28:07,584 main.py:51] epoch 1275, training loss: 6499.38, average training loss: 7482.88, base loss: 9203.20
[INFO 2017-06-27 17:28:08,793 main.py:51] epoch 1276, training loss: 6741.56, average training loss: 7480.93, base loss: 9203.05
[INFO 2017-06-27 17:28:10,008 main.py:51] epoch 1277, training loss: 7140.78, average training loss: 7478.89, base loss: 9202.58
[INFO 2017-06-27 17:28:11,220 main.py:51] epoch 1278, training loss: 6418.89, average training loss: 7474.32, base loss: 9199.32
[INFO 2017-06-27 17:28:12,434 main.py:51] epoch 1279, training loss: 6676.82, average training loss: 7473.21, base loss: 9200.59
[INFO 2017-06-27 17:28:13,650 main.py:51] epoch 1280, training loss: 6502.11, average training loss: 7471.26, base loss: 9200.32
[INFO 2017-06-27 17:28:14,860 main.py:51] epoch 1281, training loss: 6704.15, average training loss: 7470.35, base loss: 9201.34
[INFO 2017-06-27 17:28:16,075 main.py:51] epoch 1282, training loss: 7148.56, average training loss: 7468.64, base loss: 9201.62
[INFO 2017-06-27 17:28:17,291 main.py:51] epoch 1283, training loss: 6946.81, average training loss: 7467.90, base loss: 9203.14
[INFO 2017-06-27 17:28:18,506 main.py:51] epoch 1284, training loss: 8871.80, average training loss: 7469.03, base loss: 9205.42
[INFO 2017-06-27 17:28:19,720 main.py:51] epoch 1285, training loss: 6335.28, average training loss: 7467.80, base loss: 9205.60
[INFO 2017-06-27 17:28:20,933 main.py:51] epoch 1286, training loss: 6469.72, average training loss: 7467.30, base loss: 9206.79
[INFO 2017-06-27 17:28:22,150 main.py:51] epoch 1287, training loss: 7156.62, average training loss: 7465.76, base loss: 9207.30
[INFO 2017-06-27 17:28:23,365 main.py:51] epoch 1288, training loss: 6082.12, average training loss: 7464.84, base loss: 9207.68
[INFO 2017-06-27 17:28:24,584 main.py:51] epoch 1289, training loss: 6470.24, average training loss: 7463.48, base loss: 9207.44
[INFO 2017-06-27 17:28:25,796 main.py:51] epoch 1290, training loss: 7009.93, average training loss: 7462.71, base loss: 9208.10
[INFO 2017-06-27 17:28:27,008 main.py:51] epoch 1291, training loss: 6777.52, average training loss: 7461.55, base loss: 9208.65
[INFO 2017-06-27 17:28:28,220 main.py:51] epoch 1292, training loss: 6361.37, average training loss: 7459.18, base loss: 9207.37
[INFO 2017-06-27 17:28:29,438 main.py:51] epoch 1293, training loss: 6106.31, average training loss: 7456.01, base loss: 9205.22
[INFO 2017-06-27 17:28:30,651 main.py:51] epoch 1294, training loss: 6573.81, average training loss: 7454.83, base loss: 9205.53
[INFO 2017-06-27 17:28:31,861 main.py:51] epoch 1295, training loss: 5941.47, average training loss: 7452.57, base loss: 9204.12
[INFO 2017-06-27 17:28:33,075 main.py:51] epoch 1296, training loss: 6882.21, average training loss: 7451.56, base loss: 9204.82
[INFO 2017-06-27 17:28:34,292 main.py:51] epoch 1297, training loss: 6490.74, average training loss: 7449.89, base loss: 9204.73
[INFO 2017-06-27 17:28:35,507 main.py:51] epoch 1298, training loss: 6745.35, average training loss: 7447.92, base loss: 9204.76
[INFO 2017-06-27 17:28:36,722 main.py:51] epoch 1299, training loss: 6502.71, average training loss: 7446.30, base loss: 9204.54
[INFO 2017-06-27 17:28:36,722 main.py:53] epoch 1299, testing
[INFO 2017-06-27 17:28:41,430 main.py:105] average testing loss: 6603.57, base loss: 8818.53
[INFO 2017-06-27 17:28:41,430 main.py:106] improve_loss: 2214.96, improve_percent: 0.25
[INFO 2017-06-27 17:28:41,431 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 17:28:41,443 main.py:76] current best improved percent: 0.25
[INFO 2017-06-27 17:28:42,653 main.py:51] epoch 1300, training loss: 6585.48, average training loss: 7445.77, base loss: 9205.79
[INFO 2017-06-27 17:28:43,869 main.py:51] epoch 1301, training loss: 6647.29, average training loss: 7441.62, base loss: 9203.72
[INFO 2017-06-27 17:28:45,087 main.py:51] epoch 1302, training loss: 6670.74, average training loss: 7440.49, base loss: 9204.57
[INFO 2017-06-27 17:28:46,302 main.py:51] epoch 1303, training loss: 6693.24, average training loss: 7439.52, base loss: 9205.45
[INFO 2017-06-27 17:28:47,519 main.py:51] epoch 1304, training loss: 6965.08, average training loss: 7435.41, base loss: 9203.13
[INFO 2017-06-27 17:28:48,732 main.py:51] epoch 1305, training loss: 6596.92, average training loss: 7433.91, base loss: 9202.95
[INFO 2017-06-27 17:28:49,946 main.py:51] epoch 1306, training loss: 7202.39, average training loss: 7433.00, base loss: 9204.05
[INFO 2017-06-27 17:28:51,159 main.py:51] epoch 1307, training loss: 6389.80, average training loss: 7432.15, base loss: 9204.85
[INFO 2017-06-27 17:28:52,375 main.py:51] epoch 1308, training loss: 10023.73, average training loss: 7434.78, base loss: 9209.87
[INFO 2017-06-27 17:28:53,590 main.py:51] epoch 1309, training loss: 6455.75, average training loss: 7432.81, base loss: 9209.00
[INFO 2017-06-27 17:28:54,803 main.py:51] epoch 1310, training loss: 6331.27, average training loss: 7430.69, base loss: 9208.09
[INFO 2017-06-27 17:28:56,012 main.py:51] epoch 1311, training loss: 6928.32, average training loss: 7429.50, base loss: 9209.09
[INFO 2017-06-27 17:28:57,227 main.py:51] epoch 1312, training loss: 6835.38, average training loss: 7428.03, base loss: 9209.54
[INFO 2017-06-27 17:28:58,441 main.py:51] epoch 1313, training loss: 6840.56, average training loss: 7427.05, base loss: 9210.04
[INFO 2017-06-27 17:28:59,655 main.py:51] epoch 1314, training loss: 7020.19, average training loss: 7422.79, base loss: 9207.92
[INFO 2017-06-27 17:29:00,870 main.py:51] epoch 1315, training loss: 6722.05, average training loss: 7422.25, base loss: 9208.96
[INFO 2017-06-27 17:29:02,087 main.py:51] epoch 1316, training loss: 9868.90, average training loss: 7424.00, base loss: 9211.80
[INFO 2017-06-27 17:29:03,296 main.py:51] epoch 1317, training loss: 6213.51, average training loss: 7418.78, base loss: 9207.75
[INFO 2017-06-27 17:29:04,509 main.py:51] epoch 1318, training loss: 6930.86, average training loss: 7417.24, base loss: 9208.21
[INFO 2017-06-27 17:29:05,719 main.py:51] epoch 1319, training loss: 8859.49, average training loss: 7418.35, base loss: 9210.65
[INFO 2017-06-27 17:29:06,930 main.py:51] epoch 1320, training loss: 6282.02, average training loss: 7417.13, base loss: 9210.93
[INFO 2017-06-27 17:29:08,144 main.py:51] epoch 1321, training loss: 6456.89, average training loss: 7415.37, base loss: 9210.62
[INFO 2017-06-27 17:29:09,359 main.py:51] epoch 1322, training loss: 6858.92, average training loss: 7413.31, base loss: 9210.00
[INFO 2017-06-27 17:29:10,574 main.py:51] epoch 1323, training loss: 7083.05, average training loss: 7412.58, base loss: 9211.40
[INFO 2017-06-27 17:29:11,782 main.py:51] epoch 1324, training loss: 7079.91, average training loss: 7412.47, base loss: 9213.01
[INFO 2017-06-27 17:29:12,997 main.py:51] epoch 1325, training loss: 6297.87, average training loss: 7411.31, base loss: 9213.04
[INFO 2017-06-27 17:29:14,210 main.py:51] epoch 1326, training loss: 6856.36, average training loss: 7409.61, base loss: 9212.83
[INFO 2017-06-27 17:29:15,422 main.py:51] epoch 1327, training loss: 6737.31, average training loss: 7408.27, base loss: 9212.60
[INFO 2017-06-27 17:29:16,638 main.py:51] epoch 1328, training loss: 6690.26, average training loss: 7406.90, base loss: 9212.86
[INFO 2017-06-27 17:29:17,855 main.py:51] epoch 1329, training loss: 6612.90, average training loss: 7405.27, base loss: 9212.43
[INFO 2017-06-27 17:29:19,069 main.py:51] epoch 1330, training loss: 6833.75, average training loss: 7403.36, base loss: 9212.05
[INFO 2017-06-27 17:29:20,286 main.py:51] epoch 1331, training loss: 6498.47, average training loss: 7398.11, base loss: 9208.13
[INFO 2017-06-27 17:29:21,501 main.py:51] epoch 1332, training loss: 7079.34, average training loss: 7396.54, base loss: 9208.02
[INFO 2017-06-27 17:29:22,717 main.py:51] epoch 1333, training loss: 6263.46, average training loss: 7393.82, base loss: 9205.69
[INFO 2017-06-27 17:29:23,933 main.py:51] epoch 1334, training loss: 6335.07, average training loss: 7392.60, base loss: 9206.18
[INFO 2017-06-27 17:29:25,143 main.py:51] epoch 1335, training loss: 6123.43, average training loss: 7389.65, base loss: 9203.77
[INFO 2017-06-27 17:29:26,358 main.py:51] epoch 1336, training loss: 6794.69, average training loss: 7388.10, base loss: 9203.32
[INFO 2017-06-27 17:29:27,573 main.py:51] epoch 1337, training loss: 6952.93, average training loss: 7383.26, base loss: 9199.99
[INFO 2017-06-27 17:29:28,786 main.py:51] epoch 1338, training loss: 6929.05, average training loss: 7382.43, base loss: 9200.70
[INFO 2017-06-27 17:29:30,001 main.py:51] epoch 1339, training loss: 6215.15, average training loss: 7380.72, base loss: 9200.45
[INFO 2017-06-27 17:29:31,219 main.py:51] epoch 1340, training loss: 6204.51, average training loss: 7378.24, base loss: 9198.54
[INFO 2017-06-27 17:29:32,436 main.py:51] epoch 1341, training loss: 6758.99, average training loss: 7376.61, base loss: 9198.29
[INFO 2017-06-27 17:29:33,650 main.py:51] epoch 1342, training loss: 5965.53, average training loss: 7373.93, base loss: 9196.20
[INFO 2017-06-27 17:29:34,865 main.py:51] epoch 1343, training loss: 5977.85, average training loss: 7368.16, base loss: 9191.14
[INFO 2017-06-27 17:29:36,079 main.py:51] epoch 1344, training loss: 5880.07, average training loss: 7365.07, base loss: 9188.47
[INFO 2017-06-27 17:29:37,294 main.py:51] epoch 1345, training loss: 6181.02, average training loss: 7364.10, base loss: 9188.84
[INFO 2017-06-27 17:29:38,503 main.py:51] epoch 1346, training loss: 7165.02, average training loss: 7362.67, base loss: 9188.45
[INFO 2017-06-27 17:29:39,714 main.py:51] epoch 1347, training loss: 7354.43, average training loss: 7359.01, base loss: 9186.89
[INFO 2017-06-27 17:29:40,925 main.py:51] epoch 1348, training loss: 6775.71, average training loss: 7358.15, base loss: 9188.06
[INFO 2017-06-27 17:29:42,137 main.py:51] epoch 1349, training loss: 6838.94, average training loss: 7357.34, base loss: 9189.16
[INFO 2017-06-27 17:29:43,353 main.py:51] epoch 1350, training loss: 7273.04, average training loss: 7356.40, base loss: 9190.11
[INFO 2017-06-27 17:29:44,562 main.py:51] epoch 1351, training loss: 6314.67, average training loss: 7354.01, base loss: 9188.22
[INFO 2017-06-27 17:29:45,776 main.py:51] epoch 1352, training loss: 6973.81, average training loss: 7352.95, base loss: 9188.67
[INFO 2017-06-27 17:29:46,986 main.py:51] epoch 1353, training loss: 6337.39, average training loss: 7350.84, base loss: 9187.17
[INFO 2017-06-27 17:29:48,199 main.py:51] epoch 1354, training loss: 7233.43, average training loss: 7350.18, base loss: 9188.15
[INFO 2017-06-27 17:29:49,414 main.py:51] epoch 1355, training loss: 6235.73, average training loss: 7349.28, base loss: 9188.88
[INFO 2017-06-27 17:29:50,626 main.py:51] epoch 1356, training loss: 6540.23, average training loss: 7347.18, base loss: 9187.65
[INFO 2017-06-27 17:29:51,834 main.py:51] epoch 1357, training loss: 6488.78, average training loss: 7345.38, base loss: 9187.38
[INFO 2017-06-27 17:29:53,042 main.py:51] epoch 1358, training loss: 6605.93, average training loss: 7344.85, base loss: 9188.21
[INFO 2017-06-27 17:29:54,256 main.py:51] epoch 1359, training loss: 6237.73, average training loss: 7343.52, base loss: 9188.45
[INFO 2017-06-27 17:29:55,467 main.py:51] epoch 1360, training loss: 6639.48, average training loss: 7342.87, base loss: 9189.51
[INFO 2017-06-27 17:29:56,678 main.py:51] epoch 1361, training loss: 6434.76, average training loss: 7342.07, base loss: 9190.11
[INFO 2017-06-27 17:29:57,884 main.py:51] epoch 1362, training loss: 6140.18, average training loss: 7339.10, base loss: 9187.92
[INFO 2017-06-27 17:29:59,098 main.py:51] epoch 1363, training loss: 6936.52, average training loss: 7338.53, base loss: 9188.93
[INFO 2017-06-27 17:30:00,313 main.py:51] epoch 1364, training loss: 6574.57, average training loss: 7337.51, base loss: 9189.72
[INFO 2017-06-27 17:30:01,523 main.py:51] epoch 1365, training loss: 6446.20, average training loss: 7335.73, base loss: 9189.12
[INFO 2017-06-27 17:30:02,738 main.py:51] epoch 1366, training loss: 6350.44, average training loss: 7334.59, base loss: 9189.28
[INFO 2017-06-27 17:30:03,949 main.py:51] epoch 1367, training loss: 6732.28, average training loss: 7330.65, base loss: 9187.07
[INFO 2017-06-27 17:30:05,157 main.py:51] epoch 1368, training loss: 6150.26, average training loss: 7329.36, base loss: 9187.13
[INFO 2017-06-27 17:30:06,371 main.py:51] epoch 1369, training loss: 6576.37, average training loss: 7327.73, base loss: 9187.03
[INFO 2017-06-27 17:30:07,583 main.py:51] epoch 1370, training loss: 9649.13, average training loss: 7329.79, base loss: 9190.19
[INFO 2017-06-27 17:30:08,795 main.py:51] epoch 1371, training loss: 6245.40, average training loss: 7327.80, base loss: 9189.46
[INFO 2017-06-27 17:30:10,007 main.py:51] epoch 1372, training loss: 6277.81, average training loss: 7325.56, base loss: 9188.05
[INFO 2017-06-27 17:30:11,216 main.py:51] epoch 1373, training loss: 6545.90, average training loss: 7324.12, base loss: 9187.95
[INFO 2017-06-27 17:30:12,431 main.py:51] epoch 1374, training loss: 6853.48, average training loss: 7323.54, base loss: 9189.20
[INFO 2017-06-27 17:30:13,647 main.py:51] epoch 1375, training loss: 6675.93, average training loss: 7322.05, base loss: 9189.09
[INFO 2017-06-27 17:30:14,863 main.py:51] epoch 1376, training loss: 6774.59, average training loss: 7320.24, base loss: 9188.45
[INFO 2017-06-27 17:30:16,079 main.py:51] epoch 1377, training loss: 6712.83, average training loss: 7318.63, base loss: 9188.36
[INFO 2017-06-27 17:30:17,295 main.py:51] epoch 1378, training loss: 6329.48, average training loss: 7317.47, base loss: 9188.57
[INFO 2017-06-27 17:30:18,504 main.py:51] epoch 1379, training loss: 6158.49, average training loss: 7315.33, base loss: 9187.34
[INFO 2017-06-27 17:30:19,720 main.py:51] epoch 1380, training loss: 7309.33, average training loss: 7314.82, base loss: 9188.44
[INFO 2017-06-27 17:30:20,928 main.py:51] epoch 1381, training loss: 6003.29, average training loss: 7313.22, base loss: 9187.21
[INFO 2017-06-27 17:30:22,135 main.py:51] epoch 1382, training loss: 6367.01, average training loss: 7310.86, base loss: 9186.03
[INFO 2017-06-27 17:30:23,350 main.py:51] epoch 1383, training loss: 6709.32, average training loss: 7310.00, base loss: 9186.40
[INFO 2017-06-27 17:30:24,564 main.py:51] epoch 1384, training loss: 6848.98, average training loss: 7308.94, base loss: 9186.89
[INFO 2017-06-27 17:30:25,778 main.py:51] epoch 1385, training loss: 6863.10, average training loss: 7308.27, base loss: 9188.11
[INFO 2017-06-27 17:30:26,989 main.py:51] epoch 1386, training loss: 6621.30, average training loss: 7306.74, base loss: 9187.82
[INFO 2017-06-27 17:30:28,202 main.py:51] epoch 1387, training loss: 6367.19, average training loss: 7304.37, base loss: 9186.08
[INFO 2017-06-27 17:30:29,418 main.py:51] epoch 1388, training loss: 7695.95, average training loss: 7305.09, base loss: 9189.56
[INFO 2017-06-27 17:30:30,629 main.py:51] epoch 1389, training loss: 6627.30, average training loss: 7304.13, base loss: 9190.66
[INFO 2017-06-27 17:30:31,842 main.py:51] epoch 1390, training loss: 9699.81, average training loss: 7306.67, base loss: 9194.00
[INFO 2017-06-27 17:30:33,057 main.py:51] epoch 1391, training loss: 6728.03, average training loss: 7304.99, base loss: 9193.32
[INFO 2017-06-27 17:30:34,269 main.py:51] epoch 1392, training loss: 6811.97, average training loss: 7302.47, base loss: 9191.77
[INFO 2017-06-27 17:30:35,499 main.py:51] epoch 1393, training loss: 5894.46, average training loss: 7300.15, base loss: 9190.06
[INFO 2017-06-27 17:30:36,712 main.py:51] epoch 1394, training loss: 10614.09, average training loss: 7303.14, base loss: 9194.65
[INFO 2017-06-27 17:30:37,922 main.py:51] epoch 1395, training loss: 6856.95, average training loss: 7302.00, base loss: 9194.93
[INFO 2017-06-27 17:30:39,135 main.py:51] epoch 1396, training loss: 6208.73, average training loss: 7300.36, base loss: 9193.93
[INFO 2017-06-27 17:30:40,352 main.py:51] epoch 1397, training loss: 10429.36, average training loss: 7302.56, base loss: 9197.61
[INFO 2017-06-27 17:30:41,566 main.py:51] epoch 1398, training loss: 6080.35, average training loss: 7300.66, base loss: 9196.08
[INFO 2017-06-27 17:30:42,776 main.py:51] epoch 1399, training loss: 6603.82, average training loss: 7296.12, base loss: 9192.28
[INFO 2017-06-27 17:30:42,776 main.py:53] epoch 1399, testing
[INFO 2017-06-27 17:30:47,499 main.py:105] average testing loss: 6881.43, base loss: 9256.82
[INFO 2017-06-27 17:30:47,499 main.py:106] improve_loss: 2375.39, improve_percent: 0.26
[INFO 2017-06-27 17:30:47,500 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 17:30:47,513 main.py:76] current best improved percent: 0.26
[INFO 2017-06-27 17:30:48,730 main.py:51] epoch 1400, training loss: 6927.44, average training loss: 7295.60, base loss: 9192.91
[INFO 2017-06-27 17:30:49,947 main.py:51] epoch 1401, training loss: 6409.86, average training loss: 7289.90, base loss: 9188.40
[INFO 2017-06-27 17:30:51,159 main.py:51] epoch 1402, training loss: 6049.80, average training loss: 7287.86, base loss: 9187.40
[INFO 2017-06-27 17:30:52,375 main.py:51] epoch 1403, training loss: 6436.14, average training loss: 7286.39, base loss: 9186.68
[INFO 2017-06-27 17:30:53,594 main.py:51] epoch 1404, training loss: 6985.97, average training loss: 7283.86, base loss: 9185.75
[INFO 2017-06-27 17:30:54,808 main.py:51] epoch 1405, training loss: 6714.93, average training loss: 7282.83, base loss: 9185.89
[INFO 2017-06-27 17:30:56,026 main.py:51] epoch 1406, training loss: 5995.06, average training loss: 7280.11, base loss: 9183.65
[INFO 2017-06-27 17:30:57,241 main.py:51] epoch 1407, training loss: 6378.44, average training loss: 7278.03, base loss: 9182.74
[INFO 2017-06-27 17:30:58,455 main.py:51] epoch 1408, training loss: 6711.51, average training loss: 7277.16, base loss: 9183.42
[INFO 2017-06-27 17:30:59,669 main.py:51] epoch 1409, training loss: 6171.87, average training loss: 7272.10, base loss: 9179.44
[INFO 2017-06-27 17:31:00,878 main.py:51] epoch 1410, training loss: 6033.64, average training loss: 7266.89, base loss: 9174.74
[INFO 2017-06-27 17:31:02,093 main.py:51] epoch 1411, training loss: 6304.97, average training loss: 7265.63, base loss: 9174.34
[INFO 2017-06-27 17:31:03,308 main.py:51] epoch 1412, training loss: 6532.55, average training loss: 7264.16, base loss: 9174.33
[INFO 2017-06-27 17:31:04,520 main.py:51] epoch 1413, training loss: 7001.54, average training loss: 7263.81, base loss: 9175.95
[INFO 2017-06-27 17:31:05,736 main.py:51] epoch 1414, training loss: 6134.63, average training loss: 7261.43, base loss: 9174.91
[INFO 2017-06-27 17:31:06,952 main.py:51] epoch 1415, training loss: 6155.10, average training loss: 7260.36, base loss: 9175.15
[INFO 2017-06-27 17:31:08,165 main.py:51] epoch 1416, training loss: 10222.06, average training loss: 7263.05, base loss: 9179.61
[INFO 2017-06-27 17:31:09,379 main.py:51] epoch 1417, training loss: 6034.38, average training loss: 7260.78, base loss: 9178.15
[INFO 2017-06-27 17:31:10,592 main.py:51] epoch 1418, training loss: 6495.72, average training loss: 7260.56, base loss: 9179.61
[INFO 2017-06-27 17:31:11,804 main.py:51] epoch 1419, training loss: 7328.46, average training loss: 7260.57, base loss: 9181.83
[INFO 2017-06-27 17:31:13,016 main.py:51] epoch 1420, training loss: 6135.73, average training loss: 7258.71, base loss: 9180.56
[INFO 2017-06-27 17:31:14,229 main.py:51] epoch 1421, training loss: 6536.26, average training loss: 7257.20, base loss: 9180.59
[INFO 2017-06-27 17:31:15,437 main.py:51] epoch 1422, training loss: 6507.88, average training loss: 7255.62, base loss: 9179.92
[INFO 2017-06-27 17:31:16,648 main.py:51] epoch 1423, training loss: 7053.73, average training loss: 7255.32, base loss: 9181.26
[INFO 2017-06-27 17:31:17,859 main.py:51] epoch 1424, training loss: 6682.08, average training loss: 7254.40, base loss: 9181.80
[INFO 2017-06-27 17:31:19,073 main.py:51] epoch 1425, training loss: 6461.66, average training loss: 7253.01, base loss: 9181.36
[INFO 2017-06-27 17:31:20,285 main.py:51] epoch 1426, training loss: 6845.18, average training loss: 7252.02, base loss: 9181.99
[INFO 2017-06-27 17:31:21,498 main.py:51] epoch 1427, training loss: 6788.10, average training loss: 7251.50, base loss: 9182.92
[INFO 2017-06-27 17:31:22,713 main.py:51] epoch 1428, training loss: 6372.95, average training loss: 7250.61, base loss: 9183.48
[INFO 2017-06-27 17:31:23,925 main.py:51] epoch 1429, training loss: 6682.80, average training loss: 7250.16, base loss: 9184.79
[INFO 2017-06-27 17:31:25,138 main.py:51] epoch 1430, training loss: 7002.28, average training loss: 7249.45, base loss: 9186.24
[INFO 2017-06-27 17:31:26,351 main.py:51] epoch 1431, training loss: 9578.75, average training loss: 7251.22, base loss: 9189.07
[INFO 2017-06-27 17:31:27,564 main.py:51] epoch 1432, training loss: 6187.39, average training loss: 7249.14, base loss: 9187.67
[INFO 2017-06-27 17:31:28,774 main.py:51] epoch 1433, training loss: 6541.23, average training loss: 7248.58, base loss: 9188.92
[INFO 2017-06-27 17:31:29,987 main.py:51] epoch 1434, training loss: 9399.58, average training loss: 7249.82, base loss: 9190.35
[INFO 2017-06-27 17:31:31,197 main.py:51] epoch 1435, training loss: 6402.14, average training loss: 7249.15, base loss: 9190.95
[INFO 2017-06-27 17:31:32,411 main.py:51] epoch 1436, training loss: 6649.83, average training loss: 7248.51, base loss: 9191.97
[INFO 2017-06-27 17:31:33,621 main.py:51] epoch 1437, training loss: 6711.07, average training loss: 7247.89, base loss: 9192.80
[INFO 2017-06-27 17:31:34,837 main.py:51] epoch 1438, training loss: 6323.07, average training loss: 7246.30, base loss: 9192.29
[INFO 2017-06-27 17:31:36,049 main.py:51] epoch 1439, training loss: 6386.81, average training loss: 7244.95, base loss: 9192.07
[INFO 2017-06-27 17:31:37,263 main.py:51] epoch 1440, training loss: 6663.43, average training loss: 7240.60, base loss: 9189.12
[INFO 2017-06-27 17:31:38,473 main.py:51] epoch 1441, training loss: 6415.80, average training loss: 7239.39, base loss: 9188.53
[INFO 2017-06-27 17:31:39,685 main.py:51] epoch 1442, training loss: 6120.03, average training loss: 7237.65, base loss: 9188.12
[INFO 2017-06-27 17:31:40,901 main.py:51] epoch 1443, training loss: 6110.08, average training loss: 7236.69, base loss: 9188.76
[INFO 2017-06-27 17:31:42,112 main.py:51] epoch 1444, training loss: 6391.99, average training loss: 7235.77, base loss: 9188.70
[INFO 2017-06-27 17:31:43,326 main.py:51] epoch 1445, training loss: 10507.75, average training loss: 7238.12, base loss: 9192.51
[INFO 2017-06-27 17:31:44,543 main.py:51] epoch 1446, training loss: 6135.06, average training loss: 7237.20, base loss: 9192.54
[INFO 2017-06-27 17:31:45,756 main.py:51] epoch 1447, training loss: 6724.18, average training loss: 7237.07, base loss: 9193.73
[INFO 2017-06-27 17:31:46,969 main.py:51] epoch 1448, training loss: 6724.44, average training loss: 7235.71, base loss: 9194.04
[INFO 2017-06-27 17:31:48,196 main.py:51] epoch 1449, training loss: 6501.29, average training loss: 7234.63, base loss: 9194.13
[INFO 2017-06-27 17:31:49,406 main.py:51] epoch 1450, training loss: 6571.43, average training loss: 7233.92, base loss: 9195.01
[INFO 2017-06-27 17:31:50,619 main.py:51] epoch 1451, training loss: 7085.83, average training loss: 7232.32, base loss: 9194.17
[INFO 2017-06-27 17:31:51,834 main.py:51] epoch 1452, training loss: 6050.75, average training loss: 7230.30, base loss: 9192.98
[INFO 2017-06-27 17:31:53,047 main.py:51] epoch 1453, training loss: 9652.74, average training loss: 7231.97, base loss: 9195.57
[INFO 2017-06-27 17:31:54,258 main.py:51] epoch 1454, training loss: 6855.74, average training loss: 7230.59, base loss: 9195.24
[INFO 2017-06-27 17:31:55,470 main.py:51] epoch 1455, training loss: 5992.71, average training loss: 7228.73, base loss: 9194.46
[INFO 2017-06-27 17:31:56,682 main.py:51] epoch 1456, training loss: 5874.82, average training loss: 7227.40, base loss: 9193.70
[INFO 2017-06-27 17:31:57,895 main.py:51] epoch 1457, training loss: 6566.48, average training loss: 7226.49, base loss: 9194.87
[INFO 2017-06-27 17:31:59,108 main.py:51] epoch 1458, training loss: 6375.43, average training loss: 7224.72, base loss: 9194.27
[INFO 2017-06-27 17:32:00,325 main.py:51] epoch 1459, training loss: 6738.79, average training loss: 7222.99, base loss: 9193.73
[INFO 2017-06-27 17:32:01,539 main.py:51] epoch 1460, training loss: 6272.93, average training loss: 7221.47, base loss: 9192.43
[INFO 2017-06-27 17:32:02,753 main.py:51] epoch 1461, training loss: 6909.69, average training loss: 7221.82, base loss: 9194.72
[INFO 2017-06-27 17:32:03,966 main.py:51] epoch 1462, training loss: 5788.97, average training loss: 7220.29, base loss: 9194.08
[INFO 2017-06-27 17:32:05,177 main.py:51] epoch 1463, training loss: 6954.87, average training loss: 7219.36, base loss: 9194.22
[INFO 2017-06-27 17:32:06,391 main.py:51] epoch 1464, training loss: 6747.00, average training loss: 7218.19, base loss: 9194.74
[INFO 2017-06-27 17:32:07,604 main.py:51] epoch 1465, training loss: 6029.14, average training loss: 7217.03, base loss: 9194.25
[INFO 2017-06-27 17:32:08,813 main.py:51] epoch 1466, training loss: 6428.30, average training loss: 7216.31, base loss: 9194.77
[INFO 2017-06-27 17:32:10,024 main.py:51] epoch 1467, training loss: 10030.17, average training loss: 7218.56, base loss: 9197.96
[INFO 2017-06-27 17:32:11,239 main.py:51] epoch 1468, training loss: 6299.51, average training loss: 7213.41, base loss: 9193.51
[INFO 2017-06-27 17:32:12,453 main.py:51] epoch 1469, training loss: 6301.50, average training loss: 7212.07, base loss: 9193.05
[INFO 2017-06-27 17:32:13,666 main.py:51] epoch 1470, training loss: 6571.87, average training loss: 7211.29, base loss: 9193.69
[INFO 2017-06-27 17:32:14,882 main.py:51] epoch 1471, training loss: 6580.89, average training loss: 7210.16, base loss: 9193.95
[INFO 2017-06-27 17:32:16,091 main.py:51] epoch 1472, training loss: 6187.60, average training loss: 7208.60, base loss: 9193.12
[INFO 2017-06-27 17:32:17,306 main.py:51] epoch 1473, training loss: 6775.64, average training loss: 7208.15, base loss: 9194.40
[INFO 2017-06-27 17:32:18,522 main.py:51] epoch 1474, training loss: 6707.59, average training loss: 7207.46, base loss: 9195.18
[INFO 2017-06-27 17:32:19,738 main.py:51] epoch 1475, training loss: 6102.99, average training loss: 7206.47, base loss: 9194.98
[INFO 2017-06-27 17:32:20,951 main.py:51] epoch 1476, training loss: 6386.97, average training loss: 7205.53, base loss: 9194.81
[INFO 2017-06-27 17:32:22,162 main.py:51] epoch 1477, training loss: 6330.21, average training loss: 7204.47, base loss: 9195.18
[INFO 2017-06-27 17:32:23,378 main.py:51] epoch 1478, training loss: 6754.93, average training loss: 7203.99, base loss: 9195.67
[INFO 2017-06-27 17:32:24,593 main.py:51] epoch 1479, training loss: 6666.17, average training loss: 7203.97, base loss: 9197.01
[INFO 2017-06-27 17:32:25,809 main.py:51] epoch 1480, training loss: 6375.27, average training loss: 7202.25, base loss: 9196.21
[INFO 2017-06-27 17:32:27,025 main.py:51] epoch 1481, training loss: 9960.47, average training loss: 7203.76, base loss: 9198.88
[INFO 2017-06-27 17:32:28,239 main.py:51] epoch 1482, training loss: 6109.33, average training loss: 7202.49, base loss: 9198.50
[INFO 2017-06-27 17:32:29,448 main.py:51] epoch 1483, training loss: 6091.38, average training loss: 7201.04, base loss: 9198.16
[INFO 2017-06-27 17:32:30,662 main.py:51] epoch 1484, training loss: 6812.29, average training loss: 7200.56, base loss: 9199.42
[INFO 2017-06-27 17:32:31,875 main.py:51] epoch 1485, training loss: 6956.78, average training loss: 7200.01, base loss: 9200.77
[INFO 2017-06-27 17:32:33,089 main.py:51] epoch 1486, training loss: 6547.43, average training loss: 7199.40, base loss: 9201.81
[INFO 2017-06-27 17:32:34,302 main.py:51] epoch 1487, training loss: 7106.48, average training loss: 7197.96, base loss: 9202.32
[INFO 2017-06-27 17:32:35,510 main.py:51] epoch 1488, training loss: 7041.14, average training loss: 7197.76, base loss: 9203.76
[INFO 2017-06-27 17:32:36,726 main.py:51] epoch 1489, training loss: 9520.46, average training loss: 7200.15, base loss: 9207.61
[INFO 2017-06-27 17:32:37,938 main.py:51] epoch 1490, training loss: 6211.23, average training loss: 7199.11, base loss: 9207.87
[INFO 2017-06-27 17:32:39,156 main.py:51] epoch 1491, training loss: 6743.11, average training loss: 7197.53, base loss: 9206.69
[INFO 2017-06-27 17:32:40,370 main.py:51] epoch 1492, training loss: 6219.92, average training loss: 7195.93, base loss: 9206.45
[INFO 2017-06-27 17:32:41,584 main.py:51] epoch 1493, training loss: 7691.10, average training loss: 7196.68, base loss: 9209.37
[INFO 2017-06-27 17:32:42,793 main.py:51] epoch 1494, training loss: 6737.13, average training loss: 7195.37, base loss: 9208.83
[INFO 2017-06-27 17:32:44,007 main.py:51] epoch 1495, training loss: 6510.96, average training loss: 7190.42, base loss: 9204.93
[INFO 2017-06-27 17:32:45,221 main.py:51] epoch 1496, training loss: 6363.55, average training loss: 7189.80, base loss: 9205.05
[INFO 2017-06-27 17:32:46,433 main.py:51] epoch 1497, training loss: 9161.77, average training loss: 7191.28, base loss: 9207.29
[INFO 2017-06-27 17:32:47,644 main.py:51] epoch 1498, training loss: 6357.63, average training loss: 7190.09, base loss: 9206.83
[INFO 2017-06-27 17:32:48,857 main.py:51] epoch 1499, training loss: 5501.53, average training loss: 7187.82, base loss: 9204.39
[INFO 2017-06-27 17:32:48,858 main.py:53] epoch 1499, testing
[INFO 2017-06-27 17:32:53,546 main.py:105] average testing loss: 6513.07, base loss: 8940.85
[INFO 2017-06-27 17:32:53,546 main.py:106] improve_loss: 2427.77, improve_percent: 0.27
[INFO 2017-06-27 17:32:53,547 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 17:32:53,559 main.py:76] current best improved percent: 0.27
[INFO 2017-06-27 17:32:54,768 main.py:51] epoch 1500, training loss: 6485.73, average training loss: 7187.04, base loss: 9204.99
[INFO 2017-06-27 17:32:55,978 main.py:51] epoch 1501, training loss: 5616.22, average training loss: 7185.74, base loss: 9204.02
[INFO 2017-06-27 17:32:57,191 main.py:51] epoch 1502, training loss: 7140.56, average training loss: 7184.21, base loss: 9203.65
[INFO 2017-06-27 17:32:58,402 main.py:51] epoch 1503, training loss: 6699.81, average training loss: 7183.16, base loss: 9204.08
[INFO 2017-06-27 17:32:59,617 main.py:51] epoch 1504, training loss: 6638.51, average training loss: 7178.73, base loss: 9201.23
[INFO 2017-06-27 17:33:00,826 main.py:51] epoch 1505, training loss: 6063.36, average training loss: 7176.63, base loss: 9199.08
[INFO 2017-06-27 17:33:02,042 main.py:51] epoch 1506, training loss: 6081.85, average training loss: 7171.87, base loss: 9194.75
[INFO 2017-06-27 17:33:03,253 main.py:51] epoch 1507, training loss: 7011.38, average training loss: 7170.58, base loss: 9194.90
[INFO 2017-06-27 17:33:04,466 main.py:51] epoch 1508, training loss: 6216.80, average training loss: 7169.71, base loss: 9195.41
[INFO 2017-06-27 17:33:05,679 main.py:51] epoch 1509, training loss: 6045.36, average training loss: 7167.08, base loss: 9192.94
[INFO 2017-06-27 17:33:06,892 main.py:51] epoch 1510, training loss: 6444.39, average training loss: 7166.35, base loss: 9194.03
[INFO 2017-06-27 17:33:08,102 main.py:51] epoch 1511, training loss: 6474.96, average training loss: 7165.81, base loss: 9195.23
[INFO 2017-06-27 17:33:09,315 main.py:51] epoch 1512, training loss: 5942.18, average training loss: 7163.79, base loss: 9193.41
[INFO 2017-06-27 17:33:10,524 main.py:51] epoch 1513, training loss: 6761.55, average training loss: 7162.56, base loss: 9193.55
[INFO 2017-06-27 17:33:11,734 main.py:51] epoch 1514, training loss: 6013.59, average training loss: 7161.37, base loss: 9193.68
[INFO 2017-06-27 17:33:12,942 main.py:51] epoch 1515, training loss: 6656.11, average training loss: 7161.03, base loss: 9194.71
[INFO 2017-06-27 17:33:14,156 main.py:51] epoch 1516, training loss: 7097.51, average training loss: 7160.77, base loss: 9195.57
[INFO 2017-06-27 17:33:15,370 main.py:51] epoch 1517, training loss: 6730.40, average training loss: 7157.04, base loss: 9193.55
[INFO 2017-06-27 17:33:16,583 main.py:51] epoch 1518, training loss: 6540.31, average training loss: 7155.99, base loss: 9193.86
[INFO 2017-06-27 17:33:17,795 main.py:51] epoch 1519, training loss: 6324.40, average training loss: 7155.25, base loss: 9193.77
[INFO 2017-06-27 17:33:19,007 main.py:51] epoch 1520, training loss: 6769.92, average training loss: 7154.26, base loss: 9194.06
[INFO 2017-06-27 17:33:20,220 main.py:51] epoch 1521, training loss: 6201.99, average training loss: 7151.84, base loss: 9192.36
[INFO 2017-06-27 17:33:21,429 main.py:51] epoch 1522, training loss: 6377.11, average training loss: 7151.00, base loss: 9192.74
[INFO 2017-06-27 17:33:22,641 main.py:51] epoch 1523, training loss: 6442.76, average training loss: 7150.70, base loss: 9193.89
[INFO 2017-06-27 17:33:23,856 main.py:51] epoch 1524, training loss: 6683.17, average training loss: 7149.93, base loss: 9194.77
[INFO 2017-06-27 17:33:25,067 main.py:51] epoch 1525, training loss: 6616.43, average training loss: 7149.07, base loss: 9195.71
[INFO 2017-06-27 17:33:26,279 main.py:51] epoch 1526, training loss: 6416.74, average training loss: 7148.01, base loss: 9196.09
[INFO 2017-06-27 17:33:27,492 main.py:51] epoch 1527, training loss: 6339.43, average training loss: 7147.23, base loss: 9196.64
[INFO 2017-06-27 17:33:28,706 main.py:51] epoch 1528, training loss: 6698.64, average training loss: 7146.30, base loss: 9197.01
[INFO 2017-06-27 17:33:29,913 main.py:51] epoch 1529, training loss: 6818.97, average training loss: 7145.81, base loss: 9197.75
[INFO 2017-06-27 17:33:31,126 main.py:51] epoch 1530, training loss: 9406.96, average training loss: 7148.19, base loss: 9202.11
[INFO 2017-06-27 17:33:32,339 main.py:51] epoch 1531, training loss: 6516.18, average training loss: 7147.16, base loss: 9202.05
[INFO 2017-06-27 17:33:33,552 main.py:51] epoch 1532, training loss: 6008.37, average training loss: 7145.27, base loss: 9200.44
[INFO 2017-06-27 17:33:34,766 main.py:51] epoch 1533, training loss: 6546.47, average training loss: 7144.48, base loss: 9200.65
[INFO 2017-06-27 17:33:35,974 main.py:51] epoch 1534, training loss: 6197.38, average training loss: 7143.13, base loss: 9199.71
[INFO 2017-06-27 17:33:37,184 main.py:51] epoch 1535, training loss: 9248.86, average training loss: 7144.73, base loss: 9202.93
[INFO 2017-06-27 17:33:38,399 main.py:51] epoch 1536, training loss: 6568.01, average training loss: 7144.37, base loss: 9204.29
[INFO 2017-06-27 17:33:39,607 main.py:51] epoch 1537, training loss: 5968.95, average training loss: 7142.89, base loss: 9202.90
[INFO 2017-06-27 17:33:40,820 main.py:51] epoch 1538, training loss: 6892.48, average training loss: 7142.08, base loss: 9203.34
[INFO 2017-06-27 17:33:42,033 main.py:51] epoch 1539, training loss: 6208.04, average training loss: 7141.13, base loss: 9203.81
[INFO 2017-06-27 17:33:43,247 main.py:51] epoch 1540, training loss: 6778.01, average training loss: 7137.25, base loss: 9201.24
[INFO 2017-06-27 17:33:44,459 main.py:51] epoch 1541, training loss: 9222.50, average training loss: 7139.89, base loss: 9205.60
[INFO 2017-06-27 17:33:45,669 main.py:51] epoch 1542, training loss: 6378.35, average training loss: 7134.69, base loss: 9200.86
[INFO 2017-06-27 17:33:46,881 main.py:51] epoch 1543, training loss: 6899.51, average training loss: 7134.74, base loss: 9202.35
[INFO 2017-06-27 17:33:48,092 main.py:51] epoch 1544, training loss: 6706.13, average training loss: 7133.47, base loss: 9201.97
[INFO 2017-06-27 17:33:49,305 main.py:51] epoch 1545, training loss: 6213.82, average training loss: 7132.29, base loss: 9201.72
[INFO 2017-06-27 17:33:50,520 main.py:51] epoch 1546, training loss: 6860.32, average training loss: 7131.69, base loss: 9201.79
[INFO 2017-06-27 17:33:51,733 main.py:51] epoch 1547, training loss: 6253.51, average training loss: 7130.84, base loss: 9201.50
[INFO 2017-06-27 17:33:52,946 main.py:51] epoch 1548, training loss: 6942.08, average training loss: 7130.84, base loss: 9203.28
[INFO 2017-06-27 17:33:54,157 main.py:51] epoch 1549, training loss: 5951.16, average training loss: 7128.58, base loss: 9201.07
[INFO 2017-06-27 17:33:55,372 main.py:51] epoch 1550, training loss: 6168.66, average training loss: 7127.59, base loss: 9201.40
[INFO 2017-06-27 17:33:56,583 main.py:51] epoch 1551, training loss: 7836.24, average training loss: 7127.92, base loss: 9203.31
[INFO 2017-06-27 17:33:57,792 main.py:51] epoch 1552, training loss: 6344.49, average training loss: 7127.09, base loss: 9203.56
[INFO 2017-06-27 17:33:59,006 main.py:51] epoch 1553, training loss: 6682.81, average training loss: 7125.81, base loss: 9203.20
[INFO 2017-06-27 17:34:00,221 main.py:51] epoch 1554, training loss: 6707.89, average training loss: 7123.75, base loss: 9202.19
[INFO 2017-06-27 17:34:01,436 main.py:51] epoch 1555, training loss: 6414.41, average training loss: 7122.46, base loss: 9201.56
[INFO 2017-06-27 17:34:02,650 main.py:51] epoch 1556, training loss: 7122.46, average training loss: 7123.15, base loss: 9204.24
[INFO 2017-06-27 17:34:03,863 main.py:51] epoch 1557, training loss: 6217.35, average training loss: 7123.05, base loss: 9205.09
[INFO 2017-06-27 17:34:05,074 main.py:51] epoch 1558, training loss: 6592.67, average training loss: 7122.29, base loss: 9205.14
[INFO 2017-06-27 17:34:06,286 main.py:51] epoch 1559, training loss: 6514.58, average training loss: 7120.90, base loss: 9204.71
[INFO 2017-06-27 17:34:07,500 main.py:51] epoch 1560, training loss: 7039.77, average training loss: 7121.06, base loss: 9207.29
[INFO 2017-06-27 17:34:08,717 main.py:51] epoch 1561, training loss: 6664.04, average training loss: 7119.64, base loss: 9206.90
[INFO 2017-06-27 17:34:09,928 main.py:51] epoch 1562, training loss: 6526.04, average training loss: 7119.29, base loss: 9208.37
[INFO 2017-06-27 17:34:11,142 main.py:51] epoch 1563, training loss: 6526.87, average training loss: 7118.46, base loss: 9208.59
[INFO 2017-06-27 17:34:12,357 main.py:51] epoch 1564, training loss: 6462.20, average training loss: 7117.13, base loss: 9207.99
[INFO 2017-06-27 17:34:13,570 main.py:51] epoch 1565, training loss: 6235.40, average training loss: 7115.81, base loss: 9207.31
[INFO 2017-06-27 17:34:14,782 main.py:51] epoch 1566, training loss: 10079.09, average training loss: 7118.55, base loss: 9211.19
[INFO 2017-06-27 17:34:15,992 main.py:51] epoch 1567, training loss: 6254.92, average training loss: 7117.61, base loss: 9210.94
[INFO 2017-06-27 17:34:17,204 main.py:51] epoch 1568, training loss: 6740.94, average training loss: 7116.46, base loss: 9211.29
[INFO 2017-06-27 17:34:18,416 main.py:51] epoch 1569, training loss: 6586.96, average training loss: 7116.18, base loss: 9212.75
[INFO 2017-06-27 17:34:19,628 main.py:51] epoch 1570, training loss: 5791.67, average training loss: 7114.77, base loss: 9211.88
[INFO 2017-06-27 17:34:20,841 main.py:51] epoch 1571, training loss: 9121.98, average training loss: 7116.56, base loss: 9215.41
[INFO 2017-06-27 17:34:22,054 main.py:51] epoch 1572, training loss: 6310.41, average training loss: 7110.86, base loss: 9210.02
[INFO 2017-06-27 17:34:23,265 main.py:51] epoch 1573, training loss: 7003.63, average training loss: 7109.89, base loss: 9210.59
[INFO 2017-06-27 17:34:24,480 main.py:51] epoch 1574, training loss: 6504.64, average training loss: 7109.30, base loss: 9211.24
[INFO 2017-06-27 17:34:25,694 main.py:51] epoch 1575, training loss: 6446.12, average training loss: 7108.25, base loss: 9210.91
[INFO 2017-06-27 17:34:26,906 main.py:51] epoch 1576, training loss: 7004.04, average training loss: 7107.81, base loss: 9211.90
[INFO 2017-06-27 17:34:28,123 main.py:51] epoch 1577, training loss: 6570.16, average training loss: 7107.67, base loss: 9213.67
[INFO 2017-06-27 17:34:29,335 main.py:51] epoch 1578, training loss: 6555.68, average training loss: 7104.22, base loss: 9211.68
[INFO 2017-06-27 17:34:30,547 main.py:51] epoch 1579, training loss: 6416.20, average training loss: 7103.37, base loss: 9211.53
[INFO 2017-06-27 17:34:31,759 main.py:51] epoch 1580, training loss: 6035.59, average training loss: 7102.20, base loss: 9210.59
[INFO 2017-06-27 17:34:32,974 main.py:51] epoch 1581, training loss: 7401.10, average training loss: 7101.44, base loss: 9211.79
[INFO 2017-06-27 17:34:34,185 main.py:51] epoch 1582, training loss: 9764.24, average training loss: 7104.06, base loss: 9215.45
[INFO 2017-06-27 17:34:35,395 main.py:51] epoch 1583, training loss: 6241.28, average training loss: 7103.13, base loss: 9215.16
[INFO 2017-06-27 17:34:36,608 main.py:51] epoch 1584, training loss: 6326.23, average training loss: 7101.54, base loss: 9214.16
[INFO 2017-06-27 17:34:37,819 main.py:51] epoch 1585, training loss: 6546.99, average training loss: 7099.95, base loss: 9213.51
[INFO 2017-06-27 17:34:39,031 main.py:51] epoch 1586, training loss: 6986.90, average training loss: 7099.66, base loss: 9214.12
[INFO 2017-06-27 17:34:40,240 main.py:51] epoch 1587, training loss: 6222.06, average training loss: 7098.01, base loss: 9213.34
[INFO 2017-06-27 17:34:41,455 main.py:51] epoch 1588, training loss: 6753.89, average training loss: 7097.36, base loss: 9213.88
[INFO 2017-06-27 17:34:42,670 main.py:51] epoch 1589, training loss: 7157.50, average training loss: 7097.88, base loss: 9216.54
[INFO 2017-06-27 17:34:43,885 main.py:51] epoch 1590, training loss: 7245.88, average training loss: 7098.20, base loss: 9218.38
[INFO 2017-06-27 17:34:45,096 main.py:51] epoch 1591, training loss: 6338.98, average training loss: 7097.57, base loss: 9218.86
[INFO 2017-06-27 17:34:46,311 main.py:51] epoch 1592, training loss: 6525.25, average training loss: 7097.19, base loss: 9219.48
[INFO 2017-06-27 17:34:47,527 main.py:51] epoch 1593, training loss: 6210.96, average training loss: 7092.65, base loss: 9215.75
[INFO 2017-06-27 17:34:48,738 main.py:51] epoch 1594, training loss: 6008.68, average training loss: 7090.66, base loss: 9214.36
[INFO 2017-06-27 17:34:49,952 main.py:51] epoch 1595, training loss: 7236.76, average training loss: 7091.13, base loss: 9217.63
[INFO 2017-06-27 17:34:51,162 main.py:51] epoch 1596, training loss: 6855.95, average training loss: 7091.54, base loss: 9219.57
[INFO 2017-06-27 17:34:52,376 main.py:51] epoch 1597, training loss: 5981.01, average training loss: 7089.84, base loss: 9218.43
[INFO 2017-06-27 17:34:53,590 main.py:51] epoch 1598, training loss: 6639.64, average training loss: 7089.04, base loss: 9219.14
[INFO 2017-06-27 17:34:54,800 main.py:51] epoch 1599, training loss: 6269.72, average training loss: 7088.22, base loss: 9219.09
[INFO 2017-06-27 17:34:54,800 main.py:53] epoch 1599, testing
[INFO 2017-06-27 17:34:59,499 main.py:105] average testing loss: 6758.01, base loss: 9476.88
[INFO 2017-06-27 17:34:59,499 main.py:106] improve_loss: 2718.87, improve_percent: 0.29
[INFO 2017-06-27 17:34:59,501 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 17:34:59,513 main.py:76] current best improved percent: 0.29
[INFO 2017-06-27 17:35:00,723 main.py:51] epoch 1600, training loss: 6071.22, average training loss: 7086.60, base loss: 9217.79
[INFO 2017-06-27 17:35:01,930 main.py:51] epoch 1601, training loss: 5924.23, average training loss: 7084.68, base loss: 9216.11
[INFO 2017-06-27 17:35:03,138 main.py:51] epoch 1602, training loss: 6637.19, average training loss: 7083.84, base loss: 9215.98
[INFO 2017-06-27 17:35:04,352 main.py:51] epoch 1603, training loss: 6136.41, average training loss: 7083.22, base loss: 9216.44
[INFO 2017-06-27 17:35:05,561 main.py:51] epoch 1604, training loss: 6761.54, average training loss: 7082.08, base loss: 9216.21
[INFO 2017-06-27 17:35:06,774 main.py:51] epoch 1605, training loss: 6893.13, average training loss: 7081.53, base loss: 9216.67
[INFO 2017-06-27 17:35:07,988 main.py:51] epoch 1606, training loss: 6419.24, average training loss: 7079.73, base loss: 9215.29
[INFO 2017-06-27 17:35:09,199 main.py:51] epoch 1607, training loss: 6808.75, average training loss: 7078.34, base loss: 9214.08
[INFO 2017-06-27 17:35:10,414 main.py:51] epoch 1608, training loss: 6266.96, average training loss: 7077.44, base loss: 9214.54
[INFO 2017-06-27 17:35:11,626 main.py:51] epoch 1609, training loss: 9428.81, average training loss: 7079.68, base loss: 9217.92
[INFO 2017-06-27 17:35:12,838 main.py:51] epoch 1610, training loss: 6286.20, average training loss: 7078.47, base loss: 9217.87
[INFO 2017-06-27 17:35:14,050 main.py:51] epoch 1611, training loss: 6714.16, average training loss: 7077.54, base loss: 9217.82
[INFO 2017-06-27 17:35:15,261 main.py:51] epoch 1612, training loss: 6507.99, average training loss: 7076.01, base loss: 9217.10
[INFO 2017-06-27 17:35:16,472 main.py:51] epoch 1613, training loss: 8408.96, average training loss: 7076.47, base loss: 9218.43
[INFO 2017-06-27 17:35:17,682 main.py:51] epoch 1614, training loss: 6959.85, average training loss: 7075.60, base loss: 9218.61
[INFO 2017-06-27 17:35:18,894 main.py:51] epoch 1615, training loss: 6624.67, average training loss: 7074.90, base loss: 9218.88
[INFO 2017-06-27 17:35:20,108 main.py:51] epoch 1616, training loss: 6076.77, average training loss: 7073.80, base loss: 9219.01
[INFO 2017-06-27 17:35:21,322 main.py:51] epoch 1617, training loss: 6020.13, average training loss: 7072.51, base loss: 9218.55
[INFO 2017-06-27 17:35:22,539 main.py:51] epoch 1618, training loss: 6091.78, average training loss: 7071.91, base loss: 9218.40
[INFO 2017-06-27 17:35:23,748 main.py:51] epoch 1619, training loss: 6905.50, average training loss: 7071.09, base loss: 9218.70
[INFO 2017-06-27 17:35:24,958 main.py:51] epoch 1620, training loss: 6649.64, average training loss: 7070.02, base loss: 9218.74
[INFO 2017-06-27 17:35:26,172 main.py:51] epoch 1621, training loss: 6562.24, average training loss: 7069.90, base loss: 9220.20
[INFO 2017-06-27 17:35:27,390 main.py:51] epoch 1622, training loss: 6316.85, average training loss: 7065.47, base loss: 9216.31
[INFO 2017-06-27 17:35:28,607 main.py:51] epoch 1623, training loss: 6504.99, average training loss: 7064.46, base loss: 9216.13
[INFO 2017-06-27 17:35:29,817 main.py:51] epoch 1624, training loss: 6473.53, average training loss: 7059.98, base loss: 9212.30
[INFO 2017-06-27 17:35:31,028 main.py:51] epoch 1625, training loss: 6348.26, average training loss: 7059.30, base loss: 9212.64
[INFO 2017-06-27 17:35:32,242 main.py:51] epoch 1626, training loss: 6871.40, average training loss: 7059.22, base loss: 9213.40
[INFO 2017-06-27 17:35:33,454 main.py:51] epoch 1627, training loss: 6458.85, average training loss: 7058.57, base loss: 9214.53
[INFO 2017-06-27 17:35:34,663 main.py:51] epoch 1628, training loss: 6264.27, average training loss: 7058.37, base loss: 9215.44
[INFO 2017-06-27 17:35:35,871 main.py:51] epoch 1629, training loss: 6379.48, average training loss: 7057.61, base loss: 9215.63
[INFO 2017-06-27 17:35:37,083 main.py:51] epoch 1630, training loss: 6696.13, average training loss: 7056.93, base loss: 9216.27
[INFO 2017-06-27 17:35:38,298 main.py:51] epoch 1631, training loss: 9271.37, average training loss: 7058.70, base loss: 9219.65
[INFO 2017-06-27 17:35:39,510 main.py:51] epoch 1632, training loss: 6238.79, average training loss: 7058.21, base loss: 9220.15
[INFO 2017-06-27 17:35:40,719 main.py:51] epoch 1633, training loss: 6572.11, average training loss: 7058.19, base loss: 9221.93
[INFO 2017-06-27 17:35:41,931 main.py:51] epoch 1634, training loss: 6447.46, average training loss: 7056.94, base loss: 9221.39
[INFO 2017-06-27 17:35:43,140 main.py:51] epoch 1635, training loss: 6504.44, average training loss: 7056.85, base loss: 9222.37
[INFO 2017-06-27 17:35:44,355 main.py:51] epoch 1636, training loss: 6179.96, average training loss: 7055.07, base loss: 9220.79
[INFO 2017-06-27 17:35:45,566 main.py:51] epoch 1637, training loss: 10102.71, average training loss: 7054.42, base loss: 9221.76
[INFO 2017-06-27 17:35:46,782 main.py:51] epoch 1638, training loss: 6426.77, average training loss: 7052.47, base loss: 9220.36
[INFO 2017-06-27 17:35:47,994 main.py:51] epoch 1639, training loss: 6840.22, average training loss: 7048.74, base loss: 9217.67
[INFO 2017-06-27 17:35:49,208 main.py:51] epoch 1640, training loss: 6610.83, average training loss: 7047.58, base loss: 9216.98
[INFO 2017-06-27 17:35:50,426 main.py:51] epoch 1641, training loss: 6090.58, average training loss: 7045.50, base loss: 9214.49
[INFO 2017-06-27 17:35:51,633 main.py:51] epoch 1642, training loss: 6528.05, average training loss: 7043.47, base loss: 9212.65
[INFO 2017-06-27 17:35:52,847 main.py:51] epoch 1643, training loss: 7269.71, average training loss: 7043.46, base loss: 9213.75
[INFO 2017-06-27 17:35:54,059 main.py:51] epoch 1644, training loss: 6576.13, average training loss: 7042.62, base loss: 9213.66
[INFO 2017-06-27 17:35:55,274 main.py:51] epoch 1645, training loss: 8518.57, average training loss: 7040.52, base loss: 9213.14
[INFO 2017-06-27 17:35:56,490 main.py:51] epoch 1646, training loss: 6477.66, average training loss: 7040.45, base loss: 9214.47
[INFO 2017-06-27 17:35:57,705 main.py:51] epoch 1647, training loss: 6266.24, average training loss: 7038.56, base loss: 9213.12
[INFO 2017-06-27 17:35:58,919 main.py:51] epoch 1648, training loss: 9549.14, average training loss: 7041.25, base loss: 9216.70
[INFO 2017-06-27 17:36:00,131 main.py:51] epoch 1649, training loss: 6612.24, average training loss: 7040.91, base loss: 9217.41
[INFO 2017-06-27 17:36:01,345 main.py:51] epoch 1650, training loss: 6145.78, average training loss: 7039.58, base loss: 9216.87
[INFO 2017-06-27 17:36:02,559 main.py:51] epoch 1651, training loss: 6582.33, average training loss: 7038.86, base loss: 9216.68
[INFO 2017-06-27 17:36:03,774 main.py:51] epoch 1652, training loss: 6257.82, average training loss: 7034.17, base loss: 9212.69
[INFO 2017-06-27 17:36:04,989 main.py:51] epoch 1653, training loss: 6320.30, average training loss: 7033.50, base loss: 9212.71
[INFO 2017-06-27 17:36:06,205 main.py:51] epoch 1654, training loss: 7216.37, average training loss: 7033.79, base loss: 9215.23
[INFO 2017-06-27 17:36:07,417 main.py:51] epoch 1655, training loss: 6182.60, average training loss: 7032.50, base loss: 9214.35
[INFO 2017-06-27 17:36:08,632 main.py:51] epoch 1656, training loss: 9539.41, average training loss: 7034.70, base loss: 9216.99
[INFO 2017-06-27 17:36:09,845 main.py:51] epoch 1657, training loss: 5861.46, average training loss: 7034.09, base loss: 9216.58
[INFO 2017-06-27 17:36:11,060 main.py:51] epoch 1658, training loss: 6425.69, average training loss: 7033.44, base loss: 9216.72
[INFO 2017-06-27 17:36:12,269 main.py:51] epoch 1659, training loss: 5790.02, average training loss: 7032.55, base loss: 9215.80
[INFO 2017-06-27 17:36:13,484 main.py:51] epoch 1660, training loss: 6027.40, average training loss: 7031.37, base loss: 9215.29
[INFO 2017-06-27 17:36:14,699 main.py:51] epoch 1661, training loss: 6320.23, average training loss: 7030.63, base loss: 9215.28
[INFO 2017-06-27 17:36:15,911 main.py:51] epoch 1662, training loss: 6637.58, average training loss: 7030.17, base loss: 9215.90
[INFO 2017-06-27 17:36:17,123 main.py:51] epoch 1663, training loss: 8868.68, average training loss: 7030.85, base loss: 9218.21
[INFO 2017-06-27 17:36:18,337 main.py:51] epoch 1664, training loss: 6608.33, average training loss: 7030.44, base loss: 9219.00
[INFO 2017-06-27 17:36:19,551 main.py:51] epoch 1665, training loss: 6186.36, average training loss: 7029.93, base loss: 9219.48
[INFO 2017-06-27 17:36:20,767 main.py:51] epoch 1666, training loss: 6759.67, average training loss: 7029.41, base loss: 9220.06
[INFO 2017-06-27 17:36:21,980 main.py:51] epoch 1667, training loss: 6464.60, average training loss: 7028.20, base loss: 9220.01
[INFO 2017-06-27 17:36:23,194 main.py:51] epoch 1668, training loss: 6118.69, average training loss: 7026.93, base loss: 9219.18
[INFO 2017-06-27 17:36:24,409 main.py:51] epoch 1669, training loss: 6289.07, average training loss: 7026.26, base loss: 9219.34
[INFO 2017-06-27 17:36:25,622 main.py:51] epoch 1670, training loss: 6590.04, average training loss: 7022.33, base loss: 9216.37
[INFO 2017-06-27 17:36:26,836 main.py:51] epoch 1671, training loss: 6610.07, average training loss: 7021.93, base loss: 9216.87
[INFO 2017-06-27 17:36:28,049 main.py:51] epoch 1672, training loss: 6103.49, average training loss: 7021.17, base loss: 9216.81
[INFO 2017-06-27 17:36:29,259 main.py:51] epoch 1673, training loss: 6449.94, average training loss: 7020.18, base loss: 9216.14
[INFO 2017-06-27 17:36:30,475 main.py:51] epoch 1674, training loss: 6435.80, average training loss: 7019.05, base loss: 9215.84
[INFO 2017-06-27 17:36:31,684 main.py:51] epoch 1675, training loss: 6543.06, average training loss: 7018.59, base loss: 9216.97
[INFO 2017-06-27 17:36:32,899 main.py:51] epoch 1676, training loss: 6616.11, average training loss: 7017.75, base loss: 9217.77
[INFO 2017-06-27 17:36:34,115 main.py:51] epoch 1677, training loss: 6499.68, average training loss: 7017.85, base loss: 9219.64
[INFO 2017-06-27 17:36:35,330 main.py:51] epoch 1678, training loss: 6558.22, average training loss: 7017.02, base loss: 9219.87
[INFO 2017-06-27 17:36:36,546 main.py:51] epoch 1679, training loss: 6565.86, average training loss: 7015.74, base loss: 9219.54
[INFO 2017-06-27 17:36:37,759 main.py:51] epoch 1680, training loss: 6397.56, average training loss: 7015.47, base loss: 9220.93
[INFO 2017-06-27 17:36:38,978 main.py:51] epoch 1681, training loss: 6866.15, average training loss: 7015.26, base loss: 9221.73
[INFO 2017-06-27 17:36:40,194 main.py:51] epoch 1682, training loss: 8765.65, average training loss: 7016.80, base loss: 9224.31
[INFO 2017-06-27 17:36:41,412 main.py:51] epoch 1683, training loss: 6208.51, average training loss: 7012.30, base loss: 9220.12
[INFO 2017-06-27 17:36:42,623 main.py:51] epoch 1684, training loss: 6541.44, average training loss: 7011.92, base loss: 9220.86
[INFO 2017-06-27 17:36:43,837 main.py:51] epoch 1685, training loss: 6458.56, average training loss: 7011.42, base loss: 9221.74
[INFO 2017-06-27 17:36:45,046 main.py:51] epoch 1686, training loss: 6070.79, average training loss: 7009.92, base loss: 9220.42
[INFO 2017-06-27 17:36:46,261 main.py:51] epoch 1687, training loss: 6392.42, average training loss: 7008.08, base loss: 9218.95
[INFO 2017-06-27 17:36:47,477 main.py:51] epoch 1688, training loss: 5731.39, average training loss: 7007.11, base loss: 9218.35
[INFO 2017-06-27 17:36:48,693 main.py:51] epoch 1689, training loss: 6934.15, average training loss: 7003.54, base loss: 9215.68
[INFO 2017-06-27 17:36:49,902 main.py:51] epoch 1690, training loss: 6413.97, average training loss: 7002.25, base loss: 9214.87
[INFO 2017-06-27 17:36:51,120 main.py:51] epoch 1691, training loss: 6157.51, average training loss: 7001.31, base loss: 9214.01
[INFO 2017-06-27 17:36:52,337 main.py:51] epoch 1692, training loss: 6047.45, average training loss: 7000.55, base loss: 9213.38
[INFO 2017-06-27 17:36:53,551 main.py:51] epoch 1693, training loss: 6120.53, average training loss: 6999.20, base loss: 9212.10
[INFO 2017-06-27 17:36:54,765 main.py:51] epoch 1694, training loss: 6427.31, average training loss: 6998.86, base loss: 9213.28
[INFO 2017-06-27 17:36:55,983 main.py:51] epoch 1695, training loss: 6262.91, average training loss: 6998.24, base loss: 9213.32
[INFO 2017-06-27 17:36:57,194 main.py:51] epoch 1696, training loss: 6364.91, average training loss: 6998.04, base loss: 9214.46
[INFO 2017-06-27 17:36:58,407 main.py:51] epoch 1697, training loss: 6195.05, average training loss: 6996.80, base loss: 9213.35
[INFO 2017-06-27 17:36:59,620 main.py:51] epoch 1698, training loss: 6758.23, average training loss: 6996.97, base loss: 9214.92
[INFO 2017-06-27 17:37:00,830 main.py:51] epoch 1699, training loss: 8739.62, average training loss: 6998.67, base loss: 9218.46
[INFO 2017-06-27 17:37:00,831 main.py:53] epoch 1699, testing
[INFO 2017-06-27 17:37:05,542 main.py:105] average testing loss: 7027.40, base loss: 9640.57
[INFO 2017-06-27 17:37:05,542 main.py:106] improve_loss: 2613.16, improve_percent: 0.27
[INFO 2017-06-27 17:37:05,543 main.py:76] current best improved percent: 0.29
[INFO 2017-06-27 17:37:06,750 main.py:51] epoch 1700, training loss: 6181.03, average training loss: 6994.08, base loss: 9214.55
[INFO 2017-06-27 17:37:07,959 main.py:51] epoch 1701, training loss: 6392.42, average training loss: 6993.47, base loss: 9214.42
[INFO 2017-06-27 17:37:09,170 main.py:51] epoch 1702, training loss: 6461.44, average training loss: 6992.42, base loss: 9213.76
[INFO 2017-06-27 17:37:10,380 main.py:51] epoch 1703, training loss: 10414.57, average training loss: 6995.35, base loss: 9217.41
[INFO 2017-06-27 17:37:11,589 main.py:51] epoch 1704, training loss: 10315.71, average training loss: 6998.73, base loss: 9222.12
[INFO 2017-06-27 17:37:12,802 main.py:51] epoch 1705, training loss: 7056.32, average training loss: 6998.02, base loss: 9222.88
[INFO 2017-06-27 17:37:14,015 main.py:51] epoch 1706, training loss: 10254.75, average training loss: 7000.84, base loss: 9227.01
[INFO 2017-06-27 17:37:15,225 main.py:51] epoch 1707, training loss: 6719.56, average training loss: 7000.62, base loss: 9227.81
[INFO 2017-06-27 17:37:16,435 main.py:51] epoch 1708, training loss: 6171.42, average training loss: 6999.18, base loss: 9226.45
[INFO 2017-06-27 17:37:17,647 main.py:51] epoch 1709, training loss: 6361.59, average training loss: 6995.32, base loss: 9223.19
[INFO 2017-06-27 17:37:18,857 main.py:51] epoch 1710, training loss: 5933.55, average training loss: 6994.47, base loss: 9222.58
[INFO 2017-06-27 17:37:20,066 main.py:51] epoch 1711, training loss: 6713.01, average training loss: 6994.13, base loss: 9223.28
[INFO 2017-06-27 17:37:21,278 main.py:51] epoch 1712, training loss: 6974.06, average training loss: 6994.44, base loss: 9224.89
[INFO 2017-06-27 17:37:22,489 main.py:51] epoch 1713, training loss: 6219.69, average training loss: 6993.84, base loss: 9224.96
[INFO 2017-06-27 17:37:23,698 main.py:51] epoch 1714, training loss: 6299.73, average training loss: 6993.82, base loss: 9225.77
[INFO 2017-06-27 17:37:24,910 main.py:51] epoch 1715, training loss: 10383.23, average training loss: 6993.37, base loss: 9226.92
[INFO 2017-06-27 17:37:26,122 main.py:51] epoch 1716, training loss: 6154.56, average training loss: 6992.23, base loss: 9226.43
[INFO 2017-06-27 17:37:27,331 main.py:51] epoch 1717, training loss: 6138.63, average training loss: 6991.10, base loss: 9225.71
[INFO 2017-06-27 17:37:28,538 main.py:51] epoch 1718, training loss: 6895.94, average training loss: 6990.92, base loss: 9226.97
[INFO 2017-06-27 17:37:29,749 main.py:51] epoch 1719, training loss: 6598.75, average training loss: 6990.99, base loss: 9228.06
[INFO 2017-06-27 17:37:30,959 main.py:51] epoch 1720, training loss: 6398.07, average training loss: 6989.52, base loss: 9226.27
[INFO 2017-06-27 17:37:32,170 main.py:51] epoch 1721, training loss: 8645.62, average training loss: 6991.47, base loss: 9229.81
[INFO 2017-06-27 17:37:33,380 main.py:51] epoch 1722, training loss: 6251.84, average training loss: 6990.94, base loss: 9229.92
[INFO 2017-06-27 17:37:34,595 main.py:51] epoch 1723, training loss: 5966.64, average training loss: 6990.72, base loss: 9230.50
[INFO 2017-06-27 17:37:35,806 main.py:51] epoch 1724, training loss: 6491.56, average training loss: 6990.37, base loss: 9231.17
[INFO 2017-06-27 17:37:37,014 main.py:51] epoch 1725, training loss: 6066.40, average training loss: 6989.66, base loss: 9231.36
[INFO 2017-06-27 17:37:38,226 main.py:51] epoch 1726, training loss: 6277.20, average training loss: 6985.28, base loss: 9228.03
[INFO 2017-06-27 17:37:39,437 main.py:51] epoch 1727, training loss: 6315.22, average training loss: 6985.25, base loss: 9229.33
[INFO 2017-06-27 17:37:40,649 main.py:51] epoch 1728, training loss: 6556.18, average training loss: 6984.07, base loss: 9228.57
[INFO 2017-06-27 17:37:41,860 main.py:51] epoch 1729, training loss: 6306.81, average training loss: 6983.12, base loss: 9228.69
[INFO 2017-06-27 17:37:43,067 main.py:51] epoch 1730, training loss: 6035.41, average training loss: 6980.94, base loss: 9225.87
[INFO 2017-06-27 17:37:44,277 main.py:51] epoch 1731, training loss: 5599.75, average training loss: 6979.45, base loss: 9224.80
[INFO 2017-06-27 17:37:45,488 main.py:51] epoch 1732, training loss: 6203.86, average training loss: 6978.09, base loss: 9223.75
[INFO 2017-06-27 17:37:46,699 main.py:51] epoch 1733, training loss: 6729.03, average training loss: 6978.26, base loss: 9225.02
[INFO 2017-06-27 17:37:47,912 main.py:51] epoch 1734, training loss: 6214.33, average training loss: 6977.69, base loss: 9225.17
[INFO 2017-06-27 17:37:49,122 main.py:51] epoch 1735, training loss: 6359.59, average training loss: 6976.17, base loss: 9223.85
[INFO 2017-06-27 17:37:50,332 main.py:51] epoch 1736, training loss: 5521.12, average training loss: 6973.80, base loss: 9221.59
[INFO 2017-06-27 17:37:51,542 main.py:51] epoch 1737, training loss: 6244.11, average training loss: 6972.81, base loss: 9221.52
[INFO 2017-06-27 17:37:52,749 main.py:51] epoch 1738, training loss: 6047.58, average training loss: 6971.95, base loss: 9221.79
[INFO 2017-06-27 17:37:53,961 main.py:51] epoch 1739, training loss: 6483.86, average training loss: 6970.47, base loss: 9220.72
[INFO 2017-06-27 17:37:55,169 main.py:51] epoch 1740, training loss: 6391.70, average training loss: 6969.37, base loss: 9220.47
[INFO 2017-06-27 17:37:56,379 main.py:51] epoch 1741, training loss: 8594.21, average training loss: 6970.37, base loss: 9222.76
[INFO 2017-06-27 17:37:57,593 main.py:51] epoch 1742, training loss: 6519.02, average training loss: 6970.36, base loss: 9224.01
[INFO 2017-06-27 17:37:58,805 main.py:51] epoch 1743, training loss: 8773.63, average training loss: 6972.23, base loss: 9228.19
[INFO 2017-06-27 17:38:00,019 main.py:51] epoch 1744, training loss: 6474.15, average training loss: 6970.66, base loss: 9227.33
[INFO 2017-06-27 17:38:01,232 main.py:51] epoch 1745, training loss: 6146.06, average training loss: 6969.67, base loss: 9226.45
[INFO 2017-06-27 17:38:02,442 main.py:51] epoch 1746, training loss: 6165.33, average training loss: 6968.52, base loss: 9225.91
[INFO 2017-06-27 17:38:03,654 main.py:51] epoch 1747, training loss: 7426.83, average training loss: 6968.69, base loss: 9227.75
[INFO 2017-06-27 17:38:04,864 main.py:51] epoch 1748, training loss: 6457.42, average training loss: 6968.46, base loss: 9229.36
[INFO 2017-06-27 17:38:06,076 main.py:51] epoch 1749, training loss: 5996.60, average training loss: 6967.55, base loss: 9229.11
[INFO 2017-06-27 17:38:07,286 main.py:51] epoch 1750, training loss: 6638.23, average training loss: 6966.46, base loss: 9229.28
[INFO 2017-06-27 17:38:08,495 main.py:51] epoch 1751, training loss: 6195.92, average training loss: 6964.97, base loss: 9227.66
[INFO 2017-06-27 17:38:09,707 main.py:51] epoch 1752, training loss: 6174.70, average training loss: 6964.06, base loss: 9227.27
[INFO 2017-06-27 17:38:10,919 main.py:51] epoch 1753, training loss: 6594.59, average training loss: 6963.24, base loss: 9227.45
[INFO 2017-06-27 17:38:12,132 main.py:51] epoch 1754, training loss: 6258.40, average training loss: 6962.12, base loss: 9226.57
[INFO 2017-06-27 17:38:13,340 main.py:51] epoch 1755, training loss: 6032.22, average training loss: 6961.26, base loss: 9226.33
[INFO 2017-06-27 17:38:14,553 main.py:51] epoch 1756, training loss: 6038.05, average training loss: 6959.29, base loss: 9224.36
[INFO 2017-06-27 17:38:15,766 main.py:51] epoch 1757, training loss: 9435.80, average training loss: 6961.09, base loss: 9226.83
[INFO 2017-06-27 17:38:16,977 main.py:51] epoch 1758, training loss: 7308.41, average training loss: 6960.84, base loss: 9227.57
[INFO 2017-06-27 17:38:18,188 main.py:51] epoch 1759, training loss: 6608.72, average training loss: 6960.42, base loss: 9228.56
[INFO 2017-06-27 17:38:19,402 main.py:51] epoch 1760, training loss: 5871.14, average training loss: 6959.44, base loss: 9227.67
[INFO 2017-06-27 17:38:20,616 main.py:51] epoch 1761, training loss: 6320.24, average training loss: 6958.41, base loss: 9227.51
[INFO 2017-06-27 17:38:21,828 main.py:51] epoch 1762, training loss: 6056.95, average training loss: 6956.96, base loss: 9226.68
[INFO 2017-06-27 17:38:23,041 main.py:51] epoch 1763, training loss: 7014.78, average training loss: 6958.19, base loss: 9229.60
[INFO 2017-06-27 17:38:24,253 main.py:51] epoch 1764, training loss: 6592.22, average training loss: 6957.05, base loss: 9229.24
[INFO 2017-06-27 17:38:25,468 main.py:51] epoch 1765, training loss: 6624.76, average training loss: 6957.20, base loss: 9230.99
[INFO 2017-06-27 17:38:26,679 main.py:51] epoch 1766, training loss: 6321.55, average training loss: 6956.36, base loss: 9231.20
[INFO 2017-06-27 17:38:27,890 main.py:51] epoch 1767, training loss: 5947.23, average training loss: 6955.13, base loss: 9230.34
[INFO 2017-06-27 17:38:29,103 main.py:51] epoch 1768, training loss: 6991.65, average training loss: 6954.42, base loss: 9230.75
[INFO 2017-06-27 17:38:30,313 main.py:51] epoch 1769, training loss: 6357.60, average training loss: 6953.36, base loss: 9230.85
[INFO 2017-06-27 17:38:31,526 main.py:51] epoch 1770, training loss: 6386.86, average training loss: 6952.92, base loss: 9231.24
[INFO 2017-06-27 17:38:32,740 main.py:51] epoch 1771, training loss: 6100.01, average training loss: 6951.70, base loss: 9229.80
[INFO 2017-06-27 17:38:33,953 main.py:51] epoch 1772, training loss: 10224.78, average training loss: 6954.12, base loss: 9233.04
[INFO 2017-06-27 17:38:35,163 main.py:51] epoch 1773, training loss: 6070.00, average training loss: 6948.77, base loss: 9227.68
[INFO 2017-06-27 17:38:36,375 main.py:51] epoch 1774, training loss: 6580.11, average training loss: 6947.86, base loss: 9227.02
[INFO 2017-06-27 17:38:37,586 main.py:51] epoch 1775, training loss: 6447.48, average training loss: 6947.10, base loss: 9226.82
[INFO 2017-06-27 17:38:38,800 main.py:51] epoch 1776, training loss: 5874.74, average training loss: 6945.60, base loss: 9225.96
[INFO 2017-06-27 17:38:40,012 main.py:51] epoch 1777, training loss: 6105.90, average training loss: 6944.77, base loss: 9225.92
[INFO 2017-06-27 17:38:41,223 main.py:51] epoch 1778, training loss: 6340.76, average training loss: 6944.15, base loss: 9225.75
[INFO 2017-06-27 17:38:42,434 main.py:51] epoch 1779, training loss: 6269.67, average training loss: 6943.97, base loss: 9226.85
[INFO 2017-06-27 17:38:43,642 main.py:51] epoch 1780, training loss: 6466.42, average training loss: 6943.14, base loss: 9227.34
[INFO 2017-06-27 17:38:44,854 main.py:51] epoch 1781, training loss: 6334.08, average training loss: 6939.09, base loss: 9224.13
[INFO 2017-06-27 17:38:46,064 main.py:51] epoch 1782, training loss: 5642.38, average training loss: 6937.44, base loss: 9222.57
[INFO 2017-06-27 17:38:47,276 main.py:51] epoch 1783, training loss: 5767.96, average training loss: 6936.04, base loss: 9220.81
[INFO 2017-06-27 17:38:48,493 main.py:51] epoch 1784, training loss: 6406.83, average training loss: 6935.09, base loss: 9220.64
[INFO 2017-06-27 17:38:49,703 main.py:51] epoch 1785, training loss: 5961.85, average training loss: 6934.00, base loss: 9220.51
[INFO 2017-06-27 17:38:50,915 main.py:51] epoch 1786, training loss: 6608.58, average training loss: 6933.52, base loss: 9220.47
[INFO 2017-06-27 17:38:52,126 main.py:51] epoch 1787, training loss: 8114.03, average training loss: 6934.75, base loss: 9223.22
[INFO 2017-06-27 17:38:53,338 main.py:51] epoch 1788, training loss: 6262.69, average training loss: 6934.02, base loss: 9223.17
[INFO 2017-06-27 17:38:54,550 main.py:51] epoch 1789, training loss: 5973.33, average training loss: 6932.11, base loss: 9221.64
[INFO 2017-06-27 17:38:55,762 main.py:51] epoch 1790, training loss: 6184.38, average training loss: 6931.40, base loss: 9221.67
[INFO 2017-06-27 17:38:56,970 main.py:51] epoch 1791, training loss: 6514.67, average training loss: 6931.20, base loss: 9222.83
[INFO 2017-06-27 17:38:58,181 main.py:51] epoch 1792, training loss: 6361.55, average training loss: 6930.51, base loss: 9223.11
[INFO 2017-06-27 17:38:59,391 main.py:51] epoch 1793, training loss: 6415.70, average training loss: 6929.73, base loss: 9223.46
[INFO 2017-06-27 17:39:00,601 main.py:51] epoch 1794, training loss: 6145.33, average training loss: 6925.18, base loss: 9219.51
[INFO 2017-06-27 17:39:01,812 main.py:51] epoch 1795, training loss: 6141.94, average training loss: 6924.44, base loss: 9219.72
[INFO 2017-06-27 17:39:03,025 main.py:51] epoch 1796, training loss: 6303.36, average training loss: 6923.26, base loss: 9219.21
[INFO 2017-06-27 17:39:04,235 main.py:51] epoch 1797, training loss: 9567.84, average training loss: 6925.48, base loss: 9222.15
[INFO 2017-06-27 17:39:05,452 main.py:51] epoch 1798, training loss: 6414.29, average training loss: 6925.08, base loss: 9223.07
[INFO 2017-06-27 17:39:06,663 main.py:51] epoch 1799, training loss: 6023.25, average training loss: 6923.71, base loss: 9221.63
[INFO 2017-06-27 17:39:06,663 main.py:53] epoch 1799, testing
[INFO 2017-06-27 17:39:11,357 main.py:105] average testing loss: 6437.51, base loss: 9173.42
[INFO 2017-06-27 17:39:11,357 main.py:106] improve_loss: 2735.91, improve_percent: 0.30
[INFO 2017-06-27 17:39:11,358 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 17:39:11,371 main.py:76] current best improved percent: 0.30
[INFO 2017-06-27 17:39:12,581 main.py:51] epoch 1800, training loss: 6586.86, average training loss: 6922.46, base loss: 9221.04
[INFO 2017-06-27 17:39:13,792 main.py:51] epoch 1801, training loss: 9747.68, average training loss: 6925.46, base loss: 9225.26
[INFO 2017-06-27 17:39:15,004 main.py:51] epoch 1802, training loss: 6341.54, average training loss: 6925.05, base loss: 9226.21
[INFO 2017-06-27 17:39:16,215 main.py:51] epoch 1803, training loss: 5753.77, average training loss: 6923.62, base loss: 9224.57
[INFO 2017-06-27 17:39:17,430 main.py:51] epoch 1804, training loss: 6494.46, average training loss: 6922.40, base loss: 9223.63
[INFO 2017-06-27 17:39:18,640 main.py:51] epoch 1805, training loss: 6654.96, average training loss: 6921.96, base loss: 9223.81
[INFO 2017-06-27 17:39:19,848 main.py:51] epoch 1806, training loss: 6428.12, average training loss: 6921.46, base loss: 9224.45
[INFO 2017-06-27 17:39:21,056 main.py:51] epoch 1807, training loss: 6540.93, average training loss: 6921.14, base loss: 9224.42
[INFO 2017-06-27 17:39:22,265 main.py:51] epoch 1808, training loss: 5812.52, average training loss: 6920.03, base loss: 9223.66
[INFO 2017-06-27 17:39:23,473 main.py:51] epoch 1809, training loss: 6237.61, average training loss: 6918.87, base loss: 9223.31
[INFO 2017-06-27 17:39:24,685 main.py:51] epoch 1810, training loss: 6104.64, average training loss: 6917.67, base loss: 9222.86
[INFO 2017-06-27 17:39:25,893 main.py:51] epoch 1811, training loss: 6591.43, average training loss: 6917.10, base loss: 9222.82
[INFO 2017-06-27 17:39:27,108 main.py:51] epoch 1812, training loss: 6223.82, average training loss: 6916.45, base loss: 9223.25
[INFO 2017-06-27 17:39:28,318 main.py:51] epoch 1813, training loss: 6868.54, average training loss: 6915.59, base loss: 9223.32
[INFO 2017-06-27 17:39:29,529 main.py:51] epoch 1814, training loss: 6241.82, average training loss: 6914.52, base loss: 9222.47
[INFO 2017-06-27 17:39:30,738 main.py:51] epoch 1815, training loss: 6269.81, average training loss: 6914.08, base loss: 9223.25
[INFO 2017-06-27 17:39:31,948 main.py:51] epoch 1816, training loss: 6730.66, average training loss: 6914.01, base loss: 9224.28
[INFO 2017-06-27 17:39:33,159 main.py:51] epoch 1817, training loss: 6041.21, average training loss: 6913.74, base loss: 9225.01
[INFO 2017-06-27 17:39:34,369 main.py:51] epoch 1818, training loss: 6394.19, average training loss: 6913.65, base loss: 9226.16
[INFO 2017-06-27 17:39:35,577 main.py:51] epoch 1819, training loss: 6692.68, average training loss: 6912.00, base loss: 9225.36
[INFO 2017-06-27 17:39:36,788 main.py:51] epoch 1820, training loss: 6471.09, average training loss: 6911.70, base loss: 9226.33
[INFO 2017-06-27 17:39:37,998 main.py:51] epoch 1821, training loss: 6427.16, average training loss: 6911.68, base loss: 9227.48
[INFO 2017-06-27 17:39:39,208 main.py:51] epoch 1822, training loss: 6332.51, average training loss: 6911.01, base loss: 9227.32
[INFO 2017-06-27 17:39:40,419 main.py:51] epoch 1823, training loss: 6083.40, average training loss: 6909.78, base loss: 9226.38
[INFO 2017-06-27 17:39:41,630 main.py:51] epoch 1824, training loss: 6592.76, average training loss: 6909.61, base loss: 9227.80
[INFO 2017-06-27 17:39:42,844 main.py:51] epoch 1825, training loss: 6556.35, average training loss: 6909.24, base loss: 9228.30
[INFO 2017-06-27 17:39:44,053 main.py:51] epoch 1826, training loss: 6224.60, average training loss: 6908.84, base loss: 9229.02
[INFO 2017-06-27 17:39:45,265 main.py:51] epoch 1827, training loss: 6776.88, average training loss: 6908.70, base loss: 9229.49
[INFO 2017-06-27 17:39:46,475 main.py:51] epoch 1828, training loss: 6225.22, average training loss: 6908.37, base loss: 9229.20
[INFO 2017-06-27 17:39:47,685 main.py:51] epoch 1829, training loss: 5766.29, average training loss: 6907.37, base loss: 9229.01
[INFO 2017-06-27 17:39:48,895 main.py:51] epoch 1830, training loss: 6817.44, average training loss: 6907.69, base loss: 9230.57
[INFO 2017-06-27 17:39:50,109 main.py:51] epoch 1831, training loss: 9499.04, average training loss: 6909.61, base loss: 9232.71
[INFO 2017-06-27 17:39:51,324 main.py:51] epoch 1832, training loss: 5965.41, average training loss: 6908.70, base loss: 9232.11
[INFO 2017-06-27 17:39:52,538 main.py:51] epoch 1833, training loss: 9941.35, average training loss: 6908.01, base loss: 9232.26
[INFO 2017-06-27 17:39:53,746 main.py:51] epoch 1834, training loss: 6535.65, average training loss: 6907.51, base loss: 9232.93
[INFO 2017-06-27 17:39:54,957 main.py:51] epoch 1835, training loss: 6314.67, average training loss: 6907.34, base loss: 9234.39
[INFO 2017-06-27 17:39:56,168 main.py:51] epoch 1836, training loss: 6563.15, average training loss: 6907.46, base loss: 9235.55
[INFO 2017-06-27 17:39:57,381 main.py:51] epoch 1837, training loss: 6441.10, average training loss: 6907.42, base loss: 9236.16
[INFO 2017-06-27 17:39:58,590 main.py:51] epoch 1838, training loss: 6323.14, average training loss: 6906.72, base loss: 9236.17
[INFO 2017-06-27 17:39:59,800 main.py:51] epoch 1839, training loss: 6021.67, average training loss: 6905.78, base loss: 9235.18
[INFO 2017-06-27 17:40:01,009 main.py:51] epoch 1840, training loss: 6281.33, average training loss: 6905.37, base loss: 9235.79
[INFO 2017-06-27 17:40:02,220 main.py:51] epoch 1841, training loss: 6612.94, average training loss: 6904.20, base loss: 9235.36
[INFO 2017-06-27 17:40:03,431 main.py:51] epoch 1842, training loss: 6901.81, average training loss: 6900.64, base loss: 9233.19
[INFO 2017-06-27 17:40:04,638 main.py:51] epoch 1843, training loss: 6215.13, average training loss: 6896.57, base loss: 9229.78
[INFO 2017-06-27 17:40:05,852 main.py:51] epoch 1844, training loss: 6312.30, average training loss: 6895.56, base loss: 9229.28
[INFO 2017-06-27 17:40:07,063 main.py:51] epoch 1845, training loss: 6609.11, average training loss: 6895.46, base loss: 9230.05
[INFO 2017-06-27 17:40:08,271 main.py:51] epoch 1846, training loss: 6873.66, average training loss: 6894.88, base loss: 9230.35
[INFO 2017-06-27 17:40:09,480 main.py:51] epoch 1847, training loss: 6265.44, average training loss: 6893.95, base loss: 9229.92
[INFO 2017-06-27 17:40:10,694 main.py:51] epoch 1848, training loss: 5787.48, average training loss: 6893.24, base loss: 9230.04
[INFO 2017-06-27 17:40:11,905 main.py:51] epoch 1849, training loss: 6544.97, average training loss: 6892.03, base loss: 9228.89
[INFO 2017-06-27 17:40:13,112 main.py:51] epoch 1850, training loss: 6056.97, average training loss: 6888.07, base loss: 9225.75
[INFO 2017-06-27 17:40:14,320 main.py:51] epoch 1851, training loss: 6200.12, average training loss: 6887.72, base loss: 9225.80
[INFO 2017-06-27 17:40:15,531 main.py:51] epoch 1852, training loss: 6352.75, average training loss: 6886.97, base loss: 9225.80
[INFO 2017-06-27 17:40:16,743 main.py:51] epoch 1853, training loss: 6214.60, average training loss: 6886.45, base loss: 9226.18
[INFO 2017-06-27 17:40:17,954 main.py:51] epoch 1854, training loss: 6519.80, average training loss: 6886.34, base loss: 9227.45
[INFO 2017-06-27 17:40:19,164 main.py:51] epoch 1855, training loss: 6102.98, average training loss: 6885.47, base loss: 9227.14
[INFO 2017-06-27 17:40:20,377 main.py:51] epoch 1856, training loss: 6218.16, average training loss: 6884.55, base loss: 9226.39
[INFO 2017-06-27 17:40:21,584 main.py:51] epoch 1857, training loss: 6172.58, average training loss: 6883.34, base loss: 9224.98
[INFO 2017-06-27 17:40:22,794 main.py:51] epoch 1858, training loss: 6399.59, average training loss: 6878.97, base loss: 9221.73
[INFO 2017-06-27 17:40:24,005 main.py:51] epoch 1859, training loss: 6339.89, average training loss: 6878.19, base loss: 9221.83
[INFO 2017-06-27 17:40:25,214 main.py:51] epoch 1860, training loss: 6706.06, average training loss: 6877.54, base loss: 9221.94
[INFO 2017-06-27 17:40:26,426 main.py:51] epoch 1861, training loss: 5920.63, average training loss: 6876.27, base loss: 9221.05
[INFO 2017-06-27 17:40:27,636 main.py:51] epoch 1862, training loss: 6639.01, average training loss: 6875.59, base loss: 9221.28
[INFO 2017-06-27 17:40:28,848 main.py:51] epoch 1863, training loss: 6409.45, average training loss: 6875.38, base loss: 9222.33
[INFO 2017-06-27 17:40:30,057 main.py:51] epoch 1864, training loss: 6445.48, average training loss: 6874.83, base loss: 9222.34
[INFO 2017-06-27 17:40:31,271 main.py:51] epoch 1865, training loss: 5925.28, average training loss: 6873.73, base loss: 9221.72
[INFO 2017-06-27 17:40:32,479 main.py:51] epoch 1866, training loss: 6222.19, average training loss: 6869.23, base loss: 9217.09
[INFO 2017-06-27 17:40:33,691 main.py:51] epoch 1867, training loss: 6005.02, average training loss: 6868.75, base loss: 9217.67
[INFO 2017-06-27 17:40:34,899 main.py:51] epoch 1868, training loss: 6178.78, average training loss: 6867.56, base loss: 9217.22
[INFO 2017-06-27 17:40:36,111 main.py:51] epoch 1869, training loss: 9189.72, average training loss: 6869.50, base loss: 9220.33
[INFO 2017-06-27 17:40:37,321 main.py:51] epoch 1870, training loss: 6393.59, average training loss: 6869.27, base loss: 9221.59
[INFO 2017-06-27 17:40:38,530 main.py:51] epoch 1871, training loss: 6472.67, average training loss: 6868.45, base loss: 9222.07
[INFO 2017-06-27 17:40:39,738 main.py:51] epoch 1872, training loss: 5815.50, average training loss: 6866.63, base loss: 9220.01
[INFO 2017-06-27 17:40:40,953 main.py:51] epoch 1873, training loss: 6322.08, average training loss: 6866.12, base loss: 9220.25
[INFO 2017-06-27 17:40:42,166 main.py:51] epoch 1874, training loss: 6177.01, average training loss: 6864.49, base loss: 9218.59
[INFO 2017-06-27 17:40:43,380 main.py:51] epoch 1875, training loss: 5867.63, average training loss: 6863.30, base loss: 9217.45
[INFO 2017-06-27 17:40:44,593 main.py:51] epoch 1876, training loss: 6454.89, average training loss: 6862.31, base loss: 9217.42
[INFO 2017-06-27 17:40:45,807 main.py:51] epoch 1877, training loss: 6120.11, average training loss: 6861.05, base loss: 9216.50
[INFO 2017-06-27 17:40:47,017 main.py:51] epoch 1878, training loss: 6058.66, average training loss: 6856.05, base loss: 9211.73
[INFO 2017-06-27 17:40:48,231 main.py:51] epoch 1879, training loss: 6655.39, average training loss: 6855.84, base loss: 9212.33
[INFO 2017-06-27 17:40:49,443 main.py:51] epoch 1880, training loss: 6620.38, average training loss: 6851.91, base loss: 9209.38
[INFO 2017-06-27 17:40:50,655 main.py:51] epoch 1881, training loss: 6081.19, average training loss: 6851.06, base loss: 9208.00
[INFO 2017-06-27 17:40:51,862 main.py:51] epoch 1882, training loss: 5895.97, average training loss: 6846.61, base loss: 9203.80
[INFO 2017-06-27 17:40:53,069 main.py:51] epoch 1883, training loss: 5767.93, average training loss: 6845.62, base loss: 9202.64
[INFO 2017-06-27 17:40:54,279 main.py:51] epoch 1884, training loss: 6230.49, average training loss: 6844.51, base loss: 9202.02
[INFO 2017-06-27 17:40:55,492 main.py:51] epoch 1885, training loss: 6675.25, average training loss: 6840.96, base loss: 9199.82
[INFO 2017-06-27 17:40:56,706 main.py:51] epoch 1886, training loss: 6587.98, average training loss: 6841.07, base loss: 9201.51
[INFO 2017-06-27 17:40:57,911 main.py:51] epoch 1887, training loss: 6194.90, average training loss: 6839.77, base loss: 9199.91
[INFO 2017-06-27 17:40:59,126 main.py:51] epoch 1888, training loss: 6371.71, average training loss: 6839.18, base loss: 9200.06
[INFO 2017-06-27 17:41:00,341 main.py:51] epoch 1889, training loss: 6405.64, average training loss: 6838.83, base loss: 9200.47
[INFO 2017-06-27 17:41:01,550 main.py:51] epoch 1890, training loss: 6764.09, average training loss: 6838.84, base loss: 9201.37
[INFO 2017-06-27 17:41:02,760 main.py:51] epoch 1891, training loss: 6077.89, average training loss: 6838.25, base loss: 9201.29
[INFO 2017-06-27 17:41:03,969 main.py:51] epoch 1892, training loss: 6055.58, average training loss: 6837.58, base loss: 9201.26
[INFO 2017-06-27 17:41:05,180 main.py:51] epoch 1893, training loss: 5956.20, average training loss: 6836.68, base loss: 9199.98
[INFO 2017-06-27 17:41:06,391 main.py:51] epoch 1894, training loss: 6218.38, average training loss: 6835.99, base loss: 9199.74
[INFO 2017-06-27 17:41:07,601 main.py:51] epoch 1895, training loss: 7098.27, average training loss: 6835.89, base loss: 9200.27
[INFO 2017-06-27 17:41:08,810 main.py:51] epoch 1896, training loss: 9240.79, average training loss: 6838.68, base loss: 9204.54
[INFO 2017-06-27 17:41:10,022 main.py:51] epoch 1897, training loss: 8819.56, average training loss: 6840.09, base loss: 9207.76
[INFO 2017-06-27 17:41:11,232 main.py:51] epoch 1898, training loss: 6371.79, average training loss: 6835.86, base loss: 9204.05
[INFO 2017-06-27 17:41:12,444 main.py:51] epoch 1899, training loss: 6420.20, average training loss: 6835.25, base loss: 9204.84
[INFO 2017-06-27 17:41:12,444 main.py:53] epoch 1899, testing
[INFO 2017-06-27 17:41:17,162 main.py:105] average testing loss: 6230.72, base loss: 8917.69
[INFO 2017-06-27 17:41:17,162 main.py:106] improve_loss: 2686.97, improve_percent: 0.30
[INFO 2017-06-27 17:41:17,164 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 17:41:17,176 main.py:76] current best improved percent: 0.30
[INFO 2017-06-27 17:41:18,383 main.py:51] epoch 1900, training loss: 6170.59, average training loss: 6834.37, base loss: 9204.39
[INFO 2017-06-27 17:41:19,591 main.py:51] epoch 1901, training loss: 6017.82, average training loss: 6833.33, base loss: 9203.38
[INFO 2017-06-27 17:41:20,802 main.py:51] epoch 1902, training loss: 6036.14, average training loss: 6832.28, base loss: 9202.43
[INFO 2017-06-27 17:41:22,014 main.py:51] epoch 1903, training loss: 6112.68, average training loss: 6831.93, base loss: 9202.99
[INFO 2017-06-27 17:41:23,226 main.py:51] epoch 1904, training loss: 5815.92, average training loss: 6830.71, base loss: 9201.59
[INFO 2017-06-27 17:41:24,438 main.py:51] epoch 1905, training loss: 6042.80, average training loss: 6829.73, base loss: 9201.31
[INFO 2017-06-27 17:41:25,649 main.py:51] epoch 1906, training loss: 6773.54, average training loss: 6829.90, base loss: 9202.43
[INFO 2017-06-27 17:41:26,857 main.py:51] epoch 1907, training loss: 6458.12, average training loss: 6829.38, base loss: 9203.09
[INFO 2017-06-27 17:41:28,066 main.py:51] epoch 1908, training loss: 5914.40, average training loss: 6828.66, base loss: 9202.74
[INFO 2017-06-27 17:41:29,278 main.py:51] epoch 1909, training loss: 6026.50, average training loss: 6827.91, base loss: 9202.22
[INFO 2017-06-27 17:41:30,489 main.py:51] epoch 1910, training loss: 6373.21, average training loss: 6827.58, base loss: 9202.97
[INFO 2017-06-27 17:41:31,699 main.py:51] epoch 1911, training loss: 6410.89, average training loss: 6827.25, base loss: 9203.42
[INFO 2017-06-27 17:41:32,907 main.py:51] epoch 1912, training loss: 5763.22, average training loss: 6825.74, base loss: 9202.69
[INFO 2017-06-27 17:41:34,119 main.py:51] epoch 1913, training loss: 5942.62, average training loss: 6824.12, base loss: 9201.67
[INFO 2017-06-27 17:41:35,327 main.py:51] epoch 1914, training loss: 6108.15, average training loss: 6823.76, base loss: 9201.80
[INFO 2017-06-27 17:41:36,538 main.py:51] epoch 1915, training loss: 5497.87, average training loss: 6822.48, base loss: 9200.59
[INFO 2017-06-27 17:41:37,748 main.py:51] epoch 1916, training loss: 6248.34, average training loss: 6821.81, base loss: 9201.14
[INFO 2017-06-27 17:41:38,964 main.py:51] epoch 1917, training loss: 6064.38, average training loss: 6821.37, base loss: 9201.14
[INFO 2017-06-27 17:41:40,176 main.py:51] epoch 1918, training loss: 8815.53, average training loss: 6819.60, base loss: 9200.93
[INFO 2017-06-27 17:41:41,389 main.py:51] epoch 1919, training loss: 6622.14, average training loss: 6818.61, base loss: 9200.45
[INFO 2017-06-27 17:41:42,602 main.py:51] epoch 1920, training loss: 6292.68, average training loss: 6817.70, base loss: 9199.79
[INFO 2017-06-27 17:41:43,808 main.py:51] epoch 1921, training loss: 6799.46, average training loss: 6817.07, base loss: 9200.16
[INFO 2017-06-27 17:41:45,015 main.py:51] epoch 1922, training loss: 6574.18, average training loss: 6816.96, base loss: 9201.27
[INFO 2017-06-27 17:41:46,222 main.py:51] epoch 1923, training loss: 5900.65, average training loss: 6816.13, base loss: 9200.63
[INFO 2017-06-27 17:41:47,438 main.py:51] epoch 1924, training loss: 7013.91, average training loss: 6816.04, base loss: 9201.68
[INFO 2017-06-27 17:41:48,650 main.py:51] epoch 1925, training loss: 8266.33, average training loss: 6817.92, base loss: 9205.42
[INFO 2017-06-27 17:41:49,861 main.py:51] epoch 1926, training loss: 6210.95, average training loss: 6817.28, base loss: 9205.35
[INFO 2017-06-27 17:41:51,071 main.py:51] epoch 1927, training loss: 6061.57, average training loss: 6817.03, base loss: 9205.35
[INFO 2017-06-27 17:41:52,281 main.py:51] epoch 1928, training loss: 6024.43, average training loss: 6816.42, base loss: 9205.74
[INFO 2017-06-27 17:41:53,491 main.py:51] epoch 1929, training loss: 6562.72, average training loss: 6815.97, base loss: 9205.86
[INFO 2017-06-27 17:41:54,702 main.py:51] epoch 1930, training loss: 9714.39, average training loss: 6819.02, base loss: 9209.73
[INFO 2017-06-27 17:41:55,913 main.py:51] epoch 1931, training loss: 6359.56, average training loss: 6818.33, base loss: 9209.68
[INFO 2017-06-27 17:41:57,122 main.py:51] epoch 1932, training loss: 9614.02, average training loss: 6817.72, base loss: 9210.51
[INFO 2017-06-27 17:41:58,335 main.py:51] epoch 1933, training loss: 6259.36, average training loss: 6816.86, base loss: 9209.47
[INFO 2017-06-27 17:41:59,547 main.py:51] epoch 1934, training loss: 9319.90, average training loss: 6819.48, base loss: 9213.38
[INFO 2017-06-27 17:42:00,758 main.py:51] epoch 1935, training loss: 6208.25, average training loss: 6818.50, base loss: 9212.67
[INFO 2017-06-27 17:42:01,971 main.py:51] epoch 1936, training loss: 5523.49, average training loss: 6816.47, base loss: 9209.58
[INFO 2017-06-27 17:42:03,181 main.py:51] epoch 1937, training loss: 5887.29, average training loss: 6815.23, base loss: 9208.16
[INFO 2017-06-27 17:42:04,391 main.py:51] epoch 1938, training loss: 5837.47, average training loss: 6814.64, base loss: 9208.13
[INFO 2017-06-27 17:42:05,604 main.py:51] epoch 1939, training loss: 6169.47, average training loss: 6814.46, base loss: 9209.51
[INFO 2017-06-27 17:42:06,817 main.py:51] epoch 1940, training loss: 7283.30, average training loss: 6814.91, base loss: 9212.01
[INFO 2017-06-27 17:42:08,029 main.py:51] epoch 1941, training loss: 6642.85, average training loss: 6814.95, base loss: 9213.43
[INFO 2017-06-27 17:42:09,242 main.py:51] epoch 1942, training loss: 6420.33, average training loss: 6813.80, base loss: 9213.29
[INFO 2017-06-27 17:42:10,455 main.py:51] epoch 1943, training loss: 9222.55, average training loss: 6816.30, base loss: 9217.15
[INFO 2017-06-27 17:42:11,667 main.py:51] epoch 1944, training loss: 6029.02, average training loss: 6815.70, base loss: 9217.53
[INFO 2017-06-27 17:42:12,928 main.py:51] epoch 1945, training loss: 5921.97, average training loss: 6814.92, base loss: 9216.77
[INFO 2017-06-27 17:42:14,141 main.py:51] epoch 1946, training loss: 6695.32, average training loss: 6814.80, base loss: 9217.75
[INFO 2017-06-27 17:42:15,354 main.py:51] epoch 1947, training loss: 5758.58, average training loss: 6813.21, base loss: 9215.78
[INFO 2017-06-27 17:42:16,563 main.py:51] epoch 1948, training loss: 6072.48, average training loss: 6812.64, base loss: 9215.60
[INFO 2017-06-27 17:42:17,767 main.py:51] epoch 1949, training loss: 6857.79, average training loss: 6813.03, base loss: 9217.51
[INFO 2017-06-27 17:42:18,973 main.py:51] epoch 1950, training loss: 8133.86, average training loss: 6813.58, base loss: 9219.47
[INFO 2017-06-27 17:42:20,187 main.py:51] epoch 1951, training loss: 6701.55, average training loss: 6813.52, base loss: 9220.36
[INFO 2017-06-27 17:42:21,401 main.py:51] epoch 1952, training loss: 6387.30, average training loss: 6812.89, base loss: 9220.02
[INFO 2017-06-27 17:42:22,612 main.py:51] epoch 1953, training loss: 6093.03, average training loss: 6808.93, base loss: 9216.93
[INFO 2017-06-27 17:42:23,823 main.py:51] epoch 1954, training loss: 5999.89, average training loss: 6804.12, base loss: 9212.66
[INFO 2017-06-27 17:42:25,025 main.py:51] epoch 1955, training loss: 6411.97, average training loss: 6800.17, base loss: 9209.39
[INFO 2017-06-27 17:42:26,237 main.py:51] epoch 1956, training loss: 6645.40, average training loss: 6799.59, base loss: 9209.69
[INFO 2017-06-27 17:42:27,451 main.py:51] epoch 1957, training loss: 6104.82, average training loss: 6795.04, base loss: 9205.09
[INFO 2017-06-27 17:42:28,665 main.py:51] epoch 1958, training loss: 6650.67, average training loss: 6795.07, base loss: 9206.22
[INFO 2017-06-27 17:42:29,877 main.py:51] epoch 1959, training loss: 6362.79, average training loss: 6794.41, base loss: 9206.14
[INFO 2017-06-27 17:42:31,084 main.py:51] epoch 1960, training loss: 6248.39, average training loss: 6793.39, base loss: 9205.37
[INFO 2017-06-27 17:42:32,294 main.py:51] epoch 1961, training loss: 6314.15, average training loss: 6792.75, base loss: 9205.69
[INFO 2017-06-27 17:42:33,505 main.py:51] epoch 1962, training loss: 6332.28, average training loss: 6792.72, base loss: 9207.39
[INFO 2017-06-27 17:42:34,715 main.py:51] epoch 1963, training loss: 6418.52, average training loss: 6792.29, base loss: 9207.99
[INFO 2017-06-27 17:42:35,925 main.py:51] epoch 1964, training loss: 6655.52, average training loss: 6787.71, base loss: 9203.83
[INFO 2017-06-27 17:42:37,140 main.py:51] epoch 1965, training loss: 6154.25, average training loss: 6787.29, base loss: 9204.62
[INFO 2017-06-27 17:42:38,355 main.py:51] epoch 1966, training loss: 6325.89, average training loss: 6786.53, base loss: 9204.23
[INFO 2017-06-27 17:42:39,564 main.py:51] epoch 1967, training loss: 5970.80, average training loss: 6785.75, base loss: 9203.71
[INFO 2017-06-27 17:42:40,781 main.py:51] epoch 1968, training loss: 6424.42, average training loss: 6785.19, base loss: 9203.58
[INFO 2017-06-27 17:42:41,991 main.py:51] epoch 1969, training loss: 6338.04, average training loss: 6784.42, base loss: 9203.37
[INFO 2017-06-27 17:42:43,206 main.py:51] epoch 1970, training loss: 6401.08, average training loss: 6784.06, base loss: 9204.20
[INFO 2017-06-27 17:42:44,415 main.py:51] epoch 1971, training loss: 6240.88, average training loss: 6782.96, base loss: 9203.90
[INFO 2017-06-27 17:42:45,629 main.py:51] epoch 1972, training loss: 6271.23, average training loss: 6782.30, base loss: 9203.83
[INFO 2017-06-27 17:42:46,842 main.py:51] epoch 1973, training loss: 5859.45, average training loss: 6781.69, base loss: 9203.42
[INFO 2017-06-27 17:42:48,053 main.py:51] epoch 1974, training loss: 5898.14, average training loss: 6780.90, base loss: 9202.41
[INFO 2017-06-27 17:42:49,263 main.py:51] epoch 1975, training loss: 8305.53, average training loss: 6783.31, base loss: 9206.80
[INFO 2017-06-27 17:42:50,476 main.py:51] epoch 1976, training loss: 6248.19, average training loss: 6782.52, base loss: 9206.11
[INFO 2017-06-27 17:42:51,689 main.py:51] epoch 1977, training loss: 5958.84, average training loss: 6781.81, base loss: 9205.53
[INFO 2017-06-27 17:42:52,898 main.py:51] epoch 1978, training loss: 6222.91, average training loss: 6780.73, base loss: 9205.17
[INFO 2017-06-27 17:42:54,109 main.py:51] epoch 1979, training loss: 6163.34, average training loss: 6780.09, base loss: 9205.00
[INFO 2017-06-27 17:42:55,321 main.py:51] epoch 1980, training loss: 8040.98, average training loss: 6781.01, base loss: 9206.75
[INFO 2017-06-27 17:42:56,532 main.py:51] epoch 1981, training loss: 6433.71, average training loss: 6781.02, base loss: 9207.72
[INFO 2017-06-27 17:42:57,742 main.py:51] epoch 1982, training loss: 8906.92, average training loss: 6782.66, base loss: 9209.90
[INFO 2017-06-27 17:42:58,953 main.py:51] epoch 1983, training loss: 6468.77, average training loss: 6781.70, base loss: 9209.23
[INFO 2017-06-27 17:43:00,164 main.py:51] epoch 1984, training loss: 6241.10, average training loss: 6780.73, base loss: 9208.39
[INFO 2017-06-27 17:43:01,376 main.py:51] epoch 1985, training loss: 5990.71, average training loss: 6779.94, base loss: 9207.65
[INFO 2017-06-27 17:43:02,586 main.py:51] epoch 1986, training loss: 6148.28, average training loss: 6778.90, base loss: 9206.96
[INFO 2017-06-27 17:43:03,795 main.py:51] epoch 1987, training loss: 6105.10, average training loss: 6777.71, base loss: 9206.78
[INFO 2017-06-27 17:43:05,009 main.py:51] epoch 1988, training loss: 8488.73, average training loss: 6779.66, base loss: 9210.77
[INFO 2017-06-27 17:43:06,219 main.py:51] epoch 1989, training loss: 7205.09, average training loss: 6779.85, base loss: 9212.89
[INFO 2017-06-27 17:43:07,426 main.py:51] epoch 1990, training loss: 6558.94, average training loss: 6779.61, base loss: 9212.72
[INFO 2017-06-27 17:43:08,637 main.py:51] epoch 1991, training loss: 6444.12, average training loss: 6779.52, base loss: 9213.52
[INFO 2017-06-27 17:43:09,851 main.py:51] epoch 1992, training loss: 6320.85, average training loss: 6778.65, base loss: 9213.13
[INFO 2017-06-27 17:43:11,059 main.py:51] epoch 1993, training loss: 6817.78, average training loss: 6778.65, base loss: 9213.57
[INFO 2017-06-27 17:43:12,270 main.py:51] epoch 1994, training loss: 6347.96, average training loss: 6777.96, base loss: 9212.94
[INFO 2017-06-27 17:43:13,486 main.py:51] epoch 1995, training loss: 6549.96, average training loss: 6777.73, base loss: 9213.85
[INFO 2017-06-27 17:43:14,697 main.py:51] epoch 1996, training loss: 6245.25, average training loss: 6777.00, base loss: 9213.53
[INFO 2017-06-27 17:43:15,910 main.py:51] epoch 1997, training loss: 6355.24, average training loss: 6776.58, base loss: 9214.02
[INFO 2017-06-27 17:43:17,123 main.py:51] epoch 1998, training loss: 6523.23, average training loss: 6776.65, base loss: 9215.06
[INFO 2017-06-27 17:43:18,337 main.py:51] epoch 1999, training loss: 6238.87, average training loss: 6776.38, base loss: 9215.03
[INFO 2017-06-27 17:43:18,337 main.py:53] epoch 1999, testing
[INFO 2017-06-27 17:43:23,037 main.py:105] average testing loss: 6287.98, base loss: 8898.20
[INFO 2017-06-27 17:43:23,037 main.py:106] improve_loss: 2610.22, improve_percent: 0.29
[INFO 2017-06-27 17:43:23,039 main.py:76] current best improved percent: 0.30
[INFO 2017-06-27 17:43:24,247 main.py:51] epoch 2000, training loss: 5644.61, average training loss: 6775.30, base loss: 9213.70
[INFO 2017-06-27 17:43:25,456 main.py:51] epoch 2001, training loss: 6021.15, average training loss: 6774.27, base loss: 9212.91
[INFO 2017-06-27 17:43:26,665 main.py:51] epoch 2002, training loss: 6723.16, average training loss: 6773.92, base loss: 9212.61
[INFO 2017-06-27 17:43:27,875 main.py:51] epoch 2003, training loss: 6449.27, average training loss: 6773.51, base loss: 9212.82
[INFO 2017-06-27 17:43:29,086 main.py:51] epoch 2004, training loss: 6459.88, average training loss: 6773.17, base loss: 9212.60
[INFO 2017-06-27 17:43:30,293 main.py:51] epoch 2005, training loss: 6483.96, average training loss: 6773.08, base loss: 9213.97
[INFO 2017-06-27 17:43:31,505 main.py:51] epoch 2006, training loss: 6640.26, average training loss: 6772.50, base loss: 9214.06
[INFO 2017-06-27 17:43:32,715 main.py:51] epoch 2007, training loss: 6457.75, average training loss: 6772.62, base loss: 9214.64
[INFO 2017-06-27 17:43:33,925 main.py:51] epoch 2008, training loss: 6849.12, average training loss: 6772.36, base loss: 9215.62
[INFO 2017-06-27 17:43:35,134 main.py:51] epoch 2009, training loss: 6142.32, average training loss: 6771.63, base loss: 9214.72
[INFO 2017-06-27 17:43:36,342 main.py:51] epoch 2010, training loss: 5971.11, average training loss: 6770.55, base loss: 9214.61
[INFO 2017-06-27 17:43:37,549 main.py:51] epoch 2011, training loss: 6243.14, average training loss: 6770.28, base loss: 9215.71
[INFO 2017-06-27 17:43:38,753 main.py:51] epoch 2012, training loss: 5935.64, average training loss: 6769.19, base loss: 9215.03
[INFO 2017-06-27 17:43:39,961 main.py:51] epoch 2013, training loss: 6445.81, average training loss: 6768.99, base loss: 9216.75
[INFO 2017-06-27 17:43:41,174 main.py:51] epoch 2014, training loss: 6305.48, average training loss: 6768.17, base loss: 9216.31
[INFO 2017-06-27 17:43:42,386 main.py:51] epoch 2015, training loss: 6409.26, average training loss: 6767.97, base loss: 9217.52
[INFO 2017-06-27 17:43:43,594 main.py:51] epoch 2016, training loss: 6061.54, average training loss: 6767.25, base loss: 9217.03
[INFO 2017-06-27 17:43:44,804 main.py:51] epoch 2017, training loss: 6084.02, average training loss: 6766.36, base loss: 9215.85
[INFO 2017-06-27 17:43:46,016 main.py:51] epoch 2018, training loss: 6346.55, average training loss: 6765.43, base loss: 9215.37
[INFO 2017-06-27 17:43:47,226 main.py:51] epoch 2019, training loss: 6312.45, average training loss: 6765.27, base loss: 9216.40
[INFO 2017-06-27 17:43:48,438 main.py:51] epoch 2020, training loss: 6207.20, average training loss: 6764.27, base loss: 9215.57
[INFO 2017-06-27 17:43:49,650 main.py:51] epoch 2021, training loss: 6411.55, average training loss: 6764.16, base loss: 9216.69
[INFO 2017-06-27 17:43:50,859 main.py:51] epoch 2022, training loss: 9126.19, average training loss: 6766.12, base loss: 9220.25
[INFO 2017-06-27 17:43:52,071 main.py:51] epoch 2023, training loss: 6694.65, average training loss: 6766.04, base loss: 9221.39
[INFO 2017-06-27 17:43:53,282 main.py:51] epoch 2024, training loss: 6600.48, average training loss: 6765.79, base loss: 9222.01
[INFO 2017-06-27 17:43:54,495 main.py:51] epoch 2025, training loss: 6105.21, average training loss: 6765.22, base loss: 9221.91
[INFO 2017-06-27 17:43:55,705 main.py:51] epoch 2026, training loss: 8333.63, average training loss: 6766.87, base loss: 9225.59
[INFO 2017-06-27 17:43:56,912 main.py:51] epoch 2027, training loss: 6187.43, average training loss: 6765.69, base loss: 9224.52
[INFO 2017-06-27 17:43:58,123 main.py:51] epoch 2028, training loss: 6394.67, average training loss: 6764.87, base loss: 9223.96
[INFO 2017-06-27 17:43:59,332 main.py:51] epoch 2029, training loss: 6469.78, average training loss: 6764.91, base loss: 9225.79
[INFO 2017-06-27 17:44:00,540 main.py:51] epoch 2030, training loss: 5855.84, average training loss: 6764.30, base loss: 9226.16
[INFO 2017-06-27 17:44:01,750 main.py:51] epoch 2031, training loss: 5468.09, average training loss: 6762.07, base loss: 9223.41
[INFO 2017-06-27 17:44:02,954 main.py:51] epoch 2032, training loss: 6588.81, average training loss: 6761.78, base loss: 9224.45
[INFO 2017-06-27 17:44:04,159 main.py:51] epoch 2033, training loss: 5367.89, average training loss: 6760.25, base loss: 9222.47
[INFO 2017-06-27 17:44:05,368 main.py:51] epoch 2034, training loss: 6459.55, average training loss: 6760.23, base loss: 9223.81
[INFO 2017-06-27 17:44:06,576 main.py:51] epoch 2035, training loss: 5794.35, average training loss: 6759.37, base loss: 9223.32
[INFO 2017-06-27 17:44:07,785 main.py:51] epoch 2036, training loss: 5764.95, average training loss: 6758.38, base loss: 9222.34
[INFO 2017-06-27 17:44:08,996 main.py:51] epoch 2037, training loss: 6391.98, average training loss: 6758.24, base loss: 9222.79
[INFO 2017-06-27 17:44:10,207 main.py:51] epoch 2038, training loss: 6664.70, average training loss: 6758.16, base loss: 9222.91
[INFO 2017-06-27 17:44:11,417 main.py:51] epoch 2039, training loss: 6119.67, average training loss: 6758.41, base loss: 9223.86
[INFO 2017-06-27 17:44:12,626 main.py:51] epoch 2040, training loss: 6317.25, average training loss: 6758.79, base loss: 9225.32
[INFO 2017-06-27 17:44:13,833 main.py:51] epoch 2041, training loss: 9713.08, average training loss: 6761.86, base loss: 9228.53
[INFO 2017-06-27 17:44:15,042 main.py:51] epoch 2042, training loss: 6221.13, average training loss: 6761.52, base loss: 9228.92
[INFO 2017-06-27 17:44:16,253 main.py:51] epoch 2043, training loss: 6026.13, average training loss: 6761.09, base loss: 9228.97
[INFO 2017-06-27 17:44:17,463 main.py:51] epoch 2044, training loss: 6484.09, average training loss: 6761.27, base loss: 9229.54
[INFO 2017-06-27 17:44:18,669 main.py:51] epoch 2045, training loss: 5902.98, average training loss: 6760.18, base loss: 9227.91
[INFO 2017-06-27 17:44:19,874 main.py:51] epoch 2046, training loss: 6483.73, average training loss: 6759.65, base loss: 9227.27
[INFO 2017-06-27 17:44:21,081 main.py:51] epoch 2047, training loss: 6163.87, average training loss: 6756.13, base loss: 9224.54
[INFO 2017-06-27 17:44:22,291 main.py:51] epoch 2048, training loss: 8719.75, average training loss: 6758.60, base loss: 9229.10
[INFO 2017-06-27 17:44:23,499 main.py:51] epoch 2049, training loss: 6289.29, average training loss: 6754.58, base loss: 9225.35
[INFO 2017-06-27 17:44:24,709 main.py:51] epoch 2050, training loss: 6777.63, average training loss: 6754.91, base loss: 9226.27
[INFO 2017-06-27 17:44:25,918 main.py:51] epoch 2051, training loss: 6169.63, average training loss: 6753.92, base loss: 9225.08
[INFO 2017-06-27 17:44:27,128 main.py:51] epoch 2052, training loss: 6569.01, average training loss: 6750.73, base loss: 9222.92
[INFO 2017-06-27 17:44:28,341 main.py:51] epoch 2053, training loss: 6157.11, average training loss: 6749.68, base loss: 9222.23
[INFO 2017-06-27 17:44:29,554 main.py:51] epoch 2054, training loss: 5769.77, average training loss: 6745.27, base loss: 9218.23
[INFO 2017-06-27 17:44:30,762 main.py:51] epoch 2055, training loss: 6013.29, average training loss: 6737.64, base loss: 9210.52
[INFO 2017-06-27 17:44:31,971 main.py:51] epoch 2056, training loss: 6497.77, average training loss: 6733.22, base loss: 9206.30
[INFO 2017-06-27 17:44:33,181 main.py:51] epoch 2057, training loss: 6329.34, average training loss: 6733.00, base loss: 9206.55
[INFO 2017-06-27 17:44:34,392 main.py:51] epoch 2058, training loss: 6118.49, average training loss: 6731.94, base loss: 9205.81
[INFO 2017-06-27 17:44:35,599 main.py:51] epoch 2059, training loss: 6347.81, average training loss: 6731.75, base loss: 9206.05
[INFO 2017-06-27 17:44:36,812 main.py:51] epoch 2060, training loss: 6053.47, average training loss: 6730.54, base loss: 9204.53
[INFO 2017-06-27 17:44:38,024 main.py:51] epoch 2061, training loss: 6417.96, average training loss: 6730.71, base loss: 9205.83
[INFO 2017-06-27 17:44:39,234 main.py:51] epoch 2062, training loss: 6134.07, average training loss: 6729.81, base loss: 9205.26
[INFO 2017-06-27 17:44:40,446 main.py:51] epoch 2063, training loss: 6332.42, average training loss: 6729.55, base loss: 9205.82
[INFO 2017-06-27 17:44:41,658 main.py:51] epoch 2064, training loss: 6471.73, average training loss: 6729.33, base loss: 9206.59
[INFO 2017-06-27 17:44:42,866 main.py:51] epoch 2065, training loss: 6285.22, average training loss: 6728.82, base loss: 9206.18
[INFO 2017-06-27 17:44:44,076 main.py:51] epoch 2066, training loss: 9696.86, average training loss: 6731.85, base loss: 9209.87
[INFO 2017-06-27 17:44:45,283 main.py:51] epoch 2067, training loss: 6065.39, average training loss: 6731.46, base loss: 9209.45
[INFO 2017-06-27 17:44:46,496 main.py:51] epoch 2068, training loss: 6184.18, average training loss: 6730.71, base loss: 9208.37
[INFO 2017-06-27 17:44:47,707 main.py:51] epoch 2069, training loss: 9880.80, average training loss: 6733.66, base loss: 9211.49
[INFO 2017-06-27 17:44:48,917 main.py:51] epoch 2070, training loss: 6213.78, average training loss: 6733.11, base loss: 9210.71
[INFO 2017-06-27 17:44:50,129 main.py:51] epoch 2071, training loss: 5610.58, average training loss: 6732.32, base loss: 9210.31
[INFO 2017-06-27 17:44:51,338 main.py:51] epoch 2072, training loss: 6310.49, average training loss: 6732.67, base loss: 9211.87
[INFO 2017-06-27 17:44:52,549 main.py:51] epoch 2073, training loss: 6243.60, average training loss: 6732.55, base loss: 9212.91
[INFO 2017-06-27 17:44:53,761 main.py:51] epoch 2074, training loss: 5744.41, average training loss: 6731.40, base loss: 9212.21
[INFO 2017-06-27 17:44:54,967 main.py:51] epoch 2075, training loss: 5516.66, average training loss: 6726.41, base loss: 9206.83
[INFO 2017-06-27 17:44:56,172 main.py:51] epoch 2076, training loss: 5572.52, average training loss: 6724.38, base loss: 9204.08
[INFO 2017-06-27 17:44:57,380 main.py:51] epoch 2077, training loss: 6643.71, average training loss: 6723.86, base loss: 9204.41
[INFO 2017-06-27 17:44:58,588 main.py:51] epoch 2078, training loss: 5867.19, average training loss: 6722.83, base loss: 9204.13
[INFO 2017-06-27 17:44:59,799 main.py:51] epoch 2079, training loss: 6521.46, average training loss: 6722.20, base loss: 9203.54
[INFO 2017-06-27 17:45:01,004 main.py:51] epoch 2080, training loss: 6606.75, average training loss: 6722.29, base loss: 9204.71
[INFO 2017-06-27 17:45:02,210 main.py:51] epoch 2081, training loss: 6321.16, average training loss: 6722.26, base loss: 9206.27
[INFO 2017-06-27 17:45:03,420 main.py:51] epoch 2082, training loss: 6656.53, average training loss: 6721.73, base loss: 9207.58
[INFO 2017-06-27 17:45:04,629 main.py:51] epoch 2083, training loss: 6098.22, average training loss: 6720.82, base loss: 9207.26
[INFO 2017-06-27 17:45:05,838 main.py:51] epoch 2084, training loss: 6220.60, average training loss: 6719.97, base loss: 9207.15
[INFO 2017-06-27 17:45:07,045 main.py:51] epoch 2085, training loss: 5906.56, average training loss: 6718.86, base loss: 9206.12
[INFO 2017-06-27 17:45:08,254 main.py:51] epoch 2086, training loss: 6011.41, average training loss: 6718.65, base loss: 9206.53
[INFO 2017-06-27 17:45:09,461 main.py:51] epoch 2087, training loss: 6655.07, average training loss: 6718.18, base loss: 9206.66
[INFO 2017-06-27 17:45:10,667 main.py:51] epoch 2088, training loss: 6565.17, average training loss: 6718.09, base loss: 9208.38
[INFO 2017-06-27 17:45:11,875 main.py:51] epoch 2089, training loss: 6356.13, average training loss: 6718.09, base loss: 9209.42
[INFO 2017-06-27 17:45:13,085 main.py:51] epoch 2090, training loss: 6264.79, average training loss: 6717.83, base loss: 9209.54
[INFO 2017-06-27 17:45:14,295 main.py:51] epoch 2091, training loss: 10086.07, average training loss: 6721.13, base loss: 9213.60
[INFO 2017-06-27 17:45:15,506 main.py:51] epoch 2092, training loss: 5904.68, average training loss: 6720.09, base loss: 9212.99
[INFO 2017-06-27 17:45:16,716 main.py:51] epoch 2093, training loss: 5934.50, average training loss: 6719.74, base loss: 9212.88
[INFO 2017-06-27 17:45:17,926 main.py:51] epoch 2094, training loss: 6667.17, average training loss: 6719.73, base loss: 9214.14
[INFO 2017-06-27 17:45:19,136 main.py:51] epoch 2095, training loss: 5804.51, average training loss: 6718.77, base loss: 9213.24
[INFO 2017-06-27 17:45:20,349 main.py:51] epoch 2096, training loss: 6202.25, average training loss: 6718.64, base loss: 9214.10
[INFO 2017-06-27 17:45:21,556 main.py:51] epoch 2097, training loss: 5831.07, average training loss: 6717.65, base loss: 9214.03
[INFO 2017-06-27 17:45:22,765 main.py:51] epoch 2098, training loss: 6170.95, average training loss: 6716.74, base loss: 9213.44
[INFO 2017-06-27 17:45:23,975 main.py:51] epoch 2099, training loss: 6133.86, average training loss: 6715.81, base loss: 9212.57
[INFO 2017-06-27 17:45:23,976 main.py:53] epoch 2099, testing
[INFO 2017-06-27 17:45:28,660 main.py:105] average testing loss: 6834.44, base loss: 9580.64
[INFO 2017-06-27 17:45:28,660 main.py:106] improve_loss: 2746.21, improve_percent: 0.29
[INFO 2017-06-27 17:45:28,661 main.py:76] current best improved percent: 0.30
[INFO 2017-06-27 17:45:29,881 main.py:51] epoch 2100, training loss: 6691.58, average training loss: 6715.98, base loss: 9214.27
[INFO 2017-06-27 17:45:31,093 main.py:51] epoch 2101, training loss: 9884.37, average training loss: 6718.80, base loss: 9217.90
[INFO 2017-06-27 17:45:32,301 main.py:51] epoch 2102, training loss: 6038.79, average training loss: 6718.18, base loss: 9217.77
[INFO 2017-06-27 17:45:33,508 main.py:51] epoch 2103, training loss: 6144.46, average training loss: 6716.81, base loss: 9215.79
[INFO 2017-06-27 17:45:34,719 main.py:51] epoch 2104, training loss: 5904.74, average training loss: 6714.61, base loss: 9213.17
[INFO 2017-06-27 17:45:35,927 main.py:51] epoch 2105, training loss: 6870.80, average training loss: 6714.60, base loss: 9213.48
[INFO 2017-06-27 17:45:37,137 main.py:51] epoch 2106, training loss: 5887.46, average training loss: 6713.77, base loss: 9212.40
[INFO 2017-06-27 17:45:38,347 main.py:51] epoch 2107, training loss: 6201.97, average training loss: 6713.65, base loss: 9212.91
[INFO 2017-06-27 17:45:39,555 main.py:51] epoch 2108, training loss: 5622.88, average training loss: 6708.52, base loss: 9207.18
[INFO 2017-06-27 17:45:40,766 main.py:51] epoch 2109, training loss: 6043.48, average training loss: 6708.28, base loss: 9207.64
[INFO 2017-06-27 17:45:41,976 main.py:51] epoch 2110, training loss: 6327.03, average training loss: 6708.03, base loss: 9208.33
[INFO 2017-06-27 17:45:43,185 main.py:51] epoch 2111, training loss: 6813.83, average training loss: 6708.07, base loss: 9209.60
[INFO 2017-06-27 17:45:44,395 main.py:51] epoch 2112, training loss: 6180.07, average training loss: 6707.57, base loss: 9209.90
[INFO 2017-06-27 17:45:45,605 main.py:51] epoch 2113, training loss: 6795.64, average training loss: 6706.73, base loss: 9209.62
[INFO 2017-06-27 17:45:46,815 main.py:51] epoch 2114, training loss: 9643.52, average training loss: 6709.07, base loss: 9211.64
[INFO 2017-06-27 17:45:48,023 main.py:51] epoch 2115, training loss: 5969.21, average training loss: 6707.94, base loss: 9209.92
[INFO 2017-06-27 17:45:49,232 main.py:51] epoch 2116, training loss: 6646.58, average training loss: 6708.06, base loss: 9210.52
[INFO 2017-06-27 17:45:50,442 main.py:51] epoch 2117, training loss: 6507.55, average training loss: 6708.31, base loss: 9211.21
[INFO 2017-06-27 17:45:51,654 main.py:51] epoch 2118, training loss: 6048.06, average training loss: 6708.00, base loss: 9212.33
[INFO 2017-06-27 17:45:52,863 main.py:51] epoch 2119, training loss: 5463.61, average training loss: 6706.87, base loss: 9211.20
[INFO 2017-06-27 17:45:54,073 main.py:51] epoch 2120, training loss: 5984.78, average training loss: 6706.25, base loss: 9211.06
[INFO 2017-06-27 17:45:55,286 main.py:51] epoch 2121, training loss: 6377.80, average training loss: 6706.04, base loss: 9211.71
[INFO 2017-06-27 17:45:56,494 main.py:51] epoch 2122, training loss: 6333.36, average training loss: 6705.92, base loss: 9212.63
[INFO 2017-06-27 17:45:57,706 main.py:51] epoch 2123, training loss: 6245.73, average training loss: 6705.16, base loss: 9211.81
[INFO 2017-06-27 17:45:58,917 main.py:51] epoch 2124, training loss: 5947.54, average training loss: 6703.71, base loss: 9209.61
[INFO 2017-06-27 17:46:00,123 main.py:51] epoch 2125, training loss: 6159.90, average training loss: 6702.34, base loss: 9207.43
[INFO 2017-06-27 17:46:01,331 main.py:51] epoch 2126, training loss: 8379.55, average training loss: 6703.71, base loss: 9210.96
[INFO 2017-06-27 17:46:02,538 main.py:51] epoch 2127, training loss: 6140.58, average training loss: 6703.08, base loss: 9211.04
[INFO 2017-06-27 17:46:03,748 main.py:51] epoch 2128, training loss: 6148.76, average training loss: 6701.82, base loss: 9210.44
[INFO 2017-06-27 17:46:04,961 main.py:51] epoch 2129, training loss: 5717.03, average training loss: 6701.29, base loss: 9209.82
[INFO 2017-06-27 17:46:06,171 main.py:51] epoch 2130, training loss: 10773.20, average training loss: 6704.87, base loss: 9214.86
[INFO 2017-06-27 17:46:07,379 main.py:51] epoch 2131, training loss: 5553.96, average training loss: 6703.50, base loss: 9213.28
[INFO 2017-06-27 17:46:08,590 main.py:51] epoch 2132, training loss: 6525.80, average training loss: 6702.97, base loss: 9213.28
[INFO 2017-06-27 17:46:09,804 main.py:51] epoch 2133, training loss: 6430.99, average training loss: 6702.92, base loss: 9214.48
[INFO 2017-06-27 17:46:11,018 main.py:51] epoch 2134, training loss: 6408.15, average training loss: 6702.32, base loss: 9214.20
[INFO 2017-06-27 17:46:12,233 main.py:51] epoch 2135, training loss: 5885.82, average training loss: 6701.67, base loss: 9214.11
[INFO 2017-06-27 17:46:13,446 main.py:51] epoch 2136, training loss: 6985.44, average training loss: 6701.40, base loss: 9214.70
[INFO 2017-06-27 17:46:14,659 main.py:51] epoch 2137, training loss: 6707.91, average training loss: 6701.98, base loss: 9216.92
[INFO 2017-06-27 17:46:15,867 main.py:51] epoch 2138, training loss: 6188.42, average training loss: 6702.01, base loss: 9217.33
[INFO 2017-06-27 17:46:17,075 main.py:51] epoch 2139, training loss: 5945.35, average training loss: 6701.58, base loss: 9217.64
[INFO 2017-06-27 17:46:18,290 main.py:51] epoch 2140, training loss: 10398.46, average training loss: 6705.70, base loss: 9223.00
[INFO 2017-06-27 17:46:19,500 main.py:51] epoch 2141, training loss: 6249.50, average training loss: 6704.17, base loss: 9221.41
[INFO 2017-06-27 17:46:20,712 main.py:51] epoch 2142, training loss: 5879.40, average training loss: 6702.15, base loss: 9219.09
[INFO 2017-06-27 17:46:21,920 main.py:51] epoch 2143, training loss: 6972.16, average training loss: 6702.44, base loss: 9221.85
[INFO 2017-06-27 17:46:23,130 main.py:51] epoch 2144, training loss: 6347.53, average training loss: 6702.15, base loss: 9222.92
[INFO 2017-06-27 17:46:24,337 main.py:51] epoch 2145, training loss: 5889.42, average training loss: 6701.43, base loss: 9221.69
[INFO 2017-06-27 17:46:25,543 main.py:51] epoch 2146, training loss: 6334.84, average training loss: 6700.67, base loss: 9221.20
[INFO 2017-06-27 17:46:26,752 main.py:51] epoch 2147, training loss: 6280.21, average training loss: 6700.04, base loss: 9221.12
[INFO 2017-06-27 17:46:27,964 main.py:51] epoch 2148, training loss: 6470.29, average training loss: 6699.71, base loss: 9221.39
[INFO 2017-06-27 17:46:29,170 main.py:51] epoch 2149, training loss: 5649.01, average training loss: 6698.20, base loss: 9218.92
[INFO 2017-06-27 17:46:30,379 main.py:51] epoch 2150, training loss: 6207.85, average training loss: 6697.22, base loss: 9218.14
[INFO 2017-06-27 17:46:31,592 main.py:51] epoch 2151, training loss: 7838.35, average training loss: 6697.92, base loss: 9219.89
[INFO 2017-06-27 17:46:32,806 main.py:51] epoch 2152, training loss: 6156.97, average training loss: 6697.64, base loss: 9219.90
[INFO 2017-06-27 17:46:34,017 main.py:51] epoch 2153, training loss: 6414.03, average training loss: 6697.31, base loss: 9220.68
[INFO 2017-06-27 17:46:35,227 main.py:51] epoch 2154, training loss: 6103.58, average training loss: 6697.74, base loss: 9222.27
[INFO 2017-06-27 17:46:36,439 main.py:51] epoch 2155, training loss: 6567.49, average training loss: 6698.05, base loss: 9223.75
[INFO 2017-06-27 17:46:37,648 main.py:51] epoch 2156, training loss: 5694.17, average training loss: 6697.04, base loss: 9223.12
[INFO 2017-06-27 17:46:38,859 main.py:51] epoch 2157, training loss: 5829.61, average training loss: 6696.68, base loss: 9223.51
[INFO 2017-06-27 17:46:40,068 main.py:51] epoch 2158, training loss: 6053.04, average training loss: 6695.76, base loss: 9223.02
[INFO 2017-06-27 17:46:41,284 main.py:51] epoch 2159, training loss: 5862.92, average training loss: 6694.89, base loss: 9222.35
[INFO 2017-06-27 17:46:42,495 main.py:51] epoch 2160, training loss: 5719.71, average training loss: 6693.72, base loss: 9221.10
[INFO 2017-06-27 17:46:43,706 main.py:51] epoch 2161, training loss: 6352.16, average training loss: 6693.56, base loss: 9221.71
[INFO 2017-06-27 17:46:44,920 main.py:51] epoch 2162, training loss: 6442.32, average training loss: 6693.37, base loss: 9222.24
[INFO 2017-06-27 17:46:46,132 main.py:51] epoch 2163, training loss: 8744.12, average training loss: 6695.99, base loss: 9227.36
[INFO 2017-06-27 17:46:47,343 main.py:51] epoch 2164, training loss: 6012.86, average training loss: 6695.66, base loss: 9227.91
[INFO 2017-06-27 17:46:48,546 main.py:51] epoch 2165, training loss: 6208.27, average training loss: 6695.49, base loss: 9228.38
[INFO 2017-06-27 17:46:49,760 main.py:51] epoch 2166, training loss: 5951.27, average training loss: 6695.09, base loss: 9228.64
[INFO 2017-06-27 17:46:50,973 main.py:51] epoch 2167, training loss: 5941.71, average training loss: 6693.54, base loss: 9226.80
[INFO 2017-06-27 17:46:52,185 main.py:51] epoch 2168, training loss: 6829.99, average training loss: 6693.91, base loss: 9228.01
[INFO 2017-06-27 17:46:53,397 main.py:51] epoch 2169, training loss: 5898.76, average training loss: 6690.25, base loss: 9225.16
[INFO 2017-06-27 17:46:54,613 main.py:51] epoch 2170, training loss: 6209.86, average training loss: 6689.88, base loss: 9225.57
[INFO 2017-06-27 17:46:55,828 main.py:51] epoch 2171, training loss: 9286.71, average training loss: 6692.52, base loss: 9228.99
[INFO 2017-06-27 17:46:57,039 main.py:51] epoch 2172, training loss: 6098.52, average training loss: 6692.08, base loss: 9229.09
[INFO 2017-06-27 17:46:58,249 main.py:51] epoch 2173, training loss: 6600.12, average training loss: 6692.18, base loss: 9230.50
[INFO 2017-06-27 17:46:59,463 main.py:51] epoch 2174, training loss: 6418.77, average training loss: 6692.05, base loss: 9230.97
[INFO 2017-06-27 17:47:00,672 main.py:51] epoch 2175, training loss: 5999.43, average training loss: 6691.83, base loss: 9231.32
[INFO 2017-06-27 17:47:01,885 main.py:51] epoch 2176, training loss: 6234.98, average training loss: 6691.55, base loss: 9231.40
[INFO 2017-06-27 17:47:03,098 main.py:51] epoch 2177, training loss: 6560.47, average training loss: 6690.80, base loss: 9231.57
[INFO 2017-06-27 17:47:04,310 main.py:51] epoch 2178, training loss: 6047.27, average training loss: 6689.60, base loss: 9230.16
[INFO 2017-06-27 17:47:05,525 main.py:51] epoch 2179, training loss: 6189.34, average training loss: 6689.26, base loss: 9229.81
[INFO 2017-06-27 17:47:06,738 main.py:51] epoch 2180, training loss: 6108.56, average training loss: 6688.85, base loss: 9229.90
[INFO 2017-06-27 17:47:07,952 main.py:51] epoch 2181, training loss: 6700.42, average training loss: 6688.80, base loss: 9230.61
[INFO 2017-06-27 17:47:09,165 main.py:51] epoch 2182, training loss: 6524.36, average training loss: 6689.07, base loss: 9232.46
[INFO 2017-06-27 17:47:10,378 main.py:51] epoch 2183, training loss: 6058.75, average training loss: 6688.32, base loss: 9231.93
[INFO 2017-06-27 17:47:11,589 main.py:51] epoch 2184, training loss: 6627.73, average training loss: 6688.28, base loss: 9232.70
[INFO 2017-06-27 17:47:12,802 main.py:51] epoch 2185, training loss: 8233.07, average training loss: 6683.28, base loss: 9229.45
[INFO 2017-06-27 17:47:14,010 main.py:51] epoch 2186, training loss: 6200.92, average training loss: 6682.68, base loss: 9229.30
[INFO 2017-06-27 17:47:15,224 main.py:51] epoch 2187, training loss: 6268.75, average training loss: 6682.25, base loss: 9229.73
[INFO 2017-06-27 17:47:16,432 main.py:51] epoch 2188, training loss: 5974.50, average training loss: 6681.05, base loss: 9227.97
[INFO 2017-06-27 17:47:17,644 main.py:51] epoch 2189, training loss: 6569.98, average training loss: 6677.96, base loss: 9226.10
[INFO 2017-06-27 17:47:18,858 main.py:51] epoch 2190, training loss: 6435.22, average training loss: 6677.54, base loss: 9226.11
[INFO 2017-06-27 17:47:20,068 main.py:51] epoch 2191, training loss: 6140.70, average training loss: 6676.91, base loss: 9225.25
[INFO 2017-06-27 17:47:21,280 main.py:51] epoch 2192, training loss: 6388.48, average training loss: 6676.72, base loss: 9225.87
[INFO 2017-06-27 17:47:22,490 main.py:51] epoch 2193, training loss: 6034.20, average training loss: 6675.47, base loss: 9224.06
[INFO 2017-06-27 17:47:23,701 main.py:51] epoch 2194, training loss: 6541.04, average training loss: 6675.34, base loss: 9224.71
[INFO 2017-06-27 17:47:24,906 main.py:51] epoch 2195, training loss: 8581.98, average training loss: 6677.83, base loss: 9229.66
[INFO 2017-06-27 17:47:26,118 main.py:51] epoch 2196, training loss: 6563.73, average training loss: 6677.77, base loss: 9231.10
[INFO 2017-06-27 17:47:27,332 main.py:51] epoch 2197, training loss: 6206.67, average training loss: 6676.64, base loss: 9230.05
[INFO 2017-06-27 17:47:28,543 main.py:51] epoch 2198, training loss: 6088.63, average training loss: 6676.08, base loss: 9229.74
[INFO 2017-06-27 17:47:29,754 main.py:51] epoch 2199, training loss: 6248.33, average training loss: 6675.72, base loss: 9230.13
[INFO 2017-06-27 17:47:29,755 main.py:53] epoch 2199, testing
[INFO 2017-06-27 17:47:34,454 main.py:105] average testing loss: 6939.54, base loss: 9704.45
[INFO 2017-06-27 17:47:34,455 main.py:106] improve_loss: 2764.92, improve_percent: 0.28
[INFO 2017-06-27 17:47:34,455 main.py:76] current best improved percent: 0.30
[INFO 2017-06-27 17:47:35,665 main.py:51] epoch 2200, training loss: 5700.40, average training loss: 6674.40, base loss: 9228.23
[INFO 2017-06-27 17:47:36,881 main.py:51] epoch 2201, training loss: 5836.71, average training loss: 6673.47, base loss: 9227.82
[INFO 2017-06-27 17:47:38,092 main.py:51] epoch 2202, training loss: 5988.69, average training loss: 6673.17, base loss: 9228.60
[INFO 2017-06-27 17:47:39,301 main.py:51] epoch 2203, training loss: 5963.59, average training loss: 6672.23, base loss: 9227.43
[INFO 2017-06-27 17:47:40,512 main.py:51] epoch 2204, training loss: 5943.58, average training loss: 6668.02, base loss: 9223.33
[INFO 2017-06-27 17:47:41,723 main.py:51] epoch 2205, training loss: 5931.99, average training loss: 6667.27, base loss: 9223.04
[INFO 2017-06-27 17:47:42,936 main.py:51] epoch 2206, training loss: 6266.36, average training loss: 6667.18, base loss: 9223.68
[INFO 2017-06-27 17:47:44,145 main.py:51] epoch 2207, training loss: 5899.17, average training loss: 6667.03, base loss: 9224.49
[INFO 2017-06-27 17:47:45,355 main.py:51] epoch 2208, training loss: 6048.44, average training loss: 6666.54, base loss: 9224.35
[INFO 2017-06-27 17:47:46,567 main.py:51] epoch 2209, training loss: 6262.41, average training loss: 6665.50, base loss: 9223.20
[INFO 2017-06-27 17:47:47,782 main.py:51] epoch 2210, training loss: 7899.62, average training loss: 6666.59, base loss: 9225.79
[INFO 2017-06-27 17:47:48,995 main.py:51] epoch 2211, training loss: 6072.68, average training loss: 6666.13, base loss: 9225.83
[INFO 2017-06-27 17:47:50,207 main.py:51] epoch 2212, training loss: 5984.80, average training loss: 6665.71, base loss: 9226.37
[INFO 2017-06-27 17:47:51,417 main.py:51] epoch 2213, training loss: 6185.95, average training loss: 6664.61, base loss: 9224.74
[INFO 2017-06-27 17:47:52,632 main.py:51] epoch 2214, training loss: 6654.11, average training loss: 6664.41, base loss: 9225.76
[INFO 2017-06-27 17:47:53,846 main.py:51] epoch 2215, training loss: 6380.54, average training loss: 6664.37, base loss: 9226.63
[INFO 2017-06-27 17:47:55,057 main.py:51] epoch 2216, training loss: 5739.04, average training loss: 6663.40, base loss: 9225.87
[INFO 2017-06-27 17:47:56,267 main.py:51] epoch 2217, training loss: 6213.89, average training loss: 6663.28, base loss: 9226.37
[INFO 2017-06-27 17:47:57,479 main.py:51] epoch 2218, training loss: 5792.61, average training loss: 6659.49, base loss: 9222.53
[INFO 2017-06-27 17:47:58,693 main.py:51] epoch 2219, training loss: 6116.61, average training loss: 6658.38, base loss: 9222.00
[INFO 2017-06-27 17:47:59,904 main.py:51] epoch 2220, training loss: 6286.64, average training loss: 6654.85, base loss: 9219.54
[INFO 2017-06-27 17:48:01,120 main.py:51] epoch 2221, training loss: 6002.47, average training loss: 6654.18, base loss: 9218.80
[INFO 2017-06-27 17:48:02,332 main.py:51] epoch 2222, training loss: 6488.86, average training loss: 6650.09, base loss: 9215.86
[INFO 2017-06-27 17:48:03,542 main.py:51] epoch 2223, training loss: 5939.49, average training loss: 6649.01, base loss: 9215.27
[INFO 2017-06-27 17:48:04,756 main.py:51] epoch 2224, training loss: 6096.71, average training loss: 6648.52, base loss: 9215.34
[INFO 2017-06-27 17:48:05,967 main.py:51] epoch 2225, training loss: 6073.94, average training loss: 6647.95, base loss: 9215.14
[INFO 2017-06-27 17:48:07,178 main.py:51] epoch 2226, training loss: 6479.54, average training loss: 6647.67, base loss: 9215.34
[INFO 2017-06-27 17:48:08,390 main.py:51] epoch 2227, training loss: 5730.35, average training loss: 6646.69, base loss: 9214.67
[INFO 2017-06-27 17:48:09,603 main.py:51] epoch 2228, training loss: 6014.39, average training loss: 6646.08, base loss: 9215.03
[INFO 2017-06-27 17:48:10,817 main.py:51] epoch 2229, training loss: 6481.20, average training loss: 6645.93, base loss: 9216.11
[INFO 2017-06-27 17:48:12,029 main.py:51] epoch 2230, training loss: 6034.70, average training loss: 6645.58, base loss: 9215.73
[INFO 2017-06-27 17:48:13,240 main.py:51] epoch 2231, training loss: 5840.96, average training loss: 6641.74, base loss: 9211.81
[INFO 2017-06-27 17:48:14,449 main.py:51] epoch 2232, training loss: 6556.23, average training loss: 6641.22, base loss: 9211.72
[INFO 2017-06-27 17:48:15,661 main.py:51] epoch 2233, training loss: 8541.99, average training loss: 6642.81, base loss: 9214.39
[INFO 2017-06-27 17:48:16,874 main.py:51] epoch 2234, training loss: 6515.38, average training loss: 6642.32, base loss: 9214.20
[INFO 2017-06-27 17:48:18,084 main.py:51] epoch 2235, training loss: 6459.16, average training loss: 6642.26, base loss: 9214.79
[INFO 2017-06-27 17:48:19,296 main.py:51] epoch 2236, training loss: 6163.49, average training loss: 6641.43, base loss: 9214.15
[INFO 2017-06-27 17:48:20,510 main.py:51] epoch 2237, training loss: 6170.90, average training loss: 6640.76, base loss: 9213.69
[INFO 2017-06-27 17:48:21,723 main.py:51] epoch 2238, training loss: 6900.17, average training loss: 6641.26, base loss: 9215.98
[INFO 2017-06-27 17:48:22,933 main.py:51] epoch 2239, training loss: 6373.35, average training loss: 6641.21, base loss: 9216.40
[INFO 2017-06-27 17:48:24,143 main.py:51] epoch 2240, training loss: 8685.67, average training loss: 6643.47, base loss: 9220.80
[INFO 2017-06-27 17:48:25,354 main.py:51] epoch 2241, training loss: 6118.19, average training loss: 6639.03, base loss: 9216.47
[INFO 2017-06-27 17:48:26,571 main.py:51] epoch 2242, training loss: 9409.10, average training loss: 6641.00, base loss: 9218.49
[INFO 2017-06-27 17:48:27,781 main.py:51] epoch 2243, training loss: 6225.73, average training loss: 6640.47, base loss: 9218.57
[INFO 2017-06-27 17:48:28,993 main.py:51] epoch 2244, training loss: 6625.77, average training loss: 6640.56, base loss: 9220.25
[INFO 2017-06-27 17:48:30,202 main.py:51] epoch 2245, training loss: 5903.48, average training loss: 6640.34, base loss: 9220.64
[INFO 2017-06-27 17:48:31,415 main.py:51] epoch 2246, training loss: 6341.10, average training loss: 6640.28, base loss: 9221.71
[INFO 2017-06-27 17:48:32,629 main.py:51] epoch 2247, training loss: 6067.11, average training loss: 6640.00, base loss: 9222.33
[INFO 2017-06-27 17:48:33,843 main.py:51] epoch 2248, training loss: 5965.12, average training loss: 6639.98, base loss: 9223.12
[INFO 2017-06-27 17:48:35,054 main.py:51] epoch 2249, training loss: 6327.40, average training loss: 6640.20, base loss: 9225.06
[INFO 2017-06-27 17:48:36,270 main.py:51] epoch 2250, training loss: 9258.36, average training loss: 6643.07, base loss: 9230.20
[INFO 2017-06-27 17:48:37,482 main.py:51] epoch 2251, training loss: 5532.02, average training loss: 6641.65, base loss: 9228.71
[INFO 2017-06-27 17:48:38,694 main.py:51] epoch 2252, training loss: 5875.83, average training loss: 6640.62, base loss: 9228.15
[INFO 2017-06-27 17:48:39,909 main.py:51] epoch 2253, training loss: 6428.87, average training loss: 6639.92, base loss: 9228.12
[INFO 2017-06-27 17:48:41,122 main.py:51] epoch 2254, training loss: 6742.73, average training loss: 6640.03, base loss: 9229.13
[INFO 2017-06-27 17:48:42,334 main.py:51] epoch 2255, training loss: 6281.92, average training loss: 6640.12, base loss: 9230.11
[INFO 2017-06-27 17:48:43,544 main.py:51] epoch 2256, training loss: 6272.93, average training loss: 6639.89, base loss: 9230.54
[INFO 2017-06-27 17:48:44,756 main.py:51] epoch 2257, training loss: 6057.85, average training loss: 6639.37, base loss: 9230.49
[INFO 2017-06-27 17:48:45,970 main.py:51] epoch 2258, training loss: 6286.06, average training loss: 6632.16, base loss: 9223.36
[INFO 2017-06-27 17:48:47,185 main.py:51] epoch 2259, training loss: 6415.93, average training loss: 6631.12, base loss: 9223.15
[INFO 2017-06-27 17:48:48,398 main.py:51] epoch 2260, training loss: 6380.46, average training loss: 6631.07, base loss: 9223.91
[INFO 2017-06-27 17:48:49,612 main.py:51] epoch 2261, training loss: 6200.92, average training loss: 6630.49, base loss: 9223.94
[INFO 2017-06-27 17:48:50,820 main.py:51] epoch 2262, training loss: 6327.25, average training loss: 6629.80, base loss: 9223.73
[INFO 2017-06-27 17:48:52,031 main.py:51] epoch 2263, training loss: 6297.20, average training loss: 6625.58, base loss: 9219.69
[INFO 2017-06-27 17:48:53,245 main.py:51] epoch 2264, training loss: 6139.10, average training loss: 6625.03, base loss: 9219.44
[INFO 2017-06-27 17:48:54,458 main.py:51] epoch 2265, training loss: 6200.89, average training loss: 6624.80, base loss: 9219.85
[INFO 2017-06-27 17:48:55,666 main.py:51] epoch 2266, training loss: 6235.76, average training loss: 6624.10, base loss: 9219.39
[INFO 2017-06-27 17:48:56,879 main.py:51] epoch 2267, training loss: 6070.92, average training loss: 6619.93, base loss: 9215.22
[INFO 2017-06-27 17:48:58,091 main.py:51] epoch 2268, training loss: 6478.15, average training loss: 6619.98, base loss: 9215.53
[INFO 2017-06-27 17:48:59,304 main.py:51] epoch 2269, training loss: 6290.66, average training loss: 6619.48, base loss: 9215.09
[INFO 2017-06-27 17:49:00,516 main.py:51] epoch 2270, training loss: 5539.29, average training loss: 6618.55, base loss: 9214.34
[INFO 2017-06-27 17:49:01,729 main.py:51] epoch 2271, training loss: 6070.73, average training loss: 6618.31, base loss: 9214.77
[INFO 2017-06-27 17:49:02,942 main.py:51] epoch 2272, training loss: 6368.50, average training loss: 6617.73, base loss: 9214.05
[INFO 2017-06-27 17:49:04,154 main.py:51] epoch 2273, training loss: 5861.67, average training loss: 6617.11, base loss: 9214.15
[INFO 2017-06-27 17:49:05,367 main.py:51] epoch 2274, training loss: 6793.91, average training loss: 6617.51, base loss: 9216.11
[INFO 2017-06-27 17:49:06,575 main.py:51] epoch 2275, training loss: 6386.06, average training loss: 6617.40, base loss: 9217.33
[INFO 2017-06-27 17:49:07,789 main.py:51] epoch 2276, training loss: 6243.55, average training loss: 6616.90, base loss: 9217.64
[INFO 2017-06-27 17:49:08,999 main.py:51] epoch 2277, training loss: 6287.57, average training loss: 6616.05, base loss: 9217.05
[INFO 2017-06-27 17:49:10,209 main.py:51] epoch 2278, training loss: 6746.49, average training loss: 6616.38, base loss: 9219.38
[INFO 2017-06-27 17:49:11,421 main.py:51] epoch 2279, training loss: 8945.45, average training loss: 6618.64, base loss: 9223.37
[INFO 2017-06-27 17:49:12,634 main.py:51] epoch 2280, training loss: 6352.69, average training loss: 6618.49, base loss: 9224.24
[INFO 2017-06-27 17:49:13,844 main.py:51] epoch 2281, training loss: 6442.99, average training loss: 6618.23, base loss: 9224.44
[INFO 2017-06-27 17:49:15,056 main.py:51] epoch 2282, training loss: 6017.74, average training loss: 6617.10, base loss: 9223.27
[INFO 2017-06-27 17:49:16,269 main.py:51] epoch 2283, training loss: 6001.48, average training loss: 6616.16, base loss: 9221.57
[INFO 2017-06-27 17:49:17,479 main.py:51] epoch 2284, training loss: 5750.69, average training loss: 6613.04, base loss: 9218.83
[INFO 2017-06-27 17:49:18,691 main.py:51] epoch 2285, training loss: 6095.09, average training loss: 6612.80, base loss: 9219.42
[INFO 2017-06-27 17:49:19,902 main.py:51] epoch 2286, training loss: 5489.17, average training loss: 6611.82, base loss: 9218.08
[INFO 2017-06-27 17:49:21,112 main.py:51] epoch 2287, training loss: 6682.34, average training loss: 6611.34, base loss: 9218.15
[INFO 2017-06-27 17:49:22,323 main.py:51] epoch 2288, training loss: 6335.80, average training loss: 6611.60, base loss: 9219.74
[INFO 2017-06-27 17:49:23,535 main.py:51] epoch 2289, training loss: 6063.38, average training loss: 6611.19, base loss: 9220.11
[INFO 2017-06-27 17:49:24,750 main.py:51] epoch 2290, training loss: 5831.62, average training loss: 6610.01, base loss: 9219.36
[INFO 2017-06-27 17:49:25,962 main.py:51] epoch 2291, training loss: 6139.33, average training loss: 6609.37, base loss: 9218.96
[INFO 2017-06-27 17:49:27,173 main.py:51] epoch 2292, training loss: 6170.55, average training loss: 6609.18, base loss: 9219.59
[INFO 2017-06-27 17:49:28,379 main.py:51] epoch 2293, training loss: 5973.01, average training loss: 6609.05, base loss: 9220.17
[INFO 2017-06-27 17:49:29,590 main.py:51] epoch 2294, training loss: 6144.20, average training loss: 6608.62, base loss: 9220.11
[INFO 2017-06-27 17:49:30,799 main.py:51] epoch 2295, training loss: 6619.31, average training loss: 6609.30, base loss: 9222.49
[INFO 2017-06-27 17:49:32,006 main.py:51] epoch 2296, training loss: 6064.97, average training loss: 6608.48, base loss: 9222.00
[INFO 2017-06-27 17:49:33,220 main.py:51] epoch 2297, training loss: 6893.20, average training loss: 6608.88, base loss: 9223.90
[INFO 2017-06-27 17:49:34,431 main.py:51] epoch 2298, training loss: 6017.72, average training loss: 6608.15, base loss: 9223.35
[INFO 2017-06-27 17:49:35,638 main.py:51] epoch 2299, training loss: 6035.92, average training loss: 6607.69, base loss: 9223.03
[INFO 2017-06-27 17:49:35,638 main.py:53] epoch 2299, testing
[INFO 2017-06-27 17:49:40,331 main.py:105] average testing loss: 6405.88, base loss: 9353.26
[INFO 2017-06-27 17:49:40,331 main.py:106] improve_loss: 2947.38, improve_percent: 0.32
[INFO 2017-06-27 17:49:40,332 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 17:49:40,344 main.py:76] current best improved percent: 0.32
[INFO 2017-06-27 17:49:41,550 main.py:51] epoch 2300, training loss: 6434.20, average training loss: 6607.54, base loss: 9223.72
[INFO 2017-06-27 17:49:42,767 main.py:51] epoch 2301, training loss: 9326.99, average training loss: 6610.22, base loss: 9226.40
[INFO 2017-06-27 17:49:43,979 main.py:51] epoch 2302, training loss: 6192.21, average training loss: 6609.74, base loss: 9226.29
[INFO 2017-06-27 17:49:45,192 main.py:51] epoch 2303, training loss: 5828.25, average training loss: 6608.87, base loss: 9225.07
[INFO 2017-06-27 17:49:46,406 main.py:51] epoch 2304, training loss: 5776.42, average training loss: 6607.68, base loss: 9223.42
[INFO 2017-06-27 17:49:47,620 main.py:51] epoch 2305, training loss: 8229.92, average training loss: 6609.32, base loss: 9226.63
[INFO 2017-06-27 17:49:48,836 main.py:51] epoch 2306, training loss: 5882.39, average training loss: 6608.00, base loss: 9224.78
[INFO 2017-06-27 17:49:50,045 main.py:51] epoch 2307, training loss: 6741.28, average training loss: 6608.35, base loss: 9226.11
[INFO 2017-06-27 17:49:51,259 main.py:51] epoch 2308, training loss: 5777.33, average training loss: 6604.10, base loss: 9221.18
[INFO 2017-06-27 17:49:52,475 main.py:51] epoch 2309, training loss: 5917.84, average training loss: 6603.56, base loss: 9221.12
[INFO 2017-06-27 17:49:53,690 main.py:51] epoch 2310, training loss: 5977.87, average training loss: 6603.21, base loss: 9221.24
[INFO 2017-06-27 17:49:54,905 main.py:51] epoch 2311, training loss: 6338.00, average training loss: 6602.62, base loss: 9220.74
[INFO 2017-06-27 17:49:56,115 main.py:51] epoch 2312, training loss: 6271.32, average training loss: 6602.06, base loss: 9220.77
[INFO 2017-06-27 17:49:57,326 main.py:51] epoch 2313, training loss: 5784.24, average training loss: 6601.00, base loss: 9220.03
[INFO 2017-06-27 17:49:58,536 main.py:51] epoch 2314, training loss: 6230.51, average training loss: 6600.21, base loss: 9219.03
[INFO 2017-06-27 17:49:59,749 main.py:51] epoch 2315, training loss: 5710.63, average training loss: 6599.20, base loss: 9217.74
[INFO 2017-06-27 17:50:00,962 main.py:51] epoch 2316, training loss: 6161.95, average training loss: 6595.49, base loss: 9214.55
[INFO 2017-06-27 17:50:02,174 main.py:51] epoch 2317, training loss: 6194.98, average training loss: 6595.47, base loss: 9215.04
[INFO 2017-06-27 17:50:03,387 main.py:51] epoch 2318, training loss: 6406.15, average training loss: 6594.95, base loss: 9214.14
[INFO 2017-06-27 17:50:04,598 main.py:51] epoch 2319, training loss: 6349.67, average training loss: 6592.44, base loss: 9212.35
[INFO 2017-06-27 17:50:05,813 main.py:51] epoch 2320, training loss: 6137.76, average training loss: 6592.29, base loss: 9212.42
[INFO 2017-06-27 17:50:07,025 main.py:51] epoch 2321, training loss: 6500.75, average training loss: 6592.34, base loss: 9213.51
[INFO 2017-06-27 17:50:08,238 main.py:51] epoch 2322, training loss: 5693.65, average training loss: 6591.17, base loss: 9212.14
[INFO 2017-06-27 17:50:09,448 main.py:51] epoch 2323, training loss: 6493.25, average training loss: 6590.58, base loss: 9211.62
[INFO 2017-06-27 17:50:10,661 main.py:51] epoch 2324, training loss: 6540.55, average training loss: 6590.04, base loss: 9211.88
[INFO 2017-06-27 17:50:11,876 main.py:51] epoch 2325, training loss: 5589.62, average training loss: 6589.33, base loss: 9211.05
[INFO 2017-06-27 17:50:13,086 main.py:51] epoch 2326, training loss: 9587.31, average training loss: 6592.07, base loss: 9215.07
[INFO 2017-06-27 17:50:14,299 main.py:51] epoch 2327, training loss: 5771.96, average training loss: 6591.10, base loss: 9214.33
[INFO 2017-06-27 17:50:15,512 main.py:51] epoch 2328, training loss: 8859.20, average training loss: 6593.27, base loss: 9218.83
[INFO 2017-06-27 17:50:16,723 main.py:51] epoch 2329, training loss: 5897.33, average training loss: 6592.55, base loss: 9218.36
[INFO 2017-06-27 17:50:17,933 main.py:51] epoch 2330, training loss: 5690.06, average training loss: 6591.41, base loss: 9216.63
[INFO 2017-06-27 17:50:19,143 main.py:51] epoch 2331, training loss: 5944.17, average training loss: 6590.86, base loss: 9216.12
[INFO 2017-06-27 17:50:20,354 main.py:51] epoch 2332, training loss: 9203.47, average training loss: 6592.98, base loss: 9219.29
[INFO 2017-06-27 17:50:21,562 main.py:51] epoch 2333, training loss: 6367.23, average training loss: 6593.08, base loss: 9220.61
[INFO 2017-06-27 17:50:22,775 main.py:51] epoch 2334, training loss: 6788.79, average training loss: 6593.54, base loss: 9222.33
[INFO 2017-06-27 17:50:23,986 main.py:51] epoch 2335, training loss: 5752.75, average training loss: 6593.17, base loss: 9222.23
[INFO 2017-06-27 17:50:25,205 main.py:51] epoch 2336, training loss: 5948.24, average training loss: 6592.32, base loss: 9221.92
[INFO 2017-06-27 17:50:26,419 main.py:51] epoch 2337, training loss: 9363.94, average training loss: 6594.73, base loss: 9225.44
[INFO 2017-06-27 17:50:27,632 main.py:51] epoch 2338, training loss: 5942.37, average training loss: 6593.74, base loss: 9224.60
[INFO 2017-06-27 17:50:28,843 main.py:51] epoch 2339, training loss: 6166.00, average training loss: 6593.70, base loss: 9225.07
[INFO 2017-06-27 17:50:30,056 main.py:51] epoch 2340, training loss: 5862.78, average training loss: 6593.35, base loss: 9225.85
[INFO 2017-06-27 17:50:31,268 main.py:51] epoch 2341, training loss: 6372.87, average training loss: 6592.97, base loss: 9225.93
[INFO 2017-06-27 17:50:32,480 main.py:51] epoch 2342, training loss: 6583.44, average training loss: 6593.59, base loss: 9228.06
[INFO 2017-06-27 17:50:33,693 main.py:51] epoch 2343, training loss: 6208.21, average training loss: 6593.82, base loss: 9229.69
[INFO 2017-06-27 17:50:34,905 main.py:51] epoch 2344, training loss: 5885.45, average training loss: 6593.82, base loss: 9230.55
[INFO 2017-06-27 17:50:36,115 main.py:51] epoch 2345, training loss: 6277.29, average training loss: 6593.92, base loss: 9231.84
[INFO 2017-06-27 17:50:37,327 main.py:51] epoch 2346, training loss: 6175.30, average training loss: 6592.93, base loss: 9231.23
[INFO 2017-06-27 17:50:38,539 main.py:51] epoch 2347, training loss: 6168.99, average training loss: 6591.74, base loss: 9229.63
[INFO 2017-06-27 17:50:39,753 main.py:51] epoch 2348, training loss: 6018.52, average training loss: 6590.99, base loss: 9228.86
[INFO 2017-06-27 17:50:40,964 main.py:51] epoch 2349, training loss: 6507.14, average training loss: 6590.65, base loss: 9229.10
[INFO 2017-06-27 17:50:42,176 main.py:51] epoch 2350, training loss: 6471.51, average training loss: 6589.85, base loss: 9228.79
[INFO 2017-06-27 17:50:43,388 main.py:51] epoch 2351, training loss: 6686.16, average training loss: 6590.22, base loss: 9230.80
[INFO 2017-06-27 17:50:44,596 main.py:51] epoch 2352, training loss: 5974.72, average training loss: 6589.22, base loss: 9229.13
[INFO 2017-06-27 17:50:45,805 main.py:51] epoch 2353, training loss: 5990.44, average training loss: 6588.88, base loss: 9229.68
[INFO 2017-06-27 17:50:47,017 main.py:51] epoch 2354, training loss: 6484.59, average training loss: 6588.13, base loss: 9229.71
[INFO 2017-06-27 17:50:48,229 main.py:51] epoch 2355, training loss: 6679.20, average training loss: 6588.57, base loss: 9231.17
[INFO 2017-06-27 17:50:49,441 main.py:51] epoch 2356, training loss: 6308.33, average training loss: 6588.34, base loss: 9231.98
[INFO 2017-06-27 17:50:50,654 main.py:51] epoch 2357, training loss: 8507.58, average training loss: 6590.36, base loss: 9233.87
[INFO 2017-06-27 17:50:51,863 main.py:51] epoch 2358, training loss: 6608.65, average training loss: 6590.36, base loss: 9235.09
[INFO 2017-06-27 17:50:53,076 main.py:51] epoch 2359, training loss: 6209.89, average training loss: 6590.33, base loss: 9235.71
[INFO 2017-06-27 17:50:54,291 main.py:51] epoch 2360, training loss: 9172.08, average training loss: 6592.87, base loss: 9240.18
[INFO 2017-06-27 17:50:55,507 main.py:51] epoch 2361, training loss: 6323.45, average training loss: 6592.75, base loss: 9241.06
[INFO 2017-06-27 17:50:56,719 main.py:51] epoch 2362, training loss: 6131.00, average training loss: 6592.75, base loss: 9241.71
[INFO 2017-06-27 17:50:57,936 main.py:51] epoch 2363, training loss: 8863.35, average training loss: 6594.67, base loss: 9244.17
[INFO 2017-06-27 17:50:59,153 main.py:51] epoch 2364, training loss: 6054.35, average training loss: 6594.15, base loss: 9243.88
[INFO 2017-06-27 17:51:00,360 main.py:51] epoch 2365, training loss: 5867.83, average training loss: 6593.57, base loss: 9243.42
[INFO 2017-06-27 17:51:01,575 main.py:51] epoch 2366, training loss: 6762.01, average training loss: 6593.99, base loss: 9245.48
[INFO 2017-06-27 17:51:02,785 main.py:51] epoch 2367, training loss: 8192.10, average training loss: 6595.45, base loss: 9248.37
[INFO 2017-06-27 17:51:03,999 main.py:51] epoch 2368, training loss: 5850.16, average training loss: 6595.15, base loss: 9247.79
[INFO 2017-06-27 17:51:05,214 main.py:51] epoch 2369, training loss: 6095.45, average training loss: 6594.66, base loss: 9247.78
[INFO 2017-06-27 17:51:06,423 main.py:51] epoch 2370, training loss: 6195.52, average training loss: 6591.21, base loss: 9245.07
[INFO 2017-06-27 17:51:07,633 main.py:51] epoch 2371, training loss: 5962.76, average training loss: 6590.93, base loss: 9245.34
[INFO 2017-06-27 17:51:08,840 main.py:51] epoch 2372, training loss: 6216.33, average training loss: 6590.87, base loss: 9245.96
[INFO 2017-06-27 17:51:10,043 main.py:51] epoch 2373, training loss: 6821.21, average training loss: 6591.14, base loss: 9247.23
[INFO 2017-06-27 17:51:11,252 main.py:51] epoch 2374, training loss: 6335.82, average training loss: 6590.62, base loss: 9246.96
[INFO 2017-06-27 17:51:12,457 main.py:51] epoch 2375, training loss: 5890.85, average training loss: 6589.84, base loss: 9246.07
[INFO 2017-06-27 17:51:13,666 main.py:51] epoch 2376, training loss: 9022.49, average training loss: 6592.09, base loss: 9249.04
[INFO 2017-06-27 17:51:14,880 main.py:51] epoch 2377, training loss: 6116.09, average training loss: 6591.49, base loss: 9248.40
[INFO 2017-06-27 17:51:16,094 main.py:51] epoch 2378, training loss: 5978.23, average training loss: 6591.14, base loss: 9248.79
[INFO 2017-06-27 17:51:17,299 main.py:51] epoch 2379, training loss: 6150.79, average training loss: 6591.13, base loss: 9249.43
[INFO 2017-06-27 17:51:18,504 main.py:51] epoch 2380, training loss: 5695.77, average training loss: 6589.52, base loss: 9247.40
[INFO 2017-06-27 17:51:19,711 main.py:51] epoch 2381, training loss: 6283.12, average training loss: 6589.80, base loss: 9249.71
[INFO 2017-06-27 17:51:20,920 main.py:51] epoch 2382, training loss: 6105.37, average training loss: 6589.54, base loss: 9250.10
[INFO 2017-06-27 17:51:22,131 main.py:51] epoch 2383, training loss: 5982.48, average training loss: 6588.81, base loss: 9250.09
[INFO 2017-06-27 17:51:23,345 main.py:51] epoch 2384, training loss: 6031.70, average training loss: 6587.99, base loss: 9249.51
[INFO 2017-06-27 17:51:24,556 main.py:51] epoch 2385, training loss: 5740.85, average training loss: 6586.87, base loss: 9247.95
[INFO 2017-06-27 17:51:25,768 main.py:51] epoch 2386, training loss: 5919.55, average training loss: 6586.17, base loss: 9247.42
[INFO 2017-06-27 17:51:26,975 main.py:51] epoch 2387, training loss: 5825.22, average training loss: 6585.63, base loss: 9247.32
[INFO 2017-06-27 17:51:28,190 main.py:51] epoch 2388, training loss: 6246.87, average training loss: 6584.18, base loss: 9245.32
[INFO 2017-06-27 17:51:29,397 main.py:51] epoch 2389, training loss: 6164.99, average training loss: 6583.71, base loss: 9245.03
[INFO 2017-06-27 17:51:30,609 main.py:51] epoch 2390, training loss: 6214.74, average training loss: 6580.23, base loss: 9242.76
[INFO 2017-06-27 17:51:31,816 main.py:51] epoch 2391, training loss: 6048.24, average training loss: 6579.55, base loss: 9242.47
[INFO 2017-06-27 17:51:33,027 main.py:51] epoch 2392, training loss: 5860.78, average training loss: 6578.60, base loss: 9241.87
[INFO 2017-06-27 17:51:34,237 main.py:51] epoch 2393, training loss: 8741.31, average training loss: 6581.45, base loss: 9246.19
[INFO 2017-06-27 17:51:35,443 main.py:51] epoch 2394, training loss: 6079.61, average training loss: 6576.91, base loss: 9241.63
[INFO 2017-06-27 17:51:36,650 main.py:51] epoch 2395, training loss: 6369.02, average training loss: 6576.42, base loss: 9241.82
[INFO 2017-06-27 17:51:37,858 main.py:51] epoch 2396, training loss: 5935.54, average training loss: 6576.15, base loss: 9242.58
[INFO 2017-06-27 17:51:39,067 main.py:51] epoch 2397, training loss: 6013.46, average training loss: 6571.73, base loss: 9238.30
[INFO 2017-06-27 17:51:40,277 main.py:51] epoch 2398, training loss: 5746.29, average training loss: 6571.40, base loss: 9238.75
[INFO 2017-06-27 17:51:41,488 main.py:51] epoch 2399, training loss: 5658.53, average training loss: 6570.45, base loss: 9238.12
[INFO 2017-06-27 17:51:41,488 main.py:53] epoch 2399, testing
[INFO 2017-06-27 17:51:46,179 main.py:105] average testing loss: 6391.92, base loss: 9286.22
[INFO 2017-06-27 17:51:46,179 main.py:106] improve_loss: 2894.30, improve_percent: 0.31
[INFO 2017-06-27 17:51:46,180 main.py:76] current best improved percent: 0.32
[INFO 2017-06-27 17:51:47,387 main.py:51] epoch 2400, training loss: 6021.66, average training loss: 6569.55, base loss: 9237.21
[INFO 2017-06-27 17:51:48,596 main.py:51] epoch 2401, training loss: 8132.81, average training loss: 6571.27, base loss: 9240.24
[INFO 2017-06-27 17:51:49,810 main.py:51] epoch 2402, training loss: 5852.32, average training loss: 6571.07, base loss: 9240.40
[INFO 2017-06-27 17:51:51,018 main.py:51] epoch 2403, training loss: 5661.38, average training loss: 6570.30, base loss: 9239.56
[INFO 2017-06-27 17:51:52,226 main.py:51] epoch 2404, training loss: 5688.05, average training loss: 6569.00, base loss: 9237.39
[INFO 2017-06-27 17:51:53,440 main.py:51] epoch 2405, training loss: 5777.69, average training loss: 6568.06, base loss: 9236.71
[INFO 2017-06-27 17:51:54,652 main.py:51] epoch 2406, training loss: 5862.25, average training loss: 6567.93, base loss: 9237.32
[INFO 2017-06-27 17:51:55,865 main.py:51] epoch 2407, training loss: 6104.25, average training loss: 6567.66, base loss: 9237.36
[INFO 2017-06-27 17:51:57,076 main.py:51] epoch 2408, training loss: 5714.99, average training loss: 6566.66, base loss: 9235.85
[INFO 2017-06-27 17:51:58,291 main.py:51] epoch 2409, training loss: 5870.58, average training loss: 6566.36, base loss: 9236.42
[INFO 2017-06-27 17:51:59,502 main.py:51] epoch 2410, training loss: 5912.90, average training loss: 6566.24, base loss: 9237.64
[INFO 2017-06-27 17:52:00,711 main.py:51] epoch 2411, training loss: 6238.58, average training loss: 6566.17, base loss: 9238.46
[INFO 2017-06-27 17:52:01,925 main.py:51] epoch 2412, training loss: 6150.64, average training loss: 6565.79, base loss: 9238.58
[INFO 2017-06-27 17:52:03,139 main.py:51] epoch 2413, training loss: 5602.98, average training loss: 6564.39, base loss: 9236.94
[INFO 2017-06-27 17:52:04,354 main.py:51] epoch 2414, training loss: 5894.73, average training loss: 6564.15, base loss: 9236.61
[INFO 2017-06-27 17:52:05,569 main.py:51] epoch 2415, training loss: 8119.60, average training loss: 6566.12, base loss: 9240.37
[INFO 2017-06-27 17:52:06,783 main.py:51] epoch 2416, training loss: 5922.78, average training loss: 6561.82, base loss: 9235.59
[INFO 2017-06-27 17:52:07,995 main.py:51] epoch 2417, training loss: 6134.57, average training loss: 6561.92, base loss: 9236.41
[INFO 2017-06-27 17:52:09,210 main.py:51] epoch 2418, training loss: 6331.88, average training loss: 6561.75, base loss: 9236.61
[INFO 2017-06-27 17:52:10,421 main.py:51] epoch 2419, training loss: 6436.92, average training loss: 6560.86, base loss: 9235.78
[INFO 2017-06-27 17:52:11,630 main.py:51] epoch 2420, training loss: 6161.55, average training loss: 6560.89, base loss: 9236.75
[INFO 2017-06-27 17:52:12,855 main.py:51] epoch 2421, training loss: 5634.29, average training loss: 6559.99, base loss: 9235.21
[INFO 2017-06-27 17:52:14,067 main.py:51] epoch 2422, training loss: 6151.19, average training loss: 6559.63, base loss: 9235.37
[INFO 2017-06-27 17:52:15,279 main.py:51] epoch 2423, training loss: 8711.98, average training loss: 6561.29, base loss: 9239.34
[INFO 2017-06-27 17:52:16,494 main.py:51] epoch 2424, training loss: 6010.15, average training loss: 6560.62, base loss: 9238.84
[INFO 2017-06-27 17:52:17,707 main.py:51] epoch 2425, training loss: 5895.63, average training loss: 6560.05, base loss: 9238.27
[INFO 2017-06-27 17:52:18,917 main.py:51] epoch 2426, training loss: 5766.92, average training loss: 6558.97, base loss: 9236.68
[INFO 2017-06-27 17:52:20,132 main.py:51] epoch 2427, training loss: 6240.58, average training loss: 6558.42, base loss: 9236.72
[INFO 2017-06-27 17:52:21,344 main.py:51] epoch 2428, training loss: 6236.46, average training loss: 6558.29, base loss: 9237.35
[INFO 2017-06-27 17:52:22,560 main.py:51] epoch 2429, training loss: 6452.94, average training loss: 6558.06, base loss: 9238.08
[INFO 2017-06-27 17:52:23,767 main.py:51] epoch 2430, training loss: 6393.90, average training loss: 6557.45, base loss: 9237.13
[INFO 2017-06-27 17:52:24,981 main.py:51] epoch 2431, training loss: 6285.71, average training loss: 6554.16, base loss: 9234.63
[INFO 2017-06-27 17:52:26,194 main.py:51] epoch 2432, training loss: 6434.17, average training loss: 6554.40, base loss: 9236.25
[INFO 2017-06-27 17:52:27,407 main.py:51] epoch 2433, training loss: 6740.03, average training loss: 6554.60, base loss: 9237.02
[INFO 2017-06-27 17:52:28,623 main.py:51] epoch 2434, training loss: 6189.25, average training loss: 6551.39, base loss: 9234.85
[INFO 2017-06-27 17:52:29,841 main.py:51] epoch 2435, training loss: 6099.37, average training loss: 6551.09, base loss: 9235.08
[INFO 2017-06-27 17:52:31,055 main.py:51] epoch 2436, training loss: 6400.45, average training loss: 6550.84, base loss: 9235.13
[INFO 2017-06-27 17:52:32,266 main.py:51] epoch 2437, training loss: 6275.05, average training loss: 6550.40, base loss: 9235.85
[INFO 2017-06-27 17:52:33,478 main.py:51] epoch 2438, training loss: 5502.63, average training loss: 6549.58, base loss: 9235.67
[INFO 2017-06-27 17:52:34,693 main.py:51] epoch 2439, training loss: 5763.20, average training loss: 6548.96, base loss: 9235.31
[INFO 2017-06-27 17:52:35,905 main.py:51] epoch 2440, training loss: 6256.64, average training loss: 6548.55, base loss: 9235.24
[INFO 2017-06-27 17:52:37,113 main.py:51] epoch 2441, training loss: 8810.88, average training loss: 6550.95, base loss: 9238.69
[INFO 2017-06-27 17:52:38,325 main.py:51] epoch 2442, training loss: 9204.90, average training loss: 6554.03, base loss: 9243.49
[INFO 2017-06-27 17:52:39,531 main.py:51] epoch 2443, training loss: 5694.43, average training loss: 6553.62, base loss: 9242.64
[INFO 2017-06-27 17:52:40,748 main.py:51] epoch 2444, training loss: 6252.09, average training loss: 6553.48, base loss: 9242.81
[INFO 2017-06-27 17:52:41,959 main.py:51] epoch 2445, training loss: 6134.80, average training loss: 6549.10, base loss: 9238.43
[INFO 2017-06-27 17:52:43,175 main.py:51] epoch 2446, training loss: 6209.54, average training loss: 6549.18, base loss: 9239.36
[INFO 2017-06-27 17:52:44,386 main.py:51] epoch 2447, training loss: 6223.05, average training loss: 6548.68, base loss: 9239.87
[INFO 2017-06-27 17:52:45,600 main.py:51] epoch 2448, training loss: 6318.00, average training loss: 6548.27, base loss: 9239.24
[INFO 2017-06-27 17:52:46,814 main.py:51] epoch 2449, training loss: 6402.03, average training loss: 6548.17, base loss: 9239.69
[INFO 2017-06-27 17:52:48,031 main.py:51] epoch 2450, training loss: 5687.56, average training loss: 6547.29, base loss: 9238.29
[INFO 2017-06-27 17:52:49,250 main.py:51] epoch 2451, training loss: 6068.39, average training loss: 6546.27, base loss: 9237.36
[INFO 2017-06-27 17:52:50,464 main.py:51] epoch 2452, training loss: 5739.02, average training loss: 6545.96, base loss: 9237.23
[INFO 2017-06-27 17:52:51,678 main.py:51] epoch 2453, training loss: 6335.76, average training loss: 6542.64, base loss: 9234.33
[INFO 2017-06-27 17:52:52,890 main.py:51] epoch 2454, training loss: 5806.00, average training loss: 6541.59, base loss: 9232.86
[INFO 2017-06-27 17:52:54,105 main.py:51] epoch 2455, training loss: 6021.49, average training loss: 6541.62, base loss: 9233.15
[INFO 2017-06-27 17:52:55,316 main.py:51] epoch 2456, training loss: 6142.30, average training loss: 6541.89, base loss: 9234.24
[INFO 2017-06-27 17:52:56,527 main.py:51] epoch 2457, training loss: 6104.92, average training loss: 6541.43, base loss: 9233.41
[INFO 2017-06-27 17:52:57,739 main.py:51] epoch 2458, training loss: 6203.27, average training loss: 6541.25, base loss: 9233.43
[INFO 2017-06-27 17:52:58,948 main.py:51] epoch 2459, training loss: 6309.94, average training loss: 6540.83, base loss: 9233.01
[INFO 2017-06-27 17:53:00,161 main.py:51] epoch 2460, training loss: 6256.50, average training loss: 6540.81, base loss: 9233.99
[INFO 2017-06-27 17:53:01,369 main.py:51] epoch 2461, training loss: 6499.22, average training loss: 6540.40, base loss: 9234.19
[INFO 2017-06-27 17:53:02,586 main.py:51] epoch 2462, training loss: 8156.58, average training loss: 6542.77, base loss: 9238.71
[INFO 2017-06-27 17:53:03,809 main.py:51] epoch 2463, training loss: 6196.25, average training loss: 6542.01, base loss: 9238.45
[INFO 2017-06-27 17:53:05,021 main.py:51] epoch 2464, training loss: 6003.02, average training loss: 6541.26, base loss: 9237.14
[INFO 2017-06-27 17:53:06,229 main.py:51] epoch 2465, training loss: 6002.59, average training loss: 6541.24, base loss: 9238.00
[INFO 2017-06-27 17:53:07,442 main.py:51] epoch 2466, training loss: 6549.85, average training loss: 6541.36, base loss: 9238.84
[INFO 2017-06-27 17:53:08,654 main.py:51] epoch 2467, training loss: 5923.10, average training loss: 6537.25, base loss: 9234.94
[INFO 2017-06-27 17:53:09,863 main.py:51] epoch 2468, training loss: 6387.71, average training loss: 6537.34, base loss: 9235.77
[INFO 2017-06-27 17:53:11,077 main.py:51] epoch 2469, training loss: 6419.08, average training loss: 6537.46, base loss: 9236.58
[INFO 2017-06-27 17:53:12,287 main.py:51] epoch 2470, training loss: 9168.28, average training loss: 6540.05, base loss: 9240.19
[INFO 2017-06-27 17:53:13,497 main.py:51] epoch 2471, training loss: 6181.11, average training loss: 6539.65, base loss: 9239.83
[INFO 2017-06-27 17:53:14,709 main.py:51] epoch 2472, training loss: 6386.55, average training loss: 6539.85, base loss: 9241.60
[INFO 2017-06-27 17:53:15,917 main.py:51] epoch 2473, training loss: 5988.87, average training loss: 6539.07, base loss: 9240.55
[INFO 2017-06-27 17:53:17,130 main.py:51] epoch 2474, training loss: 6357.12, average training loss: 6538.72, base loss: 9241.06
[INFO 2017-06-27 17:53:18,343 main.py:51] epoch 2475, training loss: 6377.45, average training loss: 6538.99, base loss: 9242.77
[INFO 2017-06-27 17:53:19,558 main.py:51] epoch 2476, training loss: 6591.79, average training loss: 6539.19, base loss: 9244.09
[INFO 2017-06-27 17:53:20,773 main.py:51] epoch 2477, training loss: 5840.80, average training loss: 6538.71, base loss: 9243.16
[INFO 2017-06-27 17:53:21,985 main.py:51] epoch 2478, training loss: 6052.22, average training loss: 6538.00, base loss: 9243.02
[INFO 2017-06-27 17:53:23,199 main.py:51] epoch 2479, training loss: 6357.18, average training loss: 6537.69, base loss: 9243.23
[INFO 2017-06-27 17:53:24,413 main.py:51] epoch 2480, training loss: 5831.88, average training loss: 6537.15, base loss: 9242.55
[INFO 2017-06-27 17:53:25,622 main.py:51] epoch 2481, training loss: 6157.60, average training loss: 6533.35, base loss: 9238.52
[INFO 2017-06-27 17:53:26,831 main.py:51] epoch 2482, training loss: 6385.40, average training loss: 6533.62, base loss: 9239.42
[INFO 2017-06-27 17:53:28,042 main.py:51] epoch 2483, training loss: 6020.44, average training loss: 6533.55, base loss: 9240.01
[INFO 2017-06-27 17:53:29,255 main.py:51] epoch 2484, training loss: 6666.43, average training loss: 6533.41, base loss: 9240.28
[INFO 2017-06-27 17:53:30,467 main.py:51] epoch 2485, training loss: 5950.41, average training loss: 6532.40, base loss: 9238.91
[INFO 2017-06-27 17:53:31,679 main.py:51] epoch 2486, training loss: 6219.09, average training loss: 6532.07, base loss: 9238.91
[INFO 2017-06-27 17:53:32,893 main.py:51] epoch 2487, training loss: 6260.77, average training loss: 6531.23, base loss: 9237.68
[INFO 2017-06-27 17:53:34,108 main.py:51] epoch 2488, training loss: 6335.92, average training loss: 6530.52, base loss: 9237.29
[INFO 2017-06-27 17:53:35,323 main.py:51] epoch 2489, training loss: 8686.32, average training loss: 6529.69, base loss: 9238.09
[INFO 2017-06-27 17:53:36,536 main.py:51] epoch 2490, training loss: 6298.65, average training loss: 6529.77, base loss: 9239.03
[INFO 2017-06-27 17:53:37,746 main.py:51] epoch 2491, training loss: 6224.53, average training loss: 6529.26, base loss: 9238.99
[INFO 2017-06-27 17:53:38,959 main.py:51] epoch 2492, training loss: 6010.57, average training loss: 6529.05, base loss: 9238.89
[INFO 2017-06-27 17:53:40,174 main.py:51] epoch 2493, training loss: 6471.97, average training loss: 6527.83, base loss: 9237.09
[INFO 2017-06-27 17:53:41,389 main.py:51] epoch 2494, training loss: 6042.21, average training loss: 6527.13, base loss: 9236.87
[INFO 2017-06-27 17:53:42,599 main.py:51] epoch 2495, training loss: 6477.50, average training loss: 6527.10, base loss: 9237.55
[INFO 2017-06-27 17:53:43,809 main.py:51] epoch 2496, training loss: 6247.58, average training loss: 6526.98, base loss: 9238.43
[INFO 2017-06-27 17:53:45,024 main.py:51] epoch 2497, training loss: 9464.22, average training loss: 6527.29, base loss: 9239.70
[INFO 2017-06-27 17:53:46,240 main.py:51] epoch 2498, training loss: 6410.51, average training loss: 6527.34, base loss: 9240.35
[INFO 2017-06-27 17:53:47,447 main.py:51] epoch 2499, training loss: 6279.14, average training loss: 6528.12, base loss: 9242.86
[INFO 2017-06-27 17:53:47,448 main.py:53] epoch 2499, testing
[INFO 2017-06-27 17:53:52,143 main.py:105] average testing loss: 6188.99, base loss: 8957.14
[INFO 2017-06-27 17:53:52,143 main.py:106] improve_loss: 2768.15, improve_percent: 0.31
[INFO 2017-06-27 17:53:52,144 main.py:76] current best improved percent: 0.32
[INFO 2017-06-27 17:53:53,357 main.py:51] epoch 2500, training loss: 5866.88, average training loss: 6527.50, base loss: 9242.39
[INFO 2017-06-27 17:53:54,572 main.py:51] epoch 2501, training loss: 9162.62, average training loss: 6531.04, base loss: 9248.20
[INFO 2017-06-27 17:53:55,783 main.py:51] epoch 2502, training loss: 6530.92, average training loss: 6530.43, base loss: 9247.93
[INFO 2017-06-27 17:53:56,994 main.py:51] epoch 2503, training loss: 6505.72, average training loss: 6530.24, base loss: 9248.29
[INFO 2017-06-27 17:53:58,206 main.py:51] epoch 2504, training loss: 5763.17, average training loss: 6529.36, base loss: 9246.82
[INFO 2017-06-27 17:53:59,417 main.py:51] epoch 2505, training loss: 6359.85, average training loss: 6529.66, base loss: 9248.54
[INFO 2017-06-27 17:54:00,627 main.py:51] epoch 2506, training loss: 6099.89, average training loss: 6529.68, base loss: 9249.29
[INFO 2017-06-27 17:54:01,836 main.py:51] epoch 2507, training loss: 6021.44, average training loss: 6528.69, base loss: 9248.63
[INFO 2017-06-27 17:54:03,049 main.py:51] epoch 2508, training loss: 5994.94, average training loss: 6528.47, base loss: 9249.04
[INFO 2017-06-27 17:54:04,261 main.py:51] epoch 2509, training loss: 6131.08, average training loss: 6528.55, base loss: 9249.87
[INFO 2017-06-27 17:54:05,472 main.py:51] epoch 2510, training loss: 6391.98, average training loss: 6528.50, base loss: 9249.96
[INFO 2017-06-27 17:54:06,683 main.py:51] epoch 2511, training loss: 8928.16, average training loss: 6530.95, base loss: 9253.22
[INFO 2017-06-27 17:54:07,898 main.py:51] epoch 2512, training loss: 5965.17, average training loss: 6530.98, base loss: 9254.17
[INFO 2017-06-27 17:54:09,109 main.py:51] epoch 2513, training loss: 8407.31, average training loss: 6532.62, base loss: 9257.24
[INFO 2017-06-27 17:54:10,321 main.py:51] epoch 2514, training loss: 6077.16, average training loss: 6532.69, base loss: 9258.32
[INFO 2017-06-27 17:54:11,535 main.py:51] epoch 2515, training loss: 6541.76, average training loss: 6532.57, base loss: 9258.71
[INFO 2017-06-27 17:54:12,751 main.py:51] epoch 2516, training loss: 5774.59, average training loss: 6531.25, base loss: 9257.56
[INFO 2017-06-27 17:54:13,966 main.py:51] epoch 2517, training loss: 6196.67, average training loss: 6530.71, base loss: 9257.11
[INFO 2017-06-27 17:54:15,178 main.py:51] epoch 2518, training loss: 6075.85, average training loss: 6530.25, base loss: 9257.03
[INFO 2017-06-27 17:54:16,392 main.py:51] epoch 2519, training loss: 5656.96, average training loss: 6529.58, base loss: 9256.86
[INFO 2017-06-27 17:54:17,599 main.py:51] epoch 2520, training loss: 6013.71, average training loss: 6528.83, base loss: 9256.00
[INFO 2017-06-27 17:54:18,809 main.py:51] epoch 2521, training loss: 5925.55, average training loss: 6528.55, base loss: 9256.19
[INFO 2017-06-27 17:54:20,022 main.py:51] epoch 2522, training loss: 6118.48, average training loss: 6528.29, base loss: 9256.68
[INFO 2017-06-27 17:54:21,235 main.py:51] epoch 2523, training loss: 5936.75, average training loss: 6527.79, base loss: 9256.40
[INFO 2017-06-27 17:54:22,443 main.py:51] epoch 2524, training loss: 6157.96, average training loss: 6527.26, base loss: 9255.38
[INFO 2017-06-27 17:54:23,657 main.py:51] epoch 2525, training loss: 6085.49, average training loss: 6526.73, base loss: 9254.30
[INFO 2017-06-27 17:54:24,868 main.py:51] epoch 2526, training loss: 6040.66, average training loss: 6526.35, base loss: 9254.10
[INFO 2017-06-27 17:54:26,082 main.py:51] epoch 2527, training loss: 5661.28, average training loss: 6525.68, base loss: 9253.53
[INFO 2017-06-27 17:54:27,293 main.py:51] epoch 2528, training loss: 6454.90, average training loss: 6525.43, base loss: 9253.78
[INFO 2017-06-27 17:54:28,506 main.py:51] epoch 2529, training loss: 6049.68, average training loss: 6524.66, base loss: 9253.29
[INFO 2017-06-27 17:54:29,721 main.py:51] epoch 2530, training loss: 5809.87, average training loss: 6521.07, base loss: 9249.05
[INFO 2017-06-27 17:54:30,937 main.py:51] epoch 2531, training loss: 6166.84, average training loss: 6520.72, base loss: 9249.51
[INFO 2017-06-27 17:54:32,149 main.py:51] epoch 2532, training loss: 6188.70, average training loss: 6520.90, base loss: 9250.93
[INFO 2017-06-27 17:54:33,359 main.py:51] epoch 2533, training loss: 6182.14, average training loss: 6520.53, base loss: 9250.77
[INFO 2017-06-27 17:54:34,576 main.py:51] epoch 2534, training loss: 6526.29, average training loss: 6520.86, base loss: 9252.91
[INFO 2017-06-27 17:54:35,786 main.py:51] epoch 2535, training loss: 5563.83, average training loss: 6517.18, base loss: 9248.33
[INFO 2017-06-27 17:54:37,000 main.py:51] epoch 2536, training loss: 6319.69, average training loss: 6516.93, base loss: 9248.34
[INFO 2017-06-27 17:54:38,215 main.py:51] epoch 2537, training loss: 6174.93, average training loss: 6517.13, base loss: 9249.56
[INFO 2017-06-27 17:54:39,433 main.py:51] epoch 2538, training loss: 5586.80, average training loss: 6515.83, base loss: 9247.31
[INFO 2017-06-27 17:54:40,642 main.py:51] epoch 2539, training loss: 6028.03, average training loss: 6515.65, base loss: 9247.76
[INFO 2017-06-27 17:54:41,851 main.py:51] epoch 2540, training loss: 8323.84, average training loss: 6517.19, base loss: 9250.84
[INFO 2017-06-27 17:54:43,066 main.py:51] epoch 2541, training loss: 6244.03, average training loss: 6514.21, base loss: 9248.35
[INFO 2017-06-27 17:54:44,285 main.py:51] epoch 2542, training loss: 8794.87, average training loss: 6516.63, base loss: 9252.17
[INFO 2017-06-27 17:54:45,495 main.py:51] epoch 2543, training loss: 5860.84, average training loss: 6515.59, base loss: 9250.98
[INFO 2017-06-27 17:54:46,706 main.py:51] epoch 2544, training loss: 5861.02, average training loss: 6514.75, base loss: 9250.25
[INFO 2017-06-27 17:54:47,917 main.py:51] epoch 2545, training loss: 5673.27, average training loss: 6514.21, base loss: 9249.93
[INFO 2017-06-27 17:54:49,131 main.py:51] epoch 2546, training loss: 5713.84, average training loss: 6513.06, base loss: 9248.86
[INFO 2017-06-27 17:54:50,340 main.py:51] epoch 2547, training loss: 6088.31, average training loss: 6512.90, base loss: 9249.73
[INFO 2017-06-27 17:54:51,554 main.py:51] epoch 2548, training loss: 8448.87, average training loss: 6514.40, base loss: 9251.17
[INFO 2017-06-27 17:54:52,767 main.py:51] epoch 2549, training loss: 6166.37, average training loss: 6514.62, base loss: 9252.47
[INFO 2017-06-27 17:54:53,977 main.py:51] epoch 2550, training loss: 6508.63, average training loss: 6514.96, base loss: 9253.64
[INFO 2017-06-27 17:54:55,186 main.py:51] epoch 2551, training loss: 6437.74, average training loss: 6513.56, base loss: 9252.64
[INFO 2017-06-27 17:54:56,396 main.py:51] epoch 2552, training loss: 6220.06, average training loss: 6513.43, base loss: 9253.19
[INFO 2017-06-27 17:54:57,606 main.py:51] epoch 2553, training loss: 6075.47, average training loss: 6512.83, base loss: 9253.33
[INFO 2017-06-27 17:54:58,822 main.py:51] epoch 2554, training loss: 6494.97, average training loss: 6512.61, base loss: 9253.46
[INFO 2017-06-27 17:55:00,035 main.py:51] epoch 2555, training loss: 5651.28, average training loss: 6511.85, base loss: 9252.54
[INFO 2017-06-27 17:55:01,254 main.py:51] epoch 2556, training loss: 6292.95, average training loss: 6511.02, base loss: 9252.67
[INFO 2017-06-27 17:55:02,465 main.py:51] epoch 2557, training loss: 8495.69, average training loss: 6513.30, base loss: 9257.89
[INFO 2017-06-27 17:55:03,682 main.py:51] epoch 2558, training loss: 8538.25, average training loss: 6515.25, base loss: 9262.20
[INFO 2017-06-27 17:55:04,891 main.py:51] epoch 2559, training loss: 6031.81, average training loss: 6514.76, base loss: 9261.91
[INFO 2017-06-27 17:55:06,103 main.py:51] epoch 2560, training loss: 5972.08, average training loss: 6513.70, base loss: 9259.82
[INFO 2017-06-27 17:55:07,312 main.py:51] epoch 2561, training loss: 6164.08, average training loss: 6513.20, base loss: 9259.41
[INFO 2017-06-27 17:55:08,529 main.py:51] epoch 2562, training loss: 5691.03, average training loss: 6512.36, base loss: 9258.27
[INFO 2017-06-27 17:55:09,742 main.py:51] epoch 2563, training loss: 6580.63, average training loss: 6512.41, base loss: 9258.92
[INFO 2017-06-27 17:55:10,956 main.py:51] epoch 2564, training loss: 6068.31, average training loss: 6512.02, base loss: 9258.69
[INFO 2017-06-27 17:55:12,164 main.py:51] epoch 2565, training loss: 6286.01, average training loss: 6512.07, base loss: 9259.37
[INFO 2017-06-27 17:55:13,372 main.py:51] epoch 2566, training loss: 6351.88, average training loss: 6508.34, base loss: 9256.09
[INFO 2017-06-27 17:55:14,582 main.py:51] epoch 2567, training loss: 5928.84, average training loss: 6508.02, base loss: 9256.33
[INFO 2017-06-27 17:55:15,795 main.py:51] epoch 2568, training loss: 6324.77, average training loss: 6507.60, base loss: 9256.02
[INFO 2017-06-27 17:55:17,010 main.py:51] epoch 2569, training loss: 6007.20, average training loss: 6507.02, base loss: 9255.19
[INFO 2017-06-27 17:55:18,220 main.py:51] epoch 2570, training loss: 5872.45, average training loss: 6507.10, base loss: 9256.23
[INFO 2017-06-27 17:55:19,435 main.py:51] epoch 2571, training loss: 6115.93, average training loss: 6504.10, base loss: 9252.81
[INFO 2017-06-27 17:55:20,650 main.py:51] epoch 2572, training loss: 5759.50, average training loss: 6503.55, base loss: 9252.20
[INFO 2017-06-27 17:55:21,857 main.py:51] epoch 2573, training loss: 6281.43, average training loss: 6502.82, base loss: 9251.28
[INFO 2017-06-27 17:55:23,065 main.py:51] epoch 2574, training loss: 6610.64, average training loss: 6502.93, base loss: 9252.27
[INFO 2017-06-27 17:55:24,276 main.py:51] epoch 2575, training loss: 6002.33, average training loss: 6502.49, base loss: 9252.98
[INFO 2017-06-27 17:55:25,486 main.py:51] epoch 2576, training loss: 6113.53, average training loss: 6501.59, base loss: 9252.11
[INFO 2017-06-27 17:55:26,699 main.py:51] epoch 2577, training loss: 5771.21, average training loss: 6500.80, base loss: 9250.98
[INFO 2017-06-27 17:55:27,910 main.py:51] epoch 2578, training loss: 6199.78, average training loss: 6500.44, base loss: 9250.90
[INFO 2017-06-27 17:55:29,116 main.py:51] epoch 2579, training loss: 6141.33, average training loss: 6500.16, base loss: 9251.42
[INFO 2017-06-27 17:55:30,327 main.py:51] epoch 2580, training loss: 5780.32, average training loss: 6499.91, base loss: 9251.74
[INFO 2017-06-27 17:55:31,541 main.py:51] epoch 2581, training loss: 6194.20, average training loss: 6498.70, base loss: 9250.02
[INFO 2017-06-27 17:55:32,754 main.py:51] epoch 2582, training loss: 6213.05, average training loss: 6495.15, base loss: 9247.23
[INFO 2017-06-27 17:55:33,967 main.py:51] epoch 2583, training loss: 6161.27, average training loss: 6495.07, base loss: 9248.65
[INFO 2017-06-27 17:55:35,178 main.py:51] epoch 2584, training loss: 8968.42, average training loss: 6497.71, base loss: 9252.83
[INFO 2017-06-27 17:55:36,389 main.py:51] epoch 2585, training loss: 6000.98, average training loss: 6497.17, base loss: 9252.64
[INFO 2017-06-27 17:55:37,600 main.py:51] epoch 2586, training loss: 5828.87, average training loss: 6496.01, base loss: 9251.51
[INFO 2017-06-27 17:55:38,810 main.py:51] epoch 2587, training loss: 5951.21, average training loss: 6495.74, base loss: 9251.38
[INFO 2017-06-27 17:55:40,021 main.py:51] epoch 2588, training loss: 5818.04, average training loss: 6494.80, base loss: 9249.90
[INFO 2017-06-27 17:55:41,234 main.py:51] epoch 2589, training loss: 5929.45, average training loss: 6493.57, base loss: 9248.01
[INFO 2017-06-27 17:55:42,448 main.py:51] epoch 2590, training loss: 6262.30, average training loss: 6492.59, base loss: 9247.45
[INFO 2017-06-27 17:55:43,660 main.py:51] epoch 2591, training loss: 6418.99, average training loss: 6492.67, base loss: 9248.05
[INFO 2017-06-27 17:55:44,870 main.py:51] epoch 2592, training loss: 6233.74, average training loss: 6492.38, base loss: 9248.65
[INFO 2017-06-27 17:55:46,082 main.py:51] epoch 2593, training loss: 6261.70, average training loss: 6492.43, base loss: 9249.57
[INFO 2017-06-27 17:55:47,294 main.py:51] epoch 2594, training loss: 5828.35, average training loss: 6492.25, base loss: 9249.94
[INFO 2017-06-27 17:55:48,507 main.py:51] epoch 2595, training loss: 5929.59, average training loss: 6490.94, base loss: 9247.92
[INFO 2017-06-27 17:55:49,715 main.py:51] epoch 2596, training loss: 5806.73, average training loss: 6489.89, base loss: 9247.10
[INFO 2017-06-27 17:55:50,927 main.py:51] epoch 2597, training loss: 8354.74, average training loss: 6492.27, base loss: 9251.25
[INFO 2017-06-27 17:55:52,142 main.py:51] epoch 2598, training loss: 5903.62, average training loss: 6491.53, base loss: 9250.37
[INFO 2017-06-27 17:55:53,347 main.py:51] epoch 2599, training loss: 6346.33, average training loss: 6491.61, base loss: 9251.86
[INFO 2017-06-27 17:55:53,347 main.py:53] epoch 2599, testing
[INFO 2017-06-27 17:55:58,053 main.py:105] average testing loss: 6483.81, base loss: 9344.22
[INFO 2017-06-27 17:55:58,054 main.py:106] improve_loss: 2860.41, improve_percent: 0.31
[INFO 2017-06-27 17:55:58,055 main.py:76] current best improved percent: 0.32
[INFO 2017-06-27 17:55:59,263 main.py:51] epoch 2600, training loss: 6095.77, average training loss: 6491.63, base loss: 9252.56
[INFO 2017-06-27 17:56:00,469 main.py:51] epoch 2601, training loss: 6226.71, average training loss: 6491.94, base loss: 9254.02
[INFO 2017-06-27 17:56:01,694 main.py:51] epoch 2602, training loss: 6119.69, average training loss: 6491.42, base loss: 9253.71
[INFO 2017-06-27 17:56:02,905 main.py:51] epoch 2603, training loss: 6262.37, average training loss: 6491.54, base loss: 9254.93
[INFO 2017-06-27 17:56:04,110 main.py:51] epoch 2604, training loss: 5795.82, average training loss: 6490.58, base loss: 9253.95
[INFO 2017-06-27 17:56:05,321 main.py:51] epoch 2605, training loss: 5954.33, average training loss: 6489.64, base loss: 9252.98
[INFO 2017-06-27 17:56:06,533 main.py:51] epoch 2606, training loss: 6426.90, average training loss: 6489.65, base loss: 9253.78
[INFO 2017-06-27 17:56:07,739 main.py:51] epoch 2607, training loss: 5591.82, average training loss: 6488.43, base loss: 9252.13
[INFO 2017-06-27 17:56:08,951 main.py:51] epoch 2608, training loss: 6028.70, average training loss: 6488.19, base loss: 9252.32
[INFO 2017-06-27 17:56:10,159 main.py:51] epoch 2609, training loss: 5884.88, average training loss: 6484.65, base loss: 9248.65
[INFO 2017-06-27 17:56:11,367 main.py:51] epoch 2610, training loss: 6003.85, average training loss: 6484.37, base loss: 9248.47
[INFO 2017-06-27 17:56:12,577 main.py:51] epoch 2611, training loss: 5893.23, average training loss: 6483.54, base loss: 9247.47
[INFO 2017-06-27 17:56:13,787 main.py:51] epoch 2612, training loss: 6054.24, average training loss: 6483.09, base loss: 9247.19
[INFO 2017-06-27 17:56:14,997 main.py:51] epoch 2613, training loss: 5788.66, average training loss: 6480.47, base loss: 9244.47
[INFO 2017-06-27 17:56:16,205 main.py:51] epoch 2614, training loss: 5999.56, average training loss: 6479.51, base loss: 9243.61
[INFO 2017-06-27 17:56:17,419 main.py:51] epoch 2615, training loss: 6189.51, average training loss: 6479.07, base loss: 9243.77
[INFO 2017-06-27 17:56:18,629 main.py:51] epoch 2616, training loss: 5657.88, average training loss: 6478.66, base loss: 9243.44
[INFO 2017-06-27 17:56:19,842 main.py:51] epoch 2617, training loss: 6468.85, average training loss: 6479.10, base loss: 9245.00
[INFO 2017-06-27 17:56:21,055 main.py:51] epoch 2618, training loss: 6193.31, average training loss: 6479.21, base loss: 9246.66
[INFO 2017-06-27 17:56:22,266 main.py:51] epoch 2619, training loss: 5982.01, average training loss: 6478.28, base loss: 9245.53
[INFO 2017-06-27 17:56:23,481 main.py:51] epoch 2620, training loss: 6573.65, average training loss: 6478.21, base loss: 9246.17
[INFO 2017-06-27 17:56:24,695 main.py:51] epoch 2621, training loss: 5520.53, average training loss: 6477.17, base loss: 9244.42
[INFO 2017-06-27 17:56:25,903 main.py:51] epoch 2622, training loss: 5909.86, average training loss: 6476.76, base loss: 9244.11
[INFO 2017-06-27 17:56:27,115 main.py:51] epoch 2623, training loss: 5885.74, average training loss: 6476.14, base loss: 9243.41
[INFO 2017-06-27 17:56:28,322 main.py:51] epoch 2624, training loss: 6006.01, average training loss: 6475.67, base loss: 9243.78
[INFO 2017-06-27 17:56:29,534 main.py:51] epoch 2625, training loss: 6012.71, average training loss: 6475.34, base loss: 9243.97
[INFO 2017-06-27 17:56:30,742 main.py:51] epoch 2626, training loss: 6222.95, average training loss: 6474.69, base loss: 9243.94
[INFO 2017-06-27 17:56:31,952 main.py:51] epoch 2627, training loss: 5972.40, average training loss: 6474.20, base loss: 9243.17
[INFO 2017-06-27 17:56:33,164 main.py:51] epoch 2628, training loss: 6289.55, average training loss: 6474.23, base loss: 9244.12
[INFO 2017-06-27 17:56:34,374 main.py:51] epoch 2629, training loss: 5390.56, average training loss: 6473.24, base loss: 9243.48
[INFO 2017-06-27 17:56:35,587 main.py:51] epoch 2630, training loss: 6026.71, average training loss: 6472.57, base loss: 9242.44
[INFO 2017-06-27 17:56:36,797 main.py:51] epoch 2631, training loss: 6143.29, average training loss: 6469.44, base loss: 9239.06
[INFO 2017-06-27 17:56:38,005 main.py:51] epoch 2632, training loss: 5690.55, average training loss: 6468.89, base loss: 9238.42
[INFO 2017-06-27 17:56:39,217 main.py:51] epoch 2633, training loss: 6291.93, average training loss: 6468.61, base loss: 9239.08
[INFO 2017-06-27 17:56:40,425 main.py:51] epoch 2634, training loss: 6174.03, average training loss: 6468.34, base loss: 9239.56
[INFO 2017-06-27 17:56:41,636 main.py:51] epoch 2635, training loss: 5536.65, average training loss: 6467.37, base loss: 9238.33
[INFO 2017-06-27 17:56:42,840 main.py:51] epoch 2636, training loss: 8797.36, average training loss: 6469.99, base loss: 9242.87
[INFO 2017-06-27 17:56:44,049 main.py:51] epoch 2637, training loss: 5885.06, average training loss: 6465.77, base loss: 9238.53
[INFO 2017-06-27 17:56:45,258 main.py:51] epoch 2638, training loss: 5666.13, average training loss: 6465.01, base loss: 9237.31
[INFO 2017-06-27 17:56:46,465 main.py:51] epoch 2639, training loss: 5820.23, average training loss: 6463.99, base loss: 9236.37
[INFO 2017-06-27 17:56:47,678 main.py:51] epoch 2640, training loss: 5876.31, average training loss: 6463.25, base loss: 9236.30
[INFO 2017-06-27 17:56:48,890 main.py:51] epoch 2641, training loss: 6368.67, average training loss: 6463.53, base loss: 9237.84
[INFO 2017-06-27 17:56:50,102 main.py:51] epoch 2642, training loss: 5684.16, average training loss: 6462.69, base loss: 9236.78
[INFO 2017-06-27 17:56:51,311 main.py:51] epoch 2643, training loss: 6156.58, average training loss: 6461.58, base loss: 9236.01
[INFO 2017-06-27 17:56:52,522 main.py:51] epoch 2644, training loss: 5639.71, average training loss: 6460.64, base loss: 9234.62
[INFO 2017-06-27 17:56:53,731 main.py:51] epoch 2645, training loss: 5826.76, average training loss: 6457.95, base loss: 9231.05
[INFO 2017-06-27 17:56:54,941 main.py:51] epoch 2646, training loss: 5445.10, average training loss: 6456.91, base loss: 9229.21
[INFO 2017-06-27 17:56:56,148 main.py:51] epoch 2647, training loss: 9156.72, average training loss: 6459.81, base loss: 9233.12
[INFO 2017-06-27 17:56:57,357 main.py:51] epoch 2648, training loss: 6440.17, average training loss: 6456.70, base loss: 9231.05
[INFO 2017-06-27 17:56:58,565 main.py:51] epoch 2649, training loss: 6626.43, average training loss: 6456.71, base loss: 9232.08
[INFO 2017-06-27 17:56:59,775 main.py:51] epoch 2650, training loss: 5975.05, average training loss: 6456.54, base loss: 9231.76
[INFO 2017-06-27 17:57:00,985 main.py:51] epoch 2651, training loss: 5491.53, average training loss: 6455.45, base loss: 9230.25
[INFO 2017-06-27 17:57:02,195 main.py:51] epoch 2652, training loss: 6148.68, average training loss: 6455.34, base loss: 9230.39
[INFO 2017-06-27 17:57:03,411 main.py:51] epoch 2653, training loss: 6130.00, average training loss: 6455.15, base loss: 9230.71
[INFO 2017-06-27 17:57:04,622 main.py:51] epoch 2654, training loss: 6011.46, average training loss: 6453.94, base loss: 9229.16
[INFO 2017-06-27 17:57:05,831 main.py:51] epoch 2655, training loss: 5973.71, average training loss: 6453.74, base loss: 9229.22
[INFO 2017-06-27 17:57:07,040 main.py:51] epoch 2656, training loss: 6432.51, average training loss: 6450.63, base loss: 9227.18
[INFO 2017-06-27 17:57:08,250 main.py:51] epoch 2657, training loss: 6184.41, average training loss: 6450.95, base loss: 9228.79
[INFO 2017-06-27 17:57:09,463 main.py:51] epoch 2658, training loss: 6108.96, average training loss: 6450.64, base loss: 9228.72
[INFO 2017-06-27 17:57:10,674 main.py:51] epoch 2659, training loss: 6003.03, average training loss: 6450.85, base loss: 9229.95
[INFO 2017-06-27 17:57:11,884 main.py:51] epoch 2660, training loss: 6165.08, average training loss: 6450.99, base loss: 9230.70
[INFO 2017-06-27 17:57:13,098 main.py:51] epoch 2661, training loss: 5840.48, average training loss: 6450.51, base loss: 9230.43
[INFO 2017-06-27 17:57:14,307 main.py:51] epoch 2662, training loss: 6144.80, average training loss: 6450.01, base loss: 9230.24
[INFO 2017-06-27 17:57:15,518 main.py:51] epoch 2663, training loss: 6217.47, average training loss: 6447.36, base loss: 9226.98
[INFO 2017-06-27 17:57:16,731 main.py:51] epoch 2664, training loss: 5859.26, average training loss: 6446.61, base loss: 9226.11
[INFO 2017-06-27 17:57:17,940 main.py:51] epoch 2665, training loss: 6521.66, average training loss: 6446.95, base loss: 9227.16
[INFO 2017-06-27 17:57:19,149 main.py:51] epoch 2666, training loss: 5887.28, average training loss: 6446.08, base loss: 9226.09
[INFO 2017-06-27 17:57:20,359 main.py:51] epoch 2667, training loss: 8149.84, average training loss: 6447.76, base loss: 9229.33
[INFO 2017-06-27 17:57:21,565 main.py:51] epoch 2668, training loss: 6174.29, average training loss: 6447.82, base loss: 9230.76
[INFO 2017-06-27 17:57:22,773 main.py:51] epoch 2669, training loss: 6001.38, average training loss: 6447.53, base loss: 9230.52
[INFO 2017-06-27 17:57:23,980 main.py:51] epoch 2670, training loss: 6041.94, average training loss: 6446.98, base loss: 9230.08
[INFO 2017-06-27 17:57:25,191 main.py:51] epoch 2671, training loss: 6066.58, average training loss: 6446.44, base loss: 9229.80
[INFO 2017-06-27 17:57:26,403 main.py:51] epoch 2672, training loss: 5928.95, average training loss: 6446.26, base loss: 9230.10
[INFO 2017-06-27 17:57:27,620 main.py:51] epoch 2673, training loss: 6081.19, average training loss: 6445.89, base loss: 9230.82
[INFO 2017-06-27 17:57:28,831 main.py:51] epoch 2674, training loss: 5826.38, average training loss: 6445.28, base loss: 9230.45
[INFO 2017-06-27 17:57:30,046 main.py:51] epoch 2675, training loss: 6260.22, average training loss: 6445.00, base loss: 9230.75
[INFO 2017-06-27 17:57:31,260 main.py:51] epoch 2676, training loss: 6180.77, average training loss: 6444.57, base loss: 9230.28
[INFO 2017-06-27 17:57:32,475 main.py:51] epoch 2677, training loss: 5697.77, average training loss: 6443.76, base loss: 9229.75
[INFO 2017-06-27 17:57:33,688 main.py:51] epoch 2678, training loss: 5890.29, average training loss: 6443.10, base loss: 9229.08
[INFO 2017-06-27 17:57:34,899 main.py:51] epoch 2679, training loss: 6442.07, average training loss: 6442.97, base loss: 9229.60
[INFO 2017-06-27 17:57:36,107 main.py:51] epoch 2680, training loss: 5612.66, average training loss: 6442.19, base loss: 9228.38
[INFO 2017-06-27 17:57:37,320 main.py:51] epoch 2681, training loss: 5428.07, average training loss: 6440.75, base loss: 9226.10
[INFO 2017-06-27 17:57:38,531 main.py:51] epoch 2682, training loss: 6086.49, average training loss: 6438.07, base loss: 9223.43
[INFO 2017-06-27 17:57:39,740 main.py:51] epoch 2683, training loss: 5957.50, average training loss: 6437.82, base loss: 9223.70
[INFO 2017-06-27 17:57:40,957 main.py:51] epoch 2684, training loss: 6470.98, average training loss: 6437.75, base loss: 9224.53
[INFO 2017-06-27 17:57:42,168 main.py:51] epoch 2685, training loss: 5875.17, average training loss: 6437.17, base loss: 9224.11
[INFO 2017-06-27 17:57:43,380 main.py:51] epoch 2686, training loss: 6687.38, average training loss: 6437.78, base loss: 9225.89
[INFO 2017-06-27 17:57:44,595 main.py:51] epoch 2687, training loss: 6086.61, average training loss: 6437.48, base loss: 9226.06
[INFO 2017-06-27 17:57:45,809 main.py:51] epoch 2688, training loss: 8028.77, average training loss: 6439.77, base loss: 9230.97
[INFO 2017-06-27 17:57:47,027 main.py:51] epoch 2689, training loss: 7743.43, average training loss: 6440.58, base loss: 9233.27
[INFO 2017-06-27 17:57:48,236 main.py:51] epoch 2690, training loss: 5504.80, average training loss: 6439.67, base loss: 9232.05
[INFO 2017-06-27 17:57:49,448 main.py:51] epoch 2691, training loss: 5803.24, average training loss: 6439.32, base loss: 9232.50
[INFO 2017-06-27 17:57:50,657 main.py:51] epoch 2692, training loss: 6093.70, average training loss: 6439.37, base loss: 9233.89
[INFO 2017-06-27 17:57:51,872 main.py:51] epoch 2693, training loss: 6040.84, average training loss: 6439.29, base loss: 9234.15
[INFO 2017-06-27 17:57:53,085 main.py:51] epoch 2694, training loss: 5826.17, average training loss: 6438.69, base loss: 9233.03
[INFO 2017-06-27 17:57:54,298 main.py:51] epoch 2695, training loss: 5952.17, average training loss: 6438.37, base loss: 9232.94
[INFO 2017-06-27 17:57:55,511 main.py:51] epoch 2696, training loss: 6302.57, average training loss: 6438.31, base loss: 9233.82
[INFO 2017-06-27 17:57:56,725 main.py:51] epoch 2697, training loss: 5776.61, average training loss: 6437.89, base loss: 9233.78
[INFO 2017-06-27 17:57:57,940 main.py:51] epoch 2698, training loss: 5895.40, average training loss: 6437.03, base loss: 9232.71
[INFO 2017-06-27 17:57:59,154 main.py:51] epoch 2699, training loss: 6192.46, average training loss: 6434.48, base loss: 9230.11
[INFO 2017-06-27 17:57:59,154 main.py:53] epoch 2699, testing
[INFO 2017-06-27 17:58:03,851 main.py:105] average testing loss: 6187.79, base loss: 9151.12
[INFO 2017-06-27 17:58:03,851 main.py:106] improve_loss: 2963.33, improve_percent: 0.32
[INFO 2017-06-27 17:58:03,853 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 17:58:03,865 main.py:76] current best improved percent: 0.32
[INFO 2017-06-27 17:58:05,076 main.py:51] epoch 2700, training loss: 5751.34, average training loss: 6434.05, base loss: 9229.83
[INFO 2017-06-27 17:58:06,289 main.py:51] epoch 2701, training loss: 6004.27, average training loss: 6433.67, base loss: 9229.46
[INFO 2017-06-27 17:58:07,504 main.py:51] epoch 2702, training loss: 5836.21, average training loss: 6433.04, base loss: 9228.80
[INFO 2017-06-27 17:58:08,714 main.py:51] epoch 2703, training loss: 5784.69, average training loss: 6428.41, base loss: 9223.95
[INFO 2017-06-27 17:58:09,928 main.py:51] epoch 2704, training loss: 5562.09, average training loss: 6423.66, base loss: 9218.66
[INFO 2017-06-27 17:58:11,139 main.py:51] epoch 2705, training loss: 9409.17, average training loss: 6426.01, base loss: 9221.79
[INFO 2017-06-27 17:58:12,354 main.py:51] epoch 2706, training loss: 6181.07, average training loss: 6421.94, base loss: 9218.34
[INFO 2017-06-27 17:58:13,568 main.py:51] epoch 2707, training loss: 6064.94, average training loss: 6421.28, base loss: 9217.94
[INFO 2017-06-27 17:58:14,786 main.py:51] epoch 2708, training loss: 6402.31, average training loss: 6421.51, base loss: 9219.23
[INFO 2017-06-27 17:58:16,001 main.py:51] epoch 2709, training loss: 5609.96, average training loss: 6420.76, base loss: 9218.39
[INFO 2017-06-27 17:58:17,215 main.py:51] epoch 2710, training loss: 5576.66, average training loss: 6420.40, base loss: 9218.19
[INFO 2017-06-27 17:58:18,422 main.py:51] epoch 2711, training loss: 5997.96, average training loss: 6419.69, base loss: 9217.73
[INFO 2017-06-27 17:58:19,631 main.py:51] epoch 2712, training loss: 5475.31, average training loss: 6418.19, base loss: 9215.59
[INFO 2017-06-27 17:58:20,843 main.py:51] epoch 2713, training loss: 6000.99, average training loss: 6417.97, base loss: 9215.98
[INFO 2017-06-27 17:58:22,055 main.py:51] epoch 2714, training loss: 6645.15, average training loss: 6418.32, base loss: 9217.57
[INFO 2017-06-27 17:58:23,266 main.py:51] epoch 2715, training loss: 6044.52, average training loss: 6413.98, base loss: 9213.22
[INFO 2017-06-27 17:58:24,478 main.py:51] epoch 2716, training loss: 5854.50, average training loss: 6413.68, base loss: 9213.18
[INFO 2017-06-27 17:58:25,689 main.py:51] epoch 2717, training loss: 6062.03, average training loss: 6413.60, base loss: 9213.82
[INFO 2017-06-27 17:58:26,896 main.py:51] epoch 2718, training loss: 5629.26, average training loss: 6412.34, base loss: 9212.00
[INFO 2017-06-27 17:58:28,106 main.py:51] epoch 2719, training loss: 5756.30, average training loss: 6411.49, base loss: 9211.30
[INFO 2017-06-27 17:58:29,314 main.py:51] epoch 2720, training loss: 6296.03, average training loss: 6411.39, base loss: 9212.33
[INFO 2017-06-27 17:58:30,525 main.py:51] epoch 2721, training loss: 5810.85, average training loss: 6408.56, base loss: 9208.61
[INFO 2017-06-27 17:58:31,736 main.py:51] epoch 2722, training loss: 6658.77, average training loss: 6408.96, base loss: 9210.79
[INFO 2017-06-27 17:58:32,948 main.py:51] epoch 2723, training loss: 6189.20, average training loss: 6409.19, base loss: 9212.41
[INFO 2017-06-27 17:58:34,159 main.py:51] epoch 2724, training loss: 5929.03, average training loss: 6408.62, base loss: 9211.84
[INFO 2017-06-27 17:58:35,368 main.py:51] epoch 2725, training loss: 6321.93, average training loss: 6408.88, base loss: 9213.10
[INFO 2017-06-27 17:58:36,576 main.py:51] epoch 2726, training loss: 8603.23, average training loss: 6411.20, base loss: 9215.82
[INFO 2017-06-27 17:58:37,786 main.py:51] epoch 2727, training loss: 7058.86, average training loss: 6411.95, base loss: 9217.86
[INFO 2017-06-27 17:58:39,000 main.py:51] epoch 2728, training loss: 5892.41, average training loss: 6411.28, base loss: 9216.80
[INFO 2017-06-27 17:58:40,208 main.py:51] epoch 2729, training loss: 6177.76, average training loss: 6411.16, base loss: 9216.67
[INFO 2017-06-27 17:58:41,418 main.py:51] epoch 2730, training loss: 6383.90, average training loss: 6411.50, base loss: 9218.84
[INFO 2017-06-27 17:58:42,626 main.py:51] epoch 2731, training loss: 5785.21, average training loss: 6411.69, base loss: 9220.09
[INFO 2017-06-27 17:58:43,834 main.py:51] epoch 2732, training loss: 6137.98, average training loss: 6411.62, base loss: 9220.90
[INFO 2017-06-27 17:58:45,044 main.py:51] epoch 2733, training loss: 5808.33, average training loss: 6410.70, base loss: 9220.14
[INFO 2017-06-27 17:58:46,257 main.py:51] epoch 2734, training loss: 6361.34, average training loss: 6410.85, base loss: 9221.53
[INFO 2017-06-27 17:58:47,469 main.py:51] epoch 2735, training loss: 5985.57, average training loss: 6410.48, base loss: 9221.84
[INFO 2017-06-27 17:58:48,679 main.py:51] epoch 2736, training loss: 5913.07, average training loss: 6410.87, base loss: 9222.82
[INFO 2017-06-27 17:58:49,888 main.py:51] epoch 2737, training loss: 9495.82, average training loss: 6414.12, base loss: 9227.15
[INFO 2017-06-27 17:58:51,100 main.py:51] epoch 2738, training loss: 5719.12, average training loss: 6413.79, base loss: 9226.43
[INFO 2017-06-27 17:58:52,309 main.py:51] epoch 2739, training loss: 6160.67, average training loss: 6413.47, base loss: 9226.62
[INFO 2017-06-27 17:58:53,517 main.py:51] epoch 2740, training loss: 6164.12, average training loss: 6413.24, base loss: 9226.93
[INFO 2017-06-27 17:58:54,726 main.py:51] epoch 2741, training loss: 6193.03, average training loss: 6410.84, base loss: 9224.55
[INFO 2017-06-27 17:58:55,936 main.py:51] epoch 2742, training loss: 5608.82, average training loss: 6409.93, base loss: 9223.64
[INFO 2017-06-27 17:58:57,141 main.py:51] epoch 2743, training loss: 6023.79, average training loss: 6407.18, base loss: 9219.77
[INFO 2017-06-27 17:58:58,354 main.py:51] epoch 2744, training loss: 5859.16, average training loss: 6406.56, base loss: 9219.54
[INFO 2017-06-27 17:58:59,564 main.py:51] epoch 2745, training loss: 5689.58, average training loss: 6406.11, base loss: 9219.80
[INFO 2017-06-27 17:59:00,776 main.py:51] epoch 2746, training loss: 6015.50, average training loss: 6405.96, base loss: 9220.27
[INFO 2017-06-27 17:59:01,988 main.py:51] epoch 2747, training loss: 6048.51, average training loss: 6404.58, base loss: 9218.31
[INFO 2017-06-27 17:59:03,196 main.py:51] epoch 2748, training loss: 5825.66, average training loss: 6403.95, base loss: 9217.31
[INFO 2017-06-27 17:59:04,408 main.py:51] epoch 2749, training loss: 5703.75, average training loss: 6403.65, base loss: 9216.76
[INFO 2017-06-27 17:59:05,621 main.py:51] epoch 2750, training loss: 5957.35, average training loss: 6402.97, base loss: 9215.63
[INFO 2017-06-27 17:59:06,833 main.py:51] epoch 2751, training loss: 6416.09, average training loss: 6403.19, base loss: 9217.48
[INFO 2017-06-27 17:59:08,046 main.py:51] epoch 2752, training loss: 6403.08, average training loss: 6403.42, base loss: 9218.66
[INFO 2017-06-27 17:59:09,256 main.py:51] epoch 2753, training loss: 6116.05, average training loss: 6402.94, base loss: 9218.07
[INFO 2017-06-27 17:59:10,467 main.py:51] epoch 2754, training loss: 6241.39, average training loss: 6402.93, base loss: 9219.05
[INFO 2017-06-27 17:59:11,682 main.py:51] epoch 2755, training loss: 5851.62, average training loss: 6402.75, base loss: 9219.76
[INFO 2017-06-27 17:59:12,891 main.py:51] epoch 2756, training loss: 6206.80, average training loss: 6402.91, base loss: 9220.68
[INFO 2017-06-27 17:59:14,103 main.py:51] epoch 2757, training loss: 5869.26, average training loss: 6399.35, base loss: 9217.46
[INFO 2017-06-27 17:59:15,317 main.py:51] epoch 2758, training loss: 5961.08, average training loss: 6398.00, base loss: 9215.74
[INFO 2017-06-27 17:59:16,531 main.py:51] epoch 2759, training loss: 5781.95, average training loss: 6397.17, base loss: 9214.48
[INFO 2017-06-27 17:59:17,745 main.py:51] epoch 2760, training loss: 6265.60, average training loss: 6397.57, base loss: 9216.54
[INFO 2017-06-27 17:59:18,954 main.py:51] epoch 2761, training loss: 10874.17, average training loss: 6402.12, base loss: 9223.27
[INFO 2017-06-27 17:59:20,161 main.py:51] epoch 2762, training loss: 5696.53, average training loss: 6401.76, base loss: 9223.00
[INFO 2017-06-27 17:59:21,371 main.py:51] epoch 2763, training loss: 5784.19, average training loss: 6400.53, base loss: 9221.69
[INFO 2017-06-27 17:59:22,581 main.py:51] epoch 2764, training loss: 5900.79, average training loss: 6399.84, base loss: 9220.99
[INFO 2017-06-27 17:59:23,794 main.py:51] epoch 2765, training loss: 6264.40, average training loss: 6399.48, base loss: 9220.82
[INFO 2017-06-27 17:59:25,008 main.py:51] epoch 2766, training loss: 5943.64, average training loss: 6399.10, base loss: 9220.33
[INFO 2017-06-27 17:59:26,223 main.py:51] epoch 2767, training loss: 5759.05, average training loss: 6398.91, base loss: 9220.43
[INFO 2017-06-27 17:59:27,440 main.py:51] epoch 2768, training loss: 5726.09, average training loss: 6397.65, base loss: 9218.64
[INFO 2017-06-27 17:59:28,651 main.py:51] epoch 2769, training loss: 6019.11, average training loss: 6397.31, base loss: 9218.59
[INFO 2017-06-27 17:59:29,867 main.py:51] epoch 2770, training loss: 6388.52, average training loss: 6397.31, base loss: 9219.28
[INFO 2017-06-27 17:59:31,077 main.py:51] epoch 2771, training loss: 5766.25, average training loss: 6396.98, base loss: 9219.59
[INFO 2017-06-27 17:59:32,287 main.py:51] epoch 2772, training loss: 5977.04, average training loss: 6392.73, base loss: 9215.41
[INFO 2017-06-27 17:59:33,503 main.py:51] epoch 2773, training loss: 5616.06, average training loss: 6392.28, base loss: 9214.72
[INFO 2017-06-27 17:59:34,716 main.py:51] epoch 2774, training loss: 8170.26, average training loss: 6393.87, base loss: 9217.91
[INFO 2017-06-27 17:59:35,925 main.py:51] epoch 2775, training loss: 6649.86, average training loss: 6394.07, base loss: 9219.92
[INFO 2017-06-27 17:59:37,130 main.py:51] epoch 2776, training loss: 6131.24, average training loss: 6394.32, base loss: 9220.58
[INFO 2017-06-27 17:59:38,339 main.py:51] epoch 2777, training loss: 6403.56, average training loss: 6394.62, base loss: 9222.07
[INFO 2017-06-27 17:59:39,550 main.py:51] epoch 2778, training loss: 5904.80, average training loss: 6394.19, base loss: 9221.95
[INFO 2017-06-27 17:59:40,759 main.py:51] epoch 2779, training loss: 6145.20, average training loss: 6394.06, base loss: 9221.96
[INFO 2017-06-27 17:59:41,971 main.py:51] epoch 2780, training loss: 5821.90, average training loss: 6393.42, base loss: 9220.99
[INFO 2017-06-27 17:59:43,182 main.py:51] epoch 2781, training loss: 6027.52, average training loss: 6393.11, base loss: 9221.12
[INFO 2017-06-27 17:59:44,395 main.py:51] epoch 2782, training loss: 6040.26, average training loss: 6393.51, base loss: 9222.70
[INFO 2017-06-27 17:59:45,603 main.py:51] epoch 2783, training loss: 5768.35, average training loss: 6393.51, base loss: 9222.63
[INFO 2017-06-27 17:59:46,814 main.py:51] epoch 2784, training loss: 6100.83, average training loss: 6393.20, base loss: 9221.92
[INFO 2017-06-27 17:59:48,024 main.py:51] epoch 2785, training loss: 6372.84, average training loss: 6393.61, base loss: 9223.26
[INFO 2017-06-27 17:59:49,236 main.py:51] epoch 2786, training loss: 5904.34, average training loss: 6392.91, base loss: 9222.89
[INFO 2017-06-27 17:59:50,447 main.py:51] epoch 2787, training loss: 6334.62, average training loss: 6391.13, base loss: 9220.60
[INFO 2017-06-27 17:59:51,657 main.py:51] epoch 2788, training loss: 5881.01, average training loss: 6390.75, base loss: 9220.71
[INFO 2017-06-27 17:59:52,865 main.py:51] epoch 2789, training loss: 5887.07, average training loss: 6390.66, base loss: 9220.91
[INFO 2017-06-27 17:59:54,076 main.py:51] epoch 2790, training loss: 5958.93, average training loss: 6390.44, base loss: 9220.87
[INFO 2017-06-27 17:59:55,288 main.py:51] epoch 2791, training loss: 5745.38, average training loss: 6389.67, base loss: 9219.64
[INFO 2017-06-27 17:59:56,503 main.py:51] epoch 2792, training loss: 6510.10, average training loss: 6389.82, base loss: 9220.94
[INFO 2017-06-27 17:59:57,715 main.py:51] epoch 2793, training loss: 5938.77, average training loss: 6389.34, base loss: 9220.45
[INFO 2017-06-27 17:59:58,928 main.py:51] epoch 2794, training loss: 6049.70, average training loss: 6389.24, base loss: 9220.97
[INFO 2017-06-27 18:00:00,154 main.py:51] epoch 2795, training loss: 5859.80, average training loss: 6388.96, base loss: 9220.63
[INFO 2017-06-27 18:00:01,370 main.py:51] epoch 2796, training loss: 6091.08, average training loss: 6388.75, base loss: 9221.13
[INFO 2017-06-27 18:00:02,579 main.py:51] epoch 2797, training loss: 8153.36, average training loss: 6387.34, base loss: 9221.17
[INFO 2017-06-27 18:00:03,791 main.py:51] epoch 2798, training loss: 5965.24, average training loss: 6386.89, base loss: 9220.69
[INFO 2017-06-27 18:00:05,002 main.py:51] epoch 2799, training loss: 6382.75, average training loss: 6387.25, base loss: 9222.36
[INFO 2017-06-27 18:00:05,002 main.py:53] epoch 2799, testing
[INFO 2017-06-27 18:00:09,711 main.py:105] average testing loss: 6229.52, base loss: 9067.50
[INFO 2017-06-27 18:00:09,711 main.py:106] improve_loss: 2837.98, improve_percent: 0.31
[INFO 2017-06-27 18:00:09,712 main.py:76] current best improved percent: 0.32
[INFO 2017-06-27 18:00:10,924 main.py:51] epoch 2800, training loss: 6330.10, average training loss: 6386.99, base loss: 9223.02
[INFO 2017-06-27 18:00:12,135 main.py:51] epoch 2801, training loss: 6206.10, average training loss: 6383.45, base loss: 9220.36
[INFO 2017-06-27 18:00:13,347 main.py:51] epoch 2802, training loss: 6319.78, average training loss: 6383.43, base loss: 9220.66
[INFO 2017-06-27 18:00:14,563 main.py:51] epoch 2803, training loss: 8715.55, average training loss: 6386.39, base loss: 9225.62
[INFO 2017-06-27 18:00:15,771 main.py:51] epoch 2804, training loss: 6028.32, average training loss: 6385.92, base loss: 9225.56
[INFO 2017-06-27 18:00:16,987 main.py:51] epoch 2805, training loss: 6207.56, average training loss: 6385.47, base loss: 9225.20
[INFO 2017-06-27 18:00:18,201 main.py:51] epoch 2806, training loss: 6285.10, average training loss: 6385.33, base loss: 9225.45
[INFO 2017-06-27 18:00:19,410 main.py:51] epoch 2807, training loss: 6325.95, average training loss: 6385.12, base loss: 9225.88
[INFO 2017-06-27 18:00:20,626 main.py:51] epoch 2808, training loss: 6102.05, average training loss: 6385.41, base loss: 9227.19
[INFO 2017-06-27 18:00:21,839 main.py:51] epoch 2809, training loss: 5936.99, average training loss: 6385.10, base loss: 9226.97
[INFO 2017-06-27 18:00:23,053 main.py:51] epoch 2810, training loss: 6148.37, average training loss: 6385.15, base loss: 9227.80
[INFO 2017-06-27 18:00:24,269 main.py:51] epoch 2811, training loss: 5866.00, average training loss: 6384.42, base loss: 9227.47
[INFO 2017-06-27 18:00:25,479 main.py:51] epoch 2812, training loss: 5917.50, average training loss: 6384.12, base loss: 9226.98
[INFO 2017-06-27 18:00:26,692 main.py:51] epoch 2813, training loss: 5500.60, average training loss: 6382.75, base loss: 9224.59
[INFO 2017-06-27 18:00:27,902 main.py:51] epoch 2814, training loss: 6011.14, average training loss: 6382.52, base loss: 9224.16
[INFO 2017-06-27 18:00:29,114 main.py:51] epoch 2815, training loss: 5924.62, average training loss: 6382.17, base loss: 9224.21
[INFO 2017-06-27 18:00:30,326 main.py:51] epoch 2816, training loss: 6065.23, average training loss: 6381.51, base loss: 9223.92
[INFO 2017-06-27 18:00:31,533 main.py:51] epoch 2817, training loss: 8844.56, average training loss: 6384.31, base loss: 9228.26
[INFO 2017-06-27 18:00:32,741 main.py:51] epoch 2818, training loss: 8267.21, average training loss: 6386.18, base loss: 9231.46
[INFO 2017-06-27 18:00:33,954 main.py:51] epoch 2819, training loss: 5880.41, average training loss: 6385.37, base loss: 9230.38
[INFO 2017-06-27 18:00:35,162 main.py:51] epoch 2820, training loss: 5700.07, average training loss: 6384.60, base loss: 9229.28
[INFO 2017-06-27 18:00:36,373 main.py:51] epoch 2821, training loss: 6066.71, average training loss: 6384.24, base loss: 9228.92
[INFO 2017-06-27 18:00:37,582 main.py:51] epoch 2822, training loss: 6068.36, average training loss: 6383.98, base loss: 9229.61
[INFO 2017-06-27 18:00:38,789 main.py:51] epoch 2823, training loss: 5976.24, average training loss: 6383.87, base loss: 9230.04
[INFO 2017-06-27 18:00:40,001 main.py:51] epoch 2824, training loss: 5576.45, average training loss: 6382.85, base loss: 9228.58
[INFO 2017-06-27 18:00:41,212 main.py:51] epoch 2825, training loss: 5529.20, average training loss: 6381.83, base loss: 9227.30
[INFO 2017-06-27 18:00:42,426 main.py:51] epoch 2826, training loss: 6038.44, average training loss: 6381.64, base loss: 9227.54
[INFO 2017-06-27 18:00:43,638 main.py:51] epoch 2827, training loss: 6152.82, average training loss: 6381.01, base loss: 9227.31
[INFO 2017-06-27 18:00:44,847 main.py:51] epoch 2828, training loss: 5880.35, average training loss: 6380.67, base loss: 9227.40
[INFO 2017-06-27 18:00:46,059 main.py:51] epoch 2829, training loss: 6100.25, average training loss: 6381.00, base loss: 9228.63
[INFO 2017-06-27 18:00:47,273 main.py:51] epoch 2830, training loss: 5509.06, average training loss: 6379.70, base loss: 9226.53
[INFO 2017-06-27 18:00:48,488 main.py:51] epoch 2831, training loss: 5824.48, average training loss: 6376.02, base loss: 9223.35
[INFO 2017-06-27 18:00:49,699 main.py:51] epoch 2832, training loss: 6335.83, average training loss: 6376.39, base loss: 9225.57
[INFO 2017-06-27 18:00:50,911 main.py:51] epoch 2833, training loss: 6130.48, average training loss: 6372.58, base loss: 9222.15
[INFO 2017-06-27 18:00:52,124 main.py:51] epoch 2834, training loss: 7744.77, average training loss: 6373.79, base loss: 9224.59
[INFO 2017-06-27 18:00:53,337 main.py:51] epoch 2835, training loss: 5685.69, average training loss: 6373.16, base loss: 9223.57
[INFO 2017-06-27 18:00:54,552 main.py:51] epoch 2836, training loss: 5872.99, average training loss: 6372.47, base loss: 9223.14
[INFO 2017-06-27 18:00:55,767 main.py:51] epoch 2837, training loss: 5409.19, average training loss: 6371.44, base loss: 9221.62
[INFO 2017-06-27 18:00:56,979 main.py:51] epoch 2838, training loss: 6293.92, average training loss: 6371.41, base loss: 9222.40
[INFO 2017-06-27 18:00:58,190 main.py:51] epoch 2839, training loss: 5947.88, average training loss: 6371.34, base loss: 9223.24
[INFO 2017-06-27 18:00:59,404 main.py:51] epoch 2840, training loss: 6102.18, average training loss: 6371.16, base loss: 9223.46
[INFO 2017-06-27 18:01:00,613 main.py:51] epoch 2841, training loss: 8740.97, average training loss: 6373.28, base loss: 9225.67
[INFO 2017-06-27 18:01:01,825 main.py:51] epoch 2842, training loss: 5488.93, average training loss: 6371.87, base loss: 9223.45
[INFO 2017-06-27 18:01:03,039 main.py:51] epoch 2843, training loss: 6078.88, average training loss: 6371.74, base loss: 9223.73
[INFO 2017-06-27 18:01:04,254 main.py:51] epoch 2844, training loss: 5856.19, average training loss: 6371.28, base loss: 9223.49
[INFO 2017-06-27 18:01:05,467 main.py:51] epoch 2845, training loss: 6249.58, average training loss: 6370.92, base loss: 9223.83
[INFO 2017-06-27 18:01:06,681 main.py:51] epoch 2846, training loss: 6062.57, average training loss: 6370.11, base loss: 9222.89
[INFO 2017-06-27 18:01:07,894 main.py:51] epoch 2847, training loss: 5924.73, average training loss: 6369.77, base loss: 9222.96
[INFO 2017-06-27 18:01:09,107 main.py:51] epoch 2848, training loss: 6020.37, average training loss: 6370.00, base loss: 9223.98
[INFO 2017-06-27 18:01:10,319 main.py:51] epoch 2849, training loss: 6128.80, average training loss: 6369.58, base loss: 9223.82
[INFO 2017-06-27 18:01:11,530 main.py:51] epoch 2850, training loss: 5899.03, average training loss: 6369.43, base loss: 9224.22
[INFO 2017-06-27 18:01:12,744 main.py:51] epoch 2851, training loss: 5733.79, average training loss: 6368.96, base loss: 9224.33
[INFO 2017-06-27 18:01:13,953 main.py:51] epoch 2852, training loss: 5913.96, average training loss: 6368.52, base loss: 9223.94
[INFO 2017-06-27 18:01:15,164 main.py:51] epoch 2853, training loss: 6287.27, average training loss: 6368.59, base loss: 9224.48
[INFO 2017-06-27 18:01:16,375 main.py:51] epoch 2854, training loss: 6306.15, average training loss: 6368.38, base loss: 9224.39
[INFO 2017-06-27 18:01:17,589 main.py:51] epoch 2855, training loss: 6350.07, average training loss: 6368.63, base loss: 9225.00
[INFO 2017-06-27 18:01:18,803 main.py:51] epoch 2856, training loss: 6182.97, average training loss: 6368.59, base loss: 9225.61
[INFO 2017-06-27 18:01:20,008 main.py:51] epoch 2857, training loss: 5830.49, average training loss: 6368.25, base loss: 9226.30
[INFO 2017-06-27 18:01:21,222 main.py:51] epoch 2858, training loss: 6549.14, average training loss: 6368.40, base loss: 9226.71
[INFO 2017-06-27 18:01:22,434 main.py:51] epoch 2859, training loss: 6190.69, average training loss: 6368.25, base loss: 9226.82
[INFO 2017-06-27 18:01:23,642 main.py:51] epoch 2860, training loss: 6421.86, average training loss: 6367.97, base loss: 9227.26
[INFO 2017-06-27 18:01:24,852 main.py:51] epoch 2861, training loss: 5598.09, average training loss: 6367.64, base loss: 9226.30
[INFO 2017-06-27 18:01:26,057 main.py:51] epoch 2862, training loss: 5487.58, average training loss: 6366.49, base loss: 9224.85
[INFO 2017-06-27 18:01:27,268 main.py:51] epoch 2863, training loss: 4999.31, average training loss: 6365.08, base loss: 9221.78
[INFO 2017-06-27 18:01:28,480 main.py:51] epoch 2864, training loss: 5750.50, average training loss: 6364.39, base loss: 9221.11
[INFO 2017-06-27 18:01:29,691 main.py:51] epoch 2865, training loss: 8693.18, average training loss: 6367.16, base loss: 9225.17
[INFO 2017-06-27 18:01:30,902 main.py:51] epoch 2866, training loss: 5913.71, average training loss: 6366.85, base loss: 9225.20
[INFO 2017-06-27 18:01:32,113 main.py:51] epoch 2867, training loss: 6415.75, average training loss: 6367.26, base loss: 9225.95
[INFO 2017-06-27 18:01:33,326 main.py:51] epoch 2868, training loss: 5994.57, average training loss: 6367.07, base loss: 9225.86
[INFO 2017-06-27 18:01:34,535 main.py:51] epoch 2869, training loss: 5820.67, average training loss: 6363.70, base loss: 9222.64
[INFO 2017-06-27 18:01:35,747 main.py:51] epoch 2870, training loss: 5913.54, average training loss: 6363.22, base loss: 9221.94
[INFO 2017-06-27 18:01:36,957 main.py:51] epoch 2871, training loss: 6069.61, average training loss: 6362.82, base loss: 9221.63
[INFO 2017-06-27 18:01:38,170 main.py:51] epoch 2872, training loss: 5916.80, average training loss: 6362.92, base loss: 9222.70
[INFO 2017-06-27 18:01:39,383 main.py:51] epoch 2873, training loss: 8091.49, average training loss: 6364.69, base loss: 9226.09
[INFO 2017-06-27 18:01:40,596 main.py:51] epoch 2874, training loss: 6215.92, average training loss: 6364.73, base loss: 9226.36
[INFO 2017-06-27 18:01:41,804 main.py:51] epoch 2875, training loss: 5800.16, average training loss: 6364.66, base loss: 9226.55
[INFO 2017-06-27 18:01:43,016 main.py:51] epoch 2876, training loss: 6090.99, average training loss: 6364.30, base loss: 9226.44
[INFO 2017-06-27 18:01:44,226 main.py:51] epoch 2877, training loss: 5976.99, average training loss: 6364.16, base loss: 9226.16
[INFO 2017-06-27 18:01:45,437 main.py:51] epoch 2878, training loss: 6185.58, average training loss: 6364.28, base loss: 9227.50
[INFO 2017-06-27 18:01:46,648 main.py:51] epoch 2879, training loss: 6089.34, average training loss: 6363.72, base loss: 9226.78
[INFO 2017-06-27 18:01:47,858 main.py:51] epoch 2880, training loss: 6065.39, average training loss: 6363.16, base loss: 9226.58
[INFO 2017-06-27 18:01:49,065 main.py:51] epoch 2881, training loss: 5960.62, average training loss: 6363.04, base loss: 9228.01
[INFO 2017-06-27 18:01:50,277 main.py:51] epoch 2882, training loss: 6366.35, average training loss: 6363.51, base loss: 9229.54
[INFO 2017-06-27 18:01:51,490 main.py:51] epoch 2883, training loss: 6102.31, average training loss: 6363.85, base loss: 9231.32
[INFO 2017-06-27 18:01:52,701 main.py:51] epoch 2884, training loss: 5804.03, average training loss: 6363.42, base loss: 9230.66
[INFO 2017-06-27 18:01:53,916 main.py:51] epoch 2885, training loss: 6255.01, average training loss: 6363.00, base loss: 9230.58
[INFO 2017-06-27 18:01:55,127 main.py:51] epoch 2886, training loss: 5593.84, average training loss: 6362.01, base loss: 9228.97
[INFO 2017-06-27 18:01:56,338 main.py:51] epoch 2887, training loss: 8418.94, average training loss: 6364.23, base loss: 9233.57
[INFO 2017-06-27 18:01:57,549 main.py:51] epoch 2888, training loss: 6267.28, average training loss: 6364.13, base loss: 9234.19
[INFO 2017-06-27 18:01:58,759 main.py:51] epoch 2889, training loss: 8610.10, average training loss: 6366.33, base loss: 9237.26
[INFO 2017-06-27 18:01:59,969 main.py:51] epoch 2890, training loss: 5850.32, average training loss: 6365.42, base loss: 9236.63
[INFO 2017-06-27 18:02:01,181 main.py:51] epoch 2891, training loss: 6128.77, average training loss: 6365.47, base loss: 9237.30
[INFO 2017-06-27 18:02:02,391 main.py:51] epoch 2892, training loss: 6216.60, average training loss: 6365.63, base loss: 9238.15
[INFO 2017-06-27 18:02:03,600 main.py:51] epoch 2893, training loss: 6113.69, average training loss: 6365.79, base loss: 9240.05
[INFO 2017-06-27 18:02:04,809 main.py:51] epoch 2894, training loss: 6355.83, average training loss: 6365.92, base loss: 9241.07
[INFO 2017-06-27 18:02:06,015 main.py:51] epoch 2895, training loss: 5733.48, average training loss: 6364.56, base loss: 9239.30
[INFO 2017-06-27 18:02:07,229 main.py:51] epoch 2896, training loss: 5784.89, average training loss: 6361.10, base loss: 9235.72
[INFO 2017-06-27 18:02:08,440 main.py:51] epoch 2897, training loss: 6253.98, average training loss: 6358.54, base loss: 9232.19
[INFO 2017-06-27 18:02:09,651 main.py:51] epoch 2898, training loss: 5975.36, average training loss: 6358.14, base loss: 9232.23
[INFO 2017-06-27 18:02:10,860 main.py:51] epoch 2899, training loss: 5965.83, average training loss: 6357.69, base loss: 9231.40
[INFO 2017-06-27 18:02:10,860 main.py:53] epoch 2899, testing
[INFO 2017-06-27 18:02:15,565 main.py:105] average testing loss: 5977.39, base loss: 9022.70
[INFO 2017-06-27 18:02:15,565 main.py:106] improve_loss: 3045.31, improve_percent: 0.34
[INFO 2017-06-27 18:02:15,566 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 18:02:15,578 main.py:76] current best improved percent: 0.34
[INFO 2017-06-27 18:02:16,789 main.py:51] epoch 2900, training loss: 5679.70, average training loss: 6357.19, base loss: 9230.65
[INFO 2017-06-27 18:02:17,998 main.py:51] epoch 2901, training loss: 5722.93, average training loss: 6356.90, base loss: 9230.90
[INFO 2017-06-27 18:02:19,210 main.py:51] epoch 2902, training loss: 5916.79, average training loss: 6356.78, base loss: 9231.37
[INFO 2017-06-27 18:02:20,419 main.py:51] epoch 2903, training loss: 6237.61, average training loss: 6356.91, base loss: 9232.03
[INFO 2017-06-27 18:02:21,631 main.py:51] epoch 2904, training loss: 6072.58, average training loss: 6357.16, base loss: 9234.12
[INFO 2017-06-27 18:02:22,839 main.py:51] epoch 2905, training loss: 5941.53, average training loss: 6357.06, base loss: 9234.33
[INFO 2017-06-27 18:02:24,050 main.py:51] epoch 2906, training loss: 5557.18, average training loss: 6355.84, base loss: 9232.82
[INFO 2017-06-27 18:02:25,262 main.py:51] epoch 2907, training loss: 5774.79, average training loss: 6355.16, base loss: 9231.95
[INFO 2017-06-27 18:02:26,477 main.py:51] epoch 2908, training loss: 5672.79, average training loss: 6354.92, base loss: 9232.10
[INFO 2017-06-27 18:02:27,690 main.py:51] epoch 2909, training loss: 5865.88, average training loss: 6354.76, base loss: 9232.35
[INFO 2017-06-27 18:02:28,902 main.py:51] epoch 2910, training loss: 6462.38, average training loss: 6354.85, base loss: 9232.76
[INFO 2017-06-27 18:02:30,115 main.py:51] epoch 2911, training loss: 5531.60, average training loss: 6353.97, base loss: 9231.20
[INFO 2017-06-27 18:02:31,327 main.py:51] epoch 2912, training loss: 6290.30, average training loss: 6354.50, base loss: 9232.36
[INFO 2017-06-27 18:02:32,539 main.py:51] epoch 2913, training loss: 6005.16, average training loss: 6354.56, base loss: 9232.56
[INFO 2017-06-27 18:02:33,754 main.py:51] epoch 2914, training loss: 5688.47, average training loss: 6354.14, base loss: 9232.57
[INFO 2017-06-27 18:02:34,960 main.py:51] epoch 2915, training loss: 5598.78, average training loss: 6354.24, base loss: 9233.34
[INFO 2017-06-27 18:02:36,172 main.py:51] epoch 2916, training loss: 5838.27, average training loss: 6353.83, base loss: 9232.49
[INFO 2017-06-27 18:02:37,384 main.py:51] epoch 2917, training loss: 5758.70, average training loss: 6353.52, base loss: 9232.52
[INFO 2017-06-27 18:02:38,596 main.py:51] epoch 2918, training loss: 5638.28, average training loss: 6350.35, base loss: 9228.41
[INFO 2017-06-27 18:02:39,809 main.py:51] epoch 2919, training loss: 5867.27, average training loss: 6349.59, base loss: 9227.32
[INFO 2017-06-27 18:02:41,020 main.py:51] epoch 2920, training loss: 5801.06, average training loss: 6349.10, base loss: 9227.46
[INFO 2017-06-27 18:02:42,229 main.py:51] epoch 2921, training loss: 5848.47, average training loss: 6348.15, base loss: 9226.13
[INFO 2017-06-27 18:02:43,439 main.py:51] epoch 2922, training loss: 8864.08, average training loss: 6350.44, base loss: 9229.42
[INFO 2017-06-27 18:02:44,649 main.py:51] epoch 2923, training loss: 5762.28, average training loss: 6350.30, base loss: 9229.72
[INFO 2017-06-27 18:02:45,862 main.py:51] epoch 2924, training loss: 5801.96, average training loss: 6349.09, base loss: 9227.92
[INFO 2017-06-27 18:02:47,072 main.py:51] epoch 2925, training loss: 6250.00, average training loss: 6347.07, base loss: 9225.97
[INFO 2017-06-27 18:02:48,283 main.py:51] epoch 2926, training loss: 8092.29, average training loss: 6348.95, base loss: 9228.92
[INFO 2017-06-27 18:02:49,495 main.py:51] epoch 2927, training loss: 5721.64, average training loss: 6348.61, base loss: 9229.00
[INFO 2017-06-27 18:02:50,704 main.py:51] epoch 2928, training loss: 6221.87, average training loss: 6348.81, base loss: 9229.78
[INFO 2017-06-27 18:02:51,917 main.py:51] epoch 2929, training loss: 6218.92, average training loss: 6348.47, base loss: 9229.97
[INFO 2017-06-27 18:02:53,130 main.py:51] epoch 2930, training loss: 6178.69, average training loss: 6344.93, base loss: 9227.03
[INFO 2017-06-27 18:02:54,340 main.py:51] epoch 2931, training loss: 5777.76, average training loss: 6344.35, base loss: 9226.50
[INFO 2017-06-27 18:02:55,555 main.py:51] epoch 2932, training loss: 6261.80, average training loss: 6341.00, base loss: 9223.91
[INFO 2017-06-27 18:02:56,767 main.py:51] epoch 2933, training loss: 5879.23, average training loss: 6340.62, base loss: 9223.77
[INFO 2017-06-27 18:02:57,981 main.py:51] epoch 2934, training loss: 5937.65, average training loss: 6337.24, base loss: 9219.82
[INFO 2017-06-27 18:02:59,194 main.py:51] epoch 2935, training loss: 6373.61, average training loss: 6337.40, base loss: 9220.33
[INFO 2017-06-27 18:03:00,407 main.py:51] epoch 2936, training loss: 8373.12, average training loss: 6340.25, base loss: 9225.93
[INFO 2017-06-27 18:03:01,620 main.py:51] epoch 2937, training loss: 5969.08, average training loss: 6340.33, base loss: 9226.44
[INFO 2017-06-27 18:03:02,830 main.py:51] epoch 2938, training loss: 5891.74, average training loss: 6340.39, base loss: 9226.84
[INFO 2017-06-27 18:03:04,039 main.py:51] epoch 2939, training loss: 5980.67, average training loss: 6340.20, base loss: 9226.57
[INFO 2017-06-27 18:03:05,250 main.py:51] epoch 2940, training loss: 5545.93, average training loss: 6338.46, base loss: 9223.41
[INFO 2017-06-27 18:03:06,461 main.py:51] epoch 2941, training loss: 5786.52, average training loss: 6337.60, base loss: 9221.98
[INFO 2017-06-27 18:03:07,673 main.py:51] epoch 2942, training loss: 8134.08, average training loss: 6339.32, base loss: 9224.88
[INFO 2017-06-27 18:03:08,889 main.py:51] epoch 2943, training loss: 5888.21, average training loss: 6335.98, base loss: 9221.18
[INFO 2017-06-27 18:03:10,098 main.py:51] epoch 2944, training loss: 6081.81, average training loss: 6336.04, base loss: 9221.30
[INFO 2017-06-27 18:03:11,310 main.py:51] epoch 2945, training loss: 5862.87, average training loss: 6335.98, base loss: 9222.05
[INFO 2017-06-27 18:03:12,522 main.py:51] epoch 2946, training loss: 6170.23, average training loss: 6335.45, base loss: 9221.70
[INFO 2017-06-27 18:03:13,736 main.py:51] epoch 2947, training loss: 6177.65, average training loss: 6335.87, base loss: 9223.21
[INFO 2017-06-27 18:03:14,945 main.py:51] epoch 2948, training loss: 6082.84, average training loss: 6335.88, base loss: 9223.58
[INFO 2017-06-27 18:03:16,153 main.py:51] epoch 2949, training loss: 6070.73, average training loss: 6335.09, base loss: 9223.12
[INFO 2017-06-27 18:03:17,363 main.py:51] epoch 2950, training loss: 6301.18, average training loss: 6333.26, base loss: 9221.24
[INFO 2017-06-27 18:03:18,573 main.py:51] epoch 2951, training loss: 5905.47, average training loss: 6332.47, base loss: 9219.72
[INFO 2017-06-27 18:03:19,784 main.py:51] epoch 2952, training loss: 6090.10, average training loss: 6332.17, base loss: 9219.75
[INFO 2017-06-27 18:03:20,997 main.py:51] epoch 2953, training loss: 5520.85, average training loss: 6331.60, base loss: 9219.26
[INFO 2017-06-27 18:03:22,208 main.py:51] epoch 2954, training loss: 5734.35, average training loss: 6331.33, base loss: 9218.83
[INFO 2017-06-27 18:03:23,418 main.py:51] epoch 2955, training loss: 6428.67, average training loss: 6331.35, base loss: 9219.51
[INFO 2017-06-27 18:03:24,626 main.py:51] epoch 2956, training loss: 5811.44, average training loss: 6330.51, base loss: 9218.20
[INFO 2017-06-27 18:03:25,842 main.py:51] epoch 2957, training loss: 5743.79, average training loss: 6330.15, base loss: 9218.05
[INFO 2017-06-27 18:03:27,050 main.py:51] epoch 2958, training loss: 6351.23, average training loss: 6329.85, base loss: 9218.13
[INFO 2017-06-27 18:03:28,259 main.py:51] epoch 2959, training loss: 6026.05, average training loss: 6329.52, base loss: 9218.41
[INFO 2017-06-27 18:03:29,467 main.py:51] epoch 2960, training loss: 5544.02, average training loss: 6328.81, base loss: 9217.44
[INFO 2017-06-27 18:03:30,679 main.py:51] epoch 2961, training loss: 6361.29, average training loss: 6328.86, base loss: 9218.47
[INFO 2017-06-27 18:03:31,889 main.py:51] epoch 2962, training loss: 6768.30, average training loss: 6329.30, base loss: 9219.40
[INFO 2017-06-27 18:03:33,105 main.py:51] epoch 2963, training loss: 6024.57, average training loss: 6328.90, base loss: 9218.78
[INFO 2017-06-27 18:03:34,315 main.py:51] epoch 2964, training loss: 5659.90, average training loss: 6327.91, base loss: 9216.91
[INFO 2017-06-27 18:03:35,525 main.py:51] epoch 2965, training loss: 6076.54, average training loss: 6327.83, base loss: 9217.05
[INFO 2017-06-27 18:03:36,736 main.py:51] epoch 2966, training loss: 5806.86, average training loss: 6327.31, base loss: 9216.68
[INFO 2017-06-27 18:03:37,949 main.py:51] epoch 2967, training loss: 6179.38, average training loss: 6327.52, base loss: 9217.51
[INFO 2017-06-27 18:03:39,157 main.py:51] epoch 2968, training loss: 6151.95, average training loss: 6327.24, base loss: 9217.40
[INFO 2017-06-27 18:03:40,370 main.py:51] epoch 2969, training loss: 5486.75, average training loss: 6326.39, base loss: 9215.97
[INFO 2017-06-27 18:03:41,580 main.py:51] epoch 2970, training loss: 6001.88, average training loss: 6325.99, base loss: 9214.85
[INFO 2017-06-27 18:03:42,789 main.py:51] epoch 2971, training loss: 5765.18, average training loss: 6325.52, base loss: 9213.97
[INFO 2017-06-27 18:03:43,996 main.py:51] epoch 2972, training loss: 5789.40, average training loss: 6325.04, base loss: 9213.34
[INFO 2017-06-27 18:03:45,210 main.py:51] epoch 2973, training loss: 6265.81, average training loss: 6325.44, base loss: 9214.96
[INFO 2017-06-27 18:03:46,419 main.py:51] epoch 2974, training loss: 6509.24, average training loss: 6326.05, base loss: 9217.13
[INFO 2017-06-27 18:03:47,628 main.py:51] epoch 2975, training loss: 6183.55, average training loss: 6323.93, base loss: 9214.50
[INFO 2017-06-27 18:03:48,839 main.py:51] epoch 2976, training loss: 6374.32, average training loss: 6324.06, base loss: 9215.15
[INFO 2017-06-27 18:03:50,047 main.py:51] epoch 2977, training loss: 5865.61, average training loss: 6323.97, base loss: 9215.78
[INFO 2017-06-27 18:03:51,259 main.py:51] epoch 2978, training loss: 6011.22, average training loss: 6323.75, base loss: 9216.11
[INFO 2017-06-27 18:03:52,470 main.py:51] epoch 2979, training loss: 6781.64, average training loss: 6324.37, base loss: 9218.04
[INFO 2017-06-27 18:03:53,683 main.py:51] epoch 2980, training loss: 5844.41, average training loss: 6322.18, base loss: 9215.65
[INFO 2017-06-27 18:03:54,897 main.py:51] epoch 2981, training loss: 5956.31, average training loss: 6321.70, base loss: 9215.54
[INFO 2017-06-27 18:03:56,104 main.py:51] epoch 2982, training loss: 6171.89, average training loss: 6318.96, base loss: 9213.05
[INFO 2017-06-27 18:03:57,312 main.py:51] epoch 2983, training loss: 6173.10, average training loss: 6318.67, base loss: 9212.64
[INFO 2017-06-27 18:03:58,522 main.py:51] epoch 2984, training loss: 6485.51, average training loss: 6318.91, base loss: 9213.64
[INFO 2017-06-27 18:03:59,734 main.py:51] epoch 2985, training loss: 5642.11, average training loss: 6318.56, base loss: 9213.37
[INFO 2017-06-27 18:04:00,945 main.py:51] epoch 2986, training loss: 6109.54, average training loss: 6318.52, base loss: 9214.06
[INFO 2017-06-27 18:04:02,155 main.py:51] epoch 2987, training loss: 6024.75, average training loss: 6318.44, base loss: 9213.94
[INFO 2017-06-27 18:04:03,365 main.py:51] epoch 2988, training loss: 6477.36, average training loss: 6316.43, base loss: 9212.23
[INFO 2017-06-27 18:04:04,574 main.py:51] epoch 2989, training loss: 6020.77, average training loss: 6315.25, base loss: 9210.07
[INFO 2017-06-27 18:04:05,786 main.py:51] epoch 2990, training loss: 6265.82, average training loss: 6314.96, base loss: 9210.76
[INFO 2017-06-27 18:04:06,998 main.py:51] epoch 2991, training loss: 5652.14, average training loss: 6314.16, base loss: 9209.82
[INFO 2017-06-27 18:04:08,211 main.py:51] epoch 2992, training loss: 5842.97, average training loss: 6313.69, base loss: 9209.07
[INFO 2017-06-27 18:04:09,426 main.py:51] epoch 2993, training loss: 5663.36, average training loss: 6312.53, base loss: 9207.81
[INFO 2017-06-27 18:04:10,637 main.py:51] epoch 2994, training loss: 5757.13, average training loss: 6311.94, base loss: 9207.45
[INFO 2017-06-27 18:04:11,846 main.py:51] epoch 2995, training loss: 5613.89, average training loss: 6311.00, base loss: 9206.02
[INFO 2017-06-27 18:04:13,056 main.py:51] epoch 2996, training loss: 6288.76, average training loss: 6311.05, base loss: 9206.82
[INFO 2017-06-27 18:04:14,268 main.py:51] epoch 2997, training loss: 5979.46, average training loss: 6310.67, base loss: 9206.67
[INFO 2017-06-27 18:04:15,481 main.py:51] epoch 2998, training loss: 6408.03, average training loss: 6310.56, base loss: 9207.63
[INFO 2017-06-27 18:04:16,691 main.py:51] epoch 2999, training loss: 5613.84, average training loss: 6309.93, base loss: 9207.46
[INFO 2017-06-27 18:04:16,692 main.py:53] epoch 2999, testing
[INFO 2017-06-27 18:04:21,368 main.py:105] average testing loss: 6122.81, base loss: 9224.08
[INFO 2017-06-27 18:04:21,368 main.py:106] improve_loss: 3101.27, improve_percent: 0.34
[INFO 2017-06-27 18:04:21,369 main.py:76] current best improved percent: 0.34
[INFO 2017-06-27 18:04:22,574 main.py:51] epoch 3000, training loss: 5814.84, average training loss: 6310.10, base loss: 9208.07
[INFO 2017-06-27 18:04:23,785 main.py:51] epoch 3001, training loss: 10559.08, average training loss: 6314.64, base loss: 9215.65
[INFO 2017-06-27 18:04:24,995 main.py:51] epoch 3002, training loss: 8127.31, average training loss: 6316.04, base loss: 9218.92
[INFO 2017-06-27 18:04:26,209 main.py:51] epoch 3003, training loss: 7967.07, average training loss: 6317.56, base loss: 9222.09
[INFO 2017-06-27 18:04:27,418 main.py:51] epoch 3004, training loss: 6002.02, average training loss: 6317.10, base loss: 9222.06
[INFO 2017-06-27 18:04:28,625 main.py:51] epoch 3005, training loss: 5823.55, average training loss: 6316.44, base loss: 9221.11
[INFO 2017-06-27 18:04:29,836 main.py:51] epoch 3006, training loss: 5971.54, average training loss: 6315.77, base loss: 9220.51
[INFO 2017-06-27 18:04:31,044 main.py:51] epoch 3007, training loss: 5873.45, average training loss: 6315.19, base loss: 9220.44
[INFO 2017-06-27 18:04:32,253 main.py:51] epoch 3008, training loss: 5948.19, average training loss: 6314.29, base loss: 9219.12
[INFO 2017-06-27 18:04:33,463 main.py:51] epoch 3009, training loss: 6146.05, average training loss: 6314.29, base loss: 9219.99
[INFO 2017-06-27 18:04:34,672 main.py:51] epoch 3010, training loss: 5753.52, average training loss: 6314.08, base loss: 9220.00
[INFO 2017-06-27 18:04:35,880 main.py:51] epoch 3011, training loss: 6245.78, average training loss: 6314.08, base loss: 9220.13
[INFO 2017-06-27 18:04:37,089 main.py:51] epoch 3012, training loss: 6014.12, average training loss: 6314.16, base loss: 9220.69
[INFO 2017-06-27 18:04:38,298 main.py:51] epoch 3013, training loss: 8281.88, average training loss: 6315.99, base loss: 9223.26
[INFO 2017-06-27 18:04:39,507 main.py:51] epoch 3014, training loss: 5789.01, average training loss: 6315.48, base loss: 9223.10
[INFO 2017-06-27 18:04:40,715 main.py:51] epoch 3015, training loss: 6066.32, average training loss: 6315.13, base loss: 9222.72
[INFO 2017-06-27 18:04:41,926 main.py:51] epoch 3016, training loss: 6208.05, average training loss: 6315.28, base loss: 9223.58
[INFO 2017-06-27 18:04:43,140 main.py:51] epoch 3017, training loss: 5888.05, average training loss: 6315.08, base loss: 9224.24
[INFO 2017-06-27 18:04:44,349 main.py:51] epoch 3018, training loss: 6070.39, average training loss: 6314.81, base loss: 9224.59
[INFO 2017-06-27 18:04:45,559 main.py:51] epoch 3019, training loss: 5762.23, average training loss: 6314.26, base loss: 9224.49
[INFO 2017-06-27 18:04:46,770 main.py:51] epoch 3020, training loss: 5595.87, average training loss: 6313.65, base loss: 9224.38
[INFO 2017-06-27 18:04:47,983 main.py:51] epoch 3021, training loss: 6062.42, average training loss: 6313.30, base loss: 9223.86
[INFO 2017-06-27 18:04:49,192 main.py:51] epoch 3022, training loss: 6036.78, average training loss: 6310.21, base loss: 9220.17
[INFO 2017-06-27 18:04:50,403 main.py:51] epoch 3023, training loss: 5791.22, average training loss: 6309.30, base loss: 9218.66
[INFO 2017-06-27 18:04:51,610 main.py:51] epoch 3024, training loss: 5682.77, average training loss: 6308.39, base loss: 9217.28
[INFO 2017-06-27 18:04:52,819 main.py:51] epoch 3025, training loss: 8425.45, average training loss: 6310.71, base loss: 9222.14
[INFO 2017-06-27 18:04:54,031 main.py:51] epoch 3026, training loss: 5607.19, average training loss: 6307.98, base loss: 9217.62
[INFO 2017-06-27 18:04:55,243 main.py:51] epoch 3027, training loss: 6020.14, average training loss: 6307.81, base loss: 9218.29
[INFO 2017-06-27 18:04:56,453 main.py:51] epoch 3028, training loss: 5730.81, average training loss: 6307.15, base loss: 9217.60
[INFO 2017-06-27 18:04:57,664 main.py:51] epoch 3029, training loss: 5894.05, average training loss: 6306.57, base loss: 9217.21
[INFO 2017-06-27 18:04:58,877 main.py:51] epoch 3030, training loss: 5859.18, average training loss: 6306.58, base loss: 9217.45
[INFO 2017-06-27 18:05:00,087 main.py:51] epoch 3031, training loss: 6198.58, average training loss: 6307.31, base loss: 9219.54
[INFO 2017-06-27 18:05:01,299 main.py:51] epoch 3032, training loss: 5706.79, average training loss: 6306.43, base loss: 9217.93
[INFO 2017-06-27 18:05:02,511 main.py:51] epoch 3033, training loss: 5834.81, average training loss: 6306.89, base loss: 9219.50
[INFO 2017-06-27 18:05:03,723 main.py:51] epoch 3034, training loss: 8477.61, average training loss: 6308.91, base loss: 9222.44
[INFO 2017-06-27 18:05:04,935 main.py:51] epoch 3035, training loss: 6566.23, average training loss: 6309.68, base loss: 9224.54
[INFO 2017-06-27 18:05:06,145 main.py:51] epoch 3036, training loss: 5769.08, average training loss: 6309.69, base loss: 9225.29
[INFO 2017-06-27 18:05:07,355 main.py:51] epoch 3037, training loss: 5667.16, average training loss: 6308.96, base loss: 9224.47
[INFO 2017-06-27 18:05:08,570 main.py:51] epoch 3038, training loss: 6254.35, average training loss: 6308.55, base loss: 9224.92
[INFO 2017-06-27 18:05:09,780 main.py:51] epoch 3039, training loss: 6230.16, average training loss: 6308.66, base loss: 9226.07
[INFO 2017-06-27 18:05:10,989 main.py:51] epoch 3040, training loss: 5524.11, average training loss: 6307.87, base loss: 9224.71
[INFO 2017-06-27 18:05:12,198 main.py:51] epoch 3041, training loss: 6254.09, average training loss: 6304.41, base loss: 9222.54
[INFO 2017-06-27 18:05:13,406 main.py:51] epoch 3042, training loss: 6224.49, average training loss: 6304.41, base loss: 9222.71
[INFO 2017-06-27 18:05:14,618 main.py:51] epoch 3043, training loss: 8080.34, average training loss: 6306.47, base loss: 9226.90
[INFO 2017-06-27 18:05:15,826 main.py:51] epoch 3044, training loss: 6522.49, average training loss: 6306.51, base loss: 9227.89
[INFO 2017-06-27 18:05:17,036 main.py:51] epoch 3045, training loss: 5435.76, average training loss: 6306.04, base loss: 9227.71
[INFO 2017-06-27 18:05:18,248 main.py:51] epoch 3046, training loss: 6271.87, average training loss: 6305.83, base loss: 9228.00
[INFO 2017-06-27 18:05:19,457 main.py:51] epoch 3047, training loss: 5823.49, average training loss: 6305.49, base loss: 9227.98
[INFO 2017-06-27 18:05:20,667 main.py:51] epoch 3048, training loss: 5782.74, average training loss: 6302.55, base loss: 9224.03
[INFO 2017-06-27 18:05:21,881 main.py:51] epoch 3049, training loss: 6341.07, average training loss: 6302.60, base loss: 9224.78
[INFO 2017-06-27 18:05:23,091 main.py:51] epoch 3050, training loss: 5782.27, average training loss: 6301.61, base loss: 9223.60
[INFO 2017-06-27 18:05:24,302 main.py:51] epoch 3051, training loss: 5841.83, average training loss: 6301.28, base loss: 9223.93
[INFO 2017-06-27 18:05:25,511 main.py:51] epoch 3052, training loss: 6293.57, average training loss: 6301.00, base loss: 9224.03
[INFO 2017-06-27 18:05:26,723 main.py:51] epoch 3053, training loss: 6165.56, average training loss: 6301.01, base loss: 9224.83
[INFO 2017-06-27 18:05:27,932 main.py:51] epoch 3054, training loss: 6313.49, average training loss: 6301.55, base loss: 9226.49
[INFO 2017-06-27 18:05:29,140 main.py:51] epoch 3055, training loss: 5700.23, average training loss: 6301.24, base loss: 9227.10
[INFO 2017-06-27 18:05:30,352 main.py:51] epoch 3056, training loss: 5593.91, average training loss: 6300.34, base loss: 9225.93
[INFO 2017-06-27 18:05:31,563 main.py:51] epoch 3057, training loss: 5572.65, average training loss: 6299.58, base loss: 9224.74
[INFO 2017-06-27 18:05:32,772 main.py:51] epoch 3058, training loss: 5617.20, average training loss: 6299.08, base loss: 9224.59
[INFO 2017-06-27 18:05:33,983 main.py:51] epoch 3059, training loss: 6161.21, average training loss: 6298.89, base loss: 9225.19
[INFO 2017-06-27 18:05:35,192 main.py:51] epoch 3060, training loss: 5991.33, average training loss: 6298.83, base loss: 9226.32
[INFO 2017-06-27 18:05:36,402 main.py:51] epoch 3061, training loss: 5815.67, average training loss: 6298.23, base loss: 9225.10
[INFO 2017-06-27 18:05:37,611 main.py:51] epoch 3062, training loss: 6227.18, average training loss: 6298.32, base loss: 9225.80
[INFO 2017-06-27 18:05:38,821 main.py:51] epoch 3063, training loss: 5777.70, average training loss: 6297.77, base loss: 9225.37
[INFO 2017-06-27 18:05:40,033 main.py:51] epoch 3064, training loss: 5832.20, average training loss: 6297.13, base loss: 9224.26
[INFO 2017-06-27 18:05:41,249 main.py:51] epoch 3065, training loss: 6293.60, average training loss: 6297.14, base loss: 9224.97
[INFO 2017-06-27 18:05:42,459 main.py:51] epoch 3066, training loss: 5934.59, average training loss: 6293.37, base loss: 9221.47
[INFO 2017-06-27 18:05:43,668 main.py:51] epoch 3067, training loss: 5940.85, average training loss: 6293.25, base loss: 9222.46
[INFO 2017-06-27 18:05:44,878 main.py:51] epoch 3068, training loss: 5487.34, average training loss: 6292.55, base loss: 9221.77
[INFO 2017-06-27 18:05:46,085 main.py:51] epoch 3069, training loss: 5565.71, average training loss: 6288.24, base loss: 9217.49
[INFO 2017-06-27 18:05:47,296 main.py:51] epoch 3070, training loss: 6211.61, average training loss: 6288.23, base loss: 9218.10
[INFO 2017-06-27 18:05:48,506 main.py:51] epoch 3071, training loss: 6080.84, average training loss: 6288.70, base loss: 9219.29
[INFO 2017-06-27 18:05:49,714 main.py:51] epoch 3072, training loss: 6051.63, average training loss: 6288.45, base loss: 9219.64
[INFO 2017-06-27 18:05:50,924 main.py:51] epoch 3073, training loss: 6227.21, average training loss: 6288.43, base loss: 9220.23
[INFO 2017-06-27 18:05:52,134 main.py:51] epoch 3074, training loss: 5684.12, average training loss: 6288.37, base loss: 9220.47
[INFO 2017-06-27 18:05:53,345 main.py:51] epoch 3075, training loss: 5802.83, average training loss: 6288.66, base loss: 9221.77
[INFO 2017-06-27 18:05:54,558 main.py:51] epoch 3076, training loss: 5642.93, average training loss: 6288.73, base loss: 9222.95
[INFO 2017-06-27 18:05:55,767 main.py:51] epoch 3077, training loss: 6202.87, average training loss: 6288.29, base loss: 9222.26
[INFO 2017-06-27 18:05:56,973 main.py:51] epoch 3078, training loss: 8128.32, average training loss: 6290.55, base loss: 9226.12
[INFO 2017-06-27 18:05:58,183 main.py:51] epoch 3079, training loss: 5835.37, average training loss: 6289.86, base loss: 9225.31
[INFO 2017-06-27 18:05:59,395 main.py:51] epoch 3080, training loss: 6124.36, average training loss: 6289.38, base loss: 9225.14
[INFO 2017-06-27 18:06:00,607 main.py:51] epoch 3081, training loss: 6087.60, average training loss: 6289.14, base loss: 9225.19
[INFO 2017-06-27 18:06:01,816 main.py:51] epoch 3082, training loss: 7846.76, average training loss: 6290.33, base loss: 9227.09
[INFO 2017-06-27 18:06:03,026 main.py:51] epoch 3083, training loss: 5953.41, average training loss: 6290.19, base loss: 9227.28
[INFO 2017-06-27 18:06:04,237 main.py:51] epoch 3084, training loss: 6072.13, average training loss: 6290.04, base loss: 9226.86
[INFO 2017-06-27 18:06:05,443 main.py:51] epoch 3085, training loss: 6070.64, average training loss: 6290.21, base loss: 9227.59
[INFO 2017-06-27 18:06:06,657 main.py:51] epoch 3086, training loss: 5870.71, average training loss: 6290.06, base loss: 9227.81
[INFO 2017-06-27 18:06:07,866 main.py:51] epoch 3087, training loss: 5848.20, average training loss: 6289.26, base loss: 9227.01
[INFO 2017-06-27 18:06:09,073 main.py:51] epoch 3088, training loss: 6018.01, average training loss: 6288.71, base loss: 9226.16
[INFO 2017-06-27 18:06:10,280 main.py:51] epoch 3089, training loss: 5877.83, average training loss: 6288.23, base loss: 9225.90
[INFO 2017-06-27 18:06:11,493 main.py:51] epoch 3090, training loss: 5946.73, average training loss: 6287.91, base loss: 9225.98
[INFO 2017-06-27 18:06:12,706 main.py:51] epoch 3091, training loss: 6238.38, average training loss: 6284.07, base loss: 9222.12
[INFO 2017-06-27 18:06:13,916 main.py:51] epoch 3092, training loss: 8263.18, average training loss: 6286.42, base loss: 9226.26
[INFO 2017-06-27 18:06:15,125 main.py:51] epoch 3093, training loss: 5818.41, average training loss: 6286.31, base loss: 9226.41
[INFO 2017-06-27 18:06:16,335 main.py:51] epoch 3094, training loss: 5911.42, average training loss: 6285.55, base loss: 9225.54
[INFO 2017-06-27 18:06:17,545 main.py:51] epoch 3095, training loss: 6374.73, average training loss: 6286.12, base loss: 9227.32
[INFO 2017-06-27 18:06:18,756 main.py:51] epoch 3096, training loss: 6340.67, average training loss: 6286.26, base loss: 9228.22
[INFO 2017-06-27 18:06:19,968 main.py:51] epoch 3097, training loss: 5702.22, average training loss: 6286.13, base loss: 9227.36
[INFO 2017-06-27 18:06:21,176 main.py:51] epoch 3098, training loss: 6261.55, average training loss: 6286.22, base loss: 9228.03
[INFO 2017-06-27 18:06:22,387 main.py:51] epoch 3099, training loss: 6159.61, average training loss: 6286.25, base loss: 9228.82
[INFO 2017-06-27 18:06:22,387 main.py:53] epoch 3099, testing
[INFO 2017-06-27 18:06:27,082 main.py:105] average testing loss: 5880.48, base loss: 8830.51
[INFO 2017-06-27 18:06:27,082 main.py:106] improve_loss: 2950.02, improve_percent: 0.33
[INFO 2017-06-27 18:06:27,083 main.py:76] current best improved percent: 0.34
[INFO 2017-06-27 18:06:28,289 main.py:51] epoch 3100, training loss: 5673.09, average training loss: 6285.23, base loss: 9227.13
[INFO 2017-06-27 18:06:29,497 main.py:51] epoch 3101, training loss: 5918.69, average training loss: 6281.26, base loss: 9222.93
[INFO 2017-06-27 18:06:30,707 main.py:51] epoch 3102, training loss: 7932.76, average training loss: 6283.16, base loss: 9225.82
[INFO 2017-06-27 18:06:31,914 main.py:51] epoch 3103, training loss: 8015.27, average training loss: 6285.03, base loss: 9230.06
[INFO 2017-06-27 18:06:33,122 main.py:51] epoch 3104, training loss: 5597.32, average training loss: 6284.72, base loss: 9229.33
[INFO 2017-06-27 18:06:34,331 main.py:51] epoch 3105, training loss: 5757.54, average training loss: 6283.61, base loss: 9227.64
[INFO 2017-06-27 18:06:35,545 main.py:51] epoch 3106, training loss: 5994.57, average training loss: 6283.72, base loss: 9228.32
[INFO 2017-06-27 18:06:36,753 main.py:51] epoch 3107, training loss: 6013.60, average training loss: 6283.53, base loss: 9228.04
[INFO 2017-06-27 18:06:37,962 main.py:51] epoch 3108, training loss: 5765.51, average training loss: 6283.67, base loss: 9228.20
[INFO 2017-06-27 18:06:39,169 main.py:51] epoch 3109, training loss: 5818.89, average training loss: 6283.45, base loss: 9228.25
[INFO 2017-06-27 18:06:40,378 main.py:51] epoch 3110, training loss: 5999.22, average training loss: 6283.12, base loss: 9227.69
[INFO 2017-06-27 18:06:41,591 main.py:51] epoch 3111, training loss: 6303.26, average training loss: 6282.61, base loss: 9227.48
[INFO 2017-06-27 18:06:42,803 main.py:51] epoch 3112, training loss: 6298.26, average training loss: 6282.73, base loss: 9228.25
[INFO 2017-06-27 18:06:44,015 main.py:51] epoch 3113, training loss: 5845.76, average training loss: 6281.78, base loss: 9226.76
[INFO 2017-06-27 18:06:45,227 main.py:51] epoch 3114, training loss: 6572.89, average training loss: 6278.71, base loss: 9225.02
[INFO 2017-06-27 18:06:46,436 main.py:51] epoch 3115, training loss: 5999.60, average training loss: 6278.74, base loss: 9225.98
[INFO 2017-06-27 18:06:47,644 main.py:51] epoch 3116, training loss: 5313.14, average training loss: 6277.40, base loss: 9224.47
[INFO 2017-06-27 18:06:48,854 main.py:51] epoch 3117, training loss: 5925.17, average training loss: 6276.82, base loss: 9224.08
[INFO 2017-06-27 18:06:50,057 main.py:51] epoch 3118, training loss: 5975.24, average training loss: 6276.75, base loss: 9224.30
[INFO 2017-06-27 18:06:51,260 main.py:51] epoch 3119, training loss: 6195.17, average training loss: 6277.48, base loss: 9226.17
[INFO 2017-06-27 18:06:52,472 main.py:51] epoch 3120, training loss: 9288.59, average training loss: 6280.78, base loss: 9231.22
[INFO 2017-06-27 18:06:53,682 main.py:51] epoch 3121, training loss: 5697.17, average training loss: 6280.10, base loss: 9229.75
[INFO 2017-06-27 18:06:54,894 main.py:51] epoch 3122, training loss: 6277.10, average training loss: 6280.05, base loss: 9229.83
[INFO 2017-06-27 18:06:56,104 main.py:51] epoch 3123, training loss: 8605.34, average training loss: 6282.41, base loss: 9232.88
[INFO 2017-06-27 18:06:57,314 main.py:51] epoch 3124, training loss: 5538.54, average training loss: 6282.00, base loss: 9232.60
[INFO 2017-06-27 18:06:58,520 main.py:51] epoch 3125, training loss: 6639.15, average training loss: 6282.48, base loss: 9234.30
[INFO 2017-06-27 18:06:59,733 main.py:51] epoch 3126, training loss: 6191.35, average training loss: 6280.29, base loss: 9231.45
[INFO 2017-06-27 18:07:00,944 main.py:51] epoch 3127, training loss: 5915.07, average training loss: 6280.06, base loss: 9231.62
[INFO 2017-06-27 18:07:02,155 main.py:51] epoch 3128, training loss: 6009.52, average training loss: 6279.92, base loss: 9231.54
[INFO 2017-06-27 18:07:03,369 main.py:51] epoch 3129, training loss: 5966.13, average training loss: 6280.17, base loss: 9232.66
[INFO 2017-06-27 18:07:04,572 main.py:51] epoch 3130, training loss: 5893.03, average training loss: 6275.29, base loss: 9226.58
[INFO 2017-06-27 18:07:05,775 main.py:51] epoch 3131, training loss: 6008.88, average training loss: 6275.75, base loss: 9227.78
[INFO 2017-06-27 18:07:06,979 main.py:51] epoch 3132, training loss: 5882.16, average training loss: 6275.10, base loss: 9226.74
[INFO 2017-06-27 18:07:08,186 main.py:51] epoch 3133, training loss: 6098.21, average training loss: 6274.77, base loss: 9226.37
[INFO 2017-06-27 18:07:09,399 main.py:51] epoch 3134, training loss: 6019.62, average training loss: 6274.38, base loss: 9226.51
[INFO 2017-06-27 18:07:10,611 main.py:51] epoch 3135, training loss: 6469.43, average training loss: 6274.96, base loss: 9228.47
[INFO 2017-06-27 18:07:11,823 main.py:51] epoch 3136, training loss: 5981.39, average training loss: 6273.96, base loss: 9227.34
[INFO 2017-06-27 18:07:13,029 main.py:51] epoch 3137, training loss: 6190.40, average training loss: 6273.44, base loss: 9226.42
[INFO 2017-06-27 18:07:14,242 main.py:51] epoch 3138, training loss: 5809.18, average training loss: 6273.06, base loss: 9226.28
[INFO 2017-06-27 18:07:15,453 main.py:51] epoch 3139, training loss: 6178.06, average training loss: 6273.30, base loss: 9227.30
[INFO 2017-06-27 18:07:16,668 main.py:51] epoch 3140, training loss: 6145.14, average training loss: 6269.04, base loss: 9222.93
[INFO 2017-06-27 18:07:17,883 main.py:51] epoch 3141, training loss: 5876.08, average training loss: 6268.67, base loss: 9222.57
[INFO 2017-06-27 18:07:19,093 main.py:51] epoch 3142, training loss: 6330.46, average training loss: 6269.12, base loss: 9224.18
[INFO 2017-06-27 18:07:20,307 main.py:51] epoch 3143, training loss: 5533.08, average training loss: 6267.68, base loss: 9221.31
[INFO 2017-06-27 18:07:21,519 main.py:51] epoch 3144, training loss: 5783.63, average training loss: 6267.12, base loss: 9220.21
[INFO 2017-06-27 18:07:22,726 main.py:51] epoch 3145, training loss: 6070.96, average training loss: 6267.30, base loss: 9221.41
[INFO 2017-06-27 18:07:23,937 main.py:51] epoch 3146, training loss: 6097.33, average training loss: 6267.06, base loss: 9221.47
[INFO 2017-06-27 18:07:25,148 main.py:51] epoch 3147, training loss: 6156.83, average training loss: 6266.94, base loss: 9221.94
[INFO 2017-06-27 18:07:26,361 main.py:51] epoch 3148, training loss: 5836.32, average training loss: 6266.30, base loss: 9221.04
[INFO 2017-06-27 18:07:27,571 main.py:51] epoch 3149, training loss: 6006.96, average training loss: 6266.66, base loss: 9222.64
[INFO 2017-06-27 18:07:28,782 main.py:51] epoch 3150, training loss: 6100.10, average training loss: 6266.55, base loss: 9223.28
[INFO 2017-06-27 18:07:29,992 main.py:51] epoch 3151, training loss: 6065.76, average training loss: 6264.78, base loss: 9221.42
[INFO 2017-06-27 18:07:31,204 main.py:51] epoch 3152, training loss: 8344.53, average training loss: 6266.97, base loss: 9225.11
[INFO 2017-06-27 18:07:32,419 main.py:51] epoch 3153, training loss: 6203.72, average training loss: 6266.76, base loss: 9224.96
[INFO 2017-06-27 18:07:33,632 main.py:51] epoch 3154, training loss: 5925.63, average training loss: 6266.58, base loss: 9225.28
[INFO 2017-06-27 18:07:34,841 main.py:51] epoch 3155, training loss: 5629.18, average training loss: 6265.64, base loss: 9223.83
[INFO 2017-06-27 18:07:36,049 main.py:51] epoch 3156, training loss: 5764.15, average training loss: 6265.71, base loss: 9223.76
[INFO 2017-06-27 18:07:37,257 main.py:51] epoch 3157, training loss: 5838.59, average training loss: 6265.72, base loss: 9224.03
[INFO 2017-06-27 18:07:38,472 main.py:51] epoch 3158, training loss: 6324.50, average training loss: 6265.99, base loss: 9225.09
[INFO 2017-06-27 18:07:39,687 main.py:51] epoch 3159, training loss: 6075.15, average training loss: 6266.21, base loss: 9226.44
[INFO 2017-06-27 18:07:40,893 main.py:51] epoch 3160, training loss: 5857.83, average training loss: 6266.34, base loss: 9227.50
[INFO 2017-06-27 18:07:42,103 main.py:51] epoch 3161, training loss: 6352.76, average training loss: 6266.34, base loss: 9228.36
[INFO 2017-06-27 18:07:43,314 main.py:51] epoch 3162, training loss: 6580.05, average training loss: 6266.48, base loss: 9229.45
[INFO 2017-06-27 18:07:44,521 main.py:51] epoch 3163, training loss: 5921.09, average training loss: 6263.66, base loss: 9225.23
[INFO 2017-06-27 18:07:45,731 main.py:51] epoch 3164, training loss: 5856.11, average training loss: 6263.50, base loss: 9225.19
[INFO 2017-06-27 18:07:46,944 main.py:51] epoch 3165, training loss: 5968.19, average training loss: 6263.26, base loss: 9225.15
[INFO 2017-06-27 18:07:48,152 main.py:51] epoch 3166, training loss: 7817.98, average training loss: 6265.13, base loss: 9229.24
[INFO 2017-06-27 18:07:49,366 main.py:51] epoch 3167, training loss: 5485.32, average training loss: 6264.67, base loss: 9229.18
[INFO 2017-06-27 18:07:50,583 main.py:51] epoch 3168, training loss: 6251.90, average training loss: 6264.09, base loss: 9228.73
[INFO 2017-06-27 18:07:51,790 main.py:51] epoch 3169, training loss: 5607.23, average training loss: 6263.80, base loss: 9228.62
[INFO 2017-06-27 18:07:53,004 main.py:51] epoch 3170, training loss: 7914.63, average training loss: 6265.51, base loss: 9232.65
[INFO 2017-06-27 18:07:54,218 main.py:51] epoch 3171, training loss: 5887.64, average training loss: 6262.11, base loss: 9229.38
[INFO 2017-06-27 18:07:55,429 main.py:51] epoch 3172, training loss: 5826.65, average training loss: 6261.84, base loss: 9229.75
[INFO 2017-06-27 18:07:56,637 main.py:51] epoch 3173, training loss: 5716.38, average training loss: 6260.95, base loss: 9228.58
[INFO 2017-06-27 18:07:57,847 main.py:51] epoch 3174, training loss: 5815.02, average training loss: 6260.35, base loss: 9228.07
[INFO 2017-06-27 18:07:59,058 main.py:51] epoch 3175, training loss: 6218.12, average training loss: 6260.57, base loss: 9229.24
[INFO 2017-06-27 18:08:00,271 main.py:51] epoch 3176, training loss: 6182.96, average training loss: 6260.52, base loss: 9229.86
[INFO 2017-06-27 18:08:01,477 main.py:51] epoch 3177, training loss: 6014.31, average training loss: 6259.97, base loss: 9229.10
[INFO 2017-06-27 18:08:02,684 main.py:51] epoch 3178, training loss: 8033.13, average training loss: 6261.96, base loss: 9232.71
[INFO 2017-06-27 18:08:03,888 main.py:51] epoch 3179, training loss: 5798.17, average training loss: 6261.56, base loss: 9232.27
[INFO 2017-06-27 18:08:05,094 main.py:51] epoch 3180, training loss: 5768.82, average training loss: 6261.23, base loss: 9232.79
[INFO 2017-06-27 18:08:06,305 main.py:51] epoch 3181, training loss: 6140.53, average training loss: 6260.67, base loss: 9232.21
[INFO 2017-06-27 18:08:07,519 main.py:51] epoch 3182, training loss: 5768.93, average training loss: 6259.91, base loss: 9230.63
[INFO 2017-06-27 18:08:08,729 main.py:51] epoch 3183, training loss: 6144.29, average training loss: 6260.00, base loss: 9231.83
[INFO 2017-06-27 18:08:09,942 main.py:51] epoch 3184, training loss: 6020.97, average training loss: 6259.39, base loss: 9231.63
[INFO 2017-06-27 18:08:11,150 main.py:51] epoch 3185, training loss: 5891.46, average training loss: 6257.05, base loss: 9228.56
[INFO 2017-06-27 18:08:12,360 main.py:51] epoch 3186, training loss: 6030.83, average training loss: 6256.88, base loss: 9229.05
[INFO 2017-06-27 18:08:13,565 main.py:51] epoch 3187, training loss: 6012.03, average training loss: 6256.62, base loss: 9228.97
[INFO 2017-06-27 18:08:14,773 main.py:51] epoch 3188, training loss: 6190.06, average training loss: 6256.84, base loss: 9230.55
[INFO 2017-06-27 18:08:15,977 main.py:51] epoch 3189, training loss: 6117.09, average training loss: 6256.38, base loss: 9230.39
[INFO 2017-06-27 18:08:17,186 main.py:51] epoch 3190, training loss: 5830.54, average training loss: 6255.78, base loss: 9229.75
[INFO 2017-06-27 18:08:18,396 main.py:51] epoch 3191, training loss: 9574.29, average training loss: 6259.21, base loss: 9236.38
[INFO 2017-06-27 18:08:19,609 main.py:51] epoch 3192, training loss: 10242.01, average training loss: 6263.07, base loss: 9243.88
[INFO 2017-06-27 18:08:20,821 main.py:51] epoch 3193, training loss: 6455.22, average training loss: 6263.49, base loss: 9245.09
[INFO 2017-06-27 18:08:22,029 main.py:51] epoch 3194, training loss: 5869.98, average training loss: 6262.82, base loss: 9244.25
[INFO 2017-06-27 18:08:23,240 main.py:51] epoch 3195, training loss: 6133.55, average training loss: 6260.37, base loss: 9241.39
[INFO 2017-06-27 18:08:24,449 main.py:51] epoch 3196, training loss: 6086.22, average training loss: 6259.89, base loss: 9240.44
[INFO 2017-06-27 18:08:25,660 main.py:51] epoch 3197, training loss: 5597.08, average training loss: 6259.28, base loss: 9239.64
[INFO 2017-06-27 18:08:26,871 main.py:51] epoch 3198, training loss: 6024.02, average training loss: 6259.22, base loss: 9240.66
[INFO 2017-06-27 18:08:28,083 main.py:51] epoch 3199, training loss: 5830.35, average training loss: 6258.80, base loss: 9240.29
[INFO 2017-06-27 18:08:28,083 main.py:53] epoch 3199, testing
[INFO 2017-06-27 18:08:32,770 main.py:105] average testing loss: 5956.93, base loss: 8747.85
[INFO 2017-06-27 18:08:32,770 main.py:106] improve_loss: 2790.92, improve_percent: 0.32
[INFO 2017-06-27 18:08:32,771 main.py:76] current best improved percent: 0.34
[INFO 2017-06-27 18:08:33,979 main.py:51] epoch 3200, training loss: 6404.16, average training loss: 6259.50, base loss: 9242.59
[INFO 2017-06-27 18:08:35,193 main.py:51] epoch 3201, training loss: 8164.19, average training loss: 6261.83, base loss: 9246.81
[INFO 2017-06-27 18:08:36,405 main.py:51] epoch 3202, training loss: 5829.15, average training loss: 6261.67, base loss: 9246.44
[INFO 2017-06-27 18:08:37,611 main.py:51] epoch 3203, training loss: 5753.81, average training loss: 6261.46, base loss: 9246.71
[INFO 2017-06-27 18:08:38,818 main.py:51] epoch 3204, training loss: 6101.59, average training loss: 6261.62, base loss: 9247.69
[INFO 2017-06-27 18:08:40,027 main.py:51] epoch 3205, training loss: 6167.03, average training loss: 6261.85, base loss: 9249.15
[INFO 2017-06-27 18:08:41,238 main.py:51] epoch 3206, training loss: 6310.68, average training loss: 6261.90, base loss: 9250.23
[INFO 2017-06-27 18:08:42,449 main.py:51] epoch 3207, training loss: 6470.76, average training loss: 6262.47, base loss: 9251.29
[INFO 2017-06-27 18:08:43,658 main.py:51] epoch 3208, training loss: 6183.25, average training loss: 6262.60, base loss: 9252.47
[INFO 2017-06-27 18:08:44,864 main.py:51] epoch 3209, training loss: 5699.05, average training loss: 6262.04, base loss: 9251.92
[INFO 2017-06-27 18:08:46,072 main.py:51] epoch 3210, training loss: 5601.41, average training loss: 6259.74, base loss: 9247.98
[INFO 2017-06-27 18:08:47,288 main.py:51] epoch 3211, training loss: 6043.83, average training loss: 6259.71, base loss: 9248.29
[INFO 2017-06-27 18:08:48,499 main.py:51] epoch 3212, training loss: 6306.86, average training loss: 6260.03, base loss: 9249.13
[INFO 2017-06-27 18:08:49,711 main.py:51] epoch 3213, training loss: 6327.93, average training loss: 6260.18, base loss: 9249.41
[INFO 2017-06-27 18:08:50,926 main.py:51] epoch 3214, training loss: 5824.73, average training loss: 6259.35, base loss: 9247.57
[INFO 2017-06-27 18:08:52,141 main.py:51] epoch 3215, training loss: 5836.96, average training loss: 6258.80, base loss: 9247.34
[INFO 2017-06-27 18:08:53,353 main.py:51] epoch 3216, training loss: 6258.94, average training loss: 6259.32, base loss: 9249.11
[INFO 2017-06-27 18:08:54,562 main.py:51] epoch 3217, training loss: 7946.22, average training loss: 6261.06, base loss: 9251.92
[INFO 2017-06-27 18:08:55,776 main.py:51] epoch 3218, training loss: 5570.45, average training loss: 6260.83, base loss: 9251.98
[INFO 2017-06-27 18:08:56,989 main.py:51] epoch 3219, training loss: 5682.38, average training loss: 6260.40, base loss: 9251.09
[INFO 2017-06-27 18:08:58,203 main.py:51] epoch 3220, training loss: 6015.10, average training loss: 6260.13, base loss: 9250.61
[INFO 2017-06-27 18:08:59,414 main.py:51] epoch 3221, training loss: 5881.91, average training loss: 6260.01, base loss: 9250.84
[INFO 2017-06-27 18:09:00,629 main.py:51] epoch 3222, training loss: 5910.42, average training loss: 6259.43, base loss: 9249.63
[INFO 2017-06-27 18:09:01,840 main.py:51] epoch 3223, training loss: 5956.67, average training loss: 6259.45, base loss: 9250.24
[INFO 2017-06-27 18:09:03,053 main.py:51] epoch 3224, training loss: 6357.45, average training loss: 6259.71, base loss: 9251.25
[INFO 2017-06-27 18:09:04,268 main.py:51] epoch 3225, training loss: 5755.08, average training loss: 6259.39, base loss: 9251.23
[INFO 2017-06-27 18:09:05,482 main.py:51] epoch 3226, training loss: 5795.22, average training loss: 6258.70, base loss: 9249.96
[INFO 2017-06-27 18:09:06,694 main.py:51] epoch 3227, training loss: 7889.82, average training loss: 6260.86, base loss: 9254.10
[INFO 2017-06-27 18:09:07,910 main.py:51] epoch 3228, training loss: 6354.98, average training loss: 6261.20, base loss: 9255.09
[INFO 2017-06-27 18:09:09,125 main.py:51] epoch 3229, training loss: 6092.52, average training loss: 6260.81, base loss: 9254.70
[INFO 2017-06-27 18:09:10,337 main.py:51] epoch 3230, training loss: 5870.25, average training loss: 6260.65, base loss: 9255.28
[INFO 2017-06-27 18:09:11,550 main.py:51] epoch 3231, training loss: 5910.53, average training loss: 6260.72, base loss: 9256.32
[INFO 2017-06-27 18:09:12,758 main.py:51] epoch 3232, training loss: 5782.33, average training loss: 6259.95, base loss: 9255.50
[INFO 2017-06-27 18:09:13,971 main.py:51] epoch 3233, training loss: 5812.40, average training loss: 6257.22, base loss: 9251.88
[INFO 2017-06-27 18:09:15,182 main.py:51] epoch 3234, training loss: 6128.07, average training loss: 6256.83, base loss: 9251.10
[INFO 2017-06-27 18:09:16,395 main.py:51] epoch 3235, training loss: 5659.42, average training loss: 6256.03, base loss: 9249.78
[INFO 2017-06-27 18:09:17,606 main.py:51] epoch 3236, training loss: 6306.01, average training loss: 6256.17, base loss: 9250.94
[INFO 2017-06-27 18:09:18,818 main.py:51] epoch 3237, training loss: 6043.81, average training loss: 6256.04, base loss: 9251.41
[INFO 2017-06-27 18:09:20,024 main.py:51] epoch 3238, training loss: 6504.22, average training loss: 6255.65, base loss: 9251.20
[INFO 2017-06-27 18:09:21,236 main.py:51] epoch 3239, training loss: 6086.64, average training loss: 6255.36, base loss: 9250.92
[INFO 2017-06-27 18:09:22,447 main.py:51] epoch 3240, training loss: 5664.78, average training loss: 6252.34, base loss: 9246.26
[INFO 2017-06-27 18:09:23,660 main.py:51] epoch 3241, training loss: 6086.76, average training loss: 6252.31, base loss: 9246.74
[INFO 2017-06-27 18:09:24,871 main.py:51] epoch 3242, training loss: 7869.10, average training loss: 6250.77, base loss: 9246.12
[INFO 2017-06-27 18:09:26,081 main.py:51] epoch 3243, training loss: 7396.16, average training loss: 6251.94, base loss: 9247.61
[INFO 2017-06-27 18:09:27,295 main.py:51] epoch 3244, training loss: 6084.02, average training loss: 6251.40, base loss: 9246.81
[INFO 2017-06-27 18:09:28,509 main.py:51] epoch 3245, training loss: 5766.48, average training loss: 6251.26, base loss: 9247.13
[INFO 2017-06-27 18:09:29,722 main.py:51] epoch 3246, training loss: 5967.85, average training loss: 6250.89, base loss: 9246.66
[INFO 2017-06-27 18:09:30,936 main.py:51] epoch 3247, training loss: 5789.84, average training loss: 6250.61, base loss: 9246.48
[INFO 2017-06-27 18:09:32,148 main.py:51] epoch 3248, training loss: 6011.43, average training loss: 6250.66, base loss: 9247.39
[INFO 2017-06-27 18:09:33,361 main.py:51] epoch 3249, training loss: 5569.34, average training loss: 6249.90, base loss: 9245.78
[INFO 2017-06-27 18:09:34,575 main.py:51] epoch 3250, training loss: 7928.61, average training loss: 6248.57, base loss: 9245.23
[INFO 2017-06-27 18:09:35,790 main.py:51] epoch 3251, training loss: 5844.22, average training loss: 6248.88, base loss: 9246.71
[INFO 2017-06-27 18:09:37,002 main.py:51] epoch 3252, training loss: 5930.35, average training loss: 6248.94, base loss: 9247.33
[INFO 2017-06-27 18:09:38,213 main.py:51] epoch 3253, training loss: 5812.09, average training loss: 6248.32, base loss: 9246.31
[INFO 2017-06-27 18:09:39,424 main.py:51] epoch 3254, training loss: 7454.45, average training loss: 6249.03, base loss: 9247.71
[INFO 2017-06-27 18:09:40,635 main.py:51] epoch 3255, training loss: 5749.84, average training loss: 6248.50, base loss: 9247.20
[INFO 2017-06-27 18:09:41,847 main.py:51] epoch 3256, training loss: 5301.08, average training loss: 6247.53, base loss: 9245.34
[INFO 2017-06-27 18:09:43,059 main.py:51] epoch 3257, training loss: 5942.90, average training loss: 6247.41, base loss: 9245.90
[INFO 2017-06-27 18:09:44,269 main.py:51] epoch 3258, training loss: 6036.08, average training loss: 6247.16, base loss: 9246.21
[INFO 2017-06-27 18:09:45,483 main.py:51] epoch 3259, training loss: 5721.81, average training loss: 6246.47, base loss: 9244.44
[INFO 2017-06-27 18:09:46,693 main.py:51] epoch 3260, training loss: 5749.70, average training loss: 6245.84, base loss: 9244.04
[INFO 2017-06-27 18:09:47,902 main.py:51] epoch 3261, training loss: 6044.39, average training loss: 6245.68, base loss: 9244.73
[INFO 2017-06-27 18:09:49,115 main.py:51] epoch 3262, training loss: 5784.60, average training loss: 6245.14, base loss: 9244.31
[INFO 2017-06-27 18:09:50,327 main.py:51] epoch 3263, training loss: 5558.51, average training loss: 6244.40, base loss: 9243.16
[INFO 2017-06-27 18:09:51,539 main.py:51] epoch 3264, training loss: 6242.80, average training loss: 6244.50, base loss: 9244.12
[INFO 2017-06-27 18:09:52,751 main.py:51] epoch 3265, training loss: 8310.12, average training loss: 6246.61, base loss: 9247.97
[INFO 2017-06-27 18:09:53,965 main.py:51] epoch 3266, training loss: 5879.63, average training loss: 6246.26, base loss: 9247.76
[INFO 2017-06-27 18:09:55,175 main.py:51] epoch 3267, training loss: 5798.27, average training loss: 6245.98, base loss: 9247.83
[INFO 2017-06-27 18:09:56,388 main.py:51] epoch 3268, training loss: 5765.91, average training loss: 6245.27, base loss: 9247.24
[INFO 2017-06-27 18:09:57,597 main.py:51] epoch 3269, training loss: 5979.93, average training loss: 6244.96, base loss: 9246.96
[INFO 2017-06-27 18:09:58,807 main.py:51] epoch 3270, training loss: 5421.93, average training loss: 6244.84, base loss: 9246.88
[INFO 2017-06-27 18:10:00,015 main.py:51] epoch 3271, training loss: 8111.56, average training loss: 6246.88, base loss: 9250.18
[INFO 2017-06-27 18:10:01,222 main.py:51] epoch 3272, training loss: 5357.15, average training loss: 6245.87, base loss: 9248.74
[INFO 2017-06-27 18:10:02,432 main.py:51] epoch 3273, training loss: 5572.66, average training loss: 6245.58, base loss: 9248.54
[INFO 2017-06-27 18:10:03,647 main.py:51] epoch 3274, training loss: 5996.72, average training loss: 6244.79, base loss: 9247.57
[INFO 2017-06-27 18:10:04,856 main.py:51] epoch 3275, training loss: 6382.04, average training loss: 6244.78, base loss: 9247.66
[INFO 2017-06-27 18:10:06,064 main.py:51] epoch 3276, training loss: 5966.80, average training loss: 6244.51, base loss: 9247.58
[INFO 2017-06-27 18:10:07,279 main.py:51] epoch 3277, training loss: 6213.20, average training loss: 6244.43, base loss: 9248.15
[INFO 2017-06-27 18:10:08,491 main.py:51] epoch 3278, training loss: 6440.45, average training loss: 6244.13, base loss: 9247.63
[INFO 2017-06-27 18:10:09,698 main.py:51] epoch 3279, training loss: 6064.87, average training loss: 6241.24, base loss: 9243.15
[INFO 2017-06-27 18:10:10,905 main.py:51] epoch 3280, training loss: 6336.76, average training loss: 6241.23, base loss: 9243.72
[INFO 2017-06-27 18:10:12,117 main.py:51] epoch 3281, training loss: 5627.72, average training loss: 6240.41, base loss: 9242.27
[INFO 2017-06-27 18:10:13,331 main.py:51] epoch 3282, training loss: 5626.68, average training loss: 6240.02, base loss: 9241.37
[INFO 2017-06-27 18:10:14,539 main.py:51] epoch 3283, training loss: 6389.57, average training loss: 6240.41, base loss: 9243.36
[INFO 2017-06-27 18:10:15,748 main.py:51] epoch 3284, training loss: 5796.12, average training loss: 6240.46, base loss: 9243.89
[INFO 2017-06-27 18:10:16,960 main.py:51] epoch 3285, training loss: 5679.73, average training loss: 6240.04, base loss: 9243.99
[INFO 2017-06-27 18:10:18,174 main.py:51] epoch 3286, training loss: 5972.43, average training loss: 6240.52, base loss: 9245.93
[INFO 2017-06-27 18:10:19,386 main.py:51] epoch 3287, training loss: 5771.88, average training loss: 6239.61, base loss: 9244.64
[INFO 2017-06-27 18:10:20,598 main.py:51] epoch 3288, training loss: 5713.74, average training loss: 6238.99, base loss: 9243.83
[INFO 2017-06-27 18:10:21,806 main.py:51] epoch 3289, training loss: 6129.75, average training loss: 6239.06, base loss: 9244.76
[INFO 2017-06-27 18:10:23,012 main.py:51] epoch 3290, training loss: 6070.26, average training loss: 6239.30, base loss: 9245.90
[INFO 2017-06-27 18:10:24,224 main.py:51] epoch 3291, training loss: 6409.99, average training loss: 6239.57, base loss: 9247.30
[INFO 2017-06-27 18:10:25,432 main.py:51] epoch 3292, training loss: 5959.29, average training loss: 6239.36, base loss: 9247.11
[INFO 2017-06-27 18:10:26,640 main.py:51] epoch 3293, training loss: 5978.33, average training loss: 6239.36, base loss: 9247.47
[INFO 2017-06-27 18:10:27,851 main.py:51] epoch 3294, training loss: 5861.46, average training loss: 6239.08, base loss: 9247.56
[INFO 2017-06-27 18:10:29,058 main.py:51] epoch 3295, training loss: 5975.77, average training loss: 6238.44, base loss: 9246.69
[INFO 2017-06-27 18:10:30,268 main.py:51] epoch 3296, training loss: 5756.35, average training loss: 6238.13, base loss: 9246.58
[INFO 2017-06-27 18:10:31,478 main.py:51] epoch 3297, training loss: 5355.97, average training loss: 6236.59, base loss: 9243.42
[INFO 2017-06-27 18:10:32,691 main.py:51] epoch 3298, training loss: 6188.59, average training loss: 6236.76, base loss: 9243.84
[INFO 2017-06-27 18:10:33,903 main.py:51] epoch 3299, training loss: 6219.24, average training loss: 6236.94, base loss: 9245.13
[INFO 2017-06-27 18:10:33,903 main.py:53] epoch 3299, testing
[INFO 2017-06-27 18:10:38,577 main.py:105] average testing loss: 6369.54, base loss: 9918.80
[INFO 2017-06-27 18:10:38,577 main.py:106] improve_loss: 3549.26, improve_percent: 0.36
[INFO 2017-06-27 18:10:38,578 main.py:72] model save to ./model/final.pth
[INFO 2017-06-27 18:10:38,591 main.py:76] current best improved percent: 0.36
[INFO 2017-06-27 18:10:39,804 main.py:51] epoch 3300, training loss: 6042.51, average training loss: 6236.55, base loss: 9244.66
[INFO 2017-06-27 18:10:41,014 main.py:51] epoch 3301, training loss: 6413.42, average training loss: 6233.64, base loss: 9243.23
[INFO 2017-06-27 18:10:42,227 main.py:51] epoch 3302, training loss: 6719.33, average training loss: 6234.17, base loss: 9245.30
[INFO 2017-06-27 18:10:43,442 main.py:51] epoch 3303, training loss: 5724.23, average training loss: 6234.06, base loss: 9245.94
[INFO 2017-06-27 18:10:44,651 main.py:51] epoch 3304, training loss: 6328.62, average training loss: 6234.61, base loss: 9247.78
[INFO 2017-06-27 18:10:45,867 main.py:51] epoch 3305, training loss: 5501.09, average training loss: 6231.88, base loss: 9243.45
[INFO 2017-06-27 18:10:47,076 main.py:51] epoch 3306, training loss: 5953.12, average training loss: 6231.96, base loss: 9243.96
[INFO 2017-06-27 18:10:48,286 main.py:51] epoch 3307, training loss: 5803.75, average training loss: 6231.02, base loss: 9242.76
[INFO 2017-06-27 18:10:49,499 main.py:51] epoch 3308, training loss: 5846.59, average training loss: 6231.09, base loss: 9243.56
[INFO 2017-06-27 18:10:50,712 main.py:51] epoch 3309, training loss: 5937.81, average training loss: 6231.11, base loss: 9243.39
[INFO 2017-06-27 18:10:51,919 main.py:51] epoch 3310, training loss: 7944.50, average training loss: 6233.07, base loss: 9247.35
[INFO 2017-06-27 18:10:53,126 main.py:51] epoch 3311, training loss: 6038.89, average training loss: 6232.77, base loss: 9247.37
[INFO 2017-06-27 18:10:54,338 main.py:51] epoch 3312, training loss: 5673.41, average training loss: 6232.18, base loss: 9246.33
[INFO 2017-06-27 18:10:55,547 main.py:51] epoch 3313, training loss: 7591.99, average training loss: 6233.98, base loss: 9249.38
[INFO 2017-06-27 18:10:56,761 main.py:51] epoch 3314, training loss: 5712.59, average training loss: 6233.47, base loss: 9248.72
[INFO 2017-06-27 18:10:57,973 main.py:51] epoch 3315, training loss: 5858.20, average training loss: 6233.61, base loss: 9249.52
[INFO 2017-06-27 18:10:59,182 main.py:51] epoch 3316, training loss: 5516.62, average training loss: 6232.97, base loss: 9248.84
[INFO 2017-06-27 18:11:00,393 main.py:51] epoch 3317, training loss: 6391.52, average training loss: 6233.17, base loss: 9250.27
[INFO 2017-06-27 18:11:01,605 main.py:51] epoch 3318, training loss: 5654.57, average training loss: 6232.41, base loss: 9249.72
[INFO 2017-06-27 18:11:02,813 main.py:51] epoch 3319, training loss: 5918.87, average training loss: 6231.98, base loss: 9249.40
[INFO 2017-06-27 18:11:04,024 main.py:51] epoch 3320, training loss: 5900.67, average training loss: 6231.75, base loss: 9250.01
[INFO 2017-06-27 18:11:05,235 main.py:51] epoch 3321, training loss: 5709.03, average training loss: 6230.95, base loss: 9248.92
[INFO 2017-06-27 18:11:06,444 main.py:51] epoch 3322, training loss: 5988.06, average training loss: 6231.25, base loss: 9250.25
[INFO 2017-06-27 18:11:07,657 main.py:51] epoch 3323, training loss: 6090.91, average training loss: 6230.85, base loss: 9249.92
[INFO 2017-06-27 18:11:08,871 main.py:51] epoch 3324, training loss: 5830.11, average training loss: 6230.14, base loss: 9249.13
[INFO 2017-06-27 18:11:10,080 main.py:51] epoch 3325, training loss: 5844.26, average training loss: 6230.39, base loss: 9250.13
[INFO 2017-06-27 18:11:11,289 main.py:51] epoch 3326, training loss: 6666.27, average training loss: 6227.47, base loss: 9247.40
[INFO 2017-06-27 18:11:12,503 main.py:51] epoch 3327, training loss: 6056.66, average training loss: 6227.75, base loss: 9248.63
[INFO 2017-06-27 18:11:13,711 main.py:51] epoch 3328, training loss: 5982.04, average training loss: 6224.88, base loss: 9244.31
[INFO 2017-06-27 18:11:14,924 main.py:51] epoch 3329, training loss: 5438.83, average training loss: 6224.42, base loss: 9244.24
[INFO 2017-06-27 18:11:16,135 main.py:51] epoch 3330, training loss: 6000.76, average training loss: 6224.73, base loss: 9245.26
[INFO 2017-06-27 18:11:17,343 main.py:51] epoch 3331, training loss: 6482.89, average training loss: 6225.27, base loss: 9247.56
[INFO 2017-06-27 18:11:18,553 main.py:51] epoch 3332, training loss: 5671.37, average training loss: 6221.74, base loss: 9242.57
[INFO 2017-06-27 18:11:19,761 main.py:51] epoch 3333, training loss: 5885.99, average training loss: 6221.25, base loss: 9241.99
[INFO 2017-06-27 18:11:20,970 main.py:51] epoch 3334, training loss: 5353.38, average training loss: 6219.82, base loss: 9239.33
[INFO 2017-06-27 18:11:22,178 main.py:51] epoch 3335, training loss: 6034.43, average training loss: 6220.10, base loss: 9240.90
[INFO 2017-06-27 18:11:23,391 main.py:51] epoch 3336, training loss: 5618.59, average training loss: 6219.77, base loss: 9240.33
[INFO 2017-06-27 18:11:24,601 main.py:51] epoch 3337, training loss: 6140.11, average training loss: 6216.55, base loss: 9237.09
[INFO 2017-06-27 18:11:25,807 main.py:51] epoch 3338, training loss: 5846.24, average training loss: 6216.45, base loss: 9237.07
[INFO 2017-06-27 18:11:27,017 main.py:51] epoch 3339, training loss: 5938.57, average training loss: 6216.22, base loss: 9237.24
[INFO 2017-06-27 18:11:28,229 main.py:51] epoch 3340, training loss: 5901.23, average training loss: 6216.26, base loss: 9237.21
[INFO 2017-06-27 18:11:29,436 main.py:51] epoch 3341, training loss: 5764.03, average training loss: 6215.65, base loss: 9236.48
[INFO 2017-06-27 18:11:30,644 main.py:51] epoch 3342, training loss: 5642.05, average training loss: 6214.71, base loss: 9234.81
[INFO 2017-06-27 18:11:31,857 main.py:51] epoch 3343, training loss: 6069.39, average training loss: 6214.57, base loss: 9234.99
[INFO 2017-06-27 18:11:33,064 main.py:51] epoch 3344, training loss: 5936.83, average training loss: 6214.62, base loss: 9235.69
[INFO 2017-06-27 18:11:34,274 main.py:51] epoch 3345, training loss: 6214.60, average training loss: 6214.56, base loss: 9236.12
[INFO 2017-06-27 18:11:35,484 main.py:51] epoch 3346, training loss: 5374.49, average training loss: 6213.76, base loss: 9234.58
[INFO 2017-06-27 18:11:36,692 main.py:51] epoch 3347, training loss: 5604.93, average training loss: 6213.20, base loss: 9233.89
[INFO 2017-06-27 18:11:37,899 main.py:51] epoch 3348, training loss: 8352.96, average training loss: 6215.53, base loss: 9238.88
[INFO 2017-06-27 18:11:39,105 main.py:51] epoch 3349, training loss: 8024.32, average training loss: 6217.05, base loss: 9242.03
[INFO 2017-06-27 18:11:40,314 main.py:51] epoch 3350, training loss: 6189.74, average training loss: 6216.77, base loss: 9241.40
[INFO 2017-06-27 18:11:41,523 main.py:51] epoch 3351, training loss: 5939.08, average training loss: 6216.02, base loss: 9239.95
[INFO 2017-06-27 18:11:42,731 main.py:51] epoch 3352, training loss: 6175.28, average training loss: 6216.22, base loss: 9241.64
[INFO 2017-06-27 18:11:43,944 main.py:51] epoch 3353, training loss: 5682.25, average training loss: 6215.91, base loss: 9241.33
[INFO 2017-06-27 18:11:45,153 main.py:51] epoch 3354, training loss: 6128.34, average training loss: 6215.56, base loss: 9240.66
[INFO 2017-06-27 18:11:46,365 main.py:51] epoch 3355, training loss: 5660.87, average training loss: 6214.54, base loss: 9239.27
[INFO 2017-06-27 18:11:47,572 main.py:51] epoch 3356, training loss: 5757.90, average training loss: 6213.99, base loss: 9238.60
[INFO 2017-06-27 18:11:48,782 main.py:51] epoch 3357, training loss: 5491.88, average training loss: 6210.97, base loss: 9235.85
[INFO 2017-06-27 18:11:49,989 main.py:51] epoch 3358, training loss: 5916.12, average training loss: 6210.28, base loss: 9235.41
[INFO 2017-06-27 18:11:51,196 main.py:51] epoch 3359, training loss: 5584.62, average training loss: 6209.65, base loss: 9234.09
[INFO 2017-06-27 18:11:52,406 main.py:51] epoch 3360, training loss: 6155.71, average training loss: 6206.64, base loss: 9230.01
[INFO 2017-06-27 18:11:53,614 main.py:51] epoch 3361, training loss: 5858.85, average training loss: 6206.17, base loss: 9229.51
[INFO 2017-06-27 18:11:54,823 main.py:51] epoch 3362, training loss: 5713.94, average training loss: 6205.76, base loss: 9229.48
[INFO 2017-06-27 18:11:56,031 main.py:51] epoch 3363, training loss: 5630.63, average training loss: 6202.52, base loss: 9226.01
[INFO 2017-06-27 18:11:57,244 main.py:51] epoch 3364, training loss: 5396.00, average training loss: 6201.86, base loss: 9224.67
[INFO 2017-06-27 18:11:58,451 main.py:51] epoch 3365, training loss: 5754.75, average training loss: 6201.75, base loss: 9224.76
[INFO 2017-06-27 18:11:59,659 main.py:51] epoch 3366, training loss: 5313.60, average training loss: 6200.30, base loss: 9221.90
[INFO 2017-06-27 18:12:00,868 main.py:51] epoch 3367, training loss: 6293.20, average training loss: 6198.40, base loss: 9219.34
[INFO 2017-06-27 18:12:02,075 main.py:51] epoch 3368, training loss: 6025.05, average training loss: 6198.58, base loss: 9220.47
[INFO 2017-06-27 18:12:03,285 main.py:51] epoch 3369, training loss: 5827.85, average training loss: 6198.31, base loss: 9219.78
[INFO 2017-06-27 18:12:04,496 main.py:51] epoch 3370, training loss: 5533.27, average training loss: 6197.65, base loss: 9218.89
[INFO 2017-06-27 18:12:05,705 main.py:51] epoch 3371, training loss: 7908.00, average training loss: 6199.59, base loss: 9222.40
[INFO 2017-06-27 18:12:06,912 main.py:51] epoch 3372, training loss: 6064.09, average training loss: 6199.44, base loss: 9223.43
[INFO 2017-06-27 18:12:08,128 main.py:51] epoch 3373, training loss: 6194.04, average training loss: 6198.81, base loss: 9223.29
[INFO 2017-06-27 18:12:09,342 main.py:51] epoch 3374, training loss: 5888.29, average training loss: 6198.37, base loss: 9222.98
[INFO 2017-06-27 18:12:10,553 main.py:51] epoch 3375, training loss: 5934.79, average training loss: 6198.41, base loss: 9223.62
[INFO 2017-06-27 18:12:11,763 main.py:51] epoch 3376, training loss: 6317.50, average training loss: 6195.71, base loss: 9221.03
[INFO 2017-06-27 18:12:12,970 main.py:51] epoch 3377, training loss: 5891.30, average training loss: 6195.48, base loss: 9221.57
[INFO 2017-06-27 18:12:14,185 main.py:51] epoch 3378, training loss: 6121.14, average training loss: 6195.62, base loss: 9222.23
[INFO 2017-06-27 18:12:15,400 main.py:51] epoch 3379, training loss: 6257.15, average training loss: 6195.73, base loss: 9222.80
[INFO 2017-06-27 18:12:16,615 main.py:51] epoch 3380, training loss: 6285.27, average training loss: 6196.32, base loss: 9224.49
[INFO 2017-06-27 18:12:17,824 main.py:51] epoch 3381, training loss: 5628.67, average training loss: 6195.67, base loss: 9222.76
[INFO 2017-06-27 18:12:19,035 main.py:51] epoch 3382, training loss: 5793.58, average training loss: 6195.35, base loss: 9222.15
[INFO 2017-06-27 18:12:20,245 main.py:51] epoch 3383, training loss: 5636.83, average training loss: 6195.01, base loss: 9221.38
[INFO 2017-06-27 18:12:21,452 main.py:51] epoch 3384, training loss: 5563.24, average training loss: 6194.54, base loss: 9220.78
[INFO 2017-06-27 18:12:22,661 main.py:51] epoch 3385, training loss: 7198.73, average training loss: 6196.00, base loss: 9223.28
[INFO 2017-06-27 18:12:23,870 main.py:51] epoch 3386, training loss: 6360.90, average training loss: 6196.44, base loss: 9224.60
[INFO 2017-06-27 18:12:25,076 main.py:51] epoch 3387, training loss: 5877.62, average training loss: 6196.49, base loss: 9224.80
[INFO 2017-06-27 18:12:26,284 main.py:51] epoch 3388, training loss: 5359.81, average training loss: 6195.60, base loss: 9223.27
[INFO 2017-06-27 18:12:27,489 main.py:51] epoch 3389, training loss: 5794.63, average training loss: 6195.23, base loss: 9222.72
[INFO 2017-06-27 18:12:28,697 main.py:51] epoch 3390, training loss: 5512.82, average training loss: 6194.53, base loss: 9221.44
[INFO 2017-06-27 18:12:29,907 main.py:51] epoch 3391, training loss: 5348.28, average training loss: 6193.83, base loss: 9220.55
[INFO 2017-06-27 18:12:31,116 main.py:51] epoch 3392, training loss: 6138.12, average training loss: 6194.11, base loss: 9220.82
[INFO 2017-06-27 18:12:32,329 main.py:51] epoch 3393, training loss: 5785.91, average training loss: 6191.15, base loss: 9217.13
[INFO 2017-06-27 18:12:33,537 main.py:51] epoch 3394, training loss: 5793.92, average training loss: 6190.87, base loss: 9216.67
[INFO 2017-06-27 18:12:34,749 main.py:51] epoch 3395, training loss: 6353.76, average training loss: 6190.85, base loss: 9216.64
[INFO 2017-06-27 18:12:35,959 main.py:51] epoch 3396, training loss: 5973.18, average training loss: 6190.89, base loss: 9216.90
[INFO 2017-06-27 18:12:37,168 main.py:51] epoch 3397, training loss: 5778.64, average training loss: 6190.66, base loss: 9216.95
[INFO 2017-06-27 18:12:38,375 main.py:51] epoch 3398, training loss: 7980.11, average training loss: 6192.89, base loss: 9221.42
[INFO 2017-06-27 18:12:39,585 main.py:51] epoch 3399, training loss: 5912.96, average training loss: 6193.14, base loss: 9222.18
[INFO 2017-06-27 18:12:39,585 main.py:53] epoch 3399, testing
[INFO 2017-06-27 18:12:44,269 main.py:105] average testing loss: 6148.27, base loss: 9408.39
[INFO 2017-06-27 18:12:44,269 main.py:106] improve_loss: 3260.12, improve_percent: 0.35
[INFO 2017-06-27 18:12:44,270 main.py:76] current best improved percent: 0.36
[INFO 2017-06-27 18:12:45,477 main.py:51] epoch 3400, training loss: 5536.07, average training loss: 6192.66, base loss: 9221.81
[INFO 2017-06-27 18:12:46,693 main.py:51] epoch 3401, training loss: 6246.68, average training loss: 6190.77, base loss: 9219.29
[INFO 2017-06-27 18:12:47,902 main.py:51] epoch 3402, training loss: 5782.19, average training loss: 6190.70, base loss: 9219.49
[INFO 2017-06-27 18:12:49,112 main.py:51] epoch 3403, training loss: 5797.19, average training loss: 6190.84, base loss: 9220.74
[INFO 2017-06-27 18:12:50,322 main.py:51] epoch 3404, training loss: 5651.90, average training loss: 6190.80, base loss: 9221.12
[INFO 2017-06-27 18:12:51,536 main.py:51] epoch 3405, training loss: 6144.30, average training loss: 6191.17, base loss: 9222.29
[INFO 2017-06-27 18:12:52,744 main.py:51] epoch 3406, training loss: 5829.73, average training loss: 6191.14, base loss: 9222.60
[INFO 2017-06-27 18:12:53,958 main.py:51] epoch 3407, training loss: 10305.49, average training loss: 6195.34, base loss: 9231.00
[INFO 2017-06-27 18:12:55,171 main.py:51] epoch 3408, training loss: 5991.05, average training loss: 6195.61, base loss: 9232.16
[INFO 2017-06-27 18:12:56,383 main.py:51] epoch 3409, training loss: 5618.11, average training loss: 6195.36, base loss: 9231.55
[INFO 2017-06-27 18:12:57,595 main.py:51] epoch 3410, training loss: 5365.34, average training loss: 6194.81, base loss: 9230.36
[INFO 2017-06-27 18:12:58,809 main.py:51] epoch 3411, training loss: 5532.19, average training loss: 6194.11, base loss: 9229.42
[INFO 2017-06-27 18:13:00,024 main.py:51] epoch 3412, training loss: 5929.64, average training loss: 6193.89, base loss: 9229.07
[INFO 2017-06-27 18:13:01,233 main.py:51] epoch 3413, training loss: 5642.32, average training loss: 6193.93, base loss: 9229.52
[INFO 2017-06-27 18:13:02,441 main.py:51] epoch 3414, training loss: 7841.39, average training loss: 6195.87, base loss: 9232.99
[INFO 2017-06-27 18:13:03,654 main.py:51] epoch 3415, training loss: 5773.00, average training loss: 6193.53, base loss: 9229.16
[INFO 2017-06-27 18:13:04,861 main.py:51] epoch 3416, training loss: 6423.63, average training loss: 6194.03, base loss: 9230.48
[INFO 2017-06-27 18:13:06,070 main.py:51] epoch 3417, training loss: 6158.22, average training loss: 6194.05, base loss: 9231.02
[INFO 2017-06-27 18:13:07,279 main.py:51] epoch 3418, training loss: 6072.05, average training loss: 6193.79, base loss: 9231.94
[INFO 2017-06-27 18:13:08,490 main.py:51] epoch 3419, training loss: 5585.86, average training loss: 6192.94, base loss: 9230.05
[INFO 2017-06-27 18:13:09,697 main.py:51] epoch 3420, training loss: 6140.75, average training loss: 6192.92, base loss: 9230.55
[INFO 2017-06-27 18:13:10,909 main.py:51] epoch 3421, training loss: 5688.26, average training loss: 6192.97, base loss: 9231.58
[INFO 2017-06-27 18:13:12,123 main.py:51] epoch 3422, training loss: 5937.56, average training loss: 6192.76, base loss: 9232.12
[INFO 2017-06-27 18:13:13,337 main.py:51] epoch 3423, training loss: 6138.47, average training loss: 6190.19, base loss: 9227.33
[INFO 2017-06-27 18:13:14,544 main.py:51] epoch 3424, training loss: 5805.04, average training loss: 6189.98, base loss: 9227.16
[INFO 2017-06-27 18:13:15,753 main.py:51] epoch 3425, training loss: 5534.02, average training loss: 6189.62, base loss: 9227.33
[INFO 2017-06-27 18:13:16,957 main.py:51] epoch 3426, training loss: 5752.20, average training loss: 6189.60, base loss: 9227.69
[INFO 2017-06-27 18:13:18,166 main.py:51] epoch 3427, training loss: 5870.24, average training loss: 6189.23, base loss: 9227.24
[INFO 2017-06-27 18:13:19,379 main.py:51] epoch 3428, training loss: 5688.30, average training loss: 6188.69, base loss: 9226.62
[INFO 2017-06-27 18:13:20,593 main.py:51] epoch 3429, training loss: 8063.49, average training loss: 6190.30, base loss: 9229.03
[INFO 2017-06-27 18:13:21,806 main.py:51] epoch 3430, training loss: 5550.55, average training loss: 6189.45, base loss: 9227.70
[INFO 2017-06-27 18:13:23,012 main.py:51] epoch 3431, training loss: 5837.94, average training loss: 6189.00, base loss: 9227.56
[INFO 2017-06-27 18:13:24,221 main.py:51] epoch 3432, training loss: 6477.13, average training loss: 6189.05, base loss: 9228.17
[INFO 2017-06-27 18:13:25,435 main.py:51] epoch 3433, training loss: 5652.24, average training loss: 6187.96, base loss: 9226.92
[INFO 2017-06-27 18:13:26,646 main.py:51] epoch 3434, training loss: 6075.63, average training loss: 6187.85, base loss: 9227.13
[INFO 2017-06-27 18:13:27,859 main.py:51] epoch 3435, training loss: 6162.22, average training loss: 6187.91, base loss: 9227.80
[INFO 2017-06-27 18:13:29,099 main.py:51] epoch 3436, training loss: 7930.00, average training loss: 6189.44, base loss: 9231.15
[INFO 2017-06-27 18:13:30,379 main.py:51] epoch 3437, training loss: 6143.24, average training loss: 6189.31, base loss: 9230.83
[INFO 2017-06-27 18:13:31,797 main.py:51] epoch 3438, training loss: 8315.88, average training loss: 6192.12, base loss: 9235.99
[INFO 2017-06-27 18:13:33,151 main.py:51] epoch 3439, training loss: 6078.75, average training loss: 6192.44, base loss: 9237.42
[INFO 2017-06-27 18:13:34,423 main.py:51] epoch 3440, training loss: 5871.82, average training loss: 6192.05, base loss: 9236.93
[INFO 2017-06-27 18:13:35,628 main.py:51] epoch 3441, training loss: 5769.56, average training loss: 6189.01, base loss: 9233.67
[INFO 2017-06-27 18:13:36,840 main.py:51] epoch 3442, training loss: 5672.13, average training loss: 6185.48, base loss: 9228.49
[INFO 2017-06-27 18:13:38,052 main.py:51] epoch 3443, training loss: 6003.44, average training loss: 6185.79, base loss: 9230.21
[INFO 2017-06-27 18:13:39,261 main.py:51] epoch 3444, training loss: 5566.54, average training loss: 6185.10, base loss: 9229.43
[INFO 2017-06-27 18:13:40,473 main.py:51] epoch 3445, training loss: 5587.15, average training loss: 6184.55, base loss: 9228.79
[INFO 2017-06-27 18:13:41,683 main.py:51] epoch 3446, training loss: 5745.44, average training loss: 6184.09, base loss: 9228.68
[INFO 2017-06-27 18:13:42,894 main.py:51] epoch 3447, training loss: 5701.94, average training loss: 6183.57, base loss: 9227.72
[INFO 2017-06-27 18:13:44,101 main.py:51] epoch 3448, training loss: 5679.49, average training loss: 6182.93, base loss: 9226.88
[INFO 2017-06-27 18:13:45,310 main.py:51] epoch 3449, training loss: 6050.85, average training loss: 6182.58, base loss: 9226.64
[INFO 2017-06-27 18:13:46,522 main.py:51] epoch 3450, training loss: 5973.36, average training loss: 6182.86, base loss: 9227.13
[INFO 2017-06-27 18:13:47,734 main.py:51] epoch 3451, training loss: 5947.03, average training loss: 6182.74, base loss: 9226.86
[INFO 2017-06-27 18:13:48,945 main.py:51] epoch 3452, training loss: 6167.31, average training loss: 6183.17, base loss: 9228.90
[INFO 2017-06-27 18:13:50,156 main.py:51] epoch 3453, training loss: 5539.12, average training loss: 6182.37, base loss: 9227.51
[INFO 2017-06-27 18:13:51,365 main.py:51] epoch 3454, training loss: 5775.25, average training loss: 6182.34, base loss: 9228.04
[INFO 2017-06-27 18:13:52,575 main.py:51] epoch 3455, training loss: 6026.42, average training loss: 6182.35, base loss: 9229.01
[INFO 2017-06-27 18:13:53,907 main.py:51] epoch 3456, training loss: 5995.08, average training loss: 6182.20, base loss: 9228.82
[INFO 2017-06-27 18:13:55,149 main.py:51] epoch 3457, training loss: 6119.45, average training loss: 6182.22, base loss: 9229.39
[INFO 2017-06-27 18:13:56,433 main.py:51] epoch 3458, training loss: 6052.56, average training loss: 6182.06, base loss: 9229.80
[INFO 2017-06-27 18:13:57,732 main.py:51] epoch 3459, training loss: 6054.83, average training loss: 6181.81, base loss: 9230.15
[INFO 2017-06-27 18:13:58,940 main.py:51] epoch 3460, training loss: 6218.72, average training loss: 6181.77, base loss: 9230.78
[INFO 2017-06-27 18:14:00,248 main.py:51] epoch 3461, training loss: 5609.22, average training loss: 6180.88, base loss: 9228.83
[INFO 2017-06-27 18:14:01,564 main.py:51] epoch 3462, training loss: 5779.00, average training loss: 6178.50, base loss: 9225.49
[INFO 2017-06-27 18:14:02,871 main.py:51] epoch 3463, training loss: 6155.06, average training loss: 6178.46, base loss: 9225.90
[INFO 2017-06-27 18:14:04,163 main.py:51] epoch 3464, training loss: 6048.23, average training loss: 6178.51, base loss: 9226.30
[INFO 2017-06-27 18:14:05,488 main.py:51] epoch 3465, training loss: 6170.22, average training loss: 6178.68, base loss: 9226.93
[INFO 2017-06-27 18:14:06,844 main.py:51] epoch 3466, training loss: 5671.63, average training loss: 6177.80, base loss: 9225.50
[INFO 2017-06-27 18:14:08,153 main.py:51] epoch 3467, training loss: 5744.15, average training loss: 6177.62, base loss: 9225.62
[INFO 2017-06-27 18:14:09,450 main.py:51] epoch 3468, training loss: 5404.48, average training loss: 6176.64, base loss: 9223.99
[INFO 2017-06-27 18:14:10,661 main.py:51] epoch 3469, training loss: 5474.21, average training loss: 6175.69, base loss: 9222.43
[INFO 2017-06-27 18:14:11,963 main.py:51] epoch 3470, training loss: 6034.25, average training loss: 6172.56, base loss: 9218.62
[INFO 2017-06-27 18:14:13,178 main.py:51] epoch 3471, training loss: 5932.30, average training loss: 6172.31, base loss: 9218.72
[INFO 2017-06-27 18:14:14,494 main.py:51] epoch 3472, training loss: 5823.27, average training loss: 6171.74, base loss: 9218.11
[INFO 2017-06-27 18:14:15,775 main.py:51] epoch 3473, training loss: 5598.94, average training loss: 6171.35, base loss: 9218.12
[INFO 2017-06-27 18:14:16,988 main.py:51] epoch 3474, training loss: 5819.54, average training loss: 6170.82, base loss: 9217.04
[INFO 2017-06-27 18:14:18,196 main.py:51] epoch 3475, training loss: 6174.26, average training loss: 6170.61, base loss: 9216.58
[INFO 2017-06-27 18:14:19,405 main.py:51] epoch 3476, training loss: 6189.71, average training loss: 6170.21, base loss: 9216.19
[INFO 2017-06-27 18:14:20,612 main.py:51] epoch 3477, training loss: 5966.25, average training loss: 6170.34, base loss: 9217.47
[INFO 2017-06-27 18:14:21,819 main.py:51] epoch 3478, training loss: 5574.94, average training loss: 6169.86, base loss: 9216.44
[INFO 2017-06-27 18:14:23,029 main.py:51] epoch 3479, training loss: 5912.35, average training loss: 6169.41, base loss: 9216.05
[INFO 2017-06-27 18:14:24,239 main.py:51] epoch 3480, training loss: 5909.53, average training loss: 6169.49, base loss: 9217.09
[INFO 2017-06-27 18:14:25,451 main.py:51] epoch 3481, training loss: 5962.14, average training loss: 6169.30, base loss: 9217.28
[INFO 2017-06-27 18:14:26,738 main.py:51] epoch 3482, training loss: 6248.79, average training loss: 6169.16, base loss: 9218.38
[INFO 2017-06-27 18:14:28,055 main.py:51] epoch 3483, training loss: 6126.46, average training loss: 6169.27, base loss: 9218.82
[INFO 2017-06-27 18:14:29,326 main.py:51] epoch 3484, training loss: 5639.84, average training loss: 6168.24, base loss: 9217.38
[INFO 2017-06-27 18:14:30,545 main.py:51] epoch 3485, training loss: 6088.31, average training loss: 6168.38, base loss: 9218.35
[INFO 2017-06-27 18:14:31,755 main.py:51] epoch 3486, training loss: 6053.59, average training loss: 6168.21, base loss: 9218.68
[INFO 2017-06-27 18:14:32,964 main.py:51] epoch 3487, training loss: 5849.89, average training loss: 6167.80, base loss: 9218.66
[INFO 2017-06-27 18:14:34,175 main.py:51] epoch 3488, training loss: 7474.43, average training loss: 6168.94, base loss: 9220.52
[INFO 2017-06-27 18:14:35,389 main.py:51] epoch 3489, training loss: 5886.96, average training loss: 6166.14, base loss: 9216.27
[INFO 2017-06-27 18:14:36,597 main.py:51] epoch 3490, training loss: 5761.03, average training loss: 6165.60, base loss: 9215.59
[INFO 2017-06-27 18:14:37,811 main.py:51] epoch 3491, training loss: 5574.47, average training loss: 6164.95, base loss: 9214.88
[INFO 2017-06-27 18:14:39,025 main.py:51] epoch 3492, training loss: 6042.81, average training loss: 6164.99, base loss: 9215.35
[INFO 2017-06-27 18:14:40,235 main.py:51] epoch 3493, training loss: 5618.75, average training loss: 6164.13, base loss: 9214.26
[INFO 2017-06-27 18:14:41,446 main.py:51] epoch 3494, training loss: 6015.48, average training loss: 6164.11, base loss: 9215.30
[INFO 2017-06-27 18:14:42,658 main.py:51] epoch 3495, training loss: 5721.13, average training loss: 6163.35, base loss: 9214.53
[INFO 2017-06-27 18:14:43,867 main.py:51] epoch 3496, training loss: 5987.10, average training loss: 6163.09, base loss: 9214.70
[INFO 2017-06-27 18:14:45,075 main.py:51] epoch 3497, training loss: 5986.42, average training loss: 6159.61, base loss: 9211.19
[INFO 2017-06-27 18:14:46,283 main.py:51] epoch 3498, training loss: 5672.93, average training loss: 6158.87, base loss: 9210.11
[INFO 2017-06-27 18:14:47,497 main.py:51] epoch 3499, training loss: 6093.38, average training loss: 6158.69, base loss: 9210.10
[INFO 2017-06-27 18:14:47,497 main.py:53] epoch 3499, testing
[INFO 2017-06-27 18:14:52,196 main.py:105] average testing loss: 6098.38, base loss: 9425.92
[INFO 2017-06-27 18:14:52,196 main.py:106] improve_loss: 3327.54, improve_percent: 0.35
[INFO 2017-06-27 18:14:52,198 main.py:76] current best improved percent: 0.36
[INFO 2017-06-27 18:14:53,453 main.py:51] epoch 3500, training loss: 5938.49, average training loss: 6158.76, base loss: 9210.71
[INFO 2017-06-27 18:14:54,709 main.py:51] epoch 3501, training loss: 5626.95, average training loss: 6155.22, base loss: 9205.97
[INFO 2017-06-27 18:14:56,023 main.py:51] epoch 3502, training loss: 5750.30, average training loss: 6154.44, base loss: 9204.50
[INFO 2017-06-27 18:14:57,228 main.py:51] epoch 3503, training loss: 5704.45, average training loss: 6153.64, base loss: 9203.45
[INFO 2017-06-27 18:14:58,451 main.py:51] epoch 3504, training loss: 5402.93, average training loss: 6153.28, base loss: 9203.44
[INFO 2017-06-27 18:14:59,659 main.py:51] epoch 3505, training loss: 6091.74, average training loss: 6153.01, base loss: 9203.73
[INFO 2017-06-27 18:15:00,868 main.py:51] epoch 3506, training loss: 5871.38, average training loss: 6152.78, base loss: 9203.58
[INFO 2017-06-27 18:15:02,077 main.py:51] epoch 3507, training loss: 6077.15, average training loss: 6152.84, base loss: 9203.70
[INFO 2017-06-27 18:15:03,287 main.py:51] epoch 3508, training loss: 5813.17, average training loss: 6152.66, base loss: 9203.52
[INFO 2017-06-27 18:15:04,493 main.py:51] epoch 3509, training loss: 5998.74, average training loss: 6152.53, base loss: 9203.71
[INFO 2017-06-27 18:15:05,700 main.py:51] epoch 3510, training loss: 5558.93, average training loss: 6151.69, base loss: 9202.34
[INFO 2017-06-27 18:15:06,967 main.py:51] epoch 3511, training loss: 6054.83, average training loss: 6148.82, base loss: 9198.88
[INFO 2017-06-27 18:15:08,282 main.py:51] epoch 3512, training loss: 5787.08, average training loss: 6148.64, base loss: 9198.83
[INFO 2017-06-27 18:15:09,516 main.py:51] epoch 3513, training loss: 5967.47, average training loss: 6146.20, base loss: 9195.47
[INFO 2017-06-27 18:15:10,731 main.py:51] epoch 3514, training loss: 5896.55, average training loss: 6146.02, base loss: 9195.20
[INFO 2017-06-27 18:15:11,936 main.py:51] epoch 3515, training loss: 6238.87, average training loss: 6145.72, base loss: 9195.52
[INFO 2017-06-27 18:15:13,254 main.py:51] epoch 3516, training loss: 6229.87, average training loss: 6146.17, base loss: 9197.36
[INFO 2017-06-27 18:15:14,579 main.py:51] epoch 3517, training loss: 7498.13, average training loss: 6147.47, base loss: 9200.04
[INFO 2017-06-27 18:15:15,863 main.py:51] epoch 3518, training loss: 5890.04, average training loss: 6147.29, base loss: 9200.42
[INFO 2017-06-27 18:15:17,079 main.py:51] epoch 3519, training loss: 6064.32, average training loss: 6147.70, base loss: 9201.79
[INFO 2017-06-27 18:15:18,290 main.py:51] epoch 3520, training loss: 5643.70, average training loss: 6147.33, base loss: 9201.54
[INFO 2017-06-27 18:15:19,500 main.py:51] epoch 3521, training loss: 5500.11, average training loss: 6146.90, base loss: 9200.86
[INFO 2017-06-27 18:15:20,712 main.py:51] epoch 3522, training loss: 7754.87, average training loss: 6148.54, base loss: 9203.97
[INFO 2017-06-27 18:15:21,920 main.py:51] epoch 3523, training loss: 5755.90, average training loss: 6148.36, base loss: 9203.92
[INFO 2017-06-27 18:15:23,128 main.py:51] epoch 3524, training loss: 5784.28, average training loss: 6147.98, base loss: 9204.17
[INFO 2017-06-27 18:15:24,334 main.py:51] epoch 3525, training loss: 6049.28, average training loss: 6147.95, base loss: 9205.03
[INFO 2017-06-27 18:15:25,545 main.py:51] epoch 3526, training loss: 6230.52, average training loss: 6148.14, base loss: 9205.84
[INFO 2017-06-27 18:15:26,751 main.py:51] epoch 3527, training loss: 6051.70, average training loss: 6148.53, base loss: 9207.40
[INFO 2017-06-27 18:15:27,963 main.py:51] epoch 3528, training loss: 6111.03, average training loss: 6148.18, base loss: 9207.13
[INFO 2017-06-27 18:15:29,170 main.py:51] epoch 3529, training loss: 5547.20, average training loss: 6147.68, base loss: 9206.68
[INFO 2017-06-27 18:15:30,374 main.py:51] epoch 3530, training loss: 5690.90, average training loss: 6147.56, base loss: 9207.26
[INFO 2017-06-27 18:15:31,584 main.py:51] epoch 3531, training loss: 5713.49, average training loss: 6147.11, base loss: 9206.29
[INFO 2017-06-27 18:15:32,788 main.py:51] epoch 3532, training loss: 6055.75, average training loss: 6146.98, base loss: 9206.30
[INFO 2017-06-27 18:15:33,995 main.py:51] epoch 3533, training loss: 5781.61, average training loss: 6146.57, base loss: 9206.15
[INFO 2017-06-27 18:15:35,262 main.py:51] epoch 3534, training loss: 6129.64, average training loss: 6146.18, base loss: 9205.32
[INFO 2017-06-27 18:15:36,563 main.py:51] epoch 3535, training loss: 5814.59, average training loss: 6146.43, base loss: 9206.31
[INFO 2017-06-27 18:15:37,884 main.py:51] epoch 3536, training loss: 5495.03, average training loss: 6145.60, base loss: 9204.57
[INFO 2017-06-27 18:15:39,168 main.py:51] epoch 3537, training loss: 6178.15, average training loss: 6145.61, base loss: 9204.84
[INFO 2017-06-27 18:15:40,424 main.py:51] epoch 3538, training loss: 5915.11, average training loss: 6145.94, base loss: 9205.92
[INFO 2017-06-27 18:15:41,648 main.py:51] epoch 3539, training loss: 5750.65, average training loss: 6145.66, base loss: 9205.69
[INFO 2017-06-27 18:15:42,861 main.py:51] epoch 3540, training loss: 5697.13, average training loss: 6143.03, base loss: 9201.19
[INFO 2017-06-27 18:15:44,069 main.py:51] epoch 3541, training loss: 8431.70, average training loss: 6145.22, base loss: 9205.24
[INFO 2017-06-27 18:15:45,284 main.py:51] epoch 3542, training loss: 5727.71, average training loss: 6142.15, base loss: 9201.22
[INFO 2017-06-27 18:15:46,525 main.py:51] epoch 3543, training loss: 6202.11, average training loss: 6142.49, base loss: 9202.05
[INFO 2017-06-27 18:15:47,738 main.py:51] epoch 3544, training loss: 5740.86, average training loss: 6142.37, base loss: 9201.80
[INFO 2017-06-27 18:15:48,944 main.py:51] epoch 3545, training loss: 6022.19, average training loss: 6142.72, base loss: 9202.51
[INFO 2017-06-27 18:15:50,156 main.py:51] epoch 3546, training loss: 6266.02, average training loss: 6143.27, base loss: 9204.59
[INFO 2017-06-27 18:15:51,367 main.py:51] epoch 3547, training loss: 5842.80, average training loss: 6143.03, base loss: 9204.33
[INFO 2017-06-27 18:15:52,577 main.py:51] epoch 3548, training loss: 6448.46, average training loss: 6141.03, base loss: 9203.00
[INFO 2017-06-27 18:15:53,792 main.py:51] epoch 3549, training loss: 7509.98, average training loss: 6142.37, base loss: 9205.91
[INFO 2017-06-27 18:15:55,002 main.py:51] epoch 3550, training loss: 5661.78, average training loss: 6141.53, base loss: 9204.24
[INFO 2017-06-27 18:15:56,218 main.py:51] epoch 3551, training loss: 6048.31, average training loss: 6141.14, base loss: 9203.28
[INFO 2017-06-27 18:15:57,433 main.py:51] epoch 3552, training loss: 5610.57, average training loss: 6140.53, base loss: 9202.66
[INFO 2017-06-27 18:15:58,643 main.py:51] epoch 3553, training loss: 5892.30, average training loss: 6140.34, base loss: 9202.41
[INFO 2017-06-27 18:15:59,854 main.py:51] epoch 3554, training loss: 5820.45, average training loss: 6139.67, base loss: 9200.95
[INFO 2017-06-27 18:16:01,133 main.py:51] epoch 3555, training loss: 5885.34, average training loss: 6139.90, base loss: 9202.25
[INFO 2017-06-27 18:16:02,470 main.py:51] epoch 3556, training loss: 5969.19, average training loss: 6139.58, base loss: 9201.36
[INFO 2017-06-27 18:16:03,762 main.py:51] epoch 3557, training loss: 6066.44, average training loss: 6137.15, base loss: 9197.29
[INFO 2017-06-27 18:16:04,971 main.py:51] epoch 3558, training loss: 5638.93, average training loss: 6134.25, base loss: 9192.13
[INFO 2017-06-27 18:16:06,183 main.py:51] epoch 3559, training loss: 5788.27, average training loss: 6134.01, base loss: 9192.39
[INFO 2017-06-27 18:16:07,487 main.py:51] epoch 3560, training loss: 5877.76, average training loss: 6133.91, base loss: 9192.69
[INFO 2017-06-27 18:16:08,780 main.py:51] epoch 3561, training loss: 5129.46, average training loss: 6132.88, base loss: 9190.39
[INFO 2017-06-27 18:16:09,992 main.py:51] epoch 3562, training loss: 5881.67, average training loss: 6133.07, base loss: 9191.09
[INFO 2017-06-27 18:16:11,218 main.py:51] epoch 3563, training loss: 5612.58, average training loss: 6132.10, base loss: 9189.78
[INFO 2017-06-27 18:16:12,454 main.py:51] epoch 3564, training loss: 5621.89, average training loss: 6131.65, base loss: 9189.29
[INFO 2017-06-27 18:16:13,686 main.py:51] epoch 3565, training loss: 6015.26, average training loss: 6131.38, base loss: 9189.91
[INFO 2017-06-27 18:16:14,945 main.py:51] epoch 3566, training loss: 5862.58, average training loss: 6130.89, base loss: 9189.79
[INFO 2017-06-27 18:16:16,269 main.py:51] epoch 3567, training loss: 5585.45, average training loss: 6130.55, base loss: 9189.55
[INFO 2017-06-27 18:16:17,610 main.py:51] epoch 3568, training loss: 5525.60, average training loss: 6129.75, base loss: 9188.12
[INFO 2017-06-27 18:16:18,969 main.py:51] epoch 3569, training loss: 6187.18, average training loss: 6129.93, base loss: 9189.24
[INFO 2017-06-27 18:16:20,188 main.py:51] epoch 3570, training loss: 5980.57, average training loss: 6130.04, base loss: 9189.62
[INFO 2017-06-27 18:16:21,470 main.py:51] epoch 3571, training loss: 5907.63, average training loss: 6129.83, base loss: 9189.30
[INFO 2017-06-27 18:16:22,807 main.py:51] epoch 3572, training loss: 6410.03, average training loss: 6130.48, base loss: 9191.01
[INFO 2017-06-27 18:16:24,132 main.py:51] epoch 3573, training loss: 7920.89, average training loss: 6132.12, base loss: 9194.46
[INFO 2017-06-27 18:16:25,418 main.py:51] epoch 3574, training loss: 6168.41, average training loss: 6131.68, base loss: 9194.16
[INFO 2017-06-27 18:16:26,770 main.py:51] epoch 3575, training loss: 5654.75, average training loss: 6131.33, base loss: 9193.44
[INFO 2017-06-27 18:16:28,109 main.py:51] epoch 3576, training loss: 6379.59, average training loss: 6131.60, base loss: 9194.19
[INFO 2017-06-27 18:16:29,429 main.py:51] epoch 3577, training loss: 6262.29, average training loss: 6132.09, base loss: 9195.65
[INFO 2017-06-27 18:16:30,646 main.py:51] epoch 3578, training loss: 5970.63, average training loss: 6131.86, base loss: 9196.20
[INFO 2017-06-27 18:16:31,855 main.py:51] epoch 3579, training loss: 5905.18, average training loss: 6131.62, base loss: 9196.14
[INFO 2017-06-27 18:16:33,069 main.py:51] epoch 3580, training loss: 5826.46, average training loss: 6131.67, base loss: 9196.59
[INFO 2017-06-27 18:16:34,279 main.py:51] epoch 3581, training loss: 5595.73, average training loss: 6131.07, base loss: 9195.56
[INFO 2017-06-27 18:16:35,493 main.py:51] epoch 3582, training loss: 5978.02, average training loss: 6130.84, base loss: 9195.35
[INFO 2017-06-27 18:16:36,702 main.py:51] epoch 3583, training loss: 6008.82, average training loss: 6130.68, base loss: 9194.84
[INFO 2017-06-27 18:16:37,956 main.py:51] epoch 3584, training loss: 5846.90, average training loss: 6127.56, base loss: 9190.86
[INFO 2017-06-27 18:16:39,210 main.py:51] epoch 3585, training loss: 6055.55, average training loss: 6127.62, base loss: 9191.85
[INFO 2017-06-27 18:16:40,456 main.py:51] epoch 3586, training loss: 5303.80, average training loss: 6127.09, base loss: 9190.92
[INFO 2017-06-27 18:16:41,688 main.py:51] epoch 3587, training loss: 5730.19, average training loss: 6126.87, base loss: 9190.51
[INFO 2017-06-27 18:16:42,890 main.py:51] epoch 3588, training loss: 6463.94, average training loss: 6127.52, base loss: 9192.86
[INFO 2017-06-27 18:16:44,094 main.py:51] epoch 3589, training loss: 5369.53, average training loss: 6126.96, base loss: 9191.93
[INFO 2017-06-27 18:16:45,296 main.py:51] epoch 3590, training loss: 6019.37, average training loss: 6126.71, base loss: 9191.13
[INFO 2017-06-27 18:16:46,505 main.py:51] epoch 3591, training loss: 5945.87, average training loss: 6126.24, base loss: 9190.31
[INFO 2017-06-27 18:16:47,710 main.py:51] epoch 3592, training loss: 6244.40, average training loss: 6126.25, base loss: 9190.65
[INFO 2017-06-27 18:16:48,918 main.py:51] epoch 3593, training loss: 5938.86, average training loss: 6125.93, base loss: 9190.57
[INFO 2017-06-27 18:16:50,124 main.py:51] epoch 3594, training loss: 5846.50, average training loss: 6125.95, base loss: 9190.90
[INFO 2017-06-27 18:16:51,414 main.py:51] epoch 3595, training loss: 6196.52, average training loss: 6126.21, base loss: 9191.75
[INFO 2017-06-27 18:16:52,623 main.py:51] epoch 3596, training loss: 5727.11, average training loss: 6126.13, base loss: 9191.25
[INFO 2017-06-27 18:16:53,941 main.py:51] epoch 3597, training loss: 5792.71, average training loss: 6123.57, base loss: 9187.85
[INFO 2017-06-27 18:16:55,264 main.py:51] epoch 3598, training loss: 8289.30, average training loss: 6125.96, base loss: 9192.18
[INFO 2017-06-27 18:16:56,604 main.py:51] epoch 3599, training loss: 5983.18, average training loss: 6125.59, base loss: 9191.63
[INFO 2017-06-27 18:16:56,605 main.py:53] epoch 3599, testing
[INFO 2017-06-27 18:17:01,631 main.py:105] average testing loss: 5871.53, base loss: 9058.08
[INFO 2017-06-27 18:17:01,631 main.py:106] improve_loss: 3186.55, improve_percent: 0.35
[INFO 2017-06-27 18:17:01,632 main.py:76] current best improved percent: 0.36
[INFO 2017-06-27 18:17:02,876 main.py:51] epoch 3600, training loss: 6137.26, average training loss: 6125.64, base loss: 9192.26
[INFO 2017-06-27 18:17:04,213 main.py:51] epoch 3601, training loss: 6008.37, average training loss: 6125.42, base loss: 9192.25
[INFO 2017-06-27 18:17:05,579 main.py:51] epoch 3602, training loss: 5461.49, average training loss: 6124.76, base loss: 9191.39
[INFO 2017-06-27 18:17:06,821 main.py:51] epoch 3603, training loss: 6697.57, average training loss: 6125.19, base loss: 9192.71
[INFO 2017-06-27 18:17:08,097 main.py:51] epoch 3604, training loss: 5766.52, average training loss: 6125.16, base loss: 9192.36
[INFO 2017-06-27 18:17:09,305 main.py:51] epoch 3605, training loss: 6075.28, average training loss: 6125.29, base loss: 9193.04
[INFO 2017-06-27 18:17:10,515 main.py:51] epoch 3606, training loss: 5743.97, average training loss: 6124.60, base loss: 9192.08
[INFO 2017-06-27 18:17:11,720 main.py:51] epoch 3607, training loss: 6369.24, average training loss: 6125.38, base loss: 9194.66
[INFO 2017-06-27 18:17:12,927 main.py:51] epoch 3608, training loss: 5837.57, average training loss: 6125.19, base loss: 9194.67
[INFO 2017-06-27 18:17:14,132 main.py:51] epoch 3609, training loss: 5685.27, average training loss: 6124.99, base loss: 9194.74
[INFO 2017-06-27 18:17:15,337 main.py:51] epoch 3610, training loss: 5981.07, average training loss: 6124.97, base loss: 9195.29
[INFO 2017-06-27 18:17:16,541 main.py:51] epoch 3611, training loss: 5663.82, average training loss: 6124.74, base loss: 9195.13
[INFO 2017-06-27 18:17:17,746 main.py:51] epoch 3612, training loss: 6154.95, average training loss: 6124.84, base loss: 9195.85
[INFO 2017-06-27 18:17:18,952 main.py:51] epoch 3613, training loss: 5730.16, average training loss: 6124.78, base loss: 9195.82
[INFO 2017-06-27 18:17:20,159 main.py:51] epoch 3614, training loss: 7349.21, average training loss: 6126.13, base loss: 9197.90
[INFO 2017-06-27 18:17:21,369 main.py:51] epoch 3615, training loss: 5679.36, average training loss: 6125.62, base loss: 9197.33
[INFO 2017-06-27 18:17:22,575 main.py:51] epoch 3616, training loss: 5475.49, average training loss: 6125.44, base loss: 9197.25
[INFO 2017-06-27 18:17:23,787 main.py:51] epoch 3617, training loss: 5882.90, average training loss: 6124.85, base loss: 9196.00
[INFO 2017-06-27 18:17:25,056 main.py:51] epoch 3618, training loss: 6600.02, average training loss: 6125.26, base loss: 9196.97
[INFO 2017-06-27 18:17:26,296 main.py:51] epoch 3619, training loss: 5933.63, average training loss: 6125.21, base loss: 9197.69
[INFO 2017-06-27 18:17:27,572 main.py:51] epoch 3620, training loss: 6205.05, average training loss: 6124.84, base loss: 9197.19
[INFO 2017-06-27 18:17:28,866 main.py:51] epoch 3621, training loss: 6031.64, average training loss: 6125.35, base loss: 9198.92
[INFO 2017-06-27 18:17:30,177 main.py:51] epoch 3622, training loss: 5680.97, average training loss: 6125.12, base loss: 9198.93
[INFO 2017-06-27 18:17:31,487 main.py:51] epoch 3623, training loss: 5656.23, average training loss: 6124.89, base loss: 9198.38
[INFO 2017-06-27 18:17:32,779 main.py:51] epoch 3624, training loss: 8427.54, average training loss: 6127.31, base loss: 9202.42
[INFO 2017-06-27 18:17:34,111 main.py:51] epoch 3625, training loss: 5578.20, average training loss: 6126.88, base loss: 9201.65
[INFO 2017-06-27 18:17:35,418 main.py:51] epoch 3626, training loss: 6011.88, average training loss: 6126.67, base loss: 9201.79
[INFO 2017-06-27 18:17:36,716 main.py:51] epoch 3627, training loss: 6080.49, average training loss: 6126.78, base loss: 9202.32
[INFO 2017-06-27 18:17:38,012 main.py:51] epoch 3628, training loss: 5909.25, average training loss: 6126.40, base loss: 9201.59
[INFO 2017-06-27 18:17:39,302 main.py:51] epoch 3629, training loss: 6123.36, average training loss: 6127.13, base loss: 9203.48
[INFO 2017-06-27 18:17:40,651 main.py:51] epoch 3630, training loss: 5422.49, average training loss: 6126.53, base loss: 9203.05
[INFO 2017-06-27 18:17:41,972 main.py:51] epoch 3631, training loss: 5631.15, average training loss: 6126.01, base loss: 9201.96
[INFO 2017-06-27 18:17:43,314 main.py:51] epoch 3632, training loss: 5651.15, average training loss: 6125.97, base loss: 9202.44
[INFO 2017-06-27 18:17:44,613 main.py:51] epoch 3633, training loss: 6194.23, average training loss: 6125.88, base loss: 9202.40
[INFO 2017-06-27 18:17:45,824 main.py:51] epoch 3634, training loss: 6029.08, average training loss: 6125.73, base loss: 9202.48
[INFO 2017-06-27 18:17:47,049 main.py:51] epoch 3635, training loss: 6248.26, average training loss: 6126.44, base loss: 9204.74
[INFO 2017-06-27 18:17:48,255 main.py:51] epoch 3636, training loss: 5402.49, average training loss: 6123.05, base loss: 9199.70
[INFO 2017-06-27 18:17:49,462 main.py:51] epoch 3637, training loss: 5875.31, average training loss: 6123.04, base loss: 9199.87
[INFO 2017-06-27 18:17:50,669 main.py:51] epoch 3638, training loss: 5905.01, average training loss: 6123.28, base loss: 9200.79
[INFO 2017-06-27 18:17:51,877 main.py:51] epoch 3639, training loss: 5892.63, average training loss: 6123.35, base loss: 9201.28
[INFO 2017-06-27 18:17:53,124 main.py:51] epoch 3640, training loss: 5736.01, average training loss: 6123.21, base loss: 9201.04
[INFO 2017-06-27 18:17:54,458 main.py:51] epoch 3641, training loss: 7902.71, average training loss: 6124.74, base loss: 9203.94
[INFO 2017-06-27 18:17:55,786 main.py:51] epoch 3642, training loss: 6100.70, average training loss: 6125.16, base loss: 9205.13
[INFO 2017-06-27 18:17:57,121 main.py:51] epoch 3643, training loss: 5981.80, average training loss: 6124.99, base loss: 9205.29
[INFO 2017-06-27 18:17:58,450 main.py:51] epoch 3644, training loss: 6678.60, average training loss: 6126.02, base loss: 9208.27
[INFO 2017-06-27 18:17:59,748 main.py:51] epoch 3645, training loss: 5855.78, average training loss: 6126.05, base loss: 9209.15
[INFO 2017-06-27 18:18:01,048 main.py:51] epoch 3646, training loss: 6262.26, average training loss: 6126.87, base loss: 9211.01
[INFO 2017-06-27 18:18:02,299 main.py:51] epoch 3647, training loss: 6000.98, average training loss: 6123.71, base loss: 9207.79
[INFO 2017-06-27 18:18:03,578 main.py:51] epoch 3648, training loss: 6158.70, average training loss: 6123.43, base loss: 9207.08
[INFO 2017-06-27 18:18:04,900 main.py:51] epoch 3649, training loss: 6043.83, average training loss: 6122.85, base loss: 9206.05
[INFO 2017-06-27 18:18:06,219 main.py:51] epoch 3650, training loss: 5504.68, average training loss: 6122.38, base loss: 9205.80
[INFO 2017-06-27 18:18:07,521 main.py:51] epoch 3651, training loss: 5850.18, average training loss: 6122.74, base loss: 9207.21
[INFO 2017-06-27 18:18:08,824 main.py:51] epoch 3652, training loss: 5361.44, average training loss: 6121.95, base loss: 9206.10
[INFO 2017-06-27 18:18:10,128 main.py:51] epoch 3653, training loss: 5882.55, average training loss: 6121.70, base loss: 9205.99
[INFO 2017-06-27 18:18:11,468 main.py:51] epoch 3654, training loss: 5862.08, average training loss: 6121.55, base loss: 9205.77
[INFO 2017-06-27 18:18:12,804 main.py:51] epoch 3655, training loss: 5688.89, average training loss: 6121.27, base loss: 9205.70
[INFO 2017-06-27 18:18:14,145 main.py:51] epoch 3656, training loss: 6225.61, average training loss: 6121.06, base loss: 9205.90
[INFO 2017-06-27 18:18:15,469 main.py:51] epoch 3657, training loss: 5671.36, average training loss: 6120.55, base loss: 9205.13
[INFO 2017-06-27 18:18:16,792 main.py:51] epoch 3658, training loss: 5765.64, average training loss: 6120.21, base loss: 9205.08
[INFO 2017-06-27 18:18:18,128 main.py:51] epoch 3659, training loss: 5311.74, average training loss: 6119.52, base loss: 9204.48
[INFO 2017-06-27 18:18:19,451 main.py:51] epoch 3660, training loss: 5923.40, average training loss: 6119.27, base loss: 9204.27
[INFO 2017-06-27 18:18:20,722 main.py:51] epoch 3661, training loss: 6107.14, average training loss: 6119.54, base loss: 9205.34
[INFO 2017-06-27 18:18:21,935 main.py:51] epoch 3662, training loss: 5375.82, average training loss: 6118.77, base loss: 9204.02
[INFO 2017-06-27 18:18:23,144 main.py:51] epoch 3663, training loss: 5715.85, average training loss: 6118.27, base loss: 9203.51
[INFO 2017-06-27 18:18:24,359 main.py:51] epoch 3664, training loss: 5570.15, average training loss: 6117.98, base loss: 9203.46
[INFO 2017-06-27 18:18:25,583 main.py:51] epoch 3665, training loss: 5592.95, average training loss: 6117.05, base loss: 9202.18
[INFO 2017-06-27 18:18:26,793 main.py:51] epoch 3666, training loss: 7661.48, average training loss: 6118.83, base loss: 9205.98
[INFO 2017-06-27 18:18:28,006 main.py:51] epoch 3667, training loss: 8024.33, average training loss: 6118.70, base loss: 9207.06
[INFO 2017-06-27 18:18:29,211 main.py:51] epoch 3668, training loss: 6320.83, average training loss: 6118.85, base loss: 9207.32
[INFO 2017-06-27 18:18:30,419 main.py:51] epoch 3669, training loss: 6171.96, average training loss: 6119.02, base loss: 9208.16
[INFO 2017-06-27 18:18:31,632 main.py:51] epoch 3670, training loss: 5768.93, average training loss: 6118.74, base loss: 9207.51
[INFO 2017-06-27 18:18:32,838 main.py:51] epoch 3671, training loss: 5904.55, average training loss: 6118.58, base loss: 9207.76
[INFO 2017-06-27 18:18:34,045 main.py:51] epoch 3672, training loss: 5577.20, average training loss: 6118.23, base loss: 9207.34
[INFO 2017-06-27 18:18:35,258 main.py:51] epoch 3673, training loss: 6198.47, average training loss: 6118.35, base loss: 9207.65
[INFO 2017-06-27 18:18:36,467 main.py:51] epoch 3674, training loss: 5981.46, average training loss: 6118.50, base loss: 9208.09
[INFO 2017-06-27 18:18:37,675 main.py:51] epoch 3675, training loss: 5644.32, average training loss: 6117.89, base loss: 9207.06
[INFO 2017-06-27 18:18:38,889 main.py:51] epoch 3676, training loss: 8028.33, average training loss: 6119.73, base loss: 9210.09
[INFO 2017-06-27 18:18:40,096 main.py:51] epoch 3677, training loss: 6134.19, average training loss: 6120.17, base loss: 9211.13
[INFO 2017-06-27 18:18:41,304 main.py:51] epoch 3678, training loss: 5559.22, average training loss: 6119.84, base loss: 9210.82
[INFO 2017-06-27 18:18:42,518 main.py:51] epoch 3679, training loss: 5751.13, average training loss: 6119.15, base loss: 9209.34
[INFO 2017-06-27 18:18:43,726 main.py:51] epoch 3680, training loss: 5547.83, average training loss: 6119.08, base loss: 9209.53
[INFO 2017-06-27 18:18:44,933 main.py:51] epoch 3681, training loss: 5733.67, average training loss: 6119.39, base loss: 9210.41
[INFO 2017-06-27 18:18:46,141 main.py:51] epoch 3682, training loss: 5477.55, average training loss: 6118.78, base loss: 9209.23
[INFO 2017-06-27 18:18:47,348 main.py:51] epoch 3683, training loss: 5588.52, average training loss: 6118.41, base loss: 9208.99
[INFO 2017-06-27 18:18:48,557 main.py:51] epoch 3684, training loss: 5750.24, average training loss: 6117.69, base loss: 9207.83
[INFO 2017-06-27 18:18:49,765 main.py:51] epoch 3685, training loss: 5737.90, average training loss: 6117.55, base loss: 9207.54
[INFO 2017-06-27 18:18:50,976 main.py:51] epoch 3686, training loss: 6050.58, average training loss: 6116.92, base loss: 9206.88
[INFO 2017-06-27 18:18:52,186 main.py:51] epoch 3687, training loss: 7643.10, average training loss: 6118.47, base loss: 9209.41
[INFO 2017-06-27 18:18:53,397 main.py:51] epoch 3688, training loss: 5648.00, average training loss: 6116.09, base loss: 9205.68
[INFO 2017-06-27 18:18:54,611 main.py:51] epoch 3689, training loss: 6118.34, average training loss: 6114.47, base loss: 9203.24
[INFO 2017-06-27 18:18:55,822 main.py:51] epoch 3690, training loss: 6177.36, average training loss: 6115.14, base loss: 9205.17
[INFO 2017-06-27 18:18:57,034 main.py:51] epoch 3691, training loss: 6256.88, average training loss: 6115.59, base loss: 9206.54
[INFO 2017-06-27 18:18:58,244 main.py:51] epoch 3692, training loss: 6010.42, average training loss: 6115.51, base loss: 9206.66
[INFO 2017-06-27 18:18:59,452 main.py:51] epoch 3693, training loss: 5952.88, average training loss: 6115.42, base loss: 9207.21
[INFO 2017-06-27 18:19:00,660 main.py:51] epoch 3694, training loss: 7878.57, average training loss: 6117.48, base loss: 9211.52
[INFO 2017-06-27 18:19:01,869 main.py:51] epoch 3695, training loss: 6014.02, average training loss: 6117.54, base loss: 9212.19
[INFO 2017-06-27 18:19:03,080 main.py:51] epoch 3696, training loss: 5856.67, average training loss: 6117.09, base loss: 9210.85
[INFO 2017-06-27 18:19:04,294 main.py:51] epoch 3697, training loss: 8377.09, average training loss: 6119.69, base loss: 9216.39
[INFO 2017-06-27 18:19:05,509 main.py:51] epoch 3698, training loss: 5535.42, average training loss: 6119.33, base loss: 9216.07
[INFO 2017-06-27 18:19:06,724 main.py:51] epoch 3699, training loss: 5978.00, average training loss: 6119.12, base loss: 9215.27
[INFO 2017-06-27 18:19:06,725 main.py:53] epoch 3699, testing
[INFO 2017-06-27 18:19:11,407 main.py:105] average testing loss: 5839.49, base loss: 8796.80
[INFO 2017-06-27 18:19:11,408 main.py:106] improve_loss: 2957.31, improve_percent: 0.34
[INFO 2017-06-27 18:19:11,409 main.py:76] current best improved percent: 0.36
[INFO 2017-06-27 18:19:12,626 main.py:51] epoch 3700, training loss: 5931.14, average training loss: 6119.30, base loss: 9216.37
[INFO 2017-06-27 18:19:13,854 main.py:51] epoch 3701, training loss: 7591.28, average training loss: 6120.88, base loss: 9218.94
[INFO 2017-06-27 18:19:15,059 main.py:51] epoch 3702, training loss: 6007.36, average training loss: 6121.06, base loss: 9219.78
[INFO 2017-06-27 18:19:16,267 main.py:51] epoch 3703, training loss: 5521.00, average training loss: 6120.79, base loss: 9219.35
[INFO 2017-06-27 18:19:17,476 main.py:51] epoch 3704, training loss: 5807.99, average training loss: 6121.04, base loss: 9220.44
[INFO 2017-06-27 18:19:18,681 main.py:51] epoch 3705, training loss: 5420.96, average training loss: 6117.05, base loss: 9214.43
[INFO 2017-06-27 18:19:19,891 main.py:51] epoch 3706, training loss: 5569.67, average training loss: 6116.44, base loss: 9212.87
[INFO 2017-06-27 18:19:21,103 main.py:51] epoch 3707, training loss: 5865.01, average training loss: 6116.24, base loss: 9212.35
[INFO 2017-06-27 18:19:22,311 main.py:51] epoch 3708, training loss: 5417.40, average training loss: 6115.25, base loss: 9210.45
[INFO 2017-06-27 18:19:23,527 main.py:51] epoch 3709, training loss: 5828.12, average training loss: 6115.47, base loss: 9211.11
[INFO 2017-06-27 18:19:24,736 main.py:51] epoch 3710, training loss: 6163.20, average training loss: 6116.06, base loss: 9213.50
[INFO 2017-06-27 18:19:25,949 main.py:51] epoch 3711, training loss: 5459.51, average training loss: 6115.52, base loss: 9212.83
[INFO 2017-06-27 18:19:27,159 main.py:51] epoch 3712, training loss: 5522.97, average training loss: 6115.57, base loss: 9213.68
[INFO 2017-06-27 18:19:28,367 main.py:51] epoch 3713, training loss: 5633.95, average training loss: 6115.20, base loss: 9213.00
[INFO 2017-06-27 18:19:29,576 main.py:51] epoch 3714, training loss: 5905.50, average training loss: 6114.46, base loss: 9211.77
[INFO 2017-06-27 18:19:30,787 main.py:51] epoch 3715, training loss: 5754.82, average training loss: 6114.17, base loss: 9211.26
[INFO 2017-06-27 18:19:31,999 main.py:51] epoch 3716, training loss: 5884.30, average training loss: 6114.20, base loss: 9211.80
[INFO 2017-06-27 18:19:33,209 main.py:51] epoch 3717, training loss: 6155.57, average training loss: 6114.29, base loss: 9212.68
[INFO 2017-06-27 18:19:34,419 main.py:51] epoch 3718, training loss: 5534.08, average training loss: 6114.20, base loss: 9213.15
[INFO 2017-06-27 18:19:35,627 main.py:51] epoch 3719, training loss: 5853.03, average training loss: 6114.30, base loss: 9214.07
[INFO 2017-06-27 18:19:36,840 main.py:51] epoch 3720, training loss: 6084.68, average training loss: 6114.08, base loss: 9214.07
[INFO 2017-06-27 18:19:38,050 main.py:51] epoch 3721, training loss: 5799.45, average training loss: 6114.07, base loss: 9214.60
[INFO 2017-06-27 18:19:39,261 main.py:51] epoch 3722, training loss: 5926.59, average training loss: 6113.34, base loss: 9213.43
[INFO 2017-06-27 18:19:40,473 main.py:51] epoch 3723, training loss: 5671.10, average training loss: 6112.82, base loss: 9212.42
[INFO 2017-06-27 18:19:41,686 main.py:51] epoch 3724, training loss: 6395.49, average training loss: 6113.29, base loss: 9214.39
[INFO 2017-06-27 18:19:42,892 main.py:51] epoch 3725, training loss: 5859.71, average training loss: 6112.83, base loss: 9213.34
[INFO 2017-06-27 18:19:44,104 main.py:51] epoch 3726, training loss: 5574.14, average training loss: 6109.80, base loss: 9210.45
[INFO 2017-06-27 18:19:45,318 main.py:51] epoch 3727, training loss: 6054.34, average training loss: 6108.79, base loss: 9208.95
[INFO 2017-06-27 18:19:46,525 main.py:51] epoch 3728, training loss: 5727.14, average training loss: 6108.63, base loss: 9209.59
[INFO 2017-06-27 18:19:47,735 main.py:51] epoch 3729, training loss: 6086.58, average training loss: 6108.54, base loss: 9210.40
[INFO 2017-06-27 18:19:48,946 main.py:51] epoch 3730, training loss: 5535.29, average training loss: 6107.69, base loss: 9208.20
[INFO 2017-06-27 18:19:50,157 main.py:51] epoch 3731, training loss: 5775.24, average training loss: 6107.68, base loss: 9208.30
[INFO 2017-06-27 18:19:51,366 main.py:51] epoch 3732, training loss: 5593.21, average training loss: 6107.13, base loss: 9207.25
[INFO 2017-06-27 18:19:52,578 main.py:51] epoch 3733, training loss: 7733.02, average training loss: 6109.06, base loss: 9211.85
[INFO 2017-06-27 18:19:53,792 main.py:51] epoch 3734, training loss: 5974.31, average training loss: 6108.67, base loss: 9211.41
[INFO 2017-06-27 18:19:55,005 main.py:51] epoch 3735, training loss: 5901.78, average training loss: 6108.59, base loss: 9211.98
[INFO 2017-06-27 18:19:56,217 main.py:51] epoch 3736, training loss: 7659.52, average training loss: 6110.33, base loss: 9216.16
[INFO 2017-06-27 18:19:57,431 main.py:51] epoch 3737, training loss: 7715.18, average training loss: 6108.55, base loss: 9215.57
[INFO 2017-06-27 18:19:58,644 main.py:51] epoch 3738, training loss: 5836.11, average training loss: 6108.67, base loss: 9216.65
[INFO 2017-06-27 18:19:59,859 main.py:51] epoch 3739, training loss: 5954.42, average training loss: 6108.46, base loss: 9216.51
[INFO 2017-06-27 18:20:01,070 main.py:51] epoch 3740, training loss: 6142.39, average training loss: 6108.44, base loss: 9217.09
[INFO 2017-06-27 18:20:02,277 main.py:51] epoch 3741, training loss: 5891.97, average training loss: 6108.14, base loss: 9216.18
[INFO 2017-06-27 18:20:03,488 main.py:51] epoch 3742, training loss: 5927.29, average training loss: 6108.46, base loss: 9217.21
[INFO 2017-06-27 18:20:04,699 main.py:51] epoch 3743, training loss: 5483.03, average training loss: 6107.92, base loss: 9216.71
[INFO 2017-06-27 18:20:05,910 main.py:51] epoch 3744, training loss: 6480.27, average training loss: 6108.54, base loss: 9217.68
[INFO 2017-06-27 18:20:07,124 main.py:51] epoch 3745, training loss: 5505.19, average training loss: 6108.36, base loss: 9217.46
[INFO 2017-06-27 18:20:08,339 main.py:51] epoch 3746, training loss: 5595.44, average training loss: 6107.94, base loss: 9216.58
[INFO 2017-06-27 18:20:09,553 main.py:51] epoch 3747, training loss: 5022.36, average training loss: 6106.91, base loss: 9214.33
[INFO 2017-06-27 18:20:10,764 main.py:51] epoch 3748, training loss: 5473.36, average training loss: 6106.56, base loss: 9214.24
[INFO 2017-06-27 18:20:11,977 main.py:51] epoch 3749, training loss: 7485.88, average training loss: 6108.34, base loss: 9217.66
[INFO 2017-06-27 18:20:13,192 main.py:51] epoch 3750, training loss: 5846.59, average training loss: 6108.23, base loss: 9218.03
[INFO 2017-06-27 18:20:14,401 main.py:51] epoch 3751, training loss: 5979.95, average training loss: 6107.79, base loss: 9217.04
[INFO 2017-06-27 18:20:15,612 main.py:51] epoch 3752, training loss: 7551.73, average training loss: 6108.94, base loss: 9219.45
[INFO 2017-06-27 18:20:16,822 main.py:51] epoch 3753, training loss: 5677.65, average training loss: 6108.50, base loss: 9219.49
[INFO 2017-06-27 18:20:18,030 main.py:51] epoch 3754, training loss: 5784.78, average training loss: 6108.05, base loss: 9218.81
[INFO 2017-06-27 18:20:19,243 main.py:51] epoch 3755, training loss: 6043.88, average training loss: 6108.24, base loss: 9219.17
[INFO 2017-06-27 18:20:20,456 main.py:51] epoch 3756, training loss: 5577.49, average training loss: 6107.61, base loss: 9218.40
[INFO 2017-06-27 18:20:21,667 main.py:51] epoch 3757, training loss: 5721.66, average training loss: 6107.46, base loss: 9218.39
[INFO 2017-06-27 18:20:22,880 main.py:51] epoch 3758, training loss: 6264.99, average training loss: 6107.76, base loss: 9219.27
[INFO 2017-06-27 18:20:24,093 main.py:51] epoch 3759, training loss: 5391.31, average training loss: 6107.37, base loss: 9218.76
[INFO 2017-06-27 18:20:25,312 main.py:51] epoch 3760, training loss: 5791.96, average training loss: 6106.90, base loss: 9217.69
[INFO 2017-06-27 18:20:26,537 main.py:51] epoch 3761, training loss: 6157.54, average training loss: 6102.18, base loss: 9211.38
[INFO 2017-06-27 18:20:27,893 main.py:51] epoch 3762, training loss: 8061.79, average training loss: 6104.55, base loss: 9216.92
[INFO 2017-06-27 18:20:29,295 main.py:51] epoch 3763, training loss: 5892.90, average training loss: 6104.66, base loss: 9217.83
[INFO 2017-06-27 18:20:30,708 main.py:51] epoch 3764, training loss: 5654.54, average training loss: 6104.41, base loss: 9217.20
[INFO 2017-06-27 18:20:31,967 main.py:51] epoch 3765, training loss: 6026.41, average training loss: 6104.17, base loss: 9216.99
[INFO 2017-06-27 18:20:33,174 main.py:51] epoch 3766, training loss: 8202.00, average training loss: 6106.43, base loss: 9221.07
[INFO 2017-06-27 18:20:34,383 main.py:51] epoch 3767, training loss: 7741.01, average training loss: 6108.41, base loss: 9224.75
[INFO 2017-06-27 18:20:35,671 main.py:51] epoch 3768, training loss: 5918.35, average training loss: 6108.61, base loss: 9224.79
[INFO 2017-06-27 18:20:37,021 main.py:51] epoch 3769, training loss: 5784.49, average training loss: 6108.37, base loss: 9224.22
[INFO 2017-06-27 18:20:38,407 main.py:51] epoch 3770, training loss: 6252.36, average training loss: 6108.24, base loss: 9224.50
[INFO 2017-06-27 18:20:39,821 main.py:51] epoch 3771, training loss: 8045.96, average training loss: 6110.52, base loss: 9229.47
[INFO 2017-06-27 18:20:41,074 main.py:51] epoch 3772, training loss: 6089.75, average training loss: 6110.63, base loss: 9229.97
[INFO 2017-06-27 18:20:42,285 main.py:51] epoch 3773, training loss: 6062.68, average training loss: 6111.07, base loss: 9230.80
[INFO 2017-06-27 18:20:43,494 main.py:51] epoch 3774, training loss: 5708.54, average training loss: 6108.61, base loss: 9226.23
[INFO 2017-06-27 18:20:44,702 main.py:51] epoch 3775, training loss: 6178.96, average training loss: 6108.14, base loss: 9225.22
[INFO 2017-06-27 18:20:45,923 main.py:51] epoch 3776, training loss: 6014.81, average training loss: 6108.03, base loss: 9225.43
[INFO 2017-06-27 18:20:47,228 main.py:51] epoch 3777, training loss: 6202.47, average training loss: 6107.82, base loss: 9225.40
[INFO 2017-06-27 18:20:48,556 main.py:51] epoch 3778, training loss: 6361.38, average training loss: 6108.28, base loss: 9227.22
[INFO 2017-06-27 18:20:49,975 main.py:51] epoch 3779, training loss: 5473.20, average training loss: 6107.61, base loss: 9227.01
[INFO 2017-06-27 18:20:51,329 main.py:51] epoch 3780, training loss: 5623.45, average training loss: 6107.41, base loss: 9226.55
[INFO 2017-06-27 18:20:52,711 main.py:51] epoch 3781, training loss: 5883.05, average training loss: 6107.27, base loss: 9226.42
[INFO 2017-06-27 18:20:54,001 main.py:51] epoch 3782, training loss: 6133.61, average training loss: 6107.36, base loss: 9227.12
[INFO 2017-06-27 18:20:55,314 main.py:51] epoch 3783, training loss: 5688.17, average training loss: 6107.28, base loss: 9227.50
[INFO 2017-06-27 18:20:56,534 main.py:51] epoch 3784, training loss: 5759.94, average training loss: 6106.94, base loss: 9227.22
[INFO 2017-06-27 18:20:57,746 main.py:51] epoch 3785, training loss: 5745.20, average training loss: 6106.31, base loss: 9226.07
[INFO 2017-06-27 18:20:59,058 main.py:51] epoch 3786, training loss: 5825.91, average training loss: 6106.23, base loss: 9226.07
[INFO 2017-06-27 18:21:00,366 main.py:51] epoch 3787, training loss: 5800.92, average training loss: 6105.70, base loss: 9225.97
[INFO 2017-06-27 18:21:01,772 main.py:51] epoch 3788, training loss: 7756.09, average training loss: 6107.57, base loss: 9230.12
[INFO 2017-06-27 18:21:03,071 main.py:51] epoch 3789, training loss: 5529.10, average training loss: 6107.22, base loss: 9229.94
[INFO 2017-06-27 18:21:04,278 main.py:51] epoch 3790, training loss: 5753.10, average training loss: 6107.01, base loss: 9229.95
[INFO 2017-06-27 18:21:05,497 main.py:51] epoch 3791, training loss: 5961.38, average training loss: 6107.23, base loss: 9231.31
[INFO 2017-06-27 18:21:06,755 main.py:51] epoch 3792, training loss: 5839.37, average training loss: 6106.56, base loss: 9229.61
[INFO 2017-06-27 18:21:08,102 main.py:51] epoch 3793, training loss: 6085.42, average training loss: 6106.70, base loss: 9230.34
[INFO 2017-06-27 18:21:09,446 main.py:51] epoch 3794, training loss: 6052.57, average training loss: 6106.70, base loss: 9230.92
[INFO 2017-06-27 18:21:10,676 main.py:51] epoch 3795, training loss: 5958.78, average training loss: 6106.80, base loss: 9231.33
[INFO 2017-06-27 18:21:11,886 main.py:51] epoch 3796, training loss: 5790.70, average training loss: 6106.50, base loss: 9230.29
[INFO 2017-06-27 18:21:13,186 main.py:51] epoch 3797, training loss: 5498.15, average training loss: 6103.85, base loss: 9226.12
[INFO 2017-06-27 18:21:14,428 main.py:51] epoch 3798, training loss: 5623.07, average training loss: 6103.51, base loss: 9225.12
[INFO 2017-06-27 18:21:15,639 main.py:51] epoch 3799, training loss: 5815.43, average training loss: 6102.94, base loss: 9224.49
[INFO 2017-06-27 18:21:15,639 main.py:53] epoch 3799, testing
[INFO 2017-06-27 18:21:20,620 main.py:105] average testing loss: 6063.45, base loss: 9080.56
[INFO 2017-06-27 18:21:20,620 main.py:106] improve_loss: 3017.11, improve_percent: 0.33
[INFO 2017-06-27 18:21:20,622 main.py:76] current best improved percent: 0.36
[INFO 2017-06-27 18:21:21,876 main.py:51] epoch 3800, training loss: 5716.78, average training loss: 6102.33, base loss: 9222.40
[INFO 2017-06-27 18:21:23,190 main.py:51] epoch 3801, training loss: 6234.17, average training loss: 6102.35, base loss: 9222.73
[INFO 2017-06-27 18:21:24,546 main.py:51] epoch 3802, training loss: 5943.29, average training loss: 6101.98, base loss: 9222.20
[INFO 2017-06-27 18:21:25,816 main.py:51] epoch 3803, training loss: 5883.66, average training loss: 6099.14, base loss: 9218.57
[INFO 2017-06-27 18:21:27,088 main.py:51] epoch 3804, training loss: 8375.60, average training loss: 6101.49, base loss: 9222.92
[INFO 2017-06-27 18:21:28,297 main.py:51] epoch 3805, training loss: 5913.30, average training loss: 6101.20, base loss: 9223.01
[INFO 2017-06-27 18:21:29,588 main.py:51] epoch 3806, training loss: 5852.99, average training loss: 6100.77, base loss: 9222.35
[INFO 2017-06-27 18:21:30,843 main.py:51] epoch 3807, training loss: 5551.50, average training loss: 6099.99, base loss: 9220.80
[INFO 2017-06-27 18:21:32,055 main.py:51] epoch 3808, training loss: 6007.94, average training loss: 6099.90, base loss: 9220.47
[INFO 2017-06-27 18:21:33,265 main.py:51] epoch 3809, training loss: 5751.34, average training loss: 6099.71, base loss: 9221.04
[INFO 2017-06-27 18:21:34,546 main.py:51] epoch 3810, training loss: 6230.54, average training loss: 6099.79, base loss: 9221.52
[INFO 2017-06-27 18:21:35,756 main.py:51] epoch 3811, training loss: 6146.52, average training loss: 6100.07, base loss: 9222.17
[INFO 2017-06-27 18:21:36,972 main.py:51] epoch 3812, training loss: 5916.81, average training loss: 6100.07, base loss: 9222.89
[INFO 2017-06-27 18:21:38,275 main.py:51] epoch 3813, training loss: 5875.03, average training loss: 6100.45, base loss: 9224.13
[INFO 2017-06-27 18:21:39,610 main.py:51] epoch 3814, training loss: 5434.07, average training loss: 6099.87, base loss: 9223.87
[INFO 2017-06-27 18:21:40,818 main.py:51] epoch 3815, training loss: 6136.74, average training loss: 6100.08, base loss: 9224.46
[INFO 2017-06-27 18:21:42,028 main.py:51] epoch 3816, training loss: 6101.73, average training loss: 6100.12, base loss: 9224.64
[INFO 2017-06-27 18:21:43,232 main.py:51] epoch 3817, training loss: 5495.76, average training loss: 6096.77, base loss: 9220.04
[INFO 2017-06-27 18:21:44,512 main.py:51] epoch 3818, training loss: 5742.24, average training loss: 6094.25, base loss: 9216.15
[INFO 2017-06-27 18:21:45,853 main.py:51] epoch 3819, training loss: 7494.66, average training loss: 6095.86, base loss: 9219.50
[INFO 2017-06-27 18:21:47,230 main.py:51] epoch 3820, training loss: 7790.19, average training loss: 6097.95, base loss: 9223.08
[INFO 2017-06-27 18:21:48,540 main.py:51] epoch 3821, training loss: 5763.65, average training loss: 6097.65, base loss: 9222.71
[INFO 2017-06-27 18:21:49,829 main.py:51] epoch 3822, training loss: 5958.05, average training loss: 6097.54, base loss: 9222.95
[INFO 2017-06-27 18:21:51,154 main.py:51] epoch 3823, training loss: 5897.78, average training loss: 6097.46, base loss: 9222.82
[INFO 2017-06-27 18:21:52,477 main.py:51] epoch 3824, training loss: 5692.30, average training loss: 6097.57, base loss: 9223.21
[INFO 2017-06-27 18:21:53,809 main.py:51] epoch 3825, training loss: 5791.99, average training loss: 6097.84, base loss: 9223.95
[INFO 2017-06-27 18:21:55,060 main.py:51] epoch 3826, training loss: 5826.30, average training loss: 6097.62, base loss: 9223.88
[INFO 2017-06-27 18:21:56,275 main.py:51] epoch 3827, training loss: 5778.79, average training loss: 6097.25, base loss: 9223.19
[INFO 2017-06-27 18:21:57,483 main.py:51] epoch 3828, training loss: 5809.20, average training loss: 6097.18, base loss: 9223.51
[INFO 2017-06-27 18:21:58,686 main.py:51] epoch 3829, training loss: 7778.94, average training loss: 6098.86, base loss: 9225.95
[INFO 2017-06-27 18:21:59,892 main.py:51] epoch 3830, training loss: 5999.93, average training loss: 6099.35, base loss: 9227.58
[INFO 2017-06-27 18:22:01,101 main.py:51] epoch 3831, training loss: 5784.13, average training loss: 6099.31, base loss: 9227.52
[INFO 2017-06-27 18:22:02,307 main.py:51] epoch 3832, training loss: 5421.56, average training loss: 6098.39, base loss: 9225.03
[INFO 2017-06-27 18:22:03,512 main.py:51] epoch 3833, training loss: 6180.27, average training loss: 6098.44, base loss: 9225.60
[INFO 2017-06-27 18:22:04,717 main.py:51] epoch 3834, training loss: 5432.23, average training loss: 6096.13, base loss: 9221.38
[INFO 2017-06-27 18:22:05,925 main.py:51] epoch 3835, training loss: 5435.26, average training loss: 6095.88, base loss: 9220.70
[INFO 2017-06-27 18:22:07,130 main.py:51] epoch 3836, training loss: 5682.41, average training loss: 6095.69, base loss: 9220.53
[INFO 2017-06-27 18:22:08,334 main.py:51] epoch 3837, training loss: 5664.59, average training loss: 6095.95, base loss: 9221.43
[INFO 2017-06-27 18:22:09,538 main.py:51] epoch 3838, training loss: 5917.97, average training loss: 6095.57, base loss: 9221.26
[INFO 2017-06-27 18:22:10,742 main.py:51] epoch 3839, training loss: 6032.67, average training loss: 6095.66, base loss: 9221.83
[INFO 2017-06-27 18:22:11,949 main.py:51] epoch 3840, training loss: 6126.66, average training loss: 6095.68, base loss: 9222.28
[INFO 2017-06-27 18:22:13,152 main.py:51] epoch 3841, training loss: 6029.55, average training loss: 6092.97, base loss: 9219.87
[INFO 2017-06-27 18:22:14,362 main.py:51] epoch 3842, training loss: 5534.52, average training loss: 6093.01, base loss: 9220.55
[INFO 2017-06-27 18:22:15,571 main.py:51] epoch 3843, training loss: 5837.81, average training loss: 6092.77, base loss: 9220.68
[INFO 2017-06-27 18:22:16,776 main.py:51] epoch 3844, training loss: 7570.75, average training loss: 6094.49, base loss: 9224.51
[INFO 2017-06-27 18:22:17,986 main.py:51] epoch 3845, training loss: 5841.53, average training loss: 6094.08, base loss: 9223.67
[INFO 2017-06-27 18:22:19,190 main.py:51] epoch 3846, training loss: 5889.31, average training loss: 6093.91, base loss: 9223.31
[INFO 2017-06-27 18:22:20,396 main.py:51] epoch 3847, training loss: 6195.06, average training loss: 6094.18, base loss: 9224.38
[INFO 2017-06-27 18:22:21,603 main.py:51] epoch 3848, training loss: 5766.07, average training loss: 6093.92, base loss: 9223.75
[INFO 2017-06-27 18:22:22,811 main.py:51] epoch 3849, training loss: 5981.86, average training loss: 6093.78, base loss: 9223.75
[INFO 2017-06-27 18:22:24,020 main.py:51] epoch 3850, training loss: 5467.46, average training loss: 6093.34, base loss: 9223.05
[INFO 2017-06-27 18:22:25,230 main.py:51] epoch 3851, training loss: 5535.23, average training loss: 6093.14, base loss: 9222.60
[INFO 2017-06-27 18:22:26,434 main.py:51] epoch 3852, training loss: 5182.08, average training loss: 6092.41, base loss: 9221.46
[INFO 2017-06-27 18:22:27,641 main.py:51] epoch 3853, training loss: 5647.72, average training loss: 6091.77, base loss: 9220.66
[INFO 2017-06-27 18:22:28,846 main.py:51] epoch 3854, training loss: 8355.49, average training loss: 6093.82, base loss: 9225.24
[INFO 2017-06-27 18:22:30,054 main.py:51] epoch 3855, training loss: 5823.43, average training loss: 6093.30, base loss: 9224.72
[INFO 2017-06-27 18:22:31,262 main.py:51] epoch 3856, training loss: 5766.87, average training loss: 6092.88, base loss: 9224.27
[INFO 2017-06-27 18:22:32,474 main.py:51] epoch 3857, training loss: 6192.81, average training loss: 6093.24, base loss: 9224.82
[INFO 2017-06-27 18:22:33,684 main.py:51] epoch 3858, training loss: 5659.98, average training loss: 6092.35, base loss: 9223.61
[INFO 2017-06-27 18:22:34,892 main.py:51] epoch 3859, training loss: 6181.94, average training loss: 6092.34, base loss: 9224.19
[INFO 2017-06-27 18:22:36,103 main.py:51] epoch 3860, training loss: 5737.69, average training loss: 6091.66, base loss: 9222.94
[INFO 2017-06-27 18:22:37,310 main.py:51] epoch 3861, training loss: 5777.16, average training loss: 6091.84, base loss: 9223.89
[INFO 2017-06-27 18:22:38,520 main.py:51] epoch 3862, training loss: 5694.93, average training loss: 6092.05, base loss: 9224.61
[INFO 2017-06-27 18:22:39,724 main.py:51] epoch 3863, training loss: 5596.86, average training loss: 6092.64, base loss: 9226.43
[INFO 2017-06-27 18:22:40,931 main.py:51] epoch 3864, training loss: 5851.66, average training loss: 6092.75, base loss: 9227.03
[INFO 2017-06-27 18:22:42,138 main.py:51] epoch 3865, training loss: 5323.76, average training loss: 6089.38, base loss: 9222.14
[INFO 2017-06-27 18:22:43,350 main.py:51] epoch 3866, training loss: 6002.53, average training loss: 6089.46, base loss: 9223.13
[INFO 2017-06-27 18:22:44,558 main.py:51] epoch 3867, training loss: 5504.78, average training loss: 6088.55, base loss: 9221.75
[INFO 2017-06-27 18:22:45,765 main.py:51] epoch 3868, training loss: 5694.78, average training loss: 6088.25, base loss: 9221.07
[INFO 2017-06-27 18:22:46,969 main.py:51] epoch 3869, training loss: 5937.17, average training loss: 6088.37, base loss: 9221.74
[INFO 2017-06-27 18:22:48,175 main.py:51] epoch 3870, training loss: 5567.34, average training loss: 6088.02, base loss: 9221.29
[INFO 2017-06-27 18:22:49,385 main.py:51] epoch 3871, training loss: 6002.48, average training loss: 6087.96, base loss: 9221.68
[INFO 2017-06-27 18:22:50,593 main.py:51] epoch 3872, training loss: 5779.30, average training loss: 6087.82, base loss: 9221.62
[INFO 2017-06-27 18:22:51,796 main.py:51] epoch 3873, training loss: 6202.76, average training loss: 6085.93, base loss: 9219.47
[INFO 2017-06-27 18:22:53,005 main.py:51] epoch 3874, training loss: 5578.06, average training loss: 6085.29, base loss: 9218.76
[INFO 2017-06-27 18:22:54,208 main.py:51] epoch 3875, training loss: 5513.99, average training loss: 6085.01, base loss: 9218.39
[INFO 2017-06-27 18:22:55,412 main.py:51] epoch 3876, training loss: 6041.78, average training loss: 6084.96, base loss: 9218.83
[INFO 2017-06-27 18:22:56,615 main.py:51] epoch 3877, training loss: 5993.60, average training loss: 6084.97, base loss: 9219.89
[INFO 2017-06-27 18:22:57,824 main.py:51] epoch 3878, training loss: 7758.97, average training loss: 6086.55, base loss: 9223.13
[INFO 2017-06-27 18:22:59,030 main.py:51] epoch 3879, training loss: 5902.28, average training loss: 6086.36, base loss: 9223.36
[INFO 2017-06-27 18:23:00,233 main.py:51] epoch 3880, training loss: 5607.98, average training loss: 6085.90, base loss: 9222.39
[INFO 2017-06-27 18:23:01,439 main.py:51] epoch 3881, training loss: 5499.78, average training loss: 6085.44, base loss: 9221.05
[INFO 2017-06-27 18:23:02,645 main.py:51] epoch 3882, training loss: 6226.44, average training loss: 6085.30, base loss: 9221.23
[INFO 2017-06-27 18:23:03,850 main.py:51] epoch 3883, training loss: 5721.90, average training loss: 6084.92, base loss: 9220.56
[INFO 2017-06-27 18:23:05,058 main.py:51] epoch 3884, training loss: 7745.31, average training loss: 6086.86, base loss: 9224.55
[INFO 2017-06-27 18:23:06,265 main.py:51] epoch 3885, training loss: 5718.00, average training loss: 6086.33, base loss: 9223.50
[INFO 2017-06-27 18:23:07,473 main.py:51] epoch 3886, training loss: 5584.80, average training loss: 6086.32, base loss: 9223.70
[INFO 2017-06-27 18:23:08,676 main.py:51] epoch 3887, training loss: 7730.79, average training loss: 6085.63, base loss: 9223.39
[INFO 2017-06-27 18:23:09,885 main.py:51] epoch 3888, training loss: 5935.58, average training loss: 6085.30, base loss: 9222.25
[INFO 2017-06-27 18:23:11,090 main.py:51] epoch 3889, training loss: 7768.79, average training loss: 6084.46, base loss: 9222.28
[INFO 2017-06-27 18:23:12,300 main.py:51] epoch 3890, training loss: 5658.90, average training loss: 6084.26, base loss: 9221.66
[INFO 2017-06-27 18:23:13,511 main.py:51] epoch 3891, training loss: 5683.62, average training loss: 6083.82, base loss: 9221.14
[INFO 2017-06-27 18:23:14,718 main.py:51] epoch 3892, training loss: 7648.45, average training loss: 6085.25, base loss: 9223.39
[INFO 2017-06-27 18:23:15,921 main.py:51] epoch 3893, training loss: 5907.01, average training loss: 6085.04, base loss: 9222.44
[INFO 2017-06-27 18:23:17,128 main.py:51] epoch 3894, training loss: 5829.49, average training loss: 6084.52, base loss: 9221.50
[INFO 2017-06-27 18:23:18,333 main.py:51] epoch 3895, training loss: 7816.67, average training loss: 6086.60, base loss: 9226.07
[INFO 2017-06-27 18:23:19,539 main.py:51] epoch 3896, training loss: 5864.62, average training loss: 6086.68, base loss: 9226.21
[INFO 2017-06-27 18:23:20,746 main.py:51] epoch 3897, training loss: 5591.90, average training loss: 6086.02, base loss: 9225.06
[INFO 2017-06-27 18:23:21,951 main.py:51] epoch 3898, training loss: 5945.55, average training loss: 6085.99, base loss: 9225.60
[INFO 2017-06-27 18:23:23,158 main.py:51] epoch 3899, training loss: 5682.25, average training loss: 6085.71, base loss: 9225.28
[INFO 2017-06-27 18:23:23,158 main.py:53] epoch 3899, testing
[INFO 2017-06-27 18:23:27,829 main.py:105] average testing loss: 6157.92, base loss: 9459.77
[INFO 2017-06-27 18:23:27,829 main.py:106] improve_loss: 3301.85, improve_percent: 0.35
[INFO 2017-06-27 18:23:27,830 main.py:76] current best improved percent: 0.36
[INFO 2017-06-27 18:23:29,037 main.py:51] epoch 3900, training loss: 10174.73, average training loss: 6090.20, base loss: 9233.59
[INFO 2017-06-27 18:23:30,242 main.py:51] epoch 3901, training loss: 5945.90, average training loss: 6090.42, base loss: 9234.35
[INFO 2017-06-27 18:23:31,450 main.py:51] epoch 3902, training loss: 6182.45, average training loss: 6090.69, base loss: 9235.44
[INFO 2017-06-27 18:23:32,657 main.py:51] epoch 3903, training loss: 6257.61, average training loss: 6090.71, base loss: 9236.42
