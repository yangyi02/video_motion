[INFO 2017-06-26 12:25:57,746 main.py:170] Namespace(batch_size=32, display=False, image_dir='/home/yi/Downloads/mpii-64-one', image_size=64, init_model_path='', input_video_path='video', learning_rate=0.001, method='unsupervised', motion_range=2, num_channel=3, num_inputs=3, output_flow_path='flow', output_flow_video_path='flow_video', save_dir='./model', test=False, test_epoch=10, test_interval=100, test_video=False, train=True, train_epoch=10000)
[INFO 2017-06-26 12:25:59,830 main.py:50] epoch 0, training loss: 104356.12, average training loss: 104356.12, base loss: 20563.02
[INFO 2017-06-26 12:26:00,124 main.py:50] epoch 1, training loss: 83540.08, average training loss: 93948.10, base loss: 20563.54
[INFO 2017-06-26 12:26:00,410 main.py:50] epoch 2, training loss: 68834.08, average training loss: 85576.76, base loss: 20571.43
[INFO 2017-06-26 12:26:00,703 main.py:50] epoch 3, training loss: 59916.96, average training loss: 79161.81, base loss: 20566.31
[INFO 2017-06-26 12:26:00,990 main.py:50] epoch 4, training loss: 52168.36, average training loss: 73763.12, base loss: 20568.83
[INFO 2017-06-26 12:26:01,276 main.py:50] epoch 5, training loss: 46246.41, average training loss: 69177.00, base loss: 20502.81
[INFO 2017-06-26 12:26:01,561 main.py:50] epoch 6, training loss: 41139.92, average training loss: 65171.70, base loss: 20513.87
[INFO 2017-06-26 12:26:01,848 main.py:50] epoch 7, training loss: 36356.62, average training loss: 61569.82, base loss: 20504.55
[INFO 2017-06-26 12:26:02,135 main.py:50] epoch 8, training loss: 32515.26, average training loss: 58341.53, base loss: 20451.41
[INFO 2017-06-26 12:26:02,420 main.py:50] epoch 9, training loss: 29855.31, average training loss: 55492.91, base loss: 20477.30
[INFO 2017-06-26 12:26:02,707 main.py:50] epoch 10, training loss: 27221.88, average training loss: 52922.82, base loss: 20503.71
[INFO 2017-06-26 12:26:02,996 main.py:50] epoch 11, training loss: 24856.04, average training loss: 50583.92, base loss: 20502.01
[INFO 2017-06-26 12:26:03,282 main.py:50] epoch 12, training loss: 23129.43, average training loss: 48472.04, base loss: 20497.64
[INFO 2017-06-26 12:26:03,568 main.py:50] epoch 13, training loss: 21963.20, average training loss: 46578.55, base loss: 20498.61
[INFO 2017-06-26 12:26:03,855 main.py:50] epoch 14, training loss: 21200.76, average training loss: 44886.69, base loss: 20499.81
[INFO 2017-06-26 12:26:04,143 main.py:50] epoch 15, training loss: 20186.67, average training loss: 43342.94, base loss: 20500.96
[INFO 2017-06-26 12:26:04,432 main.py:50] epoch 16, training loss: 19251.71, average training loss: 41925.81, base loss: 20497.96
[INFO 2017-06-26 12:26:04,721 main.py:50] epoch 17, training loss: 18817.51, average training loss: 40642.02, base loss: 20496.07
[INFO 2017-06-26 12:26:05,008 main.py:50] epoch 18, training loss: 17949.71, average training loss: 39447.69, base loss: 20480.30
[INFO 2017-06-26 12:26:05,296 main.py:50] epoch 19, training loss: 17831.73, average training loss: 38366.89, base loss: 20482.20
[INFO 2017-06-26 12:26:05,579 main.py:50] epoch 20, training loss: 17723.71, average training loss: 37383.88, base loss: 20502.20
[INFO 2017-06-26 12:26:05,872 main.py:50] epoch 21, training loss: 17195.99, average training loss: 36466.25, base loss: 20505.80
[INFO 2017-06-26 12:26:06,164 main.py:50] epoch 22, training loss: 16697.21, average training loss: 35606.72, base loss: 20511.08
[INFO 2017-06-26 12:26:06,454 main.py:50] epoch 23, training loss: 16599.83, average training loss: 34814.77, base loss: 20520.21
[INFO 2017-06-26 12:26:06,748 main.py:50] epoch 24, training loss: 15933.62, average training loss: 34059.52, base loss: 20520.43
[INFO 2017-06-26 12:26:07,035 main.py:50] epoch 25, training loss: 15683.84, average training loss: 33352.77, base loss: 20520.07
[INFO 2017-06-26 12:26:07,330 main.py:50] epoch 26, training loss: 15428.89, average training loss: 32688.92, base loss: 20514.86
[INFO 2017-06-26 12:26:07,616 main.py:50] epoch 27, training loss: 15577.52, average training loss: 32077.80, base loss: 20522.59
[INFO 2017-06-26 12:26:07,907 main.py:50] epoch 28, training loss: 15058.30, average training loss: 31490.92, base loss: 20520.88
[INFO 2017-06-26 12:26:08,198 main.py:50] epoch 29, training loss: 14894.60, average training loss: 30937.71, base loss: 20516.98
[INFO 2017-06-26 12:26:08,483 main.py:50] epoch 30, training loss: 15174.42, average training loss: 30429.22, base loss: 20522.59
[INFO 2017-06-26 12:26:08,770 main.py:50] epoch 31, training loss: 14739.35, average training loss: 29938.91, base loss: 20518.25
[INFO 2017-06-26 12:26:09,058 main.py:50] epoch 32, training loss: 14930.30, average training loss: 29484.10, base loss: 20520.49
[INFO 2017-06-26 12:26:09,344 main.py:50] epoch 33, training loss: 14512.00, average training loss: 29043.75, base loss: 20514.39
[INFO 2017-06-26 12:26:09,630 main.py:50] epoch 34, training loss: 14377.30, average training loss: 28624.70, base loss: 20513.59
[INFO 2017-06-26 12:26:09,935 main.py:50] epoch 35, training loss: 14319.00, average training loss: 28227.32, base loss: 20518.63
[INFO 2017-06-26 12:26:10,221 main.py:50] epoch 36, training loss: 14109.39, average training loss: 27845.76, base loss: 20519.96
[INFO 2017-06-26 12:26:10,512 main.py:50] epoch 37, training loss: 14046.34, average training loss: 27482.62, base loss: 20518.26
[INFO 2017-06-26 12:26:10,803 main.py:50] epoch 38, training loss: 14138.41, average training loss: 27140.46, base loss: 20521.65
[INFO 2017-06-26 12:26:11,090 main.py:50] epoch 39, training loss: 14292.55, average training loss: 26819.26, base loss: 20526.96
[INFO 2017-06-26 12:26:11,378 main.py:50] epoch 40, training loss: 13941.17, average training loss: 26505.16, base loss: 20527.71
[INFO 2017-06-26 12:26:11,669 main.py:50] epoch 41, training loss: 13753.28, average training loss: 26201.54, base loss: 20526.58
[INFO 2017-06-26 12:26:11,959 main.py:50] epoch 42, training loss: 13766.43, average training loss: 25912.35, base loss: 20524.31
[INFO 2017-06-26 12:26:12,249 main.py:50] epoch 43, training loss: 13785.35, average training loss: 25636.74, base loss: 20525.70
[INFO 2017-06-26 12:26:12,538 main.py:50] epoch 44, training loss: 13726.02, average training loss: 25372.06, base loss: 20534.07
[INFO 2017-06-26 12:26:12,824 main.py:50] epoch 45, training loss: 13369.83, average training loss: 25111.14, base loss: 20531.59
[INFO 2017-06-26 12:26:13,113 main.py:50] epoch 46, training loss: 13261.62, average training loss: 24859.02, base loss: 20526.07
[INFO 2017-06-26 12:26:13,405 main.py:50] epoch 47, training loss: 13304.04, average training loss: 24618.29, base loss: 20523.77
[INFO 2017-06-26 12:26:13,690 main.py:50] epoch 48, training loss: 13335.95, average training loss: 24388.04, base loss: 20526.81
[INFO 2017-06-26 12:26:13,978 main.py:50] epoch 49, training loss: 13085.73, average training loss: 24162.00, base loss: 20521.30
[INFO 2017-06-26 12:26:14,268 main.py:50] epoch 50, training loss: 13130.27, average training loss: 23945.69, base loss: 20517.63
[INFO 2017-06-26 12:26:14,555 main.py:50] epoch 51, training loss: 13184.59, average training loss: 23738.74, base loss: 20518.50
[INFO 2017-06-26 12:26:14,842 main.py:50] epoch 52, training loss: 13060.33, average training loss: 23537.26, base loss: 20516.92
[INFO 2017-06-26 12:26:15,133 main.py:50] epoch 53, training loss: 13038.66, average training loss: 23342.84, base loss: 20516.87
[INFO 2017-06-26 12:26:15,425 main.py:50] epoch 54, training loss: 12918.90, average training loss: 23153.32, base loss: 20515.60
[INFO 2017-06-26 12:26:15,710 main.py:50] epoch 55, training loss: 12913.36, average training loss: 22970.46, base loss: 20513.93
[INFO 2017-06-26 12:26:16,001 main.py:50] epoch 56, training loss: 12767.74, average training loss: 22791.47, base loss: 20508.41
[INFO 2017-06-26 12:26:16,290 main.py:50] epoch 57, training loss: 12769.02, average training loss: 22618.67, base loss: 20506.37
[INFO 2017-06-26 12:26:16,577 main.py:50] epoch 58, training loss: 13136.79, average training loss: 22457.96, base loss: 20513.82
[INFO 2017-06-26 12:26:16,865 main.py:50] epoch 59, training loss: 12910.52, average training loss: 22298.83, base loss: 20515.16
[INFO 2017-06-26 12:26:17,158 main.py:50] epoch 60, training loss: 12702.13, average training loss: 22141.51, base loss: 20515.91
[INFO 2017-06-26 12:26:17,444 main.py:50] epoch 61, training loss: 12703.69, average training loss: 21989.29, base loss: 20519.90
[INFO 2017-06-26 12:26:17,736 main.py:50] epoch 62, training loss: 12448.21, average training loss: 21837.84, base loss: 20518.80
[INFO 2017-06-26 12:26:18,025 main.py:50] epoch 63, training loss: 12948.79, average training loss: 21698.95, base loss: 20525.52
[INFO 2017-06-26 12:26:18,314 main.py:50] epoch 64, training loss: 12719.74, average training loss: 21560.81, base loss: 20530.67
[INFO 2017-06-26 12:26:18,602 main.py:50] epoch 65, training loss: 12583.43, average training loss: 21424.79, base loss: 20533.10
[INFO 2017-06-26 12:26:18,888 main.py:50] epoch 66, training loss: 12607.90, average training loss: 21293.19, base loss: 20535.54
[INFO 2017-06-26 12:26:19,174 main.py:50] epoch 67, training loss: 12263.08, average training loss: 21160.40, base loss: 20531.26
[INFO 2017-06-26 12:26:19,461 main.py:50] epoch 68, training loss: 12531.65, average training loss: 21035.34, base loss: 20531.63
[INFO 2017-06-26 12:26:19,751 main.py:50] epoch 69, training loss: 12254.55, average training loss: 20909.90, base loss: 20532.00
[INFO 2017-06-26 12:26:20,042 main.py:50] epoch 70, training loss: 12281.49, average training loss: 20788.37, base loss: 20533.24
[INFO 2017-06-26 12:26:20,335 main.py:50] epoch 71, training loss: 12427.12, average training loss: 20672.25, base loss: 20535.25
[INFO 2017-06-26 12:26:20,622 main.py:50] epoch 72, training loss: 12278.29, average training loss: 20557.26, base loss: 20538.68
[INFO 2017-06-26 12:26:20,911 main.py:50] epoch 73, training loss: 12083.68, average training loss: 20442.75, base loss: 20538.88
[INFO 2017-06-26 12:26:21,200 main.py:50] epoch 74, training loss: 12167.87, average training loss: 20332.42, base loss: 20537.90
[INFO 2017-06-26 12:26:21,491 main.py:50] epoch 75, training loss: 12199.58, average training loss: 20225.41, base loss: 20540.67
[INFO 2017-06-26 12:26:21,782 main.py:50] epoch 76, training loss: 11912.35, average training loss: 20117.45, base loss: 20536.69
[INFO 2017-06-26 12:26:22,070 main.py:50] epoch 77, training loss: 11848.80, average training loss: 20011.44, base loss: 20533.84
[INFO 2017-06-26 12:26:22,355 main.py:50] epoch 78, training loss: 12169.96, average training loss: 19912.18, base loss: 20537.52
[INFO 2017-06-26 12:26:22,646 main.py:50] epoch 79, training loss: 12078.57, average training loss: 19814.26, base loss: 20537.02
[INFO 2017-06-26 12:26:22,940 main.py:50] epoch 80, training loss: 12022.48, average training loss: 19718.07, base loss: 20539.25
[INFO 2017-06-26 12:26:23,230 main.py:50] epoch 81, training loss: 11768.10, average training loss: 19621.11, base loss: 20538.26
[INFO 2017-06-26 12:26:23,522 main.py:50] epoch 82, training loss: 12134.22, average training loss: 19530.91, base loss: 20540.01
[INFO 2017-06-26 12:26:23,812 main.py:50] epoch 83, training loss: 11710.43, average training loss: 19437.81, base loss: 20539.96
[INFO 2017-06-26 12:26:24,102 main.py:50] epoch 84, training loss: 11962.35, average training loss: 19349.86, base loss: 20542.41
[INFO 2017-06-26 12:26:24,393 main.py:50] epoch 85, training loss: 11649.79, average training loss: 19260.33, base loss: 20542.41
[INFO 2017-06-26 12:26:24,683 main.py:50] epoch 86, training loss: 11581.26, average training loss: 19172.06, base loss: 20539.05
[INFO 2017-06-26 12:26:24,973 main.py:50] epoch 87, training loss: 11969.23, average training loss: 19090.21, base loss: 20544.16
[INFO 2017-06-26 12:26:25,262 main.py:50] epoch 88, training loss: 11449.39, average training loss: 19004.36, base loss: 20540.66
[INFO 2017-06-26 12:26:25,548 main.py:50] epoch 89, training loss: 11398.40, average training loss: 18919.85, base loss: 20537.17
[INFO 2017-06-26 12:26:25,843 main.py:50] epoch 90, training loss: 11483.43, average training loss: 18838.13, base loss: 20535.08
[INFO 2017-06-26 12:26:26,129 main.py:50] epoch 91, training loss: 11520.00, average training loss: 18758.59, base loss: 20533.81
[INFO 2017-06-26 12:26:26,416 main.py:50] epoch 92, training loss: 11392.99, average training loss: 18679.39, base loss: 20533.66
[INFO 2017-06-26 12:26:26,706 main.py:50] epoch 93, training loss: 11215.50, average training loss: 18599.98, base loss: 20530.22
[INFO 2017-06-26 12:26:26,998 main.py:50] epoch 94, training loss: 11521.79, average training loss: 18525.48, base loss: 20533.55
[INFO 2017-06-26 12:26:27,284 main.py:50] epoch 95, training loss: 11268.79, average training loss: 18449.88, base loss: 20531.37
[INFO 2017-06-26 12:26:27,576 main.py:50] epoch 96, training loss: 11398.94, average training loss: 18377.19, base loss: 20532.52
[INFO 2017-06-26 12:26:27,864 main.py:50] epoch 97, training loss: 11219.81, average training loss: 18304.16, base loss: 20532.17
[INFO 2017-06-26 12:26:28,159 main.py:50] epoch 98, training loss: 11192.44, average training loss: 18232.32, base loss: 20532.03
[INFO 2017-06-26 12:26:28,449 main.py:50] epoch 99, training loss: 11171.08, average training loss: 18161.71, base loss: 20531.04
[INFO 2017-06-26 12:26:28,449 main.py:52] epoch 99, testing
[INFO 2017-06-26 12:26:29,710 main.py:103] average testing loss: 11202.36, base loss: 20502.04
[INFO 2017-06-26 12:26:29,710 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 12:26:29,718 main.py:76] current best accuracy: 11202.36
[INFO 2017-06-26 12:26:30,008 main.py:50] epoch 100, training loss: 11298.09, average training loss: 18093.76, base loss: 20531.97
[INFO 2017-06-26 12:26:30,298 main.py:50] epoch 101, training loss: 11272.56, average training loss: 18026.88, base loss: 20532.56
[INFO 2017-06-26 12:26:30,585 main.py:50] epoch 102, training loss: 11103.30, average training loss: 17959.66, base loss: 20531.86
[INFO 2017-06-26 12:26:30,875 main.py:50] epoch 103, training loss: 11010.89, average training loss: 17892.85, base loss: 20530.44
[INFO 2017-06-26 12:26:31,163 main.py:50] epoch 104, training loss: 11167.03, average training loss: 17828.79, base loss: 20531.53
[INFO 2017-06-26 12:26:31,456 main.py:50] epoch 105, training loss: 10834.14, average training loss: 17762.80, base loss: 20527.03
[INFO 2017-06-26 12:26:31,749 main.py:50] epoch 106, training loss: 11014.58, average training loss: 17699.74, base loss: 20525.88
[INFO 2017-06-26 12:26:32,042 main.py:50] epoch 107, training loss: 11059.35, average training loss: 17638.25, base loss: 20526.65
[INFO 2017-06-26 12:26:32,337 main.py:50] epoch 108, training loss: 11123.39, average training loss: 17578.48, base loss: 20528.81
[INFO 2017-06-26 12:26:32,627 main.py:50] epoch 109, training loss: 11016.36, average training loss: 17518.83, base loss: 20529.49
[INFO 2017-06-26 12:26:32,918 main.py:50] epoch 110, training loss: 10980.82, average training loss: 17459.93, base loss: 20530.26
[INFO 2017-06-26 12:26:33,206 main.py:50] epoch 111, training loss: 11066.73, average training loss: 17402.84, base loss: 20532.56
[INFO 2017-06-26 12:26:33,498 main.py:50] epoch 112, training loss: 10831.00, average training loss: 17344.69, base loss: 20533.41
[INFO 2017-06-26 12:26:33,787 main.py:50] epoch 113, training loss: 10824.59, average training loss: 17287.49, base loss: 20532.68
[INFO 2017-06-26 12:26:34,076 main.py:50] epoch 114, training loss: 10825.82, average training loss: 17231.30, base loss: 20533.56
[INFO 2017-06-26 12:26:34,363 main.py:50] epoch 115, training loss: 10542.17, average training loss: 17173.64, base loss: 20532.09
[INFO 2017-06-26 12:26:34,654 main.py:50] epoch 116, training loss: 10646.24, average training loss: 17117.85, base loss: 20531.56
[INFO 2017-06-26 12:26:34,947 main.py:50] epoch 117, training loss: 10664.27, average training loss: 17063.16, base loss: 20530.95
[INFO 2017-06-26 12:26:35,234 main.py:50] epoch 118, training loss: 10442.59, average training loss: 17007.52, base loss: 20527.93
[INFO 2017-06-26 12:26:35,523 main.py:50] epoch 119, training loss: 10542.35, average training loss: 16953.65, base loss: 20527.63
[INFO 2017-06-26 12:26:35,815 main.py:50] epoch 120, training loss: 10594.69, average training loss: 16901.09, base loss: 20526.98
[INFO 2017-06-26 12:26:36,101 main.py:50] epoch 121, training loss: 10476.06, average training loss: 16848.43, base loss: 20523.72
[INFO 2017-06-26 12:26:36,393 main.py:50] epoch 122, training loss: 10598.99, average training loss: 16797.62, base loss: 20524.40
[INFO 2017-06-26 12:26:36,682 main.py:50] epoch 123, training loss: 10342.78, average training loss: 16745.56, base loss: 20523.73
[INFO 2017-06-26 12:26:36,972 main.py:50] epoch 124, training loss: 10522.10, average training loss: 16695.78, base loss: 20523.59
[INFO 2017-06-26 12:26:37,267 main.py:50] epoch 125, training loss: 10247.78, average training loss: 16644.60, base loss: 20522.21
[INFO 2017-06-26 12:26:37,557 main.py:50] epoch 126, training loss: 10448.21, average training loss: 16595.81, base loss: 20522.07
[INFO 2017-06-26 12:26:37,847 main.py:50] epoch 127, training loss: 10477.07, average training loss: 16548.01, base loss: 20523.38
[INFO 2017-06-26 12:26:38,136 main.py:50] epoch 128, training loss: 10420.79, average training loss: 16500.51, base loss: 20524.97
[INFO 2017-06-26 12:26:38,427 main.py:50] epoch 129, training loss: 10209.96, average training loss: 16452.12, base loss: 20523.51
[INFO 2017-06-26 12:26:38,721 main.py:50] epoch 130, training loss: 10320.53, average training loss: 16405.32, base loss: 20524.25
[INFO 2017-06-26 12:26:39,017 main.py:50] epoch 131, training loss: 10284.74, average training loss: 16358.95, base loss: 20524.79
[INFO 2017-06-26 12:26:39,318 main.py:50] epoch 132, training loss: 10335.73, average training loss: 16313.66, base loss: 20525.38
[INFO 2017-06-26 12:26:39,620 main.py:50] epoch 133, training loss: 10189.27, average training loss: 16267.96, base loss: 20525.01
[INFO 2017-06-26 12:26:39,922 main.py:50] epoch 134, training loss: 10248.12, average training loss: 16223.37, base loss: 20524.42
[INFO 2017-06-26 12:26:40,224 main.py:50] epoch 135, training loss: 10274.70, average training loss: 16179.62, base loss: 20524.91
[INFO 2017-06-26 12:26:40,526 main.py:50] epoch 136, training loss: 10182.12, average training loss: 16135.85, base loss: 20525.08
[INFO 2017-06-26 12:26:40,826 main.py:50] epoch 137, training loss: 10094.65, average training loss: 16092.07, base loss: 20524.96
[INFO 2017-06-26 12:26:41,132 main.py:50] epoch 138, training loss: 9989.38, average training loss: 16048.17, base loss: 20524.04
[INFO 2017-06-26 12:26:41,441 main.py:50] epoch 139, training loss: 10095.54, average training loss: 16005.65, base loss: 20523.03
[INFO 2017-06-26 12:26:41,753 main.py:50] epoch 140, training loss: 10051.98, average training loss: 15963.42, base loss: 20521.75
[INFO 2017-06-26 12:26:42,068 main.py:50] epoch 141, training loss: 10099.33, average training loss: 15922.13, base loss: 20521.62
[INFO 2017-06-26 12:26:42,385 main.py:50] epoch 142, training loss: 10146.81, average training loss: 15881.74, base loss: 20521.79
[INFO 2017-06-26 12:26:42,711 main.py:50] epoch 143, training loss: 9944.19, average training loss: 15840.51, base loss: 20520.58
[INFO 2017-06-26 12:26:43,034 main.py:50] epoch 144, training loss: 10021.63, average training loss: 15800.38, base loss: 20519.80
[INFO 2017-06-26 12:26:43,357 main.py:50] epoch 145, training loss: 9993.39, average training loss: 15760.60, base loss: 20520.25
[INFO 2017-06-26 12:26:43,680 main.py:50] epoch 146, training loss: 10153.76, average training loss: 15722.46, base loss: 20521.89
[INFO 2017-06-26 12:26:44,004 main.py:50] epoch 147, training loss: 9848.11, average training loss: 15682.77, base loss: 20520.11
[INFO 2017-06-26 12:26:44,335 main.py:50] epoch 148, training loss: 10105.23, average training loss: 15645.34, base loss: 20520.57
[INFO 2017-06-26 12:26:44,673 main.py:50] epoch 149, training loss: 9827.64, average training loss: 15606.55, base loss: 20519.23
[INFO 2017-06-26 12:26:45,002 main.py:50] epoch 150, training loss: 9890.64, average training loss: 15568.70, base loss: 20519.37
[INFO 2017-06-26 12:26:45,346 main.py:50] epoch 151, training loss: 9811.23, average training loss: 15530.82, base loss: 20518.95
[INFO 2017-06-26 12:26:45,673 main.py:50] epoch 152, training loss: 9963.96, average training loss: 15494.44, base loss: 20522.15
[INFO 2017-06-26 12:26:46,001 main.py:50] epoch 153, training loss: 9813.87, average training loss: 15457.55, base loss: 20522.02
[INFO 2017-06-26 12:26:46,342 main.py:50] epoch 154, training loss: 9734.06, average training loss: 15420.62, base loss: 20521.03
[INFO 2017-06-26 12:26:46,668 main.py:50] epoch 155, training loss: 9834.66, average training loss: 15384.82, base loss: 20520.77
[INFO 2017-06-26 12:26:46,994 main.py:50] epoch 156, training loss: 9810.32, average training loss: 15349.31, base loss: 20520.49
[INFO 2017-06-26 12:26:47,321 main.py:50] epoch 157, training loss: 9583.35, average training loss: 15312.82, base loss: 20519.15
[INFO 2017-06-26 12:26:47,647 main.py:50] epoch 158, training loss: 9763.43, average training loss: 15277.91, base loss: 20519.11
[INFO 2017-06-26 12:26:47,974 main.py:50] epoch 159, training loss: 9763.08, average training loss: 15243.45, base loss: 20518.96
[INFO 2017-06-26 12:26:48,300 main.py:50] epoch 160, training loss: 9603.51, average training loss: 15208.42, base loss: 20517.55
[INFO 2017-06-26 12:26:48,626 main.py:50] epoch 161, training loss: 9593.08, average training loss: 15173.75, base loss: 20516.79
[INFO 2017-06-26 12:26:48,952 main.py:50] epoch 162, training loss: 9647.69, average training loss: 15139.85, base loss: 20516.66
[INFO 2017-06-26 12:26:49,282 main.py:50] epoch 163, training loss: 9906.70, average training loss: 15107.94, base loss: 20517.57
[INFO 2017-06-26 12:26:49,614 main.py:50] epoch 164, training loss: 9711.98, average training loss: 15075.24, base loss: 20516.93
[INFO 2017-06-26 12:26:49,945 main.py:50] epoch 165, training loss: 9740.14, average training loss: 15043.10, base loss: 20517.86
[INFO 2017-06-26 12:26:50,275 main.py:50] epoch 166, training loss: 9595.75, average training loss: 15010.48, base loss: 20517.40
[INFO 2017-06-26 12:26:50,607 main.py:50] epoch 167, training loss: 9734.45, average training loss: 14979.08, base loss: 20517.26
[INFO 2017-06-26 12:26:50,942 main.py:50] epoch 168, training loss: 9783.45, average training loss: 14948.33, base loss: 20518.92
[INFO 2017-06-26 12:26:51,281 main.py:50] epoch 169, training loss: 9653.43, average training loss: 14917.19, base loss: 20519.13
[INFO 2017-06-26 12:26:51,620 main.py:50] epoch 170, training loss: 9599.68, average training loss: 14886.09, base loss: 20519.24
[INFO 2017-06-26 12:26:51,963 main.py:50] epoch 171, training loss: 9600.19, average training loss: 14855.36, base loss: 20519.90
[INFO 2017-06-26 12:26:52,307 main.py:50] epoch 172, training loss: 9613.48, average training loss: 14825.06, base loss: 20519.92
[INFO 2017-06-26 12:26:52,652 main.py:50] epoch 173, training loss: 9708.48, average training loss: 14795.65, base loss: 20521.19
[INFO 2017-06-26 12:26:52,998 main.py:50] epoch 174, training loss: 9616.49, average training loss: 14766.06, base loss: 20519.77
[INFO 2017-06-26 12:26:53,348 main.py:50] epoch 175, training loss: 9527.76, average training loss: 14736.29, base loss: 20519.19
[INFO 2017-06-26 12:26:53,698 main.py:50] epoch 176, training loss: 9454.39, average training loss: 14706.45, base loss: 20518.09
[INFO 2017-06-26 12:26:54,056 main.py:50] epoch 177, training loss: 9625.58, average training loss: 14677.91, base loss: 20519.14
[INFO 2017-06-26 12:26:54,411 main.py:50] epoch 178, training loss: 9478.11, average training loss: 14648.86, base loss: 20517.98
[INFO 2017-06-26 12:26:54,765 main.py:50] epoch 179, training loss: 9538.14, average training loss: 14620.47, base loss: 20518.43
[INFO 2017-06-26 12:26:55,119 main.py:50] epoch 180, training loss: 9603.65, average training loss: 14592.75, base loss: 20519.68
[INFO 2017-06-26 12:26:55,480 main.py:50] epoch 181, training loss: 9449.21, average training loss: 14564.49, base loss: 20519.53
[INFO 2017-06-26 12:26:55,839 main.py:50] epoch 182, training loss: 9472.44, average training loss: 14536.66, base loss: 20519.42
[INFO 2017-06-26 12:26:56,199 main.py:50] epoch 183, training loss: 9460.37, average training loss: 14509.07, base loss: 20520.38
[INFO 2017-06-26 12:26:56,558 main.py:50] epoch 184, training loss: 9412.58, average training loss: 14481.52, base loss: 20519.89
[INFO 2017-06-26 12:26:56,918 main.py:50] epoch 185, training loss: 9330.07, average training loss: 14453.83, base loss: 20519.57
[INFO 2017-06-26 12:26:57,277 main.py:50] epoch 186, training loss: 9453.63, average training loss: 14427.09, base loss: 20519.13
[INFO 2017-06-26 12:26:57,637 main.py:50] epoch 187, training loss: 9333.37, average training loss: 14400.00, base loss: 20519.15
[INFO 2017-06-26 12:26:57,997 main.py:50] epoch 188, training loss: 9268.41, average training loss: 14372.84, base loss: 20518.56
[INFO 2017-06-26 12:26:58,357 main.py:50] epoch 189, training loss: 9387.79, average training loss: 14346.61, base loss: 20518.83
[INFO 2017-06-26 12:26:58,716 main.py:50] epoch 190, training loss: 9397.40, average training loss: 14320.69, base loss: 20519.41
[INFO 2017-06-26 12:26:59,076 main.py:50] epoch 191, training loss: 9394.44, average training loss: 14295.04, base loss: 20520.65
[INFO 2017-06-26 12:26:59,435 main.py:50] epoch 192, training loss: 9231.67, average training loss: 14268.80, base loss: 20520.30
[INFO 2017-06-26 12:26:59,796 main.py:50] epoch 193, training loss: 9234.07, average training loss: 14242.85, base loss: 20519.31
[INFO 2017-06-26 12:27:00,157 main.py:50] epoch 194, training loss: 9292.90, average training loss: 14217.47, base loss: 20520.06
[INFO 2017-06-26 12:27:00,518 main.py:50] epoch 195, training loss: 9248.63, average training loss: 14192.11, base loss: 20520.37
[INFO 2017-06-26 12:27:00,878 main.py:50] epoch 196, training loss: 9184.81, average training loss: 14166.70, base loss: 20519.55
[INFO 2017-06-26 12:27:01,238 main.py:50] epoch 197, training loss: 9407.13, average training loss: 14142.66, base loss: 20519.80
[INFO 2017-06-26 12:27:01,598 main.py:50] epoch 198, training loss: 9390.60, average training loss: 14118.78, base loss: 20519.92
[INFO 2017-06-26 12:27:01,964 main.py:50] epoch 199, training loss: 9280.51, average training loss: 14094.59, base loss: 20520.73
[INFO 2017-06-26 12:27:01,965 main.py:52] epoch 199, testing
[INFO 2017-06-26 12:27:03,473 main.py:103] average testing loss: 9186.65, base loss: 20459.96
[INFO 2017-06-26 12:27:03,474 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 12:27:03,483 main.py:76] current best accuracy: 9186.65
[INFO 2017-06-26 12:27:03,858 main.py:50] epoch 200, training loss: 9187.88, average training loss: 14070.18, base loss: 20520.60
[INFO 2017-06-26 12:27:04,218 main.py:50] epoch 201, training loss: 9260.22, average training loss: 14046.36, base loss: 20520.75
[INFO 2017-06-26 12:27:04,578 main.py:50] epoch 202, training loss: 9158.34, average training loss: 14022.29, base loss: 20520.00
[INFO 2017-06-26 12:27:04,938 main.py:50] epoch 203, training loss: 9252.81, average training loss: 13998.91, base loss: 20519.70
[INFO 2017-06-26 12:27:05,299 main.py:50] epoch 204, training loss: 9263.28, average training loss: 13975.80, base loss: 20521.18
[INFO 2017-06-26 12:27:05,660 main.py:50] epoch 205, training loss: 9267.41, average training loss: 13952.95, base loss: 20521.70
[INFO 2017-06-26 12:27:06,020 main.py:50] epoch 206, training loss: 9156.57, average training loss: 13929.78, base loss: 20521.98
[INFO 2017-06-26 12:27:06,380 main.py:50] epoch 207, training loss: 9218.53, average training loss: 13907.13, base loss: 20522.72
[INFO 2017-06-26 12:27:06,741 main.py:50] epoch 208, training loss: 9179.69, average training loss: 13884.51, base loss: 20522.32
[INFO 2017-06-26 12:27:07,102 main.py:50] epoch 209, training loss: 9126.77, average training loss: 13861.85, base loss: 20522.01
[INFO 2017-06-26 12:27:07,462 main.py:50] epoch 210, training loss: 9315.28, average training loss: 13840.30, base loss: 20522.21
[INFO 2017-06-26 12:27:07,825 main.py:50] epoch 211, training loss: 9177.00, average training loss: 13818.31, base loss: 20520.98
[INFO 2017-06-26 12:27:08,188 main.py:50] epoch 212, training loss: 9063.97, average training loss: 13795.99, base loss: 20520.90
[INFO 2017-06-26 12:27:08,567 main.py:50] epoch 213, training loss: 9264.57, average training loss: 13774.81, base loss: 20521.44
[INFO 2017-06-26 12:27:08,927 main.py:50] epoch 214, training loss: 9165.68, average training loss: 13753.37, base loss: 20519.92
[INFO 2017-06-26 12:27:09,288 main.py:50] epoch 215, training loss: 9311.62, average training loss: 13732.81, base loss: 20520.62
[INFO 2017-06-26 12:27:09,650 main.py:50] epoch 216, training loss: 9198.46, average training loss: 13711.91, base loss: 20522.20
[INFO 2017-06-26 12:27:10,071 main.py:50] epoch 217, training loss: 9145.62, average training loss: 13690.97, base loss: 20522.11
[INFO 2017-06-26 12:27:10,459 main.py:50] epoch 218, training loss: 8945.37, average training loss: 13669.30, base loss: 20521.32
[INFO 2017-06-26 12:27:10,855 main.py:50] epoch 219, training loss: 9139.47, average training loss: 13648.71, base loss: 20522.76
[INFO 2017-06-26 12:27:11,244 main.py:50] epoch 220, training loss: 8964.86, average training loss: 13627.52, base loss: 20520.73
[INFO 2017-06-26 12:27:11,606 main.py:50] epoch 221, training loss: 9034.26, average training loss: 13606.82, base loss: 20519.58
[INFO 2017-06-26 12:27:11,992 main.py:50] epoch 222, training loss: 9155.86, average training loss: 13586.87, base loss: 20519.43
[INFO 2017-06-26 12:27:12,370 main.py:50] epoch 223, training loss: 9101.08, average training loss: 13566.84, base loss: 20519.93
[INFO 2017-06-26 12:27:12,736 main.py:50] epoch 224, training loss: 9041.11, average training loss: 13546.73, base loss: 20520.32
[INFO 2017-06-26 12:27:13,121 main.py:50] epoch 225, training loss: 9227.94, average training loss: 13527.62, base loss: 20520.89
[INFO 2017-06-26 12:27:13,479 main.py:50] epoch 226, training loss: 8877.64, average training loss: 13507.13, base loss: 20520.10
[INFO 2017-06-26 12:27:13,839 main.py:50] epoch 227, training loss: 9042.76, average training loss: 13487.55, base loss: 20520.21
[INFO 2017-06-26 12:27:14,199 main.py:50] epoch 228, training loss: 9094.56, average training loss: 13468.37, base loss: 20520.99
[INFO 2017-06-26 12:27:14,559 main.py:50] epoch 229, training loss: 8994.29, average training loss: 13448.91, base loss: 20519.97
[INFO 2017-06-26 12:27:14,919 main.py:50] epoch 230, training loss: 8897.90, average training loss: 13429.21, base loss: 20518.11
[INFO 2017-06-26 12:27:15,280 main.py:50] epoch 231, training loss: 9198.11, average training loss: 13410.98, base loss: 20517.65
[INFO 2017-06-26 12:27:15,641 main.py:50] epoch 232, training loss: 8972.73, average training loss: 13391.93, base loss: 20517.86
[INFO 2017-06-26 12:27:16,002 main.py:50] epoch 233, training loss: 9107.53, average training loss: 13373.62, base loss: 20516.80
[INFO 2017-06-26 12:27:16,361 main.py:50] epoch 234, training loss: 9058.83, average training loss: 13355.26, base loss: 20517.68
[INFO 2017-06-26 12:27:16,719 main.py:50] epoch 235, training loss: 9059.19, average training loss: 13337.05, base loss: 20518.43
[INFO 2017-06-26 12:27:17,077 main.py:50] epoch 236, training loss: 8944.88, average training loss: 13318.52, base loss: 20518.26
[INFO 2017-06-26 12:27:17,436 main.py:50] epoch 237, training loss: 8993.29, average training loss: 13300.35, base loss: 20517.88
[INFO 2017-06-26 12:27:17,794 main.py:50] epoch 238, training loss: 8990.92, average training loss: 13282.32, base loss: 20517.57
[INFO 2017-06-26 12:27:18,184 main.py:50] epoch 239, training loss: 8937.89, average training loss: 13264.22, base loss: 20517.19
[INFO 2017-06-26 12:27:18,561 main.py:50] epoch 240, training loss: 8916.96, average training loss: 13246.18, base loss: 20516.20
[INFO 2017-06-26 12:27:18,941 main.py:50] epoch 241, training loss: 8937.28, average training loss: 13228.37, base loss: 20516.42
[INFO 2017-06-26 12:27:19,302 main.py:50] epoch 242, training loss: 8774.76, average training loss: 13210.04, base loss: 20515.13
[INFO 2017-06-26 12:27:19,661 main.py:50] epoch 243, training loss: 8907.98, average training loss: 13192.41, base loss: 20515.18
[INFO 2017-06-26 12:27:20,053 main.py:50] epoch 244, training loss: 8818.43, average training loss: 13174.56, base loss: 20515.60
[INFO 2017-06-26 12:27:20,433 main.py:50] epoch 245, training loss: 9026.43, average training loss: 13157.70, base loss: 20515.13
[INFO 2017-06-26 12:27:20,804 main.py:50] epoch 246, training loss: 8830.41, average training loss: 13140.18, base loss: 20514.41
[INFO 2017-06-26 12:27:21,168 main.py:50] epoch 247, training loss: 8851.12, average training loss: 13122.88, base loss: 20513.78
[INFO 2017-06-26 12:27:21,527 main.py:50] epoch 248, training loss: 8815.87, average training loss: 13105.59, base loss: 20513.10
[INFO 2017-06-26 12:27:21,886 main.py:50] epoch 249, training loss: 8991.91, average training loss: 13089.13, base loss: 20513.03
[INFO 2017-06-26 12:27:22,244 main.py:50] epoch 250, training loss: 9008.94, average training loss: 13072.88, base loss: 20513.30
[INFO 2017-06-26 12:27:22,602 main.py:50] epoch 251, training loss: 8928.11, average training loss: 13056.43, base loss: 20513.34
[INFO 2017-06-26 12:27:22,994 main.py:50] epoch 252, training loss: 8933.11, average training loss: 13040.13, base loss: 20513.42
[INFO 2017-06-26 12:27:23,373 main.py:50] epoch 253, training loss: 8885.49, average training loss: 13023.77, base loss: 20513.41
[INFO 2017-06-26 12:27:23,736 main.py:50] epoch 254, training loss: 8847.27, average training loss: 13007.39, base loss: 20513.42
[INFO 2017-06-26 12:27:24,097 main.py:50] epoch 255, training loss: 8942.96, average training loss: 12991.52, base loss: 20513.81
[INFO 2017-06-26 12:27:24,457 main.py:50] epoch 256, training loss: 8913.77, average training loss: 12975.65, base loss: 20514.47
[INFO 2017-06-26 12:27:24,851 main.py:50] epoch 257, training loss: 8922.26, average training loss: 12959.94, base loss: 20515.35
[INFO 2017-06-26 12:27:25,212 main.py:50] epoch 258, training loss: 8830.03, average training loss: 12943.99, base loss: 20514.71
[INFO 2017-06-26 12:27:25,575 main.py:50] epoch 259, training loss: 8875.02, average training loss: 12928.35, base loss: 20514.21
[INFO 2017-06-26 12:27:25,936 main.py:50] epoch 260, training loss: 8819.48, average training loss: 12912.60, base loss: 20513.97
[INFO 2017-06-26 12:27:26,296 main.py:50] epoch 261, training loss: 8789.13, average training loss: 12896.86, base loss: 20513.55
[INFO 2017-06-26 12:27:26,655 main.py:50] epoch 262, training loss: 8783.06, average training loss: 12881.22, base loss: 20512.23
[INFO 2017-06-26 12:27:27,045 main.py:50] epoch 263, training loss: 8688.62, average training loss: 12865.34, base loss: 20510.68
[INFO 2017-06-26 12:27:27,418 main.py:50] epoch 264, training loss: 8836.79, average training loss: 12850.14, base loss: 20511.12
[INFO 2017-06-26 12:27:27,780 main.py:50] epoch 265, training loss: 8823.16, average training loss: 12835.00, base loss: 20511.60
[INFO 2017-06-26 12:27:28,141 main.py:50] epoch 266, training loss: 8889.22, average training loss: 12820.22, base loss: 20512.59
[INFO 2017-06-26 12:27:28,500 main.py:50] epoch 267, training loss: 8779.25, average training loss: 12805.14, base loss: 20512.93
[INFO 2017-06-26 12:27:28,859 main.py:50] epoch 268, training loss: 8771.36, average training loss: 12790.15, base loss: 20512.45
[INFO 2017-06-26 12:27:29,219 main.py:50] epoch 269, training loss: 8666.83, average training loss: 12774.88, base loss: 20512.48
[INFO 2017-06-26 12:27:29,578 main.py:50] epoch 270, training loss: 8880.83, average training loss: 12760.51, base loss: 20512.36
[INFO 2017-06-26 12:27:29,937 main.py:50] epoch 271, training loss: 8725.45, average training loss: 12745.67, base loss: 20511.44
[INFO 2017-06-26 12:27:30,329 main.py:50] epoch 272, training loss: 8602.31, average training loss: 12730.50, base loss: 20510.74
[INFO 2017-06-26 12:27:30,724 main.py:50] epoch 273, training loss: 8668.43, average training loss: 12715.67, base loss: 20510.14
[INFO 2017-06-26 12:27:31,090 main.py:50] epoch 274, training loss: 8789.79, average training loss: 12701.39, base loss: 20510.29
[INFO 2017-06-26 12:27:31,450 main.py:50] epoch 275, training loss: 8831.02, average training loss: 12687.37, base loss: 20510.68
[INFO 2017-06-26 12:27:31,808 main.py:50] epoch 276, training loss: 8513.54, average training loss: 12672.30, base loss: 20509.23
[INFO 2017-06-26 12:27:32,166 main.py:50] epoch 277, training loss: 8833.71, average training loss: 12658.50, base loss: 20509.65
[INFO 2017-06-26 12:27:32,558 main.py:50] epoch 278, training loss: 8691.95, average training loss: 12644.28, base loss: 20509.49
[INFO 2017-06-26 12:27:32,939 main.py:50] epoch 279, training loss: 8746.24, average training loss: 12630.36, base loss: 20509.68
[INFO 2017-06-26 12:27:33,312 main.py:50] epoch 280, training loss: 8853.02, average training loss: 12616.91, base loss: 20511.35
[INFO 2017-06-26 12:27:33,673 main.py:50] epoch 281, training loss: 8618.21, average training loss: 12602.73, base loss: 20511.29
[INFO 2017-06-26 12:27:34,031 main.py:50] epoch 282, training loss: 8585.63, average training loss: 12588.54, base loss: 20510.23
[INFO 2017-06-26 12:27:34,390 main.py:50] epoch 283, training loss: 8765.82, average training loss: 12575.08, base loss: 20509.88
[INFO 2017-06-26 12:27:34,750 main.py:50] epoch 284, training loss: 8588.34, average training loss: 12561.09, base loss: 20509.32
[INFO 2017-06-26 12:27:35,146 main.py:50] epoch 285, training loss: 8605.39, average training loss: 12547.26, base loss: 20508.59
[INFO 2017-06-26 12:27:35,524 main.py:50] epoch 286, training loss: 8699.05, average training loss: 12533.85, base loss: 20509.51
[INFO 2017-06-26 12:27:35,886 main.py:50] epoch 287, training loss: 8790.31, average training loss: 12520.85, base loss: 20510.14
[INFO 2017-06-26 12:27:36,247 main.py:50] epoch 288, training loss: 8633.79, average training loss: 12507.40, base loss: 20509.80
[INFO 2017-06-26 12:27:36,606 main.py:50] epoch 289, training loss: 8667.07, average training loss: 12494.16, base loss: 20510.13
[INFO 2017-06-26 12:27:36,998 main.py:50] epoch 290, training loss: 8735.82, average training loss: 12481.25, base loss: 20510.57
[INFO 2017-06-26 12:27:37,371 main.py:50] epoch 291, training loss: 8616.50, average training loss: 12468.01, base loss: 20510.36
[INFO 2017-06-26 12:27:37,744 main.py:50] epoch 292, training loss: 8535.03, average training loss: 12454.59, base loss: 20510.31
[INFO 2017-06-26 12:27:38,120 main.py:50] epoch 293, training loss: 8567.40, average training loss: 12441.36, base loss: 20510.07
[INFO 2017-06-26 12:27:38,484 main.py:50] epoch 294, training loss: 8562.52, average training loss: 12428.22, base loss: 20509.48
[INFO 2017-06-26 12:27:38,847 main.py:50] epoch 295, training loss: 8615.32, average training loss: 12415.33, base loss: 20510.45
[INFO 2017-06-26 12:27:39,207 main.py:50] epoch 296, training loss: 8587.51, average training loss: 12402.45, base loss: 20510.13
[INFO 2017-06-26 12:27:39,567 main.py:50] epoch 297, training loss: 8444.96, average training loss: 12389.17, base loss: 20509.26
[INFO 2017-06-26 12:27:39,928 main.py:50] epoch 298, training loss: 8634.98, average training loss: 12376.61, base loss: 20509.88
[INFO 2017-06-26 12:27:40,287 main.py:50] epoch 299, training loss: 8570.59, average training loss: 12363.92, base loss: 20510.73
[INFO 2017-06-26 12:27:40,287 main.py:52] epoch 299, testing
[INFO 2017-06-26 12:27:41,758 main.py:103] average testing loss: 8544.47, base loss: 20468.53
[INFO 2017-06-26 12:27:41,759 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 12:27:41,765 main.py:76] current best accuracy: 8544.47
[INFO 2017-06-26 12:27:42,124 main.py:50] epoch 300, training loss: 8565.30, average training loss: 12351.30, base loss: 20509.57
[INFO 2017-06-26 12:27:42,482 main.py:50] epoch 301, training loss: 8492.21, average training loss: 12338.53, base loss: 20509.56
[INFO 2017-06-26 12:27:42,841 main.py:50] epoch 302, training loss: 8464.67, average training loss: 12325.74, base loss: 20508.59
[INFO 2017-06-26 12:27:43,199 main.py:50] epoch 303, training loss: 8350.89, average training loss: 12312.67, base loss: 20507.93
[INFO 2017-06-26 12:27:43,557 main.py:50] epoch 304, training loss: 8632.43, average training loss: 12300.60, base loss: 20508.29
[INFO 2017-06-26 12:27:43,915 main.py:50] epoch 305, training loss: 8334.94, average training loss: 12287.64, base loss: 20507.27
[INFO 2017-06-26 12:27:44,276 main.py:50] epoch 306, training loss: 8579.11, average training loss: 12275.56, base loss: 20507.39
[INFO 2017-06-26 12:27:44,635 main.py:50] epoch 307, training loss: 8415.00, average training loss: 12263.02, base loss: 20506.80
[INFO 2017-06-26 12:27:44,994 main.py:50] epoch 308, training loss: 8452.08, average training loss: 12250.69, base loss: 20507.01
[INFO 2017-06-26 12:27:45,354 main.py:50] epoch 309, training loss: 8507.88, average training loss: 12238.62, base loss: 20507.95
[INFO 2017-06-26 12:27:45,714 main.py:50] epoch 310, training loss: 8360.49, average training loss: 12226.15, base loss: 20508.33
[INFO 2017-06-26 12:27:46,073 main.py:50] epoch 311, training loss: 8392.10, average training loss: 12213.86, base loss: 20507.21
[INFO 2017-06-26 12:27:46,433 main.py:50] epoch 312, training loss: 8340.67, average training loss: 12201.49, base loss: 20506.94
[INFO 2017-06-26 12:27:46,793 main.py:50] epoch 313, training loss: 8414.78, average training loss: 12189.43, base loss: 20506.47
[INFO 2017-06-26 12:27:47,152 main.py:50] epoch 314, training loss: 8362.42, average training loss: 12177.28, base loss: 20506.30
[INFO 2017-06-26 12:27:47,513 main.py:50] epoch 315, training loss: 8455.85, average training loss: 12165.50, base loss: 20506.41
[INFO 2017-06-26 12:27:47,872 main.py:50] epoch 316, training loss: 8467.37, average training loss: 12153.83, base loss: 20506.02
[INFO 2017-06-26 12:27:48,236 main.py:50] epoch 317, training loss: 8287.00, average training loss: 12141.67, base loss: 20505.78
[INFO 2017-06-26 12:27:48,596 main.py:50] epoch 318, training loss: 8534.31, average training loss: 12130.37, base loss: 20506.71
[INFO 2017-06-26 12:27:48,957 main.py:50] epoch 319, training loss: 8236.25, average training loss: 12118.20, base loss: 20506.56
[INFO 2017-06-26 12:27:49,318 main.py:50] epoch 320, training loss: 8415.84, average training loss: 12106.66, base loss: 20505.97
[INFO 2017-06-26 12:27:49,678 main.py:50] epoch 321, training loss: 8296.09, average training loss: 12094.83, base loss: 20505.65
[INFO 2017-06-26 12:27:50,038 main.py:50] epoch 322, training loss: 8356.34, average training loss: 12083.25, base loss: 20506.14
[INFO 2017-06-26 12:27:50,418 main.py:50] epoch 323, training loss: 8347.56, average training loss: 12071.72, base loss: 20505.86
[INFO 2017-06-26 12:27:50,779 main.py:50] epoch 324, training loss: 8412.09, average training loss: 12060.46, base loss: 20505.98
[INFO 2017-06-26 12:27:51,141 main.py:50] epoch 325, training loss: 8368.10, average training loss: 12049.14, base loss: 20506.06
[INFO 2017-06-26 12:27:51,502 main.py:50] epoch 326, training loss: 8241.58, average training loss: 12037.49, base loss: 20505.82
[INFO 2017-06-26 12:27:51,864 main.py:50] epoch 327, training loss: 8434.97, average training loss: 12026.51, base loss: 20506.58
[INFO 2017-06-26 12:27:52,228 main.py:50] epoch 328, training loss: 8455.13, average training loss: 12015.66, base loss: 20507.06
[INFO 2017-06-26 12:27:52,617 main.py:50] epoch 329, training loss: 8253.65, average training loss: 12004.26, base loss: 20506.69
[INFO 2017-06-26 12:27:52,998 main.py:50] epoch 330, training loss: 8331.84, average training loss: 11993.16, base loss: 20506.46
[INFO 2017-06-26 12:27:53,387 main.py:50] epoch 331, training loss: 8462.85, average training loss: 11982.53, base loss: 20507.21
[INFO 2017-06-26 12:27:53,775 main.py:50] epoch 332, training loss: 8476.88, average training loss: 11972.00, base loss: 20507.75
[INFO 2017-06-26 12:27:54,141 main.py:50] epoch 333, training loss: 8415.50, average training loss: 11961.35, base loss: 20508.59
[INFO 2017-06-26 12:27:54,536 main.py:50] epoch 334, training loss: 8422.75, average training loss: 11950.79, base loss: 20508.54
[INFO 2017-06-26 12:27:54,898 main.py:50] epoch 335, training loss: 8423.85, average training loss: 11940.29, base loss: 20507.96
[INFO 2017-06-26 12:27:55,266 main.py:50] epoch 336, training loss: 8364.94, average training loss: 11929.68, base loss: 20508.24
[INFO 2017-06-26 12:27:55,628 main.py:50] epoch 337, training loss: 8272.60, average training loss: 11918.86, base loss: 20507.75
[INFO 2017-06-26 12:27:55,990 main.py:50] epoch 338, training loss: 8345.73, average training loss: 11908.32, base loss: 20508.03
[INFO 2017-06-26 12:27:56,351 main.py:50] epoch 339, training loss: 8280.09, average training loss: 11897.65, base loss: 20507.49
[INFO 2017-06-26 12:27:56,714 main.py:50] epoch 340, training loss: 8466.69, average training loss: 11887.59, base loss: 20507.87
[INFO 2017-06-26 12:27:57,077 main.py:50] epoch 341, training loss: 8348.07, average training loss: 11877.24, base loss: 20507.82
[INFO 2017-06-26 12:27:57,437 main.py:50] epoch 342, training loss: 8209.53, average training loss: 11866.55, base loss: 20507.42
[INFO 2017-06-26 12:27:57,798 main.py:50] epoch 343, training loss: 8247.39, average training loss: 11856.03, base loss: 20506.53
[INFO 2017-06-26 12:27:58,158 main.py:50] epoch 344, training loss: 8290.74, average training loss: 11845.69, base loss: 20506.50
[INFO 2017-06-26 12:27:58,518 main.py:50] epoch 345, training loss: 8394.82, average training loss: 11835.72, base loss: 20506.38
[INFO 2017-06-26 12:27:58,879 main.py:50] epoch 346, training loss: 8321.05, average training loss: 11825.59, base loss: 20505.82
[INFO 2017-06-26 12:27:59,241 main.py:50] epoch 347, training loss: 8426.97, average training loss: 11815.82, base loss: 20506.38
[INFO 2017-06-26 12:27:59,602 main.py:50] epoch 348, training loss: 8263.66, average training loss: 11805.65, base loss: 20506.05
[INFO 2017-06-26 12:27:59,963 main.py:50] epoch 349, training loss: 8194.76, average training loss: 11795.33, base loss: 20505.16
[INFO 2017-06-26 12:28:00,324 main.py:50] epoch 350, training loss: 8305.44, average training loss: 11785.39, base loss: 20505.35
[INFO 2017-06-26 12:28:00,684 main.py:50] epoch 351, training loss: 8349.49, average training loss: 11775.62, base loss: 20505.35
[INFO 2017-06-26 12:28:01,044 main.py:50] epoch 352, training loss: 8418.20, average training loss: 11766.11, base loss: 20506.00
[INFO 2017-06-26 12:28:01,403 main.py:50] epoch 353, training loss: 8349.89, average training loss: 11756.46, base loss: 20506.16
[INFO 2017-06-26 12:28:01,762 main.py:50] epoch 354, training loss: 8241.24, average training loss: 11746.56, base loss: 20506.01
[INFO 2017-06-26 12:28:02,121 main.py:50] epoch 355, training loss: 8147.93, average training loss: 11736.45, base loss: 20505.04
[INFO 2017-06-26 12:28:02,481 main.py:50] epoch 356, training loss: 8303.42, average training loss: 11726.84, base loss: 20505.27
[INFO 2017-06-26 12:28:02,842 main.py:50] epoch 357, training loss: 8260.02, average training loss: 11717.15, base loss: 20505.64
[INFO 2017-06-26 12:28:03,203 main.py:50] epoch 358, training loss: 8316.53, average training loss: 11707.68, base loss: 20505.99
[INFO 2017-06-26 12:28:03,563 main.py:50] epoch 359, training loss: 8386.43, average training loss: 11698.45, base loss: 20505.92
[INFO 2017-06-26 12:28:03,924 main.py:50] epoch 360, training loss: 8253.92, average training loss: 11688.91, base loss: 20505.94
[INFO 2017-06-26 12:28:04,283 main.py:50] epoch 361, training loss: 8161.05, average training loss: 11679.17, base loss: 20505.47
[INFO 2017-06-26 12:28:04,643 main.py:50] epoch 362, training loss: 8291.45, average training loss: 11669.83, base loss: 20505.69
[INFO 2017-06-26 12:28:05,003 main.py:50] epoch 363, training loss: 8224.14, average training loss: 11660.37, base loss: 20505.17
[INFO 2017-06-26 12:28:05,363 main.py:50] epoch 364, training loss: 8242.55, average training loss: 11651.00, base loss: 20505.63
[INFO 2017-06-26 12:28:05,723 main.py:50] epoch 365, training loss: 8143.88, average training loss: 11641.42, base loss: 20505.60
[INFO 2017-06-26 12:28:06,115 main.py:50] epoch 366, training loss: 8160.40, average training loss: 11631.94, base loss: 20504.91
[INFO 2017-06-26 12:28:06,528 main.py:50] epoch 367, training loss: 8274.64, average training loss: 11622.81, base loss: 20505.75
[INFO 2017-06-26 12:28:06,890 main.py:50] epoch 368, training loss: 8210.60, average training loss: 11613.57, base loss: 20505.75
[INFO 2017-06-26 12:28:07,253 main.py:50] epoch 369, training loss: 8311.53, average training loss: 11604.64, base loss: 20506.15
[INFO 2017-06-26 12:28:07,616 main.py:50] epoch 370, training loss: 8241.68, average training loss: 11595.58, base loss: 20505.94
[INFO 2017-06-26 12:28:07,978 main.py:50] epoch 371, training loss: 8178.77, average training loss: 11586.39, base loss: 20505.84
[INFO 2017-06-26 12:28:08,342 main.py:50] epoch 372, training loss: 8276.12, average training loss: 11577.52, base loss: 20506.10
[INFO 2017-06-26 12:28:08,704 main.py:50] epoch 373, training loss: 8278.59, average training loss: 11568.70, base loss: 20506.21
[INFO 2017-06-26 12:28:09,064 main.py:50] epoch 374, training loss: 8270.58, average training loss: 11559.90, base loss: 20507.71
[INFO 2017-06-26 12:28:09,426 main.py:50] epoch 375, training loss: 8055.96, average training loss: 11550.58, base loss: 20507.53
[INFO 2017-06-26 12:28:09,795 main.py:50] epoch 376, training loss: 8114.77, average training loss: 11541.47, base loss: 20507.35
[INFO 2017-06-26 12:28:10,154 main.py:50] epoch 377, training loss: 8174.45, average training loss: 11532.56, base loss: 20507.32
[INFO 2017-06-26 12:28:10,516 main.py:50] epoch 378, training loss: 8262.18, average training loss: 11523.93, base loss: 20507.84
[INFO 2017-06-26 12:28:10,878 main.py:50] epoch 379, training loss: 8094.00, average training loss: 11514.91, base loss: 20507.35
[INFO 2017-06-26 12:28:11,238 main.py:50] epoch 380, training loss: 8171.62, average training loss: 11506.13, base loss: 20507.77
[INFO 2017-06-26 12:28:11,601 main.py:50] epoch 381, training loss: 8259.57, average training loss: 11497.63, base loss: 20508.28
[INFO 2017-06-26 12:28:11,961 main.py:50] epoch 382, training loss: 8079.02, average training loss: 11488.71, base loss: 20508.14
[INFO 2017-06-26 12:28:12,322 main.py:50] epoch 383, training loss: 8271.01, average training loss: 11480.33, base loss: 20508.52
[INFO 2017-06-26 12:28:12,683 main.py:50] epoch 384, training loss: 8122.04, average training loss: 11471.61, base loss: 20508.50
[INFO 2017-06-26 12:28:13,044 main.py:50] epoch 385, training loss: 8063.20, average training loss: 11462.78, base loss: 20507.83
[INFO 2017-06-26 12:28:13,406 main.py:50] epoch 386, training loss: 8329.22, average training loss: 11454.68, base loss: 20508.96
[INFO 2017-06-26 12:28:13,768 main.py:50] epoch 387, training loss: 8135.58, average training loss: 11446.12, base loss: 20509.23
[INFO 2017-06-26 12:28:14,129 main.py:50] epoch 388, training loss: 8152.77, average training loss: 11437.66, base loss: 20509.10
[INFO 2017-06-26 12:28:14,489 main.py:50] epoch 389, training loss: 8210.38, average training loss: 11429.38, base loss: 20509.34
[INFO 2017-06-26 12:28:14,849 main.py:50] epoch 390, training loss: 8275.30, average training loss: 11421.32, base loss: 20510.32
[INFO 2017-06-26 12:28:15,209 main.py:50] epoch 391, training loss: 8027.22, average training loss: 11412.66, base loss: 20509.54
[INFO 2017-06-26 12:28:15,588 main.py:50] epoch 392, training loss: 8042.33, average training loss: 11404.08, base loss: 20509.39
[INFO 2017-06-26 12:28:15,951 main.py:50] epoch 393, training loss: 8113.12, average training loss: 11395.73, base loss: 20509.17
[INFO 2017-06-26 12:28:16,312 main.py:50] epoch 394, training loss: 8202.21, average training loss: 11387.64, base loss: 20509.15
[INFO 2017-06-26 12:28:16,672 main.py:50] epoch 395, training loss: 8118.92, average training loss: 11379.39, base loss: 20508.81
[INFO 2017-06-26 12:28:17,032 main.py:50] epoch 396, training loss: 8057.25, average training loss: 11371.02, base loss: 20508.80
[INFO 2017-06-26 12:28:17,392 main.py:50] epoch 397, training loss: 8001.78, average training loss: 11362.56, base loss: 20508.34
[INFO 2017-06-26 12:28:17,752 main.py:50] epoch 398, training loss: 8153.07, average training loss: 11354.51, base loss: 20508.76
[INFO 2017-06-26 12:28:18,113 main.py:50] epoch 399, training loss: 8032.38, average training loss: 11346.21, base loss: 20508.38
[INFO 2017-06-26 12:28:18,113 main.py:52] epoch 399, testing
[INFO 2017-06-26 12:28:19,584 main.py:103] average testing loss: 8135.25, base loss: 20419.49
[INFO 2017-06-26 12:28:19,585 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 12:28:19,591 main.py:76] current best accuracy: 8135.25
[INFO 2017-06-26 12:28:19,950 main.py:50] epoch 400, training loss: 8210.97, average training loss: 11338.39, base loss: 20508.08
[INFO 2017-06-26 12:28:20,308 main.py:50] epoch 401, training loss: 8143.30, average training loss: 11330.44, base loss: 20507.86
[INFO 2017-06-26 12:28:20,668 main.py:50] epoch 402, training loss: 8082.76, average training loss: 11322.38, base loss: 20507.68
[INFO 2017-06-26 12:28:21,028 main.py:50] epoch 403, training loss: 8195.77, average training loss: 11314.64, base loss: 20507.89
[INFO 2017-06-26 12:28:21,389 main.py:50] epoch 404, training loss: 8130.22, average training loss: 11306.78, base loss: 20507.89
[INFO 2017-06-26 12:28:21,749 main.py:50] epoch 405, training loss: 8143.68, average training loss: 11298.99, base loss: 20508.57
[INFO 2017-06-26 12:28:22,109 main.py:50] epoch 406, training loss: 8192.15, average training loss: 11291.36, base loss: 20508.64
[INFO 2017-06-26 12:28:22,470 main.py:50] epoch 407, training loss: 8051.48, average training loss: 11283.41, base loss: 20508.43
[INFO 2017-06-26 12:28:22,830 main.py:50] epoch 408, training loss: 8172.04, average training loss: 11275.81, base loss: 20508.54
[INFO 2017-06-26 12:28:23,191 main.py:50] epoch 409, training loss: 8074.79, average training loss: 11268.00, base loss: 20508.63
[INFO 2017-06-26 12:28:23,552 main.py:50] epoch 410, training loss: 8106.14, average training loss: 11260.31, base loss: 20508.93
[INFO 2017-06-26 12:28:23,912 main.py:50] epoch 411, training loss: 8113.39, average training loss: 11252.67, base loss: 20508.56
[INFO 2017-06-26 12:28:24,286 main.py:50] epoch 412, training loss: 8176.37, average training loss: 11245.22, base loss: 20508.73
[INFO 2017-06-26 12:28:24,646 main.py:50] epoch 413, training loss: 7982.96, average training loss: 11237.34, base loss: 20508.22
[INFO 2017-06-26 12:28:25,008 main.py:50] epoch 414, training loss: 8249.18, average training loss: 11230.14, base loss: 20509.64
[INFO 2017-06-26 12:28:25,405 main.py:50] epoch 415, training loss: 8077.26, average training loss: 11222.56, base loss: 20509.78
[INFO 2017-06-26 12:28:25,774 main.py:50] epoch 416, training loss: 8118.24, average training loss: 11215.12, base loss: 20510.39
[INFO 2017-06-26 12:28:26,135 main.py:50] epoch 417, training loss: 7993.25, average training loss: 11207.41, base loss: 20510.42
[INFO 2017-06-26 12:28:26,531 main.py:50] epoch 418, training loss: 7963.94, average training loss: 11199.67, base loss: 20510.36
[INFO 2017-06-26 12:28:26,897 main.py:50] epoch 419, training loss: 8006.03, average training loss: 11192.06, base loss: 20510.20
[INFO 2017-06-26 12:28:27,258 main.py:50] epoch 420, training loss: 8080.27, average training loss: 11184.67, base loss: 20510.33
[INFO 2017-06-26 12:28:27,617 main.py:50] epoch 421, training loss: 7908.22, average training loss: 11176.91, base loss: 20509.51
[INFO 2017-06-26 12:28:27,975 main.py:50] epoch 422, training loss: 8050.00, average training loss: 11169.52, base loss: 20509.52
[INFO 2017-06-26 12:28:28,335 main.py:50] epoch 423, training loss: 7866.76, average training loss: 11161.73, base loss: 20509.09
[INFO 2017-06-26 12:28:28,694 main.py:50] epoch 424, training loss: 7986.92, average training loss: 11154.26, base loss: 20509.21
[INFO 2017-06-26 12:28:29,054 main.py:50] epoch 425, training loss: 7970.95, average training loss: 11146.78, base loss: 20509.36
[INFO 2017-06-26 12:28:29,414 main.py:50] epoch 426, training loss: 7930.13, average training loss: 11139.25, base loss: 20509.22
[INFO 2017-06-26 12:28:29,774 main.py:50] epoch 427, training loss: 7955.26, average training loss: 11131.81, base loss: 20509.59
[INFO 2017-06-26 12:28:30,133 main.py:50] epoch 428, training loss: 7940.48, average training loss: 11124.37, base loss: 20509.81
[INFO 2017-06-26 12:28:30,493 main.py:50] epoch 429, training loss: 8046.26, average training loss: 11117.21, base loss: 20510.59
[INFO 2017-06-26 12:28:30,852 main.py:50] epoch 430, training loss: 8033.39, average training loss: 11110.06, base loss: 20511.06
[INFO 2017-06-26 12:28:31,211 main.py:50] epoch 431, training loss: 7929.92, average training loss: 11102.70, base loss: 20510.85
[INFO 2017-06-26 12:28:31,570 main.py:50] epoch 432, training loss: 7890.49, average training loss: 11095.28, base loss: 20510.24
[INFO 2017-06-26 12:28:31,930 main.py:50] epoch 433, training loss: 7976.70, average training loss: 11088.09, base loss: 20510.32
[INFO 2017-06-26 12:28:32,289 main.py:50] epoch 434, training loss: 8110.15, average training loss: 11081.25, base loss: 20510.04
[INFO 2017-06-26 12:28:32,647 main.py:50] epoch 435, training loss: 8152.91, average training loss: 11074.53, base loss: 20510.50
[INFO 2017-06-26 12:28:33,006 main.py:50] epoch 436, training loss: 7864.35, average training loss: 11067.18, base loss: 20509.96
[INFO 2017-06-26 12:28:33,365 main.py:50] epoch 437, training loss: 8022.02, average training loss: 11060.23, base loss: 20509.96
[INFO 2017-06-26 12:28:33,725 main.py:50] epoch 438, training loss: 8002.11, average training loss: 11053.27, base loss: 20509.29
[INFO 2017-06-26 12:28:34,084 main.py:50] epoch 439, training loss: 8098.92, average training loss: 11046.55, base loss: 20509.33
[INFO 2017-06-26 12:28:34,443 main.py:50] epoch 440, training loss: 8026.34, average training loss: 11039.70, base loss: 20508.92
[INFO 2017-06-26 12:28:34,803 main.py:50] epoch 441, training loss: 8006.08, average training loss: 11032.84, base loss: 20509.34
[INFO 2017-06-26 12:28:35,162 main.py:50] epoch 442, training loss: 8101.59, average training loss: 11026.22, base loss: 20509.65
[INFO 2017-06-26 12:28:35,522 main.py:50] epoch 443, training loss: 8048.19, average training loss: 11019.52, base loss: 20510.03
[INFO 2017-06-26 12:28:35,880 main.py:50] epoch 444, training loss: 7883.82, average training loss: 11012.47, base loss: 20509.73
[INFO 2017-06-26 12:28:36,239 main.py:50] epoch 445, training loss: 7911.62, average training loss: 11005.52, base loss: 20510.56
[INFO 2017-06-26 12:28:36,600 main.py:50] epoch 446, training loss: 7918.71, average training loss: 10998.61, base loss: 20510.73
[INFO 2017-06-26 12:28:36,958 main.py:50] epoch 447, training loss: 7943.20, average training loss: 10991.79, base loss: 20510.85
[INFO 2017-06-26 12:28:37,316 main.py:50] epoch 448, training loss: 7861.14, average training loss: 10984.82, base loss: 20510.69
[INFO 2017-06-26 12:28:37,674 main.py:50] epoch 449, training loss: 7962.85, average training loss: 10978.10, base loss: 20510.52
[INFO 2017-06-26 12:28:38,034 main.py:50] epoch 450, training loss: 7885.89, average training loss: 10971.25, base loss: 20510.43
[INFO 2017-06-26 12:28:38,394 main.py:50] epoch 451, training loss: 7883.68, average training loss: 10964.42, base loss: 20509.76
[INFO 2017-06-26 12:28:38,752 main.py:50] epoch 452, training loss: 8012.20, average training loss: 10957.90, base loss: 20510.12
[INFO 2017-06-26 12:28:39,110 main.py:50] epoch 453, training loss: 7916.79, average training loss: 10951.20, base loss: 20510.38
[INFO 2017-06-26 12:28:39,471 main.py:50] epoch 454, training loss: 7979.29, average training loss: 10944.67, base loss: 20509.76
[INFO 2017-06-26 12:28:39,830 main.py:50] epoch 455, training loss: 7920.71, average training loss: 10938.04, base loss: 20509.82
[INFO 2017-06-26 12:28:40,191 main.py:50] epoch 456, training loss: 7852.13, average training loss: 10931.28, base loss: 20509.66
[INFO 2017-06-26 12:28:40,551 main.py:50] epoch 457, training loss: 7874.78, average training loss: 10924.61, base loss: 20509.70
[INFO 2017-06-26 12:28:40,910 main.py:50] epoch 458, training loss: 7828.41, average training loss: 10917.87, base loss: 20509.78
[INFO 2017-06-26 12:28:41,269 main.py:50] epoch 459, training loss: 7876.93, average training loss: 10911.25, base loss: 20509.78
[INFO 2017-06-26 12:28:41,628 main.py:50] epoch 460, training loss: 7829.15, average training loss: 10904.57, base loss: 20509.63
[INFO 2017-06-26 12:28:41,987 main.py:50] epoch 461, training loss: 7809.62, average training loss: 10897.87, base loss: 20509.21
[INFO 2017-06-26 12:28:42,345 main.py:50] epoch 462, training loss: 7822.69, average training loss: 10891.23, base loss: 20509.19
[INFO 2017-06-26 12:28:42,704 main.py:50] epoch 463, training loss: 7841.79, average training loss: 10884.66, base loss: 20508.90
[INFO 2017-06-26 12:28:43,065 main.py:50] epoch 464, training loss: 7788.93, average training loss: 10878.00, base loss: 20508.99
[INFO 2017-06-26 12:28:43,423 main.py:50] epoch 465, training loss: 7853.87, average training loss: 10871.51, base loss: 20509.38
[INFO 2017-06-26 12:28:43,782 main.py:50] epoch 466, training loss: 7887.15, average training loss: 10865.12, base loss: 20509.28
[INFO 2017-06-26 12:28:44,142 main.py:50] epoch 467, training loss: 7908.08, average training loss: 10858.80, base loss: 20509.28
[INFO 2017-06-26 12:28:44,500 main.py:50] epoch 468, training loss: 7864.84, average training loss: 10852.42, base loss: 20508.81
[INFO 2017-06-26 12:28:44,861 main.py:50] epoch 469, training loss: 7799.86, average training loss: 10845.92, base loss: 20508.45
[INFO 2017-06-26 12:28:45,220 main.py:50] epoch 470, training loss: 7982.56, average training loss: 10839.84, base loss: 20508.79
[INFO 2017-06-26 12:28:45,578 main.py:50] epoch 471, training loss: 7818.31, average training loss: 10833.44, base loss: 20508.40
[INFO 2017-06-26 12:28:45,938 main.py:50] epoch 472, training loss: 7878.24, average training loss: 10827.19, base loss: 20508.51
[INFO 2017-06-26 12:28:46,296 main.py:50] epoch 473, training loss: 7841.51, average training loss: 10820.89, base loss: 20508.21
[INFO 2017-06-26 12:28:46,655 main.py:50] epoch 474, training loss: 7888.15, average training loss: 10814.72, base loss: 20507.62
[INFO 2017-06-26 12:28:47,014 main.py:50] epoch 475, training loss: 7910.31, average training loss: 10808.62, base loss: 20507.50
[INFO 2017-06-26 12:28:47,374 main.py:50] epoch 476, training loss: 7854.63, average training loss: 10802.43, base loss: 20507.40
[INFO 2017-06-26 12:28:47,735 main.py:50] epoch 477, training loss: 7906.63, average training loss: 10796.37, base loss: 20507.48
[INFO 2017-06-26 12:28:48,098 main.py:50] epoch 478, training loss: 7877.30, average training loss: 10790.27, base loss: 20507.23
[INFO 2017-06-26 12:28:48,461 main.py:50] epoch 479, training loss: 7912.95, average training loss: 10784.28, base loss: 20507.79
[INFO 2017-06-26 12:28:48,861 main.py:50] epoch 480, training loss: 7827.25, average training loss: 10778.13, base loss: 20507.58
[INFO 2017-06-26 12:28:49,224 main.py:50] epoch 481, training loss: 7779.52, average training loss: 10771.91, base loss: 20507.65
[INFO 2017-06-26 12:28:49,590 main.py:50] epoch 482, training loss: 7724.20, average training loss: 10765.60, base loss: 20506.98
[INFO 2017-06-26 12:28:49,978 main.py:50] epoch 483, training loss: 7875.10, average training loss: 10759.63, base loss: 20507.07
[INFO 2017-06-26 12:28:50,340 main.py:50] epoch 484, training loss: 7834.95, average training loss: 10753.60, base loss: 20507.41
[INFO 2017-06-26 12:28:50,701 main.py:50] epoch 485, training loss: 7960.59, average training loss: 10747.85, base loss: 20507.84
[INFO 2017-06-26 12:28:51,062 main.py:50] epoch 486, training loss: 7854.91, average training loss: 10741.91, base loss: 20508.48
[INFO 2017-06-26 12:28:51,454 main.py:50] epoch 487, training loss: 7819.60, average training loss: 10735.92, base loss: 20508.53
[INFO 2017-06-26 12:28:51,830 main.py:50] epoch 488, training loss: 7745.76, average training loss: 10729.81, base loss: 20508.38
[INFO 2017-06-26 12:28:52,221 main.py:50] epoch 489, training loss: 7749.27, average training loss: 10723.72, base loss: 20508.56
[INFO 2017-06-26 12:28:52,608 main.py:50] epoch 490, training loss: 7713.85, average training loss: 10717.59, base loss: 20508.48
[INFO 2017-06-26 12:28:52,973 main.py:50] epoch 491, training loss: 7784.88, average training loss: 10711.63, base loss: 20508.62
[INFO 2017-06-26 12:28:53,359 main.py:50] epoch 492, training loss: 7730.43, average training loss: 10705.59, base loss: 20508.84
[INFO 2017-06-26 12:28:53,722 main.py:50] epoch 493, training loss: 7793.39, average training loss: 10699.69, base loss: 20509.18
[INFO 2017-06-26 12:28:54,089 main.py:50] epoch 494, training loss: 7867.87, average training loss: 10693.97, base loss: 20509.47
[INFO 2017-06-26 12:28:54,482 main.py:50] epoch 495, training loss: 7821.12, average training loss: 10688.18, base loss: 20509.41
[INFO 2017-06-26 12:28:54,862 main.py:50] epoch 496, training loss: 7745.90, average training loss: 10682.26, base loss: 20509.42
[INFO 2017-06-26 12:28:55,224 main.py:50] epoch 497, training loss: 7675.47, average training loss: 10676.22, base loss: 20508.93
[INFO 2017-06-26 12:28:55,617 main.py:50] epoch 498, training loss: 7925.74, average training loss: 10670.71, base loss: 20509.42
[INFO 2017-06-26 12:28:55,994 main.py:50] epoch 499, training loss: 7843.14, average training loss: 10665.05, base loss: 20509.64
[INFO 2017-06-26 12:28:55,995 main.py:52] epoch 499, testing
[INFO 2017-06-26 12:28:57,485 main.py:103] average testing loss: 7742.96, base loss: 20484.76
[INFO 2017-06-26 12:28:57,486 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 12:28:57,492 main.py:76] current best accuracy: 7742.96
[INFO 2017-06-26 12:28:57,866 main.py:50] epoch 500, training loss: 7797.26, average training loss: 10659.33, base loss: 20510.10
[INFO 2017-06-26 12:28:58,227 main.py:50] epoch 501, training loss: 7803.94, average training loss: 10653.64, base loss: 20509.77
[INFO 2017-06-26 12:28:58,606 main.py:50] epoch 502, training loss: 7748.88, average training loss: 10647.87, base loss: 20509.72
[INFO 2017-06-26 12:28:58,967 main.py:50] epoch 503, training loss: 7705.02, average training loss: 10642.03, base loss: 20509.82
[INFO 2017-06-26 12:28:59,326 main.py:50] epoch 504, training loss: 7690.88, average training loss: 10636.18, base loss: 20509.38
[INFO 2017-06-26 12:28:59,687 main.py:50] epoch 505, training loss: 7683.73, average training loss: 10630.35, base loss: 20509.35
[INFO 2017-06-26 12:29:00,047 main.py:50] epoch 506, training loss: 7670.72, average training loss: 10624.51, base loss: 20508.47
[INFO 2017-06-26 12:29:00,436 main.py:50] epoch 507, training loss: 7668.30, average training loss: 10618.69, base loss: 20508.19
[INFO 2017-06-26 12:29:00,800 main.py:50] epoch 508, training loss: 7650.69, average training loss: 10612.86, base loss: 20507.76
[INFO 2017-06-26 12:29:01,162 main.py:50] epoch 509, training loss: 7762.45, average training loss: 10607.27, base loss: 20507.69
[INFO 2017-06-26 12:29:01,540 main.py:50] epoch 510, training loss: 7711.13, average training loss: 10601.60, base loss: 20507.54
[INFO 2017-06-26 12:29:01,899 main.py:50] epoch 511, training loss: 7682.26, average training loss: 10595.90, base loss: 20507.32
[INFO 2017-06-26 12:29:02,260 main.py:50] epoch 512, training loss: 7788.72, average training loss: 10590.43, base loss: 20507.92
[INFO 2017-06-26 12:29:02,628 main.py:50] epoch 513, training loss: 7734.77, average training loss: 10584.87, base loss: 20508.12
[INFO 2017-06-26 12:29:02,990 main.py:50] epoch 514, training loss: 7686.16, average training loss: 10579.25, base loss: 20508.40
[INFO 2017-06-26 12:29:03,352 main.py:50] epoch 515, training loss: 7733.11, average training loss: 10573.73, base loss: 20508.30
[INFO 2017-06-26 12:29:03,714 main.py:50] epoch 516, training loss: 7668.85, average training loss: 10568.11, base loss: 20508.41
[INFO 2017-06-26 12:29:04,075 main.py:50] epoch 517, training loss: 7782.15, average training loss: 10562.73, base loss: 20508.20
[INFO 2017-06-26 12:29:04,442 main.py:50] epoch 518, training loss: 7846.27, average training loss: 10557.50, base loss: 20508.62
[INFO 2017-06-26 12:29:04,832 main.py:50] epoch 519, training loss: 7667.44, average training loss: 10551.94, base loss: 20508.22
[INFO 2017-06-26 12:29:05,218 main.py:50] epoch 520, training loss: 7714.28, average training loss: 10546.49, base loss: 20508.12
[INFO 2017-06-26 12:29:05,605 main.py:50] epoch 521, training loss: 7771.95, average training loss: 10541.18, base loss: 20508.04
[INFO 2017-06-26 12:29:05,996 main.py:50] epoch 522, training loss: 7746.64, average training loss: 10535.84, base loss: 20508.09
[INFO 2017-06-26 12:29:06,364 main.py:50] epoch 523, training loss: 7762.11, average training loss: 10530.54, base loss: 20508.05
[INFO 2017-06-26 12:29:06,774 main.py:50] epoch 524, training loss: 7681.00, average training loss: 10525.12, base loss: 20508.01
[INFO 2017-06-26 12:29:07,137 main.py:50] epoch 525, training loss: 7830.88, average training loss: 10519.99, base loss: 20508.23
[INFO 2017-06-26 12:29:07,498 main.py:50] epoch 526, training loss: 7712.48, average training loss: 10514.67, base loss: 20508.47
[INFO 2017-06-26 12:29:07,858 main.py:50] epoch 527, training loss: 7822.76, average training loss: 10509.57, base loss: 20508.68
[INFO 2017-06-26 12:29:08,218 main.py:50] epoch 528, training loss: 7645.91, average training loss: 10504.15, base loss: 20508.53
[INFO 2017-06-26 12:29:08,578 main.py:50] epoch 529, training loss: 7649.28, average training loss: 10498.77, base loss: 20508.00
[INFO 2017-06-26 12:29:08,945 main.py:50] epoch 530, training loss: 7759.64, average training loss: 10493.61, base loss: 20507.88
[INFO 2017-06-26 12:29:09,310 main.py:50] epoch 531, training loss: 7620.71, average training loss: 10488.21, base loss: 20507.51
[INFO 2017-06-26 12:29:09,670 main.py:50] epoch 532, training loss: 7685.18, average training loss: 10482.95, base loss: 20507.20
[INFO 2017-06-26 12:29:10,031 main.py:50] epoch 533, training loss: 7761.71, average training loss: 10477.85, base loss: 20507.67
[INFO 2017-06-26 12:29:10,391 main.py:50] epoch 534, training loss: 7715.43, average training loss: 10472.69, base loss: 20507.80
[INFO 2017-06-26 12:29:10,750 main.py:50] epoch 535, training loss: 7645.89, average training loss: 10467.42, base loss: 20507.65
[INFO 2017-06-26 12:29:11,112 main.py:50] epoch 536, training loss: 7743.53, average training loss: 10462.34, base loss: 20507.86
[INFO 2017-06-26 12:29:11,473 main.py:50] epoch 537, training loss: 7684.50, average training loss: 10457.18, base loss: 20507.73
[INFO 2017-06-26 12:29:11,867 main.py:50] epoch 538, training loss: 7742.31, average training loss: 10452.14, base loss: 20508.08
[INFO 2017-06-26 12:29:12,246 main.py:50] epoch 539, training loss: 7574.67, average training loss: 10446.82, base loss: 20508.00
[INFO 2017-06-26 12:29:12,613 main.py:50] epoch 540, training loss: 7586.42, average training loss: 10441.53, base loss: 20507.76
[INFO 2017-06-26 12:29:12,975 main.py:50] epoch 541, training loss: 7623.64, average training loss: 10436.33, base loss: 20507.85
[INFO 2017-06-26 12:29:13,336 main.py:50] epoch 542, training loss: 7656.97, average training loss: 10431.21, base loss: 20507.88
[INFO 2017-06-26 12:29:13,697 main.py:50] epoch 543, training loss: 7535.81, average training loss: 10425.89, base loss: 20507.26
[INFO 2017-06-26 12:29:14,057 main.py:50] epoch 544, training loss: 7640.57, average training loss: 10420.78, base loss: 20507.26
[INFO 2017-06-26 12:29:14,416 main.py:50] epoch 545, training loss: 7622.00, average training loss: 10415.65, base loss: 20507.21
[INFO 2017-06-26 12:29:14,777 main.py:50] epoch 546, training loss: 7577.77, average training loss: 10410.46, base loss: 20506.77
[INFO 2017-06-26 12:29:15,139 main.py:50] epoch 547, training loss: 7620.74, average training loss: 10405.37, base loss: 20506.91
[INFO 2017-06-26 12:29:15,504 main.py:50] epoch 548, training loss: 7552.97, average training loss: 10400.18, base loss: 20506.29
[INFO 2017-06-26 12:29:15,868 main.py:50] epoch 549, training loss: 7564.55, average training loss: 10395.02, base loss: 20505.82
[INFO 2017-06-26 12:29:16,229 main.py:50] epoch 550, training loss: 7648.53, average training loss: 10390.04, base loss: 20505.82
[INFO 2017-06-26 12:29:16,591 main.py:50] epoch 551, training loss: 7593.14, average training loss: 10384.97, base loss: 20505.71
[INFO 2017-06-26 12:29:16,962 main.py:50] epoch 552, training loss: 7584.36, average training loss: 10379.91, base loss: 20505.44
[INFO 2017-06-26 12:29:17,335 main.py:50] epoch 553, training loss: 7586.86, average training loss: 10374.86, base loss: 20505.60
[INFO 2017-06-26 12:29:17,698 main.py:50] epoch 554, training loss: 7607.25, average training loss: 10369.88, base loss: 20505.00
[INFO 2017-06-26 12:29:18,092 main.py:50] epoch 555, training loss: 7684.38, average training loss: 10365.05, base loss: 20505.25
[INFO 2017-06-26 12:29:18,482 main.py:50] epoch 556, training loss: 7540.25, average training loss: 10359.98, base loss: 20505.29
[INFO 2017-06-26 12:29:18,850 main.py:50] epoch 557, training loss: 7631.72, average training loss: 10355.09, base loss: 20505.15
[INFO 2017-06-26 12:29:19,250 main.py:50] epoch 558, training loss: 7680.94, average training loss: 10350.30, base loss: 20505.45
[INFO 2017-06-26 12:29:19,613 main.py:50] epoch 559, training loss: 7735.80, average training loss: 10345.63, base loss: 20505.85
[INFO 2017-06-26 12:29:20,007 main.py:50] epoch 560, training loss: 7521.68, average training loss: 10340.60, base loss: 20506.07
[INFO 2017-06-26 12:29:20,395 main.py:50] epoch 561, training loss: 7631.37, average training loss: 10335.78, base loss: 20505.59
[INFO 2017-06-26 12:29:20,756 main.py:50] epoch 562, training loss: 7613.73, average training loss: 10330.94, base loss: 20505.43
[INFO 2017-06-26 12:29:21,151 main.py:50] epoch 563, training loss: 7707.85, average training loss: 10326.29, base loss: 20505.71
[INFO 2017-06-26 12:29:21,532 main.py:50] epoch 564, training loss: 7742.48, average training loss: 10321.72, base loss: 20506.09
[INFO 2017-06-26 12:29:21,897 main.py:50] epoch 565, training loss: 7610.33, average training loss: 10316.93, base loss: 20506.10
[INFO 2017-06-26 12:29:22,259 main.py:50] epoch 566, training loss: 7663.66, average training loss: 10312.25, base loss: 20506.39
[INFO 2017-06-26 12:29:22,651 main.py:50] epoch 567, training loss: 7641.82, average training loss: 10307.55, base loss: 20506.59
[INFO 2017-06-26 12:29:23,035 main.py:50] epoch 568, training loss: 7633.96, average training loss: 10302.85, base loss: 20506.41
[INFO 2017-06-26 12:29:23,404 main.py:50] epoch 569, training loss: 7672.21, average training loss: 10298.24, base loss: 20506.47
[INFO 2017-06-26 12:29:23,775 main.py:50] epoch 570, training loss: 7632.75, average training loss: 10293.57, base loss: 20506.56
[INFO 2017-06-26 12:29:24,174 main.py:50] epoch 571, training loss: 7577.64, average training loss: 10288.82, base loss: 20506.19
[INFO 2017-06-26 12:29:24,540 main.py:50] epoch 572, training loss: 7645.89, average training loss: 10284.21, base loss: 20506.31
[INFO 2017-06-26 12:29:24,903 main.py:50] epoch 573, training loss: 7663.91, average training loss: 10279.64, base loss: 20506.56
[INFO 2017-06-26 12:29:25,296 main.py:50] epoch 574, training loss: 7706.60, average training loss: 10275.17, base loss: 20506.77
[INFO 2017-06-26 12:29:25,677 main.py:50] epoch 575, training loss: 7614.21, average training loss: 10270.55, base loss: 20506.74
[INFO 2017-06-26 12:29:26,039 main.py:50] epoch 576, training loss: 7510.54, average training loss: 10265.76, base loss: 20506.73
[INFO 2017-06-26 12:29:26,443 main.py:50] epoch 577, training loss: 7541.15, average training loss: 10261.05, base loss: 20506.55
[INFO 2017-06-26 12:29:26,812 main.py:50] epoch 578, training loss: 7618.65, average training loss: 10256.49, base loss: 20506.87
[INFO 2017-06-26 12:29:27,178 main.py:50] epoch 579, training loss: 7749.65, average training loss: 10252.16, base loss: 20507.53
[INFO 2017-06-26 12:29:27,549 main.py:50] epoch 580, training loss: 7695.77, average training loss: 10247.76, base loss: 20507.37
[INFO 2017-06-26 12:29:27,911 main.py:50] epoch 581, training loss: 7560.24, average training loss: 10243.15, base loss: 20507.21
[INFO 2017-06-26 12:29:28,275 main.py:50] epoch 582, training loss: 7528.35, average training loss: 10238.49, base loss: 20507.07
[INFO 2017-06-26 12:29:28,667 main.py:50] epoch 583, training loss: 7553.88, average training loss: 10233.89, base loss: 20506.52
[INFO 2017-06-26 12:29:29,048 main.py:50] epoch 584, training loss: 7413.03, average training loss: 10229.07, base loss: 20505.98
[INFO 2017-06-26 12:29:29,410 main.py:50] epoch 585, training loss: 7613.23, average training loss: 10224.61, base loss: 20506.42
[INFO 2017-06-26 12:29:29,809 main.py:50] epoch 586, training loss: 7543.56, average training loss: 10220.04, base loss: 20505.95
[INFO 2017-06-26 12:29:30,204 main.py:50] epoch 587, training loss: 7515.14, average training loss: 10215.44, base loss: 20505.96
[INFO 2017-06-26 12:29:30,565 main.py:50] epoch 588, training loss: 7470.22, average training loss: 10210.78, base loss: 20505.38
[INFO 2017-06-26 12:29:30,963 main.py:50] epoch 589, training loss: 7547.60, average training loss: 10206.26, base loss: 20505.41
[INFO 2017-06-26 12:29:31,336 main.py:50] epoch 590, training loss: 7576.20, average training loss: 10201.81, base loss: 20505.54
[INFO 2017-06-26 12:29:31,698 main.py:50] epoch 591, training loss: 7521.31, average training loss: 10197.29, base loss: 20505.80
[INFO 2017-06-26 12:29:32,062 main.py:50] epoch 592, training loss: 7612.03, average training loss: 10192.93, base loss: 20506.00
[INFO 2017-06-26 12:29:32,421 main.py:50] epoch 593, training loss: 7722.26, average training loss: 10188.77, base loss: 20506.69
[INFO 2017-06-26 12:29:32,781 main.py:50] epoch 594, training loss: 7497.77, average training loss: 10184.24, base loss: 20506.85
[INFO 2017-06-26 12:29:33,174 main.py:50] epoch 595, training loss: 7491.27, average training loss: 10179.73, base loss: 20506.97
[INFO 2017-06-26 12:29:33,547 main.py:50] epoch 596, training loss: 7514.34, average training loss: 10175.26, base loss: 20506.98
[INFO 2017-06-26 12:29:33,911 main.py:50] epoch 597, training loss: 7445.66, average training loss: 10170.70, base loss: 20506.28
[INFO 2017-06-26 12:29:34,278 main.py:50] epoch 598, training loss: 7620.95, average training loss: 10166.44, base loss: 20506.47
[INFO 2017-06-26 12:29:34,638 main.py:50] epoch 599, training loss: 7445.74, average training loss: 10161.91, base loss: 20506.22
[INFO 2017-06-26 12:29:34,638 main.py:52] epoch 599, testing
[INFO 2017-06-26 12:29:36,170 main.py:103] average testing loss: 7510.50, base loss: 20532.38
[INFO 2017-06-26 12:29:36,171 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 12:29:36,177 main.py:76] current best accuracy: 7510.50
[INFO 2017-06-26 12:29:36,539 main.py:50] epoch 600, training loss: 7483.57, average training loss: 10157.45, base loss: 20505.68
[INFO 2017-06-26 12:29:36,922 main.py:50] epoch 601, training loss: 7474.41, average training loss: 10152.99, base loss: 20505.64
[INFO 2017-06-26 12:29:37,284 main.py:50] epoch 602, training loss: 7538.38, average training loss: 10148.66, base loss: 20505.70
[INFO 2017-06-26 12:29:37,666 main.py:50] epoch 603, training loss: 7476.02, average training loss: 10144.23, base loss: 20505.74
[INFO 2017-06-26 12:29:38,027 main.py:50] epoch 604, training loss: 7479.24, average training loss: 10139.83, base loss: 20505.89
[INFO 2017-06-26 12:29:38,389 main.py:50] epoch 605, training loss: 7469.77, average training loss: 10135.42, base loss: 20506.01
[INFO 2017-06-26 12:29:38,751 main.py:50] epoch 606, training loss: 7479.76, average training loss: 10131.05, base loss: 20505.95
[INFO 2017-06-26 12:29:39,143 main.py:50] epoch 607, training loss: 7477.43, average training loss: 10126.68, base loss: 20505.78
[INFO 2017-06-26 12:29:39,523 main.py:50] epoch 608, training loss: 7471.43, average training loss: 10122.32, base loss: 20505.77
[INFO 2017-06-26 12:29:39,886 main.py:50] epoch 609, training loss: 7512.39, average training loss: 10118.04, base loss: 20505.35
[INFO 2017-06-26 12:29:40,282 main.py:50] epoch 610, training loss: 7463.74, average training loss: 10113.70, base loss: 20505.58
[INFO 2017-06-26 12:29:40,643 main.py:50] epoch 611, training loss: 7626.56, average training loss: 10109.63, base loss: 20505.56
[INFO 2017-06-26 12:29:41,006 main.py:50] epoch 612, training loss: 7480.79, average training loss: 10105.35, base loss: 20505.63
[INFO 2017-06-26 12:29:41,368 main.py:50] epoch 613, training loss: 7416.88, average training loss: 10100.97, base loss: 20505.46
[INFO 2017-06-26 12:29:41,761 main.py:50] epoch 614, training loss: 7531.05, average training loss: 10096.79, base loss: 20505.53
[INFO 2017-06-26 12:29:42,127 main.py:50] epoch 615, training loss: 7509.95, average training loss: 10092.59, base loss: 20505.65
[INFO 2017-06-26 12:29:42,489 main.py:50] epoch 616, training loss: 7497.03, average training loss: 10088.38, base loss: 20505.84
[INFO 2017-06-26 12:29:42,907 main.py:50] epoch 617, training loss: 7476.53, average training loss: 10084.16, base loss: 20505.77
[INFO 2017-06-26 12:29:43,273 main.py:50] epoch 618, training loss: 7432.60, average training loss: 10079.87, base loss: 20505.47
[INFO 2017-06-26 12:29:43,635 main.py:50] epoch 619, training loss: 7490.80, average training loss: 10075.70, base loss: 20505.41
[INFO 2017-06-26 12:29:43,997 main.py:50] epoch 620, training loss: 7505.91, average training loss: 10071.56, base loss: 20505.69
[INFO 2017-06-26 12:29:44,358 main.py:50] epoch 621, training loss: 7488.11, average training loss: 10067.41, base loss: 20505.94
[INFO 2017-06-26 12:29:44,719 main.py:50] epoch 622, training loss: 7475.42, average training loss: 10063.24, base loss: 20506.02
[INFO 2017-06-26 12:29:45,081 main.py:50] epoch 623, training loss: 7468.62, average training loss: 10059.09, base loss: 20506.01
[INFO 2017-06-26 12:29:45,475 main.py:50] epoch 624, training loss: 7526.26, average training loss: 10055.03, base loss: 20505.79
[INFO 2017-06-26 12:29:45,859 main.py:50] epoch 625, training loss: 7433.02, average training loss: 10050.85, base loss: 20505.37
[INFO 2017-06-26 12:29:46,227 main.py:50] epoch 626, training loss: 7476.85, average training loss: 10046.74, base loss: 20505.70
[INFO 2017-06-26 12:29:46,591 main.py:50] epoch 627, training loss: 7598.52, average training loss: 10042.84, base loss: 20505.72
[INFO 2017-06-26 12:29:46,983 main.py:50] epoch 628, training loss: 7400.34, average training loss: 10038.64, base loss: 20505.50
[INFO 2017-06-26 12:29:47,363 main.py:50] epoch 629, training loss: 7683.68, average training loss: 10034.90, base loss: 20505.61
[INFO 2017-06-26 12:29:47,739 main.py:50] epoch 630, training loss: 7416.40, average training loss: 10030.75, base loss: 20505.64
[INFO 2017-06-26 12:29:48,134 main.py:50] epoch 631, training loss: 7583.80, average training loss: 10026.88, base loss: 20505.83
[INFO 2017-06-26 12:29:48,514 main.py:50] epoch 632, training loss: 7514.70, average training loss: 10022.91, base loss: 20505.89
[INFO 2017-06-26 12:29:48,881 main.py:50] epoch 633, training loss: 7534.52, average training loss: 10018.99, base loss: 20505.73
[INFO 2017-06-26 12:29:49,278 main.py:50] epoch 634, training loss: 7482.79, average training loss: 10014.99, base loss: 20505.80
[INFO 2017-06-26 12:29:49,659 main.py:50] epoch 635, training loss: 7606.25, average training loss: 10011.21, base loss: 20505.63
[INFO 2017-06-26 12:29:50,053 main.py:50] epoch 636, training loss: 7487.37, average training loss: 10007.24, base loss: 20505.77
[INFO 2017-06-26 12:29:50,421 main.py:50] epoch 637, training loss: 7599.79, average training loss: 10003.47, base loss: 20505.97
[INFO 2017-06-26 12:29:50,786 main.py:50] epoch 638, training loss: 7505.39, average training loss: 9999.56, base loss: 20506.24
[INFO 2017-06-26 12:29:51,168 main.py:50] epoch 639, training loss: 7617.45, average training loss: 9995.84, base loss: 20506.64
[INFO 2017-06-26 12:29:51,533 main.py:50] epoch 640, training loss: 7432.56, average training loss: 9991.84, base loss: 20506.54
[INFO 2017-06-26 12:29:51,932 main.py:50] epoch 641, training loss: 7601.78, average training loss: 9988.12, base loss: 20507.02
[INFO 2017-06-26 12:29:52,326 main.py:50] epoch 642, training loss: 7465.47, average training loss: 9984.19, base loss: 20506.85
[INFO 2017-06-26 12:29:52,725 main.py:50] epoch 643, training loss: 7627.23, average training loss: 9980.53, base loss: 20507.28
[INFO 2017-06-26 12:29:53,112 main.py:50] epoch 644, training loss: 7506.63, average training loss: 9976.70, base loss: 20507.16
[INFO 2017-06-26 12:29:53,510 main.py:50] epoch 645, training loss: 7494.32, average training loss: 9972.86, base loss: 20506.98
[INFO 2017-06-26 12:29:53,886 main.py:50] epoch 646, training loss: 7467.64, average training loss: 9968.98, base loss: 20506.96
[INFO 2017-06-26 12:29:54,249 main.py:50] epoch 647, training loss: 7486.37, average training loss: 9965.15, base loss: 20507.13
[INFO 2017-06-26 12:29:54,624 main.py:50] epoch 648, training loss: 7425.68, average training loss: 9961.24, base loss: 20507.31
[INFO 2017-06-26 12:29:54,986 main.py:50] epoch 649, training loss: 7343.44, average training loss: 9957.21, base loss: 20506.81
[INFO 2017-06-26 12:29:55,347 main.py:50] epoch 650, training loss: 7462.55, average training loss: 9953.38, base loss: 20506.69
[INFO 2017-06-26 12:29:55,708 main.py:50] epoch 651, training loss: 7444.56, average training loss: 9949.53, base loss: 20506.75
[INFO 2017-06-26 12:29:56,069 main.py:50] epoch 652, training loss: 7638.10, average training loss: 9945.99, base loss: 20506.88
[INFO 2017-06-26 12:29:56,431 main.py:50] epoch 653, training loss: 7417.20, average training loss: 9942.13, base loss: 20506.95
[INFO 2017-06-26 12:29:56,793 main.py:50] epoch 654, training loss: 7474.97, average training loss: 9938.36, base loss: 20507.10
[INFO 2017-06-26 12:29:57,156 main.py:50] epoch 655, training loss: 7407.45, average training loss: 9934.50, base loss: 20507.22
[INFO 2017-06-26 12:29:57,518 main.py:50] epoch 656, training loss: 7428.58, average training loss: 9930.69, base loss: 20507.01
[INFO 2017-06-26 12:29:57,879 main.py:50] epoch 657, training loss: 7385.08, average training loss: 9926.82, base loss: 20506.77
[INFO 2017-06-26 12:29:58,239 main.py:50] epoch 658, training loss: 7443.13, average training loss: 9923.05, base loss: 20506.55
[INFO 2017-06-26 12:29:58,633 main.py:50] epoch 659, training loss: 7397.66, average training loss: 9919.22, base loss: 20506.45
[INFO 2017-06-26 12:29:59,017 main.py:50] epoch 660, training loss: 7411.77, average training loss: 9915.43, base loss: 20506.25
[INFO 2017-06-26 12:29:59,380 main.py:50] epoch 661, training loss: 7407.93, average training loss: 9911.64, base loss: 20506.21
[INFO 2017-06-26 12:29:59,742 main.py:50] epoch 662, training loss: 7397.23, average training loss: 9907.85, base loss: 20505.86
[INFO 2017-06-26 12:30:00,103 main.py:50] epoch 663, training loss: 7360.49, average training loss: 9904.01, base loss: 20505.53
[INFO 2017-06-26 12:30:00,478 main.py:50] epoch 664, training loss: 7515.61, average training loss: 9900.42, base loss: 20505.88
[INFO 2017-06-26 12:30:00,839 main.py:50] epoch 665, training loss: 7397.40, average training loss: 9896.66, base loss: 20505.19
[INFO 2017-06-26 12:30:01,203 main.py:50] epoch 666, training loss: 7471.96, average training loss: 9893.03, base loss: 20505.16
[INFO 2017-06-26 12:30:01,563 main.py:50] epoch 667, training loss: 7367.29, average training loss: 9889.25, base loss: 20504.96
[INFO 2017-06-26 12:30:01,955 main.py:50] epoch 668, training loss: 7344.25, average training loss: 9885.44, base loss: 20504.88
[INFO 2017-06-26 12:30:02,325 main.py:50] epoch 669, training loss: 7377.90, average training loss: 9881.70, base loss: 20504.97
[INFO 2017-06-26 12:30:02,690 main.py:50] epoch 670, training loss: 7416.56, average training loss: 9878.03, base loss: 20504.68
[INFO 2017-06-26 12:30:03,054 main.py:50] epoch 671, training loss: 7474.27, average training loss: 9874.45, base loss: 20504.55
[INFO 2017-06-26 12:30:03,416 main.py:50] epoch 672, training loss: 7470.18, average training loss: 9870.88, base loss: 20505.13
[INFO 2017-06-26 12:30:03,777 main.py:50] epoch 673, training loss: 7482.93, average training loss: 9867.33, base loss: 20504.97
[INFO 2017-06-26 12:30:04,169 main.py:50] epoch 674, training loss: 7423.63, average training loss: 9863.71, base loss: 20505.16
[INFO 2017-06-26 12:30:04,553 main.py:50] epoch 675, training loss: 7402.83, average training loss: 9860.07, base loss: 20505.36
[INFO 2017-06-26 12:30:04,915 main.py:50] epoch 676, training loss: 7362.93, average training loss: 9856.39, base loss: 20505.28
[INFO 2017-06-26 12:30:05,312 main.py:50] epoch 677, training loss: 7396.68, average training loss: 9852.76, base loss: 20505.15
[INFO 2017-06-26 12:30:05,685 main.py:50] epoch 678, training loss: 7390.45, average training loss: 9849.13, base loss: 20505.47
[INFO 2017-06-26 12:30:06,054 main.py:50] epoch 679, training loss: 7399.20, average training loss: 9845.53, base loss: 20505.40
[INFO 2017-06-26 12:30:06,453 main.py:50] epoch 680, training loss: 7365.23, average training loss: 9841.89, base loss: 20505.29
[INFO 2017-06-26 12:30:06,815 main.py:50] epoch 681, training loss: 7492.28, average training loss: 9838.44, base loss: 20505.63
[INFO 2017-06-26 12:30:07,179 main.py:50] epoch 682, training loss: 7536.33, average training loss: 9835.07, base loss: 20505.99
[INFO 2017-06-26 12:30:07,543 main.py:50] epoch 683, training loss: 7392.26, average training loss: 9831.50, base loss: 20505.90
[INFO 2017-06-26 12:30:07,903 main.py:50] epoch 684, training loss: 7398.19, average training loss: 9827.95, base loss: 20505.81
[INFO 2017-06-26 12:30:08,263 main.py:50] epoch 685, training loss: 7339.72, average training loss: 9824.32, base loss: 20505.70
[INFO 2017-06-26 12:30:08,624 main.py:50] epoch 686, training loss: 7396.62, average training loss: 9820.79, base loss: 20505.51
[INFO 2017-06-26 12:30:08,984 main.py:50] epoch 687, training loss: 7355.44, average training loss: 9817.20, base loss: 20505.44
[INFO 2017-06-26 12:30:09,378 main.py:50] epoch 688, training loss: 7399.67, average training loss: 9813.69, base loss: 20505.74
[INFO 2017-06-26 12:30:09,746 main.py:50] epoch 689, training loss: 7326.32, average training loss: 9810.09, base loss: 20505.42
[INFO 2017-06-26 12:30:10,128 main.py:50] epoch 690, training loss: 7394.96, average training loss: 9806.59, base loss: 20505.32
[INFO 2017-06-26 12:30:10,489 main.py:50] epoch 691, training loss: 7398.33, average training loss: 9803.11, base loss: 20505.21
[INFO 2017-06-26 12:30:10,848 main.py:50] epoch 692, training loss: 7398.08, average training loss: 9799.64, base loss: 20505.07
[INFO 2017-06-26 12:30:11,208 main.py:50] epoch 693, training loss: 7299.46, average training loss: 9796.04, base loss: 20504.70
[INFO 2017-06-26 12:30:11,568 main.py:50] epoch 694, training loss: 7245.00, average training loss: 9792.37, base loss: 20504.43
[INFO 2017-06-26 12:30:11,929 main.py:50] epoch 695, training loss: 7319.21, average training loss: 9788.82, base loss: 20504.55
[INFO 2017-06-26 12:30:12,291 main.py:50] epoch 696, training loss: 7444.77, average training loss: 9785.45, base loss: 20504.79
[INFO 2017-06-26 12:30:12,685 main.py:50] epoch 697, training loss: 7344.01, average training loss: 9781.96, base loss: 20504.65
[INFO 2017-06-26 12:30:13,048 main.py:50] epoch 698, training loss: 7315.01, average training loss: 9778.43, base loss: 20504.49
[INFO 2017-06-26 12:30:13,409 main.py:50] epoch 699, training loss: 7215.15, average training loss: 9774.76, base loss: 20504.02
[INFO 2017-06-26 12:30:13,409 main.py:52] epoch 699, testing
[INFO 2017-06-26 12:30:14,885 main.py:103] average testing loss: 7410.76, base loss: 20594.23
[INFO 2017-06-26 12:30:14,886 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 12:30:14,893 main.py:76] current best accuracy: 7410.76
[INFO 2017-06-26 12:30:15,252 main.py:50] epoch 700, training loss: 7370.38, average training loss: 9771.33, base loss: 20504.03
[INFO 2017-06-26 12:30:15,613 main.py:50] epoch 701, training loss: 7388.72, average training loss: 9767.94, base loss: 20504.03
[INFO 2017-06-26 12:30:15,973 main.py:50] epoch 702, training loss: 7361.40, average training loss: 9764.52, base loss: 20504.22
[INFO 2017-06-26 12:30:16,332 main.py:50] epoch 703, training loss: 7385.87, average training loss: 9761.14, base loss: 20504.14
[INFO 2017-06-26 12:30:16,693 main.py:50] epoch 704, training loss: 7351.02, average training loss: 9757.72, base loss: 20504.21
[INFO 2017-06-26 12:30:17,053 main.py:50] epoch 705, training loss: 7419.31, average training loss: 9754.41, base loss: 20504.34
[INFO 2017-06-26 12:30:17,414 main.py:50] epoch 706, training loss: 7366.79, average training loss: 9751.03, base loss: 20504.42
[INFO 2017-06-26 12:30:17,774 main.py:50] epoch 707, training loss: 7404.09, average training loss: 9747.72, base loss: 20504.52
[INFO 2017-06-26 12:30:18,134 main.py:50] epoch 708, training loss: 7381.61, average training loss: 9744.38, base loss: 20504.65
[INFO 2017-06-26 12:30:18,495 main.py:50] epoch 709, training loss: 7417.18, average training loss: 9741.10, base loss: 20504.91
[INFO 2017-06-26 12:30:18,855 main.py:50] epoch 710, training loss: 7344.75, average training loss: 9737.73, base loss: 20504.66
[INFO 2017-06-26 12:30:19,216 main.py:50] epoch 711, training loss: 7302.24, average training loss: 9734.31, base loss: 20504.76
[INFO 2017-06-26 12:30:19,578 main.py:50] epoch 712, training loss: 7342.57, average training loss: 9730.96, base loss: 20505.01
[INFO 2017-06-26 12:30:19,940 main.py:50] epoch 713, training loss: 7319.91, average training loss: 9727.58, base loss: 20505.02
[INFO 2017-06-26 12:30:20,302 main.py:50] epoch 714, training loss: 7272.37, average training loss: 9724.14, base loss: 20505.07
[INFO 2017-06-26 12:30:20,699 main.py:50] epoch 715, training loss: 7251.46, average training loss: 9720.69, base loss: 20505.03
[INFO 2017-06-26 12:30:21,063 main.py:50] epoch 716, training loss: 7335.90, average training loss: 9717.36, base loss: 20504.91
[INFO 2017-06-26 12:30:21,427 main.py:50] epoch 717, training loss: 7290.22, average training loss: 9713.98, base loss: 20504.97
[INFO 2017-06-26 12:30:21,791 main.py:50] epoch 718, training loss: 7314.82, average training loss: 9710.65, base loss: 20505.07
[INFO 2017-06-26 12:30:22,224 main.py:50] epoch 719, training loss: 7228.95, average training loss: 9707.20, base loss: 20505.11
[INFO 2017-06-26 12:30:22,618 main.py:50] epoch 720, training loss: 7264.12, average training loss: 9703.81, base loss: 20505.11
[INFO 2017-06-26 12:30:23,001 main.py:50] epoch 721, training loss: 7339.04, average training loss: 9700.54, base loss: 20505.04
[INFO 2017-06-26 12:30:23,392 main.py:50] epoch 722, training loss: 7248.06, average training loss: 9697.14, base loss: 20504.99
[INFO 2017-06-26 12:30:23,759 main.py:50] epoch 723, training loss: 7214.56, average training loss: 9693.72, base loss: 20504.79
[INFO 2017-06-26 12:30:24,151 main.py:50] epoch 724, training loss: 7303.84, average training loss: 9690.42, base loss: 20504.60
[INFO 2017-06-26 12:30:24,513 main.py:50] epoch 725, training loss: 7266.15, average training loss: 9687.08, base loss: 20504.61
[INFO 2017-06-26 12:30:24,873 main.py:50] epoch 726, training loss: 7274.72, average training loss: 9683.76, base loss: 20504.68
[INFO 2017-06-26 12:30:25,234 main.py:50] epoch 727, training loss: 7323.99, average training loss: 9680.52, base loss: 20504.97
[INFO 2017-06-26 12:30:25,594 main.py:50] epoch 728, training loss: 7349.92, average training loss: 9677.32, base loss: 20505.26
[INFO 2017-06-26 12:30:25,955 main.py:50] epoch 729, training loss: 7219.96, average training loss: 9673.96, base loss: 20505.20
[INFO 2017-06-26 12:30:26,315 main.py:50] epoch 730, training loss: 7335.58, average training loss: 9670.76, base loss: 20505.62
[INFO 2017-06-26 12:30:26,676 main.py:50] epoch 731, training loss: 7273.40, average training loss: 9667.48, base loss: 20505.67
[INFO 2017-06-26 12:30:27,036 main.py:50] epoch 732, training loss: 7373.09, average training loss: 9664.35, base loss: 20505.63
[INFO 2017-06-26 12:30:27,397 main.py:50] epoch 733, training loss: 7232.44, average training loss: 9661.04, base loss: 20505.60
[INFO 2017-06-26 12:30:27,757 main.py:50] epoch 734, training loss: 7260.72, average training loss: 9657.77, base loss: 20505.67
[INFO 2017-06-26 12:30:28,118 main.py:50] epoch 735, training loss: 7344.07, average training loss: 9654.63, base loss: 20506.39
[INFO 2017-06-26 12:30:28,491 main.py:50] epoch 736, training loss: 7292.62, average training loss: 9651.43, base loss: 20506.43
[INFO 2017-06-26 12:30:28,851 main.py:50] epoch 737, training loss: 7243.41, average training loss: 9648.16, base loss: 20506.54
[INFO 2017-06-26 12:30:29,246 main.py:50] epoch 738, training loss: 7294.44, average training loss: 9644.98, base loss: 20506.76
[INFO 2017-06-26 12:30:29,618 main.py:50] epoch 739, training loss: 7249.27, average training loss: 9641.74, base loss: 20507.08
[INFO 2017-06-26 12:30:29,980 main.py:50] epoch 740, training loss: 7161.39, average training loss: 9638.39, base loss: 20507.22
[INFO 2017-06-26 12:30:30,350 main.py:50] epoch 741, training loss: 7261.07, average training loss: 9635.19, base loss: 20507.20
[INFO 2017-06-26 12:30:30,744 main.py:50] epoch 742, training loss: 7258.10, average training loss: 9631.99, base loss: 20507.14
[INFO 2017-06-26 12:30:31,116 main.py:50] epoch 743, training loss: 7174.48, average training loss: 9628.69, base loss: 20507.18
[INFO 2017-06-26 12:30:31,480 main.py:50] epoch 744, training loss: 7238.81, average training loss: 9625.48, base loss: 20507.27
[INFO 2017-06-26 12:30:31,841 main.py:50] epoch 745, training loss: 7202.63, average training loss: 9622.23, base loss: 20507.24
[INFO 2017-06-26 12:30:32,235 main.py:50] epoch 746, training loss: 7277.17, average training loss: 9619.09, base loss: 20507.00
[INFO 2017-06-26 12:30:32,600 main.py:50] epoch 747, training loss: 7327.38, average training loss: 9616.03, base loss: 20507.43
[INFO 2017-06-26 12:30:32,964 main.py:50] epoch 748, training loss: 7264.87, average training loss: 9612.89, base loss: 20507.48
[INFO 2017-06-26 12:30:33,357 main.py:50] epoch 749, training loss: 7189.08, average training loss: 9609.66, base loss: 20507.35
[INFO 2017-06-26 12:30:33,719 main.py:50] epoch 750, training loss: 7366.38, average training loss: 9606.67, base loss: 20507.83
[INFO 2017-06-26 12:30:34,081 main.py:50] epoch 751, training loss: 7253.55, average training loss: 9603.54, base loss: 20507.54
[INFO 2017-06-26 12:30:34,474 main.py:50] epoch 752, training loss: 7203.96, average training loss: 9600.35, base loss: 20507.27
[INFO 2017-06-26 12:30:34,849 main.py:50] epoch 753, training loss: 7339.07, average training loss: 9597.36, base loss: 20507.47
[INFO 2017-06-26 12:30:35,250 main.py:50] epoch 754, training loss: 7226.72, average training loss: 9594.22, base loss: 20507.18
[INFO 2017-06-26 12:30:35,621 main.py:50] epoch 755, training loss: 7261.76, average training loss: 9591.13, base loss: 20507.22
[INFO 2017-06-26 12:30:35,984 main.py:50] epoch 756, training loss: 7258.53, average training loss: 9588.05, base loss: 20507.25
[INFO 2017-06-26 12:30:36,347 main.py:50] epoch 757, training loss: 7264.09, average training loss: 9584.98, base loss: 20507.32
[INFO 2017-06-26 12:30:36,742 main.py:50] epoch 758, training loss: 7381.52, average training loss: 9582.08, base loss: 20507.39
[INFO 2017-06-26 12:30:37,123 main.py:50] epoch 759, training loss: 7256.40, average training loss: 9579.02, base loss: 20507.27
[INFO 2017-06-26 12:30:37,500 main.py:50] epoch 760, training loss: 7306.86, average training loss: 9576.03, base loss: 20507.71
[INFO 2017-06-26 12:30:37,884 main.py:50] epoch 761, training loss: 7322.71, average training loss: 9573.08, base loss: 20507.62
[INFO 2017-06-26 12:30:38,247 main.py:50] epoch 762, training loss: 7246.46, average training loss: 9570.03, base loss: 20507.70
[INFO 2017-06-26 12:30:38,626 main.py:50] epoch 763, training loss: 7298.31, average training loss: 9567.05, base loss: 20507.87
[INFO 2017-06-26 12:30:39,025 main.py:50] epoch 764, training loss: 7285.54, average training loss: 9564.07, base loss: 20508.09
[INFO 2017-06-26 12:30:39,390 main.py:50] epoch 765, training loss: 7303.81, average training loss: 9561.12, base loss: 20508.23
[INFO 2017-06-26 12:30:39,771 main.py:50] epoch 766, training loss: 7089.89, average training loss: 9557.90, base loss: 20507.79
[INFO 2017-06-26 12:30:40,165 main.py:50] epoch 767, training loss: 7150.47, average training loss: 9554.76, base loss: 20507.54
[INFO 2017-06-26 12:30:40,543 main.py:50] epoch 768, training loss: 7274.40, average training loss: 9551.80, base loss: 20507.70
[INFO 2017-06-26 12:30:40,905 main.py:50] epoch 769, training loss: 7319.41, average training loss: 9548.90, base loss: 20507.70
[INFO 2017-06-26 12:30:41,301 main.py:50] epoch 770, training loss: 7257.63, average training loss: 9545.93, base loss: 20507.96
[INFO 2017-06-26 12:30:41,677 main.py:50] epoch 771, training loss: 7196.18, average training loss: 9542.88, base loss: 20507.78
[INFO 2017-06-26 12:30:42,039 main.py:50] epoch 772, training loss: 7167.33, average training loss: 9539.81, base loss: 20507.79
[INFO 2017-06-26 12:30:42,400 main.py:50] epoch 773, training loss: 7191.40, average training loss: 9536.78, base loss: 20507.80
[INFO 2017-06-26 12:30:42,760 main.py:50] epoch 774, training loss: 7267.51, average training loss: 9533.85, base loss: 20507.92
[INFO 2017-06-26 12:30:43,121 main.py:50] epoch 775, training loss: 7215.13, average training loss: 9530.86, base loss: 20508.07
[INFO 2017-06-26 12:30:43,482 main.py:50] epoch 776, training loss: 7194.50, average training loss: 9527.85, base loss: 20507.99
[INFO 2017-06-26 12:30:43,842 main.py:50] epoch 777, training loss: 7196.00, average training loss: 9524.86, base loss: 20507.78
[INFO 2017-06-26 12:30:44,204 main.py:50] epoch 778, training loss: 7254.81, average training loss: 9521.94, base loss: 20507.98
[INFO 2017-06-26 12:30:44,599 main.py:50] epoch 779, training loss: 7152.17, average training loss: 9518.90, base loss: 20508.03
[INFO 2017-06-26 12:30:44,961 main.py:50] epoch 780, training loss: 7160.93, average training loss: 9515.89, base loss: 20508.08
[INFO 2017-06-26 12:30:45,322 main.py:50] epoch 781, training loss: 7096.30, average training loss: 9512.79, base loss: 20507.96
[INFO 2017-06-26 12:30:45,717 main.py:50] epoch 782, training loss: 7103.20, average training loss: 9509.71, base loss: 20508.10
[INFO 2017-06-26 12:30:46,081 main.py:50] epoch 783, training loss: 7091.47, average training loss: 9506.63, base loss: 20507.72
[INFO 2017-06-26 12:30:46,442 main.py:50] epoch 784, training loss: 7162.03, average training loss: 9503.64, base loss: 20507.62
[INFO 2017-06-26 12:30:46,835 main.py:50] epoch 785, training loss: 7194.43, average training loss: 9500.70, base loss: 20507.72
[INFO 2017-06-26 12:30:47,201 main.py:50] epoch 786, training loss: 7286.76, average training loss: 9497.89, base loss: 20507.71
[INFO 2017-06-26 12:30:47,563 main.py:50] epoch 787, training loss: 7254.80, average training loss: 9495.04, base loss: 20508.10
[INFO 2017-06-26 12:30:47,926 main.py:50] epoch 788, training loss: 7189.86, average training loss: 9492.12, base loss: 20507.89
[INFO 2017-06-26 12:30:48,319 main.py:50] epoch 789, training loss: 7251.39, average training loss: 9489.29, base loss: 20507.71
[INFO 2017-06-26 12:30:48,688 main.py:50] epoch 790, training loss: 7234.20, average training loss: 9486.44, base loss: 20508.26
[INFO 2017-06-26 12:30:49,055 main.py:50] epoch 791, training loss: 7212.81, average training loss: 9483.57, base loss: 20508.13
[INFO 2017-06-26 12:30:49,450 main.py:50] epoch 792, training loss: 7152.40, average training loss: 9480.63, base loss: 20508.15
[INFO 2017-06-26 12:30:49,818 main.py:50] epoch 793, training loss: 7084.70, average training loss: 9477.61, base loss: 20508.18
[INFO 2017-06-26 12:30:50,180 main.py:50] epoch 794, training loss: 7140.24, average training loss: 9474.67, base loss: 20508.18
[INFO 2017-06-26 12:30:50,543 main.py:50] epoch 795, training loss: 7101.39, average training loss: 9471.69, base loss: 20508.22
[INFO 2017-06-26 12:30:50,902 main.py:50] epoch 796, training loss: 7068.56, average training loss: 9468.67, base loss: 20507.98
[INFO 2017-06-26 12:30:51,261 main.py:50] epoch 797, training loss: 7100.75, average training loss: 9465.70, base loss: 20507.99
[INFO 2017-06-26 12:30:51,622 main.py:50] epoch 798, training loss: 7253.50, average training loss: 9462.94, base loss: 20508.49
[INFO 2017-06-26 12:30:52,014 main.py:50] epoch 799, training loss: 7212.72, average training loss: 9460.12, base loss: 20508.75
[INFO 2017-06-26 12:30:52,014 main.py:52] epoch 799, testing
[INFO 2017-06-26 12:30:53,536 main.py:103] average testing loss: 7099.29, base loss: 20472.68
[INFO 2017-06-26 12:30:53,536 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 12:30:53,542 main.py:76] current best accuracy: 7099.29
[INFO 2017-06-26 12:30:53,904 main.py:50] epoch 800, training loss: 7220.47, average training loss: 9457.33, base loss: 20508.75
[INFO 2017-06-26 12:30:54,267 main.py:50] epoch 801, training loss: 7130.11, average training loss: 9454.42, base loss: 20508.48
[INFO 2017-06-26 12:30:54,632 main.py:50] epoch 802, training loss: 7171.05, average training loss: 9451.58, base loss: 20508.67
[INFO 2017-06-26 12:30:54,999 main.py:50] epoch 803, training loss: 7161.78, average training loss: 9448.73, base loss: 20508.84
[INFO 2017-06-26 12:30:55,363 main.py:50] epoch 804, training loss: 7182.71, average training loss: 9445.92, base loss: 20509.11
[INFO 2017-06-26 12:30:55,726 main.py:50] epoch 805, training loss: 7131.94, average training loss: 9443.05, base loss: 20509.41
[INFO 2017-06-26 12:30:56,123 main.py:50] epoch 806, training loss: 7158.99, average training loss: 9440.22, base loss: 20509.44
[INFO 2017-06-26 12:30:56,506 main.py:50] epoch 807, training loss: 7205.34, average training loss: 9437.45, base loss: 20509.58
[INFO 2017-06-26 12:30:56,872 main.py:50] epoch 808, training loss: 7356.52, average training loss: 9434.88, base loss: 20510.07
[INFO 2017-06-26 12:30:57,237 main.py:50] epoch 809, training loss: 7236.00, average training loss: 9432.16, base loss: 20510.12
[INFO 2017-06-26 12:30:57,630 main.py:50] epoch 810, training loss: 7144.81, average training loss: 9429.34, base loss: 20510.02
[INFO 2017-06-26 12:30:58,013 main.py:50] epoch 811, training loss: 7250.33, average training loss: 9426.66, base loss: 20510.17
[INFO 2017-06-26 12:30:58,381 main.py:50] epoch 812, training loss: 7187.24, average training loss: 9423.91, base loss: 20510.01
[INFO 2017-06-26 12:30:58,761 main.py:50] epoch 813, training loss: 7138.75, average training loss: 9421.10, base loss: 20509.72
[INFO 2017-06-26 12:30:59,155 main.py:50] epoch 814, training loss: 7283.78, average training loss: 9418.48, base loss: 20509.76
[INFO 2017-06-26 12:30:59,535 main.py:50] epoch 815, training loss: 7081.95, average training loss: 9415.61, base loss: 20509.74
[INFO 2017-06-26 12:30:59,899 main.py:50] epoch 816, training loss: 7190.34, average training loss: 9412.89, base loss: 20509.74
[INFO 2017-06-26 12:31:00,262 main.py:50] epoch 817, training loss: 7233.19, average training loss: 9410.22, base loss: 20509.60
[INFO 2017-06-26 12:31:00,655 main.py:50] epoch 818, training loss: 7250.82, average training loss: 9407.59, base loss: 20509.62
[INFO 2017-06-26 12:31:01,036 main.py:50] epoch 819, training loss: 7217.41, average training loss: 9404.92, base loss: 20509.88
[INFO 2017-06-26 12:31:01,400 main.py:50] epoch 820, training loss: 7251.45, average training loss: 9402.29, base loss: 20510.19
[INFO 2017-06-26 12:31:01,763 main.py:50] epoch 821, training loss: 7176.80, average training loss: 9399.59, base loss: 20510.55
[INFO 2017-06-26 12:31:02,124 main.py:50] epoch 822, training loss: 7105.44, average training loss: 9396.80, base loss: 20510.69
[INFO 2017-06-26 12:31:02,487 main.py:50] epoch 823, training loss: 7190.96, average training loss: 9394.12, base loss: 20510.79
[INFO 2017-06-26 12:31:02,849 main.py:50] epoch 824, training loss: 7096.85, average training loss: 9391.34, base loss: 20510.97
[INFO 2017-06-26 12:31:03,213 main.py:50] epoch 825, training loss: 7098.63, average training loss: 9388.56, base loss: 20510.71
[INFO 2017-06-26 12:31:03,597 main.py:50] epoch 826, training loss: 7096.24, average training loss: 9385.79, base loss: 20510.76
[INFO 2017-06-26 12:31:03,958 main.py:50] epoch 827, training loss: 7000.53, average training loss: 9382.91, base loss: 20510.14
[INFO 2017-06-26 12:31:04,321 main.py:50] epoch 828, training loss: 7202.00, average training loss: 9380.28, base loss: 20510.23
[INFO 2017-06-26 12:31:04,685 main.py:50] epoch 829, training loss: 7075.62, average training loss: 9377.50, base loss: 20510.10
[INFO 2017-06-26 12:31:05,048 main.py:50] epoch 830, training loss: 7084.45, average training loss: 9374.74, base loss: 20509.70
[INFO 2017-06-26 12:31:05,408 main.py:50] epoch 831, training loss: 7226.15, average training loss: 9372.16, base loss: 20510.10
[INFO 2017-06-26 12:31:05,769 main.py:50] epoch 832, training loss: 7177.51, average training loss: 9369.52, base loss: 20510.43
[INFO 2017-06-26 12:31:06,131 main.py:50] epoch 833, training loss: 7161.43, average training loss: 9366.88, base loss: 20510.19
[INFO 2017-06-26 12:31:06,495 main.py:50] epoch 834, training loss: 7248.94, average training loss: 9364.34, base loss: 20510.41
[INFO 2017-06-26 12:31:06,860 main.py:50] epoch 835, training loss: 7168.74, average training loss: 9361.71, base loss: 20510.60
[INFO 2017-06-26 12:31:07,221 main.py:50] epoch 836, training loss: 7161.77, average training loss: 9359.09, base loss: 20510.54
[INFO 2017-06-26 12:31:07,583 main.py:50] epoch 837, training loss: 7131.73, average training loss: 9356.43, base loss: 20510.52
[INFO 2017-06-26 12:31:07,944 main.py:50] epoch 838, training loss: 7099.90, average training loss: 9353.74, base loss: 20510.39
[INFO 2017-06-26 12:31:08,306 main.py:50] epoch 839, training loss: 7240.38, average training loss: 9351.22, base loss: 20510.73
[INFO 2017-06-26 12:31:08,666 main.py:50] epoch 840, training loss: 7105.19, average training loss: 9348.55, base loss: 20511.00
[INFO 2017-06-26 12:31:09,027 main.py:50] epoch 841, training loss: 7246.91, average training loss: 9346.06, base loss: 20511.18
[INFO 2017-06-26 12:31:09,390 main.py:50] epoch 842, training loss: 7147.23, average training loss: 9343.45, base loss: 20511.31
[INFO 2017-06-26 12:31:09,752 main.py:50] epoch 843, training loss: 7109.90, average training loss: 9340.80, base loss: 20511.33
[INFO 2017-06-26 12:31:10,114 main.py:50] epoch 844, training loss: 7201.12, average training loss: 9338.27, base loss: 20511.46
[INFO 2017-06-26 12:31:10,475 main.py:50] epoch 845, training loss: 7080.73, average training loss: 9335.60, base loss: 20511.51
[INFO 2017-06-26 12:31:10,836 main.py:50] epoch 846, training loss: 7059.36, average training loss: 9332.91, base loss: 20511.77
[INFO 2017-06-26 12:31:11,199 main.py:50] epoch 847, training loss: 7164.28, average training loss: 9330.36, base loss: 20512.09
[INFO 2017-06-26 12:31:11,564 main.py:50] epoch 848, training loss: 7080.38, average training loss: 9327.71, base loss: 20512.19
[INFO 2017-06-26 12:31:11,929 main.py:50] epoch 849, training loss: 7122.44, average training loss: 9325.11, base loss: 20512.09
[INFO 2017-06-26 12:31:12,293 main.py:50] epoch 850, training loss: 7043.17, average training loss: 9322.43, base loss: 20512.11
[INFO 2017-06-26 12:31:12,658 main.py:50] epoch 851, training loss: 7082.80, average training loss: 9319.80, base loss: 20512.37
[INFO 2017-06-26 12:31:13,022 main.py:50] epoch 852, training loss: 7164.76, average training loss: 9317.27, base loss: 20512.68
[INFO 2017-06-26 12:31:13,383 main.py:50] epoch 853, training loss: 7208.84, average training loss: 9314.81, base loss: 20512.97
[INFO 2017-06-26 12:31:13,745 main.py:50] epoch 854, training loss: 7119.71, average training loss: 9312.24, base loss: 20513.05
[INFO 2017-06-26 12:31:14,121 main.py:50] epoch 855, training loss: 7067.03, average training loss: 9309.62, base loss: 20512.94
[INFO 2017-06-26 12:31:14,483 main.py:50] epoch 856, training loss: 7033.55, average training loss: 9306.96, base loss: 20512.60
[INFO 2017-06-26 12:31:14,845 main.py:50] epoch 857, training loss: 7192.47, average training loss: 9304.50, base loss: 20512.89
[INFO 2017-06-26 12:31:15,207 main.py:50] epoch 858, training loss: 7090.75, average training loss: 9301.92, base loss: 20513.09
[INFO 2017-06-26 12:31:15,568 main.py:50] epoch 859, training loss: 7167.27, average training loss: 9299.44, base loss: 20513.46
[INFO 2017-06-26 12:31:15,929 main.py:50] epoch 860, training loss: 7174.77, average training loss: 9296.97, base loss: 20513.75
[INFO 2017-06-26 12:31:16,290 main.py:50] epoch 861, training loss: 7124.54, average training loss: 9294.45, base loss: 20513.59
[INFO 2017-06-26 12:31:16,652 main.py:50] epoch 862, training loss: 7214.85, average training loss: 9292.04, base loss: 20513.75
[INFO 2017-06-26 12:31:17,014 main.py:50] epoch 863, training loss: 7253.03, average training loss: 9289.68, base loss: 20514.03
[INFO 2017-06-26 12:31:17,375 main.py:50] epoch 864, training loss: 7094.18, average training loss: 9287.14, base loss: 20513.93
[INFO 2017-06-26 12:31:17,737 main.py:50] epoch 865, training loss: 7226.91, average training loss: 9284.76, base loss: 20513.98
[INFO 2017-06-26 12:31:18,099 main.py:50] epoch 866, training loss: 7071.40, average training loss: 9282.21, base loss: 20513.76
[INFO 2017-06-26 12:31:18,460 main.py:50] epoch 867, training loss: 7210.48, average training loss: 9279.82, base loss: 20513.83
[INFO 2017-06-26 12:31:18,822 main.py:50] epoch 868, training loss: 7105.88, average training loss: 9277.32, base loss: 20513.79
[INFO 2017-06-26 12:31:19,184 main.py:50] epoch 869, training loss: 7176.02, average training loss: 9274.90, base loss: 20513.98
[INFO 2017-06-26 12:31:19,546 main.py:50] epoch 870, training loss: 7086.54, average training loss: 9272.39, base loss: 20514.11
[INFO 2017-06-26 12:31:19,908 main.py:50] epoch 871, training loss: 7094.58, average training loss: 9269.89, base loss: 20513.96
[INFO 2017-06-26 12:31:20,268 main.py:50] epoch 872, training loss: 7021.92, average training loss: 9267.32, base loss: 20513.84
[INFO 2017-06-26 12:31:20,629 main.py:50] epoch 873, training loss: 7121.32, average training loss: 9264.86, base loss: 20514.02
[INFO 2017-06-26 12:31:20,990 main.py:50] epoch 874, training loss: 7075.88, average training loss: 9262.36, base loss: 20513.99
[INFO 2017-06-26 12:31:21,352 main.py:50] epoch 875, training loss: 7109.97, average training loss: 9259.91, base loss: 20513.97
[INFO 2017-06-26 12:31:21,716 main.py:50] epoch 876, training loss: 7081.97, average training loss: 9257.42, base loss: 20513.98
[INFO 2017-06-26 12:31:22,076 main.py:50] epoch 877, training loss: 7065.04, average training loss: 9254.92, base loss: 20513.52
[INFO 2017-06-26 12:31:22,437 main.py:50] epoch 878, training loss: 7081.20, average training loss: 9252.45, base loss: 20513.52
[INFO 2017-06-26 12:31:22,798 main.py:50] epoch 879, training loss: 7103.83, average training loss: 9250.01, base loss: 20513.62
[INFO 2017-06-26 12:31:23,160 main.py:50] epoch 880, training loss: 7099.96, average training loss: 9247.57, base loss: 20513.80
[INFO 2017-06-26 12:31:23,521 main.py:50] epoch 881, training loss: 7088.43, average training loss: 9245.12, base loss: 20513.80
[INFO 2017-06-26 12:31:23,883 main.py:50] epoch 882, training loss: 7022.83, average training loss: 9242.61, base loss: 20513.75
[INFO 2017-06-26 12:31:24,243 main.py:50] epoch 883, training loss: 7089.98, average training loss: 9240.17, base loss: 20513.95
[INFO 2017-06-26 12:31:24,604 main.py:50] epoch 884, training loss: 7060.25, average training loss: 9237.71, base loss: 20513.85
[INFO 2017-06-26 12:31:24,966 main.py:50] epoch 885, training loss: 7036.84, average training loss: 9235.22, base loss: 20513.91
[INFO 2017-06-26 12:31:25,326 main.py:50] epoch 886, training loss: 7071.55, average training loss: 9232.78, base loss: 20514.23
[INFO 2017-06-26 12:31:25,687 main.py:50] epoch 887, training loss: 7049.32, average training loss: 9230.32, base loss: 20514.28
[INFO 2017-06-26 12:31:26,048 main.py:50] epoch 888, training loss: 7077.51, average training loss: 9227.90, base loss: 20514.32
[INFO 2017-06-26 12:31:26,410 main.py:50] epoch 889, training loss: 6994.87, average training loss: 9225.39, base loss: 20514.43
[INFO 2017-06-26 12:31:26,771 main.py:50] epoch 890, training loss: 7017.54, average training loss: 9222.92, base loss: 20514.50
[INFO 2017-06-26 12:31:27,132 main.py:50] epoch 891, training loss: 7051.62, average training loss: 9220.48, base loss: 20514.22
[INFO 2017-06-26 12:31:27,492 main.py:50] epoch 892, training loss: 7103.61, average training loss: 9218.11, base loss: 20514.48
[INFO 2017-06-26 12:31:27,854 main.py:50] epoch 893, training loss: 7181.19, average training loss: 9215.83, base loss: 20514.46
[INFO 2017-06-26 12:31:28,216 main.py:50] epoch 894, training loss: 7074.45, average training loss: 9213.44, base loss: 20514.52
[INFO 2017-06-26 12:31:28,576 main.py:50] epoch 895, training loss: 7071.21, average training loss: 9211.05, base loss: 20514.66
[INFO 2017-06-26 12:31:28,938 main.py:50] epoch 896, training loss: 7015.87, average training loss: 9208.60, base loss: 20514.82
[INFO 2017-06-26 12:31:29,299 main.py:50] epoch 897, training loss: 7141.97, average training loss: 9206.30, base loss: 20514.56
[INFO 2017-06-26 12:31:29,660 main.py:50] epoch 898, training loss: 7063.49, average training loss: 9203.92, base loss: 20514.50
[INFO 2017-06-26 12:31:30,020 main.py:50] epoch 899, training loss: 7122.62, average training loss: 9201.60, base loss: 20514.31
[INFO 2017-06-26 12:31:30,021 main.py:52] epoch 899, testing
[INFO 2017-06-26 12:31:31,505 main.py:103] average testing loss: 7085.59, base loss: 20490.72
[INFO 2017-06-26 12:31:31,506 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 12:31:31,512 main.py:76] current best accuracy: 7085.59
[INFO 2017-06-26 12:31:31,870 main.py:50] epoch 900, training loss: 6999.24, average training loss: 9199.16, base loss: 20514.18
[INFO 2017-06-26 12:31:32,231 main.py:50] epoch 901, training loss: 7059.35, average training loss: 9196.79, base loss: 20514.31
[INFO 2017-06-26 12:31:32,592 main.py:50] epoch 902, training loss: 7014.62, average training loss: 9194.37, base loss: 20513.99
[INFO 2017-06-26 12:31:32,953 main.py:50] epoch 903, training loss: 7153.92, average training loss: 9192.11, base loss: 20513.95
[INFO 2017-06-26 12:31:33,313 main.py:50] epoch 904, training loss: 6985.15, average training loss: 9189.68, base loss: 20513.80
[INFO 2017-06-26 12:31:33,676 main.py:50] epoch 905, training loss: 7073.49, average training loss: 9187.34, base loss: 20513.78
[INFO 2017-06-26 12:31:34,036 main.py:50] epoch 906, training loss: 7022.22, average training loss: 9184.95, base loss: 20513.65
[INFO 2017-06-26 12:31:34,396 main.py:50] epoch 907, training loss: 7047.38, average training loss: 9182.60, base loss: 20513.85
[INFO 2017-06-26 12:31:34,757 main.py:50] epoch 908, training loss: 7057.96, average training loss: 9180.26, base loss: 20513.63
[INFO 2017-06-26 12:31:35,118 main.py:50] epoch 909, training loss: 7031.38, average training loss: 9177.90, base loss: 20513.66
[INFO 2017-06-26 12:31:35,479 main.py:50] epoch 910, training loss: 7063.89, average training loss: 9175.58, base loss: 20513.76
[INFO 2017-06-26 12:31:35,842 main.py:50] epoch 911, training loss: 7131.59, average training loss: 9173.34, base loss: 20514.07
[INFO 2017-06-26 12:31:36,202 main.py:50] epoch 912, training loss: 7056.11, average training loss: 9171.02, base loss: 20514.20
[INFO 2017-06-26 12:31:36,563 main.py:50] epoch 913, training loss: 7016.75, average training loss: 9168.66, base loss: 20514.28
[INFO 2017-06-26 12:31:36,924 main.py:50] epoch 914, training loss: 7021.79, average training loss: 9166.32, base loss: 20514.13
[INFO 2017-06-26 12:31:37,286 main.py:50] epoch 915, training loss: 7055.13, average training loss: 9164.01, base loss: 20514.26
[INFO 2017-06-26 12:31:37,647 main.py:50] epoch 916, training loss: 7048.67, average training loss: 9161.70, base loss: 20514.35
[INFO 2017-06-26 12:31:38,008 main.py:50] epoch 917, training loss: 6926.39, average training loss: 9159.27, base loss: 20514.29
[INFO 2017-06-26 12:31:38,370 main.py:50] epoch 918, training loss: 7012.28, average training loss: 9156.93, base loss: 20513.87
[INFO 2017-06-26 12:31:38,731 main.py:50] epoch 919, training loss: 7094.51, average training loss: 9154.69, base loss: 20514.08
[INFO 2017-06-26 12:31:39,092 main.py:50] epoch 920, training loss: 7061.76, average training loss: 9152.42, base loss: 20514.04
[INFO 2017-06-26 12:31:39,455 main.py:50] epoch 921, training loss: 6915.43, average training loss: 9149.99, base loss: 20513.55
[INFO 2017-06-26 12:31:39,815 main.py:50] epoch 922, training loss: 7087.00, average training loss: 9147.76, base loss: 20513.66
[INFO 2017-06-26 12:31:40,177 main.py:50] epoch 923, training loss: 7070.76, average training loss: 9145.51, base loss: 20513.65
[INFO 2017-06-26 12:31:40,537 main.py:50] epoch 924, training loss: 6965.99, average training loss: 9143.15, base loss: 20513.67
[INFO 2017-06-26 12:31:40,898 main.py:50] epoch 925, training loss: 7030.28, average training loss: 9140.87, base loss: 20513.72
[INFO 2017-06-26 12:31:41,260 main.py:50] epoch 926, training loss: 6939.79, average training loss: 9138.50, base loss: 20513.64
[INFO 2017-06-26 12:31:41,622 main.py:50] epoch 927, training loss: 6867.64, average training loss: 9136.05, base loss: 20513.32
[INFO 2017-06-26 12:31:41,983 main.py:50] epoch 928, training loss: 7005.08, average training loss: 9133.76, base loss: 20513.12
[INFO 2017-06-26 12:31:42,344 main.py:50] epoch 929, training loss: 6929.51, average training loss: 9131.39, base loss: 20512.97
[INFO 2017-06-26 12:31:42,704 main.py:50] epoch 930, training loss: 7001.06, average training loss: 9129.10, base loss: 20513.14
[INFO 2017-06-26 12:31:43,065 main.py:50] epoch 931, training loss: 6950.14, average training loss: 9126.76, base loss: 20513.09
[INFO 2017-06-26 12:31:43,427 main.py:50] epoch 932, training loss: 6912.67, average training loss: 9124.39, base loss: 20512.97
[INFO 2017-06-26 12:31:43,788 main.py:50] epoch 933, training loss: 6953.19, average training loss: 9122.06, base loss: 20512.82
[INFO 2017-06-26 12:31:44,148 main.py:50] epoch 934, training loss: 6914.20, average training loss: 9119.70, base loss: 20512.97
[INFO 2017-06-26 12:31:44,509 main.py:50] epoch 935, training loss: 6979.89, average training loss: 9117.41, base loss: 20512.95
[INFO 2017-06-26 12:31:44,871 main.py:50] epoch 936, training loss: 6978.38, average training loss: 9115.13, base loss: 20513.19
[INFO 2017-06-26 12:31:45,231 main.py:50] epoch 937, training loss: 6994.29, average training loss: 9112.87, base loss: 20513.36
[INFO 2017-06-26 12:31:45,592 main.py:50] epoch 938, training loss: 6884.63, average training loss: 9110.50, base loss: 20513.18
[INFO 2017-06-26 12:31:45,952 main.py:50] epoch 939, training loss: 6901.76, average training loss: 9108.15, base loss: 20513.10
[INFO 2017-06-26 12:31:46,313 main.py:50] epoch 940, training loss: 6858.78, average training loss: 9105.76, base loss: 20512.92
[INFO 2017-06-26 12:31:46,676 main.py:50] epoch 941, training loss: 6962.01, average training loss: 9103.48, base loss: 20512.94
[INFO 2017-06-26 12:31:47,038 main.py:50] epoch 942, training loss: 6932.01, average training loss: 9101.18, base loss: 20512.41
[INFO 2017-06-26 12:31:47,399 main.py:50] epoch 943, training loss: 6861.65, average training loss: 9098.81, base loss: 20512.17
[INFO 2017-06-26 12:31:47,761 main.py:50] epoch 944, training loss: 6874.02, average training loss: 9096.45, base loss: 20512.01
[INFO 2017-06-26 12:31:48,122 main.py:50] epoch 945, training loss: 6982.76, average training loss: 9094.22, base loss: 20511.86
[INFO 2017-06-26 12:31:48,484 main.py:50] epoch 946, training loss: 6967.93, average training loss: 9091.97, base loss: 20511.84
[INFO 2017-06-26 12:31:48,846 main.py:50] epoch 947, training loss: 7073.51, average training loss: 9089.84, base loss: 20511.92
[INFO 2017-06-26 12:31:49,206 main.py:50] epoch 948, training loss: 7027.13, average training loss: 9087.67, base loss: 20512.15
[INFO 2017-06-26 12:31:49,567 main.py:50] epoch 949, training loss: 6962.73, average training loss: 9085.43, base loss: 20512.15
[INFO 2017-06-26 12:31:49,927 main.py:50] epoch 950, training loss: 7080.57, average training loss: 9083.33, base loss: 20512.43
[INFO 2017-06-26 12:31:50,289 main.py:50] epoch 951, training loss: 6972.48, average training loss: 9081.11, base loss: 20512.45
[INFO 2017-06-26 12:31:50,649 main.py:50] epoch 952, training loss: 7001.06, average training loss: 9078.93, base loss: 20512.82
[INFO 2017-06-26 12:31:51,010 main.py:50] epoch 953, training loss: 6928.77, average training loss: 9076.67, base loss: 20512.62
[INFO 2017-06-26 12:31:51,371 main.py:50] epoch 954, training loss: 7045.58, average training loss: 9074.54, base loss: 20512.81
[INFO 2017-06-26 12:31:51,733 main.py:50] epoch 955, training loss: 6981.60, average training loss: 9072.36, base loss: 20512.94
[INFO 2017-06-26 12:31:52,095 main.py:50] epoch 956, training loss: 6932.69, average training loss: 9070.12, base loss: 20512.67
[INFO 2017-06-26 12:31:52,457 main.py:50] epoch 957, training loss: 6945.67, average training loss: 9067.90, base loss: 20512.59
[INFO 2017-06-26 12:31:52,817 main.py:50] epoch 958, training loss: 6895.40, average training loss: 9065.64, base loss: 20512.55
[INFO 2017-06-26 12:31:53,178 main.py:50] epoch 959, training loss: 6945.84, average training loss: 9063.43, base loss: 20512.67
[INFO 2017-06-26 12:31:53,541 main.py:50] epoch 960, training loss: 6976.05, average training loss: 9061.26, base loss: 20512.21
[INFO 2017-06-26 12:31:53,903 main.py:50] epoch 961, training loss: 6998.05, average training loss: 9059.11, base loss: 20512.31
[INFO 2017-06-26 12:31:54,265 main.py:50] epoch 962, training loss: 7056.77, average training loss: 9057.03, base loss: 20512.60
[INFO 2017-06-26 12:31:54,625 main.py:50] epoch 963, training loss: 6970.59, average training loss: 9054.87, base loss: 20512.49
[INFO 2017-06-26 12:31:54,986 main.py:50] epoch 964, training loss: 6955.35, average training loss: 9052.69, base loss: 20512.42
[INFO 2017-06-26 12:31:55,348 main.py:50] epoch 965, training loss: 6918.99, average training loss: 9050.48, base loss: 20512.31
[INFO 2017-06-26 12:31:55,709 main.py:50] epoch 966, training loss: 7052.58, average training loss: 9048.42, base loss: 20512.74
[INFO 2017-06-26 12:31:56,070 main.py:50] epoch 967, training loss: 6931.43, average training loss: 9046.23, base loss: 20512.62
[INFO 2017-06-26 12:31:56,430 main.py:50] epoch 968, training loss: 6988.99, average training loss: 9044.11, base loss: 20512.65
[INFO 2017-06-26 12:31:56,792 main.py:50] epoch 969, training loss: 6936.67, average training loss: 9041.94, base loss: 20512.67
[INFO 2017-06-26 12:31:57,153 main.py:50] epoch 970, training loss: 6984.35, average training loss: 9039.82, base loss: 20512.80
[INFO 2017-06-26 12:31:57,515 main.py:50] epoch 971, training loss: 6900.20, average training loss: 9037.61, base loss: 20512.65
[INFO 2017-06-26 12:31:57,876 main.py:50] epoch 972, training loss: 6876.94, average training loss: 9035.39, base loss: 20512.65
[INFO 2017-06-26 12:31:58,238 main.py:50] epoch 973, training loss: 6930.00, average training loss: 9033.23, base loss: 20512.68
[INFO 2017-06-26 12:31:58,614 main.py:50] epoch 974, training loss: 6912.99, average training loss: 9031.06, base loss: 20512.76
[INFO 2017-06-26 12:31:58,976 main.py:50] epoch 975, training loss: 6833.45, average training loss: 9028.81, base loss: 20512.58
[INFO 2017-06-26 12:31:59,336 main.py:50] epoch 976, training loss: 6906.67, average training loss: 9026.63, base loss: 20512.64
[INFO 2017-06-26 12:31:59,697 main.py:50] epoch 977, training loss: 6837.65, average training loss: 9024.40, base loss: 20512.49
[INFO 2017-06-26 12:32:00,058 main.py:50] epoch 978, training loss: 7026.54, average training loss: 9022.36, base loss: 20512.79
[INFO 2017-06-26 12:32:00,419 main.py:50] epoch 979, training loss: 6978.36, average training loss: 9020.27, base loss: 20512.85
[INFO 2017-06-26 12:32:00,779 main.py:50] epoch 980, training loss: 6907.09, average training loss: 9018.12, base loss: 20512.79
[INFO 2017-06-26 12:32:01,141 main.py:50] epoch 981, training loss: 6844.70, average training loss: 9015.90, base loss: 20512.39
[INFO 2017-06-26 12:32:01,502 main.py:50] epoch 982, training loss: 6990.85, average training loss: 9013.84, base loss: 20512.38
[INFO 2017-06-26 12:32:01,864 main.py:50] epoch 983, training loss: 6881.23, average training loss: 9011.67, base loss: 20512.39
[INFO 2017-06-26 12:32:02,225 main.py:50] epoch 984, training loss: 6965.38, average training loss: 9009.60, base loss: 20512.55
[INFO 2017-06-26 12:32:02,587 main.py:50] epoch 985, training loss: 6896.34, average training loss: 9007.45, base loss: 20512.40
[INFO 2017-06-26 12:32:02,948 main.py:50] epoch 986, training loss: 6886.60, average training loss: 9005.31, base loss: 20512.52
[INFO 2017-06-26 12:32:03,308 main.py:50] epoch 987, training loss: 6928.89, average training loss: 9003.20, base loss: 20512.56
[INFO 2017-06-26 12:32:03,669 main.py:50] epoch 988, training loss: 6929.25, average training loss: 9001.11, base loss: 20512.74
[INFO 2017-06-26 12:32:04,029 main.py:50] epoch 989, training loss: 6908.29, average training loss: 8998.99, base loss: 20512.80
[INFO 2017-06-26 12:32:04,391 main.py:50] epoch 990, training loss: 7005.79, average training loss: 8996.98, base loss: 20512.97
[INFO 2017-06-26 12:32:04,753 main.py:50] epoch 991, training loss: 6859.51, average training loss: 8994.83, base loss: 20512.92
[INFO 2017-06-26 12:32:05,115 main.py:50] epoch 992, training loss: 6900.10, average training loss: 8992.72, base loss: 20512.88
[INFO 2017-06-26 12:32:05,476 main.py:50] epoch 993, training loss: 6912.77, average training loss: 8990.62, base loss: 20512.69
[INFO 2017-06-26 12:32:05,837 main.py:50] epoch 994, training loss: 6853.94, average training loss: 8988.48, base loss: 20512.38
[INFO 2017-06-26 12:32:06,198 main.py:50] epoch 995, training loss: 6889.85, average training loss: 8986.37, base loss: 20512.27
[INFO 2017-06-26 12:32:06,558 main.py:50] epoch 996, training loss: 6929.81, average training loss: 8984.31, base loss: 20512.43
[INFO 2017-06-26 12:32:06,919 main.py:50] epoch 997, training loss: 6871.35, average training loss: 8982.19, base loss: 20512.07
[INFO 2017-06-26 12:32:07,281 main.py:50] epoch 998, training loss: 6935.61, average training loss: 8980.14, base loss: 20512.01
[INFO 2017-06-26 12:32:07,643 main.py:50] epoch 999, training loss: 6940.23, average training loss: 8978.10, base loss: 20512.27
[INFO 2017-06-26 12:32:07,643 main.py:52] epoch 999, testing
[INFO 2017-06-26 12:32:09,124 main.py:103] average testing loss: 6927.73, base loss: 20482.00
[INFO 2017-06-26 12:32:09,125 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 12:32:09,131 main.py:76] current best accuracy: 6927.73
[INFO 2017-06-26 12:32:09,488 main.py:50] epoch 1000, training loss: 6855.92, average training loss: 8880.60, base loss: 20512.26
[INFO 2017-06-26 12:32:09,847 main.py:50] epoch 1001, training loss: 6962.64, average training loss: 8804.02, base loss: 20512.37
[INFO 2017-06-26 12:32:10,205 main.py:50] epoch 1002, training loss: 6903.73, average training loss: 8742.09, base loss: 20511.97
[INFO 2017-06-26 12:32:10,563 main.py:50] epoch 1003, training loss: 6953.05, average training loss: 8689.13, base loss: 20512.17
[INFO 2017-06-26 12:32:10,923 main.py:50] epoch 1004, training loss: 6875.94, average training loss: 8643.84, base loss: 20512.15
[INFO 2017-06-26 12:32:11,283 main.py:50] epoch 1005, training loss: 6879.82, average training loss: 8604.47, base loss: 20512.54
[INFO 2017-06-26 12:32:11,643 main.py:50] epoch 1006, training loss: 6994.77, average training loss: 8570.33, base loss: 20512.69
[INFO 2017-06-26 12:32:12,003 main.py:50] epoch 1007, training loss: 6866.52, average training loss: 8540.84, base loss: 20512.59
[INFO 2017-06-26 12:32:12,363 main.py:50] epoch 1008, training loss: 6887.51, average training loss: 8515.21, base loss: 20512.98
[INFO 2017-06-26 12:32:12,723 main.py:50] epoch 1009, training loss: 6845.00, average training loss: 8492.20, base loss: 20512.49
[INFO 2017-06-26 12:32:13,084 main.py:50] epoch 1010, training loss: 6900.87, average training loss: 8471.88, base loss: 20512.34
[INFO 2017-06-26 12:32:13,445 main.py:50] epoch 1011, training loss: 6818.22, average training loss: 8453.84, base loss: 20512.47
[INFO 2017-06-26 12:32:13,805 main.py:50] epoch 1012, training loss: 6954.46, average training loss: 8437.66, base loss: 20512.41
[INFO 2017-06-26 12:32:14,164 main.py:50] epoch 1013, training loss: 6957.35, average training loss: 8422.66, base loss: 20512.43
[INFO 2017-06-26 12:32:14,526 main.py:50] epoch 1014, training loss: 6826.68, average training loss: 8408.28, base loss: 20512.32
[INFO 2017-06-26 12:32:14,887 main.py:50] epoch 1015, training loss: 6849.51, average training loss: 8394.95, base loss: 20512.31
[INFO 2017-06-26 12:32:15,246 main.py:50] epoch 1016, training loss: 6940.62, average training loss: 8382.64, base loss: 20512.43
[INFO 2017-06-26 12:32:15,607 main.py:50] epoch 1017, training loss: 6811.25, average training loss: 8370.63, base loss: 20512.49
[INFO 2017-06-26 12:32:15,968 main.py:50] epoch 1018, training loss: 6992.49, average training loss: 8359.67, base loss: 20512.92
[INFO 2017-06-26 12:32:16,329 main.py:50] epoch 1019, training loss: 6751.96, average training loss: 8348.59, base loss: 20512.91
[INFO 2017-06-26 12:32:16,689 main.py:50] epoch 1020, training loss: 6944.91, average training loss: 8337.81, base loss: 20512.37
[INFO 2017-06-26 12:32:17,049 main.py:50] epoch 1021, training loss: 6889.84, average training loss: 8327.51, base loss: 20512.34
[INFO 2017-06-26 12:32:17,409 main.py:50] epoch 1022, training loss: 6981.18, average training loss: 8317.79, base loss: 20512.43
[INFO 2017-06-26 12:32:17,769 main.py:50] epoch 1023, training loss: 6963.93, average training loss: 8308.16, base loss: 20512.18
[INFO 2017-06-26 12:32:18,130 main.py:50] epoch 1024, training loss: 6906.27, average training loss: 8299.13, base loss: 20512.21
[INFO 2017-06-26 12:32:18,491 main.py:50] epoch 1025, training loss: 6916.68, average training loss: 8290.36, base loss: 20512.40
[INFO 2017-06-26 12:32:18,852 main.py:50] epoch 1026, training loss: 6817.20, average training loss: 8281.75, base loss: 20512.44
[INFO 2017-06-26 12:32:19,213 main.py:50] epoch 1027, training loss: 6824.07, average training loss: 8273.00, base loss: 20512.02
[INFO 2017-06-26 12:32:19,575 main.py:50] epoch 1028, training loss: 6873.29, average training loss: 8264.81, base loss: 20511.84
[INFO 2017-06-26 12:32:19,936 main.py:50] epoch 1029, training loss: 6993.05, average training loss: 8256.91, base loss: 20512.23
[INFO 2017-06-26 12:32:20,296 main.py:50] epoch 1030, training loss: 6900.11, average training loss: 8248.63, base loss: 20511.73
[INFO 2017-06-26 12:32:20,655 main.py:50] epoch 1031, training loss: 6945.04, average training loss: 8240.84, base loss: 20511.88
[INFO 2017-06-26 12:32:21,016 main.py:50] epoch 1032, training loss: 6927.05, average training loss: 8232.84, base loss: 20511.84
[INFO 2017-06-26 12:32:21,376 main.py:50] epoch 1033, training loss: 6842.66, average training loss: 8225.17, base loss: 20512.19
[INFO 2017-06-26 12:32:21,735 main.py:50] epoch 1034, training loss: 6966.19, average training loss: 8217.76, base loss: 20512.48
[INFO 2017-06-26 12:32:22,097 main.py:50] epoch 1035, training loss: 6967.06, average training loss: 8210.41, base loss: 20512.19
[INFO 2017-06-26 12:32:22,458 main.py:50] epoch 1036, training loss: 6905.52, average training loss: 8203.20, base loss: 20512.33
[INFO 2017-06-26 12:32:22,820 main.py:50] epoch 1037, training loss: 6808.14, average training loss: 8195.96, base loss: 20512.07
[INFO 2017-06-26 12:32:23,181 main.py:50] epoch 1038, training loss: 6857.37, average training loss: 8188.68, base loss: 20511.99
[INFO 2017-06-26 12:32:23,541 main.py:50] epoch 1039, training loss: 6818.82, average training loss: 8181.21, base loss: 20511.80
[INFO 2017-06-26 12:32:23,902 main.py:50] epoch 1040, training loss: 6810.21, average training loss: 8174.08, base loss: 20511.69
[INFO 2017-06-26 12:32:24,263 main.py:50] epoch 1041, training loss: 6887.69, average training loss: 8167.21, base loss: 20511.72
[INFO 2017-06-26 12:32:24,624 main.py:50] epoch 1042, training loss: 6811.25, average training loss: 8160.26, base loss: 20511.64
[INFO 2017-06-26 12:32:24,985 main.py:50] epoch 1043, training loss: 6828.35, average training loss: 8153.30, base loss: 20511.51
[INFO 2017-06-26 12:32:25,346 main.py:50] epoch 1044, training loss: 6908.93, average training loss: 8146.48, base loss: 20511.02
[INFO 2017-06-26 12:32:25,706 main.py:50] epoch 1045, training loss: 6926.15, average training loss: 8140.04, base loss: 20511.05
[INFO 2017-06-26 12:32:26,068 main.py:50] epoch 1046, training loss: 6873.30, average training loss: 8133.65, base loss: 20511.29
[INFO 2017-06-26 12:32:26,427 main.py:50] epoch 1047, training loss: 6858.86, average training loss: 8127.21, base loss: 20511.27
[INFO 2017-06-26 12:32:26,787 main.py:50] epoch 1048, training loss: 6851.07, average training loss: 8120.72, base loss: 20510.77
[INFO 2017-06-26 12:32:27,147 main.py:50] epoch 1049, training loss: 6917.50, average training loss: 8114.55, base loss: 20511.03
[INFO 2017-06-26 12:32:27,508 main.py:50] epoch 1050, training loss: 6859.39, average training loss: 8108.28, base loss: 20511.20
[INFO 2017-06-26 12:32:27,868 main.py:50] epoch 1051, training loss: 6774.41, average training loss: 8101.87, base loss: 20510.96
[INFO 2017-06-26 12:32:28,228 main.py:50] epoch 1052, training loss: 6863.32, average training loss: 8095.67, base loss: 20511.15
[INFO 2017-06-26 12:32:28,587 main.py:50] epoch 1053, training loss: 6925.18, average training loss: 8089.56, base loss: 20511.21
[INFO 2017-06-26 12:32:28,947 main.py:50] epoch 1054, training loss: 6910.09, average training loss: 8083.55, base loss: 20511.49
[INFO 2017-06-26 12:32:29,309 main.py:50] epoch 1055, training loss: 6812.73, average training loss: 8077.45, base loss: 20511.48
[INFO 2017-06-26 12:32:29,670 main.py:50] epoch 1056, training loss: 6859.04, average training loss: 8071.54, base loss: 20512.06
[INFO 2017-06-26 12:32:30,031 main.py:50] epoch 1057, training loss: 6853.69, average training loss: 8065.63, base loss: 20512.27
[INFO 2017-06-26 12:32:30,392 main.py:50] epoch 1058, training loss: 6887.46, average training loss: 8059.38, base loss: 20511.77
[INFO 2017-06-26 12:32:30,752 main.py:50] epoch 1059, training loss: 6885.45, average training loss: 8053.35, base loss: 20512.02
[INFO 2017-06-26 12:32:31,114 main.py:50] epoch 1060, training loss: 6880.74, average training loss: 8047.53, base loss: 20512.00
[INFO 2017-06-26 12:32:31,475 main.py:50] epoch 1061, training loss: 6872.32, average training loss: 8041.70, base loss: 20511.88
[INFO 2017-06-26 12:32:31,836 main.py:50] epoch 1062, training loss: 6998.48, average training loss: 8036.25, base loss: 20511.93
[INFO 2017-06-26 12:32:32,195 main.py:50] epoch 1063, training loss: 6841.35, average training loss: 8030.14, base loss: 20511.51
[INFO 2017-06-26 12:32:32,556 main.py:50] epoch 1064, training loss: 6843.34, average training loss: 8024.27, base loss: 20511.24
[INFO 2017-06-26 12:32:32,916 main.py:50] epoch 1065, training loss: 6906.47, average training loss: 8018.59, base loss: 20511.18
[INFO 2017-06-26 12:32:33,274 main.py:50] epoch 1066, training loss: 6820.15, average training loss: 8012.80, base loss: 20510.95
[INFO 2017-06-26 12:32:33,634 main.py:50] epoch 1067, training loss: 6900.12, average training loss: 8007.44, base loss: 20511.34
[INFO 2017-06-26 12:32:33,994 main.py:50] epoch 1068, training loss: 6830.54, average training loss: 8001.74, base loss: 20511.10
[INFO 2017-06-26 12:32:34,355 main.py:50] epoch 1069, training loss: 6871.76, average training loss: 7996.35, base loss: 20511.20
[INFO 2017-06-26 12:32:34,715 main.py:50] epoch 1070, training loss: 6964.31, average training loss: 7991.04, base loss: 20511.11
[INFO 2017-06-26 12:32:35,074 main.py:50] epoch 1071, training loss: 6778.47, average training loss: 7985.39, base loss: 20510.84
[INFO 2017-06-26 12:32:35,433 main.py:50] epoch 1072, training loss: 6924.73, average training loss: 7980.04, base loss: 20510.68
[INFO 2017-06-26 12:32:35,794 main.py:50] epoch 1073, training loss: 6798.81, average training loss: 7974.75, base loss: 20510.46
[INFO 2017-06-26 12:32:36,154 main.py:50] epoch 1074, training loss: 6783.68, average training loss: 7969.37, base loss: 20510.22
[INFO 2017-06-26 12:32:36,514 main.py:50] epoch 1075, training loss: 7008.16, average training loss: 7964.17, base loss: 20510.00
[INFO 2017-06-26 12:32:36,875 main.py:50] epoch 1076, training loss: 6834.78, average training loss: 7959.10, base loss: 20510.26
[INFO 2017-06-26 12:32:37,236 main.py:50] epoch 1077, training loss: 6880.28, average training loss: 7954.13, base loss: 20510.34
[INFO 2017-06-26 12:32:37,597 main.py:50] epoch 1078, training loss: 6905.72, average training loss: 7948.86, base loss: 20509.99
[INFO 2017-06-26 12:32:37,957 main.py:50] epoch 1079, training loss: 6776.29, average training loss: 7943.56, base loss: 20509.79
[INFO 2017-06-26 12:32:38,317 main.py:50] epoch 1080, training loss: 6818.92, average training loss: 7938.36, base loss: 20509.46
[INFO 2017-06-26 12:32:38,677 main.py:50] epoch 1081, training loss: 6841.19, average training loss: 7933.43, base loss: 20509.49
[INFO 2017-06-26 12:32:39,038 main.py:50] epoch 1082, training loss: 6833.16, average training loss: 7928.13, base loss: 20509.39
[INFO 2017-06-26 12:32:39,398 main.py:50] epoch 1083, training loss: 6813.70, average training loss: 7923.23, base loss: 20509.35
[INFO 2017-06-26 12:32:39,760 main.py:50] epoch 1084, training loss: 6789.05, average training loss: 7918.06, base loss: 20509.12
[INFO 2017-06-26 12:32:40,121 main.py:50] epoch 1085, training loss: 6854.87, average training loss: 7913.27, base loss: 20509.13
[INFO 2017-06-26 12:32:40,482 main.py:50] epoch 1086, training loss: 6799.16, average training loss: 7908.48, base loss: 20509.25
[INFO 2017-06-26 12:32:40,842 main.py:50] epoch 1087, training loss: 6774.80, average training loss: 7903.29, base loss: 20508.71
[INFO 2017-06-26 12:32:41,203 main.py:50] epoch 1088, training loss: 6851.34, average training loss: 7898.69, base loss: 20508.85
[INFO 2017-06-26 12:32:41,563 main.py:50] epoch 1089, training loss: 6745.88, average training loss: 7894.04, base loss: 20508.84
[INFO 2017-06-26 12:32:41,923 main.py:50] epoch 1090, training loss: 6882.08, average training loss: 7889.44, base loss: 20508.94
[INFO 2017-06-26 12:32:42,283 main.py:50] epoch 1091, training loss: 6759.04, average training loss: 7884.68, base loss: 20508.97
[INFO 2017-06-26 12:32:42,642 main.py:50] epoch 1092, training loss: 6872.41, average training loss: 7880.16, base loss: 20508.90
[INFO 2017-06-26 12:32:43,016 main.py:50] epoch 1093, training loss: 6802.44, average training loss: 7875.74, base loss: 20509.18
[INFO 2017-06-26 12:32:43,377 main.py:50] epoch 1094, training loss: 6898.95, average training loss: 7871.12, base loss: 20509.00
[INFO 2017-06-26 12:32:43,736 main.py:50] epoch 1095, training loss: 6773.47, average training loss: 7866.62, base loss: 20509.26
[INFO 2017-06-26 12:32:44,096 main.py:50] epoch 1096, training loss: 6858.09, average training loss: 7862.08, base loss: 20509.09
[INFO 2017-06-26 12:32:44,456 main.py:50] epoch 1097, training loss: 6841.41, average training loss: 7857.71, base loss: 20509.05
[INFO 2017-06-26 12:32:44,816 main.py:50] epoch 1098, training loss: 6801.89, average training loss: 7853.31, base loss: 20508.89
[INFO 2017-06-26 12:32:45,177 main.py:50] epoch 1099, training loss: 6811.31, average training loss: 7848.96, base loss: 20509.07
[INFO 2017-06-26 12:32:45,177 main.py:52] epoch 1099, testing
[INFO 2017-06-26 12:32:46,660 main.py:103] average testing loss: 6819.62, base loss: 20486.84
[INFO 2017-06-26 12:32:46,661 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 12:32:46,667 main.py:76] current best accuracy: 6819.62
[INFO 2017-06-26 12:32:47,025 main.py:50] epoch 1100, training loss: 6735.97, average training loss: 7844.39, base loss: 20508.70
[INFO 2017-06-26 12:32:47,385 main.py:50] epoch 1101, training loss: 6778.29, average training loss: 7839.90, base loss: 20508.75
[INFO 2017-06-26 12:32:47,745 main.py:50] epoch 1102, training loss: 6719.10, average training loss: 7835.51, base loss: 20508.75
[INFO 2017-06-26 12:32:48,106 main.py:50] epoch 1103, training loss: 6783.12, average training loss: 7831.29, base loss: 20508.76
[INFO 2017-06-26 12:32:48,467 main.py:50] epoch 1104, training loss: 6762.10, average training loss: 7826.88, base loss: 20508.59
[INFO 2017-06-26 12:32:48,827 main.py:50] epoch 1105, training loss: 6753.33, average training loss: 7822.80, base loss: 20508.97
[INFO 2017-06-26 12:32:49,187 main.py:50] epoch 1106, training loss: 6774.15, average training loss: 7818.56, base loss: 20509.09
[INFO 2017-06-26 12:32:49,547 main.py:50] epoch 1107, training loss: 6890.53, average training loss: 7814.39, base loss: 20508.93
[INFO 2017-06-26 12:32:49,907 main.py:50] epoch 1108, training loss: 6912.88, average training loss: 7810.18, base loss: 20508.90
[INFO 2017-06-26 12:32:50,268 main.py:50] epoch 1109, training loss: 6882.70, average training loss: 7806.05, base loss: 20509.12
[INFO 2017-06-26 12:32:50,628 main.py:50] epoch 1110, training loss: 6771.46, average training loss: 7801.84, base loss: 20509.16
[INFO 2017-06-26 12:32:50,989 main.py:50] epoch 1111, training loss: 6771.73, average training loss: 7797.54, base loss: 20508.64
[INFO 2017-06-26 12:32:51,349 main.py:50] epoch 1112, training loss: 6954.78, average training loss: 7793.67, base loss: 20508.71
[INFO 2017-06-26 12:32:51,710 main.py:50] epoch 1113, training loss: 6827.15, average training loss: 7789.67, base loss: 20508.54
[INFO 2017-06-26 12:32:52,070 main.py:50] epoch 1114, training loss: 6712.12, average training loss: 7785.56, base loss: 20508.27
[INFO 2017-06-26 12:32:52,430 main.py:50] epoch 1115, training loss: 6984.49, average training loss: 7782.00, base loss: 20508.78
[INFO 2017-06-26 12:32:52,790 main.py:50] epoch 1116, training loss: 6867.77, average training loss: 7778.22, base loss: 20508.70
[INFO 2017-06-26 12:32:53,150 main.py:50] epoch 1117, training loss: 6927.00, average training loss: 7774.48, base loss: 20508.87
[INFO 2017-06-26 12:32:53,510 main.py:50] epoch 1118, training loss: 6794.35, average training loss: 7770.83, base loss: 20509.29
[INFO 2017-06-26 12:32:53,871 main.py:50] epoch 1119, training loss: 6876.57, average training loss: 7767.17, base loss: 20509.42
[INFO 2017-06-26 12:32:54,231 main.py:50] epoch 1120, training loss: 6858.18, average training loss: 7763.43, base loss: 20509.47
[INFO 2017-06-26 12:32:54,593 main.py:50] epoch 1121, training loss: 6665.48, average training loss: 7759.62, base loss: 20509.36
[INFO 2017-06-26 12:32:54,953 main.py:50] epoch 1122, training loss: 6840.75, average training loss: 7755.86, base loss: 20509.07
[INFO 2017-06-26 12:32:55,314 main.py:50] epoch 1123, training loss: 6916.94, average training loss: 7752.44, base loss: 20509.28
[INFO 2017-06-26 12:32:55,674 main.py:50] epoch 1124, training loss: 6834.78, average training loss: 7748.75, base loss: 20509.19
[INFO 2017-06-26 12:32:56,035 main.py:50] epoch 1125, training loss: 6884.53, average training loss: 7745.39, base loss: 20509.24
[INFO 2017-06-26 12:32:56,396 main.py:50] epoch 1126, training loss: 6853.79, average training loss: 7741.79, base loss: 20509.38
[INFO 2017-06-26 12:32:56,755 main.py:50] epoch 1127, training loss: 6884.71, average training loss: 7738.20, base loss: 20509.15
[INFO 2017-06-26 12:32:57,115 main.py:50] epoch 1128, training loss: 6836.24, average training loss: 7734.62, base loss: 20509.01
[INFO 2017-06-26 12:32:57,475 main.py:50] epoch 1129, training loss: 6874.01, average training loss: 7731.28, base loss: 20509.04
[INFO 2017-06-26 12:32:57,835 main.py:50] epoch 1130, training loss: 6807.94, average training loss: 7727.77, base loss: 20509.01
[INFO 2017-06-26 12:32:58,194 main.py:50] epoch 1131, training loss: 6893.51, average training loss: 7724.38, base loss: 20509.01
[INFO 2017-06-26 12:32:58,555 main.py:50] epoch 1132, training loss: 6725.72, average training loss: 7720.77, base loss: 20508.70
[INFO 2017-06-26 12:32:58,915 main.py:50] epoch 1133, training loss: 6801.77, average training loss: 7717.38, base loss: 20508.71
[INFO 2017-06-26 12:32:59,277 main.py:50] epoch 1134, training loss: 6766.13, average training loss: 7713.90, base loss: 20508.82
[INFO 2017-06-26 12:32:59,636 main.py:50] epoch 1135, training loss: 6827.24, average training loss: 7710.45, base loss: 20508.46
[INFO 2017-06-26 12:32:59,996 main.py:50] epoch 1136, training loss: 6777.56, average training loss: 7707.04, base loss: 20508.50
[INFO 2017-06-26 12:33:00,357 main.py:50] epoch 1137, training loss: 6798.36, average training loss: 7703.75, base loss: 20508.62
[INFO 2017-06-26 12:33:00,718 main.py:50] epoch 1138, training loss: 6767.41, average training loss: 7700.53, base loss: 20508.77
[INFO 2017-06-26 12:33:01,078 main.py:50] epoch 1139, training loss: 6781.56, average training loss: 7697.21, base loss: 20509.03
[INFO 2017-06-26 12:33:01,438 main.py:50] epoch 1140, training loss: 6774.09, average training loss: 7693.93, base loss: 20509.03
[INFO 2017-06-26 12:33:01,798 main.py:50] epoch 1141, training loss: 6648.35, average training loss: 7690.48, base loss: 20508.82
[INFO 2017-06-26 12:33:02,159 main.py:50] epoch 1142, training loss: 6850.73, average training loss: 7687.19, base loss: 20508.91
[INFO 2017-06-26 12:33:02,520 main.py:50] epoch 1143, training loss: 6836.13, average training loss: 7684.08, base loss: 20509.16
[INFO 2017-06-26 12:33:02,880 main.py:50] epoch 1144, training loss: 6752.20, average training loss: 7680.81, base loss: 20509.41
[INFO 2017-06-26 12:33:03,240 main.py:50] epoch 1145, training loss: 6787.05, average training loss: 7677.60, base loss: 20509.58
[INFO 2017-06-26 12:33:03,601 main.py:50] epoch 1146, training loss: 6794.62, average training loss: 7674.24, base loss: 20509.63
[INFO 2017-06-26 12:33:03,962 main.py:50] epoch 1147, training loss: 6749.92, average training loss: 7671.15, base loss: 20509.91
[INFO 2017-06-26 12:33:04,322 main.py:50] epoch 1148, training loss: 6713.66, average training loss: 7667.75, base loss: 20509.80
[INFO 2017-06-26 12:33:04,683 main.py:50] epoch 1149, training loss: 6813.11, average training loss: 7664.74, base loss: 20510.03
[INFO 2017-06-26 12:33:05,042 main.py:50] epoch 1150, training loss: 6848.32, average training loss: 7661.70, base loss: 20509.75
[INFO 2017-06-26 12:33:05,404 main.py:50] epoch 1151, training loss: 6768.75, average training loss: 7658.65, base loss: 20509.60
[INFO 2017-06-26 12:33:05,764 main.py:50] epoch 1152, training loss: 6820.13, average training loss: 7655.51, base loss: 20509.08
[INFO 2017-06-26 12:33:06,125 main.py:50] epoch 1153, training loss: 6725.34, average training loss: 7652.42, base loss: 20508.95
[INFO 2017-06-26 12:33:06,486 main.py:50] epoch 1154, training loss: 6883.93, average training loss: 7649.57, base loss: 20509.19
[INFO 2017-06-26 12:33:06,847 main.py:50] epoch 1155, training loss: 6753.66, average training loss: 7646.49, base loss: 20509.24
[INFO 2017-06-26 12:33:07,206 main.py:50] epoch 1156, training loss: 6758.28, average training loss: 7643.44, base loss: 20509.10
[INFO 2017-06-26 12:33:07,566 main.py:50] epoch 1157, training loss: 6691.16, average training loss: 7640.55, base loss: 20509.14
[INFO 2017-06-26 12:33:07,927 main.py:50] epoch 1158, training loss: 6741.79, average training loss: 7637.53, base loss: 20509.11
[INFO 2017-06-26 12:33:08,287 main.py:50] epoch 1159, training loss: 6747.27, average training loss: 7634.51, base loss: 20508.94
[INFO 2017-06-26 12:33:08,647 main.py:50] epoch 1160, training loss: 6818.38, average training loss: 7631.72, base loss: 20509.34
[INFO 2017-06-26 12:33:09,008 main.py:50] epoch 1161, training loss: 6851.14, average training loss: 7628.98, base loss: 20509.69
[INFO 2017-06-26 12:33:09,372 main.py:50] epoch 1162, training loss: 6761.87, average training loss: 7626.10, base loss: 20509.42
[INFO 2017-06-26 12:33:09,732 main.py:50] epoch 1163, training loss: 6760.95, average training loss: 7622.95, base loss: 20509.36
[INFO 2017-06-26 12:33:10,094 main.py:50] epoch 1164, training loss: 6754.50, average training loss: 7619.99, base loss: 20509.31
[INFO 2017-06-26 12:33:10,456 main.py:50] epoch 1165, training loss: 6820.34, average training loss: 7617.07, base loss: 20509.21
[INFO 2017-06-26 12:33:10,817 main.py:50] epoch 1166, training loss: 6887.09, average training loss: 7614.37, base loss: 20509.39
[INFO 2017-06-26 12:33:11,178 main.py:50] epoch 1167, training loss: 6721.43, average training loss: 7611.35, base loss: 20509.24
[INFO 2017-06-26 12:33:11,538 main.py:50] epoch 1168, training loss: 6697.71, average training loss: 7608.27, base loss: 20508.79
[INFO 2017-06-26 12:33:11,898 main.py:50] epoch 1169, training loss: 6751.21, average training loss: 7605.36, base loss: 20508.89
[INFO 2017-06-26 12:33:12,258 main.py:50] epoch 1170, training loss: 6760.99, average training loss: 7602.53, base loss: 20509.09
[INFO 2017-06-26 12:33:12,620 main.py:50] epoch 1171, training loss: 6742.35, average training loss: 7599.67, base loss: 20509.17
[INFO 2017-06-26 12:33:12,980 main.py:50] epoch 1172, training loss: 6736.94, average training loss: 7596.79, base loss: 20509.30
[INFO 2017-06-26 12:33:13,340 main.py:50] epoch 1173, training loss: 6775.65, average training loss: 7593.86, base loss: 20509.22
[INFO 2017-06-26 12:33:13,700 main.py:50] epoch 1174, training loss: 6773.78, average training loss: 7591.02, base loss: 20509.56
[INFO 2017-06-26 12:33:14,059 main.py:50] epoch 1175, training loss: 6759.15, average training loss: 7588.25, base loss: 20509.54
[INFO 2017-06-26 12:33:14,420 main.py:50] epoch 1176, training loss: 6822.26, average training loss: 7585.61, base loss: 20509.72
[INFO 2017-06-26 12:33:14,780 main.py:50] epoch 1177, training loss: 6657.98, average training loss: 7582.65, base loss: 20509.50
[INFO 2017-06-26 12:33:15,139 main.py:50] epoch 1178, training loss: 6696.95, average training loss: 7579.87, base loss: 20509.28
[INFO 2017-06-26 12:33:15,499 main.py:50] epoch 1179, training loss: 6732.34, average training loss: 7577.06, base loss: 20509.14
[INFO 2017-06-26 12:33:15,859 main.py:50] epoch 1180, training loss: 6726.65, average training loss: 7574.18, base loss: 20509.08
[INFO 2017-06-26 12:33:16,220 main.py:50] epoch 1181, training loss: 6840.54, average training loss: 7571.57, base loss: 20509.06
[INFO 2017-06-26 12:33:16,579 main.py:50] epoch 1182, training loss: 6669.03, average training loss: 7568.77, base loss: 20508.75
[INFO 2017-06-26 12:33:16,939 main.py:50] epoch 1183, training loss: 6784.93, average training loss: 7566.10, base loss: 20508.57
[INFO 2017-06-26 12:33:17,300 main.py:50] epoch 1184, training loss: 6756.98, average training loss: 7563.44, base loss: 20508.60
[INFO 2017-06-26 12:33:17,661 main.py:50] epoch 1185, training loss: 6688.10, average training loss: 7560.80, base loss: 20508.40
[INFO 2017-06-26 12:33:18,020 main.py:50] epoch 1186, training loss: 6804.97, average training loss: 7558.15, base loss: 20508.71
[INFO 2017-06-26 12:33:18,380 main.py:50] epoch 1187, training loss: 6819.10, average training loss: 7555.64, base loss: 20508.82
[INFO 2017-06-26 12:33:18,740 main.py:50] epoch 1188, training loss: 6701.56, average training loss: 7553.07, base loss: 20508.73
[INFO 2017-06-26 12:33:19,101 main.py:50] epoch 1189, training loss: 6655.18, average training loss: 7550.34, base loss: 20508.50
[INFO 2017-06-26 12:33:19,461 main.py:50] epoch 1190, training loss: 6749.33, average training loss: 7547.69, base loss: 20508.47
[INFO 2017-06-26 12:33:19,823 main.py:50] epoch 1191, training loss: 6824.48, average training loss: 7545.12, base loss: 20508.15
[INFO 2017-06-26 12:33:20,184 main.py:50] epoch 1192, training loss: 6727.91, average training loss: 7542.61, base loss: 20508.41
[INFO 2017-06-26 12:33:20,543 main.py:50] epoch 1193, training loss: 6779.57, average training loss: 7540.16, base loss: 20508.74
[INFO 2017-06-26 12:33:20,903 main.py:50] epoch 1194, training loss: 6829.76, average training loss: 7537.70, base loss: 20508.96
[INFO 2017-06-26 12:33:21,263 main.py:50] epoch 1195, training loss: 6713.30, average training loss: 7535.16, base loss: 20508.86
[INFO 2017-06-26 12:33:21,622 main.py:50] epoch 1196, training loss: 6890.11, average training loss: 7532.87, base loss: 20509.09
[INFO 2017-06-26 12:33:21,986 main.py:50] epoch 1197, training loss: 6670.17, average training loss: 7530.13, base loss: 20509.05
[INFO 2017-06-26 12:33:22,348 main.py:50] epoch 1198, training loss: 6793.37, average training loss: 7527.53, base loss: 20508.83
[INFO 2017-06-26 12:33:22,712 main.py:50] epoch 1199, training loss: 6801.59, average training loss: 7525.05, base loss: 20508.68
[INFO 2017-06-26 12:33:22,712 main.py:52] epoch 1199, testing
[INFO 2017-06-26 12:33:24,198 main.py:103] average testing loss: 6736.58, base loss: 20498.96
[INFO 2017-06-26 12:33:24,199 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 12:33:24,205 main.py:76] current best accuracy: 6736.58
[INFO 2017-06-26 12:33:24,581 main.py:50] epoch 1200, training loss: 6679.28, average training loss: 7522.54, base loss: 20508.41
[INFO 2017-06-26 12:33:24,945 main.py:50] epoch 1201, training loss: 6790.33, average training loss: 7520.07, base loss: 20508.47
[INFO 2017-06-26 12:33:25,310 main.py:50] epoch 1202, training loss: 6788.32, average training loss: 7517.70, base loss: 20508.53
[INFO 2017-06-26 12:33:25,675 main.py:50] epoch 1203, training loss: 6720.27, average training loss: 7515.17, base loss: 20508.51
[INFO 2017-06-26 12:33:26,039 main.py:50] epoch 1204, training loss: 6715.17, average training loss: 7512.62, base loss: 20508.21
[INFO 2017-06-26 12:33:26,405 main.py:50] epoch 1205, training loss: 6763.31, average training loss: 7510.12, base loss: 20507.82
[INFO 2017-06-26 12:33:26,770 main.py:50] epoch 1206, training loss: 6789.44, average training loss: 7507.75, base loss: 20507.73
[INFO 2017-06-26 12:33:27,135 main.py:50] epoch 1207, training loss: 6755.80, average training loss: 7505.29, base loss: 20507.72
[INFO 2017-06-26 12:33:27,499 main.py:50] epoch 1208, training loss: 6752.98, average training loss: 7502.86, base loss: 20507.76
[INFO 2017-06-26 12:33:27,864 main.py:50] epoch 1209, training loss: 6749.16, average training loss: 7500.49, base loss: 20507.67
[INFO 2017-06-26 12:33:28,229 main.py:50] epoch 1210, training loss: 6838.10, average training loss: 7498.01, base loss: 20508.07
[INFO 2017-06-26 12:33:28,592 main.py:50] epoch 1211, training loss: 6722.23, average training loss: 7495.55, base loss: 20508.43
[INFO 2017-06-26 12:33:28,957 main.py:50] epoch 1212, training loss: 6752.61, average training loss: 7493.24, base loss: 20508.35
[INFO 2017-06-26 12:33:29,320 main.py:50] epoch 1213, training loss: 6718.62, average training loss: 7490.70, base loss: 20508.38
[INFO 2017-06-26 12:33:29,684 main.py:50] epoch 1214, training loss: 6696.92, average training loss: 7488.23, base loss: 20508.61
[INFO 2017-06-26 12:33:30,049 main.py:50] epoch 1215, training loss: 6607.53, average training loss: 7485.52, base loss: 20508.21
[INFO 2017-06-26 12:33:30,414 main.py:50] epoch 1216, training loss: 6848.85, average training loss: 7483.17, base loss: 20507.82
[INFO 2017-06-26 12:33:30,779 main.py:50] epoch 1217, training loss: 6728.64, average training loss: 7480.76, base loss: 20507.83
[INFO 2017-06-26 12:33:31,142 main.py:50] epoch 1218, training loss: 6809.92, average training loss: 7478.62, base loss: 20507.72
[INFO 2017-06-26 12:33:31,507 main.py:50] epoch 1219, training loss: 6857.74, average training loss: 7476.34, base loss: 20507.53
[INFO 2017-06-26 12:33:31,871 main.py:50] epoch 1220, training loss: 6804.40, average training loss: 7474.18, base loss: 20508.06
[INFO 2017-06-26 12:33:32,235 main.py:50] epoch 1221, training loss: 6744.34, average training loss: 7471.89, base loss: 20508.13
[INFO 2017-06-26 12:33:32,600 main.py:50] epoch 1222, training loss: 6774.76, average training loss: 7469.51, base loss: 20508.24
[INFO 2017-06-26 12:33:32,963 main.py:50] epoch 1223, training loss: 6732.04, average training loss: 7467.14, base loss: 20508.26
[INFO 2017-06-26 12:33:33,330 main.py:50] epoch 1224, training loss: 6770.70, average training loss: 7464.87, base loss: 20508.27
[INFO 2017-06-26 12:33:33,694 main.py:50] epoch 1225, training loss: 6678.42, average training loss: 7462.32, base loss: 20508.16
[INFO 2017-06-26 12:33:34,059 main.py:50] epoch 1226, training loss: 6834.01, average training loss: 7460.28, base loss: 20508.65
[INFO 2017-06-26 12:33:34,424 main.py:50] epoch 1227, training loss: 6695.64, average training loss: 7457.93, base loss: 20508.60
[INFO 2017-06-26 12:33:34,788 main.py:50] epoch 1228, training loss: 6723.96, average training loss: 7455.56, base loss: 20508.57
[INFO 2017-06-26 12:33:35,153 main.py:50] epoch 1229, training loss: 6722.83, average training loss: 7453.29, base loss: 20508.80
[INFO 2017-06-26 12:33:35,517 main.py:50] epoch 1230, training loss: 6753.06, average training loss: 7451.14, base loss: 20509.10
[INFO 2017-06-26 12:33:35,881 main.py:50] epoch 1231, training loss: 6697.70, average training loss: 7448.64, base loss: 20509.30
[INFO 2017-06-26 12:33:36,245 main.py:50] epoch 1232, training loss: 6718.14, average training loss: 7446.39, base loss: 20509.15
[INFO 2017-06-26 12:33:36,610 main.py:50] epoch 1233, training loss: 6621.36, average training loss: 7443.90, base loss: 20509.01
[INFO 2017-06-26 12:33:36,974 main.py:50] epoch 1234, training loss: 6770.71, average training loss: 7441.61, base loss: 20508.92
[INFO 2017-06-26 12:33:37,338 main.py:50] epoch 1235, training loss: 6682.15, average training loss: 7439.24, base loss: 20508.47
[INFO 2017-06-26 12:33:37,702 main.py:50] epoch 1236, training loss: 6696.45, average training loss: 7436.99, base loss: 20508.65
[INFO 2017-06-26 12:33:38,066 main.py:50] epoch 1237, training loss: 6778.22, average training loss: 7434.77, base loss: 20509.07
[INFO 2017-06-26 12:33:38,431 main.py:50] epoch 1238, training loss: 6714.40, average training loss: 7432.50, base loss: 20509.34
[INFO 2017-06-26 12:33:38,795 main.py:50] epoch 1239, training loss: 6679.13, average training loss: 7430.24, base loss: 20509.33
[INFO 2017-06-26 12:33:39,160 main.py:50] epoch 1240, training loss: 6678.29, average training loss: 7428.00, base loss: 20509.56
[INFO 2017-06-26 12:33:39,525 main.py:50] epoch 1241, training loss: 6707.51, average training loss: 7425.77, base loss: 20509.56
[INFO 2017-06-26 12:33:39,890 main.py:50] epoch 1242, training loss: 6787.05, average training loss: 7423.78, base loss: 20509.84
[INFO 2017-06-26 12:33:40,254 main.py:50] epoch 1243, training loss: 6696.13, average training loss: 7421.57, base loss: 20509.82
[INFO 2017-06-26 12:33:40,617 main.py:50] epoch 1244, training loss: 6654.83, average training loss: 7419.41, base loss: 20509.65
[INFO 2017-06-26 12:33:40,981 main.py:50] epoch 1245, training loss: 6638.33, average training loss: 7417.02, base loss: 20509.68
[INFO 2017-06-26 12:33:41,346 main.py:50] epoch 1246, training loss: 6615.72, average training loss: 7414.80, base loss: 20509.84
[INFO 2017-06-26 12:33:41,711 main.py:50] epoch 1247, training loss: 6727.29, average training loss: 7412.68, base loss: 20510.15
[INFO 2017-06-26 12:33:42,076 main.py:50] epoch 1248, training loss: 6724.93, average training loss: 7410.59, base loss: 20510.44
[INFO 2017-06-26 12:33:42,440 main.py:50] epoch 1249, training loss: 6736.99, average training loss: 7408.33, base loss: 20510.30
[INFO 2017-06-26 12:33:42,805 main.py:50] epoch 1250, training loss: 6714.90, average training loss: 7406.04, base loss: 20510.14
[INFO 2017-06-26 12:33:43,169 main.py:50] epoch 1251, training loss: 6693.10, average training loss: 7403.80, base loss: 20510.09
[INFO 2017-06-26 12:33:43,533 main.py:50] epoch 1252, training loss: 6722.91, average training loss: 7401.59, base loss: 20510.09
[INFO 2017-06-26 12:33:43,896 main.py:50] epoch 1253, training loss: 6645.79, average training loss: 7399.35, base loss: 20509.92
[INFO 2017-06-26 12:33:44,261 main.py:50] epoch 1254, training loss: 6688.53, average training loss: 7397.19, base loss: 20509.75
[INFO 2017-06-26 12:33:44,624 main.py:50] epoch 1255, training loss: 6742.54, average training loss: 7394.99, base loss: 20509.62
[INFO 2017-06-26 12:33:44,988 main.py:50] epoch 1256, training loss: 6778.84, average training loss: 7392.86, base loss: 20509.48
[INFO 2017-06-26 12:33:45,352 main.py:50] epoch 1257, training loss: 6695.14, average training loss: 7390.63, base loss: 20509.13
[INFO 2017-06-26 12:33:45,716 main.py:50] epoch 1258, training loss: 6644.25, average training loss: 7388.45, base loss: 20509.05
[INFO 2017-06-26 12:33:46,080 main.py:50] epoch 1259, training loss: 6625.13, average training loss: 7386.20, base loss: 20509.02
[INFO 2017-06-26 12:33:46,443 main.py:50] epoch 1260, training loss: 6765.49, average training loss: 7384.14, base loss: 20509.43
[INFO 2017-06-26 12:33:46,808 main.py:50] epoch 1261, training loss: 6729.43, average training loss: 7382.08, base loss: 20509.86
[INFO 2017-06-26 12:33:47,172 main.py:50] epoch 1262, training loss: 6723.78, average training loss: 7380.02, base loss: 20510.15
[INFO 2017-06-26 12:33:47,536 main.py:50] epoch 1263, training loss: 6712.46, average training loss: 7378.05, base loss: 20510.70
[INFO 2017-06-26 12:33:47,899 main.py:50] epoch 1264, training loss: 6717.87, average training loss: 7375.93, base loss: 20510.32
[INFO 2017-06-26 12:33:48,273 main.py:50] epoch 1265, training loss: 6783.04, average training loss: 7373.89, base loss: 20510.41
[INFO 2017-06-26 12:33:48,637 main.py:50] epoch 1266, training loss: 6664.27, average training loss: 7371.66, base loss: 20509.95
[INFO 2017-06-26 12:33:49,001 main.py:50] epoch 1267, training loss: 6739.06, average training loss: 7369.62, base loss: 20509.93
[INFO 2017-06-26 12:33:49,365 main.py:50] epoch 1268, training loss: 6668.80, average training loss: 7367.52, base loss: 20510.13
[INFO 2017-06-26 12:33:49,730 main.py:50] epoch 1269, training loss: 6703.84, average training loss: 7365.56, base loss: 20510.17
[INFO 2017-06-26 12:33:50,095 main.py:50] epoch 1270, training loss: 6684.92, average training loss: 7363.36, base loss: 20509.88
[INFO 2017-06-26 12:33:50,460 main.py:50] epoch 1271, training loss: 6688.81, average training loss: 7361.33, base loss: 20509.90
[INFO 2017-06-26 12:33:50,824 main.py:50] epoch 1272, training loss: 6684.80, average training loss: 7359.41, base loss: 20510.05
[INFO 2017-06-26 12:33:51,189 main.py:50] epoch 1273, training loss: 6577.66, average training loss: 7357.32, base loss: 20509.88
[INFO 2017-06-26 12:33:51,555 main.py:50] epoch 1274, training loss: 6700.31, average training loss: 7355.23, base loss: 20509.87
[INFO 2017-06-26 12:33:51,919 main.py:50] epoch 1275, training loss: 6573.65, average training loss: 7352.97, base loss: 20509.62
[INFO 2017-06-26 12:33:52,283 main.py:50] epoch 1276, training loss: 6650.01, average training loss: 7351.11, base loss: 20509.95
[INFO 2017-06-26 12:33:52,647 main.py:50] epoch 1277, training loss: 6852.16, average training loss: 7349.13, base loss: 20510.25
[INFO 2017-06-26 12:33:53,012 main.py:50] epoch 1278, training loss: 6613.57, average training loss: 7347.05, base loss: 20510.42
[INFO 2017-06-26 12:33:53,376 main.py:50] epoch 1279, training loss: 6570.30, average training loss: 7344.87, base loss: 20510.38
[INFO 2017-06-26 12:33:53,742 main.py:50] epoch 1280, training loss: 6748.89, average training loss: 7342.77, base loss: 20510.20
[INFO 2017-06-26 12:33:54,107 main.py:50] epoch 1281, training loss: 6724.49, average training loss: 7340.87, base loss: 20510.23
[INFO 2017-06-26 12:33:54,471 main.py:50] epoch 1282, training loss: 6745.81, average training loss: 7339.03, base loss: 20510.61
[INFO 2017-06-26 12:33:54,837 main.py:50] epoch 1283, training loss: 6673.33, average training loss: 7336.94, base loss: 20510.85
[INFO 2017-06-26 12:33:55,204 main.py:50] epoch 1284, training loss: 6607.02, average training loss: 7334.96, base loss: 20510.84
[INFO 2017-06-26 12:33:55,569 main.py:50] epoch 1285, training loss: 6652.64, average training loss: 7333.01, base loss: 20511.03
[INFO 2017-06-26 12:33:55,934 main.py:50] epoch 1286, training loss: 6762.26, average training loss: 7331.07, base loss: 20510.75
[INFO 2017-06-26 12:33:56,300 main.py:50] epoch 1287, training loss: 6726.81, average training loss: 7329.01, base loss: 20510.47
[INFO 2017-06-26 12:33:56,665 main.py:50] epoch 1288, training loss: 6653.40, average training loss: 7327.03, base loss: 20510.60
[INFO 2017-06-26 12:33:57,031 main.py:50] epoch 1289, training loss: 6698.58, average training loss: 7325.06, base loss: 20510.44
[INFO 2017-06-26 12:33:57,394 main.py:50] epoch 1290, training loss: 6649.46, average training loss: 7322.97, base loss: 20510.14
[INFO 2017-06-26 12:33:57,759 main.py:50] epoch 1291, training loss: 6676.81, average training loss: 7321.03, base loss: 20510.01
[INFO 2017-06-26 12:33:58,123 main.py:50] epoch 1292, training loss: 6627.95, average training loss: 7319.12, base loss: 20509.84
[INFO 2017-06-26 12:33:58,489 main.py:50] epoch 1293, training loss: 6746.65, average training loss: 7317.30, base loss: 20510.03
[INFO 2017-06-26 12:33:58,855 main.py:50] epoch 1294, training loss: 6604.25, average training loss: 7315.35, base loss: 20510.07
[INFO 2017-06-26 12:33:59,220 main.py:50] epoch 1295, training loss: 6561.00, average training loss: 7313.29, base loss: 20509.69
[INFO 2017-06-26 12:33:59,585 main.py:50] epoch 1296, training loss: 6629.70, average training loss: 7311.33, base loss: 20509.67
[INFO 2017-06-26 12:33:59,951 main.py:50] epoch 1297, training loss: 6675.82, average training loss: 7309.56, base loss: 20509.87
[INFO 2017-06-26 12:34:00,317 main.py:50] epoch 1298, training loss: 6741.65, average training loss: 7307.67, base loss: 20509.90
[INFO 2017-06-26 12:34:00,682 main.py:50] epoch 1299, training loss: 6709.06, average training loss: 7305.81, base loss: 20509.63
[INFO 2017-06-26 12:34:00,682 main.py:52] epoch 1299, testing
[INFO 2017-06-26 12:34:02,168 main.py:103] average testing loss: 6644.93, base loss: 20497.51
[INFO 2017-06-26 12:34:02,168 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 12:34:02,174 main.py:76] current best accuracy: 6644.93
[INFO 2017-06-26 12:34:02,536 main.py:50] epoch 1300, training loss: 6635.20, average training loss: 7303.88, base loss: 20510.14
[INFO 2017-06-26 12:34:02,899 main.py:50] epoch 1301, training loss: 6712.54, average training loss: 7302.10, base loss: 20509.99
[INFO 2017-06-26 12:34:03,264 main.py:50] epoch 1302, training loss: 6744.83, average training loss: 7300.38, base loss: 20510.23
[INFO 2017-06-26 12:34:03,628 main.py:50] epoch 1303, training loss: 6797.74, average training loss: 7298.83, base loss: 20510.69
[INFO 2017-06-26 12:34:03,993 main.py:50] epoch 1304, training loss: 6757.30, average training loss: 7296.95, base loss: 20510.63
[INFO 2017-06-26 12:34:04,356 main.py:50] epoch 1305, training loss: 6715.50, average training loss: 7295.33, base loss: 20510.86
[INFO 2017-06-26 12:34:04,721 main.py:50] epoch 1306, training loss: 6637.28, average training loss: 7293.39, base loss: 20510.72
[INFO 2017-06-26 12:34:05,085 main.py:50] epoch 1307, training loss: 6680.62, average training loss: 7291.66, base loss: 20511.22
[INFO 2017-06-26 12:34:05,450 main.py:50] epoch 1308, training loss: 6699.02, average training loss: 7289.90, base loss: 20511.29
[INFO 2017-06-26 12:34:05,814 main.py:50] epoch 1309, training loss: 6707.53, average training loss: 7288.10, base loss: 20511.20
[INFO 2017-06-26 12:34:06,179 main.py:50] epoch 1310, training loss: 6711.26, average training loss: 7286.45, base loss: 20511.40
[INFO 2017-06-26 12:34:06,544 main.py:50] epoch 1311, training loss: 6657.57, average training loss: 7284.72, base loss: 20511.62
[INFO 2017-06-26 12:34:06,909 main.py:50] epoch 1312, training loss: 6781.59, average training loss: 7283.16, base loss: 20511.74
[INFO 2017-06-26 12:34:07,275 main.py:50] epoch 1313, training loss: 6677.72, average training loss: 7281.42, base loss: 20511.78
[INFO 2017-06-26 12:34:07,638 main.py:50] epoch 1314, training loss: 6642.39, average training loss: 7279.70, base loss: 20511.96
[INFO 2017-06-26 12:34:08,004 main.py:50] epoch 1315, training loss: 6660.77, average training loss: 7277.91, base loss: 20512.13
[INFO 2017-06-26 12:34:08,370 main.py:50] epoch 1316, training loss: 6677.04, average training loss: 7276.12, base loss: 20512.16
[INFO 2017-06-26 12:34:08,733 main.py:50] epoch 1317, training loss: 6693.91, average training loss: 7274.52, base loss: 20512.43
[INFO 2017-06-26 12:34:09,096 main.py:50] epoch 1318, training loss: 6633.34, average training loss: 7272.62, base loss: 20512.22
[INFO 2017-06-26 12:34:09,470 main.py:50] epoch 1319, training loss: 6761.48, average training loss: 7271.15, base loss: 20512.52
[INFO 2017-06-26 12:34:09,832 main.py:50] epoch 1320, training loss: 6683.57, average training loss: 7269.42, base loss: 20512.87
[INFO 2017-06-26 12:34:10,193 main.py:50] epoch 1321, training loss: 6647.87, average training loss: 7267.77, base loss: 20513.00
[INFO 2017-06-26 12:34:10,553 main.py:50] epoch 1322, training loss: 6699.88, average training loss: 7266.11, base loss: 20512.98
[INFO 2017-06-26 12:34:10,913 main.py:50] epoch 1323, training loss: 6622.15, average training loss: 7264.39, base loss: 20513.14
[INFO 2017-06-26 12:34:11,274 main.py:50] epoch 1324, training loss: 6565.28, average training loss: 7262.54, base loss: 20512.92
[INFO 2017-06-26 12:34:11,633 main.py:50] epoch 1325, training loss: 6595.50, average training loss: 7260.77, base loss: 20512.72
[INFO 2017-06-26 12:34:11,993 main.py:50] epoch 1326, training loss: 6692.04, average training loss: 7259.22, base loss: 20512.99
[INFO 2017-06-26 12:34:12,353 main.py:50] epoch 1327, training loss: 6585.63, average training loss: 7257.37, base loss: 20512.65
[INFO 2017-06-26 12:34:12,713 main.py:50] epoch 1328, training loss: 6743.88, average training loss: 7255.66, base loss: 20512.52
[INFO 2017-06-26 12:34:13,073 main.py:50] epoch 1329, training loss: 6674.66, average training loss: 7254.08, base loss: 20513.00
[INFO 2017-06-26 12:34:13,435 main.py:50] epoch 1330, training loss: 6595.51, average training loss: 7252.34, base loss: 20512.79
[INFO 2017-06-26 12:34:13,795 main.py:50] epoch 1331, training loss: 6574.24, average training loss: 7250.45, base loss: 20512.59
[INFO 2017-06-26 12:34:14,156 main.py:50] epoch 1332, training loss: 6617.86, average training loss: 7248.59, base loss: 20512.14
[INFO 2017-06-26 12:34:14,517 main.py:50] epoch 1333, training loss: 6648.15, average training loss: 7246.83, base loss: 20511.83
[INFO 2017-06-26 12:34:14,878 main.py:50] epoch 1334, training loss: 6643.59, average training loss: 7245.05, base loss: 20511.70
[INFO 2017-06-26 12:34:15,237 main.py:50] epoch 1335, training loss: 6633.98, average training loss: 7243.26, base loss: 20511.93
[INFO 2017-06-26 12:34:15,598 main.py:50] epoch 1336, training loss: 6671.74, average training loss: 7241.56, base loss: 20511.92
[INFO 2017-06-26 12:34:15,957 main.py:50] epoch 1337, training loss: 6599.69, average training loss: 7239.89, base loss: 20512.40
[INFO 2017-06-26 12:34:16,319 main.py:50] epoch 1338, training loss: 6604.52, average training loss: 7238.15, base loss: 20512.24
[INFO 2017-06-26 12:34:16,679 main.py:50] epoch 1339, training loss: 6525.98, average training loss: 7236.40, base loss: 20512.31
[INFO 2017-06-26 12:34:17,039 main.py:50] epoch 1340, training loss: 6704.77, average training loss: 7234.63, base loss: 20512.48
[INFO 2017-06-26 12:34:17,400 main.py:50] epoch 1341, training loss: 6616.59, average training loss: 7232.90, base loss: 20512.55
[INFO 2017-06-26 12:34:17,760 main.py:50] epoch 1342, training loss: 6551.81, average training loss: 7231.24, base loss: 20512.59
[INFO 2017-06-26 12:34:18,120 main.py:50] epoch 1343, training loss: 6660.43, average training loss: 7229.66, base loss: 20512.90
[INFO 2017-06-26 12:34:18,480 main.py:50] epoch 1344, training loss: 6675.35, average training loss: 7228.04, base loss: 20512.64
[INFO 2017-06-26 12:34:18,840 main.py:50] epoch 1345, training loss: 6689.93, average training loss: 7226.34, base loss: 20512.74
[INFO 2017-06-26 12:34:19,201 main.py:50] epoch 1346, training loss: 6661.24, average training loss: 7224.68, base loss: 20513.04
[INFO 2017-06-26 12:34:19,561 main.py:50] epoch 1347, training loss: 6617.86, average training loss: 7222.87, base loss: 20512.91
[INFO 2017-06-26 12:34:19,922 main.py:50] epoch 1348, training loss: 6728.17, average training loss: 7221.33, base loss: 20513.40
[INFO 2017-06-26 12:34:20,282 main.py:50] epoch 1349, training loss: 6667.27, average training loss: 7219.81, base loss: 20513.43
[INFO 2017-06-26 12:34:20,641 main.py:50] epoch 1350, training loss: 6694.92, average training loss: 7218.19, base loss: 20513.54
[INFO 2017-06-26 12:34:21,001 main.py:50] epoch 1351, training loss: 6657.07, average training loss: 7216.50, base loss: 20513.19
[INFO 2017-06-26 12:34:21,363 main.py:50] epoch 1352, training loss: 6629.33, average training loss: 7214.71, base loss: 20512.97
[INFO 2017-06-26 12:34:21,723 main.py:50] epoch 1353, training loss: 6658.33, average training loss: 7213.02, base loss: 20512.71
[INFO 2017-06-26 12:34:22,084 main.py:50] epoch 1354, training loss: 6569.83, average training loss: 7211.35, base loss: 20512.26
[INFO 2017-06-26 12:34:22,445 main.py:50] epoch 1355, training loss: 6670.38, average training loss: 7209.87, base loss: 20512.57
[INFO 2017-06-26 12:34:22,805 main.py:50] epoch 1356, training loss: 6630.93, average training loss: 7208.20, base loss: 20512.45
[INFO 2017-06-26 12:34:23,166 main.py:50] epoch 1357, training loss: 6748.63, average training loss: 7206.69, base loss: 20512.42
[INFO 2017-06-26 12:34:23,525 main.py:50] epoch 1358, training loss: 6678.42, average training loss: 7205.05, base loss: 20512.45
[INFO 2017-06-26 12:34:23,886 main.py:50] epoch 1359, training loss: 6712.69, average training loss: 7203.38, base loss: 20512.67
[INFO 2017-06-26 12:34:24,247 main.py:50] epoch 1360, training loss: 6681.01, average training loss: 7201.80, base loss: 20512.62
[INFO 2017-06-26 12:34:24,608 main.py:50] epoch 1361, training loss: 6656.95, average training loss: 7200.30, base loss: 20512.95
[INFO 2017-06-26 12:34:24,967 main.py:50] epoch 1362, training loss: 6593.89, average training loss: 7198.60, base loss: 20512.89
[INFO 2017-06-26 12:34:25,327 main.py:50] epoch 1363, training loss: 6643.88, average training loss: 7197.02, base loss: 20513.17
[INFO 2017-06-26 12:34:25,687 main.py:50] epoch 1364, training loss: 6631.36, average training loss: 7195.41, base loss: 20513.11
[INFO 2017-06-26 12:34:26,048 main.py:50] epoch 1365, training loss: 6655.64, average training loss: 7193.92, base loss: 20513.03
[INFO 2017-06-26 12:34:26,409 main.py:50] epoch 1366, training loss: 6599.41, average training loss: 7192.36, base loss: 20513.15
[INFO 2017-06-26 12:34:26,770 main.py:50] epoch 1367, training loss: 6536.13, average training loss: 7190.62, base loss: 20512.57
[INFO 2017-06-26 12:34:27,129 main.py:50] epoch 1368, training loss: 6690.04, average training loss: 7189.10, base loss: 20512.81
[INFO 2017-06-26 12:34:27,489 main.py:50] epoch 1369, training loss: 6519.77, average training loss: 7187.31, base loss: 20512.42
[INFO 2017-06-26 12:34:27,850 main.py:50] epoch 1370, training loss: 6643.40, average training loss: 7185.71, base loss: 20512.46
[INFO 2017-06-26 12:34:28,210 main.py:50] epoch 1371, training loss: 6615.16, average training loss: 7184.15, base loss: 20512.55
[INFO 2017-06-26 12:34:28,571 main.py:50] epoch 1372, training loss: 6690.49, average training loss: 7182.56, base loss: 20512.69
[INFO 2017-06-26 12:34:28,932 main.py:50] epoch 1373, training loss: 6594.20, average training loss: 7180.88, base loss: 20512.79
[INFO 2017-06-26 12:34:29,291 main.py:50] epoch 1374, training loss: 6594.02, average training loss: 7179.20, base loss: 20512.44
[INFO 2017-06-26 12:34:29,651 main.py:50] epoch 1375, training loss: 6724.62, average training loss: 7177.87, base loss: 20512.79
[INFO 2017-06-26 12:34:30,012 main.py:50] epoch 1376, training loss: 6640.90, average training loss: 7176.40, base loss: 20513.05
[INFO 2017-06-26 12:34:30,371 main.py:50] epoch 1377, training loss: 6607.21, average training loss: 7174.83, base loss: 20512.96
[INFO 2017-06-26 12:34:30,731 main.py:50] epoch 1378, training loss: 6678.72, average training loss: 7173.25, base loss: 20512.80
[INFO 2017-06-26 12:34:31,092 main.py:50] epoch 1379, training loss: 6587.11, average training loss: 7171.74, base loss: 20513.33
[INFO 2017-06-26 12:34:31,452 main.py:50] epoch 1380, training loss: 6671.08, average training loss: 7170.24, base loss: 20513.12
[INFO 2017-06-26 12:34:31,812 main.py:50] epoch 1381, training loss: 6619.06, average training loss: 7168.60, base loss: 20512.82
[INFO 2017-06-26 12:34:32,171 main.py:50] epoch 1382, training loss: 6717.61, average training loss: 7167.24, base loss: 20512.84
[INFO 2017-06-26 12:34:32,532 main.py:50] epoch 1383, training loss: 6679.25, average training loss: 7165.65, base loss: 20512.88
[INFO 2017-06-26 12:34:32,892 main.py:50] epoch 1384, training loss: 6593.54, average training loss: 7164.12, base loss: 20512.75
[INFO 2017-06-26 12:34:33,254 main.py:50] epoch 1385, training loss: 6681.17, average training loss: 7162.73, base loss: 20513.19
[INFO 2017-06-26 12:34:33,612 main.py:50] epoch 1386, training loss: 6594.53, average training loss: 7161.00, base loss: 20512.67
[INFO 2017-06-26 12:34:33,972 main.py:50] epoch 1387, training loss: 6584.74, average training loss: 7159.45, base loss: 20512.72
[INFO 2017-06-26 12:34:34,333 main.py:50] epoch 1388, training loss: 6574.91, average training loss: 7157.87, base loss: 20512.70
[INFO 2017-06-26 12:34:34,693 main.py:50] epoch 1389, training loss: 6650.26, average training loss: 7156.31, base loss: 20513.02
[INFO 2017-06-26 12:34:35,053 main.py:50] epoch 1390, training loss: 6636.99, average training loss: 7154.67, base loss: 20512.69
[INFO 2017-06-26 12:34:35,412 main.py:50] epoch 1391, training loss: 6702.36, average training loss: 7153.35, base loss: 20513.16
[INFO 2017-06-26 12:34:35,772 main.py:50] epoch 1392, training loss: 6491.85, average training loss: 7151.80, base loss: 20513.12
[INFO 2017-06-26 12:34:36,131 main.py:50] epoch 1393, training loss: 6552.47, average training loss: 7150.24, base loss: 20513.10
[INFO 2017-06-26 12:34:36,491 main.py:50] epoch 1394, training loss: 6594.15, average training loss: 7148.63, base loss: 20513.36
[INFO 2017-06-26 12:34:36,852 main.py:50] epoch 1395, training loss: 6610.51, average training loss: 7147.12, base loss: 20513.68
[INFO 2017-06-26 12:34:37,211 main.py:50] epoch 1396, training loss: 6595.14, average training loss: 7145.66, base loss: 20513.56
[INFO 2017-06-26 12:34:37,571 main.py:50] epoch 1397, training loss: 6505.51, average training loss: 7144.16, base loss: 20513.47
[INFO 2017-06-26 12:34:37,931 main.py:50] epoch 1398, training loss: 6579.37, average training loss: 7142.59, base loss: 20513.19
[INFO 2017-06-26 12:34:38,292 main.py:50] epoch 1399, training loss: 6621.59, average training loss: 7141.18, base loss: 20513.23
[INFO 2017-06-26 12:34:38,292 main.py:52] epoch 1399, testing
[INFO 2017-06-26 12:34:39,775 main.py:103] average testing loss: 6615.51, base loss: 20528.83
[INFO 2017-06-26 12:34:39,775 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 12:34:39,781 main.py:76] current best accuracy: 6615.51
[INFO 2017-06-26 12:34:40,139 main.py:50] epoch 1400, training loss: 6576.02, average training loss: 7139.54, base loss: 20512.98
[INFO 2017-06-26 12:34:40,498 main.py:50] epoch 1401, training loss: 6565.39, average training loss: 7137.96, base loss: 20512.73
[INFO 2017-06-26 12:34:40,857 main.py:50] epoch 1402, training loss: 6686.32, average training loss: 7136.57, base loss: 20512.92
[INFO 2017-06-26 12:34:41,219 main.py:50] epoch 1403, training loss: 6548.22, average training loss: 7134.92, base loss: 20513.09
[INFO 2017-06-26 12:34:41,579 main.py:50] epoch 1404, training loss: 6607.74, average training loss: 7133.40, base loss: 20512.93
[INFO 2017-06-26 12:34:41,940 main.py:50] epoch 1405, training loss: 6550.13, average training loss: 7131.80, base loss: 20512.52
[INFO 2017-06-26 12:34:42,301 main.py:50] epoch 1406, training loss: 6692.10, average training loss: 7130.30, base loss: 20512.68
[INFO 2017-06-26 12:34:42,661 main.py:50] epoch 1407, training loss: 6725.74, average training loss: 7128.98, base loss: 20513.08
[INFO 2017-06-26 12:34:43,020 main.py:50] epoch 1408, training loss: 6574.39, average training loss: 7127.38, base loss: 20512.70
[INFO 2017-06-26 12:34:43,382 main.py:50] epoch 1409, training loss: 6645.49, average training loss: 7125.95, base loss: 20512.65
[INFO 2017-06-26 12:34:43,741 main.py:50] epoch 1410, training loss: 6594.91, average training loss: 7124.44, base loss: 20512.53
[INFO 2017-06-26 12:34:44,101 main.py:50] epoch 1411, training loss: 6643.32, average training loss: 7122.97, base loss: 20512.65
[INFO 2017-06-26 12:34:44,461 main.py:50] epoch 1412, training loss: 6549.59, average training loss: 7121.34, base loss: 20512.55
[INFO 2017-06-26 12:34:44,822 main.py:50] epoch 1413, training loss: 6529.28, average training loss: 7119.89, base loss: 20512.75
[INFO 2017-06-26 12:34:45,183 main.py:50] epoch 1414, training loss: 6628.87, average training loss: 7118.27, base loss: 20512.13
[INFO 2017-06-26 12:34:45,542 main.py:50] epoch 1415, training loss: 6620.21, average training loss: 7116.81, base loss: 20512.19
[INFO 2017-06-26 12:34:45,902 main.py:50] epoch 1416, training loss: 6525.75, average training loss: 7115.22, base loss: 20511.88
[INFO 2017-06-26 12:34:46,264 main.py:50] epoch 1417, training loss: 6632.61, average training loss: 7113.86, base loss: 20511.57
[INFO 2017-06-26 12:34:46,625 main.py:50] epoch 1418, training loss: 6692.49, average training loss: 7112.59, base loss: 20511.58
[INFO 2017-06-26 12:34:46,985 main.py:50] epoch 1419, training loss: 6554.00, average training loss: 7111.14, base loss: 20511.33
[INFO 2017-06-26 12:34:47,347 main.py:50] epoch 1420, training loss: 6666.92, average training loss: 7109.72, base loss: 20511.12
[INFO 2017-06-26 12:34:47,708 main.py:50] epoch 1421, training loss: 6617.38, average training loss: 7108.43, base loss: 20511.68
[INFO 2017-06-26 12:34:48,068 main.py:50] epoch 1422, training loss: 6657.95, average training loss: 7107.04, base loss: 20511.69
[INFO 2017-06-26 12:34:48,430 main.py:50] epoch 1423, training loss: 6729.19, average training loss: 7105.90, base loss: 20512.42
[INFO 2017-06-26 12:34:48,790 main.py:50] epoch 1424, training loss: 6555.85, average training loss: 7104.47, base loss: 20512.26
[INFO 2017-06-26 12:34:49,151 main.py:50] epoch 1425, training loss: 6598.28, average training loss: 7103.10, base loss: 20512.26
[INFO 2017-06-26 12:34:49,511 main.py:50] epoch 1426, training loss: 6609.53, average training loss: 7101.78, base loss: 20512.37
[INFO 2017-06-26 12:34:49,871 main.py:50] epoch 1427, training loss: 6672.25, average training loss: 7100.50, base loss: 20512.56
[INFO 2017-06-26 12:34:50,232 main.py:50] epoch 1428, training loss: 6657.66, average training loss: 7099.21, base loss: 20512.81
[INFO 2017-06-26 12:34:50,591 main.py:50] epoch 1429, training loss: 6545.40, average training loss: 7097.71, base loss: 20512.61
[INFO 2017-06-26 12:34:50,951 main.py:50] epoch 1430, training loss: 6609.15, average training loss: 7096.29, base loss: 20512.40
[INFO 2017-06-26 12:34:51,313 main.py:50] epoch 1431, training loss: 6616.79, average training loss: 7094.97, base loss: 20512.52
[INFO 2017-06-26 12:34:51,672 main.py:50] epoch 1432, training loss: 6592.02, average training loss: 7093.68, base loss: 20512.77
[INFO 2017-06-26 12:34:52,032 main.py:50] epoch 1433, training loss: 6585.43, average training loss: 7092.28, base loss: 20512.67
[INFO 2017-06-26 12:34:52,394 main.py:50] epoch 1434, training loss: 6594.64, average training loss: 7090.77, base loss: 20512.54
[INFO 2017-06-26 12:34:52,754 main.py:50] epoch 1435, training loss: 6586.84, average training loss: 7089.20, base loss: 20512.46
[INFO 2017-06-26 12:34:53,114 main.py:50] epoch 1436, training loss: 6592.45, average training loss: 7087.93, base loss: 20512.49
[INFO 2017-06-26 12:34:53,474 main.py:50] epoch 1437, training loss: 6586.23, average training loss: 7086.50, base loss: 20512.50
[INFO 2017-06-26 12:34:53,848 main.py:50] epoch 1438, training loss: 6658.48, average training loss: 7085.15, base loss: 20512.79
[INFO 2017-06-26 12:34:54,208 main.py:50] epoch 1439, training loss: 6470.74, average training loss: 7083.52, base loss: 20512.61
[INFO 2017-06-26 12:34:54,568 main.py:50] epoch 1440, training loss: 6640.94, average training loss: 7082.14, base loss: 20512.87
[INFO 2017-06-26 12:34:54,929 main.py:50] epoch 1441, training loss: 6552.47, average training loss: 7080.68, base loss: 20512.54
[INFO 2017-06-26 12:34:55,288 main.py:50] epoch 1442, training loss: 6606.34, average training loss: 7079.19, base loss: 20512.37
[INFO 2017-06-26 12:34:55,649 main.py:50] epoch 1443, training loss: 6649.10, average training loss: 7077.79, base loss: 20512.36
[INFO 2017-06-26 12:34:56,011 main.py:50] epoch 1444, training loss: 6632.87, average training loss: 7076.54, base loss: 20512.16
[INFO 2017-06-26 12:34:56,371 main.py:50] epoch 1445, training loss: 6632.95, average training loss: 7075.26, base loss: 20511.87
[INFO 2017-06-26 12:34:56,731 main.py:50] epoch 1446, training loss: 6643.86, average training loss: 7073.99, base loss: 20511.90
[INFO 2017-06-26 12:34:57,092 main.py:50] epoch 1447, training loss: 6537.48, average training loss: 7072.58, base loss: 20511.79
[INFO 2017-06-26 12:34:57,452 main.py:50] epoch 1448, training loss: 6669.38, average training loss: 7071.39, base loss: 20512.17
[INFO 2017-06-26 12:34:57,813 main.py:50] epoch 1449, training loss: 6515.64, average training loss: 7069.94, base loss: 20512.20
[INFO 2017-06-26 12:34:58,174 main.py:50] epoch 1450, training loss: 6590.88, average training loss: 7068.65, base loss: 20512.15
[INFO 2017-06-26 12:34:58,533 main.py:50] epoch 1451, training loss: 6601.15, average training loss: 7067.36, base loss: 20512.59
[INFO 2017-06-26 12:34:58,894 main.py:50] epoch 1452, training loss: 6552.04, average training loss: 7065.90, base loss: 20512.52
[INFO 2017-06-26 12:34:59,254 main.py:50] epoch 1453, training loss: 6590.62, average training loss: 7064.58, base loss: 20512.62
[INFO 2017-06-26 12:34:59,614 main.py:50] epoch 1454, training loss: 6585.44, average training loss: 7063.18, base loss: 20513.11
[INFO 2017-06-26 12:34:59,975 main.py:50] epoch 1455, training loss: 6498.32, average training loss: 7061.76, base loss: 20512.79
[INFO 2017-06-26 12:35:00,337 main.py:50] epoch 1456, training loss: 6572.10, average training loss: 7060.48, base loss: 20512.83
[INFO 2017-06-26 12:35:00,697 main.py:50] epoch 1457, training loss: 6573.00, average training loss: 7059.18, base loss: 20512.49
[INFO 2017-06-26 12:35:01,057 main.py:50] epoch 1458, training loss: 6564.51, average training loss: 7057.92, base loss: 20512.68
[INFO 2017-06-26 12:35:01,418 main.py:50] epoch 1459, training loss: 6562.47, average training loss: 7056.60, base loss: 20512.70
[INFO 2017-06-26 12:35:01,779 main.py:50] epoch 1460, training loss: 6532.71, average training loss: 7055.30, base loss: 20512.70
[INFO 2017-06-26 12:35:02,139 main.py:50] epoch 1461, training loss: 6648.40, average training loss: 7054.14, base loss: 20512.90
[INFO 2017-06-26 12:35:02,500 main.py:50] epoch 1462, training loss: 6638.68, average training loss: 7052.96, base loss: 20513.16
[INFO 2017-06-26 12:35:02,861 main.py:50] epoch 1463, training loss: 6606.35, average training loss: 7051.72, base loss: 20513.46
[INFO 2017-06-26 12:35:03,221 main.py:50] epoch 1464, training loss: 6506.13, average training loss: 7050.44, base loss: 20513.47
[INFO 2017-06-26 12:35:03,581 main.py:50] epoch 1465, training loss: 6623.89, average training loss: 7049.21, base loss: 20513.39
[INFO 2017-06-26 12:35:03,942 main.py:50] epoch 1466, training loss: 6564.72, average training loss: 7047.89, base loss: 20513.66
[INFO 2017-06-26 12:35:04,303 main.py:50] epoch 1467, training loss: 6551.22, average training loss: 7046.53, base loss: 20513.71
[INFO 2017-06-26 12:35:04,664 main.py:50] epoch 1468, training loss: 6514.45, average training loss: 7045.18, base loss: 20514.20
[INFO 2017-06-26 12:35:05,024 main.py:50] epoch 1469, training loss: 6520.90, average training loss: 7043.90, base loss: 20514.45
[INFO 2017-06-26 12:35:05,384 main.py:50] epoch 1470, training loss: 6677.14, average training loss: 7042.60, base loss: 20514.48
[INFO 2017-06-26 12:35:05,745 main.py:50] epoch 1471, training loss: 6571.16, average training loss: 7041.35, base loss: 20514.64
[INFO 2017-06-26 12:35:06,105 main.py:50] epoch 1472, training loss: 6543.14, average training loss: 7040.01, base loss: 20514.55
[INFO 2017-06-26 12:35:06,464 main.py:50] epoch 1473, training loss: 6555.35, average training loss: 7038.73, base loss: 20514.75
[INFO 2017-06-26 12:35:06,825 main.py:50] epoch 1474, training loss: 6537.48, average training loss: 7037.38, base loss: 20514.99
[INFO 2017-06-26 12:35:07,185 main.py:50] epoch 1475, training loss: 6541.83, average training loss: 7036.01, base loss: 20514.76
[INFO 2017-06-26 12:35:07,545 main.py:50] epoch 1476, training loss: 6592.70, average training loss: 7034.75, base loss: 20514.59
[INFO 2017-06-26 12:35:07,905 main.py:50] epoch 1477, training loss: 6619.80, average training loss: 7033.46, base loss: 20514.83
[INFO 2017-06-26 12:35:08,264 main.py:50] epoch 1478, training loss: 6627.60, average training loss: 7032.21, base loss: 20514.95
[INFO 2017-06-26 12:35:08,625 main.py:50] epoch 1479, training loss: 6599.54, average training loss: 7030.90, base loss: 20514.72
[INFO 2017-06-26 12:35:08,985 main.py:50] epoch 1480, training loss: 6612.98, average training loss: 7029.68, base loss: 20515.02
[INFO 2017-06-26 12:35:09,346 main.py:50] epoch 1481, training loss: 6531.52, average training loss: 7028.43, base loss: 20514.88
[INFO 2017-06-26 12:35:09,706 main.py:50] epoch 1482, training loss: 6578.16, average training loss: 7027.29, base loss: 20515.35
[INFO 2017-06-26 12:35:10,067 main.py:50] epoch 1483, training loss: 6509.95, average training loss: 7025.92, base loss: 20515.39
[INFO 2017-06-26 12:35:10,428 main.py:50] epoch 1484, training loss: 6590.74, average training loss: 7024.68, base loss: 20515.35
[INFO 2017-06-26 12:35:10,789 main.py:50] epoch 1485, training loss: 6581.07, average training loss: 7023.30, base loss: 20515.15
[INFO 2017-06-26 12:35:11,150 main.py:50] epoch 1486, training loss: 6553.09, average training loss: 7022.00, base loss: 20514.95
[INFO 2017-06-26 12:35:11,511 main.py:50] epoch 1487, training loss: 6577.28, average training loss: 7020.76, base loss: 20514.79
[INFO 2017-06-26 12:35:11,871 main.py:50] epoch 1488, training loss: 6477.15, average training loss: 7019.49, base loss: 20514.94
[INFO 2017-06-26 12:35:12,232 main.py:50] epoch 1489, training loss: 6558.48, average training loss: 7018.30, base loss: 20515.21
[INFO 2017-06-26 12:35:12,593 main.py:50] epoch 1490, training loss: 6484.24, average training loss: 7017.07, base loss: 20515.06
[INFO 2017-06-26 12:35:12,954 main.py:50] epoch 1491, training loss: 6431.95, average training loss: 7015.71, base loss: 20514.94
[INFO 2017-06-26 12:35:13,315 main.py:50] epoch 1492, training loss: 6483.67, average training loss: 7014.47, base loss: 20514.57
[INFO 2017-06-26 12:35:13,676 main.py:50] epoch 1493, training loss: 6454.92, average training loss: 7013.13, base loss: 20514.06
[INFO 2017-06-26 12:35:14,037 main.py:50] epoch 1494, training loss: 6677.71, average training loss: 7011.94, base loss: 20514.22
[INFO 2017-06-26 12:35:14,398 main.py:50] epoch 1495, training loss: 6628.35, average training loss: 7010.75, base loss: 20514.23
[INFO 2017-06-26 12:35:14,758 main.py:50] epoch 1496, training loss: 6568.33, average training loss: 7009.57, base loss: 20514.37
[INFO 2017-06-26 12:35:15,118 main.py:50] epoch 1497, training loss: 6505.38, average training loss: 7008.40, base loss: 20514.70
[INFO 2017-06-26 12:35:15,478 main.py:50] epoch 1498, training loss: 6521.55, average training loss: 7006.99, base loss: 20514.23
[INFO 2017-06-26 12:35:15,838 main.py:50] epoch 1499, training loss: 6529.11, average training loss: 7005.68, base loss: 20514.22
[INFO 2017-06-26 12:35:15,838 main.py:52] epoch 1499, testing
[INFO 2017-06-26 12:35:17,319 main.py:103] average testing loss: 6524.88, base loss: 20481.61
[INFO 2017-06-26 12:35:17,320 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 12:35:17,326 main.py:76] current best accuracy: 6524.88
[INFO 2017-06-26 12:35:17,686 main.py:50] epoch 1500, training loss: 6512.68, average training loss: 7004.40, base loss: 20513.89
[INFO 2017-06-26 12:35:18,046 main.py:50] epoch 1501, training loss: 6538.83, average training loss: 7003.13, base loss: 20514.08
[INFO 2017-06-26 12:35:18,405 main.py:50] epoch 1502, training loss: 6482.43, average training loss: 7001.86, base loss: 20514.32
[INFO 2017-06-26 12:35:18,765 main.py:50] epoch 1503, training loss: 6508.62, average training loss: 7000.67, base loss: 20514.36
[INFO 2017-06-26 12:35:19,124 main.py:50] epoch 1504, training loss: 6615.10, average training loss: 6999.59, base loss: 20514.60
[INFO 2017-06-26 12:35:19,484 main.py:50] epoch 1505, training loss: 6590.68, average training loss: 6998.50, base loss: 20514.84
[INFO 2017-06-26 12:35:19,843 main.py:50] epoch 1506, training loss: 6479.54, average training loss: 6997.31, base loss: 20515.52
[INFO 2017-06-26 12:35:20,204 main.py:50] epoch 1507, training loss: 6517.54, average training loss: 6996.16, base loss: 20515.79
[INFO 2017-06-26 12:35:20,565 main.py:50] epoch 1508, training loss: 6474.36, average training loss: 6994.98, base loss: 20515.69
[INFO 2017-06-26 12:35:20,926 main.py:50] epoch 1509, training loss: 6480.78, average training loss: 6993.70, base loss: 20515.71
[INFO 2017-06-26 12:35:21,288 main.py:50] epoch 1510, training loss: 6519.08, average training loss: 6992.51, base loss: 20515.79
[INFO 2017-06-26 12:35:21,650 main.py:50] epoch 1511, training loss: 6550.25, average training loss: 6991.37, base loss: 20516.15
[INFO 2017-06-26 12:35:22,009 main.py:50] epoch 1512, training loss: 6493.54, average training loss: 6990.08, base loss: 20515.73
[INFO 2017-06-26 12:35:22,372 main.py:50] epoch 1513, training loss: 6528.24, average training loss: 6988.87, base loss: 20515.36
[INFO 2017-06-26 12:35:22,733 main.py:50] epoch 1514, training loss: 6450.45, average training loss: 6987.64, base loss: 20515.05
[INFO 2017-06-26 12:35:23,093 main.py:50] epoch 1515, training loss: 6510.39, average training loss: 6986.41, base loss: 20515.29
[INFO 2017-06-26 12:35:23,454 main.py:50] epoch 1516, training loss: 6534.10, average training loss: 6985.28, base loss: 20515.42
[INFO 2017-06-26 12:35:23,815 main.py:50] epoch 1517, training loss: 6513.58, average training loss: 6984.01, base loss: 20515.78
[INFO 2017-06-26 12:35:24,177 main.py:50] epoch 1518, training loss: 6469.30, average training loss: 6982.63, base loss: 20515.57
[INFO 2017-06-26 12:35:24,538 main.py:50] epoch 1519, training loss: 6473.83, average training loss: 6981.44, base loss: 20515.62
[INFO 2017-06-26 12:35:24,900 main.py:50] epoch 1520, training loss: 6484.46, average training loss: 6980.21, base loss: 20515.42
[INFO 2017-06-26 12:35:25,261 main.py:50] epoch 1521, training loss: 6481.91, average training loss: 6978.92, base loss: 20515.36
[INFO 2017-06-26 12:35:25,623 main.py:50] epoch 1522, training loss: 6571.46, average training loss: 6977.75, base loss: 20515.24
[INFO 2017-06-26 12:35:25,983 main.py:50] epoch 1523, training loss: 6585.10, average training loss: 6976.57, base loss: 20515.27
[INFO 2017-06-26 12:35:26,344 main.py:50] epoch 1524, training loss: 6658.67, average training loss: 6975.55, base loss: 20515.71
[INFO 2017-06-26 12:35:26,704 main.py:50] epoch 1525, training loss: 6602.97, average training loss: 6974.32, base loss: 20515.77
[INFO 2017-06-26 12:35:27,065 main.py:50] epoch 1526, training loss: 6541.24, average training loss: 6973.15, base loss: 20515.52
[INFO 2017-06-26 12:35:27,426 main.py:50] epoch 1527, training loss: 6632.52, average training loss: 6971.96, base loss: 20515.73
[INFO 2017-06-26 12:35:27,785 main.py:50] epoch 1528, training loss: 6496.10, average training loss: 6970.81, base loss: 20515.87
[INFO 2017-06-26 12:35:28,145 main.py:50] epoch 1529, training loss: 6550.32, average training loss: 6969.71, base loss: 20516.03
[INFO 2017-06-26 12:35:28,504 main.py:50] epoch 1530, training loss: 6541.86, average training loss: 6968.49, base loss: 20516.16
[INFO 2017-06-26 12:35:28,865 main.py:50] epoch 1531, training loss: 6594.70, average training loss: 6967.46, base loss: 20516.46
[INFO 2017-06-26 12:35:29,226 main.py:50] epoch 1532, training loss: 6546.80, average training loss: 6966.33, base loss: 20516.68
[INFO 2017-06-26 12:35:29,587 main.py:50] epoch 1533, training loss: 6520.83, average training loss: 6965.08, base loss: 20516.40
[INFO 2017-06-26 12:35:29,948 main.py:50] epoch 1534, training loss: 6576.72, average training loss: 6963.95, base loss: 20516.16
[INFO 2017-06-26 12:35:30,309 main.py:50] epoch 1535, training loss: 6549.75, average training loss: 6962.85, base loss: 20516.61
[INFO 2017-06-26 12:35:30,669 main.py:50] epoch 1536, training loss: 6576.81, average training loss: 6961.68, base loss: 20516.57
[INFO 2017-06-26 12:35:31,030 main.py:50] epoch 1537, training loss: 6492.81, average training loss: 6960.49, base loss: 20516.73
[INFO 2017-06-26 12:35:31,388 main.py:50] epoch 1538, training loss: 6595.72, average training loss: 6959.35, base loss: 20516.64
[INFO 2017-06-26 12:35:31,747 main.py:50] epoch 1539, training loss: 6486.16, average training loss: 6958.26, base loss: 20516.64
[INFO 2017-06-26 12:35:32,109 main.py:50] epoch 1540, training loss: 6541.61, average training loss: 6957.21, base loss: 20516.60
[INFO 2017-06-26 12:35:32,470 main.py:50] epoch 1541, training loss: 6491.30, average training loss: 6956.08, base loss: 20516.33
[INFO 2017-06-26 12:35:32,830 main.py:50] epoch 1542, training loss: 6526.94, average training loss: 6954.95, base loss: 20516.65
[INFO 2017-06-26 12:35:33,190 main.py:50] epoch 1543, training loss: 6564.19, average training loss: 6953.98, base loss: 20517.17
[INFO 2017-06-26 12:35:33,549 main.py:50] epoch 1544, training loss: 6429.99, average training loss: 6952.77, base loss: 20517.11
[INFO 2017-06-26 12:35:33,909 main.py:50] epoch 1545, training loss: 6520.08, average training loss: 6951.67, base loss: 20517.04
[INFO 2017-06-26 12:35:34,269 main.py:50] epoch 1546, training loss: 6511.61, average training loss: 6950.60, base loss: 20517.42
[INFO 2017-06-26 12:35:34,628 main.py:50] epoch 1547, training loss: 6531.31, average training loss: 6949.51, base loss: 20517.39
[INFO 2017-06-26 12:35:34,989 main.py:50] epoch 1548, training loss: 6500.62, average training loss: 6948.46, base loss: 20517.71
[INFO 2017-06-26 12:35:35,350 main.py:50] epoch 1549, training loss: 6558.78, average training loss: 6947.45, base loss: 20518.02
[INFO 2017-06-26 12:35:35,709 main.py:50] epoch 1550, training loss: 6501.82, average training loss: 6946.30, base loss: 20517.82
[INFO 2017-06-26 12:35:36,068 main.py:50] epoch 1551, training loss: 6552.63, average training loss: 6945.26, base loss: 20517.95
[INFO 2017-06-26 12:35:36,428 main.py:50] epoch 1552, training loss: 6514.54, average training loss: 6944.19, base loss: 20518.31
[INFO 2017-06-26 12:35:36,789 main.py:50] epoch 1553, training loss: 6496.73, average training loss: 6943.10, base loss: 20518.29
[INFO 2017-06-26 12:35:37,150 main.py:50] epoch 1554, training loss: 6498.78, average training loss: 6942.00, base loss: 20518.64
[INFO 2017-06-26 12:35:37,511 main.py:50] epoch 1555, training loss: 6526.85, average training loss: 6940.84, base loss: 20518.66
[INFO 2017-06-26 12:35:37,872 main.py:50] epoch 1556, training loss: 6504.07, average training loss: 6939.80, base loss: 20518.56
[INFO 2017-06-26 12:35:38,246 main.py:50] epoch 1557, training loss: 6550.21, average training loss: 6938.72, base loss: 20518.63
[INFO 2017-06-26 12:35:38,606 main.py:50] epoch 1558, training loss: 6498.55, average training loss: 6937.54, base loss: 20518.51
[INFO 2017-06-26 12:35:38,966 main.py:50] epoch 1559, training loss: 6414.83, average training loss: 6936.22, base loss: 20518.19
[INFO 2017-06-26 12:35:39,327 main.py:50] epoch 1560, training loss: 6469.04, average training loss: 6935.16, base loss: 20517.89
[INFO 2017-06-26 12:35:39,687 main.py:50] epoch 1561, training loss: 6415.55, average training loss: 6933.95, base loss: 20517.92
[INFO 2017-06-26 12:35:40,048 main.py:50] epoch 1562, training loss: 6481.50, average training loss: 6932.82, base loss: 20518.06
[INFO 2017-06-26 12:35:40,409 main.py:50] epoch 1563, training loss: 6508.02, average training loss: 6931.62, base loss: 20517.72
[INFO 2017-06-26 12:35:40,770 main.py:50] epoch 1564, training loss: 6522.21, average training loss: 6930.40, base loss: 20517.56
[INFO 2017-06-26 12:35:41,131 main.py:50] epoch 1565, training loss: 6499.61, average training loss: 6929.29, base loss: 20517.53
[INFO 2017-06-26 12:35:41,492 main.py:50] epoch 1566, training loss: 6500.92, average training loss: 6928.12, base loss: 20517.55
[INFO 2017-06-26 12:35:41,851 main.py:50] epoch 1567, training loss: 6455.13, average training loss: 6926.94, base loss: 20517.40
[INFO 2017-06-26 12:35:42,213 main.py:50] epoch 1568, training loss: 6498.18, average training loss: 6925.80, base loss: 20517.80
[INFO 2017-06-26 12:35:42,574 main.py:50] epoch 1569, training loss: 6553.12, average training loss: 6924.68, base loss: 20517.99
[INFO 2017-06-26 12:35:42,935 main.py:50] epoch 1570, training loss: 6481.07, average training loss: 6923.53, base loss: 20517.88
[INFO 2017-06-26 12:35:43,295 main.py:50] epoch 1571, training loss: 6504.04, average training loss: 6922.46, base loss: 20517.99
[INFO 2017-06-26 12:35:43,656 main.py:50] epoch 1572, training loss: 6486.97, average training loss: 6921.30, base loss: 20517.80
[INFO 2017-06-26 12:35:44,017 main.py:50] epoch 1573, training loss: 6481.86, average training loss: 6920.12, base loss: 20517.57
[INFO 2017-06-26 12:35:44,377 main.py:50] epoch 1574, training loss: 6511.08, average training loss: 6918.92, base loss: 20517.48
[INFO 2017-06-26 12:35:44,738 main.py:50] epoch 1575, training loss: 6524.46, average training loss: 6917.83, base loss: 20517.57
[INFO 2017-06-26 12:35:45,099 main.py:50] epoch 1576, training loss: 6582.78, average training loss: 6916.90, base loss: 20517.53
[INFO 2017-06-26 12:35:45,460 main.py:50] epoch 1577, training loss: 6516.00, average training loss: 6915.88, base loss: 20517.80
[INFO 2017-06-26 12:35:45,821 main.py:50] epoch 1578, training loss: 6532.77, average training loss: 6914.79, base loss: 20517.70
[INFO 2017-06-26 12:35:46,181 main.py:50] epoch 1579, training loss: 6474.55, average training loss: 6913.52, base loss: 20517.40
[INFO 2017-06-26 12:35:46,542 main.py:50] epoch 1580, training loss: 6578.60, average training loss: 6912.40, base loss: 20517.61
[INFO 2017-06-26 12:35:46,902 main.py:50] epoch 1581, training loss: 6522.18, average training loss: 6911.36, base loss: 20517.92
[INFO 2017-06-26 12:35:47,263 main.py:50] epoch 1582, training loss: 6632.23, average training loss: 6910.46, base loss: 20517.63
[INFO 2017-06-26 12:35:47,624 main.py:50] epoch 1583, training loss: 6613.67, average training loss: 6909.52, base loss: 20517.88
[INFO 2017-06-26 12:35:47,984 main.py:50] epoch 1584, training loss: 6585.60, average training loss: 6908.70, base loss: 20518.13
[INFO 2017-06-26 12:35:48,345 main.py:50] epoch 1585, training loss: 6621.89, average training loss: 6907.71, base loss: 20517.94
[INFO 2017-06-26 12:35:48,706 main.py:50] epoch 1586, training loss: 6572.21, average training loss: 6906.73, base loss: 20518.56
[INFO 2017-06-26 12:35:49,067 main.py:50] epoch 1587, training loss: 6575.88, average training loss: 6905.80, base loss: 20518.71
[INFO 2017-06-26 12:35:49,428 main.py:50] epoch 1588, training loss: 6536.99, average training loss: 6904.86, base loss: 20519.13
[INFO 2017-06-26 12:35:49,788 main.py:50] epoch 1589, training loss: 6483.53, average training loss: 6903.80, base loss: 20519.02
[INFO 2017-06-26 12:35:50,148 main.py:50] epoch 1590, training loss: 6480.02, average training loss: 6902.70, base loss: 20519.08
[INFO 2017-06-26 12:35:50,508 main.py:50] epoch 1591, training loss: 6511.54, average training loss: 6901.69, base loss: 20518.88
[INFO 2017-06-26 12:35:50,868 main.py:50] epoch 1592, training loss: 6545.55, average training loss: 6900.63, base loss: 20518.86
[INFO 2017-06-26 12:35:51,228 main.py:50] epoch 1593, training loss: 6522.92, average training loss: 6899.43, base loss: 20518.38
[INFO 2017-06-26 12:35:51,588 main.py:50] epoch 1594, training loss: 6530.13, average training loss: 6898.46, base loss: 20518.10
[INFO 2017-06-26 12:35:51,949 main.py:50] epoch 1595, training loss: 6588.03, average training loss: 6897.56, base loss: 20518.29
[INFO 2017-06-26 12:35:52,309 main.py:50] epoch 1596, training loss: 6450.29, average training loss: 6896.49, base loss: 20518.42
[INFO 2017-06-26 12:35:52,671 main.py:50] epoch 1597, training loss: 6474.49, average training loss: 6895.52, base loss: 20518.99
[INFO 2017-06-26 12:35:53,032 main.py:50] epoch 1598, training loss: 6436.15, average training loss: 6894.34, base loss: 20518.84
[INFO 2017-06-26 12:35:53,393 main.py:50] epoch 1599, training loss: 6478.27, average training loss: 6893.37, base loss: 20518.78
[INFO 2017-06-26 12:35:53,393 main.py:52] epoch 1599, testing
[INFO 2017-06-26 12:35:54,868 main.py:103] average testing loss: 6452.10, base loss: 20522.14
[INFO 2017-06-26 12:35:54,868 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 12:35:54,875 main.py:76] current best accuracy: 6452.10
[INFO 2017-06-26 12:35:55,235 main.py:50] epoch 1600, training loss: 6513.61, average training loss: 6892.40, base loss: 20519.38
[INFO 2017-06-26 12:35:55,596 main.py:50] epoch 1601, training loss: 6436.63, average training loss: 6891.36, base loss: 20519.50
[INFO 2017-06-26 12:35:55,956 main.py:50] epoch 1602, training loss: 6480.22, average training loss: 6890.30, base loss: 20519.51
[INFO 2017-06-26 12:35:56,317 main.py:50] epoch 1603, training loss: 6464.54, average training loss: 6889.29, base loss: 20519.43
[INFO 2017-06-26 12:35:56,677 main.py:50] epoch 1604, training loss: 6493.47, average training loss: 6888.30, base loss: 20519.54
[INFO 2017-06-26 12:35:57,036 main.py:50] epoch 1605, training loss: 6472.96, average training loss: 6887.31, base loss: 20519.26
[INFO 2017-06-26 12:35:57,396 main.py:50] epoch 1606, training loss: 6459.57, average training loss: 6886.29, base loss: 20519.29
[INFO 2017-06-26 12:35:57,755 main.py:50] epoch 1607, training loss: 6504.78, average training loss: 6885.31, base loss: 20519.49
[INFO 2017-06-26 12:35:58,115 main.py:50] epoch 1608, training loss: 6417.73, average training loss: 6884.26, base loss: 20519.54
[INFO 2017-06-26 12:35:58,475 main.py:50] epoch 1609, training loss: 6519.20, average training loss: 6883.27, base loss: 20519.99
[INFO 2017-06-26 12:35:58,836 main.py:50] epoch 1610, training loss: 6452.13, average training loss: 6882.26, base loss: 20519.99
[INFO 2017-06-26 12:35:59,197 main.py:50] epoch 1611, training loss: 6446.43, average training loss: 6881.08, base loss: 20519.99
[INFO 2017-06-26 12:35:59,558 main.py:50] epoch 1612, training loss: 6419.13, average training loss: 6880.01, base loss: 20519.84
[INFO 2017-06-26 12:35:59,918 main.py:50] epoch 1613, training loss: 6402.67, average training loss: 6879.00, base loss: 20519.72
[INFO 2017-06-26 12:36:00,279 main.py:50] epoch 1614, training loss: 6335.33, average training loss: 6877.80, base loss: 20519.39
[INFO 2017-06-26 12:36:00,640 main.py:50] epoch 1615, training loss: 6516.71, average training loss: 6876.81, base loss: 20519.65
[INFO 2017-06-26 12:36:01,000 main.py:50] epoch 1616, training loss: 6486.52, average training loss: 6875.80, base loss: 20519.27
[INFO 2017-06-26 12:36:01,362 main.py:50] epoch 1617, training loss: 6491.97, average training loss: 6874.82, base loss: 20519.22
[INFO 2017-06-26 12:36:01,722 main.py:50] epoch 1618, training loss: 6444.80, average training loss: 6873.83, base loss: 20519.32
[INFO 2017-06-26 12:36:02,083 main.py:50] epoch 1619, training loss: 6437.34, average training loss: 6872.78, base loss: 20519.13
[INFO 2017-06-26 12:36:02,442 main.py:50] epoch 1620, training loss: 6496.44, average training loss: 6871.77, base loss: 20519.43
[INFO 2017-06-26 12:36:02,802 main.py:50] epoch 1621, training loss: 6545.85, average training loss: 6870.82, base loss: 20518.98
[INFO 2017-06-26 12:36:03,162 main.py:50] epoch 1622, training loss: 6537.92, average training loss: 6869.89, base loss: 20518.99
[INFO 2017-06-26 12:36:03,521 main.py:50] epoch 1623, training loss: 6574.91, average training loss: 6868.99, base loss: 20519.02
[INFO 2017-06-26 12:36:03,879 main.py:50] epoch 1624, training loss: 6519.28, average training loss: 6867.99, base loss: 20519.45
[INFO 2017-06-26 12:36:04,239 main.py:50] epoch 1625, training loss: 6449.83, average training loss: 6867.00, base loss: 20519.72
[INFO 2017-06-26 12:36:04,600 main.py:50] epoch 1626, training loss: 6472.17, average training loss: 6866.00, base loss: 20519.64
[INFO 2017-06-26 12:36:04,961 main.py:50] epoch 1627, training loss: 6464.49, average training loss: 6864.86, base loss: 20519.36
[INFO 2017-06-26 12:36:05,320 main.py:50] epoch 1628, training loss: 6543.86, average training loss: 6864.01, base loss: 20519.77
[INFO 2017-06-26 12:36:05,680 main.py:50] epoch 1629, training loss: 6548.76, average training loss: 6862.87, base loss: 20519.83
[INFO 2017-06-26 12:36:06,041 main.py:50] epoch 1630, training loss: 6483.76, average training loss: 6861.94, base loss: 20519.81
[INFO 2017-06-26 12:36:06,402 main.py:50] epoch 1631, training loss: 6417.39, average training loss: 6860.77, base loss: 20519.75
[INFO 2017-06-26 12:36:06,763 main.py:50] epoch 1632, training loss: 6425.29, average training loss: 6859.68, base loss: 20519.48
[INFO 2017-06-26 12:36:07,123 main.py:50] epoch 1633, training loss: 6474.67, average training loss: 6858.62, base loss: 20519.80
[INFO 2017-06-26 12:36:07,482 main.py:50] epoch 1634, training loss: 6456.19, average training loss: 6857.60, base loss: 20519.91
[INFO 2017-06-26 12:36:07,843 main.py:50] epoch 1635, training loss: 6557.45, average training loss: 6856.55, base loss: 20520.48
[INFO 2017-06-26 12:36:08,205 main.py:50] epoch 1636, training loss: 6422.19, average training loss: 6855.48, base loss: 20520.48
[INFO 2017-06-26 12:36:08,566 main.py:50] epoch 1637, training loss: 6498.52, average training loss: 6854.38, base loss: 20520.55
[INFO 2017-06-26 12:36:08,927 main.py:50] epoch 1638, training loss: 6441.86, average training loss: 6853.32, base loss: 20520.13
[INFO 2017-06-26 12:36:09,285 main.py:50] epoch 1639, training loss: 6497.99, average training loss: 6852.20, base loss: 20519.95
[INFO 2017-06-26 12:36:09,643 main.py:50] epoch 1640, training loss: 6530.39, average training loss: 6851.30, base loss: 20519.97
[INFO 2017-06-26 12:36:10,005 main.py:50] epoch 1641, training loss: 6462.16, average training loss: 6850.16, base loss: 20519.64
[INFO 2017-06-26 12:36:10,364 main.py:50] epoch 1642, training loss: 6454.00, average training loss: 6849.15, base loss: 20519.66
[INFO 2017-06-26 12:36:10,724 main.py:50] epoch 1643, training loss: 6586.56, average training loss: 6848.10, base loss: 20519.41
[INFO 2017-06-26 12:36:11,085 main.py:50] epoch 1644, training loss: 6388.99, average training loss: 6846.99, base loss: 20519.19
[INFO 2017-06-26 12:36:11,445 main.py:50] epoch 1645, training loss: 6467.64, average training loss: 6845.96, base loss: 20519.20
[INFO 2017-06-26 12:36:11,806 main.py:50] epoch 1646, training loss: 6506.73, average training loss: 6845.00, base loss: 20519.22
[INFO 2017-06-26 12:36:12,167 main.py:50] epoch 1647, training loss: 6492.50, average training loss: 6844.01, base loss: 20518.97
[INFO 2017-06-26 12:36:12,526 main.py:50] epoch 1648, training loss: 6425.32, average training loss: 6843.01, base loss: 20518.75
[INFO 2017-06-26 12:36:12,886 main.py:50] epoch 1649, training loss: 6475.91, average training loss: 6842.14, base loss: 20518.89
[INFO 2017-06-26 12:36:13,246 main.py:50] epoch 1650, training loss: 6470.09, average training loss: 6841.15, base loss: 20518.88
[INFO 2017-06-26 12:36:13,607 main.py:50] epoch 1651, training loss: 6441.75, average training loss: 6840.14, base loss: 20518.81
[INFO 2017-06-26 12:36:13,967 main.py:50] epoch 1652, training loss: 6503.75, average training loss: 6839.01, base loss: 20518.65
[INFO 2017-06-26 12:36:14,327 main.py:50] epoch 1653, training loss: 6457.97, average training loss: 6838.05, base loss: 20518.59
[INFO 2017-06-26 12:36:14,689 main.py:50] epoch 1654, training loss: 6412.30, average training loss: 6836.99, base loss: 20518.33
[INFO 2017-06-26 12:36:15,050 main.py:50] epoch 1655, training loss: 6475.21, average training loss: 6836.05, base loss: 20518.47
[INFO 2017-06-26 12:36:15,411 main.py:50] epoch 1656, training loss: 6482.95, average training loss: 6835.11, base loss: 20518.41
[INFO 2017-06-26 12:36:15,771 main.py:50] epoch 1657, training loss: 6464.36, average training loss: 6834.19, base loss: 20518.39
[INFO 2017-06-26 12:36:16,132 main.py:50] epoch 1658, training loss: 6435.31, average training loss: 6833.18, base loss: 20518.40
[INFO 2017-06-26 12:36:16,492 main.py:50] epoch 1659, training loss: 6345.08, average training loss: 6832.13, base loss: 20518.38
[INFO 2017-06-26 12:36:16,853 main.py:50] epoch 1660, training loss: 6355.88, average training loss: 6831.07, base loss: 20518.30
[INFO 2017-06-26 12:36:17,214 main.py:50] epoch 1661, training loss: 6448.13, average training loss: 6830.11, base loss: 20518.14
[INFO 2017-06-26 12:36:17,575 main.py:50] epoch 1662, training loss: 6491.37, average training loss: 6829.21, base loss: 20518.44
[INFO 2017-06-26 12:36:17,936 main.py:50] epoch 1663, training loss: 6491.78, average training loss: 6828.34, base loss: 20519.07
[INFO 2017-06-26 12:36:18,297 main.py:50] epoch 1664, training loss: 6422.27, average training loss: 6827.24, base loss: 20518.85
[INFO 2017-06-26 12:36:18,658 main.py:50] epoch 1665, training loss: 6471.34, average training loss: 6826.32, base loss: 20519.27
[INFO 2017-06-26 12:36:19,019 main.py:50] epoch 1666, training loss: 6533.05, average training loss: 6825.38, base loss: 20519.53
[INFO 2017-06-26 12:36:19,380 main.py:50] epoch 1667, training loss: 6485.56, average training loss: 6824.50, base loss: 20519.48
[INFO 2017-06-26 12:36:19,741 main.py:50] epoch 1668, training loss: 6451.54, average training loss: 6823.60, base loss: 20519.47
[INFO 2017-06-26 12:36:20,101 main.py:50] epoch 1669, training loss: 6440.80, average training loss: 6822.67, base loss: 20519.43
[INFO 2017-06-26 12:36:20,460 main.py:50] epoch 1670, training loss: 6468.74, average training loss: 6821.72, base loss: 20519.79
[INFO 2017-06-26 12:36:20,822 main.py:50] epoch 1671, training loss: 6476.11, average training loss: 6820.72, base loss: 20520.04
[INFO 2017-06-26 12:36:21,182 main.py:50] epoch 1672, training loss: 6501.34, average training loss: 6819.75, base loss: 20519.63
[INFO 2017-06-26 12:36:21,544 main.py:50] epoch 1673, training loss: 6506.47, average training loss: 6818.78, base loss: 20519.69
[INFO 2017-06-26 12:36:21,904 main.py:50] epoch 1674, training loss: 6548.19, average training loss: 6817.90, base loss: 20519.68
[INFO 2017-06-26 12:36:22,265 main.py:50] epoch 1675, training loss: 6438.06, average training loss: 6816.94, base loss: 20519.52
[INFO 2017-06-26 12:36:22,641 main.py:50] epoch 1676, training loss: 6508.55, average training loss: 6816.08, base loss: 20519.44
[INFO 2017-06-26 12:36:23,002 main.py:50] epoch 1677, training loss: 6467.91, average training loss: 6815.15, base loss: 20519.57
[INFO 2017-06-26 12:36:23,361 main.py:50] epoch 1678, training loss: 6472.55, average training loss: 6814.23, base loss: 20519.37
[INFO 2017-06-26 12:36:23,721 main.py:50] epoch 1679, training loss: 6512.03, average training loss: 6813.35, base loss: 20519.52
[INFO 2017-06-26 12:36:24,081 main.py:50] epoch 1680, training loss: 6510.64, average training loss: 6812.49, base loss: 20519.82
[INFO 2017-06-26 12:36:24,442 main.py:50] epoch 1681, training loss: 6451.09, average training loss: 6811.45, base loss: 20519.71
[INFO 2017-06-26 12:36:24,803 main.py:50] epoch 1682, training loss: 6551.11, average training loss: 6810.47, base loss: 20519.68
[INFO 2017-06-26 12:36:25,164 main.py:50] epoch 1683, training loss: 6479.82, average training loss: 6809.55, base loss: 20519.85
[INFO 2017-06-26 12:36:25,525 main.py:50] epoch 1684, training loss: 6414.55, average training loss: 6808.57, base loss: 20519.78
[INFO 2017-06-26 12:36:25,885 main.py:50] epoch 1685, training loss: 6463.04, average training loss: 6807.69, base loss: 20519.97
[INFO 2017-06-26 12:36:26,245 main.py:50] epoch 1686, training loss: 6424.84, average training loss: 6806.72, base loss: 20520.09
[INFO 2017-06-26 12:36:26,606 main.py:50] epoch 1687, training loss: 6480.67, average training loss: 6805.85, base loss: 20520.05
[INFO 2017-06-26 12:36:26,967 main.py:50] epoch 1688, training loss: 6513.96, average training loss: 6804.96, base loss: 20520.23
[INFO 2017-06-26 12:36:27,328 main.py:50] epoch 1689, training loss: 6438.79, average training loss: 6804.07, base loss: 20520.60
[INFO 2017-06-26 12:36:27,687 main.py:50] epoch 1690, training loss: 6418.12, average training loss: 6803.10, base loss: 20520.36
[INFO 2017-06-26 12:36:28,049 main.py:50] epoch 1691, training loss: 6483.39, average training loss: 6802.18, base loss: 20520.73
[INFO 2017-06-26 12:36:28,410 main.py:50] epoch 1692, training loss: 6374.45, average training loss: 6801.16, base loss: 20521.04
[INFO 2017-06-26 12:36:28,771 main.py:50] epoch 1693, training loss: 6463.70, average training loss: 6800.32, base loss: 20521.24
[INFO 2017-06-26 12:36:29,131 main.py:50] epoch 1694, training loss: 6453.36, average training loss: 6799.53, base loss: 20521.84
[INFO 2017-06-26 12:36:29,492 main.py:50] epoch 1695, training loss: 6436.09, average training loss: 6798.65, base loss: 20521.67
[INFO 2017-06-26 12:36:29,852 main.py:50] epoch 1696, training loss: 6421.15, average training loss: 6797.62, base loss: 20521.43
[INFO 2017-06-26 12:36:30,213 main.py:50] epoch 1697, training loss: 6377.56, average training loss: 6796.66, base loss: 20521.35
[INFO 2017-06-26 12:36:30,573 main.py:50] epoch 1698, training loss: 6452.51, average training loss: 6795.80, base loss: 20521.51
[INFO 2017-06-26 12:36:30,934 main.py:50] epoch 1699, training loss: 6384.74, average training loss: 6794.97, base loss: 20521.90
[INFO 2017-06-26 12:36:30,934 main.py:52] epoch 1699, testing
[INFO 2017-06-26 12:36:32,419 main.py:103] average testing loss: 6428.36, base loss: 20522.73
[INFO 2017-06-26 12:36:32,419 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 12:36:32,425 main.py:76] current best accuracy: 6428.36
[INFO 2017-06-26 12:36:32,784 main.py:50] epoch 1700, training loss: 6398.35, average training loss: 6793.99, base loss: 20521.90
[INFO 2017-06-26 12:36:33,142 main.py:50] epoch 1701, training loss: 6368.20, average training loss: 6792.97, base loss: 20522.08
[INFO 2017-06-26 12:36:33,503 main.py:50] epoch 1702, training loss: 6436.64, average training loss: 6792.05, base loss: 20521.85
[INFO 2017-06-26 12:36:33,864 main.py:50] epoch 1703, training loss: 6444.87, average training loss: 6791.11, base loss: 20522.04
[INFO 2017-06-26 12:36:34,224 main.py:50] epoch 1704, training loss: 6362.13, average training loss: 6790.12, base loss: 20522.16
[INFO 2017-06-26 12:36:34,585 main.py:50] epoch 1705, training loss: 6471.91, average training loss: 6789.17, base loss: 20522.27
[INFO 2017-06-26 12:36:34,946 main.py:50] epoch 1706, training loss: 6431.54, average training loss: 6788.24, base loss: 20522.19
[INFO 2017-06-26 12:36:35,306 main.py:50] epoch 1707, training loss: 6428.05, average training loss: 6787.26, base loss: 20522.15
[INFO 2017-06-26 12:36:35,667 main.py:50] epoch 1708, training loss: 6428.89, average training loss: 6786.31, base loss: 20522.26
[INFO 2017-06-26 12:36:36,029 main.py:50] epoch 1709, training loss: 6461.05, average training loss: 6785.35, base loss: 20522.21
[INFO 2017-06-26 12:36:36,389 main.py:50] epoch 1710, training loss: 6418.99, average training loss: 6784.42, base loss: 20522.71
[INFO 2017-06-26 12:36:36,750 main.py:50] epoch 1711, training loss: 6448.08, average training loss: 6783.57, base loss: 20522.29
[INFO 2017-06-26 12:36:37,110 main.py:50] epoch 1712, training loss: 6320.91, average training loss: 6782.55, base loss: 20521.74
[INFO 2017-06-26 12:36:37,471 main.py:50] epoch 1713, training loss: 6508.79, average training loss: 6781.74, base loss: 20522.08
[INFO 2017-06-26 12:36:37,831 main.py:50] epoch 1714, training loss: 6429.13, average training loss: 6780.89, base loss: 20521.66
[INFO 2017-06-26 12:36:38,192 main.py:50] epoch 1715, training loss: 6441.36, average training loss: 6780.08, base loss: 20521.54
[INFO 2017-06-26 12:36:38,551 main.py:50] epoch 1716, training loss: 6587.96, average training loss: 6779.34, base loss: 20521.92
[INFO 2017-06-26 12:36:38,913 main.py:50] epoch 1717, training loss: 6458.34, average training loss: 6778.50, base loss: 20521.91
[INFO 2017-06-26 12:36:39,273 main.py:50] epoch 1718, training loss: 6497.06, average training loss: 6777.69, base loss: 20521.75
[INFO 2017-06-26 12:36:39,634 main.py:50] epoch 1719, training loss: 6500.86, average training loss: 6776.96, base loss: 20521.73
[INFO 2017-06-26 12:36:39,995 main.py:50] epoch 1720, training loss: 6488.55, average training loss: 6776.18, base loss: 20521.73
[INFO 2017-06-26 12:36:40,356 main.py:50] epoch 1721, training loss: 6487.67, average training loss: 6775.33, base loss: 20521.87
[INFO 2017-06-26 12:36:40,716 main.py:50] epoch 1722, training loss: 6492.55, average training loss: 6774.58, base loss: 20522.19
[INFO 2017-06-26 12:36:41,077 main.py:50] epoch 1723, training loss: 6496.03, average training loss: 6773.86, base loss: 20522.35
[INFO 2017-06-26 12:36:41,439 main.py:50] epoch 1724, training loss: 6559.91, average training loss: 6773.11, base loss: 20522.75
[INFO 2017-06-26 12:36:41,799 main.py:50] epoch 1725, training loss: 6338.38, average training loss: 6772.19, base loss: 20522.51
[INFO 2017-06-26 12:36:42,160 main.py:50] epoch 1726, training loss: 6439.00, average training loss: 6771.35, base loss: 20522.29
[INFO 2017-06-26 12:36:42,521 main.py:50] epoch 1727, training loss: 6389.90, average training loss: 6770.42, base loss: 20521.59
[INFO 2017-06-26 12:36:42,880 main.py:50] epoch 1728, training loss: 6450.79, average training loss: 6769.52, base loss: 20521.13
[INFO 2017-06-26 12:36:43,241 main.py:50] epoch 1729, training loss: 6510.61, average training loss: 6768.81, base loss: 20521.08
[INFO 2017-06-26 12:36:43,601 main.py:50] epoch 1730, training loss: 6405.27, average training loss: 6767.88, base loss: 20520.86
[INFO 2017-06-26 12:36:43,963 main.py:50] epoch 1731, training loss: 6402.19, average training loss: 6767.01, base loss: 20520.71
[INFO 2017-06-26 12:36:44,324 main.py:50] epoch 1732, training loss: 6433.88, average training loss: 6766.07, base loss: 20520.46
[INFO 2017-06-26 12:36:44,685 main.py:50] epoch 1733, training loss: 6488.76, average training loss: 6765.32, base loss: 20520.25
[INFO 2017-06-26 12:36:45,045 main.py:50] epoch 1734, training loss: 6484.11, average training loss: 6764.55, base loss: 20520.09
[INFO 2017-06-26 12:36:45,405 main.py:50] epoch 1735, training loss: 6645.86, average training loss: 6763.85, base loss: 20519.84
[INFO 2017-06-26 12:36:45,765 main.py:50] epoch 1736, training loss: 6497.44, average training loss: 6763.05, base loss: 20519.94
[INFO 2017-06-26 12:36:46,124 main.py:50] epoch 1737, training loss: 6488.90, average training loss: 6762.30, base loss: 20519.72
[INFO 2017-06-26 12:36:46,484 main.py:50] epoch 1738, training loss: 6543.97, average training loss: 6761.55, base loss: 20519.58
[INFO 2017-06-26 12:36:46,844 main.py:50] epoch 1739, training loss: 6464.21, average training loss: 6760.76, base loss: 20519.45
[INFO 2017-06-26 12:36:47,204 main.py:50] epoch 1740, training loss: 6539.59, average training loss: 6760.14, base loss: 20519.19
[INFO 2017-06-26 12:36:47,564 main.py:50] epoch 1741, training loss: 6476.40, average training loss: 6759.36, base loss: 20519.49
[INFO 2017-06-26 12:36:47,924 main.py:50] epoch 1742, training loss: 6396.98, average training loss: 6758.50, base loss: 20519.65
[INFO 2017-06-26 12:36:48,286 main.py:50] epoch 1743, training loss: 6363.61, average training loss: 6757.68, base loss: 20519.33
[INFO 2017-06-26 12:36:48,647 main.py:50] epoch 1744, training loss: 6484.41, average training loss: 6756.93, base loss: 20519.07
[INFO 2017-06-26 12:36:49,008 main.py:50] epoch 1745, training loss: 6384.22, average training loss: 6756.11, base loss: 20518.74
[INFO 2017-06-26 12:36:49,369 main.py:50] epoch 1746, training loss: 6416.24, average training loss: 6755.25, base loss: 20519.08
[INFO 2017-06-26 12:36:49,730 main.py:50] epoch 1747, training loss: 6425.29, average training loss: 6754.35, base loss: 20518.82
[INFO 2017-06-26 12:36:50,091 main.py:50] epoch 1748, training loss: 6394.77, average training loss: 6753.48, base loss: 20518.87
[INFO 2017-06-26 12:36:50,451 main.py:50] epoch 1749, training loss: 6425.73, average training loss: 6752.72, base loss: 20518.93
[INFO 2017-06-26 12:36:50,811 main.py:50] epoch 1750, training loss: 6384.67, average training loss: 6751.73, base loss: 20518.45
[INFO 2017-06-26 12:36:51,172 main.py:50] epoch 1751, training loss: 6440.74, average training loss: 6750.92, base loss: 20518.46
[INFO 2017-06-26 12:36:51,531 main.py:50] epoch 1752, training loss: 6422.72, average training loss: 6750.14, base loss: 20518.71
[INFO 2017-06-26 12:36:51,893 main.py:50] epoch 1753, training loss: 6404.85, average training loss: 6749.21, base loss: 20518.75
[INFO 2017-06-26 12:36:52,254 main.py:50] epoch 1754, training loss: 6432.14, average training loss: 6748.41, base loss: 20519.03
[INFO 2017-06-26 12:36:52,615 main.py:50] epoch 1755, training loss: 6336.62, average training loss: 6747.49, base loss: 20518.73
[INFO 2017-06-26 12:36:52,976 main.py:50] epoch 1756, training loss: 6424.04, average training loss: 6746.65, base loss: 20518.66
[INFO 2017-06-26 12:36:53,335 main.py:50] epoch 1757, training loss: 6410.19, average training loss: 6745.80, base loss: 20518.71
[INFO 2017-06-26 12:36:53,696 main.py:50] epoch 1758, training loss: 6414.23, average training loss: 6744.83, base loss: 20518.58
[INFO 2017-06-26 12:36:54,056 main.py:50] epoch 1759, training loss: 6453.33, average training loss: 6744.03, base loss: 20518.85
[INFO 2017-06-26 12:36:54,417 main.py:50] epoch 1760, training loss: 6395.48, average training loss: 6743.12, base loss: 20518.32
[INFO 2017-06-26 12:36:54,778 main.py:50] epoch 1761, training loss: 6322.76, average training loss: 6742.12, base loss: 20518.48
[INFO 2017-06-26 12:36:55,137 main.py:50] epoch 1762, training loss: 6379.00, average training loss: 6741.25, base loss: 20518.23
[INFO 2017-06-26 12:36:55,497 main.py:50] epoch 1763, training loss: 6401.23, average training loss: 6740.35, base loss: 20518.31
[INFO 2017-06-26 12:36:55,858 main.py:50] epoch 1764, training loss: 6382.08, average training loss: 6739.45, base loss: 20518.10
[INFO 2017-06-26 12:36:56,219 main.py:50] epoch 1765, training loss: 6373.28, average training loss: 6738.52, base loss: 20517.85
[INFO 2017-06-26 12:36:56,579 main.py:50] epoch 1766, training loss: 6361.39, average training loss: 6737.79, base loss: 20518.18
[INFO 2017-06-26 12:36:56,939 main.py:50] epoch 1767, training loss: 6440.58, average training loss: 6737.08, base loss: 20518.57
[INFO 2017-06-26 12:36:57,300 main.py:50] epoch 1768, training loss: 6476.00, average training loss: 6736.28, base loss: 20518.77
[INFO 2017-06-26 12:36:57,661 main.py:50] epoch 1769, training loss: 6416.81, average training loss: 6735.38, base loss: 20518.78
[INFO 2017-06-26 12:36:58,022 main.py:50] epoch 1770, training loss: 6449.34, average training loss: 6734.57, base loss: 20518.89
[INFO 2017-06-26 12:36:58,382 main.py:50] epoch 1771, training loss: 6393.05, average training loss: 6733.77, base loss: 20518.91
[INFO 2017-06-26 12:36:58,743 main.py:50] epoch 1772, training loss: 6372.03, average training loss: 6732.97, base loss: 20519.09
[INFO 2017-06-26 12:36:59,104 main.py:50] epoch 1773, training loss: 6462.79, average training loss: 6732.24, base loss: 20519.13
[INFO 2017-06-26 12:36:59,464 main.py:50] epoch 1774, training loss: 6423.23, average training loss: 6731.40, base loss: 20519.30
[INFO 2017-06-26 12:36:59,824 main.py:50] epoch 1775, training loss: 6497.62, average training loss: 6730.68, base loss: 20519.18
[INFO 2017-06-26 12:37:00,185 main.py:50] epoch 1776, training loss: 6446.26, average training loss: 6729.93, base loss: 20519.17
[INFO 2017-06-26 12:37:00,546 main.py:50] epoch 1777, training loss: 6488.72, average training loss: 6729.23, base loss: 20519.61
[INFO 2017-06-26 12:37:00,907 main.py:50] epoch 1778, training loss: 6457.60, average training loss: 6728.43, base loss: 20519.52
[INFO 2017-06-26 12:37:01,268 main.py:50] epoch 1779, training loss: 6372.16, average training loss: 6727.65, base loss: 20519.47
[INFO 2017-06-26 12:37:01,629 main.py:50] epoch 1780, training loss: 6380.65, average training loss: 6726.87, base loss: 20519.68
[INFO 2017-06-26 12:37:01,990 main.py:50] epoch 1781, training loss: 6420.45, average training loss: 6726.19, base loss: 20519.82
[INFO 2017-06-26 12:37:02,351 main.py:50] epoch 1782, training loss: 6504.20, average training loss: 6725.59, base loss: 20520.10
[INFO 2017-06-26 12:37:02,710 main.py:50] epoch 1783, training loss: 6432.54, average training loss: 6724.93, base loss: 20520.37
[INFO 2017-06-26 12:37:03,069 main.py:50] epoch 1784, training loss: 6413.73, average training loss: 6724.19, base loss: 20520.55
[INFO 2017-06-26 12:37:03,430 main.py:50] epoch 1785, training loss: 6347.80, average training loss: 6723.34, base loss: 20520.43
[INFO 2017-06-26 12:37:03,790 main.py:50] epoch 1786, training loss: 6346.53, average training loss: 6722.40, base loss: 20520.64
[INFO 2017-06-26 12:37:04,150 main.py:50] epoch 1787, training loss: 6390.47, average training loss: 6721.53, base loss: 20520.36
[INFO 2017-06-26 12:37:04,509 main.py:50] epoch 1788, training loss: 6379.66, average training loss: 6720.72, base loss: 20520.44
[INFO 2017-06-26 12:37:04,871 main.py:50] epoch 1789, training loss: 6381.56, average training loss: 6719.85, base loss: 20520.78
[INFO 2017-06-26 12:37:05,232 main.py:50] epoch 1790, training loss: 6453.18, average training loss: 6719.07, base loss: 20520.51
[INFO 2017-06-26 12:37:05,593 main.py:50] epoch 1791, training loss: 6459.39, average training loss: 6718.32, base loss: 20520.72
[INFO 2017-06-26 12:37:05,954 main.py:50] epoch 1792, training loss: 6407.58, average training loss: 6717.58, base loss: 20520.60
[INFO 2017-06-26 12:37:06,315 main.py:50] epoch 1793, training loss: 6388.71, average training loss: 6716.88, base loss: 20520.63
[INFO 2017-06-26 12:37:06,674 main.py:50] epoch 1794, training loss: 6364.58, average training loss: 6716.10, base loss: 20520.75
[INFO 2017-06-26 12:37:07,050 main.py:50] epoch 1795, training loss: 6379.20, average training loss: 6715.38, base loss: 20520.56
[INFO 2017-06-26 12:37:07,411 main.py:50] epoch 1796, training loss: 6354.39, average training loss: 6714.67, base loss: 20520.50
[INFO 2017-06-26 12:37:07,771 main.py:50] epoch 1797, training loss: 6416.99, average training loss: 6713.98, base loss: 20520.57
[INFO 2017-06-26 12:37:08,131 main.py:50] epoch 1798, training loss: 6444.25, average training loss: 6713.17, base loss: 20520.22
[INFO 2017-06-26 12:37:08,490 main.py:50] epoch 1799, training loss: 6390.98, average training loss: 6712.35, base loss: 20519.85
[INFO 2017-06-26 12:37:08,490 main.py:52] epoch 1799, testing
[INFO 2017-06-26 12:37:09,971 main.py:103] average testing loss: 6411.64, base loss: 20515.37
[INFO 2017-06-26 12:37:09,972 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 12:37:09,978 main.py:76] current best accuracy: 6411.64
[INFO 2017-06-26 12:37:10,338 main.py:50] epoch 1800, training loss: 6447.25, average training loss: 6711.58, base loss: 20519.92
[INFO 2017-06-26 12:37:10,696 main.py:50] epoch 1801, training loss: 6400.13, average training loss: 6710.85, base loss: 20519.94
[INFO 2017-06-26 12:37:11,056 main.py:50] epoch 1802, training loss: 6407.08, average training loss: 6710.09, base loss: 20519.76
[INFO 2017-06-26 12:37:11,418 main.py:50] epoch 1803, training loss: 6365.35, average training loss: 6709.29, base loss: 20519.61
[INFO 2017-06-26 12:37:11,777 main.py:50] epoch 1804, training loss: 6457.17, average training loss: 6708.56, base loss: 20519.64
[INFO 2017-06-26 12:37:12,136 main.py:50] epoch 1805, training loss: 6347.42, average training loss: 6707.78, base loss: 20519.47
[INFO 2017-06-26 12:37:12,495 main.py:50] epoch 1806, training loss: 6318.14, average training loss: 6706.94, base loss: 20519.53
[INFO 2017-06-26 12:37:12,857 main.py:50] epoch 1807, training loss: 6293.09, average training loss: 6706.03, base loss: 20519.07
[INFO 2017-06-26 12:37:13,218 main.py:50] epoch 1808, training loss: 6321.50, average training loss: 6704.99, base loss: 20518.77
[INFO 2017-06-26 12:37:13,580 main.py:50] epoch 1809, training loss: 6415.24, average training loss: 6704.17, base loss: 20518.93
[INFO 2017-06-26 12:37:13,940 main.py:50] epoch 1810, training loss: 6370.81, average training loss: 6703.40, base loss: 20518.95
[INFO 2017-06-26 12:37:14,302 main.py:50] epoch 1811, training loss: 6319.84, average training loss: 6702.47, base loss: 20518.66
[INFO 2017-06-26 12:37:14,660 main.py:50] epoch 1812, training loss: 6379.32, average training loss: 6701.66, base loss: 20518.85
[INFO 2017-06-26 12:37:15,019 main.py:50] epoch 1813, training loss: 6325.57, average training loss: 6700.84, base loss: 20518.99
[INFO 2017-06-26 12:37:15,379 main.py:50] epoch 1814, training loss: 6367.11, average training loss: 6699.93, base loss: 20518.54
[INFO 2017-06-26 12:37:15,740 main.py:50] epoch 1815, training loss: 6365.26, average training loss: 6699.21, base loss: 20518.35
[INFO 2017-06-26 12:37:16,102 main.py:50] epoch 1816, training loss: 6483.95, average training loss: 6698.50, base loss: 20518.67
[INFO 2017-06-26 12:37:16,461 main.py:50] epoch 1817, training loss: 6432.39, average training loss: 6697.70, base loss: 20518.35
[INFO 2017-06-26 12:37:16,824 main.py:50] epoch 1818, training loss: 6375.95, average training loss: 6696.83, base loss: 20518.63
[INFO 2017-06-26 12:37:17,184 main.py:50] epoch 1819, training loss: 6321.05, average training loss: 6695.93, base loss: 20518.28
[INFO 2017-06-26 12:37:17,544 main.py:50] epoch 1820, training loss: 6353.16, average training loss: 6695.03, base loss: 20518.02
[INFO 2017-06-26 12:37:17,904 main.py:50] epoch 1821, training loss: 6341.37, average training loss: 6694.20, base loss: 20517.26
[INFO 2017-06-26 12:37:18,263 main.py:50] epoch 1822, training loss: 6440.25, average training loss: 6693.53, base loss: 20517.15
[INFO 2017-06-26 12:37:18,623 main.py:50] epoch 1823, training loss: 6348.33, average training loss: 6692.69, base loss: 20516.99
[INFO 2017-06-26 12:37:18,982 main.py:50] epoch 1824, training loss: 6509.70, average training loss: 6692.10, base loss: 20517.20
[INFO 2017-06-26 12:37:19,343 main.py:50] epoch 1825, training loss: 6414.35, average training loss: 6691.42, base loss: 20517.51
[INFO 2017-06-26 12:37:19,703 main.py:50] epoch 1826, training loss: 6392.69, average training loss: 6690.72, base loss: 20517.51
[INFO 2017-06-26 12:37:20,062 main.py:50] epoch 1827, training loss: 6386.61, average training loss: 6690.10, base loss: 20518.19
[INFO 2017-06-26 12:37:20,422 main.py:50] epoch 1828, training loss: 6376.45, average training loss: 6689.28, base loss: 20518.24
[INFO 2017-06-26 12:37:20,782 main.py:50] epoch 1829, training loss: 6340.22, average training loss: 6688.54, base loss: 20518.27
[INFO 2017-06-26 12:37:21,145 main.py:50] epoch 1830, training loss: 6436.82, average training loss: 6687.89, base loss: 20518.63
[INFO 2017-06-26 12:37:21,505 main.py:50] epoch 1831, training loss: 6297.04, average training loss: 6686.96, base loss: 20518.14
[INFO 2017-06-26 12:37:21,864 main.py:50] epoch 1832, training loss: 6358.41, average training loss: 6686.15, base loss: 20517.94
[INFO 2017-06-26 12:37:22,224 main.py:50] epoch 1833, training loss: 6338.43, average training loss: 6685.32, base loss: 20518.01
[INFO 2017-06-26 12:37:22,586 main.py:50] epoch 1834, training loss: 6308.98, average training loss: 6684.38, base loss: 20517.92
[INFO 2017-06-26 12:37:22,947 main.py:50] epoch 1835, training loss: 6340.92, average training loss: 6683.55, base loss: 20517.64
[INFO 2017-06-26 12:37:23,307 main.py:50] epoch 1836, training loss: 6294.32, average training loss: 6682.69, base loss: 20517.73
[INFO 2017-06-26 12:37:23,666 main.py:50] epoch 1837, training loss: 6321.30, average training loss: 6681.88, base loss: 20517.84
[INFO 2017-06-26 12:37:24,029 main.py:50] epoch 1838, training loss: 6376.45, average training loss: 6681.15, base loss: 20517.91
[INFO 2017-06-26 12:37:24,388 main.py:50] epoch 1839, training loss: 6320.79, average training loss: 6680.23, base loss: 20517.48
[INFO 2017-06-26 12:37:24,748 main.py:50] epoch 1840, training loss: 6361.97, average training loss: 6679.49, base loss: 20517.17
[INFO 2017-06-26 12:37:25,108 main.py:50] epoch 1841, training loss: 6317.01, average training loss: 6678.56, base loss: 20517.06
[INFO 2017-06-26 12:37:25,467 main.py:50] epoch 1842, training loss: 6335.83, average training loss: 6677.75, base loss: 20516.92
[INFO 2017-06-26 12:37:25,829 main.py:50] epoch 1843, training loss: 6394.88, average training loss: 6677.03, base loss: 20517.09
[INFO 2017-06-26 12:37:26,188 main.py:50] epoch 1844, training loss: 6387.29, average training loss: 6676.22, base loss: 20517.16
[INFO 2017-06-26 12:37:26,560 main.py:50] epoch 1845, training loss: 6413.61, average training loss: 6675.55, base loss: 20517.27
[INFO 2017-06-26 12:37:26,920 main.py:50] epoch 1846, training loss: 6341.96, average training loss: 6674.84, base loss: 20516.77
[INFO 2017-06-26 12:37:27,280 main.py:50] epoch 1847, training loss: 6395.16, average training loss: 6674.07, base loss: 20516.58
[INFO 2017-06-26 12:37:27,640 main.py:50] epoch 1848, training loss: 6296.65, average training loss: 6673.28, base loss: 20516.28
[INFO 2017-06-26 12:37:27,999 main.py:50] epoch 1849, training loss: 6433.94, average training loss: 6672.59, base loss: 20516.24
[INFO 2017-06-26 12:37:28,361 main.py:50] epoch 1850, training loss: 6376.44, average training loss: 6671.93, base loss: 20516.48
[INFO 2017-06-26 12:37:28,720 main.py:50] epoch 1851, training loss: 6513.16, average training loss: 6671.36, base loss: 20515.92
[INFO 2017-06-26 12:37:29,081 main.py:50] epoch 1852, training loss: 6381.31, average training loss: 6670.57, base loss: 20515.48
[INFO 2017-06-26 12:37:29,442 main.py:50] epoch 1853, training loss: 6479.06, average training loss: 6669.84, base loss: 20515.49
[INFO 2017-06-26 12:37:29,801 main.py:50] epoch 1854, training loss: 6484.18, average training loss: 6669.21, base loss: 20515.44
[INFO 2017-06-26 12:37:30,161 main.py:50] epoch 1855, training loss: 6529.00, average training loss: 6668.67, base loss: 20515.56
[INFO 2017-06-26 12:37:30,521 main.py:50] epoch 1856, training loss: 6409.12, average training loss: 6668.05, base loss: 20515.93
[INFO 2017-06-26 12:37:30,881 main.py:50] epoch 1857, training loss: 6345.54, average training loss: 6667.20, base loss: 20515.32
[INFO 2017-06-26 12:37:31,241 main.py:50] epoch 1858, training loss: 6403.41, average training loss: 6666.51, base loss: 20515.31
[INFO 2017-06-26 12:37:31,602 main.py:50] epoch 1859, training loss: 6351.66, average training loss: 6665.70, base loss: 20514.82
[INFO 2017-06-26 12:37:31,963 main.py:50] epoch 1860, training loss: 6380.07, average training loss: 6664.90, base loss: 20514.37
[INFO 2017-06-26 12:37:32,322 main.py:50] epoch 1861, training loss: 6401.17, average training loss: 6664.18, base loss: 20514.57
[INFO 2017-06-26 12:37:32,682 main.py:50] epoch 1862, training loss: 6361.52, average training loss: 6663.33, base loss: 20514.65
[INFO 2017-06-26 12:37:33,043 main.py:50] epoch 1863, training loss: 6353.39, average training loss: 6662.43, base loss: 20514.19
[INFO 2017-06-26 12:37:33,403 main.py:50] epoch 1864, training loss: 6374.45, average training loss: 6661.71, base loss: 20514.09
[INFO 2017-06-26 12:37:33,763 main.py:50] epoch 1865, training loss: 6365.99, average training loss: 6660.85, base loss: 20513.94
[INFO 2017-06-26 12:37:34,123 main.py:50] epoch 1866, training loss: 6405.11, average training loss: 6660.18, base loss: 20514.23
[INFO 2017-06-26 12:37:34,484 main.py:50] epoch 1867, training loss: 6288.53, average training loss: 6659.26, base loss: 20514.18
[INFO 2017-06-26 12:37:34,844 main.py:50] epoch 1868, training loss: 6290.62, average training loss: 6658.44, base loss: 20514.16
[INFO 2017-06-26 12:37:35,205 main.py:50] epoch 1869, training loss: 6354.76, average training loss: 6657.62, base loss: 20514.03
[INFO 2017-06-26 12:37:35,564 main.py:50] epoch 1870, training loss: 6339.81, average training loss: 6656.87, base loss: 20514.02
[INFO 2017-06-26 12:37:35,924 main.py:50] epoch 1871, training loss: 6306.81, average training loss: 6656.09, base loss: 20514.02
[INFO 2017-06-26 12:37:36,284 main.py:50] epoch 1872, training loss: 6416.90, average training loss: 6655.48, base loss: 20514.12
[INFO 2017-06-26 12:37:36,645 main.py:50] epoch 1873, training loss: 6280.15, average training loss: 6654.64, base loss: 20513.65
[INFO 2017-06-26 12:37:37,006 main.py:50] epoch 1874, training loss: 6296.37, average training loss: 6653.86, base loss: 20513.75
[INFO 2017-06-26 12:37:37,367 main.py:50] epoch 1875, training loss: 6374.58, average training loss: 6653.12, base loss: 20513.74
[INFO 2017-06-26 12:37:37,727 main.py:50] epoch 1876, training loss: 6329.17, average training loss: 6652.37, base loss: 20513.98
[INFO 2017-06-26 12:37:38,088 main.py:50] epoch 1877, training loss: 6299.61, average training loss: 6651.61, base loss: 20514.57
[INFO 2017-06-26 12:37:38,447 main.py:50] epoch 1878, training loss: 6300.75, average training loss: 6650.83, base loss: 20514.58
[INFO 2017-06-26 12:37:38,805 main.py:50] epoch 1879, training loss: 6299.83, average training loss: 6650.02, base loss: 20514.57
[INFO 2017-06-26 12:37:39,166 main.py:50] epoch 1880, training loss: 6198.34, average training loss: 6649.12, base loss: 20514.06
[INFO 2017-06-26 12:37:39,525 main.py:50] epoch 1881, training loss: 6361.97, average training loss: 6648.39, base loss: 20514.29
[INFO 2017-06-26 12:37:39,886 main.py:50] epoch 1882, training loss: 6274.06, average training loss: 6647.65, base loss: 20514.42
[INFO 2017-06-26 12:37:40,247 main.py:50] epoch 1883, training loss: 6270.23, average training loss: 6646.83, base loss: 20513.88
[INFO 2017-06-26 12:37:40,606 main.py:50] epoch 1884, training loss: 6420.45, average training loss: 6646.19, base loss: 20513.94
[INFO 2017-06-26 12:37:40,967 main.py:50] epoch 1885, training loss: 6364.98, average training loss: 6645.51, base loss: 20514.28
[INFO 2017-06-26 12:37:41,328 main.py:50] epoch 1886, training loss: 6426.87, average training loss: 6644.87, base loss: 20513.75
[INFO 2017-06-26 12:37:41,689 main.py:50] epoch 1887, training loss: 6324.70, average training loss: 6644.14, base loss: 20513.65
[INFO 2017-06-26 12:37:42,048 main.py:50] epoch 1888, training loss: 6504.27, average training loss: 6643.57, base loss: 20513.99
[INFO 2017-06-26 12:37:42,407 main.py:50] epoch 1889, training loss: 6362.08, average training loss: 6642.94, base loss: 20514.01
[INFO 2017-06-26 12:37:42,767 main.py:50] epoch 1890, training loss: 6295.91, average training loss: 6642.22, base loss: 20513.95
[INFO 2017-06-26 12:37:43,128 main.py:50] epoch 1891, training loss: 6369.38, average training loss: 6641.53, base loss: 20514.28
[INFO 2017-06-26 12:37:43,489 main.py:50] epoch 1892, training loss: 6307.26, average training loss: 6640.74, base loss: 20514.06
[INFO 2017-06-26 12:37:43,849 main.py:50] epoch 1893, training loss: 6300.31, average training loss: 6639.86, base loss: 20513.99
[INFO 2017-06-26 12:37:44,209 main.py:50] epoch 1894, training loss: 6312.90, average training loss: 6639.10, base loss: 20514.05
[INFO 2017-06-26 12:37:44,570 main.py:50] epoch 1895, training loss: 6311.64, average training loss: 6638.34, base loss: 20513.85
[INFO 2017-06-26 12:37:44,931 main.py:50] epoch 1896, training loss: 6312.07, average training loss: 6637.63, base loss: 20513.66
[INFO 2017-06-26 12:37:45,292 main.py:50] epoch 1897, training loss: 6308.94, average training loss: 6636.80, base loss: 20513.55
[INFO 2017-06-26 12:37:45,652 main.py:50] epoch 1898, training loss: 6292.98, average training loss: 6636.03, base loss: 20513.59
[INFO 2017-06-26 12:37:46,011 main.py:50] epoch 1899, training loss: 6336.36, average training loss: 6635.24, base loss: 20513.86
[INFO 2017-06-26 12:37:46,011 main.py:52] epoch 1899, testing
[INFO 2017-06-26 12:37:47,494 main.py:103] average testing loss: 6326.42, base loss: 20584.44
[INFO 2017-06-26 12:37:47,495 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 12:37:47,501 main.py:76] current best accuracy: 6326.42
[INFO 2017-06-26 12:37:47,861 main.py:50] epoch 1900, training loss: 6313.49, average training loss: 6634.56, base loss: 20513.99
[INFO 2017-06-26 12:37:48,234 main.py:50] epoch 1901, training loss: 6389.30, average training loss: 6633.89, base loss: 20513.79
[INFO 2017-06-26 12:37:48,593 main.py:50] epoch 1902, training loss: 6297.29, average training loss: 6633.17, base loss: 20513.86
[INFO 2017-06-26 12:37:48,950 main.py:50] epoch 1903, training loss: 6308.72, average training loss: 6632.32, base loss: 20513.92
[INFO 2017-06-26 12:37:49,312 main.py:50] epoch 1904, training loss: 6269.39, average training loss: 6631.61, base loss: 20513.83
[INFO 2017-06-26 12:37:49,671 main.py:50] epoch 1905, training loss: 6308.90, average training loss: 6630.84, base loss: 20513.73
[INFO 2017-06-26 12:37:50,032 main.py:50] epoch 1906, training loss: 6359.66, average training loss: 6630.18, base loss: 20513.52
[INFO 2017-06-26 12:37:50,391 main.py:50] epoch 1907, training loss: 6377.92, average training loss: 6629.51, base loss: 20513.22
[INFO 2017-06-26 12:37:50,751 main.py:50] epoch 1908, training loss: 6384.38, average training loss: 6628.84, base loss: 20513.55
[INFO 2017-06-26 12:37:51,111 main.py:50] epoch 1909, training loss: 6340.21, average training loss: 6628.15, base loss: 20513.69
[INFO 2017-06-26 12:37:51,471 main.py:50] epoch 1910, training loss: 6384.53, average training loss: 6627.47, base loss: 20513.67
[INFO 2017-06-26 12:37:51,832 main.py:50] epoch 1911, training loss: 6357.06, average training loss: 6626.69, base loss: 20513.28
[INFO 2017-06-26 12:37:52,205 main.py:50] epoch 1912, training loss: 6355.98, average training loss: 6625.99, base loss: 20513.08
[INFO 2017-06-26 12:37:52,564 main.py:50] epoch 1913, training loss: 6392.02, average training loss: 6625.37, base loss: 20513.18
[INFO 2017-06-26 12:37:52,926 main.py:50] epoch 1914, training loss: 6411.24, average training loss: 6624.76, base loss: 20513.22
[INFO 2017-06-26 12:37:53,285 main.py:50] epoch 1915, training loss: 6370.13, average training loss: 6624.07, base loss: 20513.33
[INFO 2017-06-26 12:37:53,646 main.py:50] epoch 1916, training loss: 6331.44, average training loss: 6623.36, base loss: 20513.19
[INFO 2017-06-26 12:37:54,007 main.py:50] epoch 1917, training loss: 6332.08, average training loss: 6622.76, base loss: 20513.29
[INFO 2017-06-26 12:37:54,367 main.py:50] epoch 1918, training loss: 6396.83, average training loss: 6622.15, base loss: 20513.70
[INFO 2017-06-26 12:37:54,728 main.py:50] epoch 1919, training loss: 6401.12, average training loss: 6621.45, base loss: 20513.64
[INFO 2017-06-26 12:37:55,089 main.py:50] epoch 1920, training loss: 6425.55, average training loss: 6620.82, base loss: 20513.81
[INFO 2017-06-26 12:37:55,449 main.py:50] epoch 1921, training loss: 6297.60, average training loss: 6620.20, base loss: 20514.10
[INFO 2017-06-26 12:37:55,810 main.py:50] epoch 1922, training loss: 6227.56, average training loss: 6619.34, base loss: 20514.05
[INFO 2017-06-26 12:37:56,170 main.py:50] epoch 1923, training loss: 6344.05, average training loss: 6618.61, base loss: 20514.02
[INFO 2017-06-26 12:37:56,531 main.py:50] epoch 1924, training loss: 6301.08, average training loss: 6617.95, base loss: 20513.96
[INFO 2017-06-26 12:37:56,890 main.py:50] epoch 1925, training loss: 6321.93, average training loss: 6617.24, base loss: 20513.70
[INFO 2017-06-26 12:37:57,251 main.py:50] epoch 1926, training loss: 6440.34, average training loss: 6616.74, base loss: 20513.78
[INFO 2017-06-26 12:37:57,612 main.py:50] epoch 1927, training loss: 6331.88, average training loss: 6616.20, base loss: 20513.95
[INFO 2017-06-26 12:37:57,972 main.py:50] epoch 1928, training loss: 6348.17, average training loss: 6615.55, base loss: 20514.01
[INFO 2017-06-26 12:37:58,333 main.py:50] epoch 1929, training loss: 6338.45, average training loss: 6614.96, base loss: 20514.20
[INFO 2017-06-26 12:37:58,695 main.py:50] epoch 1930, training loss: 6351.26, average training loss: 6614.31, base loss: 20514.33
[INFO 2017-06-26 12:37:59,055 main.py:50] epoch 1931, training loss: 6323.01, average training loss: 6613.68, base loss: 20514.40
[INFO 2017-06-26 12:37:59,416 main.py:50] epoch 1932, training loss: 6329.15, average training loss: 6613.10, base loss: 20514.40
[INFO 2017-06-26 12:37:59,776 main.py:50] epoch 1933, training loss: 6310.30, average training loss: 6612.45, base loss: 20514.29
[INFO 2017-06-26 12:38:00,137 main.py:50] epoch 1934, training loss: 6268.67, average training loss: 6611.81, base loss: 20514.15
[INFO 2017-06-26 12:38:00,498 main.py:50] epoch 1935, training loss: 6355.94, average training loss: 6611.18, base loss: 20514.49
[INFO 2017-06-26 12:38:00,859 main.py:50] epoch 1936, training loss: 6304.64, average training loss: 6610.51, base loss: 20514.29
[INFO 2017-06-26 12:38:01,219 main.py:50] epoch 1937, training loss: 6301.66, average training loss: 6609.82, base loss: 20514.17
[INFO 2017-06-26 12:38:01,578 main.py:50] epoch 1938, training loss: 6319.19, average training loss: 6609.25, base loss: 20514.27
[INFO 2017-06-26 12:38:01,940 main.py:50] epoch 1939, training loss: 6361.95, average training loss: 6608.71, base loss: 20514.44
[INFO 2017-06-26 12:38:02,300 main.py:50] epoch 1940, training loss: 6323.31, average training loss: 6608.18, base loss: 20514.63
[INFO 2017-06-26 12:38:02,660 main.py:50] epoch 1941, training loss: 6315.31, average training loss: 6607.53, base loss: 20514.48
[INFO 2017-06-26 12:38:03,020 main.py:50] epoch 1942, training loss: 6298.90, average training loss: 6606.90, base loss: 20514.64
[INFO 2017-06-26 12:38:03,381 main.py:50] epoch 1943, training loss: 6276.99, average training loss: 6606.31, base loss: 20514.84
[INFO 2017-06-26 12:38:03,741 main.py:50] epoch 1944, training loss: 6283.57, average training loss: 6605.72, base loss: 20514.84
[INFO 2017-06-26 12:38:04,100 main.py:50] epoch 1945, training loss: 6262.65, average training loss: 6605.00, base loss: 20514.91
[INFO 2017-06-26 12:38:04,461 main.py:50] epoch 1946, training loss: 6295.48, average training loss: 6604.33, base loss: 20514.92
[INFO 2017-06-26 12:38:04,822 main.py:50] epoch 1947, training loss: 6308.52, average training loss: 6603.56, base loss: 20514.91
[INFO 2017-06-26 12:38:05,182 main.py:50] epoch 1948, training loss: 6339.20, average training loss: 6602.88, base loss: 20514.93
[INFO 2017-06-26 12:38:05,542 main.py:50] epoch 1949, training loss: 6378.00, average training loss: 6602.29, base loss: 20514.87
[INFO 2017-06-26 12:38:05,903 main.py:50] epoch 1950, training loss: 6275.09, average training loss: 6601.49, base loss: 20514.53
[INFO 2017-06-26 12:38:06,263 main.py:50] epoch 1951, training loss: 6283.39, average training loss: 6600.80, base loss: 20514.15
[INFO 2017-06-26 12:38:06,622 main.py:50] epoch 1952, training loss: 6336.64, average training loss: 6600.13, base loss: 20514.28
[INFO 2017-06-26 12:38:06,984 main.py:50] epoch 1953, training loss: 6297.99, average training loss: 6599.50, base loss: 20514.32
[INFO 2017-06-26 12:38:07,344 main.py:50] epoch 1954, training loss: 6258.20, average training loss: 6598.71, base loss: 20514.26
[INFO 2017-06-26 12:38:07,705 main.py:50] epoch 1955, training loss: 6295.97, average training loss: 6598.03, base loss: 20514.03
[INFO 2017-06-26 12:38:08,066 main.py:50] epoch 1956, training loss: 6272.17, average training loss: 6597.37, base loss: 20514.26
[INFO 2017-06-26 12:38:08,427 main.py:50] epoch 1957, training loss: 6261.64, average training loss: 6596.68, base loss: 20514.30
[INFO 2017-06-26 12:38:08,787 main.py:50] epoch 1958, training loss: 6419.44, average training loss: 6596.21, base loss: 20514.46
[INFO 2017-06-26 12:38:09,148 main.py:50] epoch 1959, training loss: 6313.50, average training loss: 6595.58, base loss: 20514.06
[INFO 2017-06-26 12:38:09,509 main.py:50] epoch 1960, training loss: 6328.79, average training loss: 6594.93, base loss: 20514.28
[INFO 2017-06-26 12:38:09,869 main.py:50] epoch 1961, training loss: 6234.96, average training loss: 6594.16, base loss: 20514.24
[INFO 2017-06-26 12:38:10,229 main.py:50] epoch 1962, training loss: 6311.33, average training loss: 6593.42, base loss: 20513.75
[INFO 2017-06-26 12:38:10,589 main.py:50] epoch 1963, training loss: 6420.04, average training loss: 6592.87, base loss: 20514.19
[INFO 2017-06-26 12:38:10,949 main.py:50] epoch 1964, training loss: 6345.12, average training loss: 6592.26, base loss: 20514.20
[INFO 2017-06-26 12:38:11,310 main.py:50] epoch 1965, training loss: 6341.52, average training loss: 6591.68, base loss: 20514.41
[INFO 2017-06-26 12:38:11,671 main.py:50] epoch 1966, training loss: 6374.21, average training loss: 6591.00, base loss: 20514.29
[INFO 2017-06-26 12:38:12,032 main.py:50] epoch 1967, training loss: 6347.54, average training loss: 6590.42, base loss: 20514.54
[INFO 2017-06-26 12:38:12,391 main.py:50] epoch 1968, training loss: 6270.23, average training loss: 6589.70, base loss: 20514.10
[INFO 2017-06-26 12:38:12,752 main.py:50] epoch 1969, training loss: 6342.11, average training loss: 6589.11, base loss: 20513.75
[INFO 2017-06-26 12:38:13,113 main.py:50] epoch 1970, training loss: 6279.14, average training loss: 6588.40, base loss: 20513.61
[INFO 2017-06-26 12:38:13,472 main.py:50] epoch 1971, training loss: 6385.34, average training loss: 6587.89, base loss: 20513.89
[INFO 2017-06-26 12:38:13,833 main.py:50] epoch 1972, training loss: 6382.76, average training loss: 6587.39, base loss: 20513.95
[INFO 2017-06-26 12:38:14,194 main.py:50] epoch 1973, training loss: 6339.77, average training loss: 6586.80, base loss: 20513.98
[INFO 2017-06-26 12:38:14,555 main.py:50] epoch 1974, training loss: 6330.73, average training loss: 6586.22, base loss: 20514.06
[INFO 2017-06-26 12:38:14,915 main.py:50] epoch 1975, training loss: 6359.15, average training loss: 6585.74, base loss: 20514.35
[INFO 2017-06-26 12:38:15,276 main.py:50] epoch 1976, training loss: 6412.64, average training loss: 6585.25, base loss: 20514.26
[INFO 2017-06-26 12:38:15,635 main.py:50] epoch 1977, training loss: 6397.62, average training loss: 6584.81, base loss: 20514.71
[INFO 2017-06-26 12:38:15,997 main.py:50] epoch 1978, training loss: 6399.68, average training loss: 6584.18, base loss: 20514.58
[INFO 2017-06-26 12:38:16,358 main.py:50] epoch 1979, training loss: 6386.83, average training loss: 6583.59, base loss: 20514.63
[INFO 2017-06-26 12:38:16,719 main.py:50] epoch 1980, training loss: 6369.88, average training loss: 6583.05, base loss: 20514.95
[INFO 2017-06-26 12:38:17,080 main.py:50] epoch 1981, training loss: 6303.81, average training loss: 6582.51, base loss: 20515.14
[INFO 2017-06-26 12:38:17,440 main.py:50] epoch 1982, training loss: 6315.91, average training loss: 6581.84, base loss: 20515.09
[INFO 2017-06-26 12:38:17,800 main.py:50] epoch 1983, training loss: 6278.41, average training loss: 6581.24, base loss: 20514.61
[INFO 2017-06-26 12:38:18,161 main.py:50] epoch 1984, training loss: 6254.91, average training loss: 6580.53, base loss: 20514.20
[INFO 2017-06-26 12:38:18,520 main.py:50] epoch 1985, training loss: 6385.51, average training loss: 6580.02, base loss: 20514.31
[INFO 2017-06-26 12:38:18,880 main.py:50] epoch 1986, training loss: 6441.15, average training loss: 6579.57, base loss: 20514.52
[INFO 2017-06-26 12:38:19,240 main.py:50] epoch 1987, training loss: 6375.32, average training loss: 6579.02, base loss: 20514.72
[INFO 2017-06-26 12:38:19,600 main.py:50] epoch 1988, training loss: 6301.90, average training loss: 6578.39, base loss: 20514.55
[INFO 2017-06-26 12:38:19,962 main.py:50] epoch 1989, training loss: 6390.73, average training loss: 6577.87, base loss: 20514.58
[INFO 2017-06-26 12:38:20,322 main.py:50] epoch 1990, training loss: 6322.16, average training loss: 6577.19, base loss: 20514.08
[INFO 2017-06-26 12:38:20,682 main.py:50] epoch 1991, training loss: 6357.13, average training loss: 6576.69, base loss: 20514.26
[INFO 2017-06-26 12:38:21,042 main.py:50] epoch 1992, training loss: 6415.67, average training loss: 6576.20, base loss: 20514.42
[INFO 2017-06-26 12:38:21,403 main.py:50] epoch 1993, training loss: 6391.66, average training loss: 6575.68, base loss: 20514.98
[INFO 2017-06-26 12:38:21,764 main.py:50] epoch 1994, training loss: 6412.17, average training loss: 6575.24, base loss: 20515.43
[INFO 2017-06-26 12:38:22,124 main.py:50] epoch 1995, training loss: 6337.14, average training loss: 6574.69, base loss: 20515.52
[INFO 2017-06-26 12:38:22,484 main.py:50] epoch 1996, training loss: 6334.92, average training loss: 6574.09, base loss: 20515.32
[INFO 2017-06-26 12:38:22,845 main.py:50] epoch 1997, training loss: 6327.61, average training loss: 6573.55, base loss: 20515.80
[INFO 2017-06-26 12:38:23,205 main.py:50] epoch 1998, training loss: 6324.90, average training loss: 6572.94, base loss: 20515.84
[INFO 2017-06-26 12:38:23,565 main.py:50] epoch 1999, training loss: 6286.30, average training loss: 6572.28, base loss: 20515.42
[INFO 2017-06-26 12:38:23,565 main.py:52] epoch 1999, testing
[INFO 2017-06-26 12:38:25,046 main.py:103] average testing loss: 6289.64, base loss: 20492.17
[INFO 2017-06-26 12:38:25,046 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 12:38:25,052 main.py:76] current best accuracy: 6289.64
[INFO 2017-06-26 12:38:25,411 main.py:50] epoch 2000, training loss: 6279.32, average training loss: 6571.71, base loss: 20515.52
[INFO 2017-06-26 12:38:25,771 main.py:50] epoch 2001, training loss: 6293.39, average training loss: 6571.04, base loss: 20515.61
[INFO 2017-06-26 12:38:26,131 main.py:50] epoch 2002, training loss: 6328.81, average training loss: 6570.46, base loss: 20516.18
[INFO 2017-06-26 12:38:26,491 main.py:50] epoch 2003, training loss: 6232.64, average training loss: 6569.74, base loss: 20516.05
[INFO 2017-06-26 12:38:26,849 main.py:50] epoch 2004, training loss: 6453.99, average training loss: 6569.32, base loss: 20516.17
[INFO 2017-06-26 12:38:27,209 main.py:50] epoch 2005, training loss: 6295.13, average training loss: 6568.73, base loss: 20516.06
[INFO 2017-06-26 12:38:27,568 main.py:50] epoch 2006, training loss: 6325.82, average training loss: 6568.07, base loss: 20515.83
[INFO 2017-06-26 12:38:27,927 main.py:50] epoch 2007, training loss: 6346.66, average training loss: 6567.55, base loss: 20516.09
[INFO 2017-06-26 12:38:28,286 main.py:50] epoch 2008, training loss: 6315.91, average training loss: 6566.97, base loss: 20516.30
[INFO 2017-06-26 12:38:28,646 main.py:50] epoch 2009, training loss: 6294.97, average training loss: 6566.42, base loss: 20516.28
[INFO 2017-06-26 12:38:29,007 main.py:50] epoch 2010, training loss: 6315.99, average training loss: 6565.84, base loss: 20516.44
[INFO 2017-06-26 12:38:29,368 main.py:50] epoch 2011, training loss: 6410.10, average training loss: 6565.43, base loss: 20516.69
[INFO 2017-06-26 12:38:29,727 main.py:50] epoch 2012, training loss: 6287.92, average training loss: 6564.76, base loss: 20516.90
[INFO 2017-06-26 12:38:30,088 main.py:50] epoch 2013, training loss: 6302.77, average training loss: 6564.11, base loss: 20516.97
[INFO 2017-06-26 12:38:30,448 main.py:50] epoch 2014, training loss: 6246.89, average training loss: 6563.53, base loss: 20517.17
[INFO 2017-06-26 12:38:30,809 main.py:50] epoch 2015, training loss: 6319.80, average training loss: 6563.00, base loss: 20517.34
[INFO 2017-06-26 12:38:31,168 main.py:50] epoch 2016, training loss: 6275.23, average training loss: 6562.33, base loss: 20517.10
[INFO 2017-06-26 12:38:31,527 main.py:50] epoch 2017, training loss: 6235.02, average training loss: 6561.76, base loss: 20516.95
[INFO 2017-06-26 12:38:31,889 main.py:50] epoch 2018, training loss: 6356.12, average training loss: 6561.12, base loss: 20517.11
[INFO 2017-06-26 12:38:32,250 main.py:50] epoch 2019, training loss: 6303.83, average training loss: 6560.67, base loss: 20517.32
[INFO 2017-06-26 12:38:32,625 main.py:50] epoch 2020, training loss: 6329.25, average training loss: 6560.06, base loss: 20517.60
[INFO 2017-06-26 12:38:32,986 main.py:50] epoch 2021, training loss: 6316.67, average training loss: 6559.48, base loss: 20517.75
[INFO 2017-06-26 12:38:33,347 main.py:50] epoch 2022, training loss: 6362.47, average training loss: 6558.87, base loss: 20517.73
[INFO 2017-06-26 12:38:33,707 main.py:50] epoch 2023, training loss: 6225.29, average training loss: 6558.13, base loss: 20517.78
[INFO 2017-06-26 12:38:34,068 main.py:50] epoch 2024, training loss: 6348.66, average training loss: 6557.57, base loss: 20517.96
[INFO 2017-06-26 12:38:34,429 main.py:50] epoch 2025, training loss: 6324.67, average training loss: 6556.98, base loss: 20517.43
[INFO 2017-06-26 12:38:34,790 main.py:50] epoch 2026, training loss: 6333.00, average training loss: 6556.49, base loss: 20517.66
[INFO 2017-06-26 12:38:35,150 main.py:50] epoch 2027, training loss: 6366.74, average training loss: 6556.04, base loss: 20517.96
[INFO 2017-06-26 12:38:35,511 main.py:50] epoch 2028, training loss: 6301.21, average training loss: 6555.46, base loss: 20517.82
[INFO 2017-06-26 12:38:35,872 main.py:50] epoch 2029, training loss: 6408.22, average training loss: 6554.88, base loss: 20517.11
[INFO 2017-06-26 12:38:36,232 main.py:50] epoch 2030, training loss: 6402.12, average training loss: 6554.38, base loss: 20517.56
[INFO 2017-06-26 12:38:36,593 main.py:50] epoch 2031, training loss: 6424.05, average training loss: 6553.86, base loss: 20517.53
[INFO 2017-06-26 12:38:36,953 main.py:50] epoch 2032, training loss: 6252.03, average training loss: 6553.19, base loss: 20517.43
[INFO 2017-06-26 12:38:37,314 main.py:50] epoch 2033, training loss: 6311.48, average training loss: 6552.65, base loss: 20517.11
[INFO 2017-06-26 12:38:37,674 main.py:50] epoch 2034, training loss: 6292.94, average training loss: 6551.98, base loss: 20516.83
[INFO 2017-06-26 12:38:38,034 main.py:50] epoch 2035, training loss: 6324.44, average training loss: 6551.34, base loss: 20516.84
[INFO 2017-06-26 12:38:38,394 main.py:50] epoch 2036, training loss: 6238.28, average training loss: 6550.67, base loss: 20516.49
[INFO 2017-06-26 12:38:38,755 main.py:50] epoch 2037, training loss: 6293.85, average training loss: 6550.16, base loss: 20516.85
[INFO 2017-06-26 12:38:39,116 main.py:50] epoch 2038, training loss: 6314.63, average training loss: 6549.61, base loss: 20516.45
[INFO 2017-06-26 12:38:39,476 main.py:50] epoch 2039, training loss: 6370.63, average training loss: 6549.17, base loss: 20516.55
[INFO 2017-06-26 12:38:39,837 main.py:50] epoch 2040, training loss: 6288.00, average training loss: 6548.64, base loss: 20516.66
[INFO 2017-06-26 12:38:40,197 main.py:50] epoch 2041, training loss: 6305.08, average training loss: 6548.06, base loss: 20516.67
[INFO 2017-06-26 12:38:40,557 main.py:50] epoch 2042, training loss: 6303.98, average training loss: 6547.55, base loss: 20516.73
[INFO 2017-06-26 12:38:40,918 main.py:50] epoch 2043, training loss: 6387.00, average training loss: 6547.11, base loss: 20516.75
[INFO 2017-06-26 12:38:41,279 main.py:50] epoch 2044, training loss: 6316.78, average training loss: 6546.52, base loss: 20516.69
[INFO 2017-06-26 12:38:41,639 main.py:50] epoch 2045, training loss: 6376.03, average training loss: 6545.97, base loss: 20517.03
[INFO 2017-06-26 12:38:42,000 main.py:50] epoch 2046, training loss: 6467.77, average training loss: 6545.56, base loss: 20516.92
[INFO 2017-06-26 12:38:42,360 main.py:50] epoch 2047, training loss: 6331.50, average training loss: 6545.04, base loss: 20516.95
[INFO 2017-06-26 12:38:42,721 main.py:50] epoch 2048, training loss: 6330.26, average training loss: 6544.52, base loss: 20517.28
[INFO 2017-06-26 12:38:43,082 main.py:50] epoch 2049, training loss: 6312.20, average training loss: 6543.91, base loss: 20517.33
[INFO 2017-06-26 12:38:43,443 main.py:50] epoch 2050, training loss: 6358.07, average training loss: 6543.41, base loss: 20517.15
[INFO 2017-06-26 12:38:43,804 main.py:50] epoch 2051, training loss: 6369.45, average training loss: 6543.01, base loss: 20517.58
[INFO 2017-06-26 12:38:44,165 main.py:50] epoch 2052, training loss: 6374.50, average training loss: 6542.52, base loss: 20517.39
[INFO 2017-06-26 12:38:44,524 main.py:50] epoch 2053, training loss: 6311.00, average training loss: 6541.90, base loss: 20517.27
[INFO 2017-06-26 12:38:44,885 main.py:50] epoch 2054, training loss: 6242.21, average training loss: 6541.23, base loss: 20517.09
[INFO 2017-06-26 12:38:45,244 main.py:50] epoch 2055, training loss: 6321.17, average training loss: 6540.74, base loss: 20517.42
[INFO 2017-06-26 12:38:45,605 main.py:50] epoch 2056, training loss: 6315.99, average training loss: 6540.20, base loss: 20517.21
[INFO 2017-06-26 12:38:45,964 main.py:50] epoch 2057, training loss: 6316.66, average training loss: 6539.66, base loss: 20517.22
[INFO 2017-06-26 12:38:46,327 main.py:50] epoch 2058, training loss: 6365.20, average training loss: 6539.14, base loss: 20517.07
[INFO 2017-06-26 12:38:46,687 main.py:50] epoch 2059, training loss: 6260.59, average training loss: 6538.52, base loss: 20516.73
[INFO 2017-06-26 12:38:47,047 main.py:50] epoch 2060, training loss: 6250.18, average training loss: 6537.88, base loss: 20516.79
[INFO 2017-06-26 12:38:47,409 main.py:50] epoch 2061, training loss: 6281.54, average training loss: 6537.29, base loss: 20516.95
[INFO 2017-06-26 12:38:47,769 main.py:50] epoch 2062, training loss: 6340.78, average training loss: 6536.64, base loss: 20517.08
[INFO 2017-06-26 12:38:48,128 main.py:50] epoch 2063, training loss: 6315.48, average training loss: 6536.11, base loss: 20517.09
[INFO 2017-06-26 12:38:48,490 main.py:50] epoch 2064, training loss: 6234.59, average training loss: 6535.50, base loss: 20516.52
[INFO 2017-06-26 12:38:48,849 main.py:50] epoch 2065, training loss: 6307.54, average training loss: 6534.90, base loss: 20516.40
[INFO 2017-06-26 12:38:49,211 main.py:50] epoch 2066, training loss: 6352.21, average training loss: 6534.43, base loss: 20516.71
[INFO 2017-06-26 12:38:49,571 main.py:50] epoch 2067, training loss: 6341.33, average training loss: 6533.88, base loss: 20516.61
[INFO 2017-06-26 12:38:49,931 main.py:50] epoch 2068, training loss: 6281.17, average training loss: 6533.33, base loss: 20516.62
[INFO 2017-06-26 12:38:50,290 main.py:50] epoch 2069, training loss: 6344.34, average training loss: 6532.80, base loss: 20516.56
[INFO 2017-06-26 12:38:50,649 main.py:50] epoch 2070, training loss: 6328.75, average training loss: 6532.16, base loss: 20516.75
[INFO 2017-06-26 12:38:51,010 main.py:50] epoch 2071, training loss: 6359.22, average training loss: 6531.74, base loss: 20516.90
[INFO 2017-06-26 12:38:51,371 main.py:50] epoch 2072, training loss: 6339.08, average training loss: 6531.16, base loss: 20516.77
[INFO 2017-06-26 12:38:51,731 main.py:50] epoch 2073, training loss: 6321.59, average training loss: 6530.68, base loss: 20516.75
[INFO 2017-06-26 12:38:52,092 main.py:50] epoch 2074, training loss: 6341.50, average training loss: 6530.24, base loss: 20516.94
[INFO 2017-06-26 12:38:52,453 main.py:50] epoch 2075, training loss: 6288.66, average training loss: 6529.52, base loss: 20516.89
[INFO 2017-06-26 12:38:52,813 main.py:50] epoch 2076, training loss: 6282.91, average training loss: 6528.97, base loss: 20516.99
[INFO 2017-06-26 12:38:53,174 main.py:50] epoch 2077, training loss: 6237.30, average training loss: 6528.33, base loss: 20517.19
[INFO 2017-06-26 12:38:53,535 main.py:50] epoch 2078, training loss: 6290.64, average training loss: 6527.71, base loss: 20517.45
[INFO 2017-06-26 12:38:53,896 main.py:50] epoch 2079, training loss: 6294.75, average training loss: 6527.23, base loss: 20517.54
[INFO 2017-06-26 12:38:54,256 main.py:50] epoch 2080, training loss: 6253.41, average training loss: 6526.66, base loss: 20517.60
[INFO 2017-06-26 12:38:54,615 main.py:50] epoch 2081, training loss: 6246.01, average training loss: 6526.07, base loss: 20517.70
[INFO 2017-06-26 12:38:54,975 main.py:50] epoch 2082, training loss: 6235.91, average training loss: 6525.47, base loss: 20517.70
[INFO 2017-06-26 12:38:55,387 main.py:50] epoch 2083, training loss: 6220.96, average training loss: 6524.88, base loss: 20517.63
[INFO 2017-06-26 12:38:55,753 main.py:50] epoch 2084, training loss: 6221.50, average training loss: 6524.31, base loss: 20517.53
[INFO 2017-06-26 12:38:56,114 main.py:50] epoch 2085, training loss: 6249.48, average training loss: 6523.70, base loss: 20517.47
[INFO 2017-06-26 12:38:56,475 main.py:50] epoch 2086, training loss: 6308.69, average training loss: 6523.21, base loss: 20517.82
[INFO 2017-06-26 12:38:56,836 main.py:50] epoch 2087, training loss: 6190.58, average training loss: 6522.63, base loss: 20517.78
[INFO 2017-06-26 12:38:57,195 main.py:50] epoch 2088, training loss: 6275.21, average training loss: 6522.05, base loss: 20518.05
[INFO 2017-06-26 12:38:57,556 main.py:50] epoch 2089, training loss: 6290.85, average training loss: 6521.60, base loss: 20518.27
[INFO 2017-06-26 12:38:57,918 main.py:50] epoch 2090, training loss: 6292.15, average training loss: 6521.01, base loss: 20518.35
[INFO 2017-06-26 12:38:58,279 main.py:50] epoch 2091, training loss: 6367.00, average training loss: 6520.62, base loss: 20518.66
[INFO 2017-06-26 12:38:58,639 main.py:50] epoch 2092, training loss: 6336.01, average training loss: 6520.08, base loss: 20518.94
[INFO 2017-06-26 12:38:58,998 main.py:50] epoch 2093, training loss: 6280.72, average training loss: 6519.56, base loss: 20519.07
[INFO 2017-06-26 12:38:59,360 main.py:50] epoch 2094, training loss: 6238.99, average training loss: 6518.90, base loss: 20518.91
[INFO 2017-06-26 12:38:59,721 main.py:50] epoch 2095, training loss: 6208.14, average training loss: 6518.33, base loss: 20518.84
[INFO 2017-06-26 12:39:00,083 main.py:50] epoch 2096, training loss: 6348.50, average training loss: 6517.82, base loss: 20518.95
[INFO 2017-06-26 12:39:00,443 main.py:50] epoch 2097, training loss: 6286.30, average training loss: 6517.27, base loss: 20519.01
[INFO 2017-06-26 12:39:00,804 main.py:50] epoch 2098, training loss: 6243.70, average training loss: 6516.71, base loss: 20519.12
[INFO 2017-06-26 12:39:01,165 main.py:50] epoch 2099, training loss: 6199.16, average training loss: 6516.10, base loss: 20518.83
[INFO 2017-06-26 12:39:01,165 main.py:52] epoch 2099, testing
[INFO 2017-06-26 12:39:02,636 main.py:103] average testing loss: 6318.27, base loss: 20602.55
[INFO 2017-06-26 12:39:02,637 main.py:76] current best accuracy: 6289.64
[INFO 2017-06-26 12:39:02,994 main.py:50] epoch 2100, training loss: 6357.87, average training loss: 6515.72, base loss: 20519.11
[INFO 2017-06-26 12:39:03,352 main.py:50] epoch 2101, training loss: 6342.22, average training loss: 6515.28, base loss: 20519.10
[INFO 2017-06-26 12:39:03,711 main.py:50] epoch 2102, training loss: 6226.41, average training loss: 6514.79, base loss: 20519.10
[INFO 2017-06-26 12:39:04,071 main.py:50] epoch 2103, training loss: 6278.19, average training loss: 6514.29, base loss: 20519.33
[INFO 2017-06-26 12:39:04,431 main.py:50] epoch 2104, training loss: 6238.65, average training loss: 6513.76, base loss: 20519.34
[INFO 2017-06-26 12:39:04,791 main.py:50] epoch 2105, training loss: 6248.99, average training loss: 6513.26, base loss: 20519.43
[INFO 2017-06-26 12:39:05,151 main.py:50] epoch 2106, training loss: 6291.97, average training loss: 6512.78, base loss: 20519.34
[INFO 2017-06-26 12:39:05,512 main.py:50] epoch 2107, training loss: 6248.06, average training loss: 6512.13, base loss: 20519.46
[INFO 2017-06-26 12:39:05,873 main.py:50] epoch 2108, training loss: 6211.80, average training loss: 6511.43, base loss: 20519.15
[INFO 2017-06-26 12:39:06,234 main.py:50] epoch 2109, training loss: 6241.53, average training loss: 6510.79, base loss: 20519.01
[INFO 2017-06-26 12:39:06,594 main.py:50] epoch 2110, training loss: 6197.10, average training loss: 6510.22, base loss: 20518.81
[INFO 2017-06-26 12:39:06,954 main.py:50] epoch 2111, training loss: 6189.08, average training loss: 6509.64, base loss: 20518.88
[INFO 2017-06-26 12:39:07,314 main.py:50] epoch 2112, training loss: 6281.36, average training loss: 6508.96, base loss: 20518.70
[INFO 2017-06-26 12:39:07,673 main.py:50] epoch 2113, training loss: 6259.73, average training loss: 6508.39, base loss: 20519.01
[INFO 2017-06-26 12:39:08,034 main.py:50] epoch 2114, training loss: 6245.37, average training loss: 6507.93, base loss: 20519.06
[INFO 2017-06-26 12:39:08,395 main.py:50] epoch 2115, training loss: 6235.82, average training loss: 6507.18, base loss: 20518.72
[INFO 2017-06-26 12:39:08,754 main.py:50] epoch 2116, training loss: 6257.12, average training loss: 6506.57, base loss: 20518.80
[INFO 2017-06-26 12:39:09,113 main.py:50] epoch 2117, training loss: 6239.90, average training loss: 6505.88, base loss: 20518.68
[INFO 2017-06-26 12:39:09,470 main.py:50] epoch 2118, training loss: 6250.55, average training loss: 6505.34, base loss: 20518.60
[INFO 2017-06-26 12:39:09,831 main.py:50] epoch 2119, training loss: 6223.14, average training loss: 6504.68, base loss: 20518.21
[INFO 2017-06-26 12:39:10,189 main.py:50] epoch 2120, training loss: 6153.21, average training loss: 6503.98, base loss: 20517.87
[INFO 2017-06-26 12:39:10,548 main.py:50] epoch 2121, training loss: 6237.41, average training loss: 6503.55, base loss: 20518.06
[INFO 2017-06-26 12:39:10,908 main.py:50] epoch 2122, training loss: 6335.88, average training loss: 6503.05, base loss: 20518.30
[INFO 2017-06-26 12:39:11,269 main.py:50] epoch 2123, training loss: 6290.68, average training loss: 6502.42, base loss: 20518.18
[INFO 2017-06-26 12:39:11,629 main.py:50] epoch 2124, training loss: 6216.40, average training loss: 6501.80, base loss: 20518.20
[INFO 2017-06-26 12:39:11,990 main.py:50] epoch 2125, training loss: 6244.78, average training loss: 6501.16, base loss: 20518.38
[INFO 2017-06-26 12:39:12,349 main.py:50] epoch 2126, training loss: 6314.18, average training loss: 6500.62, base loss: 20518.20
[INFO 2017-06-26 12:39:12,711 main.py:50] epoch 2127, training loss: 6256.49, average training loss: 6499.99, base loss: 20518.13
[INFO 2017-06-26 12:39:13,071 main.py:50] epoch 2128, training loss: 6308.77, average training loss: 6499.47, base loss: 20518.11
[INFO 2017-06-26 12:39:13,430 main.py:50] epoch 2129, training loss: 6253.91, average training loss: 6498.85, base loss: 20518.07
[INFO 2017-06-26 12:39:13,791 main.py:50] epoch 2130, training loss: 6244.38, average training loss: 6498.28, base loss: 20517.89
[INFO 2017-06-26 12:39:14,152 main.py:50] epoch 2131, training loss: 6288.91, average training loss: 6497.68, base loss: 20517.87
[INFO 2017-06-26 12:39:14,511 main.py:50] epoch 2132, training loss: 6295.09, average training loss: 6497.25, base loss: 20518.11
[INFO 2017-06-26 12:39:14,871 main.py:50] epoch 2133, training loss: 6276.19, average training loss: 6496.72, base loss: 20518.22
[INFO 2017-06-26 12:39:15,231 main.py:50] epoch 2134, training loss: 6270.09, average training loss: 6496.23, base loss: 20518.14
[INFO 2017-06-26 12:39:15,591 main.py:50] epoch 2135, training loss: 6239.26, average training loss: 6495.64, base loss: 20518.28
[INFO 2017-06-26 12:39:15,951 main.py:50] epoch 2136, training loss: 6299.37, average training loss: 6495.16, base loss: 20518.24
[INFO 2017-06-26 12:39:16,310 main.py:50] epoch 2137, training loss: 6287.11, average training loss: 6494.65, base loss: 20518.25
[INFO 2017-06-26 12:39:16,670 main.py:50] epoch 2138, training loss: 6218.14, average training loss: 6494.10, base loss: 20518.17
[INFO 2017-06-26 12:39:17,030 main.py:50] epoch 2139, training loss: 6362.66, average training loss: 6493.68, base loss: 20518.42
[INFO 2017-06-26 12:39:17,405 main.py:50] epoch 2140, training loss: 6324.81, average training loss: 6493.23, base loss: 20518.53
[INFO 2017-06-26 12:39:17,766 main.py:50] epoch 2141, training loss: 6207.16, average training loss: 6492.79, base loss: 20518.58
[INFO 2017-06-26 12:39:18,127 main.py:50] epoch 2142, training loss: 6248.50, average training loss: 6492.19, base loss: 20518.32
[INFO 2017-06-26 12:39:18,488 main.py:50] epoch 2143, training loss: 6264.17, average training loss: 6491.62, base loss: 20518.24
[INFO 2017-06-26 12:39:18,848 main.py:50] epoch 2144, training loss: 6287.76, average training loss: 6491.15, base loss: 20518.26
[INFO 2017-06-26 12:39:19,207 main.py:50] epoch 2145, training loss: 6332.58, average training loss: 6490.70, base loss: 20518.05
[INFO 2017-06-26 12:39:19,568 main.py:50] epoch 2146, training loss: 6238.36, average training loss: 6490.14, base loss: 20517.95
[INFO 2017-06-26 12:39:19,928 main.py:50] epoch 2147, training loss: 6304.06, average training loss: 6489.69, base loss: 20517.78
[INFO 2017-06-26 12:39:20,289 main.py:50] epoch 2148, training loss: 6289.56, average training loss: 6489.27, base loss: 20517.83
[INFO 2017-06-26 12:39:20,650 main.py:50] epoch 2149, training loss: 6272.07, average training loss: 6488.73, base loss: 20517.75
[INFO 2017-06-26 12:39:21,009 main.py:50] epoch 2150, training loss: 6327.26, average training loss: 6488.21, base loss: 20518.22
[INFO 2017-06-26 12:39:21,370 main.py:50] epoch 2151, training loss: 6267.80, average training loss: 6487.71, base loss: 20518.46
[INFO 2017-06-26 12:39:21,730 main.py:50] epoch 2152, training loss: 6259.31, average training loss: 6487.15, base loss: 20518.42
[INFO 2017-06-26 12:39:22,090 main.py:50] epoch 2153, training loss: 6236.93, average training loss: 6486.66, base loss: 20518.41
[INFO 2017-06-26 12:39:22,450 main.py:50] epoch 2154, training loss: 6303.03, average training loss: 6486.08, base loss: 20518.23
[INFO 2017-06-26 12:39:22,810 main.py:50] epoch 2155, training loss: 6234.19, average training loss: 6485.56, base loss: 20517.97
[INFO 2017-06-26 12:39:23,171 main.py:50] epoch 2156, training loss: 6244.76, average training loss: 6485.04, base loss: 20518.14
[INFO 2017-06-26 12:39:23,530 main.py:50] epoch 2157, training loss: 6315.94, average training loss: 6484.67, base loss: 20518.17
[INFO 2017-06-26 12:39:23,891 main.py:50] epoch 2158, training loss: 6250.81, average training loss: 6484.18, base loss: 20518.13
[INFO 2017-06-26 12:39:24,251 main.py:50] epoch 2159, training loss: 6205.15, average training loss: 6483.64, base loss: 20518.04
[INFO 2017-06-26 12:39:24,611 main.py:50] epoch 2160, training loss: 6302.87, average training loss: 6483.12, base loss: 20517.94
[INFO 2017-06-26 12:39:24,971 main.py:50] epoch 2161, training loss: 6196.56, average training loss: 6482.47, base loss: 20517.66
[INFO 2017-06-26 12:39:25,331 main.py:50] epoch 2162, training loss: 6280.79, average training loss: 6481.98, base loss: 20517.72
[INFO 2017-06-26 12:39:25,690 main.py:50] epoch 2163, training loss: 6277.89, average training loss: 6481.50, base loss: 20517.63
[INFO 2017-06-26 12:39:26,051 main.py:50] epoch 2164, training loss: 6255.90, average training loss: 6481.00, base loss: 20517.93
[INFO 2017-06-26 12:39:26,411 main.py:50] epoch 2165, training loss: 6237.20, average training loss: 6480.42, base loss: 20517.73
[INFO 2017-06-26 12:39:26,772 main.py:50] epoch 2166, training loss: 6209.73, average training loss: 6479.74, base loss: 20517.42
[INFO 2017-06-26 12:39:27,132 main.py:50] epoch 2167, training loss: 6291.95, average training loss: 6479.31, base loss: 20517.47
[INFO 2017-06-26 12:39:27,492 main.py:50] epoch 2168, training loss: 6266.54, average training loss: 6478.88, base loss: 20517.46
[INFO 2017-06-26 12:39:27,851 main.py:50] epoch 2169, training loss: 6238.99, average training loss: 6478.37, base loss: 20517.43
[INFO 2017-06-26 12:39:28,211 main.py:50] epoch 2170, training loss: 6299.41, average training loss: 6477.91, base loss: 20517.19
[INFO 2017-06-26 12:39:28,571 main.py:50] epoch 2171, training loss: 6318.62, average training loss: 6477.48, base loss: 20517.27
[INFO 2017-06-26 12:39:28,932 main.py:50] epoch 2172, training loss: 6290.75, average training loss: 6477.04, base loss: 20517.33
[INFO 2017-06-26 12:39:29,292 main.py:50] epoch 2173, training loss: 6229.88, average training loss: 6476.49, base loss: 20517.36
[INFO 2017-06-26 12:39:29,651 main.py:50] epoch 2174, training loss: 6241.34, average training loss: 6475.96, base loss: 20517.17
[INFO 2017-06-26 12:39:30,011 main.py:50] epoch 2175, training loss: 6244.99, average training loss: 6475.45, base loss: 20517.39
[INFO 2017-06-26 12:39:30,372 main.py:50] epoch 2176, training loss: 6199.68, average training loss: 6474.82, base loss: 20517.17
[INFO 2017-06-26 12:39:30,731 main.py:50] epoch 2177, training loss: 6222.07, average training loss: 6474.39, base loss: 20517.28
[INFO 2017-06-26 12:39:31,091 main.py:50] epoch 2178, training loss: 6235.52, average training loss: 6473.93, base loss: 20517.53
[INFO 2017-06-26 12:39:31,453 main.py:50] epoch 2179, training loss: 6258.58, average training loss: 6473.45, base loss: 20517.78
[INFO 2017-06-26 12:39:31,813 main.py:50] epoch 2180, training loss: 6183.36, average training loss: 6472.91, base loss: 20517.33
[INFO 2017-06-26 12:39:32,173 main.py:50] epoch 2181, training loss: 6230.17, average training loss: 6472.30, base loss: 20517.25
[INFO 2017-06-26 12:39:32,532 main.py:50] epoch 2182, training loss: 6191.08, average training loss: 6471.82, base loss: 20517.37
[INFO 2017-06-26 12:39:32,892 main.py:50] epoch 2183, training loss: 6251.94, average training loss: 6471.29, base loss: 20517.44
[INFO 2017-06-26 12:39:33,253 main.py:50] epoch 2184, training loss: 6217.84, average training loss: 6470.75, base loss: 20517.56
[INFO 2017-06-26 12:39:33,612 main.py:50] epoch 2185, training loss: 6294.03, average training loss: 6470.35, base loss: 20517.81
[INFO 2017-06-26 12:39:33,972 main.py:50] epoch 2186, training loss: 6131.86, average training loss: 6469.68, base loss: 20517.44
[INFO 2017-06-26 12:39:34,332 main.py:50] epoch 2187, training loss: 6322.47, average training loss: 6469.18, base loss: 20517.33
[INFO 2017-06-26 12:39:34,692 main.py:50] epoch 2188, training loss: 6273.39, average training loss: 6468.76, base loss: 20517.89
[INFO 2017-06-26 12:39:35,051 main.py:50] epoch 2189, training loss: 6278.77, average training loss: 6468.38, base loss: 20517.84
[INFO 2017-06-26 12:39:35,410 main.py:50] epoch 2190, training loss: 6316.94, average training loss: 6467.95, base loss: 20517.55
[INFO 2017-06-26 12:39:35,772 main.py:50] epoch 2191, training loss: 6242.48, average training loss: 6467.37, base loss: 20517.67
[INFO 2017-06-26 12:39:36,132 main.py:50] epoch 2192, training loss: 6281.52, average training loss: 6466.92, base loss: 20517.45
[INFO 2017-06-26 12:39:36,492 main.py:50] epoch 2193, training loss: 6277.29, average training loss: 6466.42, base loss: 20517.41
[INFO 2017-06-26 12:39:36,852 main.py:50] epoch 2194, training loss: 6219.40, average training loss: 6465.81, base loss: 20516.82
[INFO 2017-06-26 12:39:37,211 main.py:50] epoch 2195, training loss: 6250.37, average training loss: 6465.34, base loss: 20516.86
[INFO 2017-06-26 12:39:37,571 main.py:50] epoch 2196, training loss: 6253.06, average training loss: 6464.71, base loss: 20516.76
[INFO 2017-06-26 12:39:37,931 main.py:50] epoch 2197, training loss: 6269.14, average training loss: 6464.31, base loss: 20516.70
[INFO 2017-06-26 12:39:38,292 main.py:50] epoch 2198, training loss: 6174.82, average training loss: 6463.69, base loss: 20516.69
[INFO 2017-06-26 12:39:38,652 main.py:50] epoch 2199, training loss: 6258.53, average training loss: 6463.14, base loss: 20516.69
[INFO 2017-06-26 12:39:38,652 main.py:52] epoch 2199, testing
[INFO 2017-06-26 12:39:40,134 main.py:103] average testing loss: 6233.08, base loss: 20533.56
[INFO 2017-06-26 12:39:40,135 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 12:39:40,141 main.py:76] current best accuracy: 6233.08
[INFO 2017-06-26 12:39:40,500 main.py:50] epoch 2200, training loss: 6239.51, average training loss: 6462.70, base loss: 20516.88
[INFO 2017-06-26 12:39:40,860 main.py:50] epoch 2201, training loss: 6288.78, average training loss: 6462.20, base loss: 20516.98
[INFO 2017-06-26 12:39:41,221 main.py:50] epoch 2202, training loss: 6279.01, average training loss: 6461.69, base loss: 20517.10
[INFO 2017-06-26 12:39:41,583 main.py:50] epoch 2203, training loss: 6205.35, average training loss: 6461.18, base loss: 20517.29
[INFO 2017-06-26 12:39:41,944 main.py:50] epoch 2204, training loss: 6261.20, average training loss: 6460.72, base loss: 20517.57
[INFO 2017-06-26 12:39:42,306 main.py:50] epoch 2205, training loss: 6220.11, average training loss: 6460.18, base loss: 20517.76
[INFO 2017-06-26 12:39:42,667 main.py:50] epoch 2206, training loss: 6209.22, average training loss: 6459.60, base loss: 20517.75
[INFO 2017-06-26 12:39:43,028 main.py:50] epoch 2207, training loss: 6281.12, average training loss: 6459.13, base loss: 20517.73
[INFO 2017-06-26 12:39:43,390 main.py:50] epoch 2208, training loss: 6228.62, average training loss: 6458.60, base loss: 20517.96
[INFO 2017-06-26 12:39:43,752 main.py:50] epoch 2209, training loss: 6250.86, average training loss: 6458.10, base loss: 20517.98
[INFO 2017-06-26 12:39:44,113 main.py:50] epoch 2210, training loss: 6127.46, average training loss: 6457.39, base loss: 20517.18
[INFO 2017-06-26 12:39:44,475 main.py:50] epoch 2211, training loss: 6214.47, average training loss: 6456.89, base loss: 20516.80
[INFO 2017-06-26 12:39:44,836 main.py:50] epoch 2212, training loss: 6267.81, average training loss: 6456.40, base loss: 20516.98
[INFO 2017-06-26 12:39:45,197 main.py:50] epoch 2213, training loss: 6196.85, average training loss: 6455.88, base loss: 20516.85
[INFO 2017-06-26 12:39:45,560 main.py:50] epoch 2214, training loss: 6179.80, average training loss: 6455.36, base loss: 20516.79
[INFO 2017-06-26 12:39:45,921 main.py:50] epoch 2215, training loss: 6347.65, average training loss: 6455.10, base loss: 20517.36
[INFO 2017-06-26 12:39:46,283 main.py:50] epoch 2216, training loss: 6165.85, average training loss: 6454.42, base loss: 20517.10
[INFO 2017-06-26 12:39:46,644 main.py:50] epoch 2217, training loss: 6305.52, average training loss: 6454.00, base loss: 20517.23
[INFO 2017-06-26 12:39:47,006 main.py:50] epoch 2218, training loss: 6226.47, average training loss: 6453.41, base loss: 20517.77
[INFO 2017-06-26 12:39:47,368 main.py:50] epoch 2219, training loss: 6313.61, average training loss: 6452.87, base loss: 20517.68
[INFO 2017-06-26 12:39:47,729 main.py:50] epoch 2220, training loss: 6234.21, average training loss: 6452.30, base loss: 20517.71
[INFO 2017-06-26 12:39:48,091 main.py:50] epoch 2221, training loss: 6262.48, average training loss: 6451.82, base loss: 20518.02
[INFO 2017-06-26 12:39:48,452 main.py:50] epoch 2222, training loss: 6265.80, average training loss: 6451.31, base loss: 20518.37
[INFO 2017-06-26 12:39:48,814 main.py:50] epoch 2223, training loss: 6302.10, average training loss: 6450.88, base loss: 20518.11
[INFO 2017-06-26 12:39:49,175 main.py:50] epoch 2224, training loss: 6242.29, average training loss: 6450.35, base loss: 20517.82
[INFO 2017-06-26 12:39:49,536 main.py:50] epoch 2225, training loss: 6352.44, average training loss: 6450.02, base loss: 20517.89
[INFO 2017-06-26 12:39:49,898 main.py:50] epoch 2226, training loss: 6121.69, average training loss: 6449.31, base loss: 20517.34
[INFO 2017-06-26 12:39:50,259 main.py:50] epoch 2227, training loss: 6195.07, average training loss: 6448.81, base loss: 20517.19
[INFO 2017-06-26 12:39:50,621 main.py:50] epoch 2228, training loss: 6237.17, average training loss: 6448.32, base loss: 20517.26
[INFO 2017-06-26 12:39:50,982 main.py:50] epoch 2229, training loss: 6254.97, average training loss: 6447.86, base loss: 20517.31
[INFO 2017-06-26 12:39:51,344 main.py:50] epoch 2230, training loss: 6197.22, average training loss: 6447.30, base loss: 20517.34
[INFO 2017-06-26 12:39:51,705 main.py:50] epoch 2231, training loss: 6260.95, average training loss: 6446.86, base loss: 20517.02
[INFO 2017-06-26 12:39:52,067 main.py:50] epoch 2232, training loss: 6268.31, average training loss: 6446.41, base loss: 20517.15
[INFO 2017-06-26 12:39:52,428 main.py:50] epoch 2233, training loss: 6257.25, average training loss: 6446.05, base loss: 20517.71
[INFO 2017-06-26 12:39:52,790 main.py:50] epoch 2234, training loss: 6164.88, average training loss: 6445.44, base loss: 20517.31
[INFO 2017-06-26 12:39:53,151 main.py:50] epoch 2235, training loss: 6156.60, average training loss: 6444.92, base loss: 20517.36
[INFO 2017-06-26 12:39:53,513 main.py:50] epoch 2236, training loss: 6205.72, average training loss: 6444.43, base loss: 20517.24
[INFO 2017-06-26 12:39:53,876 main.py:50] epoch 2237, training loss: 6228.03, average training loss: 6443.88, base loss: 20516.83
[INFO 2017-06-26 12:39:54,238 main.py:50] epoch 2238, training loss: 6231.61, average training loss: 6443.39, base loss: 20516.85
[INFO 2017-06-26 12:39:54,599 main.py:50] epoch 2239, training loss: 6254.77, average training loss: 6442.97, base loss: 20516.83
[INFO 2017-06-26 12:39:54,961 main.py:50] epoch 2240, training loss: 6229.22, average training loss: 6442.52, base loss: 20516.98
[INFO 2017-06-26 12:39:55,322 main.py:50] epoch 2241, training loss: 6274.97, average training loss: 6442.09, base loss: 20517.16
[INFO 2017-06-26 12:39:55,684 main.py:50] epoch 2242, training loss: 6224.56, average training loss: 6441.52, base loss: 20516.98
[INFO 2017-06-26 12:39:56,045 main.py:50] epoch 2243, training loss: 6190.80, average training loss: 6441.02, base loss: 20516.58
[INFO 2017-06-26 12:39:56,406 main.py:50] epoch 2244, training loss: 6262.00, average training loss: 6440.63, base loss: 20516.60
[INFO 2017-06-26 12:39:56,767 main.py:50] epoch 2245, training loss: 6179.70, average training loss: 6440.17, base loss: 20516.50
[INFO 2017-06-26 12:39:57,128 main.py:50] epoch 2246, training loss: 6178.72, average training loss: 6439.73, base loss: 20516.64
[INFO 2017-06-26 12:39:57,489 main.py:50] epoch 2247, training loss: 6218.26, average training loss: 6439.22, base loss: 20516.54
[INFO 2017-06-26 12:39:57,850 main.py:50] epoch 2248, training loss: 6267.80, average training loss: 6438.77, base loss: 20516.22
[INFO 2017-06-26 12:39:58,212 main.py:50] epoch 2249, training loss: 6190.01, average training loss: 6438.22, base loss: 20516.35
[INFO 2017-06-26 12:39:58,573 main.py:50] epoch 2250, training loss: 6295.46, average training loss: 6437.80, base loss: 20516.48
[INFO 2017-06-26 12:39:58,934 main.py:50] epoch 2251, training loss: 6206.56, average training loss: 6437.31, base loss: 20516.32
[INFO 2017-06-26 12:39:59,295 main.py:50] epoch 2252, training loss: 6199.02, average training loss: 6436.79, base loss: 20516.12
[INFO 2017-06-26 12:39:59,655 main.py:50] epoch 2253, training loss: 6266.37, average training loss: 6436.41, base loss: 20516.38
[INFO 2017-06-26 12:40:00,019 main.py:50] epoch 2254, training loss: 6164.14, average training loss: 6435.88, base loss: 20516.66
[INFO 2017-06-26 12:40:00,381 main.py:50] epoch 2255, training loss: 6196.21, average training loss: 6435.34, base loss: 20516.80
[INFO 2017-06-26 12:40:00,741 main.py:50] epoch 2256, training loss: 6240.59, average training loss: 6434.80, base loss: 20517.03
[INFO 2017-06-26 12:40:01,102 main.py:50] epoch 2257, training loss: 6167.08, average training loss: 6434.27, base loss: 20517.14
[INFO 2017-06-26 12:40:01,464 main.py:50] epoch 2258, training loss: 6220.81, average training loss: 6433.85, base loss: 20517.38
[INFO 2017-06-26 12:40:01,839 main.py:50] epoch 2259, training loss: 6165.98, average training loss: 6433.39, base loss: 20517.67
[INFO 2017-06-26 12:40:02,201 main.py:50] epoch 2260, training loss: 6175.32, average training loss: 6432.80, base loss: 20517.42
[INFO 2017-06-26 12:40:02,562 main.py:50] epoch 2261, training loss: 6183.89, average training loss: 6432.25, base loss: 20517.01
[INFO 2017-06-26 12:40:02,924 main.py:50] epoch 2262, training loss: 6121.06, average training loss: 6431.65, base loss: 20516.94
[INFO 2017-06-26 12:40:03,286 main.py:50] epoch 2263, training loss: 6163.54, average training loss: 6431.10, base loss: 20517.02
[INFO 2017-06-26 12:40:03,647 main.py:50] epoch 2264, training loss: 6199.58, average training loss: 6430.58, base loss: 20517.57
[INFO 2017-06-26 12:40:04,009 main.py:50] epoch 2265, training loss: 6140.10, average training loss: 6429.94, base loss: 20517.28
[INFO 2017-06-26 12:40:04,370 main.py:50] epoch 2266, training loss: 6183.23, average training loss: 6429.46, base loss: 20517.48
[INFO 2017-06-26 12:40:04,732 main.py:50] epoch 2267, training loss: 6175.54, average training loss: 6428.90, base loss: 20517.25
[INFO 2017-06-26 12:40:05,094 main.py:50] epoch 2268, training loss: 6197.84, average training loss: 6428.43, base loss: 20517.41
[INFO 2017-06-26 12:40:05,455 main.py:50] epoch 2269, training loss: 6237.24, average training loss: 6427.96, base loss: 20517.60
[INFO 2017-06-26 12:40:05,817 main.py:50] epoch 2270, training loss: 6238.66, average training loss: 6427.51, base loss: 20518.02
[INFO 2017-06-26 12:40:06,178 main.py:50] epoch 2271, training loss: 6176.91, average training loss: 6427.00, base loss: 20518.36
[INFO 2017-06-26 12:40:06,539 main.py:50] epoch 2272, training loss: 6169.96, average training loss: 6426.49, base loss: 20518.51
[INFO 2017-06-26 12:40:06,901 main.py:50] epoch 2273, training loss: 6188.39, average training loss: 6426.10, base loss: 20518.83
[INFO 2017-06-26 12:40:07,263 main.py:50] epoch 2274, training loss: 6207.37, average training loss: 6425.60, base loss: 20519.18
[INFO 2017-06-26 12:40:07,624 main.py:50] epoch 2275, training loss: 6198.36, average training loss: 6425.23, base loss: 20519.47
[INFO 2017-06-26 12:40:07,986 main.py:50] epoch 2276, training loss: 6115.48, average training loss: 6424.69, base loss: 20519.39
[INFO 2017-06-26 12:40:08,347 main.py:50] epoch 2277, training loss: 6156.78, average training loss: 6424.00, base loss: 20518.45
[INFO 2017-06-26 12:40:08,709 main.py:50] epoch 2278, training loss: 6282.77, average training loss: 6423.67, base loss: 20518.59
[INFO 2017-06-26 12:40:09,071 main.py:50] epoch 2279, training loss: 6201.22, average training loss: 6423.30, base loss: 20518.75
[INFO 2017-06-26 12:40:09,432 main.py:50] epoch 2280, training loss: 6206.66, average training loss: 6422.76, base loss: 20518.10
[INFO 2017-06-26 12:40:09,794 main.py:50] epoch 2281, training loss: 6286.99, average training loss: 6422.32, base loss: 20518.20
[INFO 2017-06-26 12:40:10,155 main.py:50] epoch 2282, training loss: 6163.62, average training loss: 6421.74, base loss: 20517.95
[INFO 2017-06-26 12:40:10,516 main.py:50] epoch 2283, training loss: 6251.87, average training loss: 6421.31, base loss: 20517.50
[INFO 2017-06-26 12:40:10,877 main.py:50] epoch 2284, training loss: 6196.34, average training loss: 6420.90, base loss: 20517.56
[INFO 2017-06-26 12:40:11,237 main.py:50] epoch 2285, training loss: 6185.16, average training loss: 6420.44, base loss: 20517.68
[INFO 2017-06-26 12:40:11,599 main.py:50] epoch 2286, training loss: 6208.89, average training loss: 6419.88, base loss: 20517.64
[INFO 2017-06-26 12:40:11,960 main.py:50] epoch 2287, training loss: 6277.61, average training loss: 6419.43, base loss: 20517.65
[INFO 2017-06-26 12:40:12,322 main.py:50] epoch 2288, training loss: 6234.91, average training loss: 6419.02, base loss: 20517.58
[INFO 2017-06-26 12:40:12,683 main.py:50] epoch 2289, training loss: 6258.00, average training loss: 6418.57, base loss: 20517.64
[INFO 2017-06-26 12:40:13,045 main.py:50] epoch 2290, training loss: 6161.10, average training loss: 6418.09, base loss: 20517.83
[INFO 2017-06-26 12:40:13,407 main.py:50] epoch 2291, training loss: 6150.32, average training loss: 6417.56, base loss: 20517.80
[INFO 2017-06-26 12:40:13,766 main.py:50] epoch 2292, training loss: 6203.82, average training loss: 6417.14, base loss: 20518.05
[INFO 2017-06-26 12:40:14,127 main.py:50] epoch 2293, training loss: 6206.96, average training loss: 6416.60, base loss: 20518.04
[INFO 2017-06-26 12:40:14,488 main.py:50] epoch 2294, training loss: 6252.50, average training loss: 6416.24, base loss: 20518.44
[INFO 2017-06-26 12:40:14,850 main.py:50] epoch 2295, training loss: 6145.39, average training loss: 6415.83, base loss: 20518.44
[INFO 2017-06-26 12:40:15,212 main.py:50] epoch 2296, training loss: 6209.88, average training loss: 6415.41, base loss: 20518.48
[INFO 2017-06-26 12:40:15,573 main.py:50] epoch 2297, training loss: 6188.84, average training loss: 6414.92, base loss: 20518.71
[INFO 2017-06-26 12:40:15,935 main.py:50] epoch 2298, training loss: 6168.91, average training loss: 6414.35, base loss: 20518.63
[INFO 2017-06-26 12:40:16,297 main.py:50] epoch 2299, training loss: 6213.17, average training loss: 6413.85, base loss: 20518.53
[INFO 2017-06-26 12:40:16,297 main.py:52] epoch 2299, testing
[INFO 2017-06-26 12:40:17,781 main.py:103] average testing loss: 6205.67, base loss: 20472.88
[INFO 2017-06-26 12:40:17,782 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 12:40:17,788 main.py:76] current best accuracy: 6205.67
[INFO 2017-06-26 12:40:18,146 main.py:50] epoch 2300, training loss: 6197.85, average training loss: 6413.42, base loss: 20518.40
[INFO 2017-06-26 12:40:18,506 main.py:50] epoch 2301, training loss: 6279.29, average training loss: 6412.98, base loss: 20518.60
[INFO 2017-06-26 12:40:18,865 main.py:50] epoch 2302, training loss: 6253.31, average training loss: 6412.49, base loss: 20518.70
[INFO 2017-06-26 12:40:19,223 main.py:50] epoch 2303, training loss: 6224.15, average training loss: 6411.92, base loss: 20518.50
[INFO 2017-06-26 12:40:19,582 main.py:50] epoch 2304, training loss: 6189.94, average training loss: 6411.35, base loss: 20518.35
[INFO 2017-06-26 12:40:19,942 main.py:50] epoch 2305, training loss: 6216.24, average training loss: 6410.85, base loss: 20518.47
[INFO 2017-06-26 12:40:20,302 main.py:50] epoch 2306, training loss: 6282.24, average training loss: 6410.50, base loss: 20519.04
[INFO 2017-06-26 12:40:20,660 main.py:50] epoch 2307, training loss: 6251.83, average training loss: 6410.07, base loss: 20518.89
[INFO 2017-06-26 12:40:21,020 main.py:50] epoch 2308, training loss: 6211.68, average training loss: 6409.58, base loss: 20518.72
[INFO 2017-06-26 12:40:21,380 main.py:50] epoch 2309, training loss: 6232.03, average training loss: 6409.10, base loss: 20518.31
[INFO 2017-06-26 12:40:21,739 main.py:50] epoch 2310, training loss: 6268.00, average training loss: 6408.66, base loss: 20518.25
[INFO 2017-06-26 12:40:22,096 main.py:50] epoch 2311, training loss: 6171.33, average training loss: 6408.17, base loss: 20518.23
[INFO 2017-06-26 12:40:22,455 main.py:50] epoch 2312, training loss: 6369.06, average training loss: 6407.76, base loss: 20518.24
[INFO 2017-06-26 12:40:22,815 main.py:50] epoch 2313, training loss: 6235.42, average training loss: 6407.32, base loss: 20518.65
[INFO 2017-06-26 12:40:23,174 main.py:50] epoch 2314, training loss: 6192.66, average training loss: 6406.87, base loss: 20518.38
[INFO 2017-06-26 12:40:23,532 main.py:50] epoch 2315, training loss: 6131.42, average training loss: 6406.34, base loss: 20518.16
[INFO 2017-06-26 12:40:23,891 main.py:50] epoch 2316, training loss: 6197.27, average training loss: 6405.86, base loss: 20518.29
[INFO 2017-06-26 12:40:24,249 main.py:50] epoch 2317, training loss: 6203.85, average training loss: 6405.37, base loss: 20518.07
[INFO 2017-06-26 12:40:24,606 main.py:50] epoch 2318, training loss: 6157.88, average training loss: 6404.90, base loss: 20517.95
[INFO 2017-06-26 12:40:24,966 main.py:50] epoch 2319, training loss: 6215.75, average training loss: 6404.35, base loss: 20517.52
[INFO 2017-06-26 12:40:25,323 main.py:50] epoch 2320, training loss: 6241.99, average training loss: 6403.91, base loss: 20517.28
[INFO 2017-06-26 12:40:25,681 main.py:50] epoch 2321, training loss: 6223.89, average training loss: 6403.48, base loss: 20517.28
[INFO 2017-06-26 12:40:26,040 main.py:50] epoch 2322, training loss: 6229.76, average training loss: 6403.01, base loss: 20517.27
[INFO 2017-06-26 12:40:26,397 main.py:50] epoch 2323, training loss: 6224.94, average training loss: 6402.62, base loss: 20517.02
[INFO 2017-06-26 12:40:26,757 main.py:50] epoch 2324, training loss: 6178.51, average training loss: 6402.23, base loss: 20517.09
[INFO 2017-06-26 12:40:27,116 main.py:50] epoch 2325, training loss: 6110.85, average training loss: 6401.75, base loss: 20517.06
[INFO 2017-06-26 12:40:27,475 main.py:50] epoch 2326, training loss: 6220.19, average training loss: 6401.27, base loss: 20516.60
[INFO 2017-06-26 12:40:27,835 main.py:50] epoch 2327, training loss: 6111.86, average training loss: 6400.80, base loss: 20516.61
[INFO 2017-06-26 12:40:28,193 main.py:50] epoch 2328, training loss: 6233.15, average training loss: 6400.29, base loss: 20516.41
[INFO 2017-06-26 12:40:28,551 main.py:50] epoch 2329, training loss: 6214.59, average training loss: 6399.83, base loss: 20516.17
[INFO 2017-06-26 12:40:28,910 main.py:50] epoch 2330, training loss: 6168.12, average training loss: 6399.40, base loss: 20516.58
[INFO 2017-06-26 12:40:29,268 main.py:50] epoch 2331, training loss: 6282.01, average training loss: 6399.11, base loss: 20517.10
[INFO 2017-06-26 12:40:29,629 main.py:50] epoch 2332, training loss: 6184.53, average training loss: 6398.68, base loss: 20517.27
[INFO 2017-06-26 12:40:29,988 main.py:50] epoch 2333, training loss: 6248.65, average training loss: 6398.28, base loss: 20517.41
[INFO 2017-06-26 12:40:30,345 main.py:50] epoch 2334, training loss: 6202.61, average training loss: 6397.84, base loss: 20517.79
[INFO 2017-06-26 12:40:30,704 main.py:50] epoch 2335, training loss: 6261.39, average training loss: 6397.46, base loss: 20517.77
[INFO 2017-06-26 12:40:31,063 main.py:50] epoch 2336, training loss: 6225.93, average training loss: 6397.02, base loss: 20517.82
[INFO 2017-06-26 12:40:31,421 main.py:50] epoch 2337, training loss: 6196.39, average training loss: 6396.61, base loss: 20517.50
[INFO 2017-06-26 12:40:31,780 main.py:50] epoch 2338, training loss: 6285.35, average training loss: 6396.30, base loss: 20517.84
[INFO 2017-06-26 12:40:32,138 main.py:50] epoch 2339, training loss: 6179.03, average training loss: 6395.95, base loss: 20517.65
[INFO 2017-06-26 12:40:32,499 main.py:50] epoch 2340, training loss: 6250.35, average training loss: 6395.49, base loss: 20517.42
[INFO 2017-06-26 12:40:32,858 main.py:50] epoch 2341, training loss: 6254.10, average training loss: 6395.13, base loss: 20517.51
[INFO 2017-06-26 12:40:33,217 main.py:50] epoch 2342, training loss: 6234.85, average training loss: 6394.81, base loss: 20517.63
[INFO 2017-06-26 12:40:33,575 main.py:50] epoch 2343, training loss: 6118.06, average training loss: 6394.27, base loss: 20517.53
[INFO 2017-06-26 12:40:33,934 main.py:50] epoch 2344, training loss: 6278.49, average training loss: 6393.88, base loss: 20517.96
[INFO 2017-06-26 12:40:34,293 main.py:50] epoch 2345, training loss: 6159.11, average training loss: 6393.34, base loss: 20518.03
[INFO 2017-06-26 12:40:34,652 main.py:50] epoch 2346, training loss: 6178.06, average training loss: 6392.86, base loss: 20517.87
[INFO 2017-06-26 12:40:35,012 main.py:50] epoch 2347, training loss: 6168.51, average training loss: 6392.41, base loss: 20517.81
[INFO 2017-06-26 12:40:35,369 main.py:50] epoch 2348, training loss: 6204.84, average training loss: 6391.89, base loss: 20517.52
[INFO 2017-06-26 12:40:35,729 main.py:50] epoch 2349, training loss: 6249.56, average training loss: 6391.47, base loss: 20518.10
[INFO 2017-06-26 12:40:36,087 main.py:50] epoch 2350, training loss: 6154.37, average training loss: 6390.93, base loss: 20517.93
[INFO 2017-06-26 12:40:36,446 main.py:50] epoch 2351, training loss: 6213.77, average training loss: 6390.49, base loss: 20518.19
[INFO 2017-06-26 12:40:36,805 main.py:50] epoch 2352, training loss: 6125.69, average training loss: 6389.98, base loss: 20518.00
[INFO 2017-06-26 12:40:37,165 main.py:50] epoch 2353, training loss: 6145.26, average training loss: 6389.47, base loss: 20518.17
[INFO 2017-06-26 12:40:37,525 main.py:50] epoch 2354, training loss: 6206.27, average training loss: 6389.11, base loss: 20518.72
[INFO 2017-06-26 12:40:37,883 main.py:50] epoch 2355, training loss: 6184.74, average training loss: 6388.62, base loss: 20518.42
[INFO 2017-06-26 12:40:38,240 main.py:50] epoch 2356, training loss: 6122.60, average training loss: 6388.11, base loss: 20518.34
[INFO 2017-06-26 12:40:38,601 main.py:50] epoch 2357, training loss: 6175.45, average training loss: 6387.54, base loss: 20518.12
[INFO 2017-06-26 12:40:38,960 main.py:50] epoch 2358, training loss: 6145.54, average training loss: 6387.01, base loss: 20517.68
[INFO 2017-06-26 12:40:39,319 main.py:50] epoch 2359, training loss: 6130.95, average training loss: 6386.42, base loss: 20517.34
[INFO 2017-06-26 12:40:39,678 main.py:50] epoch 2360, training loss: 6169.10, average training loss: 6385.91, base loss: 20517.45
[INFO 2017-06-26 12:40:40,037 main.py:50] epoch 2361, training loss: 6087.58, average training loss: 6385.34, base loss: 20517.12
[INFO 2017-06-26 12:40:40,398 main.py:50] epoch 2362, training loss: 6181.25, average training loss: 6384.93, base loss: 20517.35
[INFO 2017-06-26 12:40:40,756 main.py:50] epoch 2363, training loss: 6146.82, average training loss: 6384.43, base loss: 20517.09
[INFO 2017-06-26 12:40:41,114 main.py:50] epoch 2364, training loss: 6149.55, average training loss: 6383.95, base loss: 20517.12
[INFO 2017-06-26 12:40:41,472 main.py:50] epoch 2365, training loss: 6084.92, average training loss: 6383.38, base loss: 20517.00
[INFO 2017-06-26 12:40:41,831 main.py:50] epoch 2366, training loss: 6092.98, average training loss: 6382.87, base loss: 20517.11
[INFO 2017-06-26 12:40:42,192 main.py:50] epoch 2367, training loss: 6089.87, average training loss: 6382.43, base loss: 20517.23
[INFO 2017-06-26 12:40:42,552 main.py:50] epoch 2368, training loss: 6137.01, average training loss: 6381.88, base loss: 20517.32
[INFO 2017-06-26 12:40:42,909 main.py:50] epoch 2369, training loss: 6120.83, average training loss: 6381.48, base loss: 20517.58
[INFO 2017-06-26 12:40:43,269 main.py:50] epoch 2370, training loss: 6179.12, average training loss: 6381.01, base loss: 20517.42
[INFO 2017-06-26 12:40:43,628 main.py:50] epoch 2371, training loss: 6143.41, average training loss: 6380.54, base loss: 20517.62
[INFO 2017-06-26 12:40:43,988 main.py:50] epoch 2372, training loss: 6158.06, average training loss: 6380.01, base loss: 20517.44
[INFO 2017-06-26 12:40:44,347 main.py:50] epoch 2373, training loss: 6157.79, average training loss: 6379.57, base loss: 20517.05
[INFO 2017-06-26 12:40:44,707 main.py:50] epoch 2374, training loss: 6189.65, average training loss: 6379.17, base loss: 20516.80
[INFO 2017-06-26 12:40:45,066 main.py:50] epoch 2375, training loss: 6077.98, average training loss: 6378.52, base loss: 20516.38
[INFO 2017-06-26 12:40:45,424 main.py:50] epoch 2376, training loss: 6141.52, average training loss: 6378.02, base loss: 20515.86
[INFO 2017-06-26 12:40:45,783 main.py:50] epoch 2377, training loss: 6188.83, average training loss: 6377.60, base loss: 20516.06
[INFO 2017-06-26 12:40:46,156 main.py:50] epoch 2378, training loss: 6065.47, average training loss: 6376.99, base loss: 20516.00
[INFO 2017-06-26 12:40:46,515 main.py:50] epoch 2379, training loss: 6160.37, average training loss: 6376.56, base loss: 20515.64
[INFO 2017-06-26 12:40:46,875 main.py:50] epoch 2380, training loss: 6166.14, average training loss: 6376.06, base loss: 20515.56
[INFO 2017-06-26 12:40:47,234 main.py:50] epoch 2381, training loss: 6287.82, average training loss: 6375.73, base loss: 20515.88
[INFO 2017-06-26 12:40:47,593 main.py:50] epoch 2382, training loss: 6210.08, average training loss: 6375.22, base loss: 20516.28
[INFO 2017-06-26 12:40:47,952 main.py:50] epoch 2383, training loss: 6148.51, average training loss: 6374.69, base loss: 20516.08
[INFO 2017-06-26 12:40:48,311 main.py:50] epoch 2384, training loss: 6149.50, average training loss: 6374.24, base loss: 20516.24
[INFO 2017-06-26 12:40:48,670 main.py:50] epoch 2385, training loss: 6140.88, average training loss: 6373.70, base loss: 20515.89
[INFO 2017-06-26 12:40:49,029 main.py:50] epoch 2386, training loss: 6142.69, average training loss: 6373.25, base loss: 20515.83
[INFO 2017-06-26 12:40:49,390 main.py:50] epoch 2387, training loss: 6133.99, average training loss: 6372.80, base loss: 20515.70
[INFO 2017-06-26 12:40:49,747 main.py:50] epoch 2388, training loss: 6199.57, average training loss: 6372.43, base loss: 20515.67
[INFO 2017-06-26 12:40:50,106 main.py:50] epoch 2389, training loss: 6100.53, average training loss: 6371.88, base loss: 20515.14
[INFO 2017-06-26 12:40:50,467 main.py:50] epoch 2390, training loss: 6181.23, average training loss: 6371.42, base loss: 20515.07
[INFO 2017-06-26 12:40:50,824 main.py:50] epoch 2391, training loss: 6135.59, average training loss: 6370.85, base loss: 20514.88
[INFO 2017-06-26 12:40:51,184 main.py:50] epoch 2392, training loss: 6139.97, average training loss: 6370.50, base loss: 20514.88
[INFO 2017-06-26 12:40:51,542 main.py:50] epoch 2393, training loss: 6099.82, average training loss: 6370.05, base loss: 20514.89
[INFO 2017-06-26 12:40:51,901 main.py:50] epoch 2394, training loss: 6139.62, average training loss: 6369.59, base loss: 20514.60
[INFO 2017-06-26 12:40:52,261 main.py:50] epoch 2395, training loss: 6162.58, average training loss: 6369.15, base loss: 20514.35
[INFO 2017-06-26 12:40:52,620 main.py:50] epoch 2396, training loss: 6240.28, average training loss: 6368.79, base loss: 20514.55
[INFO 2017-06-26 12:40:52,980 main.py:50] epoch 2397, training loss: 6158.60, average training loss: 6368.45, base loss: 20514.90
[INFO 2017-06-26 12:40:53,341 main.py:50] epoch 2398, training loss: 6170.78, average training loss: 6368.04, base loss: 20515.14
[INFO 2017-06-26 12:40:53,699 main.py:50] epoch 2399, training loss: 6207.08, average training loss: 6367.62, base loss: 20515.47
[INFO 2017-06-26 12:40:53,700 main.py:52] epoch 2399, testing
[INFO 2017-06-26 12:40:55,165 main.py:103] average testing loss: 6162.43, base loss: 20530.21
[INFO 2017-06-26 12:40:55,166 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 12:40:55,172 main.py:76] current best accuracy: 6162.43
[INFO 2017-06-26 12:40:55,532 main.py:50] epoch 2400, training loss: 6144.56, average training loss: 6367.19, base loss: 20515.86
[INFO 2017-06-26 12:40:55,889 main.py:50] epoch 2401, training loss: 6275.06, average training loss: 6366.90, base loss: 20516.44
[INFO 2017-06-26 12:40:56,250 main.py:50] epoch 2402, training loss: 6203.26, average training loss: 6366.42, base loss: 20516.43
[INFO 2017-06-26 12:40:56,607 main.py:50] epoch 2403, training loss: 6243.96, average training loss: 6366.11, base loss: 20516.39
[INFO 2017-06-26 12:40:56,966 main.py:50] epoch 2404, training loss: 6219.41, average training loss: 6365.72, base loss: 20516.55
[INFO 2017-06-26 12:40:57,325 main.py:50] epoch 2405, training loss: 6166.79, average training loss: 6365.34, base loss: 20516.65
[INFO 2017-06-26 12:40:57,683 main.py:50] epoch 2406, training loss: 6168.18, average training loss: 6364.82, base loss: 20516.70
[INFO 2017-06-26 12:40:58,042 main.py:50] epoch 2407, training loss: 6141.60, average training loss: 6364.23, base loss: 20516.22
[INFO 2017-06-26 12:40:58,400 main.py:50] epoch 2408, training loss: 6218.62, average training loss: 6363.88, base loss: 20516.47
[INFO 2017-06-26 12:40:58,759 main.py:50] epoch 2409, training loss: 6146.16, average training loss: 6363.38, base loss: 20516.30
[INFO 2017-06-26 12:40:59,118 main.py:50] epoch 2410, training loss: 6191.10, average training loss: 6362.97, base loss: 20516.37
[INFO 2017-06-26 12:40:59,477 main.py:50] epoch 2411, training loss: 6205.25, average training loss: 6362.54, base loss: 20516.67
[INFO 2017-06-26 12:40:59,836 main.py:50] epoch 2412, training loss: 6173.60, average training loss: 6362.16, base loss: 20516.40
[INFO 2017-06-26 12:41:00,195 main.py:50] epoch 2413, training loss: 6181.55, average training loss: 6361.81, base loss: 20516.26
[INFO 2017-06-26 12:41:00,555 main.py:50] epoch 2414, training loss: 6238.60, average training loss: 6361.42, base loss: 20516.42
[INFO 2017-06-26 12:41:00,913 main.py:50] epoch 2415, training loss: 6170.29, average training loss: 6360.97, base loss: 20516.51
[INFO 2017-06-26 12:41:01,272 main.py:50] epoch 2416, training loss: 6207.66, average training loss: 6360.65, base loss: 20516.55
[INFO 2017-06-26 12:41:01,631 main.py:50] epoch 2417, training loss: 6247.55, average training loss: 6360.27, base loss: 20516.88
[INFO 2017-06-26 12:41:01,990 main.py:50] epoch 2418, training loss: 6183.89, average training loss: 6359.76, base loss: 20517.06
[INFO 2017-06-26 12:41:02,349 main.py:50] epoch 2419, training loss: 6132.57, average training loss: 6359.34, base loss: 20517.07
[INFO 2017-06-26 12:41:02,708 main.py:50] epoch 2420, training loss: 6120.05, average training loss: 6358.79, base loss: 20517.26
[INFO 2017-06-26 12:41:03,067 main.py:50] epoch 2421, training loss: 6121.63, average training loss: 6358.30, base loss: 20516.90
[INFO 2017-06-26 12:41:03,426 main.py:50] epoch 2422, training loss: 6127.91, average training loss: 6357.77, base loss: 20516.55
[INFO 2017-06-26 12:41:03,785 main.py:50] epoch 2423, training loss: 6158.92, average training loss: 6357.20, base loss: 20516.15
[INFO 2017-06-26 12:41:04,143 main.py:50] epoch 2424, training loss: 6191.12, average training loss: 6356.83, base loss: 20516.39
[INFO 2017-06-26 12:41:04,503 main.py:50] epoch 2425, training loss: 6127.59, average training loss: 6356.36, base loss: 20516.49
[INFO 2017-06-26 12:41:04,862 main.py:50] epoch 2426, training loss: 6119.24, average training loss: 6355.87, base loss: 20516.52
[INFO 2017-06-26 12:41:05,221 main.py:50] epoch 2427, training loss: 6155.25, average training loss: 6355.35, base loss: 20516.41
[INFO 2017-06-26 12:41:05,582 main.py:50] epoch 2428, training loss: 6171.47, average training loss: 6354.87, base loss: 20515.84
[INFO 2017-06-26 12:41:05,941 main.py:50] epoch 2429, training loss: 6120.87, average training loss: 6354.44, base loss: 20515.65
[INFO 2017-06-26 12:41:06,300 main.py:50] epoch 2430, training loss: 6124.09, average training loss: 6353.96, base loss: 20515.58
[INFO 2017-06-26 12:41:06,659 main.py:50] epoch 2431, training loss: 6146.29, average training loss: 6353.49, base loss: 20515.72
[INFO 2017-06-26 12:41:07,017 main.py:50] epoch 2432, training loss: 6133.18, average training loss: 6353.03, base loss: 20515.83
[INFO 2017-06-26 12:41:07,377 main.py:50] epoch 2433, training loss: 6168.53, average training loss: 6352.61, base loss: 20516.13
[INFO 2017-06-26 12:41:07,736 main.py:50] epoch 2434, training loss: 6087.65, average training loss: 6352.10, base loss: 20516.34
[INFO 2017-06-26 12:41:08,095 main.py:50] epoch 2435, training loss: 6142.38, average training loss: 6351.66, base loss: 20516.08
[INFO 2017-06-26 12:41:08,454 main.py:50] epoch 2436, training loss: 6085.55, average training loss: 6351.15, base loss: 20516.17
[INFO 2017-06-26 12:41:08,813 main.py:50] epoch 2437, training loss: 6113.51, average training loss: 6350.68, base loss: 20516.01
[INFO 2017-06-26 12:41:09,171 main.py:50] epoch 2438, training loss: 6177.46, average training loss: 6350.20, base loss: 20516.38
[INFO 2017-06-26 12:41:09,532 main.py:50] epoch 2439, training loss: 6090.78, average training loss: 6349.82, base loss: 20516.48
[INFO 2017-06-26 12:41:09,891 main.py:50] epoch 2440, training loss: 6099.71, average training loss: 6349.28, base loss: 20516.31
[INFO 2017-06-26 12:41:10,250 main.py:50] epoch 2441, training loss: 6106.40, average training loss: 6348.83, base loss: 20516.37
[INFO 2017-06-26 12:41:10,609 main.py:50] epoch 2442, training loss: 6053.89, average training loss: 6348.28, base loss: 20516.22
[INFO 2017-06-26 12:41:10,968 main.py:50] epoch 2443, training loss: 6125.93, average training loss: 6347.76, base loss: 20516.18
[INFO 2017-06-26 12:41:11,327 main.py:50] epoch 2444, training loss: 6130.80, average training loss: 6347.25, base loss: 20516.49
[INFO 2017-06-26 12:41:11,685 main.py:50] epoch 2445, training loss: 6017.16, average training loss: 6346.64, base loss: 20516.20
[INFO 2017-06-26 12:41:12,045 main.py:50] epoch 2446, training loss: 6183.47, average training loss: 6346.18, base loss: 20515.90
[INFO 2017-06-26 12:41:12,404 main.py:50] epoch 2447, training loss: 6119.08, average training loss: 6345.76, base loss: 20515.60
[INFO 2017-06-26 12:41:12,763 main.py:50] epoch 2448, training loss: 6049.61, average training loss: 6345.14, base loss: 20514.82
[INFO 2017-06-26 12:41:13,120 main.py:50] epoch 2449, training loss: 6182.78, average training loss: 6344.81, base loss: 20514.95
[INFO 2017-06-26 12:41:13,480 main.py:50] epoch 2450, training loss: 6112.47, average training loss: 6344.33, base loss: 20514.99
[INFO 2017-06-26 12:41:13,840 main.py:50] epoch 2451, training loss: 6135.41, average training loss: 6343.86, base loss: 20514.85
[INFO 2017-06-26 12:41:14,199 main.py:50] epoch 2452, training loss: 6197.29, average training loss: 6343.51, base loss: 20514.83
[INFO 2017-06-26 12:41:14,558 main.py:50] epoch 2453, training loss: 6071.72, average training loss: 6342.99, base loss: 20514.23
[INFO 2017-06-26 12:41:14,916 main.py:50] epoch 2454, training loss: 6129.11, average training loss: 6342.53, base loss: 20513.88
[INFO 2017-06-26 12:41:15,275 main.py:50] epoch 2455, training loss: 6149.01, average training loss: 6342.18, base loss: 20514.26
[INFO 2017-06-26 12:41:15,635 main.py:50] epoch 2456, training loss: 6136.50, average training loss: 6341.75, base loss: 20514.32
[INFO 2017-06-26 12:41:15,994 main.py:50] epoch 2457, training loss: 6163.74, average training loss: 6341.34, base loss: 20514.67
[INFO 2017-06-26 12:41:16,353 main.py:50] epoch 2458, training loss: 6122.40, average training loss: 6340.90, base loss: 20514.42
[INFO 2017-06-26 12:41:16,712 main.py:50] epoch 2459, training loss: 6121.41, average training loss: 6340.46, base loss: 20514.22
[INFO 2017-06-26 12:41:17,071 main.py:50] epoch 2460, training loss: 6127.87, average training loss: 6340.05, base loss: 20514.25
[INFO 2017-06-26 12:41:17,430 main.py:50] epoch 2461, training loss: 6088.75, average training loss: 6339.49, base loss: 20514.05
[INFO 2017-06-26 12:41:17,790 main.py:50] epoch 2462, training loss: 6096.58, average training loss: 6338.95, base loss: 20513.86
[INFO 2017-06-26 12:41:18,149 main.py:50] epoch 2463, training loss: 6174.70, average training loss: 6338.52, base loss: 20513.92
[INFO 2017-06-26 12:41:18,509 main.py:50] epoch 2464, training loss: 6201.62, average training loss: 6338.21, base loss: 20514.22
[INFO 2017-06-26 12:41:18,867 main.py:50] epoch 2465, training loss: 6129.60, average training loss: 6337.72, base loss: 20514.20
[INFO 2017-06-26 12:41:19,227 main.py:50] epoch 2466, training loss: 6106.94, average training loss: 6337.26, base loss: 20514.03
[INFO 2017-06-26 12:41:19,587 main.py:50] epoch 2467, training loss: 6059.18, average training loss: 6336.77, base loss: 20513.74
[INFO 2017-06-26 12:41:19,946 main.py:50] epoch 2468, training loss: 6108.01, average training loss: 6336.36, base loss: 20513.11
[INFO 2017-06-26 12:41:20,305 main.py:50] epoch 2469, training loss: 6172.64, average training loss: 6336.01, base loss: 20513.23
[INFO 2017-06-26 12:41:20,664 main.py:50] epoch 2470, training loss: 6103.47, average training loss: 6335.44, base loss: 20513.23
[INFO 2017-06-26 12:41:21,023 main.py:50] epoch 2471, training loss: 6146.90, average training loss: 6335.02, base loss: 20513.19
[INFO 2017-06-26 12:41:21,382 main.py:50] epoch 2472, training loss: 6118.53, average training loss: 6334.59, base loss: 20513.20
[INFO 2017-06-26 12:41:21,740 main.py:50] epoch 2473, training loss: 6140.19, average training loss: 6334.18, base loss: 20513.11
[INFO 2017-06-26 12:41:22,099 main.py:50] epoch 2474, training loss: 6179.41, average training loss: 6333.82, base loss: 20513.32
[INFO 2017-06-26 12:41:22,458 main.py:50] epoch 2475, training loss: 6079.74, average training loss: 6333.36, base loss: 20513.76
[INFO 2017-06-26 12:41:22,816 main.py:50] epoch 2476, training loss: 6146.28, average training loss: 6332.91, base loss: 20514.18
[INFO 2017-06-26 12:41:23,176 main.py:50] epoch 2477, training loss: 6147.24, average training loss: 6332.44, base loss: 20513.76
[INFO 2017-06-26 12:41:23,536 main.py:50] epoch 2478, training loss: 6113.40, average training loss: 6331.92, base loss: 20513.99
[INFO 2017-06-26 12:41:23,894 main.py:50] epoch 2479, training loss: 6157.53, average training loss: 6331.48, base loss: 20514.02
[INFO 2017-06-26 12:41:24,254 main.py:50] epoch 2480, training loss: 6211.11, average training loss: 6331.08, base loss: 20514.08
[INFO 2017-06-26 12:41:24,614 main.py:50] epoch 2481, training loss: 6201.39, average training loss: 6330.75, base loss: 20514.02
[INFO 2017-06-26 12:41:24,974 main.py:50] epoch 2482, training loss: 6091.56, average training loss: 6330.26, base loss: 20513.82
[INFO 2017-06-26 12:41:25,333 main.py:50] epoch 2483, training loss: 6197.21, average training loss: 6329.95, base loss: 20513.76
[INFO 2017-06-26 12:41:25,691 main.py:50] epoch 2484, training loss: 6175.92, average training loss: 6329.54, base loss: 20513.94
[INFO 2017-06-26 12:41:26,050 main.py:50] epoch 2485, training loss: 6103.62, average training loss: 6329.06, base loss: 20513.64
[INFO 2017-06-26 12:41:26,409 main.py:50] epoch 2486, training loss: 6238.18, average training loss: 6328.74, base loss: 20513.67
[INFO 2017-06-26 12:41:26,768 main.py:50] epoch 2487, training loss: 6130.97, average training loss: 6328.30, base loss: 20513.79
[INFO 2017-06-26 12:41:27,126 main.py:50] epoch 2488, training loss: 6156.37, average training loss: 6327.98, base loss: 20513.85
[INFO 2017-06-26 12:41:27,485 main.py:50] epoch 2489, training loss: 6121.37, average training loss: 6327.54, base loss: 20513.43
[INFO 2017-06-26 12:41:27,844 main.py:50] epoch 2490, training loss: 6071.20, average training loss: 6327.13, base loss: 20513.67
[INFO 2017-06-26 12:41:28,203 main.py:50] epoch 2491, training loss: 6084.86, average training loss: 6326.78, base loss: 20513.72
[INFO 2017-06-26 12:41:28,564 main.py:50] epoch 2492, training loss: 6039.49, average training loss: 6326.33, base loss: 20513.90
[INFO 2017-06-26 12:41:28,922 main.py:50] epoch 2493, training loss: 6082.08, average training loss: 6325.96, base loss: 20514.48
[INFO 2017-06-26 12:41:29,281 main.py:50] epoch 2494, training loss: 6157.20, average training loss: 6325.44, base loss: 20514.34
[INFO 2017-06-26 12:41:29,642 main.py:50] epoch 2495, training loss: 6094.99, average training loss: 6324.91, base loss: 20514.56
[INFO 2017-06-26 12:41:30,001 main.py:50] epoch 2496, training loss: 6040.45, average training loss: 6324.38, base loss: 20514.33
[INFO 2017-06-26 12:41:30,372 main.py:50] epoch 2497, training loss: 6187.54, average training loss: 6324.06, base loss: 20514.64
[INFO 2017-06-26 12:41:30,731 main.py:50] epoch 2498, training loss: 6159.58, average training loss: 6323.70, base loss: 20514.92
[INFO 2017-06-26 12:41:31,091 main.py:50] epoch 2499, training loss: 6109.51, average training loss: 6323.28, base loss: 20514.77
[INFO 2017-06-26 12:41:31,091 main.py:52] epoch 2499, testing
[INFO 2017-06-26 12:41:32,569 main.py:103] average testing loss: 6091.70, base loss: 20502.96
[INFO 2017-06-26 12:41:32,569 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 12:41:32,575 main.py:76] current best accuracy: 6091.70
[INFO 2017-06-26 12:41:32,933 main.py:50] epoch 2500, training loss: 6111.62, average training loss: 6322.88, base loss: 20514.79
[INFO 2017-06-26 12:41:33,293 main.py:50] epoch 2501, training loss: 6125.48, average training loss: 6322.47, base loss: 20514.78
[INFO 2017-06-26 12:41:33,652 main.py:50] epoch 2502, training loss: 6108.83, average training loss: 6322.09, base loss: 20514.49
[INFO 2017-06-26 12:41:34,011 main.py:50] epoch 2503, training loss: 6099.74, average training loss: 6321.68, base loss: 20514.32
[INFO 2017-06-26 12:41:34,372 main.py:50] epoch 2504, training loss: 6106.51, average training loss: 6321.17, base loss: 20514.38
[INFO 2017-06-26 12:41:34,731 main.py:50] epoch 2505, training loss: 6091.77, average training loss: 6320.68, base loss: 20514.21
[INFO 2017-06-26 12:41:35,089 main.py:50] epoch 2506, training loss: 6144.41, average training loss: 6320.34, base loss: 20514.18
[INFO 2017-06-26 12:41:35,450 main.py:50] epoch 2507, training loss: 6118.69, average training loss: 6319.94, base loss: 20513.81
[INFO 2017-06-26 12:41:35,809 main.py:50] epoch 2508, training loss: 6188.26, average training loss: 6319.66, base loss: 20514.10
[INFO 2017-06-26 12:41:36,169 main.py:50] epoch 2509, training loss: 6164.35, average training loss: 6319.34, base loss: 20514.23
[INFO 2017-06-26 12:41:36,529 main.py:50] epoch 2510, training loss: 6174.32, average training loss: 6318.99, base loss: 20514.33
[INFO 2017-06-26 12:41:36,887 main.py:50] epoch 2511, training loss: 6048.96, average training loss: 6318.49, base loss: 20514.00
[INFO 2017-06-26 12:41:37,247 main.py:50] epoch 2512, training loss: 6121.13, average training loss: 6318.12, base loss: 20514.01
[INFO 2017-06-26 12:41:37,607 main.py:50] epoch 2513, training loss: 6161.31, average training loss: 6317.75, base loss: 20514.27
[INFO 2017-06-26 12:41:37,968 main.py:50] epoch 2514, training loss: 6203.40, average training loss: 6317.51, base loss: 20514.64
[INFO 2017-06-26 12:41:38,325 main.py:50] epoch 2515, training loss: 6106.94, average training loss: 6317.10, base loss: 20514.56
[INFO 2017-06-26 12:41:38,684 main.py:50] epoch 2516, training loss: 6106.16, average training loss: 6316.68, base loss: 20514.47
[INFO 2017-06-26 12:41:39,042 main.py:50] epoch 2517, training loss: 6148.95, average training loss: 6316.31, base loss: 20514.36
[INFO 2017-06-26 12:41:39,402 main.py:50] epoch 2518, training loss: 6201.50, average training loss: 6316.04, base loss: 20514.51
[INFO 2017-06-26 12:41:39,760 main.py:50] epoch 2519, training loss: 6174.47, average training loss: 6315.74, base loss: 20514.86
[INFO 2017-06-26 12:41:40,119 main.py:50] epoch 2520, training loss: 6139.86, average training loss: 6315.40, base loss: 20515.09
[INFO 2017-06-26 12:41:40,479 main.py:50] epoch 2521, training loss: 6139.86, average training loss: 6315.06, base loss: 20515.52
[INFO 2017-06-26 12:41:40,837 main.py:50] epoch 2522, training loss: 6134.08, average training loss: 6314.62, base loss: 20515.81
[INFO 2017-06-26 12:41:41,196 main.py:50] epoch 2523, training loss: 6108.75, average training loss: 6314.14, base loss: 20515.92
[INFO 2017-06-26 12:41:41,556 main.py:50] epoch 2524, training loss: 6151.90, average training loss: 6313.64, base loss: 20515.63
[INFO 2017-06-26 12:41:41,914 main.py:50] epoch 2525, training loss: 6128.29, average training loss: 6313.16, base loss: 20515.49
[INFO 2017-06-26 12:41:42,272 main.py:50] epoch 2526, training loss: 6171.82, average training loss: 6312.79, base loss: 20515.72
[INFO 2017-06-26 12:41:42,631 main.py:50] epoch 2527, training loss: 6093.57, average training loss: 6312.25, base loss: 20515.38
[INFO 2017-06-26 12:41:42,990 main.py:50] epoch 2528, training loss: 6099.56, average training loss: 6311.86, base loss: 20515.30
[INFO 2017-06-26 12:41:43,350 main.py:50] epoch 2529, training loss: 6106.33, average training loss: 6311.41, base loss: 20515.53
[INFO 2017-06-26 12:41:43,708 main.py:50] epoch 2530, training loss: 6133.78, average training loss: 6311.00, base loss: 20515.54
[INFO 2017-06-26 12:41:44,069 main.py:50] epoch 2531, training loss: 6180.21, average training loss: 6310.59, base loss: 20515.54
[INFO 2017-06-26 12:41:44,428 main.py:50] epoch 2532, training loss: 6102.85, average training loss: 6310.15, base loss: 20515.28
[INFO 2017-06-26 12:41:44,786 main.py:50] epoch 2533, training loss: 6115.04, average training loss: 6309.74, base loss: 20515.49
[INFO 2017-06-26 12:41:45,145 main.py:50] epoch 2534, training loss: 6095.26, average training loss: 6309.26, base loss: 20515.79
[INFO 2017-06-26 12:41:45,505 main.py:50] epoch 2535, training loss: 6103.43, average training loss: 6308.81, base loss: 20515.43
[INFO 2017-06-26 12:41:45,865 main.py:50] epoch 2536, training loss: 6091.33, average training loss: 6308.33, base loss: 20515.40
[INFO 2017-06-26 12:41:46,225 main.py:50] epoch 2537, training loss: 6102.97, average training loss: 6307.94, base loss: 20515.33
[INFO 2017-06-26 12:41:46,584 main.py:50] epoch 2538, training loss: 6099.49, average training loss: 6307.44, base loss: 20515.31
[INFO 2017-06-26 12:41:46,941 main.py:50] epoch 2539, training loss: 6082.15, average training loss: 6307.04, base loss: 20515.67
[INFO 2017-06-26 12:41:47,301 main.py:50] epoch 2540, training loss: 6116.92, average training loss: 6306.61, base loss: 20515.78
[INFO 2017-06-26 12:41:47,659 main.py:50] epoch 2541, training loss: 6146.35, average training loss: 6306.27, base loss: 20516.09
[INFO 2017-06-26 12:41:48,018 main.py:50] epoch 2542, training loss: 6033.27, average training loss: 6305.77, base loss: 20515.42
[INFO 2017-06-26 12:41:48,378 main.py:50] epoch 2543, training loss: 6050.84, average training loss: 6305.26, base loss: 20515.09
[INFO 2017-06-26 12:41:48,737 main.py:50] epoch 2544, training loss: 6103.23, average training loss: 6304.93, base loss: 20514.87
[INFO 2017-06-26 12:41:49,096 main.py:50] epoch 2545, training loss: 6132.29, average training loss: 6304.55, base loss: 20514.90
[INFO 2017-06-26 12:41:49,456 main.py:50] epoch 2546, training loss: 6181.72, average training loss: 6304.22, base loss: 20514.75
[INFO 2017-06-26 12:41:49,814 main.py:50] epoch 2547, training loss: 6212.21, average training loss: 6303.90, base loss: 20514.90
[INFO 2017-06-26 12:41:50,174 main.py:50] epoch 2548, training loss: 6102.75, average training loss: 6303.50, base loss: 20514.83
[INFO 2017-06-26 12:41:50,534 main.py:50] epoch 2549, training loss: 6117.30, average training loss: 6303.06, base loss: 20514.91
[INFO 2017-06-26 12:41:50,893 main.py:50] epoch 2550, training loss: 6124.26, average training loss: 6302.68, base loss: 20515.13
[INFO 2017-06-26 12:41:51,250 main.py:50] epoch 2551, training loss: 6127.56, average training loss: 6302.26, base loss: 20515.13
[INFO 2017-06-26 12:41:51,609 main.py:50] epoch 2552, training loss: 6073.57, average training loss: 6301.81, base loss: 20514.80
[INFO 2017-06-26 12:41:51,969 main.py:50] epoch 2553, training loss: 6175.70, average training loss: 6301.49, base loss: 20514.92
[INFO 2017-06-26 12:41:52,326 main.py:50] epoch 2554, training loss: 6099.32, average training loss: 6301.09, base loss: 20515.01
[INFO 2017-06-26 12:41:52,687 main.py:50] epoch 2555, training loss: 6073.37, average training loss: 6300.64, base loss: 20514.74
[INFO 2017-06-26 12:41:53,046 main.py:50] epoch 2556, training loss: 6083.17, average training loss: 6300.22, base loss: 20514.78
[INFO 2017-06-26 12:41:53,406 main.py:50] epoch 2557, training loss: 6067.28, average training loss: 6299.74, base loss: 20514.52
[INFO 2017-06-26 12:41:53,765 main.py:50] epoch 2558, training loss: 6092.23, average training loss: 6299.33, base loss: 20514.67
[INFO 2017-06-26 12:41:54,123 main.py:50] epoch 2559, training loss: 6009.40, average training loss: 6298.92, base loss: 20514.56
[INFO 2017-06-26 12:41:54,483 main.py:50] epoch 2560, training loss: 6134.81, average training loss: 6298.59, base loss: 20514.83
[INFO 2017-06-26 12:41:54,842 main.py:50] epoch 2561, training loss: 6073.13, average training loss: 6298.25, base loss: 20515.29
[INFO 2017-06-26 12:41:55,200 main.py:50] epoch 2562, training loss: 6054.16, average training loss: 6297.82, base loss: 20515.25
[INFO 2017-06-26 12:41:55,559 main.py:50] epoch 2563, training loss: 6054.02, average training loss: 6297.37, base loss: 20515.50
[INFO 2017-06-26 12:41:55,918 main.py:50] epoch 2564, training loss: 6063.38, average training loss: 6296.91, base loss: 20515.75
[INFO 2017-06-26 12:41:56,278 main.py:50] epoch 2565, training loss: 6080.05, average training loss: 6296.49, base loss: 20515.90
[INFO 2017-06-26 12:41:56,636 main.py:50] epoch 2566, training loss: 6097.12, average training loss: 6296.08, base loss: 20515.66
[INFO 2017-06-26 12:41:56,996 main.py:50] epoch 2567, training loss: 6017.76, average training loss: 6295.65, base loss: 20515.61
[INFO 2017-06-26 12:41:57,357 main.py:50] epoch 2568, training loss: 6179.47, average training loss: 6295.33, base loss: 20515.71
[INFO 2017-06-26 12:41:57,717 main.py:50] epoch 2569, training loss: 6060.33, average training loss: 6294.84, base loss: 20515.69
[INFO 2017-06-26 12:41:58,076 main.py:50] epoch 2570, training loss: 6015.95, average training loss: 6294.37, base loss: 20515.66
[INFO 2017-06-26 12:41:58,433 main.py:50] epoch 2571, training loss: 6076.09, average training loss: 6293.94, base loss: 20515.82
[INFO 2017-06-26 12:41:58,792 main.py:50] epoch 2572, training loss: 6099.79, average training loss: 6293.56, base loss: 20515.94
[INFO 2017-06-26 12:41:59,152 main.py:50] epoch 2573, training loss: 6074.12, average training loss: 6293.15, base loss: 20515.82
[INFO 2017-06-26 12:41:59,512 main.py:50] epoch 2574, training loss: 6126.04, average training loss: 6292.76, base loss: 20515.85
[INFO 2017-06-26 12:41:59,871 main.py:50] epoch 2575, training loss: 6083.89, average training loss: 6292.32, base loss: 20515.66
[INFO 2017-06-26 12:42:00,230 main.py:50] epoch 2576, training loss: 6074.94, average training loss: 6291.81, base loss: 20515.63
[INFO 2017-06-26 12:42:00,591 main.py:50] epoch 2577, training loss: 6062.17, average training loss: 6291.36, base loss: 20515.32
[INFO 2017-06-26 12:42:00,950 main.py:50] epoch 2578, training loss: 6181.66, average training loss: 6291.01, base loss: 20515.56
[INFO 2017-06-26 12:42:01,309 main.py:50] epoch 2579, training loss: 6125.73, average training loss: 6290.66, base loss: 20515.40
[INFO 2017-06-26 12:42:01,668 main.py:50] epoch 2580, training loss: 6026.26, average training loss: 6290.11, base loss: 20515.14
[INFO 2017-06-26 12:42:02,025 main.py:50] epoch 2581, training loss: 6092.11, average training loss: 6289.68, base loss: 20514.99
[INFO 2017-06-26 12:42:02,385 main.py:50] epoch 2582, training loss: 6146.82, average training loss: 6289.19, base loss: 20515.55
[INFO 2017-06-26 12:42:02,744 main.py:50] epoch 2583, training loss: 6184.65, average training loss: 6288.76, base loss: 20515.86
[INFO 2017-06-26 12:42:03,104 main.py:50] epoch 2584, training loss: 6135.11, average training loss: 6288.31, base loss: 20515.97
[INFO 2017-06-26 12:42:03,463 main.py:50] epoch 2585, training loss: 6135.71, average training loss: 6287.83, base loss: 20515.71
[INFO 2017-06-26 12:42:03,822 main.py:50] epoch 2586, training loss: 6112.33, average training loss: 6287.37, base loss: 20515.23
[INFO 2017-06-26 12:42:04,180 main.py:50] epoch 2587, training loss: 6113.68, average training loss: 6286.90, base loss: 20515.18
[INFO 2017-06-26 12:42:04,541 main.py:50] epoch 2588, training loss: 6117.56, average training loss: 6286.49, base loss: 20515.05
[INFO 2017-06-26 12:42:04,902 main.py:50] epoch 2589, training loss: 6120.16, average training loss: 6286.12, base loss: 20515.16
[INFO 2017-06-26 12:42:05,261 main.py:50] epoch 2590, training loss: 6132.37, average training loss: 6285.77, base loss: 20515.10
[INFO 2017-06-26 12:42:05,621 main.py:50] epoch 2591, training loss: 6088.33, average training loss: 6285.35, base loss: 20515.13
[INFO 2017-06-26 12:42:05,980 main.py:50] epoch 2592, training loss: 6157.24, average training loss: 6284.96, base loss: 20515.08
[INFO 2017-06-26 12:42:06,339 main.py:50] epoch 2593, training loss: 6170.36, average training loss: 6284.61, base loss: 20515.19
[INFO 2017-06-26 12:42:06,698 main.py:50] epoch 2594, training loss: 6131.97, average training loss: 6284.21, base loss: 20515.29
[INFO 2017-06-26 12:42:07,057 main.py:50] epoch 2595, training loss: 6160.75, average training loss: 6283.78, base loss: 20515.01
[INFO 2017-06-26 12:42:07,417 main.py:50] epoch 2596, training loss: 6045.03, average training loss: 6283.38, base loss: 20514.61
[INFO 2017-06-26 12:42:07,776 main.py:50] epoch 2597, training loss: 6055.99, average training loss: 6282.96, base loss: 20514.34
[INFO 2017-06-26 12:42:08,134 main.py:50] epoch 2598, training loss: 6098.47, average training loss: 6282.62, base loss: 20514.55
[INFO 2017-06-26 12:42:08,494 main.py:50] epoch 2599, training loss: 6039.31, average training loss: 6282.18, base loss: 20514.54
[INFO 2017-06-26 12:42:08,494 main.py:52] epoch 2599, testing
[INFO 2017-06-26 12:42:09,967 main.py:103] average testing loss: 6097.89, base loss: 20507.88
[INFO 2017-06-26 12:42:09,967 main.py:76] current best accuracy: 6091.70
[INFO 2017-06-26 12:42:10,326 main.py:50] epoch 2600, training loss: 6087.40, average training loss: 6281.76, base loss: 20514.22
[INFO 2017-06-26 12:42:10,685 main.py:50] epoch 2601, training loss: 6149.24, average training loss: 6281.47, base loss: 20514.25
[INFO 2017-06-26 12:42:11,044 main.py:50] epoch 2602, training loss: 6152.83, average training loss: 6281.14, base loss: 20514.41
[INFO 2017-06-26 12:42:11,404 main.py:50] epoch 2603, training loss: 6123.29, average training loss: 6280.80, base loss: 20514.43
[INFO 2017-06-26 12:42:11,775 main.py:50] epoch 2604, training loss: 6170.83, average training loss: 6280.48, base loss: 20514.31
[INFO 2017-06-26 12:42:12,134 main.py:50] epoch 2605, training loss: 6079.53, average training loss: 6280.09, base loss: 20514.32
[INFO 2017-06-26 12:42:12,493 main.py:50] epoch 2606, training loss: 6244.85, average training loss: 6279.87, base loss: 20514.64
[INFO 2017-06-26 12:42:12,852 main.py:50] epoch 2607, training loss: 6162.95, average training loss: 6279.53, base loss: 20514.55
[INFO 2017-06-26 12:42:13,212 main.py:50] epoch 2608, training loss: 6184.14, average training loss: 6279.30, base loss: 20514.63
[INFO 2017-06-26 12:42:13,570 main.py:50] epoch 2609, training loss: 6083.98, average training loss: 6278.86, base loss: 20514.42
[INFO 2017-06-26 12:42:13,929 main.py:50] epoch 2610, training loss: 6119.70, average training loss: 6278.53, base loss: 20514.13
[INFO 2017-06-26 12:42:14,289 main.py:50] epoch 2611, training loss: 6120.32, average training loss: 6278.20, base loss: 20514.08
[INFO 2017-06-26 12:42:14,647 main.py:50] epoch 2612, training loss: 6044.62, average training loss: 6277.83, base loss: 20514.22
[INFO 2017-06-26 12:42:15,004 main.py:50] epoch 2613, training loss: 6191.51, average training loss: 6277.62, base loss: 20514.16
[INFO 2017-06-26 12:42:15,367 main.py:50] epoch 2614, training loss: 6135.49, average training loss: 6277.42, base loss: 20514.23
[INFO 2017-06-26 12:42:15,726 main.py:50] epoch 2615, training loss: 6164.30, average training loss: 6277.06, base loss: 20513.95
[INFO 2017-06-26 12:42:16,084 main.py:50] epoch 2616, training loss: 6134.47, average training loss: 6276.71, base loss: 20514.39
[INFO 2017-06-26 12:42:16,445 main.py:50] epoch 2617, training loss: 6201.03, average training loss: 6276.42, base loss: 20514.61
[INFO 2017-06-26 12:42:16,804 main.py:50] epoch 2618, training loss: 6225.24, average training loss: 6276.20, base loss: 20515.06
[INFO 2017-06-26 12:42:17,162 main.py:50] epoch 2619, training loss: 6206.78, average training loss: 6275.97, base loss: 20515.32
[INFO 2017-06-26 12:42:17,522 main.py:50] epoch 2620, training loss: 6161.90, average training loss: 6275.64, base loss: 20514.92
[INFO 2017-06-26 12:42:17,881 main.py:50] epoch 2621, training loss: 6191.51, average training loss: 6275.28, base loss: 20515.34
[INFO 2017-06-26 12:42:18,240 main.py:50] epoch 2622, training loss: 6084.89, average training loss: 6274.83, base loss: 20515.34
[INFO 2017-06-26 12:42:18,599 main.py:50] epoch 2623, training loss: 6175.54, average training loss: 6274.43, base loss: 20515.24
[INFO 2017-06-26 12:42:18,958 main.py:50] epoch 2624, training loss: 6060.71, average training loss: 6273.97, base loss: 20515.12
[INFO 2017-06-26 12:42:19,316 main.py:50] epoch 2625, training loss: 6125.48, average training loss: 6273.65, base loss: 20515.34
[INFO 2017-06-26 12:42:19,676 main.py:50] epoch 2626, training loss: 6165.73, average training loss: 6273.34, base loss: 20515.44
[INFO 2017-06-26 12:42:20,035 main.py:50] epoch 2627, training loss: 6107.38, average training loss: 6272.98, base loss: 20515.94
[INFO 2017-06-26 12:42:20,394 main.py:50] epoch 2628, training loss: 6095.81, average training loss: 6272.54, base loss: 20515.81
[INFO 2017-06-26 12:42:20,753 main.py:50] epoch 2629, training loss: 6200.74, average training loss: 6272.19, base loss: 20515.46
[INFO 2017-06-26 12:42:21,113 main.py:50] epoch 2630, training loss: 6113.69, average training loss: 6271.82, base loss: 20515.49
[INFO 2017-06-26 12:42:21,473 main.py:50] epoch 2631, training loss: 6117.40, average training loss: 6271.52, base loss: 20515.10
[INFO 2017-06-26 12:42:21,831 main.py:50] epoch 2632, training loss: 6130.36, average training loss: 6271.22, base loss: 20515.32
[INFO 2017-06-26 12:42:22,191 main.py:50] epoch 2633, training loss: 6106.28, average training loss: 6270.85, base loss: 20515.10
[INFO 2017-06-26 12:42:22,549 main.py:50] epoch 2634, training loss: 6116.17, average training loss: 6270.51, base loss: 20514.95
[INFO 2017-06-26 12:42:22,909 main.py:50] epoch 2635, training loss: 6223.05, average training loss: 6270.18, base loss: 20514.75
[INFO 2017-06-26 12:42:23,269 main.py:50] epoch 2636, training loss: 6053.68, average training loss: 6269.81, base loss: 20514.22
[INFO 2017-06-26 12:42:23,629 main.py:50] epoch 2637, training loss: 6174.16, average training loss: 6269.49, base loss: 20514.24
[INFO 2017-06-26 12:42:23,987 main.py:50] epoch 2638, training loss: 6088.86, average training loss: 6269.13, base loss: 20514.44
[INFO 2017-06-26 12:42:24,345 main.py:50] epoch 2639, training loss: 6165.43, average training loss: 6268.80, base loss: 20514.54
[INFO 2017-06-26 12:42:24,704 main.py:50] epoch 2640, training loss: 6170.86, average training loss: 6268.44, base loss: 20514.63
[INFO 2017-06-26 12:42:25,064 main.py:50] epoch 2641, training loss: 6107.53, average training loss: 6268.09, base loss: 20514.68
[INFO 2017-06-26 12:42:25,423 main.py:50] epoch 2642, training loss: 6226.10, average training loss: 6267.86, base loss: 20515.02
[INFO 2017-06-26 12:42:25,782 main.py:50] epoch 2643, training loss: 6171.10, average training loss: 6267.44, base loss: 20515.31
[INFO 2017-06-26 12:42:26,141 main.py:50] epoch 2644, training loss: 6099.74, average training loss: 6267.15, base loss: 20515.49
[INFO 2017-06-26 12:42:26,500 main.py:50] epoch 2645, training loss: 6158.35, average training loss: 6266.85, base loss: 20515.78
[INFO 2017-06-26 12:42:26,859 main.py:50] epoch 2646, training loss: 6150.93, average training loss: 6266.49, base loss: 20515.98
[INFO 2017-06-26 12:42:27,218 main.py:50] epoch 2647, training loss: 6209.93, average training loss: 6266.21, base loss: 20516.33
[INFO 2017-06-26 12:42:27,579 main.py:50] epoch 2648, training loss: 6104.55, average training loss: 6265.89, base loss: 20516.60
[INFO 2017-06-26 12:42:27,938 main.py:50] epoch 2649, training loss: 6078.37, average training loss: 6265.49, base loss: 20516.65
[INFO 2017-06-26 12:42:28,298 main.py:50] epoch 2650, training loss: 6044.54, average training loss: 6265.06, base loss: 20516.61
[INFO 2017-06-26 12:42:28,656 main.py:50] epoch 2651, training loss: 6143.38, average training loss: 6264.76, base loss: 20516.80
[INFO 2017-06-26 12:42:29,015 main.py:50] epoch 2652, training loss: 6109.79, average training loss: 6264.37, base loss: 20516.71
[INFO 2017-06-26 12:42:29,374 main.py:50] epoch 2653, training loss: 6112.22, average training loss: 6264.02, base loss: 20516.51
[INFO 2017-06-26 12:42:29,734 main.py:50] epoch 2654, training loss: 6089.53, average training loss: 6263.70, base loss: 20516.50
[INFO 2017-06-26 12:42:30,093 main.py:50] epoch 2655, training loss: 6090.62, average training loss: 6263.32, base loss: 20516.52
[INFO 2017-06-26 12:42:30,453 main.py:50] epoch 2656, training loss: 6129.76, average training loss: 6262.96, base loss: 20516.65
[INFO 2017-06-26 12:42:30,812 main.py:50] epoch 2657, training loss: 6077.93, average training loss: 6262.58, base loss: 20516.77
[INFO 2017-06-26 12:42:31,171 main.py:50] epoch 2658, training loss: 6081.16, average training loss: 6262.22, base loss: 20517.08
[INFO 2017-06-26 12:42:31,531 main.py:50] epoch 2659, training loss: 6038.17, average training loss: 6261.92, base loss: 20517.19
[INFO 2017-06-26 12:42:31,889 main.py:50] epoch 2660, training loss: 6115.30, average training loss: 6261.68, base loss: 20517.40
[INFO 2017-06-26 12:42:32,249 main.py:50] epoch 2661, training loss: 6054.43, average training loss: 6261.28, base loss: 20517.49
[INFO 2017-06-26 12:42:32,608 main.py:50] epoch 2662, training loss: 6039.19, average training loss: 6260.83, base loss: 20517.32
[INFO 2017-06-26 12:42:32,967 main.py:50] epoch 2663, training loss: 6073.47, average training loss: 6260.41, base loss: 20517.11
[INFO 2017-06-26 12:42:33,326 main.py:50] epoch 2664, training loss: 6078.87, average training loss: 6260.07, base loss: 20517.26
[INFO 2017-06-26 12:42:33,685 main.py:50] epoch 2665, training loss: 6039.61, average training loss: 6259.64, base loss: 20517.25
[INFO 2017-06-26 12:42:34,044 main.py:50] epoch 2666, training loss: 6100.58, average training loss: 6259.20, base loss: 20517.10
[INFO 2017-06-26 12:42:34,403 main.py:50] epoch 2667, training loss: 6108.45, average training loss: 6258.83, base loss: 20517.49
[INFO 2017-06-26 12:42:34,762 main.py:50] epoch 2668, training loss: 6094.71, average training loss: 6258.47, base loss: 20517.69
[INFO 2017-06-26 12:42:35,121 main.py:50] epoch 2669, training loss: 6055.52, average training loss: 6258.09, base loss: 20517.79
[INFO 2017-06-26 12:42:35,481 main.py:50] epoch 2670, training loss: 6113.34, average training loss: 6257.73, base loss: 20517.65
[INFO 2017-06-26 12:42:35,839 main.py:50] epoch 2671, training loss: 6013.74, average training loss: 6257.27, base loss: 20517.63
[INFO 2017-06-26 12:42:36,197 main.py:50] epoch 2672, training loss: 6049.82, average training loss: 6256.82, base loss: 20517.85
[INFO 2017-06-26 12:42:36,557 main.py:50] epoch 2673, training loss: 6117.55, average training loss: 6256.43, base loss: 20517.77
[INFO 2017-06-26 12:42:36,916 main.py:50] epoch 2674, training loss: 6134.93, average training loss: 6256.01, base loss: 20517.39
[INFO 2017-06-26 12:42:37,276 main.py:50] epoch 2675, training loss: 6109.01, average training loss: 6255.68, base loss: 20517.59
[INFO 2017-06-26 12:42:37,636 main.py:50] epoch 2676, training loss: 6121.83, average training loss: 6255.30, base loss: 20517.87
[INFO 2017-06-26 12:42:37,996 main.py:50] epoch 2677, training loss: 6118.93, average training loss: 6254.95, base loss: 20517.91
[INFO 2017-06-26 12:42:38,356 main.py:50] epoch 2678, training loss: 6114.97, average training loss: 6254.59, base loss: 20517.75
[INFO 2017-06-26 12:42:38,715 main.py:50] epoch 2679, training loss: 6142.13, average training loss: 6254.22, base loss: 20517.92
[INFO 2017-06-26 12:42:39,074 main.py:50] epoch 2680, training loss: 6047.86, average training loss: 6253.76, base loss: 20517.76
[INFO 2017-06-26 12:42:39,434 main.py:50] epoch 2681, training loss: 6131.48, average training loss: 6253.44, base loss: 20518.15
[INFO 2017-06-26 12:42:39,793 main.py:50] epoch 2682, training loss: 6040.33, average training loss: 6252.93, base loss: 20517.87
[INFO 2017-06-26 12:42:40,152 main.py:50] epoch 2683, training loss: 6069.46, average training loss: 6252.52, base loss: 20517.55
[INFO 2017-06-26 12:42:40,511 main.py:50] epoch 2684, training loss: 6073.07, average training loss: 6252.18, base loss: 20517.65
[INFO 2017-06-26 12:42:40,870 main.py:50] epoch 2685, training loss: 6021.63, average training loss: 6251.73, base loss: 20517.64
[INFO 2017-06-26 12:42:41,229 main.py:50] epoch 2686, training loss: 6057.20, average training loss: 6251.37, base loss: 20517.55
[INFO 2017-06-26 12:42:41,590 main.py:50] epoch 2687, training loss: 6058.35, average training loss: 6250.95, base loss: 20517.94
[INFO 2017-06-26 12:42:41,948 main.py:50] epoch 2688, training loss: 6023.24, average training loss: 6250.45, base loss: 20517.57
[INFO 2017-06-26 12:42:42,306 main.py:50] epoch 2689, training loss: 6066.40, average training loss: 6250.08, base loss: 20517.57
[INFO 2017-06-26 12:42:42,666 main.py:50] epoch 2690, training loss: 6016.88, average training loss: 6249.68, base loss: 20517.71
[INFO 2017-06-26 12:42:43,025 main.py:50] epoch 2691, training loss: 6046.66, average training loss: 6249.24, base loss: 20517.62
[INFO 2017-06-26 12:42:43,384 main.py:50] epoch 2692, training loss: 6063.66, average training loss: 6248.93, base loss: 20517.53
[INFO 2017-06-26 12:42:43,742 main.py:50] epoch 2693, training loss: 5968.23, average training loss: 6248.44, base loss: 20517.49
[INFO 2017-06-26 12:42:44,102 main.py:50] epoch 2694, training loss: 6059.42, average training loss: 6248.04, base loss: 20517.13
[INFO 2017-06-26 12:42:44,462 main.py:50] epoch 2695, training loss: 6121.00, average training loss: 6247.73, base loss: 20517.38
[INFO 2017-06-26 12:42:44,821 main.py:50] epoch 2696, training loss: 6047.61, average training loss: 6247.36, base loss: 20517.68
[INFO 2017-06-26 12:42:45,180 main.py:50] epoch 2697, training loss: 6124.81, average training loss: 6247.10, base loss: 20518.02
[INFO 2017-06-26 12:42:45,541 main.py:50] epoch 2698, training loss: 6097.73, average training loss: 6246.75, base loss: 20517.91
[INFO 2017-06-26 12:42:45,900 main.py:50] epoch 2699, training loss: 6076.65, average training loss: 6246.44, base loss: 20517.99
[INFO 2017-06-26 12:42:45,900 main.py:52] epoch 2699, testing
[INFO 2017-06-26 12:42:47,367 main.py:103] average testing loss: 6063.58, base loss: 20558.05
[INFO 2017-06-26 12:42:47,367 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 12:42:47,373 main.py:76] current best accuracy: 6063.58
[INFO 2017-06-26 12:42:47,733 main.py:50] epoch 2700, training loss: 5996.14, average training loss: 6246.04, base loss: 20517.87
[INFO 2017-06-26 12:42:48,093 main.py:50] epoch 2701, training loss: 6070.99, average training loss: 6245.74, base loss: 20517.59
[INFO 2017-06-26 12:42:48,453 main.py:50] epoch 2702, training loss: 6039.61, average training loss: 6245.34, base loss: 20517.82
[INFO 2017-06-26 12:42:48,812 main.py:50] epoch 2703, training loss: 6098.56, average training loss: 6245.00, base loss: 20517.82
[INFO 2017-06-26 12:42:49,171 main.py:50] epoch 2704, training loss: 6054.53, average training loss: 6244.69, base loss: 20517.48
[INFO 2017-06-26 12:42:49,531 main.py:50] epoch 2705, training loss: 6033.76, average training loss: 6244.25, base loss: 20517.21
[INFO 2017-06-26 12:42:49,890 main.py:50] epoch 2706, training loss: 6017.93, average training loss: 6243.84, base loss: 20517.13
[INFO 2017-06-26 12:42:50,249 main.py:50] epoch 2707, training loss: 6101.90, average training loss: 6243.51, base loss: 20517.19
[INFO 2017-06-26 12:42:50,606 main.py:50] epoch 2708, training loss: 6040.00, average training loss: 6243.12, base loss: 20516.97
[INFO 2017-06-26 12:42:50,967 main.py:50] epoch 2709, training loss: 6106.19, average training loss: 6242.77, base loss: 20516.85
[INFO 2017-06-26 12:42:51,326 main.py:50] epoch 2710, training loss: 6018.46, average training loss: 6242.37, base loss: 20516.43
[INFO 2017-06-26 12:42:51,685 main.py:50] epoch 2711, training loss: 6040.67, average training loss: 6241.96, base loss: 20516.66
[INFO 2017-06-26 12:42:52,043 main.py:50] epoch 2712, training loss: 6082.67, average training loss: 6241.72, base loss: 20517.12
[INFO 2017-06-26 12:42:52,403 main.py:50] epoch 2713, training loss: 6095.57, average training loss: 6241.31, base loss: 20516.74
[INFO 2017-06-26 12:42:52,761 main.py:50] epoch 2714, training loss: 5996.46, average training loss: 6240.88, base loss: 20516.78
[INFO 2017-06-26 12:42:53,122 main.py:50] epoch 2715, training loss: 6045.58, average training loss: 6240.48, base loss: 20516.86
[INFO 2017-06-26 12:42:53,480 main.py:50] epoch 2716, training loss: 6127.76, average training loss: 6240.02, base loss: 20516.45
[INFO 2017-06-26 12:42:53,838 main.py:50] epoch 2717, training loss: 6090.09, average training loss: 6239.65, base loss: 20516.40
[INFO 2017-06-26 12:42:54,198 main.py:50] epoch 2718, training loss: 6081.85, average training loss: 6239.24, base loss: 20516.19
[INFO 2017-06-26 12:42:54,556 main.py:50] epoch 2719, training loss: 6014.88, average training loss: 6238.75, base loss: 20515.88
[INFO 2017-06-26 12:42:54,915 main.py:50] epoch 2720, training loss: 6087.23, average training loss: 6238.35, base loss: 20515.72
[INFO 2017-06-26 12:42:55,273 main.py:50] epoch 2721, training loss: 6184.12, average training loss: 6238.05, base loss: 20515.69
[INFO 2017-06-26 12:42:55,632 main.py:50] epoch 2722, training loss: 6109.33, average training loss: 6237.66, base loss: 20515.46
[INFO 2017-06-26 12:42:56,004 main.py:50] epoch 2723, training loss: 6065.07, average training loss: 6237.23, base loss: 20515.39
[INFO 2017-06-26 12:42:56,362 main.py:50] epoch 2724, training loss: 6161.52, average training loss: 6236.83, base loss: 20515.09
[INFO 2017-06-26 12:42:56,721 main.py:50] epoch 2725, training loss: 6124.63, average training loss: 6236.62, base loss: 20515.45
[INFO 2017-06-26 12:42:57,080 main.py:50] epoch 2726, training loss: 6078.56, average training loss: 6236.26, base loss: 20515.50
[INFO 2017-06-26 12:42:57,438 main.py:50] epoch 2727, training loss: 6077.24, average training loss: 6235.95, base loss: 20515.97
[INFO 2017-06-26 12:42:57,798 main.py:50] epoch 2728, training loss: 6126.43, average training loss: 6235.62, base loss: 20516.25
[INFO 2017-06-26 12:42:58,157 main.py:50] epoch 2729, training loss: 6068.21, average training loss: 6235.18, base loss: 20516.45
[INFO 2017-06-26 12:42:58,516 main.py:50] epoch 2730, training loss: 6046.60, average training loss: 6234.82, base loss: 20516.35
[INFO 2017-06-26 12:42:58,874 main.py:50] epoch 2731, training loss: 6005.90, average training loss: 6234.42, base loss: 20516.37
[INFO 2017-06-26 12:42:59,233 main.py:50] epoch 2732, training loss: 6075.98, average training loss: 6234.07, base loss: 20516.51
[INFO 2017-06-26 12:42:59,595 main.py:50] epoch 2733, training loss: 6124.90, average training loss: 6233.70, base loss: 20517.12
[INFO 2017-06-26 12:42:59,954 main.py:50] epoch 2734, training loss: 6131.07, average training loss: 6233.35, base loss: 20517.10
[INFO 2017-06-26 12:43:00,314 main.py:50] epoch 2735, training loss: 5936.69, average training loss: 6232.64, base loss: 20516.93
[INFO 2017-06-26 12:43:00,672 main.py:50] epoch 2736, training loss: 6042.58, average training loss: 6232.19, base loss: 20516.37
[INFO 2017-06-26 12:43:01,032 main.py:50] epoch 2737, training loss: 6005.50, average training loss: 6231.70, base loss: 20516.39
[INFO 2017-06-26 12:43:01,391 main.py:50] epoch 2738, training loss: 6051.83, average training loss: 6231.21, base loss: 20516.42
[INFO 2017-06-26 12:43:01,751 main.py:50] epoch 2739, training loss: 6057.55, average training loss: 6230.80, base loss: 20516.49
[INFO 2017-06-26 12:43:02,110 main.py:50] epoch 2740, training loss: 6053.27, average training loss: 6230.32, base loss: 20516.38
[INFO 2017-06-26 12:43:02,468 main.py:50] epoch 2741, training loss: 6098.16, average training loss: 6229.94, base loss: 20516.02
[INFO 2017-06-26 12:43:02,826 main.py:50] epoch 2742, training loss: 6049.06, average training loss: 6229.59, base loss: 20515.94
[INFO 2017-06-26 12:43:03,185 main.py:50] epoch 2743, training loss: 6119.90, average training loss: 6229.35, base loss: 20516.26
[INFO 2017-06-26 12:43:03,544 main.py:50] epoch 2744, training loss: 6001.52, average training loss: 6228.86, base loss: 20516.22
[INFO 2017-06-26 12:43:03,913 main.py:50] epoch 2745, training loss: 6113.12, average training loss: 6228.59, base loss: 20516.61
[INFO 2017-06-26 12:43:04,272 main.py:50] epoch 2746, training loss: 6098.04, average training loss: 6228.27, base loss: 20516.54
[INFO 2017-06-26 12:43:04,632 main.py:50] epoch 2747, training loss: 6174.23, average training loss: 6228.02, base loss: 20516.65
[INFO 2017-06-26 12:43:04,991 main.py:50] epoch 2748, training loss: 6044.78, average training loss: 6227.67, base loss: 20516.68
[INFO 2017-06-26 12:43:05,350 main.py:50] epoch 2749, training loss: 5992.60, average training loss: 6227.24, base loss: 20516.55
[INFO 2017-06-26 12:43:05,710 main.py:50] epoch 2750, training loss: 6013.05, average training loss: 6226.87, base loss: 20516.74
[INFO 2017-06-26 12:43:06,069 main.py:50] epoch 2751, training loss: 6097.90, average training loss: 6226.53, base loss: 20516.98
[INFO 2017-06-26 12:43:06,428 main.py:50] epoch 2752, training loss: 6015.58, average training loss: 6226.12, base loss: 20516.87
[INFO 2017-06-26 12:43:06,787 main.py:50] epoch 2753, training loss: 6008.80, average training loss: 6225.72, base loss: 20516.56
[INFO 2017-06-26 12:43:07,146 main.py:50] epoch 2754, training loss: 6095.03, average training loss: 6225.39, base loss: 20516.60
[INFO 2017-06-26 12:43:07,505 main.py:50] epoch 2755, training loss: 6083.80, average training loss: 6225.13, base loss: 20516.94
[INFO 2017-06-26 12:43:07,864 main.py:50] epoch 2756, training loss: 6050.72, average training loss: 6224.76, base loss: 20516.96
[INFO 2017-06-26 12:43:08,221 main.py:50] epoch 2757, training loss: 6075.02, average training loss: 6224.42, base loss: 20517.01
[INFO 2017-06-26 12:43:08,582 main.py:50] epoch 2758, training loss: 6044.84, average training loss: 6224.05, base loss: 20517.18
[INFO 2017-06-26 12:43:08,941 main.py:50] epoch 2759, training loss: 6022.89, average training loss: 6223.62, base loss: 20516.88
[INFO 2017-06-26 12:43:09,300 main.py:50] epoch 2760, training loss: 6079.82, average training loss: 6223.31, base loss: 20517.57
[INFO 2017-06-26 12:43:09,659 main.py:50] epoch 2761, training loss: 6021.21, average training loss: 6223.01, base loss: 20517.81
[INFO 2017-06-26 12:43:10,018 main.py:50] epoch 2762, training loss: 6052.82, average training loss: 6222.68, base loss: 20517.81
[INFO 2017-06-26 12:43:10,378 main.py:50] epoch 2763, training loss: 6127.11, average training loss: 6222.41, base loss: 20517.69
[INFO 2017-06-26 12:43:10,736 main.py:50] epoch 2764, training loss: 6102.58, average training loss: 6222.13, base loss: 20517.85
[INFO 2017-06-26 12:43:11,096 main.py:50] epoch 2765, training loss: 6123.84, average training loss: 6221.88, base loss: 20518.10
[INFO 2017-06-26 12:43:11,456 main.py:50] epoch 2766, training loss: 6061.66, average training loss: 6221.58, base loss: 20518.08
[INFO 2017-06-26 12:43:11,815 main.py:50] epoch 2767, training loss: 6080.43, average training loss: 6221.22, base loss: 20518.03
[INFO 2017-06-26 12:43:12,174 main.py:50] epoch 2768, training loss: 6076.45, average training loss: 6220.82, base loss: 20518.11
[INFO 2017-06-26 12:43:12,533 main.py:50] epoch 2769, training loss: 6082.43, average training loss: 6220.48, base loss: 20518.28
[INFO 2017-06-26 12:43:12,892 main.py:50] epoch 2770, training loss: 6160.60, average training loss: 6220.20, base loss: 20518.05
[INFO 2017-06-26 12:43:13,251 main.py:50] epoch 2771, training loss: 6040.70, average training loss: 6219.84, base loss: 20518.13
[INFO 2017-06-26 12:43:13,610 main.py:50] epoch 2772, training loss: 6077.52, average training loss: 6219.55, base loss: 20518.08
[INFO 2017-06-26 12:43:13,970 main.py:50] epoch 2773, training loss: 6007.75, average training loss: 6219.09, base loss: 20517.80
[INFO 2017-06-26 12:43:14,328 main.py:50] epoch 2774, training loss: 6088.48, average training loss: 6218.76, base loss: 20517.39
[INFO 2017-06-26 12:43:14,687 main.py:50] epoch 2775, training loss: 6068.05, average training loss: 6218.33, base loss: 20517.28
[INFO 2017-06-26 12:43:15,048 main.py:50] epoch 2776, training loss: 6092.94, average training loss: 6217.98, base loss: 20517.31
[INFO 2017-06-26 12:43:15,405 main.py:50] epoch 2777, training loss: 6096.02, average training loss: 6217.58, base loss: 20516.95
[INFO 2017-06-26 12:43:15,764 main.py:50] epoch 2778, training loss: 6044.35, average training loss: 6217.17, base loss: 20516.84
[INFO 2017-06-26 12:43:16,123 main.py:50] epoch 2779, training loss: 6129.43, average training loss: 6216.93, base loss: 20517.14
[INFO 2017-06-26 12:43:16,481 main.py:50] epoch 2780, training loss: 6155.73, average training loss: 6216.70, base loss: 20517.02
[INFO 2017-06-26 12:43:16,840 main.py:50] epoch 2781, training loss: 6046.66, average training loss: 6216.33, base loss: 20516.91
[INFO 2017-06-26 12:43:17,199 main.py:50] epoch 2782, training loss: 6008.28, average training loss: 6215.83, base loss: 20516.40
[INFO 2017-06-26 12:43:17,558 main.py:50] epoch 2783, training loss: 6072.71, average training loss: 6215.47, base loss: 20516.65
[INFO 2017-06-26 12:43:17,918 main.py:50] epoch 2784, training loss: 6093.42, average training loss: 6215.15, base loss: 20516.69
[INFO 2017-06-26 12:43:18,276 main.py:50] epoch 2785, training loss: 6102.01, average training loss: 6214.91, base loss: 20516.59
[INFO 2017-06-26 12:43:18,634 main.py:50] epoch 2786, training loss: 6163.03, average training loss: 6214.72, base loss: 20516.31
[INFO 2017-06-26 12:43:18,994 main.py:50] epoch 2787, training loss: 6095.54, average training loss: 6214.43, base loss: 20516.41
[INFO 2017-06-26 12:43:19,353 main.py:50] epoch 2788, training loss: 6098.37, average training loss: 6214.15, base loss: 20516.10
[INFO 2017-06-26 12:43:19,713 main.py:50] epoch 2789, training loss: 6165.64, average training loss: 6213.93, base loss: 20515.96
[INFO 2017-06-26 12:43:20,073 main.py:50] epoch 2790, training loss: 6017.43, average training loss: 6213.50, base loss: 20515.74
[INFO 2017-06-26 12:43:20,433 main.py:50] epoch 2791, training loss: 6106.70, average training loss: 6213.14, base loss: 20515.50
[INFO 2017-06-26 12:43:20,792 main.py:50] epoch 2792, training loss: 6077.39, average training loss: 6212.81, base loss: 20515.64
[INFO 2017-06-26 12:43:21,151 main.py:50] epoch 2793, training loss: 6066.79, average training loss: 6212.49, base loss: 20515.60
[INFO 2017-06-26 12:43:21,510 main.py:50] epoch 2794, training loss: 6069.08, average training loss: 6212.20, base loss: 20515.51
[INFO 2017-06-26 12:43:21,867 main.py:50] epoch 2795, training loss: 6092.06, average training loss: 6211.91, base loss: 20515.66
[INFO 2017-06-26 12:43:22,226 main.py:50] epoch 2796, training loss: 5959.26, average training loss: 6211.51, base loss: 20515.81
[INFO 2017-06-26 12:43:22,584 main.py:50] epoch 2797, training loss: 6225.99, average training loss: 6211.32, base loss: 20516.05
[INFO 2017-06-26 12:43:22,942 main.py:50] epoch 2798, training loss: 6046.28, average training loss: 6210.92, base loss: 20515.93
[INFO 2017-06-26 12:43:23,301 main.py:50] epoch 2799, training loss: 6052.51, average training loss: 6210.59, base loss: 20515.99
[INFO 2017-06-26 12:43:23,301 main.py:52] epoch 2799, testing
[INFO 2017-06-26 12:43:24,767 main.py:103] average testing loss: 6110.49, base loss: 20490.16
[INFO 2017-06-26 12:43:24,768 main.py:76] current best accuracy: 6063.58
[INFO 2017-06-26 12:43:25,125 main.py:50] epoch 2800, training loss: 6058.01, average training loss: 6210.20, base loss: 20515.79
[INFO 2017-06-26 12:43:25,485 main.py:50] epoch 2801, training loss: 6084.59, average training loss: 6209.88, base loss: 20516.12
[INFO 2017-06-26 12:43:25,844 main.py:50] epoch 2802, training loss: 6084.23, average training loss: 6209.56, base loss: 20516.28
[INFO 2017-06-26 12:43:26,204 main.py:50] epoch 2803, training loss: 6087.76, average training loss: 6209.28, base loss: 20516.35
[INFO 2017-06-26 12:43:26,563 main.py:50] epoch 2804, training loss: 6095.29, average training loss: 6208.92, base loss: 20515.96
[INFO 2017-06-26 12:43:26,922 main.py:50] epoch 2805, training loss: 6107.45, average training loss: 6208.68, base loss: 20515.94
[INFO 2017-06-26 12:43:27,282 main.py:50] epoch 2806, training loss: 6205.03, average training loss: 6208.57, base loss: 20515.99
[INFO 2017-06-26 12:43:27,640 main.py:50] epoch 2807, training loss: 6130.67, average training loss: 6208.40, base loss: 20516.59
[INFO 2017-06-26 12:43:27,999 main.py:50] epoch 2808, training loss: 6091.01, average training loss: 6208.17, base loss: 20516.66
[INFO 2017-06-26 12:43:28,357 main.py:50] epoch 2809, training loss: 6104.03, average training loss: 6207.86, base loss: 20516.53
[INFO 2017-06-26 12:43:28,717 main.py:50] epoch 2810, training loss: 6091.54, average training loss: 6207.58, base loss: 20516.94
[INFO 2017-06-26 12:43:29,077 main.py:50] epoch 2811, training loss: 6067.99, average training loss: 6207.33, base loss: 20517.08
[INFO 2017-06-26 12:43:29,435 main.py:50] epoch 2812, training loss: 6047.59, average training loss: 6207.00, base loss: 20517.12
[INFO 2017-06-26 12:43:29,795 main.py:50] epoch 2813, training loss: 6063.71, average training loss: 6206.74, base loss: 20517.25
[INFO 2017-06-26 12:43:30,153 main.py:50] epoch 2814, training loss: 6069.56, average training loss: 6206.44, base loss: 20517.51
[INFO 2017-06-26 12:43:30,513 main.py:50] epoch 2815, training loss: 6062.11, average training loss: 6206.14, base loss: 20517.87
[INFO 2017-06-26 12:43:30,871 main.py:50] epoch 2816, training loss: 6076.80, average training loss: 6205.73, base loss: 20517.71
[INFO 2017-06-26 12:43:31,230 main.py:50] epoch 2817, training loss: 5997.31, average training loss: 6205.29, base loss: 20518.08
[INFO 2017-06-26 12:43:31,588 main.py:50] epoch 2818, training loss: 6082.19, average training loss: 6205.00, base loss: 20517.95
[INFO 2017-06-26 12:43:31,947 main.py:50] epoch 2819, training loss: 5997.10, average training loss: 6204.68, base loss: 20518.14
[INFO 2017-06-26 12:43:32,307 main.py:50] epoch 2820, training loss: 5998.51, average training loss: 6204.32, base loss: 20518.22
[INFO 2017-06-26 12:43:32,665 main.py:50] epoch 2821, training loss: 6000.16, average training loss: 6203.98, base loss: 20518.67
[INFO 2017-06-26 12:43:33,024 main.py:50] epoch 2822, training loss: 6051.75, average training loss: 6203.59, base loss: 20518.48
[INFO 2017-06-26 12:43:33,384 main.py:50] epoch 2823, training loss: 5995.85, average training loss: 6203.24, base loss: 20518.65
[INFO 2017-06-26 12:43:33,743 main.py:50] epoch 2824, training loss: 6059.25, average training loss: 6202.79, base loss: 20518.40
[INFO 2017-06-26 12:43:34,103 main.py:50] epoch 2825, training loss: 5993.27, average training loss: 6202.37, base loss: 20518.23
[INFO 2017-06-26 12:43:34,463 main.py:50] epoch 2826, training loss: 6004.42, average training loss: 6201.98, base loss: 20518.04
[INFO 2017-06-26 12:43:34,822 main.py:50] epoch 2827, training loss: 6137.19, average training loss: 6201.73, base loss: 20518.01
[INFO 2017-06-26 12:43:35,180 main.py:50] epoch 2828, training loss: 6088.74, average training loss: 6201.44, base loss: 20517.99
[INFO 2017-06-26 12:43:35,539 main.py:50] epoch 2829, training loss: 6054.54, average training loss: 6201.16, base loss: 20518.22
[INFO 2017-06-26 12:43:35,898 main.py:50] epoch 2830, training loss: 6005.96, average training loss: 6200.73, base loss: 20518.06
[INFO 2017-06-26 12:43:36,257 main.py:50] epoch 2831, training loss: 6039.75, average training loss: 6200.47, base loss: 20518.15
[INFO 2017-06-26 12:43:36,616 main.py:50] epoch 2832, training loss: 6031.43, average training loss: 6200.14, base loss: 20518.30
[INFO 2017-06-26 12:43:36,976 main.py:50] epoch 2833, training loss: 5964.88, average training loss: 6199.77, base loss: 20518.32
[INFO 2017-06-26 12:43:37,335 main.py:50] epoch 2834, training loss: 6028.03, average training loss: 6199.49, base loss: 20518.38
[INFO 2017-06-26 12:43:37,694 main.py:50] epoch 2835, training loss: 6020.51, average training loss: 6199.17, base loss: 20518.25
[INFO 2017-06-26 12:43:38,053 main.py:50] epoch 2836, training loss: 6105.74, average training loss: 6198.98, base loss: 20518.33
[INFO 2017-06-26 12:43:38,412 main.py:50] epoch 2837, training loss: 6092.90, average training loss: 6198.75, base loss: 20518.20
[INFO 2017-06-26 12:43:38,772 main.py:50] epoch 2838, training loss: 6009.71, average training loss: 6198.38, base loss: 20518.27
[INFO 2017-06-26 12:43:39,129 main.py:50] epoch 2839, training loss: 6022.54, average training loss: 6198.08, base loss: 20518.36
[INFO 2017-06-26 12:43:39,491 main.py:50] epoch 2840, training loss: 5949.07, average training loss: 6197.67, base loss: 20518.41
[INFO 2017-06-26 12:43:39,850 main.py:50] epoch 2841, training loss: 5978.16, average training loss: 6197.33, base loss: 20518.27
[INFO 2017-06-26 12:43:40,208 main.py:50] epoch 2842, training loss: 6021.17, average training loss: 6197.02, base loss: 20518.33
[INFO 2017-06-26 12:43:40,580 main.py:50] epoch 2843, training loss: 6035.42, average training loss: 6196.66, base loss: 20517.97
[INFO 2017-06-26 12:43:40,940 main.py:50] epoch 2844, training loss: 6099.52, average training loss: 6196.37, base loss: 20517.97
[INFO 2017-06-26 12:43:41,300 main.py:50] epoch 2845, training loss: 6019.53, average training loss: 6195.98, base loss: 20517.86
[INFO 2017-06-26 12:43:41,658 main.py:50] epoch 2846, training loss: 5994.65, average training loss: 6195.63, base loss: 20518.05
[INFO 2017-06-26 12:43:42,016 main.py:50] epoch 2847, training loss: 6072.23, average training loss: 6195.31, base loss: 20518.15
[INFO 2017-06-26 12:43:42,375 main.py:50] epoch 2848, training loss: 6025.33, average training loss: 6195.03, base loss: 20518.68
[INFO 2017-06-26 12:43:42,734 main.py:50] epoch 2849, training loss: 5989.25, average training loss: 6194.59, base loss: 20518.86
[INFO 2017-06-26 12:43:43,093 main.py:50] epoch 2850, training loss: 6043.85, average training loss: 6194.26, base loss: 20518.90
[INFO 2017-06-26 12:43:43,452 main.py:50] epoch 2851, training loss: 5912.63, average training loss: 6193.66, base loss: 20519.17
[INFO 2017-06-26 12:43:43,811 main.py:50] epoch 2852, training loss: 5954.01, average training loss: 6193.23, base loss: 20519.13
[INFO 2017-06-26 12:43:44,170 main.py:50] epoch 2853, training loss: 5955.53, average training loss: 6192.71, base loss: 20518.78
[INFO 2017-06-26 12:43:44,530 main.py:50] epoch 2854, training loss: 6089.16, average training loss: 6192.31, base loss: 20518.90
[INFO 2017-06-26 12:43:44,887 main.py:50] epoch 2855, training loss: 6026.97, average training loss: 6191.81, base loss: 20519.09
[INFO 2017-06-26 12:43:45,246 main.py:50] epoch 2856, training loss: 5918.51, average training loss: 6191.32, base loss: 20519.05
[INFO 2017-06-26 12:43:45,605 main.py:50] epoch 2857, training loss: 6007.79, average training loss: 6190.98, base loss: 20519.53
[INFO 2017-06-26 12:43:45,964 main.py:50] epoch 2858, training loss: 6129.85, average training loss: 6190.71, base loss: 20519.42
[INFO 2017-06-26 12:43:46,323 main.py:50] epoch 2859, training loss: 6035.65, average training loss: 6190.39, base loss: 20519.76
[INFO 2017-06-26 12:43:46,681 main.py:50] epoch 2860, training loss: 6043.01, average training loss: 6190.05, base loss: 20520.18
[INFO 2017-06-26 12:43:47,041 main.py:50] epoch 2861, training loss: 6035.58, average training loss: 6189.69, base loss: 20520.18
[INFO 2017-06-26 12:43:47,399 main.py:50] epoch 2862, training loss: 6005.97, average training loss: 6189.33, base loss: 20519.85
[INFO 2017-06-26 12:43:47,757 main.py:50] epoch 2863, training loss: 6008.24, average training loss: 6188.99, base loss: 20520.01
[INFO 2017-06-26 12:43:48,116 main.py:50] epoch 2864, training loss: 6013.41, average training loss: 6188.63, base loss: 20520.41
[INFO 2017-06-26 12:43:48,474 main.py:50] epoch 2865, training loss: 6024.87, average training loss: 6188.29, base loss: 20520.70
[INFO 2017-06-26 12:43:48,834 main.py:50] epoch 2866, training loss: 5944.26, average training loss: 6187.82, base loss: 20520.60
[INFO 2017-06-26 12:43:49,192 main.py:50] epoch 2867, training loss: 6007.22, average training loss: 6187.54, base loss: 20520.44
[INFO 2017-06-26 12:43:49,553 main.py:50] epoch 2868, training loss: 6005.44, average training loss: 6187.26, base loss: 20520.41
[INFO 2017-06-26 12:43:49,911 main.py:50] epoch 2869, training loss: 5961.19, average training loss: 6186.86, base loss: 20520.36
[INFO 2017-06-26 12:43:50,269 main.py:50] epoch 2870, training loss: 6026.26, average training loss: 6186.55, base loss: 20519.98
[INFO 2017-06-26 12:43:50,629 main.py:50] epoch 2871, training loss: 6030.50, average training loss: 6186.27, base loss: 20520.15
[INFO 2017-06-26 12:43:50,987 main.py:50] epoch 2872, training loss: 6011.35, average training loss: 6185.87, base loss: 20519.96
[INFO 2017-06-26 12:43:51,346 main.py:50] epoch 2873, training loss: 6026.44, average training loss: 6185.62, base loss: 20520.40
[INFO 2017-06-26 12:43:51,705 main.py:50] epoch 2874, training loss: 6033.78, average training loss: 6185.35, base loss: 20520.49
[INFO 2017-06-26 12:43:52,064 main.py:50] epoch 2875, training loss: 6020.17, average training loss: 6185.00, base loss: 20520.50
[INFO 2017-06-26 12:43:52,424 main.py:50] epoch 2876, training loss: 6008.91, average training loss: 6184.68, base loss: 20520.23
[INFO 2017-06-26 12:43:52,782 main.py:50] epoch 2877, training loss: 6024.41, average training loss: 6184.40, base loss: 20519.81
[INFO 2017-06-26 12:43:53,140 main.py:50] epoch 2878, training loss: 6058.65, average training loss: 6184.16, base loss: 20519.82
[INFO 2017-06-26 12:43:53,499 main.py:50] epoch 2879, training loss: 6031.33, average training loss: 6183.89, base loss: 20519.77
[INFO 2017-06-26 12:43:53,857 main.py:50] epoch 2880, training loss: 6030.11, average training loss: 6183.72, base loss: 20520.39
[INFO 2017-06-26 12:43:54,216 main.py:50] epoch 2881, training loss: 6145.23, average training loss: 6183.51, base loss: 20519.87
[INFO 2017-06-26 12:43:54,575 main.py:50] epoch 2882, training loss: 6151.57, average training loss: 6183.39, base loss: 20519.99
[INFO 2017-06-26 12:43:54,934 main.py:50] epoch 2883, training loss: 6092.38, average training loss: 6183.21, base loss: 20520.43
[INFO 2017-06-26 12:43:55,294 main.py:50] epoch 2884, training loss: 6050.21, average training loss: 6182.84, base loss: 20520.57
[INFO 2017-06-26 12:43:55,652 main.py:50] epoch 2885, training loss: 6105.99, average training loss: 6182.58, base loss: 20520.39
[INFO 2017-06-26 12:43:56,010 main.py:50] epoch 2886, training loss: 6059.47, average training loss: 6182.21, base loss: 20520.67
[INFO 2017-06-26 12:43:56,369 main.py:50] epoch 2887, training loss: 6047.65, average training loss: 6181.93, base loss: 20520.76
[INFO 2017-06-26 12:43:56,728 main.py:50] epoch 2888, training loss: 6089.66, average training loss: 6181.52, base loss: 20520.05
[INFO 2017-06-26 12:43:57,087 main.py:50] epoch 2889, training loss: 6106.41, average training loss: 6181.26, base loss: 20520.10
[INFO 2017-06-26 12:43:57,446 main.py:50] epoch 2890, training loss: 5980.81, average training loss: 6180.95, base loss: 20519.73
[INFO 2017-06-26 12:43:57,807 main.py:50] epoch 2891, training loss: 6093.35, average training loss: 6180.67, base loss: 20519.95
[INFO 2017-06-26 12:43:58,166 main.py:50] epoch 2892, training loss: 6100.41, average training loss: 6180.47, base loss: 20520.05
[INFO 2017-06-26 12:43:58,524 main.py:50] epoch 2893, training loss: 6051.28, average training loss: 6180.22, base loss: 20520.28
[INFO 2017-06-26 12:43:58,882 main.py:50] epoch 2894, training loss: 6053.86, average training loss: 6179.96, base loss: 20520.16
[INFO 2017-06-26 12:43:59,241 main.py:50] epoch 2895, training loss: 5979.42, average training loss: 6179.63, base loss: 20520.12
[INFO 2017-06-26 12:43:59,601 main.py:50] epoch 2896, training loss: 6025.84, average training loss: 6179.34, base loss: 20520.26
[INFO 2017-06-26 12:43:59,960 main.py:50] epoch 2897, training loss: 5983.69, average training loss: 6179.01, base loss: 20520.66
[INFO 2017-06-26 12:44:00,320 main.py:50] epoch 2898, training loss: 6043.66, average training loss: 6178.76, base loss: 20520.83
[INFO 2017-06-26 12:44:00,679 main.py:50] epoch 2899, training loss: 6040.60, average training loss: 6178.47, base loss: 20520.70
[INFO 2017-06-26 12:44:00,679 main.py:52] epoch 2899, testing
[INFO 2017-06-26 12:44:02,144 main.py:103] average testing loss: 6028.02, base loss: 20507.09
[INFO 2017-06-26 12:44:02,145 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 12:44:02,151 main.py:76] current best accuracy: 6028.02
[INFO 2017-06-26 12:44:02,510 main.py:50] epoch 2900, training loss: 5978.16, average training loss: 6178.13, base loss: 20520.62
[INFO 2017-06-26 12:44:02,870 main.py:50] epoch 2901, training loss: 5943.02, average training loss: 6177.69, base loss: 20520.51
[INFO 2017-06-26 12:44:03,227 main.py:50] epoch 2902, training loss: 5963.64, average training loss: 6177.35, base loss: 20520.63
[INFO 2017-06-26 12:44:03,586 main.py:50] epoch 2903, training loss: 6017.88, average training loss: 6177.06, base loss: 20520.73
[INFO 2017-06-26 12:44:03,946 main.py:50] epoch 2904, training loss: 5982.53, average training loss: 6176.78, base loss: 20520.87
[INFO 2017-06-26 12:44:04,307 main.py:50] epoch 2905, training loss: 6093.84, average training loss: 6176.56, base loss: 20520.95
[INFO 2017-06-26 12:44:04,665 main.py:50] epoch 2906, training loss: 6032.05, average training loss: 6176.23, base loss: 20521.31
[INFO 2017-06-26 12:44:05,026 main.py:50] epoch 2907, training loss: 6057.28, average training loss: 6175.91, base loss: 20521.37
[INFO 2017-06-26 12:44:05,384 main.py:50] epoch 2908, training loss: 5980.61, average training loss: 6175.51, base loss: 20521.24
[INFO 2017-06-26 12:44:05,743 main.py:50] epoch 2909, training loss: 5959.39, average training loss: 6175.13, base loss: 20521.11
[INFO 2017-06-26 12:44:06,103 main.py:50] epoch 2910, training loss: 5945.19, average training loss: 6174.69, base loss: 20521.06
[INFO 2017-06-26 12:44:06,462 main.py:50] epoch 2911, training loss: 5995.79, average training loss: 6174.33, base loss: 20521.12
[INFO 2017-06-26 12:44:06,820 main.py:50] epoch 2912, training loss: 6028.22, average training loss: 6174.00, base loss: 20521.37
[INFO 2017-06-26 12:44:07,181 main.py:50] epoch 2913, training loss: 5968.73, average training loss: 6173.58, base loss: 20521.30
[INFO 2017-06-26 12:44:07,539 main.py:50] epoch 2914, training loss: 6010.38, average training loss: 6173.18, base loss: 20521.49
[INFO 2017-06-26 12:44:07,897 main.py:50] epoch 2915, training loss: 6052.47, average training loss: 6172.86, base loss: 20521.45
[INFO 2017-06-26 12:44:08,254 main.py:50] epoch 2916, training loss: 6002.67, average training loss: 6172.53, base loss: 20521.56
[INFO 2017-06-26 12:44:08,612 main.py:50] epoch 2917, training loss: 5972.03, average training loss: 6172.17, base loss: 20521.24
[INFO 2017-06-26 12:44:08,970 main.py:50] epoch 2918, training loss: 5972.28, average training loss: 6171.74, base loss: 20521.16
[INFO 2017-06-26 12:44:09,327 main.py:50] epoch 2919, training loss: 5971.81, average training loss: 6171.31, base loss: 20520.88
[INFO 2017-06-26 12:44:09,687 main.py:50] epoch 2920, training loss: 6078.13, average training loss: 6170.97, base loss: 20520.86
[INFO 2017-06-26 12:44:10,045 main.py:50] epoch 2921, training loss: 5973.61, average training loss: 6170.64, base loss: 20521.04
[INFO 2017-06-26 12:44:10,404 main.py:50] epoch 2922, training loss: 6009.13, average training loss: 6170.43, base loss: 20521.01
[INFO 2017-06-26 12:44:10,763 main.py:50] epoch 2923, training loss: 5947.41, average training loss: 6170.03, base loss: 20521.21
[INFO 2017-06-26 12:44:11,122 main.py:50] epoch 2924, training loss: 5997.72, average training loss: 6169.73, base loss: 20521.16
[INFO 2017-06-26 12:44:11,481 main.py:50] epoch 2925, training loss: 5991.62, average training loss: 6169.39, base loss: 20521.35
[INFO 2017-06-26 12:44:11,841 main.py:50] epoch 2926, training loss: 5942.95, average training loss: 6168.90, base loss: 20521.24
[INFO 2017-06-26 12:44:12,198 main.py:50] epoch 2927, training loss: 5926.43, average training loss: 6168.49, base loss: 20521.48
[INFO 2017-06-26 12:44:12,558 main.py:50] epoch 2928, training loss: 5999.03, average training loss: 6168.14, base loss: 20521.83
[INFO 2017-06-26 12:44:12,916 main.py:50] epoch 2929, training loss: 6062.85, average training loss: 6167.87, base loss: 20522.05
[INFO 2017-06-26 12:44:13,276 main.py:50] epoch 2930, training loss: 6080.12, average training loss: 6167.60, base loss: 20522.21
[INFO 2017-06-26 12:44:13,634 main.py:50] epoch 2931, training loss: 6016.89, average training loss: 6167.29, base loss: 20522.09
[INFO 2017-06-26 12:44:13,993 main.py:50] epoch 2932, training loss: 5971.10, average training loss: 6166.93, base loss: 20521.98
[INFO 2017-06-26 12:44:14,352 main.py:50] epoch 2933, training loss: 6052.52, average training loss: 6166.67, base loss: 20522.12
[INFO 2017-06-26 12:44:14,711 main.py:50] epoch 2934, training loss: 6039.20, average training loss: 6166.44, base loss: 20522.37
[INFO 2017-06-26 12:44:15,071 main.py:50] epoch 2935, training loss: 6021.69, average training loss: 6166.11, base loss: 20521.99
[INFO 2017-06-26 12:44:15,431 main.py:50] epoch 2936, training loss: 5989.12, average training loss: 6165.79, base loss: 20522.02
[INFO 2017-06-26 12:44:15,790 main.py:50] epoch 2937, training loss: 6017.92, average training loss: 6165.51, base loss: 20521.94
[INFO 2017-06-26 12:44:16,149 main.py:50] epoch 2938, training loss: 6098.03, average training loss: 6165.29, base loss: 20522.03
[INFO 2017-06-26 12:44:16,508 main.py:50] epoch 2939, training loss: 5912.48, average training loss: 6164.84, base loss: 20521.86
[INFO 2017-06-26 12:44:16,867 main.py:50] epoch 2940, training loss: 6005.16, average training loss: 6164.52, base loss: 20521.80
[INFO 2017-06-26 12:44:17,225 main.py:50] epoch 2941, training loss: 5976.31, average training loss: 6164.18, base loss: 20521.89
[INFO 2017-06-26 12:44:17,586 main.py:50] epoch 2942, training loss: 5997.48, average training loss: 6163.88, base loss: 20522.16
[INFO 2017-06-26 12:44:17,944 main.py:50] epoch 2943, training loss: 6053.76, average training loss: 6163.66, base loss: 20522.63
[INFO 2017-06-26 12:44:18,302 main.py:50] epoch 2944, training loss: 5920.36, average training loss: 6163.30, base loss: 20522.98
[INFO 2017-06-26 12:44:18,661 main.py:50] epoch 2945, training loss: 6042.98, average training loss: 6163.08, base loss: 20523.04
[INFO 2017-06-26 12:44:19,021 main.py:50] epoch 2946, training loss: 6040.80, average training loss: 6162.82, base loss: 20522.96
[INFO 2017-06-26 12:44:19,381 main.py:50] epoch 2947, training loss: 5976.06, average training loss: 6162.49, base loss: 20522.79
[INFO 2017-06-26 12:44:19,740 main.py:50] epoch 2948, training loss: 6025.40, average training loss: 6162.17, base loss: 20522.30
[INFO 2017-06-26 12:44:20,099 main.py:50] epoch 2949, training loss: 6028.18, average training loss: 6161.82, base loss: 20522.30
[INFO 2017-06-26 12:44:20,458 main.py:50] epoch 2950, training loss: 6036.01, average training loss: 6161.59, base loss: 20522.63
[INFO 2017-06-26 12:44:20,817 main.py:50] epoch 2951, training loss: 6056.59, average training loss: 6161.36, base loss: 20523.06
[INFO 2017-06-26 12:44:21,177 main.py:50] epoch 2952, training loss: 5999.54, average training loss: 6161.02, base loss: 20522.57
[INFO 2017-06-26 12:44:21,535 main.py:50] epoch 2953, training loss: 6034.82, average training loss: 6160.76, base loss: 20522.82
[INFO 2017-06-26 12:44:21,893 main.py:50] epoch 2954, training loss: 6000.95, average training loss: 6160.50, base loss: 20522.53
[INFO 2017-06-26 12:44:22,252 main.py:50] epoch 2955, training loss: 5990.51, average training loss: 6160.20, base loss: 20522.58
[INFO 2017-06-26 12:44:22,611 main.py:50] epoch 2956, training loss: 6013.69, average training loss: 6159.94, base loss: 20522.47
[INFO 2017-06-26 12:44:22,969 main.py:50] epoch 2957, training loss: 6015.96, average training loss: 6159.69, base loss: 20522.30
[INFO 2017-06-26 12:44:23,329 main.py:50] epoch 2958, training loss: 6029.09, average training loss: 6159.30, base loss: 20522.01
[INFO 2017-06-26 12:44:23,687 main.py:50] epoch 2959, training loss: 5997.41, average training loss: 6158.99, base loss: 20522.19
[INFO 2017-06-26 12:44:24,046 main.py:50] epoch 2960, training loss: 6035.59, average training loss: 6158.69, base loss: 20522.55
[INFO 2017-06-26 12:44:24,406 main.py:50] epoch 2961, training loss: 6004.16, average training loss: 6158.46, base loss: 20522.41
[INFO 2017-06-26 12:44:24,776 main.py:50] epoch 2962, training loss: 6006.68, average training loss: 6158.16, base loss: 20522.44
[INFO 2017-06-26 12:44:25,135 main.py:50] epoch 2963, training loss: 6037.57, average training loss: 6157.77, base loss: 20522.08
[INFO 2017-06-26 12:44:25,495 main.py:50] epoch 2964, training loss: 6079.32, average training loss: 6157.51, base loss: 20521.97
[INFO 2017-06-26 12:44:25,853 main.py:50] epoch 2965, training loss: 6022.23, average training loss: 6157.19, base loss: 20521.94
[INFO 2017-06-26 12:44:26,214 main.py:50] epoch 2966, training loss: 6037.27, average training loss: 6156.85, base loss: 20521.92
[INFO 2017-06-26 12:44:26,573 main.py:50] epoch 2967, training loss: 6016.47, average training loss: 6156.52, base loss: 20521.92
[INFO 2017-06-26 12:44:26,932 main.py:50] epoch 2968, training loss: 5955.92, average training loss: 6156.21, base loss: 20522.31
[INFO 2017-06-26 12:44:27,291 main.py:50] epoch 2969, training loss: 5985.64, average training loss: 6155.85, base loss: 20522.54
[INFO 2017-06-26 12:44:27,650 main.py:50] epoch 2970, training loss: 6012.07, average training loss: 6155.58, base loss: 20522.67
[INFO 2017-06-26 12:44:28,009 main.py:50] epoch 2971, training loss: 6050.12, average training loss: 6155.25, base loss: 20522.46
[INFO 2017-06-26 12:44:28,368 main.py:50] epoch 2972, training loss: 5990.45, average training loss: 6154.86, base loss: 20522.08
[INFO 2017-06-26 12:44:28,727 main.py:50] epoch 2973, training loss: 6002.35, average training loss: 6154.52, base loss: 20522.08
[INFO 2017-06-26 12:44:29,086 main.py:50] epoch 2974, training loss: 5994.58, average training loss: 6154.18, base loss: 20521.88
[INFO 2017-06-26 12:44:29,445 main.py:50] epoch 2975, training loss: 6000.16, average training loss: 6153.82, base loss: 20521.70
[INFO 2017-06-26 12:44:29,804 main.py:50] epoch 2976, training loss: 6071.20, average training loss: 6153.48, base loss: 20521.75
[INFO 2017-06-26 12:44:30,163 main.py:50] epoch 2977, training loss: 5944.43, average training loss: 6153.03, base loss: 20521.35
[INFO 2017-06-26 12:44:30,521 main.py:50] epoch 2978, training loss: 5965.54, average training loss: 6152.59, base loss: 20521.14
[INFO 2017-06-26 12:44:30,880 main.py:50] epoch 2979, training loss: 5977.51, average training loss: 6152.19, base loss: 20521.17
[INFO 2017-06-26 12:44:31,238 main.py:50] epoch 2980, training loss: 5971.61, average training loss: 6151.79, base loss: 20520.76
[INFO 2017-06-26 12:44:31,597 main.py:50] epoch 2981, training loss: 6066.43, average training loss: 6151.55, base loss: 20521.17
[INFO 2017-06-26 12:44:31,958 main.py:50] epoch 2982, training loss: 6016.34, average training loss: 6151.25, base loss: 20521.23
[INFO 2017-06-26 12:44:32,315 main.py:50] epoch 2983, training loss: 6044.62, average training loss: 6151.02, base loss: 20521.80
[INFO 2017-06-26 12:44:32,675 main.py:50] epoch 2984, training loss: 5980.60, average training loss: 6150.74, base loss: 20521.98
[INFO 2017-06-26 12:44:33,033 main.py:50] epoch 2985, training loss: 6000.26, average training loss: 6150.36, base loss: 20521.85
[INFO 2017-06-26 12:44:33,392 main.py:50] epoch 2986, training loss: 6034.97, average training loss: 6149.95, base loss: 20521.61
[INFO 2017-06-26 12:44:33,752 main.py:50] epoch 2987, training loss: 5979.08, average training loss: 6149.55, base loss: 20521.05
[INFO 2017-06-26 12:44:34,110 main.py:50] epoch 2988, training loss: 5952.03, average training loss: 6149.20, base loss: 20520.81
[INFO 2017-06-26 12:44:34,469 main.py:50] epoch 2989, training loss: 6069.94, average training loss: 6148.88, base loss: 20520.81
[INFO 2017-06-26 12:44:34,828 main.py:50] epoch 2990, training loss: 5994.84, average training loss: 6148.56, base loss: 20521.12
[INFO 2017-06-26 12:44:35,188 main.py:50] epoch 2991, training loss: 5975.86, average training loss: 6148.18, base loss: 20521.11
[INFO 2017-06-26 12:44:35,546 main.py:50] epoch 2992, training loss: 5971.77, average training loss: 6147.73, base loss: 20520.85
[INFO 2017-06-26 12:44:35,906 main.py:50] epoch 2993, training loss: 5947.19, average training loss: 6147.29, base loss: 20520.36
[INFO 2017-06-26 12:44:36,264 main.py:50] epoch 2994, training loss: 6008.67, average training loss: 6146.88, base loss: 20520.08
[INFO 2017-06-26 12:44:36,623 main.py:50] epoch 2995, training loss: 6052.32, average training loss: 6146.60, base loss: 20519.95
[INFO 2017-06-26 12:44:36,983 main.py:50] epoch 2996, training loss: 6057.87, average training loss: 6146.32, base loss: 20520.14
[INFO 2017-06-26 12:44:37,342 main.py:50] epoch 2997, training loss: 6035.59, average training loss: 6146.03, base loss: 20519.88
[INFO 2017-06-26 12:44:37,702 main.py:50] epoch 2998, training loss: 6031.99, average training loss: 6145.74, base loss: 20519.93
[INFO 2017-06-26 12:44:38,061 main.py:50] epoch 2999, training loss: 6022.46, average training loss: 6145.47, base loss: 20520.16
[INFO 2017-06-26 12:44:38,061 main.py:52] epoch 2999, testing
[INFO 2017-06-26 12:44:39,526 main.py:103] average testing loss: 6016.28, base loss: 20532.50
[INFO 2017-06-26 12:44:39,526 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 12:44:39,533 main.py:76] current best accuracy: 6016.28
[INFO 2017-06-26 12:44:39,892 main.py:50] epoch 3000, training loss: 6093.20, average training loss: 6145.29, base loss: 20520.13
[INFO 2017-06-26 12:44:40,250 main.py:50] epoch 3001, training loss: 6011.21, average training loss: 6145.00, base loss: 20520.00
[INFO 2017-06-26 12:44:40,609 main.py:50] epoch 3002, training loss: 5980.06, average training loss: 6144.66, base loss: 20519.55
[INFO 2017-06-26 12:44:40,968 main.py:50] epoch 3003, training loss: 6038.68, average training loss: 6144.46, base loss: 20519.37
[INFO 2017-06-26 12:44:41,326 main.py:50] epoch 3004, training loss: 5959.81, average training loss: 6143.97, base loss: 20518.96
[INFO 2017-06-26 12:44:41,685 main.py:50] epoch 3005, training loss: 5942.83, average training loss: 6143.61, base loss: 20518.77
[INFO 2017-06-26 12:44:42,044 main.py:50] epoch 3006, training loss: 6031.01, average training loss: 6143.32, base loss: 20518.69
[INFO 2017-06-26 12:44:42,402 main.py:50] epoch 3007, training loss: 6061.94, average training loss: 6143.04, base loss: 20518.49
[INFO 2017-06-26 12:44:42,761 main.py:50] epoch 3008, training loss: 6085.28, average training loss: 6142.80, base loss: 20518.77
[INFO 2017-06-26 12:44:43,120 main.py:50] epoch 3009, training loss: 5985.67, average training loss: 6142.50, base loss: 20519.16
[INFO 2017-06-26 12:44:43,480 main.py:50] epoch 3010, training loss: 6017.43, average training loss: 6142.20, base loss: 20519.03
[INFO 2017-06-26 12:44:43,839 main.py:50] epoch 3011, training loss: 5963.42, average training loss: 6141.75, base loss: 20518.79
[INFO 2017-06-26 12:44:44,198 main.py:50] epoch 3012, training loss: 6021.76, average training loss: 6141.48, base loss: 20518.73
[INFO 2017-06-26 12:44:44,558 main.py:50] epoch 3013, training loss: 6034.88, average training loss: 6141.22, base loss: 20518.66
[INFO 2017-06-26 12:44:44,917 main.py:50] epoch 3014, training loss: 6007.22, average training loss: 6140.98, base loss: 20518.36
[INFO 2017-06-26 12:44:45,275 main.py:50] epoch 3015, training loss: 6003.72, average training loss: 6140.66, base loss: 20518.53
[INFO 2017-06-26 12:44:45,634 main.py:50] epoch 3016, training loss: 5941.99, average training loss: 6140.33, base loss: 20518.54
[INFO 2017-06-26 12:44:45,992 main.py:50] epoch 3017, training loss: 5943.02, average training loss: 6140.04, base loss: 20518.50
[INFO 2017-06-26 12:44:46,351 main.py:50] epoch 3018, training loss: 5964.05, average training loss: 6139.64, base loss: 20518.14
[INFO 2017-06-26 12:44:46,710 main.py:50] epoch 3019, training loss: 6014.12, average training loss: 6139.35, base loss: 20517.88
[INFO 2017-06-26 12:44:47,070 main.py:50] epoch 3020, training loss: 6083.32, average training loss: 6139.11, base loss: 20518.15
[INFO 2017-06-26 12:44:47,427 main.py:50] epoch 3021, training loss: 5970.29, average training loss: 6138.76, base loss: 20517.87
[INFO 2017-06-26 12:44:47,786 main.py:50] epoch 3022, training loss: 6003.84, average training loss: 6138.40, base loss: 20517.56
[INFO 2017-06-26 12:44:48,145 main.py:50] epoch 3023, training loss: 6012.00, average training loss: 6138.19, base loss: 20517.60
[INFO 2017-06-26 12:44:48,503 main.py:50] epoch 3024, training loss: 5948.20, average training loss: 6137.79, base loss: 20517.07
[INFO 2017-06-26 12:44:48,861 main.py:50] epoch 3025, training loss: 5955.31, average training loss: 6137.42, base loss: 20517.21
[INFO 2017-06-26 12:44:49,218 main.py:50] epoch 3026, training loss: 5983.97, average training loss: 6137.07, base loss: 20517.03
[INFO 2017-06-26 12:44:49,578 main.py:50] epoch 3027, training loss: 6021.05, average training loss: 6136.72, base loss: 20517.07
[INFO 2017-06-26 12:44:49,936 main.py:50] epoch 3028, training loss: 5972.93, average training loss: 6136.40, base loss: 20517.58
[INFO 2017-06-26 12:44:50,296 main.py:50] epoch 3029, training loss: 5971.08, average training loss: 6135.96, base loss: 20517.79
[INFO 2017-06-26 12:44:50,654 main.py:50] epoch 3030, training loss: 5985.41, average training loss: 6135.54, base loss: 20517.74
[INFO 2017-06-26 12:44:51,013 main.py:50] epoch 3031, training loss: 6044.79, average training loss: 6135.16, base loss: 20517.98
[INFO 2017-06-26 12:44:51,372 main.py:50] epoch 3032, training loss: 6019.21, average training loss: 6134.93, base loss: 20518.23
[INFO 2017-06-26 12:44:51,731 main.py:50] epoch 3033, training loss: 5977.44, average training loss: 6134.60, base loss: 20518.41
[INFO 2017-06-26 12:44:52,090 main.py:50] epoch 3034, training loss: 6076.28, average training loss: 6134.38, base loss: 20518.72
[INFO 2017-06-26 12:44:52,449 main.py:50] epoch 3035, training loss: 6032.56, average training loss: 6134.09, base loss: 20518.71
[INFO 2017-06-26 12:44:52,808 main.py:50] epoch 3036, training loss: 5904.41, average training loss: 6133.75, base loss: 20518.68
[INFO 2017-06-26 12:44:53,167 main.py:50] epoch 3037, training loss: 6063.53, average training loss: 6133.52, base loss: 20518.46
[INFO 2017-06-26 12:44:53,526 main.py:50] epoch 3038, training loss: 6022.85, average training loss: 6133.23, base loss: 20518.74
[INFO 2017-06-26 12:44:53,883 main.py:50] epoch 3039, training loss: 5997.90, average training loss: 6132.86, base loss: 20518.56
[INFO 2017-06-26 12:44:54,242 main.py:50] epoch 3040, training loss: 6048.61, average training loss: 6132.62, base loss: 20518.45
[INFO 2017-06-26 12:44:54,601 main.py:50] epoch 3041, training loss: 5969.97, average training loss: 6132.28, base loss: 20518.20
[INFO 2017-06-26 12:44:54,961 main.py:50] epoch 3042, training loss: 5963.76, average training loss: 6131.94, base loss: 20518.14
[INFO 2017-06-26 12:44:55,321 main.py:50] epoch 3043, training loss: 6018.61, average training loss: 6131.58, base loss: 20518.24
[INFO 2017-06-26 12:44:55,680 main.py:50] epoch 3044, training loss: 6040.14, average training loss: 6131.30, base loss: 20518.44
[INFO 2017-06-26 12:44:56,039 main.py:50] epoch 3045, training loss: 5962.32, average training loss: 6130.89, base loss: 20517.90
[INFO 2017-06-26 12:44:56,398 main.py:50] epoch 3046, training loss: 6023.86, average training loss: 6130.44, base loss: 20517.89
[INFO 2017-06-26 12:44:56,757 main.py:50] epoch 3047, training loss: 6021.88, average training loss: 6130.13, base loss: 20518.00
[INFO 2017-06-26 12:44:57,115 main.py:50] epoch 3048, training loss: 5996.02, average training loss: 6129.80, base loss: 20518.33
[INFO 2017-06-26 12:44:57,474 main.py:50] epoch 3049, training loss: 6068.93, average training loss: 6129.55, base loss: 20518.45
[INFO 2017-06-26 12:44:57,833 main.py:50] epoch 3050, training loss: 5982.08, average training loss: 6129.18, base loss: 20518.71
[INFO 2017-06-26 12:44:58,192 main.py:50] epoch 3051, training loss: 5968.91, average training loss: 6128.78, base loss: 20518.47
[INFO 2017-06-26 12:44:58,551 main.py:50] epoch 3052, training loss: 5966.80, average training loss: 6128.37, base loss: 20518.40
[INFO 2017-06-26 12:44:58,909 main.py:50] epoch 3053, training loss: 5985.42, average training loss: 6128.04, base loss: 20518.35
[INFO 2017-06-26 12:44:59,268 main.py:50] epoch 3054, training loss: 6086.16, average training loss: 6127.89, base loss: 20518.43
[INFO 2017-06-26 12:44:59,627 main.py:50] epoch 3055, training loss: 6024.38, average training loss: 6127.59, base loss: 20518.18
[INFO 2017-06-26 12:44:59,986 main.py:50] epoch 3056, training loss: 5990.51, average training loss: 6127.27, base loss: 20518.11
[INFO 2017-06-26 12:45:00,345 main.py:50] epoch 3057, training loss: 6071.25, average training loss: 6127.02, base loss: 20518.24
[INFO 2017-06-26 12:45:00,705 main.py:50] epoch 3058, training loss: 6015.36, average training loss: 6126.67, base loss: 20518.54
[INFO 2017-06-26 12:45:01,064 main.py:50] epoch 3059, training loss: 6042.62, average training loss: 6126.45, base loss: 20518.37
[INFO 2017-06-26 12:45:01,422 main.py:50] epoch 3060, training loss: 6003.75, average training loss: 6126.21, base loss: 20518.31
[INFO 2017-06-26 12:45:01,781 main.py:50] epoch 3061, training loss: 5928.95, average training loss: 6125.85, base loss: 20517.97
[INFO 2017-06-26 12:45:02,139 main.py:50] epoch 3062, training loss: 6028.47, average training loss: 6125.54, base loss: 20517.75
[INFO 2017-06-26 12:45:02,498 main.py:50] epoch 3063, training loss: 5995.56, average training loss: 6125.22, base loss: 20517.86
[INFO 2017-06-26 12:45:02,859 main.py:50] epoch 3064, training loss: 6091.22, average training loss: 6125.08, base loss: 20518.62
[INFO 2017-06-26 12:45:03,218 main.py:50] epoch 3065, training loss: 5975.03, average training loss: 6124.75, base loss: 20518.54
[INFO 2017-06-26 12:45:03,577 main.py:50] epoch 3066, training loss: 6060.90, average training loss: 6124.45, base loss: 20518.55
[INFO 2017-06-26 12:45:03,938 main.py:50] epoch 3067, training loss: 5934.68, average training loss: 6124.05, base loss: 20518.31
[INFO 2017-06-26 12:45:04,295 main.py:50] epoch 3068, training loss: 5984.72, average training loss: 6123.75, base loss: 20518.53
[INFO 2017-06-26 12:45:04,653 main.py:50] epoch 3069, training loss: 5965.06, average training loss: 6123.37, base loss: 20518.29
[INFO 2017-06-26 12:45:05,010 main.py:50] epoch 3070, training loss: 5892.36, average training loss: 6122.94, base loss: 20517.83
[INFO 2017-06-26 12:45:05,369 main.py:50] epoch 3071, training loss: 5999.68, average training loss: 6122.58, base loss: 20518.06
[INFO 2017-06-26 12:45:05,727 main.py:50] epoch 3072, training loss: 5980.09, average training loss: 6122.22, base loss: 20518.00
[INFO 2017-06-26 12:45:06,086 main.py:50] epoch 3073, training loss: 5974.78, average training loss: 6121.87, base loss: 20518.23
[INFO 2017-06-26 12:45:06,446 main.py:50] epoch 3074, training loss: 5996.73, average training loss: 6121.53, base loss: 20518.22
[INFO 2017-06-26 12:45:06,804 main.py:50] epoch 3075, training loss: 6039.48, average training loss: 6121.28, base loss: 20518.30
[INFO 2017-06-26 12:45:07,165 main.py:50] epoch 3076, training loss: 5948.40, average training loss: 6120.94, base loss: 20518.34
[INFO 2017-06-26 12:45:07,523 main.py:50] epoch 3077, training loss: 5965.40, average training loss: 6120.67, base loss: 20518.13
[INFO 2017-06-26 12:45:07,884 main.py:50] epoch 3078, training loss: 5949.67, average training loss: 6120.33, base loss: 20517.81
[INFO 2017-06-26 12:45:08,244 main.py:50] epoch 3079, training loss: 6007.13, average training loss: 6120.04, base loss: 20517.72
[INFO 2017-06-26 12:45:08,602 main.py:50] epoch 3080, training loss: 6025.78, average training loss: 6119.81, base loss: 20517.84
[INFO 2017-06-26 12:45:08,974 main.py:50] epoch 3081, training loss: 5949.72, average training loss: 6119.52, base loss: 20517.90
[INFO 2017-06-26 12:45:09,332 main.py:50] epoch 3082, training loss: 5977.04, average training loss: 6119.26, base loss: 20517.82
[INFO 2017-06-26 12:45:09,690 main.py:50] epoch 3083, training loss: 5998.59, average training loss: 6119.04, base loss: 20518.16
[INFO 2017-06-26 12:45:10,048 main.py:50] epoch 3084, training loss: 5910.08, average training loss: 6118.73, base loss: 20518.20
[INFO 2017-06-26 12:45:10,407 main.py:50] epoch 3085, training loss: 5967.44, average training loss: 6118.44, base loss: 20518.18
[INFO 2017-06-26 12:45:10,766 main.py:50] epoch 3086, training loss: 5993.03, average training loss: 6118.13, base loss: 20517.95
[INFO 2017-06-26 12:45:11,126 main.py:50] epoch 3087, training loss: 5949.13, average training loss: 6117.89, base loss: 20517.99
[INFO 2017-06-26 12:45:11,484 main.py:50] epoch 3088, training loss: 5977.78, average training loss: 6117.59, base loss: 20517.85
[INFO 2017-06-26 12:45:11,843 main.py:50] epoch 3089, training loss: 5974.08, average training loss: 6117.27, base loss: 20517.89
[INFO 2017-06-26 12:45:12,200 main.py:50] epoch 3090, training loss: 5986.25, average training loss: 6116.97, base loss: 20517.67
[INFO 2017-06-26 12:45:12,559 main.py:50] epoch 3091, training loss: 5987.25, average training loss: 6116.59, base loss: 20517.32
[INFO 2017-06-26 12:45:12,918 main.py:50] epoch 3092, training loss: 6066.60, average training loss: 6116.32, base loss: 20517.46
[INFO 2017-06-26 12:45:13,277 main.py:50] epoch 3093, training loss: 5988.06, average training loss: 6116.02, base loss: 20517.66
[INFO 2017-06-26 12:45:13,636 main.py:50] epoch 3094, training loss: 5957.00, average training loss: 6115.74, base loss: 20517.63
[INFO 2017-06-26 12:45:13,995 main.py:50] epoch 3095, training loss: 5966.24, average training loss: 6115.50, base loss: 20517.68
[INFO 2017-06-26 12:45:14,355 main.py:50] epoch 3096, training loss: 5948.21, average training loss: 6115.10, base loss: 20517.51
[INFO 2017-06-26 12:45:14,714 main.py:50] epoch 3097, training loss: 5969.82, average training loss: 6114.78, base loss: 20517.46
[INFO 2017-06-26 12:45:15,074 main.py:50] epoch 3098, training loss: 5974.56, average training loss: 6114.51, base loss: 20517.21
[INFO 2017-06-26 12:45:15,432 main.py:50] epoch 3099, training loss: 5915.97, average training loss: 6114.23, base loss: 20517.35
[INFO 2017-06-26 12:45:15,432 main.py:52] epoch 3099, testing
[INFO 2017-06-26 12:45:16,896 main.py:103] average testing loss: 5947.98, base loss: 20517.00
[INFO 2017-06-26 12:45:16,897 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 12:45:16,903 main.py:76] current best accuracy: 5947.98
[INFO 2017-06-26 12:45:17,260 main.py:50] epoch 3100, training loss: 5984.23, average training loss: 6113.86, base loss: 20517.35
[INFO 2017-06-26 12:45:17,618 main.py:50] epoch 3101, training loss: 6000.40, average training loss: 6113.52, base loss: 20517.29
[INFO 2017-06-26 12:45:17,978 main.py:50] epoch 3102, training loss: 5960.79, average training loss: 6113.25, base loss: 20517.41
[INFO 2017-06-26 12:45:18,336 main.py:50] epoch 3103, training loss: 5927.23, average training loss: 6112.90, base loss: 20517.37
[INFO 2017-06-26 12:45:18,696 main.py:50] epoch 3104, training loss: 5996.28, average training loss: 6112.66, base loss: 20517.51
[INFO 2017-06-26 12:45:19,054 main.py:50] epoch 3105, training loss: 5938.15, average training loss: 6112.35, base loss: 20517.11
[INFO 2017-06-26 12:45:19,413 main.py:50] epoch 3106, training loss: 5954.50, average training loss: 6112.01, base loss: 20517.19
[INFO 2017-06-26 12:45:19,771 main.py:50] epoch 3107, training loss: 5966.69, average training loss: 6111.73, base loss: 20517.11
[INFO 2017-06-26 12:45:20,130 main.py:50] epoch 3108, training loss: 5907.97, average training loss: 6111.42, base loss: 20517.08
[INFO 2017-06-26 12:45:20,489 main.py:50] epoch 3109, training loss: 5946.55, average training loss: 6111.13, base loss: 20516.65
[INFO 2017-06-26 12:45:20,848 main.py:50] epoch 3110, training loss: 6056.02, average training loss: 6110.99, base loss: 20516.91
[INFO 2017-06-26 12:45:21,208 main.py:50] epoch 3111, training loss: 5972.19, average training loss: 6110.77, base loss: 20517.28
[INFO 2017-06-26 12:45:21,567 main.py:50] epoch 3112, training loss: 6022.46, average training loss: 6110.51, base loss: 20517.29
[INFO 2017-06-26 12:45:21,926 main.py:50] epoch 3113, training loss: 5953.14, average training loss: 6110.20, base loss: 20517.18
[INFO 2017-06-26 12:45:22,284 main.py:50] epoch 3114, training loss: 5932.40, average training loss: 6109.89, base loss: 20517.09
[INFO 2017-06-26 12:45:22,643 main.py:50] epoch 3115, training loss: 5982.25, average training loss: 6109.64, base loss: 20516.97
[INFO 2017-06-26 12:45:23,000 main.py:50] epoch 3116, training loss: 6011.10, average training loss: 6109.39, base loss: 20517.31
[INFO 2017-06-26 12:45:23,359 main.py:50] epoch 3117, training loss: 5930.11, average training loss: 6109.08, base loss: 20517.16
[INFO 2017-06-26 12:45:23,716 main.py:50] epoch 3118, training loss: 5968.55, average training loss: 6108.80, base loss: 20517.34
[INFO 2017-06-26 12:45:24,075 main.py:50] epoch 3119, training loss: 5957.21, average training loss: 6108.53, base loss: 20517.48
[INFO 2017-06-26 12:45:24,434 main.py:50] epoch 3120, training loss: 5996.83, average training loss: 6108.38, base loss: 20517.96
[INFO 2017-06-26 12:45:24,793 main.py:50] epoch 3121, training loss: 5999.11, average training loss: 6108.14, base loss: 20518.18
[INFO 2017-06-26 12:45:25,150 main.py:50] epoch 3122, training loss: 6007.37, average training loss: 6107.81, base loss: 20518.16
[INFO 2017-06-26 12:45:25,510 main.py:50] epoch 3123, training loss: 6043.09, average training loss: 6107.56, base loss: 20518.17
[INFO 2017-06-26 12:45:25,868 main.py:50] epoch 3124, training loss: 5988.62, average training loss: 6107.34, base loss: 20518.25
[INFO 2017-06-26 12:45:26,225 main.py:50] epoch 3125, training loss: 5992.50, average training loss: 6107.08, base loss: 20518.39
[INFO 2017-06-26 12:45:26,582 main.py:50] epoch 3126, training loss: 5944.39, average training loss: 6106.71, base loss: 20518.18
[INFO 2017-06-26 12:45:26,941 main.py:50] epoch 3127, training loss: 6007.36, average training loss: 6106.46, base loss: 20518.15
[INFO 2017-06-26 12:45:27,299 main.py:50] epoch 3128, training loss: 5961.12, average training loss: 6106.12, base loss: 20517.75
[INFO 2017-06-26 12:45:27,657 main.py:50] epoch 3129, training loss: 5985.32, average training loss: 6105.85, base loss: 20517.76
[INFO 2017-06-26 12:45:28,015 main.py:50] epoch 3130, training loss: 6008.78, average training loss: 6105.61, base loss: 20517.73
[INFO 2017-06-26 12:45:28,373 main.py:50] epoch 3131, training loss: 5967.94, average training loss: 6105.29, base loss: 20517.53
[INFO 2017-06-26 12:45:28,732 main.py:50] epoch 3132, training loss: 5947.39, average training loss: 6104.94, base loss: 20517.18
[INFO 2017-06-26 12:45:29,090 main.py:50] epoch 3133, training loss: 6014.91, average training loss: 6104.68, base loss: 20517.21
[INFO 2017-06-26 12:45:29,449 main.py:50] epoch 3134, training loss: 5968.28, average training loss: 6104.38, base loss: 20517.26
[INFO 2017-06-26 12:45:29,809 main.py:50] epoch 3135, training loss: 5963.80, average training loss: 6104.11, base loss: 20517.50
[INFO 2017-06-26 12:45:30,169 main.py:50] epoch 3136, training loss: 5941.52, average training loss: 6103.75, base loss: 20517.44
[INFO 2017-06-26 12:45:30,527 main.py:50] epoch 3137, training loss: 6013.54, average training loss: 6103.47, base loss: 20517.60
[INFO 2017-06-26 12:45:30,888 main.py:50] epoch 3138, training loss: 5900.56, average training loss: 6103.16, base loss: 20517.57
[INFO 2017-06-26 12:45:31,247 main.py:50] epoch 3139, training loss: 5938.13, average training loss: 6102.73, base loss: 20517.27
[INFO 2017-06-26 12:45:31,605 main.py:50] epoch 3140, training loss: 5959.85, average training loss: 6102.37, base loss: 20517.25
[INFO 2017-06-26 12:45:31,965 main.py:50] epoch 3141, training loss: 5931.09, average training loss: 6102.09, base loss: 20517.20
[INFO 2017-06-26 12:45:32,323 main.py:50] epoch 3142, training loss: 5969.29, average training loss: 6101.81, base loss: 20517.51
[INFO 2017-06-26 12:45:32,683 main.py:50] epoch 3143, training loss: 5943.97, average training loss: 6101.49, base loss: 20517.44
[INFO 2017-06-26 12:45:33,040 main.py:50] epoch 3144, training loss: 5950.72, average training loss: 6101.15, base loss: 20517.16
[INFO 2017-06-26 12:45:33,401 main.py:50] epoch 3145, training loss: 5987.48, average training loss: 6100.81, base loss: 20517.11
[INFO 2017-06-26 12:45:33,760 main.py:50] epoch 3146, training loss: 6018.43, average training loss: 6100.59, base loss: 20516.84
[INFO 2017-06-26 12:45:34,119 main.py:50] epoch 3147, training loss: 5930.84, average training loss: 6100.22, base loss: 20517.03
[INFO 2017-06-26 12:45:34,478 main.py:50] epoch 3148, training loss: 5925.26, average training loss: 6099.85, base loss: 20517.12
[INFO 2017-06-26 12:45:34,835 main.py:50] epoch 3149, training loss: 5984.15, average training loss: 6099.56, base loss: 20516.84
[INFO 2017-06-26 12:45:35,195 main.py:50] epoch 3150, training loss: 5946.88, average training loss: 6099.18, base loss: 20516.56
[INFO 2017-06-26 12:45:35,553 main.py:50] epoch 3151, training loss: 5975.38, average training loss: 6098.89, base loss: 20516.45
[INFO 2017-06-26 12:45:35,912 main.py:50] epoch 3152, training loss: 5960.67, average training loss: 6098.59, base loss: 20516.73
[INFO 2017-06-26 12:45:36,271 main.py:50] epoch 3153, training loss: 6093.22, average training loss: 6098.45, base loss: 20517.29
[INFO 2017-06-26 12:45:36,631 main.py:50] epoch 3154, training loss: 6002.57, average training loss: 6098.15, base loss: 20517.68
[INFO 2017-06-26 12:45:36,990 main.py:50] epoch 3155, training loss: 6088.90, average training loss: 6098.00, base loss: 20517.91
[INFO 2017-06-26 12:45:37,347 main.py:50] epoch 3156, training loss: 6053.03, average training loss: 6097.81, base loss: 20518.31
[INFO 2017-06-26 12:45:37,707 main.py:50] epoch 3157, training loss: 5990.38, average training loss: 6097.49, base loss: 20518.35
[INFO 2017-06-26 12:45:38,066 main.py:50] epoch 3158, training loss: 5991.59, average training loss: 6097.23, base loss: 20518.59
[INFO 2017-06-26 12:45:38,426 main.py:50] epoch 3159, training loss: 5916.13, average training loss: 6096.94, base loss: 20518.84
[INFO 2017-06-26 12:45:38,784 main.py:50] epoch 3160, training loss: 5937.92, average training loss: 6096.57, base loss: 20518.79
[INFO 2017-06-26 12:45:39,143 main.py:50] epoch 3161, training loss: 5883.70, average training loss: 6096.26, base loss: 20518.86
[INFO 2017-06-26 12:45:39,501 main.py:50] epoch 3162, training loss: 5953.95, average training loss: 6095.93, base loss: 20519.28
[INFO 2017-06-26 12:45:39,860 main.py:50] epoch 3163, training loss: 5869.87, average training loss: 6095.53, base loss: 20519.23
[INFO 2017-06-26 12:45:40,219 main.py:50] epoch 3164, training loss: 5986.70, average training loss: 6095.26, base loss: 20519.17
[INFO 2017-06-26 12:45:40,578 main.py:50] epoch 3165, training loss: 5958.44, average training loss: 6094.98, base loss: 20519.64
[INFO 2017-06-26 12:45:40,937 main.py:50] epoch 3166, training loss: 6005.17, average training loss: 6094.77, base loss: 20520.01
[INFO 2017-06-26 12:45:41,298 main.py:50] epoch 3167, training loss: 5917.24, average training loss: 6094.40, base loss: 20520.15
[INFO 2017-06-26 12:45:41,656 main.py:50] epoch 3168, training loss: 5973.01, average training loss: 6094.10, base loss: 20520.17
[INFO 2017-06-26 12:45:42,015 main.py:50] epoch 3169, training loss: 5960.17, average training loss: 6093.83, base loss: 20520.04
[INFO 2017-06-26 12:45:42,374 main.py:50] epoch 3170, training loss: 5967.72, average training loss: 6093.49, base loss: 20520.37
[INFO 2017-06-26 12:45:42,733 main.py:50] epoch 3171, training loss: 5923.10, average training loss: 6093.10, base loss: 20519.82
[INFO 2017-06-26 12:45:43,092 main.py:50] epoch 3172, training loss: 5925.97, average training loss: 6092.73, base loss: 20519.55
[INFO 2017-06-26 12:45:43,450 main.py:50] epoch 3173, training loss: 5945.51, average training loss: 6092.45, base loss: 20519.34
[INFO 2017-06-26 12:45:43,811 main.py:50] epoch 3174, training loss: 5964.18, average training loss: 6092.17, base loss: 20519.41
[INFO 2017-06-26 12:45:44,169 main.py:50] epoch 3175, training loss: 5988.52, average training loss: 6091.92, base loss: 20519.40
[INFO 2017-06-26 12:45:44,530 main.py:50] epoch 3176, training loss: 5911.57, average training loss: 6091.63, base loss: 20519.50
[INFO 2017-06-26 12:45:44,888 main.py:50] epoch 3177, training loss: 5949.14, average training loss: 6091.35, base loss: 20519.45
[INFO 2017-06-26 12:45:45,246 main.py:50] epoch 3178, training loss: 6015.74, average training loss: 6091.13, base loss: 20519.47
[INFO 2017-06-26 12:45:45,607 main.py:50] epoch 3179, training loss: 5949.58, average training loss: 6090.83, base loss: 20519.22
[INFO 2017-06-26 12:45:45,966 main.py:50] epoch 3180, training loss: 5914.04, average training loss: 6090.56, base loss: 20519.30
[INFO 2017-06-26 12:45:46,324 main.py:50] epoch 3181, training loss: 5931.51, average training loss: 6090.26, base loss: 20519.20
[INFO 2017-06-26 12:45:46,681 main.py:50] epoch 3182, training loss: 5943.22, average training loss: 6090.01, base loss: 20519.61
[INFO 2017-06-26 12:45:47,038 main.py:50] epoch 3183, training loss: 5974.17, average training loss: 6089.73, base loss: 20519.13
[INFO 2017-06-26 12:45:47,397 main.py:50] epoch 3184, training loss: 5985.55, average training loss: 6089.50, base loss: 20519.24
[INFO 2017-06-26 12:45:47,756 main.py:50] epoch 3185, training loss: 5948.90, average training loss: 6089.15, base loss: 20519.55
[INFO 2017-06-26 12:45:48,114 main.py:50] epoch 3186, training loss: 5945.57, average training loss: 6088.97, base loss: 20519.77
[INFO 2017-06-26 12:45:48,474 main.py:50] epoch 3187, training loss: 6022.21, average training loss: 6088.67, base loss: 20519.58
[INFO 2017-06-26 12:45:48,833 main.py:50] epoch 3188, training loss: 5935.59, average training loss: 6088.33, base loss: 20519.32
[INFO 2017-06-26 12:45:49,192 main.py:50] epoch 3189, training loss: 6039.62, average training loss: 6088.09, base loss: 20519.79
[INFO 2017-06-26 12:45:49,552 main.py:50] epoch 3190, training loss: 5916.93, average training loss: 6087.69, base loss: 20519.96
[INFO 2017-06-26 12:45:49,912 main.py:50] epoch 3191, training loss: 5966.40, average training loss: 6087.42, base loss: 20519.85
[INFO 2017-06-26 12:45:50,272 main.py:50] epoch 3192, training loss: 5922.92, average training loss: 6087.06, base loss: 20519.97
[INFO 2017-06-26 12:45:50,630 main.py:50] epoch 3193, training loss: 5994.97, average training loss: 6086.77, base loss: 20519.76
[INFO 2017-06-26 12:45:50,988 main.py:50] epoch 3194, training loss: 5981.47, average training loss: 6086.54, base loss: 20519.64
[INFO 2017-06-26 12:45:51,346 main.py:50] epoch 3195, training loss: 5976.17, average training loss: 6086.26, base loss: 20519.32
[INFO 2017-06-26 12:45:51,705 main.py:50] epoch 3196, training loss: 5901.71, average training loss: 6085.91, base loss: 20519.03
[INFO 2017-06-26 12:45:52,069 main.py:50] epoch 3197, training loss: 5949.53, average training loss: 6085.59, base loss: 20519.12
[INFO 2017-06-26 12:45:52,428 main.py:50] epoch 3198, training loss: 5932.51, average training loss: 6085.35, base loss: 20519.12
[INFO 2017-06-26 12:45:52,788 main.py:50] epoch 3199, training loss: 5997.65, average training loss: 6085.09, base loss: 20519.38
[INFO 2017-06-26 12:45:52,788 main.py:52] epoch 3199, testing
[INFO 2017-06-26 12:45:54,269 main.py:103] average testing loss: 5977.28, base loss: 20461.65
[INFO 2017-06-26 12:45:54,269 main.py:76] current best accuracy: 5947.98
[INFO 2017-06-26 12:45:54,628 main.py:50] epoch 3200, training loss: 6004.99, average training loss: 6084.85, base loss: 20519.59
[INFO 2017-06-26 12:45:54,988 main.py:50] epoch 3201, training loss: 5856.70, average training loss: 6084.42, base loss: 20519.11
[INFO 2017-06-26 12:45:55,347 main.py:50] epoch 3202, training loss: 5948.00, average training loss: 6084.09, base loss: 20518.74
[INFO 2017-06-26 12:45:55,706 main.py:50] epoch 3203, training loss: 5938.39, average training loss: 6083.82, base loss: 20518.52
[INFO 2017-06-26 12:45:56,064 main.py:50] epoch 3204, training loss: 6012.67, average training loss: 6083.57, base loss: 20518.47
[INFO 2017-06-26 12:45:56,425 main.py:50] epoch 3205, training loss: 5955.95, average training loss: 6083.31, base loss: 20518.49
[INFO 2017-06-26 12:45:56,784 main.py:50] epoch 3206, training loss: 6026.45, average training loss: 6083.13, base loss: 20518.56
[INFO 2017-06-26 12:45:57,143 main.py:50] epoch 3207, training loss: 5930.36, average training loss: 6082.78, base loss: 20518.56
[INFO 2017-06-26 12:45:57,502 main.py:50] epoch 3208, training loss: 6004.61, average training loss: 6082.55, base loss: 20518.33
[INFO 2017-06-26 12:45:57,861 main.py:50] epoch 3209, training loss: 5973.68, average training loss: 6082.28, base loss: 20518.29
[INFO 2017-06-26 12:45:58,219 main.py:50] epoch 3210, training loss: 6011.91, average training loss: 6082.16, base loss: 20518.47
[INFO 2017-06-26 12:45:58,578 main.py:50] epoch 3211, training loss: 6022.98, average training loss: 6081.97, base loss: 20518.92
[INFO 2017-06-26 12:45:58,937 main.py:50] epoch 3212, training loss: 5989.60, average training loss: 6081.69, base loss: 20519.04
[INFO 2017-06-26 12:45:59,296 main.py:50] epoch 3213, training loss: 5912.58, average training loss: 6081.41, base loss: 20518.91
[INFO 2017-06-26 12:45:59,655 main.py:50] epoch 3214, training loss: 5969.54, average training loss: 6081.20, base loss: 20519.03
[INFO 2017-06-26 12:46:00,015 main.py:50] epoch 3215, training loss: 5925.46, average training loss: 6080.77, base loss: 20518.57
[INFO 2017-06-26 12:46:00,374 main.py:50] epoch 3216, training loss: 5901.24, average training loss: 6080.51, base loss: 20518.97
[INFO 2017-06-26 12:46:00,732 main.py:50] epoch 3217, training loss: 6031.14, average training loss: 6080.24, base loss: 20518.94
[INFO 2017-06-26 12:46:01,093 main.py:50] epoch 3218, training loss: 5853.78, average training loss: 6079.86, base loss: 20518.69
[INFO 2017-06-26 12:46:01,452 main.py:50] epoch 3219, training loss: 5989.40, average training loss: 6079.54, base loss: 20518.43
[INFO 2017-06-26 12:46:01,811 main.py:50] epoch 3220, training loss: 5892.48, average training loss: 6079.20, base loss: 20518.36
[INFO 2017-06-26 12:46:02,172 main.py:50] epoch 3221, training loss: 6008.83, average training loss: 6078.94, base loss: 20518.39
[INFO 2017-06-26 12:46:02,529 main.py:50] epoch 3222, training loss: 5840.41, average training loss: 6078.52, base loss: 20517.86
[INFO 2017-06-26 12:46:02,889 main.py:50] epoch 3223, training loss: 6013.51, average training loss: 6078.23, base loss: 20518.09
[INFO 2017-06-26 12:46:03,250 main.py:50] epoch 3224, training loss: 5971.14, average training loss: 6077.96, base loss: 20517.97
[INFO 2017-06-26 12:46:03,609 main.py:50] epoch 3225, training loss: 5916.57, average training loss: 6077.52, base loss: 20517.56
[INFO 2017-06-26 12:46:03,969 main.py:50] epoch 3226, training loss: 6011.89, average training loss: 6077.41, base loss: 20518.29
[INFO 2017-06-26 12:46:04,328 main.py:50] epoch 3227, training loss: 5946.83, average training loss: 6077.16, base loss: 20518.66
[INFO 2017-06-26 12:46:04,687 main.py:50] epoch 3228, training loss: 5971.96, average training loss: 6076.90, base loss: 20518.44
[INFO 2017-06-26 12:46:05,045 main.py:50] epoch 3229, training loss: 5881.82, average training loss: 6076.53, base loss: 20518.24
[INFO 2017-06-26 12:46:05,405 main.py:50] epoch 3230, training loss: 5945.17, average training loss: 6076.27, base loss: 20518.45
[INFO 2017-06-26 12:46:05,763 main.py:50] epoch 3231, training loss: 5949.30, average training loss: 6075.96, base loss: 20518.43
[INFO 2017-06-26 12:46:06,122 main.py:50] epoch 3232, training loss: 5886.82, average training loss: 6075.58, base loss: 20518.30
[INFO 2017-06-26 12:46:06,481 main.py:50] epoch 3233, training loss: 6014.43, average training loss: 6075.34, base loss: 20518.02
[INFO 2017-06-26 12:46:06,840 main.py:50] epoch 3234, training loss: 5968.50, average training loss: 6075.14, base loss: 20518.21
[INFO 2017-06-26 12:46:07,199 main.py:50] epoch 3235, training loss: 5871.02, average training loss: 6074.86, base loss: 20518.23
[INFO 2017-06-26 12:46:07,558 main.py:50] epoch 3236, training loss: 5956.00, average training loss: 6074.61, base loss: 20518.11
[INFO 2017-06-26 12:46:07,918 main.py:50] epoch 3237, training loss: 5924.28, average training loss: 6074.30, base loss: 20518.24
[INFO 2017-06-26 12:46:08,277 main.py:50] epoch 3238, training loss: 5912.63, average training loss: 6073.98, base loss: 20518.11
[INFO 2017-06-26 12:46:08,636 main.py:50] epoch 3239, training loss: 5937.78, average training loss: 6073.67, base loss: 20518.45
[INFO 2017-06-26 12:46:08,995 main.py:50] epoch 3240, training loss: 5915.25, average training loss: 6073.35, base loss: 20518.17
[INFO 2017-06-26 12:46:09,354 main.py:50] epoch 3241, training loss: 5976.80, average training loss: 6073.05, base loss: 20517.94
[INFO 2017-06-26 12:46:09,712 main.py:50] epoch 3242, training loss: 5950.22, average training loss: 6072.78, base loss: 20518.25
[INFO 2017-06-26 12:46:10,069 main.py:50] epoch 3243, training loss: 5940.23, average training loss: 6072.53, base loss: 20518.72
[INFO 2017-06-26 12:46:10,428 main.py:50] epoch 3244, training loss: 5917.95, average training loss: 6072.18, base loss: 20518.82
[INFO 2017-06-26 12:46:10,788 main.py:50] epoch 3245, training loss: 5880.20, average training loss: 6071.89, base loss: 20518.90
[INFO 2017-06-26 12:46:11,147 main.py:50] epoch 3246, training loss: 5984.35, average training loss: 6071.69, base loss: 20518.99
[INFO 2017-06-26 12:46:11,506 main.py:50] epoch 3247, training loss: 5910.41, average training loss: 6071.38, base loss: 20519.02
[INFO 2017-06-26 12:46:11,865 main.py:50] epoch 3248, training loss: 5902.00, average training loss: 6071.02, base loss: 20519.24
[INFO 2017-06-26 12:46:12,222 main.py:50] epoch 3249, training loss: 5883.90, average training loss: 6070.71, base loss: 20519.11
[INFO 2017-06-26 12:46:12,583 main.py:50] epoch 3250, training loss: 6042.38, average training loss: 6070.46, base loss: 20519.32
[INFO 2017-06-26 12:46:12,943 main.py:50] epoch 3251, training loss: 6009.89, average training loss: 6070.26, base loss: 20519.42
[INFO 2017-06-26 12:46:13,302 main.py:50] epoch 3252, training loss: 5885.76, average training loss: 6069.95, base loss: 20519.57
[INFO 2017-06-26 12:46:13,662 main.py:50] epoch 3253, training loss: 5949.26, average training loss: 6069.63, base loss: 20519.15
[INFO 2017-06-26 12:46:14,021 main.py:50] epoch 3254, training loss: 5887.08, average training loss: 6069.35, base loss: 20518.80
[INFO 2017-06-26 12:46:14,380 main.py:50] epoch 3255, training loss: 5927.99, average training loss: 6069.09, base loss: 20518.36
[INFO 2017-06-26 12:46:14,740 main.py:50] epoch 3256, training loss: 5963.03, average training loss: 6068.81, base loss: 20518.13
[INFO 2017-06-26 12:46:15,099 main.py:50] epoch 3257, training loss: 6025.93, average training loss: 6068.67, base loss: 20517.96
[INFO 2017-06-26 12:46:15,459 main.py:50] epoch 3258, training loss: 5959.31, average training loss: 6068.41, base loss: 20517.78
[INFO 2017-06-26 12:46:15,818 main.py:50] epoch 3259, training loss: 6098.23, average training loss: 6068.34, base loss: 20518.07
[INFO 2017-06-26 12:46:16,178 main.py:50] epoch 3260, training loss: 5956.27, average training loss: 6068.12, base loss: 20517.88
[INFO 2017-06-26 12:46:16,537 main.py:50] epoch 3261, training loss: 5849.48, average training loss: 6067.78, base loss: 20517.93
[INFO 2017-06-26 12:46:16,896 main.py:50] epoch 3262, training loss: 5980.37, average training loss: 6067.64, base loss: 20518.01
[INFO 2017-06-26 12:46:17,255 main.py:50] epoch 3263, training loss: 5988.48, average training loss: 6067.47, base loss: 20517.96
[INFO 2017-06-26 12:46:17,614 main.py:50] epoch 3264, training loss: 5945.55, average training loss: 6067.21, base loss: 20517.68
[INFO 2017-06-26 12:46:17,973 main.py:50] epoch 3265, training loss: 5927.45, average training loss: 6067.00, base loss: 20517.62
[INFO 2017-06-26 12:46:18,332 main.py:50] epoch 3266, training loss: 5871.40, average training loss: 6066.69, base loss: 20517.41
[INFO 2017-06-26 12:46:18,689 main.py:50] epoch 3267, training loss: 5960.81, average training loss: 6066.48, base loss: 20517.90
[INFO 2017-06-26 12:46:19,048 main.py:50] epoch 3268, training loss: 5927.09, average training loss: 6066.20, base loss: 20517.75
[INFO 2017-06-26 12:46:19,408 main.py:50] epoch 3269, training loss: 5996.95, average training loss: 6065.96, base loss: 20517.55
[INFO 2017-06-26 12:46:19,767 main.py:50] epoch 3270, training loss: 5926.86, average training loss: 6065.65, base loss: 20517.29
[INFO 2017-06-26 12:46:20,127 main.py:50] epoch 3271, training loss: 5997.44, average training loss: 6065.47, base loss: 20517.33
[INFO 2017-06-26 12:46:20,487 main.py:50] epoch 3272, training loss: 5933.87, average training loss: 6065.24, base loss: 20517.30
[INFO 2017-06-26 12:46:20,848 main.py:50] epoch 3273, training loss: 5979.97, average training loss: 6065.03, base loss: 20517.41
[INFO 2017-06-26 12:46:21,207 main.py:50] epoch 3274, training loss: 5927.08, average training loss: 6064.75, base loss: 20517.02
[INFO 2017-06-26 12:46:21,566 main.py:50] epoch 3275, training loss: 5969.86, average training loss: 6064.52, base loss: 20516.98
[INFO 2017-06-26 12:46:21,926 main.py:50] epoch 3276, training loss: 5905.72, average training loss: 6064.31, base loss: 20516.87
[INFO 2017-06-26 12:46:22,285 main.py:50] epoch 3277, training loss: 5925.63, average training loss: 6064.08, base loss: 20517.40
[INFO 2017-06-26 12:46:22,644 main.py:50] epoch 3278, training loss: 5982.93, average training loss: 6063.78, base loss: 20517.36
[INFO 2017-06-26 12:46:23,004 main.py:50] epoch 3279, training loss: 5974.04, average training loss: 6063.55, base loss: 20517.33
[INFO 2017-06-26 12:46:23,363 main.py:50] epoch 3280, training loss: 5953.17, average training loss: 6063.30, base loss: 20517.68
[INFO 2017-06-26 12:46:23,722 main.py:50] epoch 3281, training loss: 5956.87, average training loss: 6062.97, base loss: 20517.58
[INFO 2017-06-26 12:46:24,081 main.py:50] epoch 3282, training loss: 5988.31, average training loss: 6062.79, base loss: 20517.79
[INFO 2017-06-26 12:46:24,442 main.py:50] epoch 3283, training loss: 5939.96, average training loss: 6062.48, base loss: 20517.84
[INFO 2017-06-26 12:46:24,801 main.py:50] epoch 3284, training loss: 5913.98, average training loss: 6062.20, base loss: 20517.87
[INFO 2017-06-26 12:46:25,161 main.py:50] epoch 3285, training loss: 5971.84, average training loss: 6061.99, base loss: 20517.63
[INFO 2017-06-26 12:46:25,521 main.py:50] epoch 3286, training loss: 5982.50, average training loss: 6061.76, base loss: 20517.85
[INFO 2017-06-26 12:46:25,880 main.py:50] epoch 3287, training loss: 6058.91, average training loss: 6061.54, base loss: 20518.39
[INFO 2017-06-26 12:46:26,237 main.py:50] epoch 3288, training loss: 5983.06, average training loss: 6061.29, base loss: 20518.26
[INFO 2017-06-26 12:46:26,597 main.py:50] epoch 3289, training loss: 6000.74, average training loss: 6061.03, base loss: 20518.28
[INFO 2017-06-26 12:46:26,957 main.py:50] epoch 3290, training loss: 5902.52, average training loss: 6060.77, base loss: 20518.21
[INFO 2017-06-26 12:46:27,316 main.py:50] epoch 3291, training loss: 5973.08, average training loss: 6060.60, base loss: 20518.32
[INFO 2017-06-26 12:46:27,676 main.py:50] epoch 3292, training loss: 5901.69, average training loss: 6060.29, base loss: 20518.09
[INFO 2017-06-26 12:46:28,033 main.py:50] epoch 3293, training loss: 5959.80, average training loss: 6060.05, base loss: 20518.02
[INFO 2017-06-26 12:46:28,394 main.py:50] epoch 3294, training loss: 5906.43, average training loss: 6059.70, base loss: 20517.64
[INFO 2017-06-26 12:46:28,752 main.py:50] epoch 3295, training loss: 5931.24, average training loss: 6059.49, base loss: 20517.64
[INFO 2017-06-26 12:46:29,111 main.py:50] epoch 3296, training loss: 5937.38, average training loss: 6059.21, base loss: 20517.61
[INFO 2017-06-26 12:46:29,470 main.py:50] epoch 3297, training loss: 5881.33, average training loss: 6058.91, base loss: 20517.33
[INFO 2017-06-26 12:46:29,831 main.py:50] epoch 3298, training loss: 5968.12, average training loss: 6058.71, base loss: 20517.32
[INFO 2017-06-26 12:46:30,188 main.py:50] epoch 3299, training loss: 5955.69, average training loss: 6058.45, base loss: 20517.44
[INFO 2017-06-26 12:46:30,189 main.py:52] epoch 3299, testing
[INFO 2017-06-26 12:46:31,655 main.py:103] average testing loss: 5938.60, base loss: 20378.77
[INFO 2017-06-26 12:46:31,656 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 12:46:31,662 main.py:76] current best accuracy: 5938.60
[INFO 2017-06-26 12:46:32,019 main.py:50] epoch 3300, training loss: 5910.87, average training loss: 6058.16, base loss: 20517.30
[INFO 2017-06-26 12:46:32,378 main.py:50] epoch 3301, training loss: 5978.90, average training loss: 6057.86, base loss: 20517.30
[INFO 2017-06-26 12:46:32,738 main.py:50] epoch 3302, training loss: 5860.47, average training loss: 6057.47, base loss: 20517.20
[INFO 2017-06-26 12:46:33,095 main.py:50] epoch 3303, training loss: 5949.69, average training loss: 6057.19, base loss: 20517.18
[INFO 2017-06-26 12:46:33,454 main.py:50] epoch 3304, training loss: 5954.18, average training loss: 6056.96, base loss: 20517.19
[INFO 2017-06-26 12:46:33,812 main.py:50] epoch 3305, training loss: 5915.58, average training loss: 6056.66, base loss: 20517.16
[INFO 2017-06-26 12:46:34,173 main.py:50] epoch 3306, training loss: 5987.26, average training loss: 6056.36, base loss: 20517.00
[INFO 2017-06-26 12:46:34,542 main.py:50] epoch 3307, training loss: 5913.35, average training loss: 6056.02, base loss: 20516.72
[INFO 2017-06-26 12:46:34,900 main.py:50] epoch 3308, training loss: 5973.23, average training loss: 6055.78, base loss: 20516.59
[INFO 2017-06-26 12:46:35,258 main.py:50] epoch 3309, training loss: 5925.42, average training loss: 6055.48, base loss: 20516.81
[INFO 2017-06-26 12:46:35,615 main.py:50] epoch 3310, training loss: 5943.63, average training loss: 6055.15, base loss: 20516.68
[INFO 2017-06-26 12:46:35,972 main.py:50] epoch 3311, training loss: 5941.13, average training loss: 6054.92, base loss: 20516.84
[INFO 2017-06-26 12:46:36,329 main.py:50] epoch 3312, training loss: 5911.58, average training loss: 6054.47, base loss: 20517.00
[INFO 2017-06-26 12:46:36,687 main.py:50] epoch 3313, training loss: 5963.77, average training loss: 6054.19, base loss: 20516.46
[INFO 2017-06-26 12:46:37,044 main.py:50] epoch 3314, training loss: 5979.52, average training loss: 6053.98, base loss: 20516.58
[INFO 2017-06-26 12:46:37,405 main.py:50] epoch 3315, training loss: 5940.28, average training loss: 6053.79, base loss: 20516.52
[INFO 2017-06-26 12:46:37,763 main.py:50] epoch 3316, training loss: 5907.35, average training loss: 6053.50, base loss: 20516.30
[INFO 2017-06-26 12:46:38,121 main.py:50] epoch 3317, training loss: 5955.23, average training loss: 6053.25, base loss: 20516.44
[INFO 2017-06-26 12:46:38,480 main.py:50] epoch 3318, training loss: 5903.29, average training loss: 6053.00, base loss: 20516.53
[INFO 2017-06-26 12:46:38,839 main.py:50] epoch 3319, training loss: 5901.80, average training loss: 6052.68, base loss: 20516.87
[INFO 2017-06-26 12:46:39,198 main.py:50] epoch 3320, training loss: 5958.08, average training loss: 6052.40, base loss: 20517.00
[INFO 2017-06-26 12:46:39,557 main.py:50] epoch 3321, training loss: 5946.57, average training loss: 6052.12, base loss: 20517.05
[INFO 2017-06-26 12:46:39,917 main.py:50] epoch 3322, training loss: 5950.50, average training loss: 6051.84, base loss: 20517.02
[INFO 2017-06-26 12:46:40,276 main.py:50] epoch 3323, training loss: 5929.27, average training loss: 6051.55, base loss: 20517.24
[INFO 2017-06-26 12:46:40,635 main.py:50] epoch 3324, training loss: 5996.51, average training loss: 6051.36, base loss: 20517.52
[INFO 2017-06-26 12:46:40,995 main.py:50] epoch 3325, training loss: 5920.84, average training loss: 6051.17, base loss: 20517.62
[INFO 2017-06-26 12:46:41,354 main.py:50] epoch 3326, training loss: 5943.13, average training loss: 6050.90, base loss: 20517.88
[INFO 2017-06-26 12:46:41,713 main.py:50] epoch 3327, training loss: 5900.93, average training loss: 6050.69, base loss: 20517.84
[INFO 2017-06-26 12:46:42,072 main.py:50] epoch 3328, training loss: 5925.12, average training loss: 6050.38, base loss: 20518.32
[INFO 2017-06-26 12:46:42,431 main.py:50] epoch 3329, training loss: 5976.45, average training loss: 6050.14, base loss: 20518.13
[INFO 2017-06-26 12:46:42,791 main.py:50] epoch 3330, training loss: 5946.25, average training loss: 6049.92, base loss: 20518.13
[INFO 2017-06-26 12:46:43,151 main.py:50] epoch 3331, training loss: 5996.76, average training loss: 6049.63, base loss: 20517.75
[INFO 2017-06-26 12:46:43,508 main.py:50] epoch 3332, training loss: 5928.71, average training loss: 6049.38, base loss: 20518.06
[INFO 2017-06-26 12:46:43,868 main.py:50] epoch 3333, training loss: 5965.07, average training loss: 6049.09, base loss: 20517.91
[INFO 2017-06-26 12:46:44,226 main.py:50] epoch 3334, training loss: 5966.99, average training loss: 6048.86, base loss: 20517.80
[INFO 2017-06-26 12:46:44,585 main.py:50] epoch 3335, training loss: 5856.98, average training loss: 6048.45, base loss: 20517.69
[INFO 2017-06-26 12:46:44,943 main.py:50] epoch 3336, training loss: 5905.04, average training loss: 6048.13, base loss: 20517.46
[INFO 2017-06-26 12:46:45,301 main.py:50] epoch 3337, training loss: 5878.73, average training loss: 6047.82, base loss: 20517.35
[INFO 2017-06-26 12:46:45,660 main.py:50] epoch 3338, training loss: 5939.36, average training loss: 6047.47, base loss: 20517.13
[INFO 2017-06-26 12:46:46,019 main.py:50] epoch 3339, training loss: 5918.05, average training loss: 6047.21, base loss: 20517.51
[INFO 2017-06-26 12:46:46,378 main.py:50] epoch 3340, training loss: 5888.77, average training loss: 6046.85, base loss: 20517.33
[INFO 2017-06-26 12:46:46,737 main.py:50] epoch 3341, training loss: 5937.75, average training loss: 6046.53, base loss: 20517.38
[INFO 2017-06-26 12:46:47,096 main.py:50] epoch 3342, training loss: 5934.41, average training loss: 6046.23, base loss: 20517.73
[INFO 2017-06-26 12:46:47,454 main.py:50] epoch 3343, training loss: 5908.74, average training loss: 6046.02, base loss: 20517.82
[INFO 2017-06-26 12:46:47,812 main.py:50] epoch 3344, training loss: 5823.26, average training loss: 6045.57, base loss: 20517.57
[INFO 2017-06-26 12:46:48,172 main.py:50] epoch 3345, training loss: 5882.25, average training loss: 6045.29, base loss: 20517.14
[INFO 2017-06-26 12:46:48,529 main.py:50] epoch 3346, training loss: 5949.01, average training loss: 6045.06, base loss: 20517.39
[INFO 2017-06-26 12:46:48,890 main.py:50] epoch 3347, training loss: 5910.00, average training loss: 6044.80, base loss: 20517.17
[INFO 2017-06-26 12:46:49,249 main.py:50] epoch 3348, training loss: 5898.49, average training loss: 6044.49, base loss: 20517.03
[INFO 2017-06-26 12:46:49,607 main.py:50] epoch 3349, training loss: 5918.77, average training loss: 6044.16, base loss: 20516.83
[INFO 2017-06-26 12:46:49,967 main.py:50] epoch 3350, training loss: 5886.84, average training loss: 6043.90, base loss: 20516.78
[INFO 2017-06-26 12:46:50,324 main.py:50] epoch 3351, training loss: 5921.13, average training loss: 6043.60, base loss: 20516.70
[INFO 2017-06-26 12:46:50,683 main.py:50] epoch 3352, training loss: 5884.64, average training loss: 6043.36, base loss: 20517.12
[INFO 2017-06-26 12:46:51,043 main.py:50] epoch 3353, training loss: 5895.52, average training loss: 6043.11, base loss: 20517.11
[INFO 2017-06-26 12:46:51,401 main.py:50] epoch 3354, training loss: 5890.49, average training loss: 6042.80, base loss: 20516.75
[INFO 2017-06-26 12:46:51,761 main.py:50] epoch 3355, training loss: 5928.52, average training loss: 6042.54, base loss: 20517.14
[INFO 2017-06-26 12:46:52,121 main.py:50] epoch 3356, training loss: 5871.07, average training loss: 6042.29, base loss: 20517.39
[INFO 2017-06-26 12:46:52,482 main.py:50] epoch 3357, training loss: 5938.40, average training loss: 6042.05, base loss: 20517.53
[INFO 2017-06-26 12:46:52,841 main.py:50] epoch 3358, training loss: 5907.01, average training loss: 6041.81, base loss: 20517.49
[INFO 2017-06-26 12:46:53,201 main.py:50] epoch 3359, training loss: 5953.49, average training loss: 6041.64, base loss: 20517.74
[INFO 2017-06-26 12:46:53,562 main.py:50] epoch 3360, training loss: 5914.74, average training loss: 6041.38, base loss: 20517.61
[INFO 2017-06-26 12:46:53,920 main.py:50] epoch 3361, training loss: 5875.39, average training loss: 6041.17, base loss: 20517.72
[INFO 2017-06-26 12:46:54,278 main.py:50] epoch 3362, training loss: 5855.98, average training loss: 6040.84, base loss: 20517.49
[INFO 2017-06-26 12:46:54,637 main.py:50] epoch 3363, training loss: 5906.53, average training loss: 6040.60, base loss: 20517.26
[INFO 2017-06-26 12:46:54,996 main.py:50] epoch 3364, training loss: 5912.15, average training loss: 6040.37, base loss: 20516.97
[INFO 2017-06-26 12:46:55,353 main.py:50] epoch 3365, training loss: 5897.67, average training loss: 6040.18, base loss: 20517.05
[INFO 2017-06-26 12:46:55,713 main.py:50] epoch 3366, training loss: 5885.93, average training loss: 6039.97, base loss: 20516.92
[INFO 2017-06-26 12:46:56,073 main.py:50] epoch 3367, training loss: 5922.05, average training loss: 6039.80, base loss: 20516.90
[INFO 2017-06-26 12:46:56,433 main.py:50] epoch 3368, training loss: 5902.47, average training loss: 6039.57, base loss: 20516.67
[INFO 2017-06-26 12:46:56,791 main.py:50] epoch 3369, training loss: 5927.31, average training loss: 6039.38, base loss: 20516.71
[INFO 2017-06-26 12:46:57,152 main.py:50] epoch 3370, training loss: 5951.11, average training loss: 6039.15, base loss: 20517.15
[INFO 2017-06-26 12:46:57,509 main.py:50] epoch 3371, training loss: 5854.32, average training loss: 6038.86, base loss: 20516.62
[INFO 2017-06-26 12:46:57,869 main.py:50] epoch 3372, training loss: 5893.82, average training loss: 6038.60, base loss: 20516.69
[INFO 2017-06-26 12:46:58,227 main.py:50] epoch 3373, training loss: 5916.29, average training loss: 6038.35, base loss: 20517.16
[INFO 2017-06-26 12:46:58,586 main.py:50] epoch 3374, training loss: 5976.15, average training loss: 6038.14, base loss: 20517.63
[INFO 2017-06-26 12:46:58,945 main.py:50] epoch 3375, training loss: 5914.34, average training loss: 6037.98, base loss: 20517.80
[INFO 2017-06-26 12:46:59,309 main.py:50] epoch 3376, training loss: 5935.17, average training loss: 6037.77, base loss: 20518.15
[INFO 2017-06-26 12:46:59,670 main.py:50] epoch 3377, training loss: 5944.23, average training loss: 6037.53, base loss: 20518.23
[INFO 2017-06-26 12:47:00,028 main.py:50] epoch 3378, training loss: 5949.20, average training loss: 6037.41, base loss: 20518.40
[INFO 2017-06-26 12:47:00,386 main.py:50] epoch 3379, training loss: 5947.01, average training loss: 6037.20, base loss: 20518.20
[INFO 2017-06-26 12:47:00,745 main.py:50] epoch 3380, training loss: 5916.45, average training loss: 6036.95, base loss: 20518.50
[INFO 2017-06-26 12:47:01,104 main.py:50] epoch 3381, training loss: 5898.03, average training loss: 6036.56, base loss: 20518.52
[INFO 2017-06-26 12:47:01,462 main.py:50] epoch 3382, training loss: 5910.27, average training loss: 6036.26, base loss: 20518.21
[INFO 2017-06-26 12:47:01,821 main.py:50] epoch 3383, training loss: 5942.75, average training loss: 6036.05, base loss: 20518.21
[INFO 2017-06-26 12:47:02,180 main.py:50] epoch 3384, training loss: 5936.00, average training loss: 6035.84, base loss: 20518.28
[INFO 2017-06-26 12:47:02,540 main.py:50] epoch 3385, training loss: 5966.41, average training loss: 6035.66, base loss: 20518.28
[INFO 2017-06-26 12:47:02,900 main.py:50] epoch 3386, training loss: 5941.13, average training loss: 6035.46, base loss: 20518.63
[INFO 2017-06-26 12:47:03,259 main.py:50] epoch 3387, training loss: 5908.45, average training loss: 6035.24, base loss: 20518.67
[INFO 2017-06-26 12:47:03,616 main.py:50] epoch 3388, training loss: 5880.14, average training loss: 6034.92, base loss: 20518.53
[INFO 2017-06-26 12:47:03,975 main.py:50] epoch 3389, training loss: 5905.36, average training loss: 6034.72, base loss: 20518.76
[INFO 2017-06-26 12:47:04,333 main.py:50] epoch 3390, training loss: 5868.26, average training loss: 6034.41, base loss: 20518.93
[INFO 2017-06-26 12:47:04,692 main.py:50] epoch 3391, training loss: 5871.85, average training loss: 6034.14, base loss: 20519.11
[INFO 2017-06-26 12:47:05,051 main.py:50] epoch 3392, training loss: 5871.28, average training loss: 6033.88, base loss: 20518.98
[INFO 2017-06-26 12:47:05,410 main.py:50] epoch 3393, training loss: 5899.73, average training loss: 6033.68, base loss: 20519.02
[INFO 2017-06-26 12:47:05,773 main.py:50] epoch 3394, training loss: 5916.23, average training loss: 6033.45, base loss: 20519.27
[INFO 2017-06-26 12:47:06,132 main.py:50] epoch 3395, training loss: 5862.44, average training loss: 6033.15, base loss: 20519.18
[INFO 2017-06-26 12:47:06,493 main.py:50] epoch 3396, training loss: 5854.27, average training loss: 6032.77, base loss: 20518.97
[INFO 2017-06-26 12:47:06,853 main.py:50] epoch 3397, training loss: 5851.20, average training loss: 6032.46, base loss: 20518.50
[INFO 2017-06-26 12:47:07,210 main.py:50] epoch 3398, training loss: 5895.01, average training loss: 6032.18, base loss: 20518.15
[INFO 2017-06-26 12:47:07,568 main.py:50] epoch 3399, training loss: 5908.37, average training loss: 6031.88, base loss: 20518.14
[INFO 2017-06-26 12:47:07,569 main.py:52] epoch 3399, testing
[INFO 2017-06-26 12:47:09,034 main.py:103] average testing loss: 5898.00, base loss: 20557.25
[INFO 2017-06-26 12:47:09,035 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 12:47:09,041 main.py:76] current best accuracy: 5898.00
[INFO 2017-06-26 12:47:09,399 main.py:50] epoch 3400, training loss: 5943.69, average training loss: 6031.68, base loss: 20517.83
[INFO 2017-06-26 12:47:09,759 main.py:50] epoch 3401, training loss: 5912.82, average training loss: 6031.32, base loss: 20517.75
[INFO 2017-06-26 12:47:10,119 main.py:50] epoch 3402, training loss: 5928.52, average training loss: 6031.05, base loss: 20517.51
[INFO 2017-06-26 12:47:10,479 main.py:50] epoch 3403, training loss: 5971.03, average training loss: 6030.77, base loss: 20517.33
[INFO 2017-06-26 12:47:10,840 main.py:50] epoch 3404, training loss: 5929.27, average training loss: 6030.48, base loss: 20517.47
[INFO 2017-06-26 12:47:11,200 main.py:50] epoch 3405, training loss: 5927.28, average training loss: 6030.24, base loss: 20517.36
[INFO 2017-06-26 12:47:11,559 main.py:50] epoch 3406, training loss: 5890.39, average training loss: 6029.97, base loss: 20516.89
[INFO 2017-06-26 12:47:11,917 main.py:50] epoch 3407, training loss: 5899.36, average training loss: 6029.72, base loss: 20517.05
[INFO 2017-06-26 12:47:12,276 main.py:50] epoch 3408, training loss: 5899.77, average training loss: 6029.41, base loss: 20517.30
[INFO 2017-06-26 12:47:12,635 main.py:50] epoch 3409, training loss: 5930.07, average training loss: 6029.19, base loss: 20517.50
[INFO 2017-06-26 12:47:12,994 main.py:50] epoch 3410, training loss: 5878.43, average training loss: 6028.88, base loss: 20517.38
[INFO 2017-06-26 12:47:13,352 main.py:50] epoch 3411, training loss: 5940.41, average training loss: 6028.61, base loss: 20516.78
[INFO 2017-06-26 12:47:13,712 main.py:50] epoch 3412, training loss: 5954.93, average training loss: 6028.39, base loss: 20516.85
[INFO 2017-06-26 12:47:14,072 main.py:50] epoch 3413, training loss: 5910.65, average training loss: 6028.12, base loss: 20516.89
[INFO 2017-06-26 12:47:14,431 main.py:50] epoch 3414, training loss: 5946.49, average training loss: 6027.83, base loss: 20516.59
[INFO 2017-06-26 12:47:14,791 main.py:50] epoch 3415, training loss: 5932.65, average training loss: 6027.59, base loss: 20516.16
[INFO 2017-06-26 12:47:15,152 main.py:50] epoch 3416, training loss: 5973.69, average training loss: 6027.36, base loss: 20516.30
[INFO 2017-06-26 12:47:15,510 main.py:50] epoch 3417, training loss: 5951.15, average training loss: 6027.06, base loss: 20516.43
[INFO 2017-06-26 12:47:15,872 main.py:50] epoch 3418, training loss: 5970.21, average training loss: 6026.85, base loss: 20516.49
[INFO 2017-06-26 12:47:16,229 main.py:50] epoch 3419, training loss: 5943.75, average training loss: 6026.66, base loss: 20516.88
[INFO 2017-06-26 12:47:16,588 main.py:50] epoch 3420, training loss: 5903.41, average training loss: 6026.44, base loss: 20516.81
[INFO 2017-06-26 12:47:16,947 main.py:50] epoch 3421, training loss: 5897.47, average training loss: 6026.22, base loss: 20517.00
[INFO 2017-06-26 12:47:17,307 main.py:50] epoch 3422, training loss: 5911.45, average training loss: 6026.00, base loss: 20517.50
[INFO 2017-06-26 12:47:17,666 main.py:50] epoch 3423, training loss: 5879.90, average training loss: 6025.72, base loss: 20516.92
[INFO 2017-06-26 12:47:18,032 main.py:50] epoch 3424, training loss: 5961.98, average training loss: 6025.49, base loss: 20516.98
[INFO 2017-06-26 12:47:18,390 main.py:50] epoch 3425, training loss: 5893.83, average training loss: 6025.26, base loss: 20516.52
[INFO 2017-06-26 12:47:18,763 main.py:50] epoch 3426, training loss: 5917.49, average training loss: 6025.06, base loss: 20516.50
[INFO 2017-06-26 12:47:19,123 main.py:50] epoch 3427, training loss: 5936.14, average training loss: 6024.84, base loss: 20516.42
[INFO 2017-06-26 12:47:19,482 main.py:50] epoch 3428, training loss: 6005.23, average training loss: 6024.67, base loss: 20516.93
[INFO 2017-06-26 12:47:19,842 main.py:50] epoch 3429, training loss: 5888.03, average training loss: 6024.44, base loss: 20516.90
[INFO 2017-06-26 12:47:20,202 main.py:50] epoch 3430, training loss: 5920.07, average training loss: 6024.24, base loss: 20516.96
[INFO 2017-06-26 12:47:20,561 main.py:50] epoch 3431, training loss: 5863.49, average training loss: 6023.95, base loss: 20516.69
[INFO 2017-06-26 12:47:20,919 main.py:50] epoch 3432, training loss: 5869.58, average training loss: 6023.69, base loss: 20516.68
[INFO 2017-06-26 12:47:21,280 main.py:50] epoch 3433, training loss: 5959.65, average training loss: 6023.48, base loss: 20516.74
[INFO 2017-06-26 12:47:21,640 main.py:50] epoch 3434, training loss: 5950.22, average training loss: 6023.34, base loss: 20516.81
[INFO 2017-06-26 12:47:21,999 main.py:50] epoch 3435, training loss: 5840.44, average training loss: 6023.04, base loss: 20517.00
[INFO 2017-06-26 12:47:22,359 main.py:50] epoch 3436, training loss: 5886.07, average training loss: 6022.84, base loss: 20516.95
[INFO 2017-06-26 12:47:22,717 main.py:50] epoch 3437, training loss: 5910.90, average training loss: 6022.64, base loss: 20517.22
[INFO 2017-06-26 12:47:23,078 main.py:50] epoch 3438, training loss: 5936.09, average training loss: 6022.40, base loss: 20517.14
[INFO 2017-06-26 12:47:23,436 main.py:50] epoch 3439, training loss: 5985.44, average training loss: 6022.29, base loss: 20517.40
[INFO 2017-06-26 12:47:23,795 main.py:50] epoch 3440, training loss: 5929.20, average training loss: 6022.12, base loss: 20517.58
[INFO 2017-06-26 12:47:24,156 main.py:50] epoch 3441, training loss: 5843.92, average training loss: 6021.86, base loss: 20517.51
[INFO 2017-06-26 12:47:24,514 main.py:50] epoch 3442, training loss: 5994.75, average training loss: 6021.80, base loss: 20518.12
[INFO 2017-06-26 12:47:24,874 main.py:50] epoch 3443, training loss: 5826.48, average training loss: 6021.50, base loss: 20518.00
[INFO 2017-06-26 12:47:25,233 main.py:50] epoch 3444, training loss: 5912.42, average training loss: 6021.28, base loss: 20517.96
[INFO 2017-06-26 12:47:25,593 main.py:50] epoch 3445, training loss: 5878.61, average training loss: 6021.14, base loss: 20518.18
[INFO 2017-06-26 12:47:25,952 main.py:50] epoch 3446, training loss: 5882.79, average training loss: 6020.84, base loss: 20518.25
[INFO 2017-06-26 12:47:26,313 main.py:50] epoch 3447, training loss: 5849.15, average training loss: 6020.57, base loss: 20518.33
[INFO 2017-06-26 12:47:26,671 main.py:50] epoch 3448, training loss: 5927.02, average training loss: 6020.45, base loss: 20518.92
[INFO 2017-06-26 12:47:27,030 main.py:50] epoch 3449, training loss: 5959.62, average training loss: 6020.23, base loss: 20518.76
[INFO 2017-06-26 12:47:27,391 main.py:50] epoch 3450, training loss: 5927.35, average training loss: 6020.04, base loss: 20518.97
[INFO 2017-06-26 12:47:27,749 main.py:50] epoch 3451, training loss: 5937.88, average training loss: 6019.85, base loss: 20519.09
[INFO 2017-06-26 12:47:28,110 main.py:50] epoch 3452, training loss: 5920.16, average training loss: 6019.57, base loss: 20519.08
[INFO 2017-06-26 12:47:28,470 main.py:50] epoch 3453, training loss: 5918.09, average training loss: 6019.41, base loss: 20519.52
[INFO 2017-06-26 12:47:28,828 main.py:50] epoch 3454, training loss: 5856.18, average training loss: 6019.14, base loss: 20519.67
[INFO 2017-06-26 12:47:29,187 main.py:50] epoch 3455, training loss: 5910.57, average training loss: 6018.90, base loss: 20519.57
[INFO 2017-06-26 12:47:29,548 main.py:50] epoch 3456, training loss: 5907.80, average training loss: 6018.67, base loss: 20519.80
[INFO 2017-06-26 12:47:29,906 main.py:50] epoch 3457, training loss: 5890.20, average training loss: 6018.40, base loss: 20519.79
[INFO 2017-06-26 12:47:30,264 main.py:50] epoch 3458, training loss: 6021.02, average training loss: 6018.30, base loss: 20519.61
[INFO 2017-06-26 12:47:30,623 main.py:50] epoch 3459, training loss: 5895.74, average training loss: 6018.07, base loss: 20519.69
[INFO 2017-06-26 12:47:30,982 main.py:50] epoch 3460, training loss: 6012.13, average training loss: 6017.96, base loss: 20519.97
[INFO 2017-06-26 12:47:31,340 main.py:50] epoch 3461, training loss: 5911.38, average training loss: 6017.78, base loss: 20520.02
[INFO 2017-06-26 12:47:31,701 main.py:50] epoch 3462, training loss: 5938.84, average training loss: 6017.62, base loss: 20519.84
[INFO 2017-06-26 12:47:32,058 main.py:50] epoch 3463, training loss: 5872.16, average training loss: 6017.32, base loss: 20519.74
[INFO 2017-06-26 12:47:32,420 main.py:50] epoch 3464, training loss: 5954.45, average training loss: 6017.07, base loss: 20519.44
[INFO 2017-06-26 12:47:32,777 main.py:50] epoch 3465, training loss: 5932.49, average training loss: 6016.88, base loss: 20519.43
[INFO 2017-06-26 12:47:33,137 main.py:50] epoch 3466, training loss: 5914.75, average training loss: 6016.68, base loss: 20519.11
[INFO 2017-06-26 12:47:33,497 main.py:50] epoch 3467, training loss: 5941.27, average training loss: 6016.57, base loss: 20519.23
[INFO 2017-06-26 12:47:33,854 main.py:50] epoch 3468, training loss: 5885.50, average training loss: 6016.34, base loss: 20519.67
[INFO 2017-06-26 12:47:34,214 main.py:50] epoch 3469, training loss: 5914.97, average training loss: 6016.09, base loss: 20519.49
[INFO 2017-06-26 12:47:34,573 main.py:50] epoch 3470, training loss: 5866.82, average training loss: 6015.85, base loss: 20519.42
[INFO 2017-06-26 12:47:34,932 main.py:50] epoch 3471, training loss: 5983.80, average training loss: 6015.69, base loss: 20519.83
[INFO 2017-06-26 12:47:35,291 main.py:50] epoch 3472, training loss: 5939.83, average training loss: 6015.51, base loss: 20520.06
[INFO 2017-06-26 12:47:35,649 main.py:50] epoch 3473, training loss: 5890.97, average training loss: 6015.26, base loss: 20520.16
[INFO 2017-06-26 12:47:36,009 main.py:50] epoch 3474, training loss: 5922.82, average training loss: 6015.00, base loss: 20520.04
[INFO 2017-06-26 12:47:36,368 main.py:50] epoch 3475, training loss: 5871.69, average training loss: 6014.79, base loss: 20519.95
[INFO 2017-06-26 12:47:36,726 main.py:50] epoch 3476, training loss: 5888.23, average training loss: 6014.54, base loss: 20519.53
[INFO 2017-06-26 12:47:37,086 main.py:50] epoch 3477, training loss: 5945.28, average training loss: 6014.33, base loss: 20519.94
[INFO 2017-06-26 12:47:37,446 main.py:50] epoch 3478, training loss: 5831.77, average training loss: 6014.05, base loss: 20519.50
[INFO 2017-06-26 12:47:37,805 main.py:50] epoch 3479, training loss: 5911.42, average training loss: 6013.81, base loss: 20519.41
[INFO 2017-06-26 12:47:38,165 main.py:50] epoch 3480, training loss: 5912.72, average training loss: 6013.51, base loss: 20518.69
[INFO 2017-06-26 12:47:38,524 main.py:50] epoch 3481, training loss: 5913.22, average training loss: 6013.22, base loss: 20518.93
[INFO 2017-06-26 12:47:38,881 main.py:50] epoch 3482, training loss: 5958.02, average training loss: 6013.09, base loss: 20519.28
[INFO 2017-06-26 12:47:39,240 main.py:50] epoch 3483, training loss: 5945.62, average training loss: 6012.83, base loss: 20519.32
[INFO 2017-06-26 12:47:39,600 main.py:50] epoch 3484, training loss: 5877.67, average training loss: 6012.54, base loss: 20519.02
[INFO 2017-06-26 12:47:39,959 main.py:50] epoch 3485, training loss: 5868.94, average training loss: 6012.30, base loss: 20519.25
[INFO 2017-06-26 12:47:40,318 main.py:50] epoch 3486, training loss: 5981.58, average training loss: 6012.04, base loss: 20519.06
[INFO 2017-06-26 12:47:40,676 main.py:50] epoch 3487, training loss: 5981.76, average training loss: 6011.90, base loss: 20519.37
[INFO 2017-06-26 12:47:41,035 main.py:50] epoch 3488, training loss: 5861.32, average training loss: 6011.60, base loss: 20519.48
[INFO 2017-06-26 12:47:41,396 main.py:50] epoch 3489, training loss: 5936.27, average training loss: 6011.42, base loss: 20519.72
[INFO 2017-06-26 12:47:41,755 main.py:50] epoch 3490, training loss: 5844.17, average training loss: 6011.19, base loss: 20519.74
[INFO 2017-06-26 12:47:42,113 main.py:50] epoch 3491, training loss: 5852.82, average training loss: 6010.96, base loss: 20519.71
[INFO 2017-06-26 12:47:42,473 main.py:50] epoch 3492, training loss: 5917.29, average training loss: 6010.83, base loss: 20519.75
[INFO 2017-06-26 12:47:42,831 main.py:50] epoch 3493, training loss: 5919.25, average training loss: 6010.67, base loss: 20519.63
[INFO 2017-06-26 12:47:43,191 main.py:50] epoch 3494, training loss: 5848.65, average training loss: 6010.36, base loss: 20519.27
[INFO 2017-06-26 12:47:43,551 main.py:50] epoch 3495, training loss: 5890.41, average training loss: 6010.16, base loss: 20518.98
[INFO 2017-06-26 12:47:43,910 main.py:50] epoch 3496, training loss: 5897.52, average training loss: 6010.02, base loss: 20519.30
[INFO 2017-06-26 12:47:44,269 main.py:50] epoch 3497, training loss: 5846.03, average training loss: 6009.67, base loss: 20518.94
[INFO 2017-06-26 12:47:44,627 main.py:50] epoch 3498, training loss: 5855.46, average training loss: 6009.37, base loss: 20518.71
[INFO 2017-06-26 12:47:44,986 main.py:50] epoch 3499, training loss: 5838.19, average training loss: 6009.10, base loss: 20518.79
[INFO 2017-06-26 12:47:44,987 main.py:52] epoch 3499, testing
[INFO 2017-06-26 12:47:46,453 main.py:103] average testing loss: 5862.86, base loss: 20475.07
[INFO 2017-06-26 12:47:46,453 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 12:47:46,459 main.py:76] current best accuracy: 5862.86
[INFO 2017-06-26 12:47:46,818 main.py:50] epoch 3500, training loss: 5800.31, average training loss: 6008.79, base loss: 20518.87
[INFO 2017-06-26 12:47:47,178 main.py:50] epoch 3501, training loss: 5916.24, average training loss: 6008.58, base loss: 20518.64
[INFO 2017-06-26 12:47:47,537 main.py:50] epoch 3502, training loss: 5949.37, average training loss: 6008.42, base loss: 20519.07
[INFO 2017-06-26 12:47:47,897 main.py:50] epoch 3503, training loss: 5826.73, average training loss: 6008.15, base loss: 20519.03
[INFO 2017-06-26 12:47:48,256 main.py:50] epoch 3504, training loss: 5871.87, average training loss: 6007.91, base loss: 20519.00
[INFO 2017-06-26 12:47:48,614 main.py:50] epoch 3505, training loss: 5804.98, average training loss: 6007.62, base loss: 20518.92
[INFO 2017-06-26 12:47:48,973 main.py:50] epoch 3506, training loss: 5839.60, average training loss: 6007.32, base loss: 20518.73
[INFO 2017-06-26 12:47:49,332 main.py:50] epoch 3507, training loss: 5846.72, average training loss: 6007.05, base loss: 20519.12
[INFO 2017-06-26 12:47:49,692 main.py:50] epoch 3508, training loss: 5882.77, average training loss: 6006.74, base loss: 20519.24
[INFO 2017-06-26 12:47:50,050 main.py:50] epoch 3509, training loss: 5884.95, average training loss: 6006.46, base loss: 20518.92
[INFO 2017-06-26 12:47:50,410 main.py:50] epoch 3510, training loss: 5877.71, average training loss: 6006.17, base loss: 20519.09
[INFO 2017-06-26 12:47:50,768 main.py:50] epoch 3511, training loss: 5882.98, average training loss: 6006.00, base loss: 20519.14
[INFO 2017-06-26 12:47:51,128 main.py:50] epoch 3512, training loss: 5836.32, average training loss: 6005.71, base loss: 20518.95
[INFO 2017-06-26 12:47:51,487 main.py:50] epoch 3513, training loss: 5816.90, average training loss: 6005.37, base loss: 20518.91
[INFO 2017-06-26 12:47:51,846 main.py:50] epoch 3514, training loss: 5872.18, average training loss: 6005.04, base loss: 20518.87
[INFO 2017-06-26 12:47:52,205 main.py:50] epoch 3515, training loss: 5857.55, average training loss: 6004.79, base loss: 20518.78
[INFO 2017-06-26 12:47:52,563 main.py:50] epoch 3516, training loss: 5839.74, average training loss: 6004.52, base loss: 20518.45
[INFO 2017-06-26 12:47:52,923 main.py:50] epoch 3517, training loss: 5919.31, average training loss: 6004.29, base loss: 20518.39
[INFO 2017-06-26 12:47:53,281 main.py:50] epoch 3518, training loss: 5851.80, average training loss: 6003.94, base loss: 20517.71
[INFO 2017-06-26 12:47:53,641 main.py:50] epoch 3519, training loss: 5853.79, average training loss: 6003.62, base loss: 20517.36
[INFO 2017-06-26 12:47:53,999 main.py:50] epoch 3520, training loss: 5932.17, average training loss: 6003.42, base loss: 20517.51
[INFO 2017-06-26 12:47:54,357 main.py:50] epoch 3521, training loss: 5874.02, average training loss: 6003.15, base loss: 20516.77
[INFO 2017-06-26 12:47:54,715 main.py:50] epoch 3522, training loss: 5793.44, average training loss: 6002.81, base loss: 20516.37
[INFO 2017-06-26 12:47:55,073 main.py:50] epoch 3523, training loss: 5910.39, average training loss: 6002.61, base loss: 20516.19
[INFO 2017-06-26 12:47:55,432 main.py:50] epoch 3524, training loss: 5861.68, average training loss: 6002.32, base loss: 20515.99
[INFO 2017-06-26 12:47:55,791 main.py:50] epoch 3525, training loss: 5860.61, average training loss: 6002.05, base loss: 20515.70
[INFO 2017-06-26 12:47:56,149 main.py:50] epoch 3526, training loss: 5884.35, average training loss: 6001.77, base loss: 20515.50
[INFO 2017-06-26 12:47:56,507 main.py:50] epoch 3527, training loss: 5866.81, average training loss: 6001.54, base loss: 20515.63
[INFO 2017-06-26 12:47:56,865 main.py:50] epoch 3528, training loss: 5945.53, average training loss: 6001.38, base loss: 20515.81
[INFO 2017-06-26 12:47:57,224 main.py:50] epoch 3529, training loss: 5851.72, average training loss: 6001.13, base loss: 20515.81
[INFO 2017-06-26 12:47:57,583 main.py:50] epoch 3530, training loss: 5915.35, average training loss: 6000.91, base loss: 20515.66
[INFO 2017-06-26 12:47:57,941 main.py:50] epoch 3531, training loss: 5830.30, average training loss: 6000.56, base loss: 20515.52
[INFO 2017-06-26 12:47:58,302 main.py:50] epoch 3532, training loss: 5954.45, average training loss: 6000.41, base loss: 20516.02
[INFO 2017-06-26 12:47:58,659 main.py:50] epoch 3533, training loss: 5870.17, average training loss: 6000.17, base loss: 20515.91
[INFO 2017-06-26 12:47:59,019 main.py:50] epoch 3534, training loss: 5953.57, average training loss: 6000.03, base loss: 20515.86
[INFO 2017-06-26 12:47:59,378 main.py:50] epoch 3535, training loss: 5895.93, average training loss: 5999.82, base loss: 20515.71
[INFO 2017-06-26 12:47:59,737 main.py:50] epoch 3536, training loss: 5915.27, average training loss: 5999.64, base loss: 20515.87
[INFO 2017-06-26 12:48:00,097 main.py:50] epoch 3537, training loss: 5941.21, average training loss: 5999.48, base loss: 20516.02
[INFO 2017-06-26 12:48:00,457 main.py:50] epoch 3538, training loss: 5856.33, average training loss: 5999.24, base loss: 20516.13
[INFO 2017-06-26 12:48:00,814 main.py:50] epoch 3539, training loss: 5949.24, average training loss: 5999.11, base loss: 20515.64
[INFO 2017-06-26 12:48:01,175 main.py:50] epoch 3540, training loss: 5863.77, average training loss: 5998.85, base loss: 20515.65
[INFO 2017-06-26 12:48:01,533 main.py:50] epoch 3541, training loss: 5816.30, average training loss: 5998.52, base loss: 20515.27
[INFO 2017-06-26 12:48:01,895 main.py:50] epoch 3542, training loss: 5946.15, average training loss: 5998.44, base loss: 20515.69
[INFO 2017-06-26 12:48:02,256 main.py:50] epoch 3543, training loss: 5915.66, average training loss: 5998.30, base loss: 20515.68
[INFO 2017-06-26 12:48:02,617 main.py:50] epoch 3544, training loss: 5946.95, average training loss: 5998.14, base loss: 20516.23
[INFO 2017-06-26 12:48:02,990 main.py:50] epoch 3545, training loss: 5886.28, average training loss: 5997.90, base loss: 20516.23
[INFO 2017-06-26 12:48:03,352 main.py:50] epoch 3546, training loss: 5917.93, average training loss: 5997.63, base loss: 20516.35
[INFO 2017-06-26 12:48:03,714 main.py:50] epoch 3547, training loss: 5826.98, average training loss: 5997.25, base loss: 20515.93
[INFO 2017-06-26 12:48:04,073 main.py:50] epoch 3548, training loss: 5939.84, average training loss: 5997.09, base loss: 20516.03
[INFO 2017-06-26 12:48:04,433 main.py:50] epoch 3549, training loss: 5893.28, average training loss: 5996.86, base loss: 20516.00
[INFO 2017-06-26 12:48:04,791 main.py:50] epoch 3550, training loss: 5887.82, average training loss: 5996.63, base loss: 20515.85
[INFO 2017-06-26 12:48:05,150 main.py:50] epoch 3551, training loss: 5831.19, average training loss: 5996.33, base loss: 20515.55
[INFO 2017-06-26 12:48:05,507 main.py:50] epoch 3552, training loss: 5876.90, average training loss: 5996.13, base loss: 20515.58
[INFO 2017-06-26 12:48:05,868 main.py:50] epoch 3553, training loss: 5912.93, average training loss: 5995.87, base loss: 20515.66
[INFO 2017-06-26 12:48:06,227 main.py:50] epoch 3554, training loss: 5918.91, average training loss: 5995.69, base loss: 20515.80
[INFO 2017-06-26 12:48:06,586 main.py:50] epoch 3555, training loss: 5914.62, average training loss: 5995.53, base loss: 20515.92
[INFO 2017-06-26 12:48:06,945 main.py:50] epoch 3556, training loss: 5958.76, average training loss: 5995.41, base loss: 20516.08
[INFO 2017-06-26 12:48:07,303 main.py:50] epoch 3557, training loss: 5850.48, average training loss: 5995.19, base loss: 20516.30
[INFO 2017-06-26 12:48:07,661 main.py:50] epoch 3558, training loss: 5852.18, average training loss: 5994.95, base loss: 20516.09
[INFO 2017-06-26 12:48:08,019 main.py:50] epoch 3559, training loss: 5934.36, average training loss: 5994.87, base loss: 20516.42
[INFO 2017-06-26 12:48:08,378 main.py:50] epoch 3560, training loss: 5871.65, average training loss: 5994.61, base loss: 20516.39
[INFO 2017-06-26 12:48:08,738 main.py:50] epoch 3561, training loss: 5859.34, average training loss: 5994.40, base loss: 20516.03
[INFO 2017-06-26 12:48:09,097 main.py:50] epoch 3562, training loss: 5950.36, average training loss: 5994.29, base loss: 20516.11
[INFO 2017-06-26 12:48:09,456 main.py:50] epoch 3563, training loss: 5887.13, average training loss: 5994.13, base loss: 20516.19
[INFO 2017-06-26 12:48:09,816 main.py:50] epoch 3564, training loss: 5934.50, average training loss: 5994.00, base loss: 20515.80
[INFO 2017-06-26 12:48:10,175 main.py:50] epoch 3565, training loss: 5923.54, average training loss: 5993.84, base loss: 20515.88
[INFO 2017-06-26 12:48:10,534 main.py:50] epoch 3566, training loss: 5801.80, average training loss: 5993.55, base loss: 20516.19
[INFO 2017-06-26 12:48:10,893 main.py:50] epoch 3567, training loss: 5919.61, average training loss: 5993.45, base loss: 20516.37
[INFO 2017-06-26 12:48:11,252 main.py:50] epoch 3568, training loss: 5863.84, average training loss: 5993.13, base loss: 20516.18
[INFO 2017-06-26 12:48:11,611 main.py:50] epoch 3569, training loss: 5932.77, average training loss: 5993.00, base loss: 20516.00
[INFO 2017-06-26 12:48:11,970 main.py:50] epoch 3570, training loss: 5871.96, average training loss: 5992.86, base loss: 20516.07
[INFO 2017-06-26 12:48:12,329 main.py:50] epoch 3571, training loss: 5960.26, average training loss: 5992.74, base loss: 20516.52
[INFO 2017-06-26 12:48:12,688 main.py:50] epoch 3572, training loss: 5843.66, average training loss: 5992.49, base loss: 20516.60
[INFO 2017-06-26 12:48:13,047 main.py:50] epoch 3573, training loss: 5954.26, average training loss: 5992.37, base loss: 20516.93
[INFO 2017-06-26 12:48:13,406 main.py:50] epoch 3574, training loss: 5951.07, average training loss: 5992.19, base loss: 20516.63
[INFO 2017-06-26 12:48:13,765 main.py:50] epoch 3575, training loss: 5845.11, average training loss: 5991.95, base loss: 20516.75
[INFO 2017-06-26 12:48:14,124 main.py:50] epoch 3576, training loss: 5901.29, average training loss: 5991.78, base loss: 20516.94
[INFO 2017-06-26 12:48:14,483 main.py:50] epoch 3577, training loss: 5994.74, average training loss: 5991.71, base loss: 20517.29
[INFO 2017-06-26 12:48:14,842 main.py:50] epoch 3578, training loss: 5959.79, average training loss: 5991.49, base loss: 20517.15
[INFO 2017-06-26 12:48:15,201 main.py:50] epoch 3579, training loss: 5809.23, average training loss: 5991.18, base loss: 20517.02
[INFO 2017-06-26 12:48:15,560 main.py:50] epoch 3580, training loss: 5829.31, average training loss: 5990.98, base loss: 20517.27
[INFO 2017-06-26 12:48:15,919 main.py:50] epoch 3581, training loss: 5905.14, average training loss: 5990.79, base loss: 20517.47
[INFO 2017-06-26 12:48:16,278 main.py:50] epoch 3582, training loss: 5904.00, average training loss: 5990.55, base loss: 20517.28
[INFO 2017-06-26 12:48:16,636 main.py:50] epoch 3583, training loss: 5955.50, average training loss: 5990.32, base loss: 20517.05
[INFO 2017-06-26 12:48:16,995 main.py:50] epoch 3584, training loss: 5896.41, average training loss: 5990.08, base loss: 20516.86
[INFO 2017-06-26 12:48:17,354 main.py:50] epoch 3585, training loss: 5955.23, average training loss: 5989.90, base loss: 20517.20
[INFO 2017-06-26 12:48:17,713 main.py:50] epoch 3586, training loss: 5978.85, average training loss: 5989.77, base loss: 20517.45
[INFO 2017-06-26 12:48:18,072 main.py:50] epoch 3587, training loss: 5927.64, average training loss: 5989.58, base loss: 20517.52
[INFO 2017-06-26 12:48:18,429 main.py:50] epoch 3588, training loss: 5978.90, average training loss: 5989.44, base loss: 20517.99
[INFO 2017-06-26 12:48:18,790 main.py:50] epoch 3589, training loss: 5880.24, average training loss: 5989.20, base loss: 20517.86
[INFO 2017-06-26 12:48:19,149 main.py:50] epoch 3590, training loss: 5848.60, average training loss: 5988.92, base loss: 20517.68
[INFO 2017-06-26 12:48:19,508 main.py:50] epoch 3591, training loss: 5938.66, average training loss: 5988.77, base loss: 20517.96
[INFO 2017-06-26 12:48:19,867 main.py:50] epoch 3592, training loss: 5870.43, average training loss: 5988.48, base loss: 20517.82
[INFO 2017-06-26 12:48:20,226 main.py:50] epoch 3593, training loss: 5857.47, average training loss: 5988.17, base loss: 20517.87
[INFO 2017-06-26 12:48:20,584 main.py:50] epoch 3594, training loss: 5853.09, average training loss: 5987.89, base loss: 20517.94
[INFO 2017-06-26 12:48:20,943 main.py:50] epoch 3595, training loss: 5822.16, average training loss: 5987.55, base loss: 20518.02
[INFO 2017-06-26 12:48:21,302 main.py:50] epoch 3596, training loss: 5847.25, average training loss: 5987.35, base loss: 20518.09
[INFO 2017-06-26 12:48:21,660 main.py:50] epoch 3597, training loss: 5826.95, average training loss: 5987.12, base loss: 20518.36
[INFO 2017-06-26 12:48:22,020 main.py:50] epoch 3598, training loss: 5937.00, average training loss: 5986.96, base loss: 20518.29
[INFO 2017-06-26 12:48:22,378 main.py:50] epoch 3599, training loss: 5834.41, average training loss: 5986.76, base loss: 20518.27
[INFO 2017-06-26 12:48:22,378 main.py:52] epoch 3599, testing
[INFO 2017-06-26 12:48:23,845 main.py:103] average testing loss: 5867.66, base loss: 20629.06
[INFO 2017-06-26 12:48:23,846 main.py:76] current best accuracy: 5862.86
[INFO 2017-06-26 12:48:24,204 main.py:50] epoch 3600, training loss: 5807.59, average training loss: 5986.48, base loss: 20518.24
[INFO 2017-06-26 12:48:24,563 main.py:50] epoch 3601, training loss: 5796.86, average training loss: 5986.13, base loss: 20518.07
[INFO 2017-06-26 12:48:24,922 main.py:50] epoch 3602, training loss: 5841.35, average training loss: 5985.81, base loss: 20517.92
[INFO 2017-06-26 12:48:25,281 main.py:50] epoch 3603, training loss: 5820.35, average training loss: 5985.51, base loss: 20518.01
[INFO 2017-06-26 12:48:25,641 main.py:50] epoch 3604, training loss: 5863.76, average training loss: 5985.20, base loss: 20518.08
[INFO 2017-06-26 12:48:26,001 main.py:50] epoch 3605, training loss: 5825.90, average training loss: 5984.95, base loss: 20518.23
[INFO 2017-06-26 12:48:26,361 main.py:50] epoch 3606, training loss: 5881.80, average training loss: 5984.59, base loss: 20518.02
[INFO 2017-06-26 12:48:26,721 main.py:50] epoch 3607, training loss: 5840.74, average training loss: 5984.27, base loss: 20517.92
[INFO 2017-06-26 12:48:27,081 main.py:50] epoch 3608, training loss: 5850.31, average training loss: 5983.93, base loss: 20518.00
[INFO 2017-06-26 12:48:27,440 main.py:50] epoch 3609, training loss: 5852.51, average training loss: 5983.70, base loss: 20517.92
[INFO 2017-06-26 12:48:27,799 main.py:50] epoch 3610, training loss: 5842.00, average training loss: 5983.42, base loss: 20518.04
[INFO 2017-06-26 12:48:28,158 main.py:50] epoch 3611, training loss: 5867.66, average training loss: 5983.17, base loss: 20517.97
[INFO 2017-06-26 12:48:28,519 main.py:50] epoch 3612, training loss: 5849.52, average training loss: 5982.97, base loss: 20518.04
[INFO 2017-06-26 12:48:28,880 main.py:50] epoch 3613, training loss: 5876.97, average training loss: 5982.66, base loss: 20518.30
[INFO 2017-06-26 12:48:29,240 main.py:50] epoch 3614, training loss: 5805.77, average training loss: 5982.33, base loss: 20518.60
[INFO 2017-06-26 12:48:29,600 main.py:50] epoch 3615, training loss: 5859.20, average training loss: 5982.03, base loss: 20518.70
[INFO 2017-06-26 12:48:29,959 main.py:50] epoch 3616, training loss: 5884.83, average training loss: 5981.78, base loss: 20518.54
[INFO 2017-06-26 12:48:30,319 main.py:50] epoch 3617, training loss: 5835.41, average training loss: 5981.41, base loss: 20518.42
[INFO 2017-06-26 12:48:30,679 main.py:50] epoch 3618, training loss: 5846.76, average training loss: 5981.03, base loss: 20518.11
[INFO 2017-06-26 12:48:31,040 main.py:50] epoch 3619, training loss: 5890.31, average training loss: 5980.72, base loss: 20518.29
[INFO 2017-06-26 12:48:31,400 main.py:50] epoch 3620, training loss: 5813.22, average training loss: 5980.37, base loss: 20518.37
[INFO 2017-06-26 12:48:31,761 main.py:50] epoch 3621, training loss: 5858.95, average training loss: 5980.03, base loss: 20518.27
[INFO 2017-06-26 12:48:32,121 main.py:50] epoch 3622, training loss: 5883.48, average training loss: 5979.83, base loss: 20518.15
[INFO 2017-06-26 12:48:32,480 main.py:50] epoch 3623, training loss: 5868.58, average training loss: 5979.53, base loss: 20518.28
[INFO 2017-06-26 12:48:32,842 main.py:50] epoch 3624, training loss: 5830.51, average training loss: 5979.30, base loss: 20518.05
[INFO 2017-06-26 12:48:33,203 main.py:50] epoch 3625, training loss: 5856.27, average training loss: 5979.03, base loss: 20517.88
[INFO 2017-06-26 12:48:33,563 main.py:50] epoch 3626, training loss: 5854.41, average training loss: 5978.72, base loss: 20517.51
[INFO 2017-06-26 12:48:33,924 main.py:50] epoch 3627, training loss: 5857.22, average training loss: 5978.46, base loss: 20517.34
[INFO 2017-06-26 12:48:34,284 main.py:50] epoch 3628, training loss: 5853.51, average training loss: 5978.22, base loss: 20517.02
[INFO 2017-06-26 12:48:34,644 main.py:50] epoch 3629, training loss: 5845.51, average training loss: 5977.87, base loss: 20517.27
[INFO 2017-06-26 12:48:35,003 main.py:50] epoch 3630, training loss: 5872.24, average training loss: 5977.63, base loss: 20517.25
[INFO 2017-06-26 12:48:35,361 main.py:50] epoch 3631, training loss: 5806.92, average training loss: 5977.32, base loss: 20517.33
[INFO 2017-06-26 12:48:35,722 main.py:50] epoch 3632, training loss: 5861.77, average training loss: 5977.05, base loss: 20517.33
[INFO 2017-06-26 12:48:36,082 main.py:50] epoch 3633, training loss: 5822.85, average training loss: 5976.76, base loss: 20517.22
[INFO 2017-06-26 12:48:36,441 main.py:50] epoch 3634, training loss: 5800.15, average training loss: 5976.45, base loss: 20516.92
[INFO 2017-06-26 12:48:36,801 main.py:50] epoch 3635, training loss: 5853.73, average training loss: 5976.08, base loss: 20516.75
[INFO 2017-06-26 12:48:37,160 main.py:50] epoch 3636, training loss: 5808.88, average training loss: 5975.83, base loss: 20517.09
[INFO 2017-06-26 12:48:37,520 main.py:50] epoch 3637, training loss: 5858.85, average training loss: 5975.52, base loss: 20516.87
[INFO 2017-06-26 12:48:37,880 main.py:50] epoch 3638, training loss: 5932.51, average training loss: 5975.36, base loss: 20516.82
[INFO 2017-06-26 12:48:38,239 main.py:50] epoch 3639, training loss: 5728.39, average training loss: 5974.92, base loss: 20516.27
[INFO 2017-06-26 12:48:38,598 main.py:50] epoch 3640, training loss: 5935.10, average training loss: 5974.69, base loss: 20516.32
[INFO 2017-06-26 12:48:38,959 main.py:50] epoch 3641, training loss: 5913.91, average training loss: 5974.50, base loss: 20516.00
[INFO 2017-06-26 12:48:39,320 main.py:50] epoch 3642, training loss: 6011.15, average training loss: 5974.28, base loss: 20515.98
[INFO 2017-06-26 12:48:39,680 main.py:50] epoch 3643, training loss: 5834.16, average training loss: 5973.94, base loss: 20515.26
[INFO 2017-06-26 12:48:40,040 main.py:50] epoch 3644, training loss: 5976.70, average training loss: 5973.82, base loss: 20515.76
[INFO 2017-06-26 12:48:40,400 main.py:50] epoch 3645, training loss: 5968.23, average training loss: 5973.63, base loss: 20515.82
[INFO 2017-06-26 12:48:40,760 main.py:50] epoch 3646, training loss: 5902.83, average training loss: 5973.38, base loss: 20515.73
[INFO 2017-06-26 12:48:41,120 main.py:50] epoch 3647, training loss: 6052.13, average training loss: 5973.22, base loss: 20515.83
[INFO 2017-06-26 12:48:41,481 main.py:50] epoch 3648, training loss: 5876.37, average training loss: 5973.00, base loss: 20515.76
[INFO 2017-06-26 12:48:41,841 main.py:50] epoch 3649, training loss: 5972.09, average training loss: 5972.89, base loss: 20516.25
[INFO 2017-06-26 12:48:42,201 main.py:50] epoch 3650, training loss: 5927.15, average training loss: 5972.77, base loss: 20516.72
[INFO 2017-06-26 12:48:42,561 main.py:50] epoch 3651, training loss: 5975.18, average training loss: 5972.60, base loss: 20516.63
[INFO 2017-06-26 12:48:42,921 main.py:50] epoch 3652, training loss: 5839.96, average training loss: 5972.33, base loss: 20516.71
[INFO 2017-06-26 12:48:43,280 main.py:50] epoch 3653, training loss: 5938.42, average training loss: 5972.16, base loss: 20517.09
[INFO 2017-06-26 12:48:43,641 main.py:50] epoch 3654, training loss: 5853.15, average training loss: 5971.92, base loss: 20517.20
[INFO 2017-06-26 12:48:44,001 main.py:50] epoch 3655, training loss: 5847.15, average training loss: 5971.68, base loss: 20516.82
[INFO 2017-06-26 12:48:44,361 main.py:50] epoch 3656, training loss: 5875.36, average training loss: 5971.43, base loss: 20516.59
[INFO 2017-06-26 12:48:44,722 main.py:50] epoch 3657, training loss: 5917.79, average training loss: 5971.27, base loss: 20516.73
[INFO 2017-06-26 12:48:45,082 main.py:50] epoch 3658, training loss: 5918.45, average training loss: 5971.10, base loss: 20516.68
[INFO 2017-06-26 12:48:45,442 main.py:50] epoch 3659, training loss: 5881.01, average training loss: 5970.95, base loss: 20516.79
[INFO 2017-06-26 12:48:45,801 main.py:50] epoch 3660, training loss: 5867.97, average training loss: 5970.70, base loss: 20516.96
[INFO 2017-06-26 12:48:46,162 main.py:50] epoch 3661, training loss: 5885.73, average training loss: 5970.53, base loss: 20516.72
[INFO 2017-06-26 12:48:46,522 main.py:50] epoch 3662, training loss: 5835.91, average training loss: 5970.33, base loss: 20516.66
[INFO 2017-06-26 12:48:46,882 main.py:50] epoch 3663, training loss: 5912.10, average training loss: 5970.17, base loss: 20516.57
[INFO 2017-06-26 12:48:47,242 main.py:50] epoch 3664, training loss: 5902.62, average training loss: 5969.99, base loss: 20516.29
[INFO 2017-06-26 12:48:47,615 main.py:50] epoch 3665, training loss: 5799.37, average training loss: 5969.75, base loss: 20516.29
[INFO 2017-06-26 12:48:47,975 main.py:50] epoch 3666, training loss: 5811.93, average training loss: 5969.46, base loss: 20516.26
[INFO 2017-06-26 12:48:48,334 main.py:50] epoch 3667, training loss: 5875.23, average training loss: 5969.23, base loss: 20515.92
[INFO 2017-06-26 12:48:48,695 main.py:50] epoch 3668, training loss: 5817.73, average training loss: 5968.95, base loss: 20515.84
[INFO 2017-06-26 12:48:49,054 main.py:50] epoch 3669, training loss: 5839.12, average training loss: 5968.73, base loss: 20515.78
[INFO 2017-06-26 12:48:49,414 main.py:50] epoch 3670, training loss: 5841.63, average training loss: 5968.46, base loss: 20515.90
[INFO 2017-06-26 12:48:49,774 main.py:50] epoch 3671, training loss: 5813.42, average training loss: 5968.26, base loss: 20515.91
[INFO 2017-06-26 12:48:50,134 main.py:50] epoch 3672, training loss: 5895.09, average training loss: 5968.11, base loss: 20516.01
[INFO 2017-06-26 12:48:50,494 main.py:50] epoch 3673, training loss: 5891.50, average training loss: 5967.88, base loss: 20516.22
[INFO 2017-06-26 12:48:50,853 main.py:50] epoch 3674, training loss: 5860.46, average training loss: 5967.61, base loss: 20516.44
[INFO 2017-06-26 12:48:51,213 main.py:50] epoch 3675, training loss: 5899.38, average training loss: 5967.40, base loss: 20516.39
[INFO 2017-06-26 12:48:51,574 main.py:50] epoch 3676, training loss: 5822.34, average training loss: 5967.10, base loss: 20516.27
[INFO 2017-06-26 12:48:51,933 main.py:50] epoch 3677, training loss: 5931.99, average training loss: 5966.91, base loss: 20515.97
[INFO 2017-06-26 12:48:52,293 main.py:50] epoch 3678, training loss: 5936.64, average training loss: 5966.73, base loss: 20516.38
[INFO 2017-06-26 12:48:52,654 main.py:50] epoch 3679, training loss: 5961.99, average training loss: 5966.55, base loss: 20516.21
[INFO 2017-06-26 12:48:53,013 main.py:50] epoch 3680, training loss: 5881.12, average training loss: 5966.39, base loss: 20516.23
[INFO 2017-06-26 12:48:53,373 main.py:50] epoch 3681, training loss: 5910.22, average training loss: 5966.16, base loss: 20515.95
[INFO 2017-06-26 12:48:53,734 main.py:50] epoch 3682, training loss: 5811.60, average training loss: 5965.94, base loss: 20515.79
[INFO 2017-06-26 12:48:54,094 main.py:50] epoch 3683, training loss: 5786.35, average training loss: 5965.65, base loss: 20515.98
[INFO 2017-06-26 12:48:54,454 main.py:50] epoch 3684, training loss: 5936.07, average training loss: 5965.52, base loss: 20516.36
[INFO 2017-06-26 12:48:54,812 main.py:50] epoch 3685, training loss: 5852.88, average training loss: 5965.35, base loss: 20516.21
[INFO 2017-06-26 12:48:55,173 main.py:50] epoch 3686, training loss: 5873.11, average training loss: 5965.16, base loss: 20516.52
[INFO 2017-06-26 12:48:55,533 main.py:50] epoch 3687, training loss: 5883.05, average training loss: 5964.99, base loss: 20516.17
[INFO 2017-06-26 12:48:55,893 main.py:50] epoch 3688, training loss: 5892.42, average training loss: 5964.86, base loss: 20516.38
[INFO 2017-06-26 12:48:56,253 main.py:50] epoch 3689, training loss: 5902.41, average training loss: 5964.69, base loss: 20516.33
[INFO 2017-06-26 12:48:56,613 main.py:50] epoch 3690, training loss: 5856.07, average training loss: 5964.53, base loss: 20516.39
[INFO 2017-06-26 12:48:56,973 main.py:50] epoch 3691, training loss: 5844.53, average training loss: 5964.33, base loss: 20516.19
[INFO 2017-06-26 12:48:57,333 main.py:50] epoch 3692, training loss: 5828.45, average training loss: 5964.09, base loss: 20515.96
[INFO 2017-06-26 12:48:57,692 main.py:50] epoch 3693, training loss: 5864.68, average training loss: 5963.99, base loss: 20516.32
[INFO 2017-06-26 12:48:58,052 main.py:50] epoch 3694, training loss: 5831.40, average training loss: 5963.76, base loss: 20516.41
[INFO 2017-06-26 12:48:58,410 main.py:50] epoch 3695, training loss: 5753.66, average training loss: 5963.40, base loss: 20516.18
[INFO 2017-06-26 12:48:58,770 main.py:50] epoch 3696, training loss: 5813.14, average training loss: 5963.16, base loss: 20516.02
[INFO 2017-06-26 12:48:59,131 main.py:50] epoch 3697, training loss: 5884.46, average training loss: 5962.92, base loss: 20516.00
[INFO 2017-06-26 12:48:59,490 main.py:50] epoch 3698, training loss: 5833.27, average training loss: 5962.66, base loss: 20516.09
[INFO 2017-06-26 12:48:59,850 main.py:50] epoch 3699, training loss: 5814.96, average training loss: 5962.39, base loss: 20515.96
[INFO 2017-06-26 12:48:59,850 main.py:52] epoch 3699, testing
[INFO 2017-06-26 12:49:01,330 main.py:103] average testing loss: 5833.35, base loss: 20575.04
[INFO 2017-06-26 12:49:01,330 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 12:49:01,336 main.py:76] current best accuracy: 5833.35
[INFO 2017-06-26 12:49:01,695 main.py:50] epoch 3700, training loss: 5802.71, average training loss: 5962.20, base loss: 20516.13
[INFO 2017-06-26 12:49:02,056 main.py:50] epoch 3701, training loss: 5789.10, average training loss: 5961.92, base loss: 20516.16
[INFO 2017-06-26 12:49:02,415 main.py:50] epoch 3702, training loss: 5844.80, average training loss: 5961.72, base loss: 20516.12
[INFO 2017-06-26 12:49:02,774 main.py:50] epoch 3703, training loss: 5913.20, average training loss: 5961.54, base loss: 20516.00
[INFO 2017-06-26 12:49:03,132 main.py:50] epoch 3704, training loss: 5837.62, average training loss: 5961.32, base loss: 20516.08
[INFO 2017-06-26 12:49:03,491 main.py:50] epoch 3705, training loss: 5847.25, average training loss: 5961.14, base loss: 20516.03
[INFO 2017-06-26 12:49:03,849 main.py:50] epoch 3706, training loss: 5818.11, average training loss: 5960.94, base loss: 20516.15
[INFO 2017-06-26 12:49:04,207 main.py:50] epoch 3707, training loss: 5800.16, average training loss: 5960.63, base loss: 20515.81
[INFO 2017-06-26 12:49:04,566 main.py:50] epoch 3708, training loss: 5833.14, average training loss: 5960.43, base loss: 20516.04
[INFO 2017-06-26 12:49:04,925 main.py:50] epoch 3709, training loss: 5793.40, average training loss: 5960.11, base loss: 20515.88
[INFO 2017-06-26 12:49:05,285 main.py:50] epoch 3710, training loss: 5826.44, average training loss: 5959.92, base loss: 20515.89
[INFO 2017-06-26 12:49:05,644 main.py:50] epoch 3711, training loss: 5891.62, average training loss: 5959.77, base loss: 20516.23
[INFO 2017-06-26 12:49:06,002 main.py:50] epoch 3712, training loss: 5789.02, average training loss: 5959.48, base loss: 20516.39
[INFO 2017-06-26 12:49:06,361 main.py:50] epoch 3713, training loss: 5870.98, average training loss: 5959.25, base loss: 20516.78
[INFO 2017-06-26 12:49:06,719 main.py:50] epoch 3714, training loss: 5904.58, average training loss: 5959.16, base loss: 20517.60
[INFO 2017-06-26 12:49:07,078 main.py:50] epoch 3715, training loss: 5861.89, average training loss: 5958.98, base loss: 20517.75
[INFO 2017-06-26 12:49:07,438 main.py:50] epoch 3716, training loss: 5861.82, average training loss: 5958.71, base loss: 20517.94
[INFO 2017-06-26 12:49:07,798 main.py:50] epoch 3717, training loss: 5863.31, average training loss: 5958.49, base loss: 20517.75
[INFO 2017-06-26 12:49:08,157 main.py:50] epoch 3718, training loss: 5819.57, average training loss: 5958.22, base loss: 20517.51
[INFO 2017-06-26 12:49:08,516 main.py:50] epoch 3719, training loss: 5883.35, average training loss: 5958.09, base loss: 20517.92
[INFO 2017-06-26 12:49:08,875 main.py:50] epoch 3720, training loss: 5835.00, average training loss: 5957.84, base loss: 20518.01
[INFO 2017-06-26 12:49:09,235 main.py:50] epoch 3721, training loss: 5887.14, average training loss: 5957.54, base loss: 20517.91
[INFO 2017-06-26 12:49:09,594 main.py:50] epoch 3722, training loss: 5843.35, average training loss: 5957.28, base loss: 20518.04
[INFO 2017-06-26 12:49:09,953 main.py:50] epoch 3723, training loss: 5876.52, average training loss: 5957.09, base loss: 20518.53
[INFO 2017-06-26 12:49:10,313 main.py:50] epoch 3724, training loss: 5822.08, average training loss: 5956.75, base loss: 20518.30
[INFO 2017-06-26 12:49:10,672 main.py:50] epoch 3725, training loss: 5876.30, average training loss: 5956.50, base loss: 20518.28
[INFO 2017-06-26 12:49:11,031 main.py:50] epoch 3726, training loss: 5809.11, average training loss: 5956.23, base loss: 20518.28
[INFO 2017-06-26 12:49:11,391 main.py:50] epoch 3727, training loss: 5889.86, average training loss: 5956.04, base loss: 20518.21
[INFO 2017-06-26 12:49:11,748 main.py:50] epoch 3728, training loss: 5855.48, average training loss: 5955.77, base loss: 20518.41
[INFO 2017-06-26 12:49:12,108 main.py:50] epoch 3729, training loss: 5831.30, average training loss: 5955.54, base loss: 20518.02
[INFO 2017-06-26 12:49:12,467 main.py:50] epoch 3730, training loss: 5807.96, average training loss: 5955.30, base loss: 20517.88
[INFO 2017-06-26 12:49:12,827 main.py:50] epoch 3731, training loss: 5834.93, average training loss: 5955.13, base loss: 20517.99
[INFO 2017-06-26 12:49:13,186 main.py:50] epoch 3732, training loss: 5903.81, average training loss: 5954.95, base loss: 20518.17
[INFO 2017-06-26 12:49:13,544 main.py:50] epoch 3733, training loss: 5882.14, average training loss: 5954.71, base loss: 20517.61
[INFO 2017-06-26 12:49:13,904 main.py:50] epoch 3734, training loss: 5838.15, average training loss: 5954.42, base loss: 20517.72
[INFO 2017-06-26 12:49:14,262 main.py:50] epoch 3735, training loss: 5864.10, average training loss: 5954.35, base loss: 20517.42
[INFO 2017-06-26 12:49:14,620 main.py:50] epoch 3736, training loss: 5917.28, average training loss: 5954.22, base loss: 20518.07
[INFO 2017-06-26 12:49:14,979 main.py:50] epoch 3737, training loss: 5831.01, average training loss: 5954.05, base loss: 20517.93
[INFO 2017-06-26 12:49:15,338 main.py:50] epoch 3738, training loss: 5773.82, average training loss: 5953.77, base loss: 20517.56
[INFO 2017-06-26 12:49:15,697 main.py:50] epoch 3739, training loss: 5880.09, average training loss: 5953.59, base loss: 20517.17
[INFO 2017-06-26 12:49:16,057 main.py:50] epoch 3740, training loss: 5882.61, average training loss: 5953.42, base loss: 20517.45
[INFO 2017-06-26 12:49:16,415 main.py:50] epoch 3741, training loss: 5878.47, average training loss: 5953.20, base loss: 20517.51
[INFO 2017-06-26 12:49:16,773 main.py:50] epoch 3742, training loss: 5850.53, average training loss: 5953.00, base loss: 20517.58
[INFO 2017-06-26 12:49:17,132 main.py:50] epoch 3743, training loss: 5858.52, average training loss: 5952.74, base loss: 20517.08
[INFO 2017-06-26 12:49:17,490 main.py:50] epoch 3744, training loss: 5852.53, average training loss: 5952.59, base loss: 20516.81
[INFO 2017-06-26 12:49:17,849 main.py:50] epoch 3745, training loss: 5932.44, average training loss: 5952.41, base loss: 20516.70
[INFO 2017-06-26 12:49:18,207 main.py:50] epoch 3746, training loss: 5964.51, average training loss: 5952.28, base loss: 20517.08
[INFO 2017-06-26 12:49:18,564 main.py:50] epoch 3747, training loss: 5866.15, average training loss: 5951.97, base loss: 20516.77
[INFO 2017-06-26 12:49:18,922 main.py:50] epoch 3748, training loss: 5932.73, average training loss: 5951.86, base loss: 20516.89
[INFO 2017-06-26 12:49:19,281 main.py:50] epoch 3749, training loss: 5889.77, average training loss: 5951.75, base loss: 20517.26
[INFO 2017-06-26 12:49:19,640 main.py:50] epoch 3750, training loss: 5924.46, average training loss: 5951.67, base loss: 20517.24
[INFO 2017-06-26 12:49:20,000 main.py:50] epoch 3751, training loss: 5880.68, average training loss: 5951.45, base loss: 20517.32
[INFO 2017-06-26 12:49:20,358 main.py:50] epoch 3752, training loss: 5799.32, average training loss: 5951.23, base loss: 20517.10
[INFO 2017-06-26 12:49:20,716 main.py:50] epoch 3753, training loss: 5932.25, average training loss: 5951.16, base loss: 20517.10
[INFO 2017-06-26 12:49:21,077 main.py:50] epoch 3754, training loss: 5893.83, average training loss: 5950.95, base loss: 20517.10
[INFO 2017-06-26 12:49:21,435 main.py:50] epoch 3755, training loss: 5868.16, average training loss: 5950.74, base loss: 20517.12
[INFO 2017-06-26 12:49:21,794 main.py:50] epoch 3756, training loss: 5939.21, average training loss: 5950.63, base loss: 20517.45
[INFO 2017-06-26 12:49:22,153 main.py:50] epoch 3757, training loss: 5908.92, average training loss: 5950.46, base loss: 20517.03
[INFO 2017-06-26 12:49:22,513 main.py:50] epoch 3758, training loss: 5937.07, average training loss: 5950.35, base loss: 20517.24
[INFO 2017-06-26 12:49:22,872 main.py:50] epoch 3759, training loss: 5928.35, average training loss: 5950.26, base loss: 20517.41
[INFO 2017-06-26 12:49:23,231 main.py:50] epoch 3760, training loss: 5883.12, average training loss: 5950.06, base loss: 20516.79
[INFO 2017-06-26 12:49:23,590 main.py:50] epoch 3761, training loss: 5887.92, average training loss: 5949.93, base loss: 20516.45
[INFO 2017-06-26 12:49:23,949 main.py:50] epoch 3762, training loss: 5885.06, average training loss: 5949.76, base loss: 20516.39
[INFO 2017-06-26 12:49:24,308 main.py:50] epoch 3763, training loss: 5912.45, average training loss: 5949.55, base loss: 20516.53
[INFO 2017-06-26 12:49:24,668 main.py:50] epoch 3764, training loss: 5902.67, average training loss: 5949.35, base loss: 20516.47
[INFO 2017-06-26 12:49:25,028 main.py:50] epoch 3765, training loss: 5875.37, average training loss: 5949.10, base loss: 20516.19
[INFO 2017-06-26 12:49:25,386 main.py:50] epoch 3766, training loss: 5815.67, average training loss: 5948.85, base loss: 20515.91
[INFO 2017-06-26 12:49:25,743 main.py:50] epoch 3767, training loss: 5825.37, average training loss: 5948.60, base loss: 20515.80
[INFO 2017-06-26 12:49:26,102 main.py:50] epoch 3768, training loss: 5846.37, average training loss: 5948.37, base loss: 20515.67
[INFO 2017-06-26 12:49:26,462 main.py:50] epoch 3769, training loss: 5838.75, average training loss: 5948.12, base loss: 20515.44
[INFO 2017-06-26 12:49:26,820 main.py:50] epoch 3770, training loss: 5890.60, average training loss: 5947.85, base loss: 20515.27
[INFO 2017-06-26 12:49:27,181 main.py:50] epoch 3771, training loss: 5857.49, average training loss: 5947.67, base loss: 20514.99
[INFO 2017-06-26 12:49:27,540 main.py:50] epoch 3772, training loss: 5787.75, average training loss: 5947.38, base loss: 20514.57
[INFO 2017-06-26 12:49:27,899 main.py:50] epoch 3773, training loss: 5746.80, average training loss: 5947.12, base loss: 20514.86
[INFO 2017-06-26 12:49:28,256 main.py:50] epoch 3774, training loss: 5869.60, average training loss: 5946.90, base loss: 20514.99
[INFO 2017-06-26 12:49:28,614 main.py:50] epoch 3775, training loss: 5853.68, average training loss: 5946.69, base loss: 20515.11
[INFO 2017-06-26 12:49:28,974 main.py:50] epoch 3776, training loss: 5866.35, average training loss: 5946.46, base loss: 20514.95
[INFO 2017-06-26 12:49:29,334 main.py:50] epoch 3777, training loss: 5842.75, average training loss: 5946.21, base loss: 20515.04
[INFO 2017-06-26 12:49:29,693 main.py:50] epoch 3778, training loss: 5792.73, average training loss: 5945.96, base loss: 20515.08
[INFO 2017-06-26 12:49:30,050 main.py:50] epoch 3779, training loss: 5831.01, average training loss: 5945.66, base loss: 20514.79
[INFO 2017-06-26 12:49:30,408 main.py:50] epoch 3780, training loss: 5843.80, average training loss: 5945.34, base loss: 20514.64
[INFO 2017-06-26 12:49:30,767 main.py:50] epoch 3781, training loss: 5801.72, average training loss: 5945.10, base loss: 20514.61
[INFO 2017-06-26 12:49:31,126 main.py:50] epoch 3782, training loss: 5861.68, average training loss: 5944.95, base loss: 20514.71
[INFO 2017-06-26 12:49:31,484 main.py:50] epoch 3783, training loss: 5779.41, average training loss: 5944.66, base loss: 20514.46
[INFO 2017-06-26 12:49:31,855 main.py:50] epoch 3784, training loss: 5771.33, average training loss: 5944.34, base loss: 20514.13
[INFO 2017-06-26 12:49:32,212 main.py:50] epoch 3785, training loss: 5871.81, average training loss: 5944.11, base loss: 20514.69
[INFO 2017-06-26 12:49:32,571 main.py:50] epoch 3786, training loss: 5854.18, average training loss: 5943.80, base loss: 20514.59
[INFO 2017-06-26 12:49:32,930 main.py:50] epoch 3787, training loss: 5878.35, average training loss: 5943.58, base loss: 20514.54
[INFO 2017-06-26 12:49:33,289 main.py:50] epoch 3788, training loss: 5819.12, average training loss: 5943.30, base loss: 20514.87
[INFO 2017-06-26 12:49:33,648 main.py:50] epoch 3789, training loss: 5870.76, average training loss: 5943.01, base loss: 20514.85
[INFO 2017-06-26 12:49:34,009 main.py:50] epoch 3790, training loss: 5787.19, average training loss: 5942.78, base loss: 20514.77
[INFO 2017-06-26 12:49:34,366 main.py:50] epoch 3791, training loss: 5834.82, average training loss: 5942.51, base loss: 20514.82
[INFO 2017-06-26 12:49:34,726 main.py:50] epoch 3792, training loss: 5852.33, average training loss: 5942.28, base loss: 20514.64
[INFO 2017-06-26 12:49:35,085 main.py:50] epoch 3793, training loss: 5833.42, average training loss: 5942.05, base loss: 20514.72
[INFO 2017-06-26 12:49:35,444 main.py:50] epoch 3794, training loss: 5800.39, average training loss: 5941.78, base loss: 20514.57
[INFO 2017-06-26 12:49:35,804 main.py:50] epoch 3795, training loss: 5843.59, average training loss: 5941.53, base loss: 20514.52
[INFO 2017-06-26 12:49:36,164 main.py:50] epoch 3796, training loss: 5864.87, average training loss: 5941.44, base loss: 20514.61
[INFO 2017-06-26 12:49:36,522 main.py:50] epoch 3797, training loss: 5845.74, average training loss: 5941.06, base loss: 20514.31
[INFO 2017-06-26 12:49:36,879 main.py:50] epoch 3798, training loss: 5888.99, average training loss: 5940.90, base loss: 20514.65
[INFO 2017-06-26 12:49:37,237 main.py:50] epoch 3799, training loss: 5921.74, average training loss: 5940.77, base loss: 20515.11
[INFO 2017-06-26 12:49:37,237 main.py:52] epoch 3799, testing
[INFO 2017-06-26 12:49:38,705 main.py:103] average testing loss: 5863.37, base loss: 20583.39
[INFO 2017-06-26 12:49:38,706 main.py:76] current best accuracy: 5833.35
[INFO 2017-06-26 12:49:39,066 main.py:50] epoch 3800, training loss: 5819.27, average training loss: 5940.53, base loss: 20515.27
[INFO 2017-06-26 12:49:39,424 main.py:50] epoch 3801, training loss: 5850.38, average training loss: 5940.29, base loss: 20514.85
[INFO 2017-06-26 12:49:39,782 main.py:50] epoch 3802, training loss: 5920.63, average training loss: 5940.13, base loss: 20514.91
[INFO 2017-06-26 12:49:40,142 main.py:50] epoch 3803, training loss: 5911.25, average training loss: 5939.95, base loss: 20514.97
[INFO 2017-06-26 12:49:40,501 main.py:50] epoch 3804, training loss: 5841.67, average training loss: 5939.70, base loss: 20515.24
[INFO 2017-06-26 12:49:40,860 main.py:50] epoch 3805, training loss: 5898.51, average training loss: 5939.49, base loss: 20515.16
[INFO 2017-06-26 12:49:41,219 main.py:50] epoch 3806, training loss: 5831.89, average training loss: 5939.12, base loss: 20515.40
[INFO 2017-06-26 12:49:41,577 main.py:50] epoch 3807, training loss: 5897.63, average training loss: 5938.89, base loss: 20514.92
[INFO 2017-06-26 12:49:41,936 main.py:50] epoch 3808, training loss: 5810.56, average training loss: 5938.60, base loss: 20514.86
[INFO 2017-06-26 12:49:42,295 main.py:50] epoch 3809, training loss: 5947.73, average training loss: 5938.45, base loss: 20514.89
[INFO 2017-06-26 12:49:42,652 main.py:50] epoch 3810, training loss: 5840.19, average training loss: 5938.20, base loss: 20514.86
[INFO 2017-06-26 12:49:43,010 main.py:50] epoch 3811, training loss: 6012.94, average training loss: 5938.14, base loss: 20515.04
[INFO 2017-06-26 12:49:43,368 main.py:50] epoch 3812, training loss: 5878.26, average training loss: 5937.97, base loss: 20514.80
[INFO 2017-06-26 12:49:43,727 main.py:50] epoch 3813, training loss: 5882.95, average training loss: 5937.79, base loss: 20514.65
[INFO 2017-06-26 12:49:44,086 main.py:50] epoch 3814, training loss: 5840.27, average training loss: 5937.56, base loss: 20514.86
[INFO 2017-06-26 12:49:44,446 main.py:50] epoch 3815, training loss: 5857.97, average training loss: 5937.36, base loss: 20514.65
[INFO 2017-06-26 12:49:44,806 main.py:50] epoch 3816, training loss: 5864.94, average training loss: 5937.15, base loss: 20514.48
[INFO 2017-06-26 12:49:45,166 main.py:50] epoch 3817, training loss: 5866.17, average training loss: 5937.02, base loss: 20514.42
[INFO 2017-06-26 12:49:45,525 main.py:50] epoch 3818, training loss: 5873.82, average training loss: 5936.81, base loss: 20514.26
[INFO 2017-06-26 12:49:45,882 main.py:50] epoch 3819, training loss: 5839.68, average training loss: 5936.65, base loss: 20514.21
[INFO 2017-06-26 12:49:46,241 main.py:50] epoch 3820, training loss: 5871.15, average training loss: 5936.52, base loss: 20514.31
[INFO 2017-06-26 12:49:46,598 main.py:50] epoch 3821, training loss: 5809.00, average training loss: 5936.33, base loss: 20514.09
[INFO 2017-06-26 12:49:46,957 main.py:50] epoch 3822, training loss: 5926.68, average training loss: 5936.21, base loss: 20514.47
[INFO 2017-06-26 12:49:47,314 main.py:50] epoch 3823, training loss: 5739.90, average training loss: 5935.95, base loss: 20514.10
[INFO 2017-06-26 12:49:47,674 main.py:50] epoch 3824, training loss: 5842.75, average training loss: 5935.73, base loss: 20514.17
[INFO 2017-06-26 12:49:48,031 main.py:50] epoch 3825, training loss: 5853.33, average training loss: 5935.59, base loss: 20514.45
[INFO 2017-06-26 12:49:48,391 main.py:50] epoch 3826, training loss: 5919.85, average training loss: 5935.51, base loss: 20514.94
[INFO 2017-06-26 12:49:48,751 main.py:50] epoch 3827, training loss: 5824.60, average training loss: 5935.20, base loss: 20514.77
[INFO 2017-06-26 12:49:49,110 main.py:50] epoch 3828, training loss: 5858.78, average training loss: 5934.97, base loss: 20514.68
[INFO 2017-06-26 12:49:49,469 main.py:50] epoch 3829, training loss: 5788.07, average training loss: 5934.70, base loss: 20514.55
[INFO 2017-06-26 12:49:49,828 main.py:50] epoch 3830, training loss: 5832.99, average training loss: 5934.53, base loss: 20514.60
[INFO 2017-06-26 12:49:50,188 main.py:50] epoch 3831, training loss: 5797.99, average training loss: 5934.29, base loss: 20514.76
[INFO 2017-06-26 12:49:50,547 main.py:50] epoch 3832, training loss: 5750.61, average training loss: 5934.00, base loss: 20514.28
[INFO 2017-06-26 12:49:50,905 main.py:50] epoch 3833, training loss: 5813.43, average training loss: 5933.85, base loss: 20514.29
[INFO 2017-06-26 12:49:51,264 main.py:50] epoch 3834, training loss: 5791.38, average training loss: 5933.62, base loss: 20514.10
[INFO 2017-06-26 12:49:51,624 main.py:50] epoch 3835, training loss: 5762.62, average training loss: 5933.36, base loss: 20514.18
[INFO 2017-06-26 12:49:51,983 main.py:50] epoch 3836, training loss: 5819.27, average training loss: 5933.07, base loss: 20513.86
[INFO 2017-06-26 12:49:52,341 main.py:50] epoch 3837, training loss: 5822.61, average training loss: 5932.80, base loss: 20513.85
[INFO 2017-06-26 12:49:52,701 main.py:50] epoch 3838, training loss: 5870.99, average training loss: 5932.66, base loss: 20514.04
[INFO 2017-06-26 12:49:53,059 main.py:50] epoch 3839, training loss: 5815.00, average training loss: 5932.46, base loss: 20514.43
[INFO 2017-06-26 12:49:53,417 main.py:50] epoch 3840, training loss: 5813.75, average training loss: 5932.32, base loss: 20514.37
[INFO 2017-06-26 12:49:53,776 main.py:50] epoch 3841, training loss: 5735.42, average training loss: 5932.08, base loss: 20514.40
[INFO 2017-06-26 12:49:54,135 main.py:50] epoch 3842, training loss: 5854.14, average training loss: 5931.91, base loss: 20514.36
[INFO 2017-06-26 12:49:54,494 main.py:50] epoch 3843, training loss: 5858.51, average training loss: 5931.73, base loss: 20514.68
[INFO 2017-06-26 12:49:54,853 main.py:50] epoch 3844, training loss: 5861.54, average training loss: 5931.50, base loss: 20514.52
[INFO 2017-06-26 12:49:55,213 main.py:50] epoch 3845, training loss: 5784.04, average training loss: 5931.26, base loss: 20514.49
[INFO 2017-06-26 12:49:55,572 main.py:50] epoch 3846, training loss: 5818.99, average training loss: 5931.08, base loss: 20514.28
[INFO 2017-06-26 12:49:55,930 main.py:50] epoch 3847, training loss: 5791.65, average training loss: 5930.80, base loss: 20513.98
[INFO 2017-06-26 12:49:56,289 main.py:50] epoch 3848, training loss: 5896.01, average training loss: 5930.67, base loss: 20513.87
[INFO 2017-06-26 12:49:56,650 main.py:50] epoch 3849, training loss: 5811.50, average training loss: 5930.50, base loss: 20513.89
[INFO 2017-06-26 12:49:57,008 main.py:50] epoch 3850, training loss: 5872.06, average training loss: 5930.33, base loss: 20513.82
[INFO 2017-06-26 12:49:57,367 main.py:50] epoch 3851, training loss: 5790.33, average training loss: 5930.20, base loss: 20513.81
[INFO 2017-06-26 12:49:57,725 main.py:50] epoch 3852, training loss: 5732.46, average training loss: 5929.98, base loss: 20513.53
[INFO 2017-06-26 12:49:58,085 main.py:50] epoch 3853, training loss: 5839.13, average training loss: 5929.86, base loss: 20513.82
[INFO 2017-06-26 12:49:58,445 main.py:50] epoch 3854, training loss: 5779.76, average training loss: 5929.56, base loss: 20513.56
[INFO 2017-06-26 12:49:58,805 main.py:50] epoch 3855, training loss: 5824.69, average training loss: 5929.35, base loss: 20513.21
[INFO 2017-06-26 12:49:59,165 main.py:50] epoch 3856, training loss: 5814.00, average training loss: 5929.25, base loss: 20513.10
[INFO 2017-06-26 12:49:59,522 main.py:50] epoch 3857, training loss: 5826.57, average training loss: 5929.07, base loss: 20513.18
[INFO 2017-06-26 12:49:59,881 main.py:50] epoch 3858, training loss: 5779.55, average training loss: 5928.72, base loss: 20513.23
[INFO 2017-06-26 12:50:00,240 main.py:50] epoch 3859, training loss: 5715.96, average training loss: 5928.40, base loss: 20512.85
[INFO 2017-06-26 12:50:00,601 main.py:50] epoch 3860, training loss: 5869.86, average training loss: 5928.22, base loss: 20512.67
[INFO 2017-06-26 12:50:00,961 main.py:50] epoch 3861, training loss: 5845.20, average training loss: 5928.03, base loss: 20512.64
[INFO 2017-06-26 12:50:01,320 main.py:50] epoch 3862, training loss: 5810.98, average training loss: 5927.84, base loss: 20512.88
[INFO 2017-06-26 12:50:01,678 main.py:50] epoch 3863, training loss: 5879.43, average training loss: 5927.71, base loss: 20512.61
[INFO 2017-06-26 12:50:02,038 main.py:50] epoch 3864, training loss: 5817.43, average training loss: 5927.51, base loss: 20512.40
[INFO 2017-06-26 12:50:02,397 main.py:50] epoch 3865, training loss: 5863.33, average training loss: 5927.35, base loss: 20512.19
[INFO 2017-06-26 12:50:02,757 main.py:50] epoch 3866, training loss: 5860.93, average training loss: 5927.27, base loss: 20512.30
[INFO 2017-06-26 12:50:03,115 main.py:50] epoch 3867, training loss: 5896.79, average training loss: 5927.16, base loss: 20512.42
[INFO 2017-06-26 12:50:03,475 main.py:50] epoch 3868, training loss: 5839.77, average training loss: 5926.99, base loss: 20512.64
[INFO 2017-06-26 12:50:03,835 main.py:50] epoch 3869, training loss: 5813.41, average training loss: 5926.85, base loss: 20512.53
[INFO 2017-06-26 12:50:04,195 main.py:50] epoch 3870, training loss: 5822.23, average training loss: 5926.64, base loss: 20512.64
[INFO 2017-06-26 12:50:04,555 main.py:50] epoch 3871, training loss: 5859.96, average training loss: 5926.47, base loss: 20512.50
[INFO 2017-06-26 12:50:04,915 main.py:50] epoch 3872, training loss: 5786.14, average training loss: 5926.25, base loss: 20512.48
[INFO 2017-06-26 12:50:05,274 main.py:50] epoch 3873, training loss: 5924.31, average training loss: 5926.14, base loss: 20512.24
[INFO 2017-06-26 12:50:05,633 main.py:50] epoch 3874, training loss: 5839.22, average training loss: 5925.95, base loss: 20512.00
[INFO 2017-06-26 12:50:05,990 main.py:50] epoch 3875, training loss: 6015.07, average training loss: 5925.94, base loss: 20512.19
[INFO 2017-06-26 12:50:06,351 main.py:50] epoch 3876, training loss: 5828.85, average training loss: 5925.76, base loss: 20511.98
[INFO 2017-06-26 12:50:06,710 main.py:50] epoch 3877, training loss: 5901.71, average training loss: 5925.64, base loss: 20512.14
[INFO 2017-06-26 12:50:07,068 main.py:50] epoch 3878, training loss: 5763.02, average training loss: 5925.35, base loss: 20512.00
[INFO 2017-06-26 12:50:07,427 main.py:50] epoch 3879, training loss: 5802.27, average training loss: 5925.12, base loss: 20512.08
[INFO 2017-06-26 12:50:07,787 main.py:50] epoch 3880, training loss: 5782.48, average training loss: 5924.87, base loss: 20511.86
[INFO 2017-06-26 12:50:08,145 main.py:50] epoch 3881, training loss: 5881.48, average training loss: 5924.60, base loss: 20512.15
[INFO 2017-06-26 12:50:08,505 main.py:50] epoch 3882, training loss: 5818.62, average training loss: 5924.27, base loss: 20511.97
[INFO 2017-06-26 12:50:08,865 main.py:50] epoch 3883, training loss: 5830.98, average training loss: 5924.01, base loss: 20511.82
[INFO 2017-06-26 12:50:09,224 main.py:50] epoch 3884, training loss: 5861.17, average training loss: 5923.82, base loss: 20511.65
[INFO 2017-06-26 12:50:09,584 main.py:50] epoch 3885, training loss: 5828.98, average training loss: 5923.54, base loss: 20511.72
[INFO 2017-06-26 12:50:09,943 main.py:50] epoch 3886, training loss: 5791.90, average training loss: 5923.28, base loss: 20511.69
[INFO 2017-06-26 12:50:10,302 main.py:50] epoch 3887, training loss: 5831.02, average training loss: 5923.06, base loss: 20511.70
[INFO 2017-06-26 12:50:10,660 main.py:50] epoch 3888, training loss: 5830.42, average training loss: 5922.80, base loss: 20512.06
[INFO 2017-06-26 12:50:11,021 main.py:50] epoch 3889, training loss: 5807.74, average training loss: 5922.50, base loss: 20511.90
[INFO 2017-06-26 12:50:11,380 main.py:50] epoch 3890, training loss: 5792.77, average training loss: 5922.31, base loss: 20512.29
[INFO 2017-06-26 12:50:11,739 main.py:50] epoch 3891, training loss: 5818.99, average training loss: 5922.04, base loss: 20512.03
[INFO 2017-06-26 12:50:12,099 main.py:50] epoch 3892, training loss: 5828.56, average training loss: 5921.77, base loss: 20512.02
[INFO 2017-06-26 12:50:12,460 main.py:50] epoch 3893, training loss: 5854.86, average training loss: 5921.57, base loss: 20511.96
[INFO 2017-06-26 12:50:12,819 main.py:50] epoch 3894, training loss: 5741.97, average training loss: 5921.26, base loss: 20511.73
[INFO 2017-06-26 12:50:13,179 main.py:50] epoch 3895, training loss: 5782.55, average training loss: 5921.06, base loss: 20511.95
[INFO 2017-06-26 12:50:13,537 main.py:50] epoch 3896, training loss: 5796.63, average training loss: 5920.83, base loss: 20511.82
[INFO 2017-06-26 12:50:13,896 main.py:50] epoch 3897, training loss: 5845.88, average training loss: 5920.70, base loss: 20512.14
[INFO 2017-06-26 12:50:14,255 main.py:50] epoch 3898, training loss: 5829.73, average training loss: 5920.48, base loss: 20512.26
[INFO 2017-06-26 12:50:14,614 main.py:50] epoch 3899, training loss: 5832.92, average training loss: 5920.27, base loss: 20512.52
[INFO 2017-06-26 12:50:14,614 main.py:52] epoch 3899, testing
[INFO 2017-06-26 12:50:16,104 main.py:103] average testing loss: 5827.19, base loss: 20497.53
[INFO 2017-06-26 12:50:16,105 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 12:50:16,111 main.py:76] current best accuracy: 5827.19
[INFO 2017-06-26 12:50:16,469 main.py:50] epoch 3900, training loss: 5899.03, average training loss: 5920.20, base loss: 20512.74
[INFO 2017-06-26 12:50:16,828 main.py:50] epoch 3901, training loss: 5767.09, average training loss: 5920.02, base loss: 20512.75
[INFO 2017-06-26 12:50:17,186 main.py:50] epoch 3902, training loss: 5832.72, average training loss: 5919.89, base loss: 20512.80
[INFO 2017-06-26 12:50:17,546 main.py:50] epoch 3903, training loss: 5760.21, average training loss: 5919.63, base loss: 20512.68
[INFO 2017-06-26 12:50:17,905 main.py:50] epoch 3904, training loss: 5808.06, average training loss: 5919.46, base loss: 20512.61
[INFO 2017-06-26 12:50:18,264 main.py:50] epoch 3905, training loss: 5870.74, average training loss: 5919.23, base loss: 20512.77
[INFO 2017-06-26 12:50:18,623 main.py:50] epoch 3906, training loss: 5782.12, average training loss: 5918.98, base loss: 20512.85
[INFO 2017-06-26 12:50:18,981 main.py:50] epoch 3907, training loss: 5923.54, average training loss: 5918.85, base loss: 20513.12
[INFO 2017-06-26 12:50:19,341 main.py:50] epoch 3908, training loss: 5811.98, average training loss: 5918.68, base loss: 20513.10
[INFO 2017-06-26 12:50:19,702 main.py:50] epoch 3909, training loss: 5868.71, average training loss: 5918.59, base loss: 20513.07
[INFO 2017-06-26 12:50:20,062 main.py:50] epoch 3910, training loss: 5789.73, average training loss: 5918.43, base loss: 20512.98
[INFO 2017-06-26 12:50:20,420 main.py:50] epoch 3911, training loss: 5810.44, average training loss: 5918.25, base loss: 20513.03
[INFO 2017-06-26 12:50:20,780 main.py:50] epoch 3912, training loss: 5824.80, average training loss: 5918.05, base loss: 20513.10
[INFO 2017-06-26 12:50:21,139 main.py:50] epoch 3913, training loss: 5862.55, average training loss: 5917.94, base loss: 20513.18
[INFO 2017-06-26 12:50:21,498 main.py:50] epoch 3914, training loss: 5776.32, average training loss: 5917.71, base loss: 20513.07
[INFO 2017-06-26 12:50:21,857 main.py:50] epoch 3915, training loss: 5823.00, average training loss: 5917.48, base loss: 20512.83
[INFO 2017-06-26 12:50:22,216 main.py:50] epoch 3916, training loss: 5832.96, average training loss: 5917.31, base loss: 20512.32
[INFO 2017-06-26 12:50:22,575 main.py:50] epoch 3917, training loss: 5925.56, average training loss: 5917.26, base loss: 20512.69
[INFO 2017-06-26 12:50:22,934 main.py:50] epoch 3918, training loss: 5944.67, average training loss: 5917.23, base loss: 20513.01
[INFO 2017-06-26 12:50:23,294 main.py:50] epoch 3919, training loss: 5890.46, average training loss: 5917.15, base loss: 20512.98
[INFO 2017-06-26 12:50:23,653 main.py:50] epoch 3920, training loss: 5867.54, average training loss: 5916.94, base loss: 20512.92
[INFO 2017-06-26 12:50:24,013 main.py:50] epoch 3921, training loss: 5934.03, average training loss: 5916.90, base loss: 20513.22
[INFO 2017-06-26 12:50:24,372 main.py:50] epoch 3922, training loss: 5875.18, average training loss: 5916.77, base loss: 20513.10
[INFO 2017-06-26 12:50:24,732 main.py:50] epoch 3923, training loss: 5831.12, average training loss: 5916.65, base loss: 20513.02
[INFO 2017-06-26 12:50:25,091 main.py:50] epoch 3924, training loss: 5841.19, average training loss: 5916.49, base loss: 20512.96
[INFO 2017-06-26 12:50:25,450 main.py:50] epoch 3925, training loss: 5846.30, average training loss: 5916.35, base loss: 20513.03
[INFO 2017-06-26 12:50:25,809 main.py:50] epoch 3926, training loss: 5831.11, average training loss: 5916.24, base loss: 20512.75
[INFO 2017-06-26 12:50:26,168 main.py:50] epoch 3927, training loss: 5821.34, average training loss: 5916.13, base loss: 20512.42
[INFO 2017-06-26 12:50:26,527 main.py:50] epoch 3928, training loss: 5872.98, average training loss: 5916.01, base loss: 20512.47
[INFO 2017-06-26 12:50:26,886 main.py:50] epoch 3929, training loss: 5827.05, average training loss: 5915.77, base loss: 20512.06
[INFO 2017-06-26 12:50:27,246 main.py:50] epoch 3930, training loss: 5938.00, average training loss: 5915.63, base loss: 20511.56
[INFO 2017-06-26 12:50:27,606 main.py:50] epoch 3931, training loss: 5806.45, average training loss: 5915.42, base loss: 20511.66
[INFO 2017-06-26 12:50:27,965 main.py:50] epoch 3932, training loss: 5857.29, average training loss: 5915.30, base loss: 20511.81
[INFO 2017-06-26 12:50:28,324 main.py:50] epoch 3933, training loss: 5925.16, average training loss: 5915.18, base loss: 20511.82
[INFO 2017-06-26 12:50:28,684 main.py:50] epoch 3934, training loss: 5972.77, average training loss: 5915.11, base loss: 20511.88
[INFO 2017-06-26 12:50:29,043 main.py:50] epoch 3935, training loss: 5835.48, average training loss: 5914.92, base loss: 20511.81
[INFO 2017-06-26 12:50:29,402 main.py:50] epoch 3936, training loss: 5847.77, average training loss: 5914.78, base loss: 20511.76
[INFO 2017-06-26 12:50:29,761 main.py:50] epoch 3937, training loss: 5865.38, average training loss: 5914.63, base loss: 20511.91
[INFO 2017-06-26 12:50:30,118 main.py:50] epoch 3938, training loss: 5784.83, average training loss: 5914.32, base loss: 20511.87
[INFO 2017-06-26 12:50:30,479 main.py:50] epoch 3939, training loss: 5794.19, average training loss: 5914.20, base loss: 20511.80
[INFO 2017-06-26 12:50:30,838 main.py:50] epoch 3940, training loss: 5795.19, average training loss: 5913.99, base loss: 20512.06
[INFO 2017-06-26 12:50:31,196 main.py:50] epoch 3941, training loss: 5825.60, average training loss: 5913.84, base loss: 20512.04
[INFO 2017-06-26 12:50:31,556 main.py:50] epoch 3942, training loss: 5801.85, average training loss: 5913.64, base loss: 20512.15
[INFO 2017-06-26 12:50:31,916 main.py:50] epoch 3943, training loss: 5814.50, average training loss: 5913.40, base loss: 20511.53
[INFO 2017-06-26 12:50:32,275 main.py:50] epoch 3944, training loss: 5855.34, average training loss: 5913.34, base loss: 20511.35
[INFO 2017-06-26 12:50:32,636 main.py:50] epoch 3945, training loss: 5817.42, average training loss: 5913.11, base loss: 20511.30
[INFO 2017-06-26 12:50:32,994 main.py:50] epoch 3946, training loss: 5851.19, average training loss: 5912.92, base loss: 20511.16
[INFO 2017-06-26 12:50:33,354 main.py:50] epoch 3947, training loss: 5838.08, average training loss: 5912.78, base loss: 20511.11
[INFO 2017-06-26 12:50:33,714 main.py:50] epoch 3948, training loss: 5876.05, average training loss: 5912.64, base loss: 20511.46
[INFO 2017-06-26 12:50:34,073 main.py:50] epoch 3949, training loss: 5792.86, average training loss: 5912.40, base loss: 20511.35
[INFO 2017-06-26 12:50:34,431 main.py:50] epoch 3950, training loss: 5866.76, average training loss: 5912.23, base loss: 20511.24
[INFO 2017-06-26 12:50:34,791 main.py:50] epoch 3951, training loss: 5816.24, average training loss: 5911.99, base loss: 20511.21
[INFO 2017-06-26 12:50:35,149 main.py:50] epoch 3952, training loss: 5778.96, average training loss: 5911.77, base loss: 20511.22
[INFO 2017-06-26 12:50:35,509 main.py:50] epoch 3953, training loss: 5802.31, average training loss: 5911.54, base loss: 20511.02
[INFO 2017-06-26 12:50:35,867 main.py:50] epoch 3954, training loss: 5850.27, average training loss: 5911.39, base loss: 20511.28
[INFO 2017-06-26 12:50:36,227 main.py:50] epoch 3955, training loss: 5792.88, average training loss: 5911.19, base loss: 20511.62
[INFO 2017-06-26 12:50:36,588 main.py:50] epoch 3956, training loss: 5795.09, average training loss: 5910.97, base loss: 20511.91
[INFO 2017-06-26 12:50:36,947 main.py:50] epoch 3957, training loss: 5782.46, average training loss: 5910.74, base loss: 20512.31
[INFO 2017-06-26 12:50:37,306 main.py:50] epoch 3958, training loss: 5783.25, average training loss: 5910.49, base loss: 20512.47
[INFO 2017-06-26 12:50:37,665 main.py:50] epoch 3959, training loss: 5831.20, average training loss: 5910.32, base loss: 20512.70
[INFO 2017-06-26 12:50:38,023 main.py:50] epoch 3960, training loss: 5740.21, average training loss: 5910.03, base loss: 20512.48
[INFO 2017-06-26 12:50:38,384 main.py:50] epoch 3961, training loss: 5741.85, average training loss: 5909.77, base loss: 20512.59
[INFO 2017-06-26 12:50:38,743 main.py:50] epoch 3962, training loss: 5826.69, average training loss: 5909.59, base loss: 20512.92
[INFO 2017-06-26 12:50:39,103 main.py:50] epoch 3963, training loss: 5880.21, average training loss: 5909.43, base loss: 20513.20
[INFO 2017-06-26 12:50:39,461 main.py:50] epoch 3964, training loss: 5799.82, average training loss: 5909.15, base loss: 20513.37
[INFO 2017-06-26 12:50:39,821 main.py:50] epoch 3965, training loss: 5840.36, average training loss: 5908.97, base loss: 20513.26
[INFO 2017-06-26 12:50:40,180 main.py:50] epoch 3966, training loss: 5824.15, average training loss: 5908.76, base loss: 20513.09
[INFO 2017-06-26 12:50:40,540 main.py:50] epoch 3967, training loss: 5804.54, average training loss: 5908.54, base loss: 20512.84
[INFO 2017-06-26 12:50:40,899 main.py:50] epoch 3968, training loss: 5819.14, average training loss: 5908.41, base loss: 20512.88
[INFO 2017-06-26 12:50:41,258 main.py:50] epoch 3969, training loss: 5824.17, average training loss: 5908.25, base loss: 20513.05
[INFO 2017-06-26 12:50:41,618 main.py:50] epoch 3970, training loss: 5792.79, average training loss: 5908.03, base loss: 20512.99
[INFO 2017-06-26 12:50:41,977 main.py:50] epoch 3971, training loss: 5811.00, average training loss: 5907.79, base loss: 20513.09
[INFO 2017-06-26 12:50:42,336 main.py:50] epoch 3972, training loss: 5828.09, average training loss: 5907.62, base loss: 20513.17
[INFO 2017-06-26 12:50:42,696 main.py:50] epoch 3973, training loss: 5793.74, average training loss: 5907.42, base loss: 20513.26
[INFO 2017-06-26 12:50:43,055 main.py:50] epoch 3974, training loss: 5788.85, average training loss: 5907.21, base loss: 20513.22
[INFO 2017-06-26 12:50:43,415 main.py:50] epoch 3975, training loss: 5833.22, average training loss: 5907.04, base loss: 20513.40
[INFO 2017-06-26 12:50:43,774 main.py:50] epoch 3976, training loss: 5843.17, average training loss: 5906.82, base loss: 20513.58
[INFO 2017-06-26 12:50:44,131 main.py:50] epoch 3977, training loss: 5834.58, average training loss: 5906.71, base loss: 20513.80
[INFO 2017-06-26 12:50:44,492 main.py:50] epoch 3978, training loss: 5786.98, average training loss: 5906.53, base loss: 20513.95
[INFO 2017-06-26 12:50:44,851 main.py:50] epoch 3979, training loss: 5847.81, average training loss: 5906.40, base loss: 20513.92
[INFO 2017-06-26 12:50:45,211 main.py:50] epoch 3980, training loss: 5818.47, average training loss: 5906.24, base loss: 20513.92
[INFO 2017-06-26 12:50:45,571 main.py:50] epoch 3981, training loss: 5891.09, average training loss: 5906.07, base loss: 20513.76
[INFO 2017-06-26 12:50:45,931 main.py:50] epoch 3982, training loss: 5865.77, average training loss: 5905.92, base loss: 20514.07
[INFO 2017-06-26 12:50:46,289 main.py:50] epoch 3983, training loss: 5826.26, average training loss: 5905.70, base loss: 20513.98
[INFO 2017-06-26 12:50:46,649 main.py:50] epoch 3984, training loss: 5776.00, average training loss: 5905.49, base loss: 20514.23
[INFO 2017-06-26 12:50:47,008 main.py:50] epoch 3985, training loss: 5836.03, average training loss: 5905.33, base loss: 20514.59
[INFO 2017-06-26 12:50:47,367 main.py:50] epoch 3986, training loss: 5803.32, average training loss: 5905.10, base loss: 20514.68
[INFO 2017-06-26 12:50:47,727 main.py:50] epoch 3987, training loss: 5885.21, average training loss: 5905.01, base loss: 20514.90
[INFO 2017-06-26 12:50:48,087 main.py:50] epoch 3988, training loss: 5833.41, average training loss: 5904.89, base loss: 20515.06
[INFO 2017-06-26 12:50:48,447 main.py:50] epoch 3989, training loss: 5827.10, average training loss: 5904.64, base loss: 20515.12
[INFO 2017-06-26 12:50:48,806 main.py:50] epoch 3990, training loss: 5777.26, average training loss: 5904.43, base loss: 20515.21
[INFO 2017-06-26 12:50:49,168 main.py:50] epoch 3991, training loss: 5782.90, average training loss: 5904.23, base loss: 20514.99
[INFO 2017-06-26 12:50:49,528 main.py:50] epoch 3992, training loss: 5887.17, average training loss: 5904.15, base loss: 20515.02
[INFO 2017-06-26 12:50:49,885 main.py:50] epoch 3993, training loss: 5737.88, average training loss: 5903.94, base loss: 20514.87
[INFO 2017-06-26 12:50:50,242 main.py:50] epoch 3994, training loss: 5871.69, average training loss: 5903.80, base loss: 20514.84
[INFO 2017-06-26 12:50:50,600 main.py:50] epoch 3995, training loss: 5868.21, average training loss: 5903.62, base loss: 20515.10
[INFO 2017-06-26 12:50:50,959 main.py:50] epoch 3996, training loss: 5845.56, average training loss: 5903.41, base loss: 20515.13
[INFO 2017-06-26 12:50:51,318 main.py:50] epoch 3997, training loss: 5885.21, average training loss: 5903.26, base loss: 20515.05
[INFO 2017-06-26 12:50:51,677 main.py:50] epoch 3998, training loss: 5843.65, average training loss: 5903.07, base loss: 20515.32
[INFO 2017-06-26 12:50:52,037 main.py:50] epoch 3999, training loss: 5810.35, average training loss: 5902.86, base loss: 20515.19
[INFO 2017-06-26 12:50:52,037 main.py:52] epoch 3999, testing
[INFO 2017-06-26 12:50:53,512 main.py:103] average testing loss: 5836.90, base loss: 20505.41
[INFO 2017-06-26 12:50:53,513 main.py:76] current best accuracy: 5827.19
[INFO 2017-06-26 12:50:53,872 main.py:50] epoch 4000, training loss: 5874.86, average training loss: 5902.64, base loss: 20515.20
[INFO 2017-06-26 12:50:54,230 main.py:50] epoch 4001, training loss: 5802.81, average training loss: 5902.43, base loss: 20515.02
[INFO 2017-06-26 12:50:54,590 main.py:50] epoch 4002, training loss: 5801.44, average training loss: 5902.25, base loss: 20515.13
[INFO 2017-06-26 12:50:54,949 main.py:50] epoch 4003, training loss: 5799.69, average training loss: 5902.01, base loss: 20515.08
[INFO 2017-06-26 12:50:55,309 main.py:50] epoch 4004, training loss: 5777.04, average training loss: 5901.83, base loss: 20515.00
[INFO 2017-06-26 12:50:55,670 main.py:50] epoch 4005, training loss: 5764.98, average training loss: 5901.65, base loss: 20515.14
[INFO 2017-06-26 12:50:56,029 main.py:50] epoch 4006, training loss: 5820.02, average training loss: 5901.44, base loss: 20515.53
[INFO 2017-06-26 12:50:56,390 main.py:50] epoch 4007, training loss: 5834.09, average training loss: 5901.21, base loss: 20515.49
[INFO 2017-06-26 12:50:56,751 main.py:50] epoch 4008, training loss: 5724.43, average training loss: 5900.85, base loss: 20514.89
[INFO 2017-06-26 12:50:57,111 main.py:50] epoch 4009, training loss: 5786.17, average training loss: 5900.65, base loss: 20514.84
[INFO 2017-06-26 12:50:57,483 main.py:50] epoch 4010, training loss: 5833.15, average training loss: 5900.47, base loss: 20514.65
[INFO 2017-06-26 12:50:57,842 main.py:50] epoch 4011, training loss: 5797.03, average training loss: 5900.30, base loss: 20514.50
[INFO 2017-06-26 12:50:58,202 main.py:50] epoch 4012, training loss: 5764.65, average training loss: 5900.04, base loss: 20514.27
[INFO 2017-06-26 12:50:58,561 main.py:50] epoch 4013, training loss: 5787.83, average training loss: 5899.80, base loss: 20514.16
[INFO 2017-06-26 12:50:58,921 main.py:50] epoch 4014, training loss: 5777.90, average training loss: 5899.57, base loss: 20514.41
[INFO 2017-06-26 12:50:59,280 main.py:50] epoch 4015, training loss: 5798.81, average training loss: 5899.36, base loss: 20514.08
[INFO 2017-06-26 12:50:59,640 main.py:50] epoch 4016, training loss: 5759.23, average training loss: 5899.18, base loss: 20513.99
[INFO 2017-06-26 12:51:00,000 main.py:50] epoch 4017, training loss: 5847.62, average training loss: 5899.08, base loss: 20514.10
[INFO 2017-06-26 12:51:00,359 main.py:50] epoch 4018, training loss: 5886.14, average training loss: 5899.01, base loss: 20514.31
[INFO 2017-06-26 12:51:00,718 main.py:50] epoch 4019, training loss: 5891.20, average training loss: 5898.88, base loss: 20514.37
[INFO 2017-06-26 12:51:01,078 main.py:50] epoch 4020, training loss: 5828.07, average training loss: 5898.63, base loss: 20514.00
[INFO 2017-06-26 12:51:01,438 main.py:50] epoch 4021, training loss: 5847.11, average training loss: 5898.50, base loss: 20513.90
[INFO 2017-06-26 12:51:01,797 main.py:50] epoch 4022, training loss: 5797.56, average training loss: 5898.30, base loss: 20514.22
[INFO 2017-06-26 12:51:02,158 main.py:50] epoch 4023, training loss: 5807.88, average training loss: 5898.09, base loss: 20514.20
[INFO 2017-06-26 12:51:02,517 main.py:50] epoch 4024, training loss: 5834.79, average training loss: 5897.98, base loss: 20514.33
[INFO 2017-06-26 12:51:02,875 main.py:50] epoch 4025, training loss: 5877.52, average training loss: 5897.90, base loss: 20514.65
[INFO 2017-06-26 12:51:03,235 main.py:50] epoch 4026, training loss: 5828.89, average training loss: 5897.75, base loss: 20514.69
[INFO 2017-06-26 12:51:03,595 main.py:50] epoch 4027, training loss: 5843.14, average training loss: 5897.57, base loss: 20514.82
[INFO 2017-06-26 12:51:03,953 main.py:50] epoch 4028, training loss: 5865.69, average training loss: 5897.46, base loss: 20514.77
[INFO 2017-06-26 12:51:04,314 main.py:50] epoch 4029, training loss: 5851.08, average training loss: 5897.34, base loss: 20514.74
[INFO 2017-06-26 12:51:04,672 main.py:50] epoch 4030, training loss: 5744.17, average training loss: 5897.10, base loss: 20514.81
[INFO 2017-06-26 12:51:05,032 main.py:50] epoch 4031, training loss: 5783.00, average training loss: 5896.84, base loss: 20514.57
[INFO 2017-06-26 12:51:05,390 main.py:50] epoch 4032, training loss: 5785.02, average training loss: 5896.61, base loss: 20514.30
[INFO 2017-06-26 12:51:05,751 main.py:50] epoch 4033, training loss: 5746.54, average training loss: 5896.37, base loss: 20514.33
[INFO 2017-06-26 12:51:06,109 main.py:50] epoch 4034, training loss: 5902.44, average training loss: 5896.20, base loss: 20513.97
[INFO 2017-06-26 12:51:06,468 main.py:50] epoch 4035, training loss: 5764.86, average training loss: 5895.93, base loss: 20513.95
[INFO 2017-06-26 12:51:06,827 main.py:50] epoch 4036, training loss: 5871.57, average training loss: 5895.90, base loss: 20514.41
[INFO 2017-06-26 12:51:07,187 main.py:50] epoch 4037, training loss: 5798.62, average training loss: 5895.64, base loss: 20514.90
[INFO 2017-06-26 12:51:07,547 main.py:50] epoch 4038, training loss: 5872.47, average training loss: 5895.48, base loss: 20515.04
[INFO 2017-06-26 12:51:07,905 main.py:50] epoch 4039, training loss: 5797.14, average training loss: 5895.28, base loss: 20514.87
[INFO 2017-06-26 12:51:08,265 main.py:50] epoch 4040, training loss: 5849.20, average training loss: 5895.08, base loss: 20514.78
[INFO 2017-06-26 12:51:08,626 main.py:50] epoch 4041, training loss: 5761.60, average training loss: 5894.88, base loss: 20514.87
[INFO 2017-06-26 12:51:08,984 main.py:50] epoch 4042, training loss: 5808.28, average training loss: 5894.72, base loss: 20515.28
[INFO 2017-06-26 12:51:09,343 main.py:50] epoch 4043, training loss: 5851.46, average training loss: 5894.55, base loss: 20515.41
[INFO 2017-06-26 12:51:09,703 main.py:50] epoch 4044, training loss: 5919.95, average training loss: 5894.43, base loss: 20515.42
[INFO 2017-06-26 12:51:10,062 main.py:50] epoch 4045, training loss: 5828.10, average training loss: 5894.30, base loss: 20515.80
[INFO 2017-06-26 12:51:10,421 main.py:50] epoch 4046, training loss: 5891.03, average training loss: 5894.17, base loss: 20516.14
[INFO 2017-06-26 12:51:10,782 main.py:50] epoch 4047, training loss: 5821.97, average training loss: 5893.97, base loss: 20516.04
[INFO 2017-06-26 12:51:11,140 main.py:50] epoch 4048, training loss: 5890.47, average training loss: 5893.86, base loss: 20515.79
[INFO 2017-06-26 12:51:11,500 main.py:50] epoch 4049, training loss: 5800.96, average training loss: 5893.59, base loss: 20515.31
[INFO 2017-06-26 12:51:11,860 main.py:50] epoch 4050, training loss: 5868.41, average training loss: 5893.48, base loss: 20515.17
[INFO 2017-06-26 12:51:12,219 main.py:50] epoch 4051, training loss: 5817.41, average training loss: 5893.33, base loss: 20515.02
[INFO 2017-06-26 12:51:12,580 main.py:50] epoch 4052, training loss: 5895.74, average training loss: 5893.26, base loss: 20514.95
[INFO 2017-06-26 12:51:12,939 main.py:50] epoch 4053, training loss: 5923.78, average training loss: 5893.20, base loss: 20515.47
[INFO 2017-06-26 12:51:13,299 main.py:50] epoch 4054, training loss: 5876.16, average training loss: 5892.99, base loss: 20515.31
[INFO 2017-06-26 12:51:13,661 main.py:50] epoch 4055, training loss: 5892.14, average training loss: 5892.85, base loss: 20515.14
[INFO 2017-06-26 12:51:14,019 main.py:50] epoch 4056, training loss: 5800.96, average training loss: 5892.66, base loss: 20514.84
[INFO 2017-06-26 12:51:14,378 main.py:50] epoch 4057, training loss: 5932.02, average training loss: 5892.52, base loss: 20514.70
[INFO 2017-06-26 12:51:14,739 main.py:50] epoch 4058, training loss: 5918.30, average training loss: 5892.43, base loss: 20514.44
[INFO 2017-06-26 12:51:15,100 main.py:50] epoch 4059, training loss: 5941.44, average training loss: 5892.33, base loss: 20514.59
[INFO 2017-06-26 12:51:15,459 main.py:50] epoch 4060, training loss: 5846.75, average training loss: 5892.17, base loss: 20514.46
[INFO 2017-06-26 12:51:15,817 main.py:50] epoch 4061, training loss: 5950.46, average training loss: 5892.19, base loss: 20514.78
[INFO 2017-06-26 12:51:16,177 main.py:50] epoch 4062, training loss: 5885.55, average training loss: 5892.05, base loss: 20515.05
[INFO 2017-06-26 12:51:16,536 main.py:50] epoch 4063, training loss: 5894.07, average training loss: 5891.95, base loss: 20514.99
[INFO 2017-06-26 12:51:16,894 main.py:50] epoch 4064, training loss: 5856.97, average training loss: 5891.71, base loss: 20514.92
[INFO 2017-06-26 12:51:17,253 main.py:50] epoch 4065, training loss: 5862.39, average training loss: 5891.60, base loss: 20515.10
[INFO 2017-06-26 12:51:17,613 main.py:50] epoch 4066, training loss: 5921.52, average training loss: 5891.46, base loss: 20514.85
[INFO 2017-06-26 12:51:17,974 main.py:50] epoch 4067, training loss: 5819.03, average training loss: 5891.34, base loss: 20515.30
[INFO 2017-06-26 12:51:18,333 main.py:50] epoch 4068, training loss: 5917.47, average training loss: 5891.28, base loss: 20515.75
[INFO 2017-06-26 12:51:18,693 main.py:50] epoch 4069, training loss: 5807.57, average training loss: 5891.12, base loss: 20516.06
[INFO 2017-06-26 12:51:19,052 main.py:50] epoch 4070, training loss: 5833.55, average training loss: 5891.06, base loss: 20516.37
[INFO 2017-06-26 12:51:19,411 main.py:50] epoch 4071, training loss: 5826.84, average training loss: 5890.89, base loss: 20516.19
[INFO 2017-06-26 12:51:19,771 main.py:50] epoch 4072, training loss: 5826.52, average training loss: 5890.73, base loss: 20516.30
[INFO 2017-06-26 12:51:20,130 main.py:50] epoch 4073, training loss: 5803.36, average training loss: 5890.56, base loss: 20516.43
[INFO 2017-06-26 12:51:20,488 main.py:50] epoch 4074, training loss: 5827.05, average training loss: 5890.39, base loss: 20516.39
[INFO 2017-06-26 12:51:20,847 main.py:50] epoch 4075, training loss: 5831.36, average training loss: 5890.18, base loss: 20516.15
[INFO 2017-06-26 12:51:21,208 main.py:50] epoch 4076, training loss: 5775.75, average training loss: 5890.01, base loss: 20516.16
[INFO 2017-06-26 12:51:21,567 main.py:50] epoch 4077, training loss: 5831.23, average training loss: 5889.88, base loss: 20516.30
[INFO 2017-06-26 12:51:21,926 main.py:50] epoch 4078, training loss: 5782.82, average training loss: 5889.71, base loss: 20516.43
[INFO 2017-06-26 12:51:22,285 main.py:50] epoch 4079, training loss: 5819.84, average training loss: 5889.52, base loss: 20516.67
[INFO 2017-06-26 12:51:22,643 main.py:50] epoch 4080, training loss: 5864.63, average training loss: 5889.36, base loss: 20516.63
[INFO 2017-06-26 12:51:23,003 main.py:50] epoch 4081, training loss: 5871.10, average training loss: 5889.28, base loss: 20516.53
[INFO 2017-06-26 12:51:23,362 main.py:50] epoch 4082, training loss: 5825.31, average training loss: 5889.13, base loss: 20516.90
[INFO 2017-06-26 12:51:23,723 main.py:50] epoch 4083, training loss: 5766.45, average training loss: 5888.90, base loss: 20516.70
[INFO 2017-06-26 12:51:24,082 main.py:50] epoch 4084, training loss: 5775.25, average training loss: 5888.77, base loss: 20516.70
[INFO 2017-06-26 12:51:24,441 main.py:50] epoch 4085, training loss: 5805.15, average training loss: 5888.60, base loss: 20516.54
[INFO 2017-06-26 12:51:24,801 main.py:50] epoch 4086, training loss: 5770.95, average training loss: 5888.38, base loss: 20516.44
[INFO 2017-06-26 12:51:25,161 main.py:50] epoch 4087, training loss: 5870.51, average training loss: 5888.30, base loss: 20516.69
[INFO 2017-06-26 12:51:25,520 main.py:50] epoch 4088, training loss: 5818.55, average training loss: 5888.14, base loss: 20516.94
[INFO 2017-06-26 12:51:25,879 main.py:50] epoch 4089, training loss: 5847.13, average training loss: 5888.02, base loss: 20516.92
[INFO 2017-06-26 12:51:26,239 main.py:50] epoch 4090, training loss: 5813.01, average training loss: 5887.84, base loss: 20517.27
[INFO 2017-06-26 12:51:26,598 main.py:50] epoch 4091, training loss: 5840.34, average training loss: 5887.70, base loss: 20517.35
[INFO 2017-06-26 12:51:26,957 main.py:50] epoch 4092, training loss: 5849.02, average training loss: 5887.48, base loss: 20516.63
[INFO 2017-06-26 12:51:27,316 main.py:50] epoch 4093, training loss: 5875.27, average training loss: 5887.37, base loss: 20516.29
[INFO 2017-06-26 12:51:27,677 main.py:50] epoch 4094, training loss: 5918.24, average training loss: 5887.33, base loss: 20516.24
[INFO 2017-06-26 12:51:28,036 main.py:50] epoch 4095, training loss: 5833.96, average training loss: 5887.19, base loss: 20516.26
[INFO 2017-06-26 12:51:28,394 main.py:50] epoch 4096, training loss: 5858.65, average training loss: 5887.11, base loss: 20516.39
[INFO 2017-06-26 12:51:28,756 main.py:50] epoch 4097, training loss: 5867.75, average training loss: 5887.00, base loss: 20516.63
[INFO 2017-06-26 12:51:29,114 main.py:50] epoch 4098, training loss: 5796.74, average training loss: 5886.83, base loss: 20516.78
[INFO 2017-06-26 12:51:29,472 main.py:50] epoch 4099, training loss: 5844.16, average training loss: 5886.75, base loss: 20516.84
[INFO 2017-06-26 12:51:29,472 main.py:52] epoch 4099, testing
[INFO 2017-06-26 12:51:30,939 main.py:103] average testing loss: 5796.41, base loss: 20560.07
[INFO 2017-06-26 12:51:30,940 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 12:51:30,946 main.py:76] current best accuracy: 5796.41
[INFO 2017-06-26 12:51:31,305 main.py:50] epoch 4100, training loss: 5839.09, average training loss: 5886.61, base loss: 20516.98
[INFO 2017-06-26 12:51:31,664 main.py:50] epoch 4101, training loss: 5809.01, average training loss: 5886.42, base loss: 20517.00
[INFO 2017-06-26 12:51:32,024 main.py:50] epoch 4102, training loss: 5741.54, average training loss: 5886.20, base loss: 20516.87
[INFO 2017-06-26 12:51:32,383 main.py:50] epoch 4103, training loss: 5820.79, average training loss: 5886.09, base loss: 20516.60
[INFO 2017-06-26 12:51:32,744 main.py:50] epoch 4104, training loss: 5773.65, average training loss: 5885.87, base loss: 20516.56
[INFO 2017-06-26 12:51:33,101 main.py:50] epoch 4105, training loss: 5861.59, average training loss: 5885.79, base loss: 20517.04
[INFO 2017-06-26 12:51:33,461 main.py:50] epoch 4106, training loss: 5786.07, average training loss: 5885.62, base loss: 20516.75
[INFO 2017-06-26 12:51:33,819 main.py:50] epoch 4107, training loss: 5890.31, average training loss: 5885.55, base loss: 20516.78
[INFO 2017-06-26 12:51:34,179 main.py:50] epoch 4108, training loss: 5824.94, average training loss: 5885.46, base loss: 20516.96
[INFO 2017-06-26 12:51:34,539 main.py:50] epoch 4109, training loss: 5833.25, average training loss: 5885.35, base loss: 20516.98
[INFO 2017-06-26 12:51:34,897 main.py:50] epoch 4110, training loss: 5762.50, average training loss: 5885.06, base loss: 20516.62
[INFO 2017-06-26 12:51:35,257 main.py:50] epoch 4111, training loss: 5874.65, average training loss: 5884.96, base loss: 20516.45
[INFO 2017-06-26 12:51:35,616 main.py:50] epoch 4112, training loss: 5871.82, average training loss: 5884.81, base loss: 20516.65
[INFO 2017-06-26 12:51:35,975 main.py:50] epoch 4113, training loss: 5853.52, average training loss: 5884.71, base loss: 20516.60
[INFO 2017-06-26 12:51:36,334 main.py:50] epoch 4114, training loss: 5910.23, average training loss: 5884.69, base loss: 20516.64
[INFO 2017-06-26 12:51:36,694 main.py:50] epoch 4115, training loss: 5888.12, average training loss: 5884.59, base loss: 20516.97
[INFO 2017-06-26 12:51:37,052 main.py:50] epoch 4116, training loss: 5942.04, average training loss: 5884.52, base loss: 20516.68
[INFO 2017-06-26 12:51:37,411 main.py:50] epoch 4117, training loss: 5791.45, average training loss: 5884.39, base loss: 20516.90
[INFO 2017-06-26 12:51:37,772 main.py:50] epoch 4118, training loss: 5995.01, average training loss: 5884.41, base loss: 20516.81
[INFO 2017-06-26 12:51:38,132 main.py:50] epoch 4119, training loss: 5836.45, average training loss: 5884.29, base loss: 20516.68
[INFO 2017-06-26 12:51:38,491 main.py:50] epoch 4120, training loss: 5900.32, average training loss: 5884.19, base loss: 20516.23
[INFO 2017-06-26 12:51:38,851 main.py:50] epoch 4121, training loss: 5960.59, average training loss: 5884.16, base loss: 20516.59
[INFO 2017-06-26 12:51:39,210 main.py:50] epoch 4122, training loss: 5775.03, average training loss: 5883.92, base loss: 20516.49
[INFO 2017-06-26 12:51:39,571 main.py:50] epoch 4123, training loss: 5900.99, average training loss: 5883.78, base loss: 20516.39
[INFO 2017-06-26 12:51:39,931 main.py:50] epoch 4124, training loss: 5834.31, average training loss: 5883.63, base loss: 20516.61
[INFO 2017-06-26 12:51:40,289 main.py:50] epoch 4125, training loss: 5881.92, average training loss: 5883.52, base loss: 20516.45
[INFO 2017-06-26 12:51:40,647 main.py:50] epoch 4126, training loss: 5797.81, average training loss: 5883.37, base loss: 20516.51
[INFO 2017-06-26 12:51:41,007 main.py:50] epoch 4127, training loss: 5766.62, average training loss: 5883.13, base loss: 20516.57
[INFO 2017-06-26 12:51:41,366 main.py:50] epoch 4128, training loss: 5812.26, average training loss: 5882.98, base loss: 20517.29
[INFO 2017-06-26 12:51:41,738 main.py:50] epoch 4129, training loss: 5815.04, average training loss: 5882.81, base loss: 20517.58
[INFO 2017-06-26 12:51:42,097 main.py:50] epoch 4130, training loss: 5796.79, average training loss: 5882.60, base loss: 20517.99
[INFO 2017-06-26 12:51:42,457 main.py:50] epoch 4131, training loss: 5742.89, average training loss: 5882.37, base loss: 20518.19
[INFO 2017-06-26 12:51:42,815 main.py:50] epoch 4132, training loss: 5814.38, average training loss: 5882.24, base loss: 20518.63
[INFO 2017-06-26 12:51:43,176 main.py:50] epoch 4133, training loss: 5807.17, average training loss: 5882.03, base loss: 20518.63
[INFO 2017-06-26 12:51:43,536 main.py:50] epoch 4134, training loss: 5807.08, average training loss: 5881.87, base loss: 20518.64
[INFO 2017-06-26 12:51:43,895 main.py:50] epoch 4135, training loss: 5763.73, average training loss: 5881.67, base loss: 20518.40
[INFO 2017-06-26 12:51:44,254 main.py:50] epoch 4136, training loss: 5773.33, average training loss: 5881.50, base loss: 20518.17
[INFO 2017-06-26 12:51:44,612 main.py:50] epoch 4137, training loss: 5820.44, average training loss: 5881.31, base loss: 20517.79
[INFO 2017-06-26 12:51:44,971 main.py:50] epoch 4138, training loss: 5773.01, average training loss: 5881.18, base loss: 20517.98
[INFO 2017-06-26 12:51:45,330 main.py:50] epoch 4139, training loss: 5802.08, average training loss: 5881.05, base loss: 20518.01
[INFO 2017-06-26 12:51:45,688 main.py:50] epoch 4140, training loss: 5776.82, average training loss: 5880.86, base loss: 20517.86
[INFO 2017-06-26 12:51:46,046 main.py:50] epoch 4141, training loss: 5799.19, average training loss: 5880.73, base loss: 20518.11
[INFO 2017-06-26 12:51:46,405 main.py:50] epoch 4142, training loss: 5764.98, average training loss: 5880.53, base loss: 20518.02
[INFO 2017-06-26 12:51:46,764 main.py:50] epoch 4143, training loss: 5784.60, average training loss: 5880.37, base loss: 20518.19
[INFO 2017-06-26 12:51:47,123 main.py:50] epoch 4144, training loss: 5763.32, average training loss: 5880.18, base loss: 20518.15
[INFO 2017-06-26 12:51:47,481 main.py:50] epoch 4145, training loss: 5778.46, average training loss: 5879.97, base loss: 20518.25
[INFO 2017-06-26 12:51:47,840 main.py:50] epoch 4146, training loss: 5785.15, average training loss: 5879.74, base loss: 20518.01
[INFO 2017-06-26 12:51:48,200 main.py:50] epoch 4147, training loss: 5806.49, average training loss: 5879.61, base loss: 20518.18
[INFO 2017-06-26 12:51:48,557 main.py:50] epoch 4148, training loss: 5804.10, average training loss: 5879.49, base loss: 20518.46
[INFO 2017-06-26 12:51:48,917 main.py:50] epoch 4149, training loss: 5799.00, average training loss: 5879.31, base loss: 20518.91
[INFO 2017-06-26 12:51:49,277 main.py:50] epoch 4150, training loss: 5887.99, average training loss: 5879.25, base loss: 20518.82
[INFO 2017-06-26 12:51:49,636 main.py:50] epoch 4151, training loss: 5856.49, average training loss: 5879.13, base loss: 20519.26
[INFO 2017-06-26 12:51:49,995 main.py:50] epoch 4152, training loss: 5797.90, average training loss: 5878.97, base loss: 20519.05
[INFO 2017-06-26 12:51:50,354 main.py:50] epoch 4153, training loss: 5846.16, average training loss: 5878.72, base loss: 20518.52
[INFO 2017-06-26 12:51:50,713 main.py:50] epoch 4154, training loss: 5717.09, average training loss: 5878.43, base loss: 20517.99
[INFO 2017-06-26 12:51:51,072 main.py:50] epoch 4155, training loss: 5794.37, average training loss: 5878.14, base loss: 20518.13
[INFO 2017-06-26 12:51:51,432 main.py:50] epoch 4156, training loss: 5765.13, average training loss: 5877.85, base loss: 20517.75
[INFO 2017-06-26 12:51:51,791 main.py:50] epoch 4157, training loss: 5777.90, average training loss: 5877.64, base loss: 20518.04
[INFO 2017-06-26 12:51:52,150 main.py:50] epoch 4158, training loss: 5776.07, average training loss: 5877.42, base loss: 20517.98
[INFO 2017-06-26 12:51:52,509 main.py:50] epoch 4159, training loss: 5781.96, average training loss: 5877.29, base loss: 20517.88
[INFO 2017-06-26 12:51:52,870 main.py:50] epoch 4160, training loss: 5786.22, average training loss: 5877.14, base loss: 20517.85
[INFO 2017-06-26 12:51:53,229 main.py:50] epoch 4161, training loss: 5838.28, average training loss: 5877.09, base loss: 20518.01
[INFO 2017-06-26 12:51:53,589 main.py:50] epoch 4162, training loss: 5806.75, average training loss: 5876.95, base loss: 20517.80
[INFO 2017-06-26 12:51:53,946 main.py:50] epoch 4163, training loss: 5859.12, average training loss: 5876.93, base loss: 20518.06
[INFO 2017-06-26 12:51:54,307 main.py:50] epoch 4164, training loss: 5820.94, average training loss: 5876.77, base loss: 20518.11
[INFO 2017-06-26 12:51:54,666 main.py:50] epoch 4165, training loss: 5799.87, average training loss: 5876.61, base loss: 20518.15
[INFO 2017-06-26 12:51:55,025 main.py:50] epoch 4166, training loss: 5861.32, average training loss: 5876.47, base loss: 20518.01
[INFO 2017-06-26 12:51:55,385 main.py:50] epoch 4167, training loss: 5811.03, average training loss: 5876.36, base loss: 20518.16
[INFO 2017-06-26 12:51:55,745 main.py:50] epoch 4168, training loss: 5793.66, average training loss: 5876.18, base loss: 20518.24
[INFO 2017-06-26 12:51:56,104 main.py:50] epoch 4169, training loss: 5804.56, average training loss: 5876.03, base loss: 20518.03
[INFO 2017-06-26 12:51:56,464 main.py:50] epoch 4170, training loss: 5827.58, average training loss: 5875.89, base loss: 20517.84
[INFO 2017-06-26 12:51:56,824 main.py:50] epoch 4171, training loss: 5814.96, average training loss: 5875.78, base loss: 20517.89
[INFO 2017-06-26 12:51:57,183 main.py:50] epoch 4172, training loss: 5766.33, average training loss: 5875.62, base loss: 20518.10
[INFO 2017-06-26 12:51:57,542 main.py:50] epoch 4173, training loss: 5824.05, average training loss: 5875.50, base loss: 20518.36
[INFO 2017-06-26 12:51:57,903 main.py:50] epoch 4174, training loss: 5783.22, average training loss: 5875.32, base loss: 20518.49
[INFO 2017-06-26 12:51:58,262 main.py:50] epoch 4175, training loss: 5814.53, average training loss: 5875.14, base loss: 20518.48
[INFO 2017-06-26 12:51:58,620 main.py:50] epoch 4176, training loss: 5818.69, average training loss: 5875.05, base loss: 20518.69
[INFO 2017-06-26 12:51:58,977 main.py:50] epoch 4177, training loss: 5863.80, average training loss: 5874.96, base loss: 20518.80
[INFO 2017-06-26 12:51:59,337 main.py:50] epoch 4178, training loss: 5721.96, average training loss: 5874.67, base loss: 20518.90
[INFO 2017-06-26 12:51:59,698 main.py:50] epoch 4179, training loss: 5714.63, average training loss: 5874.43, base loss: 20518.73
[INFO 2017-06-26 12:52:00,057 main.py:50] epoch 4180, training loss: 5799.18, average training loss: 5874.32, base loss: 20519.13
[INFO 2017-06-26 12:52:00,415 main.py:50] epoch 4181, training loss: 5808.63, average training loss: 5874.20, base loss: 20519.46
[INFO 2017-06-26 12:52:00,776 main.py:50] epoch 4182, training loss: 5788.72, average training loss: 5874.04, base loss: 20519.45
[INFO 2017-06-26 12:52:01,136 main.py:50] epoch 4183, training loss: 5795.03, average training loss: 5873.86, base loss: 20519.78
[INFO 2017-06-26 12:52:01,495 main.py:50] epoch 4184, training loss: 5796.53, average training loss: 5873.67, base loss: 20519.84
[INFO 2017-06-26 12:52:01,854 main.py:50] epoch 4185, training loss: 5777.98, average training loss: 5873.50, base loss: 20519.50
[INFO 2017-06-26 12:52:02,214 main.py:50] epoch 4186, training loss: 5727.57, average training loss: 5873.28, base loss: 20519.19
[INFO 2017-06-26 12:52:02,572 main.py:50] epoch 4187, training loss: 5807.70, average training loss: 5873.07, base loss: 20519.38
[INFO 2017-06-26 12:52:02,932 main.py:50] epoch 4188, training loss: 5828.86, average training loss: 5872.96, base loss: 20519.07
[INFO 2017-06-26 12:52:03,291 main.py:50] epoch 4189, training loss: 5778.96, average training loss: 5872.70, base loss: 20518.82
[INFO 2017-06-26 12:52:03,651 main.py:50] epoch 4190, training loss: 5787.97, average training loss: 5872.57, base loss: 20519.10
[INFO 2017-06-26 12:52:04,011 main.py:50] epoch 4191, training loss: 5828.72, average training loss: 5872.44, base loss: 20519.34
[INFO 2017-06-26 12:52:04,371 main.py:50] epoch 4192, training loss: 5735.09, average training loss: 5872.25, base loss: 20519.14
[INFO 2017-06-26 12:52:04,732 main.py:50] epoch 4193, training loss: 5810.76, average training loss: 5872.06, base loss: 20519.49
[INFO 2017-06-26 12:52:05,091 main.py:50] epoch 4194, training loss: 5721.14, average training loss: 5871.80, base loss: 20519.94
[INFO 2017-06-26 12:52:05,451 main.py:50] epoch 4195, training loss: 5740.95, average training loss: 5871.57, base loss: 20520.22
[INFO 2017-06-26 12:52:05,811 main.py:50] epoch 4196, training loss: 5748.11, average training loss: 5871.42, base loss: 20520.26
[INFO 2017-06-26 12:52:06,171 main.py:50] epoch 4197, training loss: 5765.91, average training loss: 5871.23, base loss: 20520.05
[INFO 2017-06-26 12:52:06,531 main.py:50] epoch 4198, training loss: 5798.97, average training loss: 5871.10, base loss: 20520.30
[INFO 2017-06-26 12:52:06,890 main.py:50] epoch 4199, training loss: 5744.13, average training loss: 5870.84, base loss: 20520.32
[INFO 2017-06-26 12:52:06,891 main.py:52] epoch 4199, testing
[INFO 2017-06-26 12:52:08,365 main.py:103] average testing loss: 5762.19, base loss: 20460.77
[INFO 2017-06-26 12:52:08,366 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 12:52:08,372 main.py:76] current best accuracy: 5762.19
[INFO 2017-06-26 12:52:08,732 main.py:50] epoch 4200, training loss: 5748.41, average training loss: 5870.59, base loss: 20520.10
[INFO 2017-06-26 12:52:09,091 main.py:50] epoch 4201, training loss: 5705.89, average training loss: 5870.44, base loss: 20520.60
[INFO 2017-06-26 12:52:09,451 main.py:50] epoch 4202, training loss: 5680.80, average training loss: 5870.17, base loss: 20520.91
[INFO 2017-06-26 12:52:09,812 main.py:50] epoch 4203, training loss: 5782.65, average training loss: 5870.01, base loss: 20520.92
[INFO 2017-06-26 12:52:10,173 main.py:50] epoch 4204, training loss: 5767.39, average training loss: 5869.77, base loss: 20520.62
[INFO 2017-06-26 12:52:10,532 main.py:50] epoch 4205, training loss: 5744.30, average training loss: 5869.56, base loss: 20520.54
[INFO 2017-06-26 12:52:10,891 main.py:50] epoch 4206, training loss: 5703.52, average training loss: 5869.23, base loss: 20520.43
[INFO 2017-06-26 12:52:11,250 main.py:50] epoch 4207, training loss: 5779.97, average training loss: 5869.08, base loss: 20520.20
[INFO 2017-06-26 12:52:11,611 main.py:50] epoch 4208, training loss: 5793.00, average training loss: 5868.87, base loss: 20520.27
[INFO 2017-06-26 12:52:11,972 main.py:50] epoch 4209, training loss: 5716.59, average training loss: 5868.62, base loss: 20520.55
[INFO 2017-06-26 12:52:12,331 main.py:50] epoch 4210, training loss: 5716.69, average training loss: 5868.32, base loss: 20520.73
[INFO 2017-06-26 12:52:12,692 main.py:50] epoch 4211, training loss: 5788.62, average training loss: 5868.09, base loss: 20520.55
[INFO 2017-06-26 12:52:13,049 main.py:50] epoch 4212, training loss: 5696.62, average training loss: 5867.79, base loss: 20520.12
[INFO 2017-06-26 12:52:13,409 main.py:50] epoch 4213, training loss: 5775.43, average training loss: 5867.66, base loss: 20520.06
[INFO 2017-06-26 12:52:13,770 main.py:50] epoch 4214, training loss: 5802.27, average training loss: 5867.49, base loss: 20520.34
[INFO 2017-06-26 12:52:14,131 main.py:50] epoch 4215, training loss: 5818.86, average training loss: 5867.38, base loss: 20520.58
[INFO 2017-06-26 12:52:14,491 main.py:50] epoch 4216, training loss: 5800.26, average training loss: 5867.28, base loss: 20520.43
[INFO 2017-06-26 12:52:14,851 main.py:50] epoch 4217, training loss: 5828.13, average training loss: 5867.08, base loss: 20520.42
[INFO 2017-06-26 12:52:15,212 main.py:50] epoch 4218, training loss: 5812.93, average training loss: 5867.04, base loss: 20520.30
[INFO 2017-06-26 12:52:15,571 main.py:50] epoch 4219, training loss: 5788.68, average training loss: 5866.84, base loss: 20520.60
[INFO 2017-06-26 12:52:15,930 main.py:50] epoch 4220, training loss: 5863.47, average training loss: 5866.81, base loss: 20520.68
[INFO 2017-06-26 12:52:16,291 main.py:50] epoch 4221, training loss: 5785.11, average training loss: 5866.58, base loss: 20520.67
[INFO 2017-06-26 12:52:16,650 main.py:50] epoch 4222, training loss: 5856.42, average training loss: 5866.60, base loss: 20520.70
[INFO 2017-06-26 12:52:17,009 main.py:50] epoch 4223, training loss: 5760.01, average training loss: 5866.35, base loss: 20520.60
[INFO 2017-06-26 12:52:17,367 main.py:50] epoch 4224, training loss: 5840.23, average training loss: 5866.21, base loss: 20521.02
[INFO 2017-06-26 12:52:17,726 main.py:50] epoch 4225, training loss: 5778.73, average training loss: 5866.08, base loss: 20521.37
[INFO 2017-06-26 12:52:18,086 main.py:50] epoch 4226, training loss: 5756.98, average training loss: 5865.82, base loss: 20520.91
[INFO 2017-06-26 12:52:18,447 main.py:50] epoch 4227, training loss: 5772.55, average training loss: 5865.65, base loss: 20520.55
[INFO 2017-06-26 12:52:18,807 main.py:50] epoch 4228, training loss: 5808.28, average training loss: 5865.48, base loss: 20520.50
[INFO 2017-06-26 12:52:19,168 main.py:50] epoch 4229, training loss: 5788.25, average training loss: 5865.39, base loss: 20520.75
[INFO 2017-06-26 12:52:19,526 main.py:50] epoch 4230, training loss: 5788.59, average training loss: 5865.23, base loss: 20520.88
[INFO 2017-06-26 12:52:19,885 main.py:50] epoch 4231, training loss: 5801.84, average training loss: 5865.09, base loss: 20521.24
[INFO 2017-06-26 12:52:20,244 main.py:50] epoch 4232, training loss: 5759.71, average training loss: 5864.96, base loss: 20521.19
[INFO 2017-06-26 12:52:20,604 main.py:50] epoch 4233, training loss: 5705.01, average training loss: 5864.65, base loss: 20520.98
[INFO 2017-06-26 12:52:20,962 main.py:50] epoch 4234, training loss: 5791.20, average training loss: 5864.47, base loss: 20521.03
[INFO 2017-06-26 12:52:21,321 main.py:50] epoch 4235, training loss: 5726.18, average training loss: 5864.33, base loss: 20521.31
[INFO 2017-06-26 12:52:21,681 main.py:50] epoch 4236, training loss: 5784.51, average training loss: 5864.16, base loss: 20521.37
[INFO 2017-06-26 12:52:22,040 main.py:50] epoch 4237, training loss: 5770.88, average training loss: 5864.00, base loss: 20521.10
[INFO 2017-06-26 12:52:22,399 main.py:50] epoch 4238, training loss: 5800.40, average training loss: 5863.89, base loss: 20520.95
[INFO 2017-06-26 12:52:22,758 main.py:50] epoch 4239, training loss: 5808.71, average training loss: 5863.76, base loss: 20520.68
[INFO 2017-06-26 12:52:23,119 main.py:50] epoch 4240, training loss: 5763.45, average training loss: 5863.61, base loss: 20521.00
[INFO 2017-06-26 12:52:23,478 main.py:50] epoch 4241, training loss: 5781.29, average training loss: 5863.41, base loss: 20520.93
[INFO 2017-06-26 12:52:23,837 main.py:50] epoch 4242, training loss: 5775.56, average training loss: 5863.24, base loss: 20520.73
[INFO 2017-06-26 12:52:24,198 main.py:50] epoch 4243, training loss: 5864.40, average training loss: 5863.16, base loss: 20520.86
[INFO 2017-06-26 12:52:24,555 main.py:50] epoch 4244, training loss: 5843.16, average training loss: 5863.09, base loss: 20521.03
[INFO 2017-06-26 12:52:24,915 main.py:50] epoch 4245, training loss: 5735.40, average training loss: 5862.94, base loss: 20521.18
[INFO 2017-06-26 12:52:25,273 main.py:50] epoch 4246, training loss: 5805.77, average training loss: 5862.77, base loss: 20521.12
[INFO 2017-06-26 12:52:25,633 main.py:50] epoch 4247, training loss: 5766.67, average training loss: 5862.62, base loss: 20520.96
[INFO 2017-06-26 12:52:26,005 main.py:50] epoch 4248, training loss: 5894.33, average training loss: 5862.61, base loss: 20520.97
[INFO 2017-06-26 12:52:26,364 main.py:50] epoch 4249, training loss: 5795.21, average training loss: 5862.53, base loss: 20520.99
[INFO 2017-06-26 12:52:26,722 main.py:50] epoch 4250, training loss: 5837.95, average training loss: 5862.32, base loss: 20520.96
[INFO 2017-06-26 12:52:27,083 main.py:50] epoch 4251, training loss: 5837.08, average training loss: 5862.15, base loss: 20521.09
[INFO 2017-06-26 12:52:27,442 main.py:50] epoch 4252, training loss: 5727.99, average training loss: 5861.99, base loss: 20520.95
[INFO 2017-06-26 12:52:27,803 main.py:50] epoch 4253, training loss: 5830.92, average training loss: 5861.87, base loss: 20521.40
[INFO 2017-06-26 12:52:28,163 main.py:50] epoch 4254, training loss: 5770.77, average training loss: 5861.76, base loss: 20521.46
[INFO 2017-06-26 12:52:28,521 main.py:50] epoch 4255, training loss: 5813.06, average training loss: 5861.64, base loss: 20521.65
[INFO 2017-06-26 12:52:28,881 main.py:50] epoch 4256, training loss: 5777.42, average training loss: 5861.46, base loss: 20521.54
[INFO 2017-06-26 12:52:29,239 main.py:50] epoch 4257, training loss: 5736.38, average training loss: 5861.17, base loss: 20521.78
[INFO 2017-06-26 12:52:29,598 main.py:50] epoch 4258, training loss: 5829.83, average training loss: 5861.04, base loss: 20521.70
[INFO 2017-06-26 12:52:29,957 main.py:50] epoch 4259, training loss: 5820.86, average training loss: 5860.76, base loss: 20521.29
[INFO 2017-06-26 12:52:30,315 main.py:50] epoch 4260, training loss: 5808.92, average training loss: 5860.61, base loss: 20521.14
[INFO 2017-06-26 12:52:30,674 main.py:50] epoch 4261, training loss: 5746.24, average training loss: 5860.51, base loss: 20521.20
[INFO 2017-06-26 12:52:31,032 main.py:50] epoch 4262, training loss: 5810.43, average training loss: 5860.34, base loss: 20521.12
[INFO 2017-06-26 12:52:31,391 main.py:50] epoch 4263, training loss: 5772.49, average training loss: 5860.12, base loss: 20520.83
[INFO 2017-06-26 12:52:31,750 main.py:50] epoch 4264, training loss: 5785.37, average training loss: 5859.96, base loss: 20521.10
[INFO 2017-06-26 12:52:32,108 main.py:50] epoch 4265, training loss: 5771.39, average training loss: 5859.81, base loss: 20521.13
[INFO 2017-06-26 12:52:32,467 main.py:50] epoch 4266, training loss: 5814.63, average training loss: 5859.75, base loss: 20521.56
[INFO 2017-06-26 12:52:32,827 main.py:50] epoch 4267, training loss: 5762.30, average training loss: 5859.55, base loss: 20521.38
[INFO 2017-06-26 12:52:33,187 main.py:50] epoch 4268, training loss: 5785.51, average training loss: 5859.41, base loss: 20521.28
[INFO 2017-06-26 12:52:33,546 main.py:50] epoch 4269, training loss: 5763.78, average training loss: 5859.18, base loss: 20521.01
[INFO 2017-06-26 12:52:33,905 main.py:50] epoch 4270, training loss: 5808.57, average training loss: 5859.06, base loss: 20521.18
[INFO 2017-06-26 12:52:34,266 main.py:50] epoch 4271, training loss: 5732.12, average training loss: 5858.79, base loss: 20520.78
[INFO 2017-06-26 12:52:34,624 main.py:50] epoch 4272, training loss: 5762.94, average training loss: 5858.62, base loss: 20520.51
[INFO 2017-06-26 12:52:34,983 main.py:50] epoch 4273, training loss: 5708.99, average training loss: 5858.35, base loss: 20520.27
[INFO 2017-06-26 12:52:35,343 main.py:50] epoch 4274, training loss: 5826.85, average training loss: 5858.25, base loss: 20520.45
[INFO 2017-06-26 12:52:35,702 main.py:50] epoch 4275, training loss: 5749.24, average training loss: 5858.03, base loss: 20520.22
[INFO 2017-06-26 12:52:36,061 main.py:50] epoch 4276, training loss: 5736.54, average training loss: 5857.86, base loss: 20520.33
[INFO 2017-06-26 12:52:36,421 main.py:50] epoch 4277, training loss: 5745.99, average training loss: 5857.68, base loss: 20520.53
[INFO 2017-06-26 12:52:36,780 main.py:50] epoch 4278, training loss: 5800.51, average training loss: 5857.50, base loss: 20520.50
[INFO 2017-06-26 12:52:37,140 main.py:50] epoch 4279, training loss: 5799.81, average training loss: 5857.32, base loss: 20520.29
[INFO 2017-06-26 12:52:37,498 main.py:50] epoch 4280, training loss: 5748.37, average training loss: 5857.12, base loss: 20520.54
[INFO 2017-06-26 12:52:37,859 main.py:50] epoch 4281, training loss: 5811.37, average training loss: 5856.97, base loss: 20520.57
[INFO 2017-06-26 12:52:38,219 main.py:50] epoch 4282, training loss: 5734.74, average training loss: 5856.72, base loss: 20520.47
[INFO 2017-06-26 12:52:38,578 main.py:50] epoch 4283, training loss: 5741.61, average training loss: 5856.52, base loss: 20520.87
[INFO 2017-06-26 12:52:38,937 main.py:50] epoch 4284, training loss: 5719.07, average training loss: 5856.33, base loss: 20520.79
[INFO 2017-06-26 12:52:39,296 main.py:50] epoch 4285, training loss: 5756.73, average training loss: 5856.11, base loss: 20521.32
[INFO 2017-06-26 12:52:39,656 main.py:50] epoch 4286, training loss: 5708.10, average training loss: 5855.84, base loss: 20521.18
[INFO 2017-06-26 12:52:40,016 main.py:50] epoch 4287, training loss: 5761.62, average training loss: 5855.54, base loss: 20520.49
[INFO 2017-06-26 12:52:40,375 main.py:50] epoch 4288, training loss: 5796.52, average training loss: 5855.35, base loss: 20520.65
[INFO 2017-06-26 12:52:40,735 main.py:50] epoch 4289, training loss: 5686.51, average training loss: 5855.04, base loss: 20520.71
[INFO 2017-06-26 12:52:41,095 main.py:50] epoch 4290, training loss: 5752.62, average training loss: 5854.89, base loss: 20521.07
[INFO 2017-06-26 12:52:41,454 main.py:50] epoch 4291, training loss: 5766.78, average training loss: 5854.68, base loss: 20521.06
[INFO 2017-06-26 12:52:41,812 main.py:50] epoch 4292, training loss: 5740.32, average training loss: 5854.52, base loss: 20521.11
[INFO 2017-06-26 12:52:42,172 main.py:50] epoch 4293, training loss: 5791.87, average training loss: 5854.35, base loss: 20521.07
[INFO 2017-06-26 12:52:42,530 main.py:50] epoch 4294, training loss: 5717.61, average training loss: 5854.17, base loss: 20520.97
[INFO 2017-06-26 12:52:42,895 main.py:50] epoch 4295, training loss: 5807.17, average training loss: 5854.04, base loss: 20521.26
[INFO 2017-06-26 12:52:43,254 main.py:50] epoch 4296, training loss: 5718.61, average training loss: 5853.82, base loss: 20521.36
[INFO 2017-06-26 12:52:43,614 main.py:50] epoch 4297, training loss: 5718.34, average training loss: 5853.66, base loss: 20521.32
[INFO 2017-06-26 12:52:43,974 main.py:50] epoch 4298, training loss: 5802.55, average training loss: 5853.49, base loss: 20521.29
[INFO 2017-06-26 12:52:44,332 main.py:50] epoch 4299, training loss: 5727.19, average training loss: 5853.27, base loss: 20521.21
[INFO 2017-06-26 12:52:44,333 main.py:52] epoch 4299, testing
[INFO 2017-06-26 12:52:45,799 main.py:103] average testing loss: 5799.39, base loss: 20443.11
[INFO 2017-06-26 12:52:45,799 main.py:76] current best accuracy: 5762.19
[INFO 2017-06-26 12:52:46,158 main.py:50] epoch 4300, training loss: 5762.73, average training loss: 5853.12, base loss: 20521.26
[INFO 2017-06-26 12:52:46,518 main.py:50] epoch 4301, training loss: 5739.90, average training loss: 5852.88, base loss: 20521.18
[INFO 2017-06-26 12:52:46,878 main.py:50] epoch 4302, training loss: 5853.84, average training loss: 5852.87, base loss: 20521.43
[INFO 2017-06-26 12:52:47,236 main.py:50] epoch 4303, training loss: 5816.65, average training loss: 5852.74, base loss: 20521.25
[INFO 2017-06-26 12:52:47,594 main.py:50] epoch 4304, training loss: 5828.71, average training loss: 5852.61, base loss: 20521.18
[INFO 2017-06-26 12:52:47,954 main.py:50] epoch 4305, training loss: 5742.94, average training loss: 5852.44, base loss: 20521.20
[INFO 2017-06-26 12:52:48,313 main.py:50] epoch 4306, training loss: 5800.97, average training loss: 5852.25, base loss: 20520.92
[INFO 2017-06-26 12:52:48,672 main.py:50] epoch 4307, training loss: 5807.20, average training loss: 5852.15, base loss: 20520.89
[INFO 2017-06-26 12:52:49,030 main.py:50] epoch 4308, training loss: 5825.53, average training loss: 5852.00, base loss: 20521.30
[INFO 2017-06-26 12:52:49,390 main.py:50] epoch 4309, training loss: 5796.20, average training loss: 5851.87, base loss: 20520.91
[INFO 2017-06-26 12:52:49,750 main.py:50] epoch 4310, training loss: 5820.96, average training loss: 5851.75, base loss: 20520.79
[INFO 2017-06-26 12:52:50,109 main.py:50] epoch 4311, training loss: 5771.88, average training loss: 5851.58, base loss: 20520.88
[INFO 2017-06-26 12:52:50,468 main.py:50] epoch 4312, training loss: 5803.55, average training loss: 5851.47, base loss: 20520.96
[INFO 2017-06-26 12:52:50,828 main.py:50] epoch 4313, training loss: 5732.77, average training loss: 5851.24, base loss: 20521.32
[INFO 2017-06-26 12:52:51,187 main.py:50] epoch 4314, training loss: 5758.04, average training loss: 5851.02, base loss: 20521.40
[INFO 2017-06-26 12:52:51,546 main.py:50] epoch 4315, training loss: 5788.17, average training loss: 5850.87, base loss: 20521.76
[INFO 2017-06-26 12:52:51,905 main.py:50] epoch 4316, training loss: 5750.26, average training loss: 5850.71, base loss: 20522.06
[INFO 2017-06-26 12:52:52,264 main.py:50] epoch 4317, training loss: 5788.23, average training loss: 5850.54, base loss: 20521.98
[INFO 2017-06-26 12:52:52,623 main.py:50] epoch 4318, training loss: 5747.44, average training loss: 5850.39, base loss: 20521.97
[INFO 2017-06-26 12:52:52,982 main.py:50] epoch 4319, training loss: 5750.19, average training loss: 5850.24, base loss: 20521.83
[INFO 2017-06-26 12:52:53,341 main.py:50] epoch 4320, training loss: 5748.33, average training loss: 5850.03, base loss: 20521.85
[INFO 2017-06-26 12:52:53,700 main.py:50] epoch 4321, training loss: 5740.68, average training loss: 5849.82, base loss: 20521.80
[INFO 2017-06-26 12:52:54,058 main.py:50] epoch 4322, training loss: 5828.68, average training loss: 5849.70, base loss: 20521.95
[INFO 2017-06-26 12:52:54,418 main.py:50] epoch 4323, training loss: 5743.83, average training loss: 5849.51, base loss: 20521.86
[INFO 2017-06-26 12:52:54,777 main.py:50] epoch 4324, training loss: 5845.38, average training loss: 5849.36, base loss: 20522.21
[INFO 2017-06-26 12:52:55,135 main.py:50] epoch 4325, training loss: 5715.18, average training loss: 5849.16, base loss: 20522.37
[INFO 2017-06-26 12:52:55,496 main.py:50] epoch 4326, training loss: 5813.08, average training loss: 5849.03, base loss: 20522.45
[INFO 2017-06-26 12:52:55,854 main.py:50] epoch 4327, training loss: 5748.32, average training loss: 5848.87, base loss: 20522.90
[INFO 2017-06-26 12:52:56,213 main.py:50] epoch 4328, training loss: 5759.39, average training loss: 5848.71, base loss: 20522.54
[INFO 2017-06-26 12:52:56,573 main.py:50] epoch 4329, training loss: 5735.84, average training loss: 5848.47, base loss: 20522.66
[INFO 2017-06-26 12:52:56,933 main.py:50] epoch 4330, training loss: 5708.19, average training loss: 5848.23, base loss: 20522.46
[INFO 2017-06-26 12:52:57,292 main.py:50] epoch 4331, training loss: 5707.30, average training loss: 5847.94, base loss: 20522.20
[INFO 2017-06-26 12:52:57,651 main.py:50] epoch 4332, training loss: 5729.62, average training loss: 5847.74, base loss: 20522.05
[INFO 2017-06-26 12:52:58,010 main.py:50] epoch 4333, training loss: 5747.33, average training loss: 5847.52, base loss: 20521.93
[INFO 2017-06-26 12:52:58,368 main.py:50] epoch 4334, training loss: 5802.07, average training loss: 5847.36, base loss: 20521.95
[INFO 2017-06-26 12:52:58,729 main.py:50] epoch 4335, training loss: 5729.01, average training loss: 5847.23, base loss: 20521.91
[INFO 2017-06-26 12:52:59,089 main.py:50] epoch 4336, training loss: 5696.61, average training loss: 5847.02, base loss: 20521.57
[INFO 2017-06-26 12:52:59,448 main.py:50] epoch 4337, training loss: 5777.23, average training loss: 5846.92, base loss: 20521.83
[INFO 2017-06-26 12:52:59,807 main.py:50] epoch 4338, training loss: 5702.68, average training loss: 5846.68, base loss: 20521.75
[INFO 2017-06-26 12:53:00,168 main.py:50] epoch 4339, training loss: 5707.04, average training loss: 5846.47, base loss: 20521.61
[INFO 2017-06-26 12:53:00,526 main.py:50] epoch 4340, training loss: 5684.74, average training loss: 5846.27, base loss: 20521.77
[INFO 2017-06-26 12:53:00,884 main.py:50] epoch 4341, training loss: 5794.12, average training loss: 5846.12, base loss: 20521.68
[INFO 2017-06-26 12:53:01,243 main.py:50] epoch 4342, training loss: 5682.27, average training loss: 5845.87, base loss: 20521.02
[INFO 2017-06-26 12:53:01,600 main.py:50] epoch 4343, training loss: 5817.04, average training loss: 5845.78, base loss: 20521.10
[INFO 2017-06-26 12:53:01,961 main.py:50] epoch 4344, training loss: 5772.82, average training loss: 5845.73, base loss: 20521.05
[INFO 2017-06-26 12:53:02,320 main.py:50] epoch 4345, training loss: 5735.17, average training loss: 5845.58, base loss: 20521.52
[INFO 2017-06-26 12:53:02,679 main.py:50] epoch 4346, training loss: 5732.76, average training loss: 5845.37, base loss: 20521.25
[INFO 2017-06-26 12:53:03,039 main.py:50] epoch 4347, training loss: 5784.48, average training loss: 5845.24, base loss: 20521.46
[INFO 2017-06-26 12:53:03,398 main.py:50] epoch 4348, training loss: 5759.91, average training loss: 5845.10, base loss: 20521.46
[INFO 2017-06-26 12:53:03,757 main.py:50] epoch 4349, training loss: 5794.13, average training loss: 5844.98, base loss: 20521.34
[INFO 2017-06-26 12:53:04,116 main.py:50] epoch 4350, training loss: 5826.97, average training loss: 5844.92, base loss: 20521.39
[INFO 2017-06-26 12:53:04,475 main.py:50] epoch 4351, training loss: 5733.29, average training loss: 5844.73, base loss: 20521.62
[INFO 2017-06-26 12:53:04,834 main.py:50] epoch 4352, training loss: 5805.96, average training loss: 5844.65, base loss: 20521.18
[INFO 2017-06-26 12:53:05,195 main.py:50] epoch 4353, training loss: 5770.32, average training loss: 5844.53, base loss: 20521.09
[INFO 2017-06-26 12:53:05,554 main.py:50] epoch 4354, training loss: 5905.83, average training loss: 5844.54, base loss: 20521.68
[INFO 2017-06-26 12:53:05,915 main.py:50] epoch 4355, training loss: 5805.48, average training loss: 5844.42, base loss: 20521.90
[INFO 2017-06-26 12:53:06,273 main.py:50] epoch 4356, training loss: 5827.26, average training loss: 5844.37, base loss: 20521.72
[INFO 2017-06-26 12:53:06,633 main.py:50] epoch 4357, training loss: 5808.02, average training loss: 5844.24, base loss: 20521.70
[INFO 2017-06-26 12:53:06,991 main.py:50] epoch 4358, training loss: 5771.33, average training loss: 5844.11, base loss: 20522.32
[INFO 2017-06-26 12:53:07,349 main.py:50] epoch 4359, training loss: 5855.58, average training loss: 5844.01, base loss: 20521.83
[INFO 2017-06-26 12:53:07,709 main.py:50] epoch 4360, training loss: 5803.79, average training loss: 5843.90, base loss: 20521.77
[INFO 2017-06-26 12:53:08,069 main.py:50] epoch 4361, training loss: 5778.76, average training loss: 5843.80, base loss: 20521.57
[INFO 2017-06-26 12:53:08,427 main.py:50] epoch 4362, training loss: 5799.23, average training loss: 5843.75, base loss: 20521.63
[INFO 2017-06-26 12:53:08,787 main.py:50] epoch 4363, training loss: 5776.49, average training loss: 5843.62, base loss: 20522.14
[INFO 2017-06-26 12:53:09,146 main.py:50] epoch 4364, training loss: 5845.91, average training loss: 5843.55, base loss: 20522.46
[INFO 2017-06-26 12:53:09,505 main.py:50] epoch 4365, training loss: 5745.15, average training loss: 5843.40, base loss: 20522.49
[INFO 2017-06-26 12:53:09,864 main.py:50] epoch 4366, training loss: 5655.66, average training loss: 5843.17, base loss: 20522.33
[INFO 2017-06-26 12:53:10,225 main.py:50] epoch 4367, training loss: 5695.90, average training loss: 5842.94, base loss: 20522.23
[INFO 2017-06-26 12:53:10,597 main.py:50] epoch 4368, training loss: 5798.83, average training loss: 5842.84, base loss: 20522.19
[INFO 2017-06-26 12:53:10,957 main.py:50] epoch 4369, training loss: 5795.28, average training loss: 5842.71, base loss: 20522.22
[INFO 2017-06-26 12:53:11,316 main.py:50] epoch 4370, training loss: 5788.35, average training loss: 5842.54, base loss: 20522.05
[INFO 2017-06-26 12:53:11,674 main.py:50] epoch 4371, training loss: 5731.16, average training loss: 5842.42, base loss: 20522.35
[INFO 2017-06-26 12:53:12,033 main.py:50] epoch 4372, training loss: 5785.06, average training loss: 5842.31, base loss: 20522.45
[INFO 2017-06-26 12:53:12,393 main.py:50] epoch 4373, training loss: 5748.30, average training loss: 5842.14, base loss: 20522.40
[INFO 2017-06-26 12:53:12,753 main.py:50] epoch 4374, training loss: 5744.86, average training loss: 5841.91, base loss: 20521.75
[INFO 2017-06-26 12:53:13,116 main.py:50] epoch 4375, training loss: 5776.31, average training loss: 5841.77, base loss: 20521.96
[INFO 2017-06-26 12:53:13,478 main.py:50] epoch 4376, training loss: 5796.53, average training loss: 5841.63, base loss: 20522.11
[INFO 2017-06-26 12:53:13,838 main.py:50] epoch 4377, training loss: 5800.64, average training loss: 5841.49, base loss: 20521.95
[INFO 2017-06-26 12:53:14,197 main.py:50] epoch 4378, training loss: 5817.65, average training loss: 5841.36, base loss: 20521.85
[INFO 2017-06-26 12:53:14,556 main.py:50] epoch 4379, training loss: 5779.26, average training loss: 5841.19, base loss: 20522.06
[INFO 2017-06-26 12:53:14,916 main.py:50] epoch 4380, training loss: 5730.09, average training loss: 5841.01, base loss: 20521.78
[INFO 2017-06-26 12:53:15,276 main.py:50] epoch 4381, training loss: 5755.12, average training loss: 5840.86, base loss: 20521.42
[INFO 2017-06-26 12:53:15,635 main.py:50] epoch 4382, training loss: 5803.17, average training loss: 5840.76, base loss: 20521.42
[INFO 2017-06-26 12:53:15,994 main.py:50] epoch 4383, training loss: 5840.94, average training loss: 5840.65, base loss: 20521.43
[INFO 2017-06-26 12:53:16,353 main.py:50] epoch 4384, training loss: 5786.50, average training loss: 5840.50, base loss: 20521.37
[INFO 2017-06-26 12:53:16,712 main.py:50] epoch 4385, training loss: 5772.34, average training loss: 5840.31, base loss: 20521.46
[INFO 2017-06-26 12:53:17,071 main.py:50] epoch 4386, training loss: 5778.83, average training loss: 5840.15, base loss: 20521.42
[INFO 2017-06-26 12:53:17,430 main.py:50] epoch 4387, training loss: 5715.96, average training loss: 5839.96, base loss: 20521.27
[INFO 2017-06-26 12:53:17,789 main.py:50] epoch 4388, training loss: 5765.43, average training loss: 5839.84, base loss: 20521.44
[INFO 2017-06-26 12:53:18,148 main.py:50] epoch 4389, training loss: 5748.59, average training loss: 5839.68, base loss: 20521.56
[INFO 2017-06-26 12:53:18,507 main.py:50] epoch 4390, training loss: 5752.08, average training loss: 5839.57, base loss: 20521.21
[INFO 2017-06-26 12:53:18,866 main.py:50] epoch 4391, training loss: 5701.68, average training loss: 5839.40, base loss: 20521.08
[INFO 2017-06-26 12:53:19,224 main.py:50] epoch 4392, training loss: 5774.44, average training loss: 5839.30, base loss: 20521.24
[INFO 2017-06-26 12:53:19,583 main.py:50] epoch 4393, training loss: 5681.48, average training loss: 5839.08, base loss: 20521.12
[INFO 2017-06-26 12:53:19,943 main.py:50] epoch 4394, training loss: 5777.01, average training loss: 5838.94, base loss: 20520.79
[INFO 2017-06-26 12:53:20,302 main.py:50] epoch 4395, training loss: 5801.43, average training loss: 5838.88, base loss: 20520.86
[INFO 2017-06-26 12:53:20,661 main.py:50] epoch 4396, training loss: 5757.31, average training loss: 5838.79, base loss: 20521.04
[INFO 2017-06-26 12:53:21,019 main.py:50] epoch 4397, training loss: 5770.28, average training loss: 5838.70, base loss: 20521.27
[INFO 2017-06-26 12:53:21,379 main.py:50] epoch 4398, training loss: 5681.08, average training loss: 5838.49, base loss: 20521.45
[INFO 2017-06-26 12:53:21,738 main.py:50] epoch 4399, training loss: 5731.72, average training loss: 5838.31, base loss: 20521.12
[INFO 2017-06-26 12:53:21,739 main.py:52] epoch 4399, testing
[INFO 2017-06-26 12:53:23,209 main.py:103] average testing loss: 5784.92, base loss: 20539.52
[INFO 2017-06-26 12:53:23,210 main.py:76] current best accuracy: 5762.19
[INFO 2017-06-26 12:53:23,571 main.py:50] epoch 4400, training loss: 5801.86, average training loss: 5838.17, base loss: 20521.50
[INFO 2017-06-26 12:53:23,932 main.py:50] epoch 4401, training loss: 5729.21, average training loss: 5837.99, base loss: 20521.36
[INFO 2017-06-26 12:53:24,291 main.py:50] epoch 4402, training loss: 5778.61, average training loss: 5837.84, base loss: 20521.36
[INFO 2017-06-26 12:53:24,651 main.py:50] epoch 4403, training loss: 5726.29, average training loss: 5837.59, base loss: 20521.39
[INFO 2017-06-26 12:53:25,012 main.py:50] epoch 4404, training loss: 5716.81, average training loss: 5837.38, base loss: 20521.32
[INFO 2017-06-26 12:53:25,370 main.py:50] epoch 4405, training loss: 5700.11, average training loss: 5837.15, base loss: 20521.52
[INFO 2017-06-26 12:53:25,730 main.py:50] epoch 4406, training loss: 5755.17, average training loss: 5837.02, base loss: 20521.74
[INFO 2017-06-26 12:53:26,092 main.py:50] epoch 4407, training loss: 5748.66, average training loss: 5836.87, base loss: 20521.65
[INFO 2017-06-26 12:53:26,451 main.py:50] epoch 4408, training loss: 5846.33, average training loss: 5836.81, base loss: 20521.59
[INFO 2017-06-26 12:53:26,812 main.py:50] epoch 4409, training loss: 5733.06, average training loss: 5836.62, base loss: 20521.66
[INFO 2017-06-26 12:53:27,172 main.py:50] epoch 4410, training loss: 5692.99, average training loss: 5836.43, base loss: 20521.64
[INFO 2017-06-26 12:53:27,531 main.py:50] epoch 4411, training loss: 5694.00, average training loss: 5836.19, base loss: 20521.80
[INFO 2017-06-26 12:53:27,893 main.py:50] epoch 4412, training loss: 5732.68, average training loss: 5835.96, base loss: 20522.32
[INFO 2017-06-26 12:53:28,252 main.py:50] epoch 4413, training loss: 5661.49, average training loss: 5835.71, base loss: 20522.11
[INFO 2017-06-26 12:53:28,611 main.py:50] epoch 4414, training loss: 5724.82, average training loss: 5835.49, base loss: 20522.32
[INFO 2017-06-26 12:53:28,973 main.py:50] epoch 4415, training loss: 5678.00, average training loss: 5835.24, base loss: 20522.65
[INFO 2017-06-26 12:53:29,332 main.py:50] epoch 4416, training loss: 5723.50, average training loss: 5834.99, base loss: 20522.44
[INFO 2017-06-26 12:53:29,691 main.py:50] epoch 4417, training loss: 5728.02, average training loss: 5834.76, base loss: 20522.33
[INFO 2017-06-26 12:53:30,051 main.py:50] epoch 4418, training loss: 5763.93, average training loss: 5834.56, base loss: 20522.08
[INFO 2017-06-26 12:53:30,412 main.py:50] epoch 4419, training loss: 5735.86, average training loss: 5834.35, base loss: 20521.95
[INFO 2017-06-26 12:53:30,774 main.py:50] epoch 4420, training loss: 5721.56, average training loss: 5834.17, base loss: 20521.91
[INFO 2017-06-26 12:53:31,133 main.py:50] epoch 4421, training loss: 5718.47, average training loss: 5833.99, base loss: 20521.61
[INFO 2017-06-26 12:53:31,492 main.py:50] epoch 4422, training loss: 5680.79, average training loss: 5833.76, base loss: 20521.27
[INFO 2017-06-26 12:53:31,852 main.py:50] epoch 4423, training loss: 5738.25, average training loss: 5833.62, base loss: 20521.63
[INFO 2017-06-26 12:53:32,213 main.py:50] epoch 4424, training loss: 5771.39, average training loss: 5833.43, base loss: 20521.49
[INFO 2017-06-26 12:53:32,574 main.py:50] epoch 4425, training loss: 5739.84, average training loss: 5833.27, base loss: 20521.72
[INFO 2017-06-26 12:53:32,935 main.py:50] epoch 4426, training loss: 5755.97, average training loss: 5833.11, base loss: 20521.66
[INFO 2017-06-26 12:53:33,295 main.py:50] epoch 4427, training loss: 5740.94, average training loss: 5832.92, base loss: 20521.37
[INFO 2017-06-26 12:53:33,656 main.py:50] epoch 4428, training loss: 5747.98, average training loss: 5832.66, base loss: 20521.16
[INFO 2017-06-26 12:53:34,016 main.py:50] epoch 4429, training loss: 5836.13, average training loss: 5832.61, base loss: 20521.39
[INFO 2017-06-26 12:53:34,376 main.py:50] epoch 4430, training loss: 5764.17, average training loss: 5832.45, base loss: 20521.10
[INFO 2017-06-26 12:53:34,735 main.py:50] epoch 4431, training loss: 5754.96, average training loss: 5832.34, base loss: 20521.31
[INFO 2017-06-26 12:53:35,096 main.py:50] epoch 4432, training loss: 5807.16, average training loss: 5832.28, base loss: 20521.31
[INFO 2017-06-26 12:53:35,457 main.py:50] epoch 4433, training loss: 5847.56, average training loss: 5832.17, base loss: 20521.10
[INFO 2017-06-26 12:53:35,816 main.py:50] epoch 4434, training loss: 5718.70, average training loss: 5831.94, base loss: 20521.10
[INFO 2017-06-26 12:53:36,176 main.py:50] epoch 4435, training loss: 5794.43, average training loss: 5831.89, base loss: 20521.27
[INFO 2017-06-26 12:53:36,536 main.py:50] epoch 4436, training loss: 5787.49, average training loss: 5831.79, base loss: 20521.25
[INFO 2017-06-26 12:53:36,896 main.py:50] epoch 4437, training loss: 5841.90, average training loss: 5831.72, base loss: 20521.02
[INFO 2017-06-26 12:53:37,257 main.py:50] epoch 4438, training loss: 5817.46, average training loss: 5831.60, base loss: 20520.80
[INFO 2017-06-26 12:53:37,619 main.py:50] epoch 4439, training loss: 5766.54, average training loss: 5831.39, base loss: 20520.57
[INFO 2017-06-26 12:53:37,980 main.py:50] epoch 4440, training loss: 5760.92, average training loss: 5831.22, base loss: 20520.53
[INFO 2017-06-26 12:53:38,340 main.py:50] epoch 4441, training loss: 5800.67, average training loss: 5831.17, base loss: 20520.72
[INFO 2017-06-26 12:53:38,701 main.py:50] epoch 4442, training loss: 5760.07, average training loss: 5830.94, base loss: 20520.17
[INFO 2017-06-26 12:53:39,060 main.py:50] epoch 4443, training loss: 5828.64, average training loss: 5830.94, base loss: 20520.30
[INFO 2017-06-26 12:53:39,422 main.py:50] epoch 4444, training loss: 5739.27, average training loss: 5830.77, base loss: 20520.24
[INFO 2017-06-26 12:53:39,782 main.py:50] epoch 4445, training loss: 5772.75, average training loss: 5830.66, base loss: 20520.27
[INFO 2017-06-26 12:53:40,142 main.py:50] epoch 4446, training loss: 5754.00, average training loss: 5830.53, base loss: 20520.30
[INFO 2017-06-26 12:53:40,503 main.py:50] epoch 4447, training loss: 5823.01, average training loss: 5830.51, base loss: 20520.67
[INFO 2017-06-26 12:53:40,861 main.py:50] epoch 4448, training loss: 5748.33, average training loss: 5830.33, base loss: 20520.59
[INFO 2017-06-26 12:53:41,223 main.py:50] epoch 4449, training loss: 5822.08, average training loss: 5830.19, base loss: 20520.77
[INFO 2017-06-26 12:53:41,581 main.py:50] epoch 4450, training loss: 5834.89, average training loss: 5830.10, base loss: 20521.13
[INFO 2017-06-26 12:53:41,942 main.py:50] epoch 4451, training loss: 5805.37, average training loss: 5829.97, base loss: 20521.12
[INFO 2017-06-26 12:53:42,303 main.py:50] epoch 4452, training loss: 5733.12, average training loss: 5829.78, base loss: 20520.97
[INFO 2017-06-26 12:53:42,663 main.py:50] epoch 4453, training loss: 5739.50, average training loss: 5829.60, base loss: 20521.02
[INFO 2017-06-26 12:53:43,022 main.py:50] epoch 4454, training loss: 5709.21, average training loss: 5829.45, base loss: 20521.00
[INFO 2017-06-26 12:53:43,382 main.py:50] epoch 4455, training loss: 5728.65, average training loss: 5829.27, base loss: 20521.09
[INFO 2017-06-26 12:53:43,742 main.py:50] epoch 4456, training loss: 5752.15, average training loss: 5829.12, base loss: 20521.00
[INFO 2017-06-26 12:53:44,102 main.py:50] epoch 4457, training loss: 5767.23, average training loss: 5828.99, base loss: 20520.94
[INFO 2017-06-26 12:53:44,462 main.py:50] epoch 4458, training loss: 5718.53, average training loss: 5828.69, base loss: 20521.01
[INFO 2017-06-26 12:53:44,823 main.py:50] epoch 4459, training loss: 5726.21, average training loss: 5828.52, base loss: 20521.06
[INFO 2017-06-26 12:53:45,183 main.py:50] epoch 4460, training loss: 5708.16, average training loss: 5828.22, base loss: 20520.62
[INFO 2017-06-26 12:53:45,545 main.py:50] epoch 4461, training loss: 5760.44, average training loss: 5828.07, base loss: 20520.90
[INFO 2017-06-26 12:53:45,904 main.py:50] epoch 4462, training loss: 5694.54, average training loss: 5827.82, base loss: 20520.91
[INFO 2017-06-26 12:53:46,264 main.py:50] epoch 4463, training loss: 5733.81, average training loss: 5827.68, base loss: 20520.65
[INFO 2017-06-26 12:53:46,624 main.py:50] epoch 4464, training loss: 5706.61, average training loss: 5827.44, base loss: 20520.63
[INFO 2017-06-26 12:53:46,983 main.py:50] epoch 4465, training loss: 5722.82, average training loss: 5827.23, base loss: 20520.61
[INFO 2017-06-26 12:53:47,343 main.py:50] epoch 4466, training loss: 5710.69, average training loss: 5827.02, base loss: 20520.76
[INFO 2017-06-26 12:53:47,703 main.py:50] epoch 4467, training loss: 5739.63, average training loss: 5826.82, base loss: 20520.75
[INFO 2017-06-26 12:53:48,064 main.py:50] epoch 4468, training loss: 5749.63, average training loss: 5826.68, base loss: 20520.64
[INFO 2017-06-26 12:53:48,425 main.py:50] epoch 4469, training loss: 5740.36, average training loss: 5826.51, base loss: 20520.67
[INFO 2017-06-26 12:53:48,787 main.py:50] epoch 4470, training loss: 5774.98, average training loss: 5826.42, base loss: 20520.70
[INFO 2017-06-26 12:53:49,147 main.py:50] epoch 4471, training loss: 5798.14, average training loss: 5826.23, base loss: 20520.24
[INFO 2017-06-26 12:53:49,507 main.py:50] epoch 4472, training loss: 5724.60, average training loss: 5826.02, base loss: 20519.60
[INFO 2017-06-26 12:53:49,867 main.py:50] epoch 4473, training loss: 5909.69, average training loss: 5826.04, base loss: 20519.70
[INFO 2017-06-26 12:53:50,227 main.py:50] epoch 4474, training loss: 5821.65, average training loss: 5825.93, base loss: 20519.57
[INFO 2017-06-26 12:53:50,587 main.py:50] epoch 4475, training loss: 5741.83, average training loss: 5825.80, base loss: 20519.47
[INFO 2017-06-26 12:53:50,947 main.py:50] epoch 4476, training loss: 5783.51, average training loss: 5825.70, base loss: 20519.38
[INFO 2017-06-26 12:53:51,308 main.py:50] epoch 4477, training loss: 5751.95, average training loss: 5825.51, base loss: 20519.17
[INFO 2017-06-26 12:53:51,667 main.py:50] epoch 4478, training loss: 5826.35, average training loss: 5825.50, base loss: 20519.42
[INFO 2017-06-26 12:53:52,027 main.py:50] epoch 4479, training loss: 5736.42, average training loss: 5825.33, base loss: 20519.14
[INFO 2017-06-26 12:53:52,388 main.py:50] epoch 4480, training loss: 5863.83, average training loss: 5825.28, base loss: 20519.58
[INFO 2017-06-26 12:53:52,749 main.py:50] epoch 4481, training loss: 5747.42, average training loss: 5825.11, base loss: 20519.57
[INFO 2017-06-26 12:53:53,109 main.py:50] epoch 4482, training loss: 5742.00, average training loss: 5824.90, base loss: 20519.28
[INFO 2017-06-26 12:53:53,469 main.py:50] epoch 4483, training loss: 5763.73, average training loss: 5824.71, base loss: 20519.24
[INFO 2017-06-26 12:53:53,829 main.py:50] epoch 4484, training loss: 5737.27, average training loss: 5824.57, base loss: 20519.06
[INFO 2017-06-26 12:53:54,190 main.py:50] epoch 4485, training loss: 5737.88, average training loss: 5824.44, base loss: 20519.16
[INFO 2017-06-26 12:53:54,548 main.py:50] epoch 4486, training loss: 5757.14, average training loss: 5824.22, base loss: 20519.05
[INFO 2017-06-26 12:53:54,908 main.py:50] epoch 4487, training loss: 5787.25, average training loss: 5824.02, base loss: 20518.74
[INFO 2017-06-26 12:53:55,280 main.py:50] epoch 4488, training loss: 5725.88, average training loss: 5823.89, base loss: 20518.65
[INFO 2017-06-26 12:53:55,640 main.py:50] epoch 4489, training loss: 5761.29, average training loss: 5823.71, base loss: 20518.52
[INFO 2017-06-26 12:53:56,001 main.py:50] epoch 4490, training loss: 5746.68, average training loss: 5823.62, base loss: 20518.61
[INFO 2017-06-26 12:53:56,361 main.py:50] epoch 4491, training loss: 5659.15, average training loss: 5823.42, base loss: 20518.78
[INFO 2017-06-26 12:53:56,722 main.py:50] epoch 4492, training loss: 5743.31, average training loss: 5823.25, base loss: 20518.84
[INFO 2017-06-26 12:53:57,082 main.py:50] epoch 4493, training loss: 5772.69, average training loss: 5823.10, base loss: 20519.16
[INFO 2017-06-26 12:53:57,443 main.py:50] epoch 4494, training loss: 5782.70, average training loss: 5823.03, base loss: 20519.54
[INFO 2017-06-26 12:53:57,804 main.py:50] epoch 4495, training loss: 5677.46, average training loss: 5822.82, base loss: 20519.56
[INFO 2017-06-26 12:53:58,166 main.py:50] epoch 4496, training loss: 5777.75, average training loss: 5822.70, base loss: 20519.42
[INFO 2017-06-26 12:53:58,526 main.py:50] epoch 4497, training loss: 5696.51, average training loss: 5822.55, base loss: 20519.14
[INFO 2017-06-26 12:53:58,886 main.py:50] epoch 4498, training loss: 5757.79, average training loss: 5822.46, base loss: 20519.30
[INFO 2017-06-26 12:53:59,245 main.py:50] epoch 4499, training loss: 5706.26, average training loss: 5822.32, base loss: 20518.94
[INFO 2017-06-26 12:53:59,245 main.py:52] epoch 4499, testing
[INFO 2017-06-26 12:54:00,716 main.py:103] average testing loss: 5777.42, base loss: 20515.92
[INFO 2017-06-26 12:54:00,717 main.py:76] current best accuracy: 5762.19
[INFO 2017-06-26 12:54:01,076 main.py:50] epoch 4500, training loss: 5776.82, average training loss: 5822.30, base loss: 20519.08
[INFO 2017-06-26 12:54:01,435 main.py:50] epoch 4501, training loss: 5774.28, average training loss: 5822.16, base loss: 20519.24
[INFO 2017-06-26 12:54:01,793 main.py:50] epoch 4502, training loss: 5679.97, average training loss: 5821.89, base loss: 20518.98
[INFO 2017-06-26 12:54:02,154 main.py:50] epoch 4503, training loss: 5657.09, average training loss: 5821.72, base loss: 20519.01
[INFO 2017-06-26 12:54:02,515 main.py:50] epoch 4504, training loss: 5732.19, average training loss: 5821.58, base loss: 20519.17
[INFO 2017-06-26 12:54:02,875 main.py:50] epoch 4505, training loss: 5749.85, average training loss: 5821.52, base loss: 20519.19
[INFO 2017-06-26 12:54:03,236 main.py:50] epoch 4506, training loss: 5769.68, average training loss: 5821.45, base loss: 20519.11
[INFO 2017-06-26 12:54:03,595 main.py:50] epoch 4507, training loss: 5757.51, average training loss: 5821.36, base loss: 20518.87
[INFO 2017-06-26 12:54:03,955 main.py:50] epoch 4508, training loss: 5706.57, average training loss: 5821.19, base loss: 20518.52
[INFO 2017-06-26 12:54:04,315 main.py:50] epoch 4509, training loss: 5750.96, average training loss: 5821.05, base loss: 20518.74
[INFO 2017-06-26 12:54:04,675 main.py:50] epoch 4510, training loss: 5701.89, average training loss: 5820.88, base loss: 20518.30
[INFO 2017-06-26 12:54:05,034 main.py:50] epoch 4511, training loss: 5751.12, average training loss: 5820.75, base loss: 20518.42
[INFO 2017-06-26 12:54:05,394 main.py:50] epoch 4512, training loss: 5727.29, average training loss: 5820.64, base loss: 20519.01
[INFO 2017-06-26 12:54:05,755 main.py:50] epoch 4513, training loss: 5786.16, average training loss: 5820.61, base loss: 20518.95
[INFO 2017-06-26 12:54:06,115 main.py:50] epoch 4514, training loss: 5736.14, average training loss: 5820.47, base loss: 20518.98
[INFO 2017-06-26 12:54:06,475 main.py:50] epoch 4515, training loss: 5739.61, average training loss: 5820.35, base loss: 20519.06
[INFO 2017-06-26 12:54:06,833 main.py:50] epoch 4516, training loss: 5635.72, average training loss: 5820.15, base loss: 20519.25
[INFO 2017-06-26 12:54:07,192 main.py:50] epoch 4517, training loss: 5725.41, average training loss: 5819.96, base loss: 20519.15
[INFO 2017-06-26 12:54:07,552 main.py:50] epoch 4518, training loss: 5706.72, average training loss: 5819.81, base loss: 20519.46
[INFO 2017-06-26 12:54:07,910 main.py:50] epoch 4519, training loss: 5770.58, average training loss: 5819.73, base loss: 20519.60
[INFO 2017-06-26 12:54:08,272 main.py:50] epoch 4520, training loss: 5768.99, average training loss: 5819.56, base loss: 20519.69
[INFO 2017-06-26 12:54:08,632 main.py:50] epoch 4521, training loss: 5785.68, average training loss: 5819.48, base loss: 20520.29
[INFO 2017-06-26 12:54:08,992 main.py:50] epoch 4522, training loss: 5729.80, average training loss: 5819.41, base loss: 20520.34
[INFO 2017-06-26 12:54:09,352 main.py:50] epoch 4523, training loss: 5758.22, average training loss: 5819.26, base loss: 20520.56
[INFO 2017-06-26 12:54:09,713 main.py:50] epoch 4524, training loss: 5768.86, average training loss: 5819.17, base loss: 20520.65
[INFO 2017-06-26 12:54:10,073 main.py:50] epoch 4525, training loss: 5711.21, average training loss: 5819.02, base loss: 20520.69
[INFO 2017-06-26 12:54:10,433 main.py:50] epoch 4526, training loss: 5816.08, average training loss: 5818.95, base loss: 20521.05
[INFO 2017-06-26 12:54:10,794 main.py:50] epoch 4527, training loss: 5766.24, average training loss: 5818.85, base loss: 20521.09
[INFO 2017-06-26 12:54:11,155 main.py:50] epoch 4528, training loss: 5733.34, average training loss: 5818.64, base loss: 20520.93
[INFO 2017-06-26 12:54:11,515 main.py:50] epoch 4529, training loss: 5778.75, average training loss: 5818.56, base loss: 20520.93
[INFO 2017-06-26 12:54:11,873 main.py:50] epoch 4530, training loss: 5734.92, average training loss: 5818.38, base loss: 20521.09
[INFO 2017-06-26 12:54:12,232 main.py:50] epoch 4531, training loss: 5736.39, average training loss: 5818.29, base loss: 20521.24
[INFO 2017-06-26 12:54:12,594 main.py:50] epoch 4532, training loss: 5770.35, average training loss: 5818.10, base loss: 20520.95
[INFO 2017-06-26 12:54:12,954 main.py:50] epoch 4533, training loss: 5740.77, average training loss: 5817.98, base loss: 20520.88
[INFO 2017-06-26 12:54:13,314 main.py:50] epoch 4534, training loss: 5781.99, average training loss: 5817.80, base loss: 20520.67
[INFO 2017-06-26 12:54:13,674 main.py:50] epoch 4535, training loss: 5747.29, average training loss: 5817.66, base loss: 20520.85
[INFO 2017-06-26 12:54:14,034 main.py:50] epoch 4536, training loss: 5738.63, average training loss: 5817.48, base loss: 20520.53
[INFO 2017-06-26 12:54:14,394 main.py:50] epoch 4537, training loss: 5667.35, average training loss: 5817.20, base loss: 20520.08
[INFO 2017-06-26 12:54:14,754 main.py:50] epoch 4538, training loss: 5770.14, average training loss: 5817.12, base loss: 20519.98
[INFO 2017-06-26 12:54:15,114 main.py:50] epoch 4539, training loss: 5785.64, average training loss: 5816.95, base loss: 20520.19
[INFO 2017-06-26 12:54:15,474 main.py:50] epoch 4540, training loss: 5713.87, average training loss: 5816.81, base loss: 20520.02
[INFO 2017-06-26 12:54:15,835 main.py:50] epoch 4541, training loss: 5749.71, average training loss: 5816.74, base loss: 20520.55
[INFO 2017-06-26 12:54:16,195 main.py:50] epoch 4542, training loss: 5830.96, average training loss: 5816.62, base loss: 20520.50
[INFO 2017-06-26 12:54:16,555 main.py:50] epoch 4543, training loss: 5762.54, average training loss: 5816.47, base loss: 20520.90
[INFO 2017-06-26 12:54:16,916 main.py:50] epoch 4544, training loss: 5771.89, average training loss: 5816.30, base loss: 20520.41
[INFO 2017-06-26 12:54:17,275 main.py:50] epoch 4545, training loss: 5716.71, average training loss: 5816.13, base loss: 20520.57
[INFO 2017-06-26 12:54:17,635 main.py:50] epoch 4546, training loss: 5793.66, average training loss: 5816.00, base loss: 20520.43
[INFO 2017-06-26 12:54:17,994 main.py:50] epoch 4547, training loss: 5618.91, average training loss: 5815.79, base loss: 20520.36
[INFO 2017-06-26 12:54:18,354 main.py:50] epoch 4548, training loss: 5838.09, average training loss: 5815.69, base loss: 20520.66
[INFO 2017-06-26 12:54:18,714 main.py:50] epoch 4549, training loss: 5786.96, average training loss: 5815.59, base loss: 20520.82
[INFO 2017-06-26 12:54:19,073 main.py:50] epoch 4550, training loss: 5809.24, average training loss: 5815.51, base loss: 20521.16
[INFO 2017-06-26 12:54:19,434 main.py:50] epoch 4551, training loss: 5847.55, average training loss: 5815.52, base loss: 20521.42
[INFO 2017-06-26 12:54:19,794 main.py:50] epoch 4552, training loss: 5813.16, average training loss: 5815.46, base loss: 20521.46
[INFO 2017-06-26 12:54:20,156 main.py:50] epoch 4553, training loss: 5919.40, average training loss: 5815.47, base loss: 20521.11
[INFO 2017-06-26 12:54:20,516 main.py:50] epoch 4554, training loss: 5802.70, average training loss: 5815.35, base loss: 20520.68
[INFO 2017-06-26 12:54:20,876 main.py:50] epoch 4555, training loss: 5862.96, average training loss: 5815.30, base loss: 20521.03
[INFO 2017-06-26 12:54:21,237 main.py:50] epoch 4556, training loss: 5734.28, average training loss: 5815.07, base loss: 20520.86
[INFO 2017-06-26 12:54:21,597 main.py:50] epoch 4557, training loss: 5750.35, average training loss: 5814.97, base loss: 20520.81
[INFO 2017-06-26 12:54:21,957 main.py:50] epoch 4558, training loss: 5767.68, average training loss: 5814.89, base loss: 20520.83
[INFO 2017-06-26 12:54:22,317 main.py:50] epoch 4559, training loss: 5714.38, average training loss: 5814.67, base loss: 20520.79
[INFO 2017-06-26 12:54:22,677 main.py:50] epoch 4560, training loss: 5776.57, average training loss: 5814.57, base loss: 20520.91
[INFO 2017-06-26 12:54:23,038 main.py:50] epoch 4561, training loss: 5777.51, average training loss: 5814.49, base loss: 20520.79
[INFO 2017-06-26 12:54:23,399 main.py:50] epoch 4562, training loss: 5743.07, average training loss: 5814.28, base loss: 20520.55
[INFO 2017-06-26 12:54:23,759 main.py:50] epoch 4563, training loss: 5809.48, average training loss: 5814.21, base loss: 20520.46
[INFO 2017-06-26 12:54:24,120 main.py:50] epoch 4564, training loss: 5769.56, average training loss: 5814.04, base loss: 20520.56
[INFO 2017-06-26 12:54:24,480 main.py:50] epoch 4565, training loss: 5835.47, average training loss: 5813.95, base loss: 20520.20
[INFO 2017-06-26 12:54:24,841 main.py:50] epoch 4566, training loss: 5810.33, average training loss: 5813.96, base loss: 20520.11
[INFO 2017-06-26 12:54:25,202 main.py:50] epoch 4567, training loss: 5876.62, average training loss: 5813.92, base loss: 20519.81
[INFO 2017-06-26 12:54:25,562 main.py:50] epoch 4568, training loss: 5786.16, average training loss: 5813.84, base loss: 20519.76
[INFO 2017-06-26 12:54:25,922 main.py:50] epoch 4569, training loss: 5781.62, average training loss: 5813.69, base loss: 20519.93
[INFO 2017-06-26 12:54:26,282 main.py:50] epoch 4570, training loss: 5762.93, average training loss: 5813.58, base loss: 20519.92
[INFO 2017-06-26 12:54:26,643 main.py:50] epoch 4571, training loss: 5706.53, average training loss: 5813.33, base loss: 20518.94
[INFO 2017-06-26 12:54:27,003 main.py:50] epoch 4572, training loss: 5762.84, average training loss: 5813.25, base loss: 20518.94
[INFO 2017-06-26 12:54:27,362 main.py:50] epoch 4573, training loss: 5788.35, average training loss: 5813.08, base loss: 20519.03
[INFO 2017-06-26 12:54:27,724 main.py:50] epoch 4574, training loss: 5733.69, average training loss: 5812.86, base loss: 20519.06
[INFO 2017-06-26 12:54:28,084 main.py:50] epoch 4575, training loss: 5742.73, average training loss: 5812.76, base loss: 20518.92
[INFO 2017-06-26 12:54:28,446 main.py:50] epoch 4576, training loss: 5755.24, average training loss: 5812.62, base loss: 20518.71
[INFO 2017-06-26 12:54:28,805 main.py:50] epoch 4577, training loss: 5740.85, average training loss: 5812.36, base loss: 20518.67
[INFO 2017-06-26 12:54:29,166 main.py:50] epoch 4578, training loss: 5784.21, average training loss: 5812.19, base loss: 20518.58
[INFO 2017-06-26 12:54:29,527 main.py:50] epoch 4579, training loss: 5764.62, average training loss: 5812.14, base loss: 20518.68
[INFO 2017-06-26 12:54:29,886 main.py:50] epoch 4580, training loss: 5700.20, average training loss: 5812.01, base loss: 20518.35
[INFO 2017-06-26 12:54:30,247 main.py:50] epoch 4581, training loss: 5850.41, average training loss: 5811.96, base loss: 20518.34
[INFO 2017-06-26 12:54:30,607 main.py:50] epoch 4582, training loss: 5787.04, average training loss: 5811.84, base loss: 20518.46
[INFO 2017-06-26 12:54:30,968 main.py:50] epoch 4583, training loss: 5735.01, average training loss: 5811.62, base loss: 20518.38
[INFO 2017-06-26 12:54:31,327 main.py:50] epoch 4584, training loss: 5825.77, average training loss: 5811.55, base loss: 20518.25
[INFO 2017-06-26 12:54:31,686 main.py:50] epoch 4585, training loss: 5768.53, average training loss: 5811.36, base loss: 20517.83
[INFO 2017-06-26 12:54:32,045 main.py:50] epoch 4586, training loss: 5792.17, average training loss: 5811.18, base loss: 20517.62
[INFO 2017-06-26 12:54:32,406 main.py:50] epoch 4587, training loss: 5790.74, average training loss: 5811.04, base loss: 20517.59
[INFO 2017-06-26 12:54:32,767 main.py:50] epoch 4588, training loss: 5745.84, average training loss: 5810.81, base loss: 20517.06
[INFO 2017-06-26 12:54:33,127 main.py:50] epoch 4589, training loss: 5807.78, average training loss: 5810.73, base loss: 20516.84
[INFO 2017-06-26 12:54:33,488 main.py:50] epoch 4590, training loss: 5756.33, average training loss: 5810.64, base loss: 20516.97
[INFO 2017-06-26 12:54:33,848 main.py:50] epoch 4591, training loss: 5829.77, average training loss: 5810.53, base loss: 20516.96
[INFO 2017-06-26 12:54:34,208 main.py:50] epoch 4592, training loss: 5786.61, average training loss: 5810.45, base loss: 20517.20
[INFO 2017-06-26 12:54:34,568 main.py:50] epoch 4593, training loss: 5797.41, average training loss: 5810.39, base loss: 20517.26
[INFO 2017-06-26 12:54:34,927 main.py:50] epoch 4594, training loss: 5810.59, average training loss: 5810.35, base loss: 20517.16
[INFO 2017-06-26 12:54:35,287 main.py:50] epoch 4595, training loss: 5787.04, average training loss: 5810.31, base loss: 20517.40
[INFO 2017-06-26 12:54:35,647 main.py:50] epoch 4596, training loss: 5761.51, average training loss: 5810.23, base loss: 20517.68
[INFO 2017-06-26 12:54:36,007 main.py:50] epoch 4597, training loss: 5690.27, average training loss: 5810.09, base loss: 20517.54
[INFO 2017-06-26 12:54:36,366 main.py:50] epoch 4598, training loss: 5666.36, average training loss: 5809.82, base loss: 20517.30
[INFO 2017-06-26 12:54:36,727 main.py:50] epoch 4599, training loss: 5750.14, average training loss: 5809.73, base loss: 20517.71
[INFO 2017-06-26 12:54:36,727 main.py:52] epoch 4599, testing
[INFO 2017-06-26 12:54:38,207 main.py:103] average testing loss: 5742.41, base loss: 20581.36
[INFO 2017-06-26 12:54:38,207 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 12:54:38,213 main.py:76] current best accuracy: 5742.41
[INFO 2017-06-26 12:54:38,572 main.py:50] epoch 4600, training loss: 5798.60, average training loss: 5809.72, base loss: 20517.78
[INFO 2017-06-26 12:54:38,931 main.py:50] epoch 4601, training loss: 5694.65, average training loss: 5809.62, base loss: 20517.77
[INFO 2017-06-26 12:54:39,291 main.py:50] epoch 4602, training loss: 5730.17, average training loss: 5809.51, base loss: 20517.63
[INFO 2017-06-26 12:54:39,651 main.py:50] epoch 4603, training loss: 5711.01, average training loss: 5809.40, base loss: 20517.28
[INFO 2017-06-26 12:54:40,011 main.py:50] epoch 4604, training loss: 5762.73, average training loss: 5809.30, base loss: 20517.06
[INFO 2017-06-26 12:54:40,370 main.py:50] epoch 4605, training loss: 5701.59, average training loss: 5809.18, base loss: 20516.88
[INFO 2017-06-26 12:54:40,732 main.py:50] epoch 4606, training loss: 5743.45, average training loss: 5809.04, base loss: 20517.08
[INFO 2017-06-26 12:54:41,091 main.py:50] epoch 4607, training loss: 5689.42, average training loss: 5808.89, base loss: 20517.07
[INFO 2017-06-26 12:54:41,451 main.py:50] epoch 4608, training loss: 5755.24, average training loss: 5808.79, base loss: 20516.98
[INFO 2017-06-26 12:54:41,811 main.py:50] epoch 4609, training loss: 5709.42, average training loss: 5808.65, base loss: 20517.09
[INFO 2017-06-26 12:54:42,173 main.py:50] epoch 4610, training loss: 5722.11, average training loss: 5808.53, base loss: 20517.02
[INFO 2017-06-26 12:54:42,532 main.py:50] epoch 4611, training loss: 5756.83, average training loss: 5808.42, base loss: 20517.06
[INFO 2017-06-26 12:54:42,891 main.py:50] epoch 4612, training loss: 5754.77, average training loss: 5808.32, base loss: 20517.22
[INFO 2017-06-26 12:54:43,251 main.py:50] epoch 4613, training loss: 5757.32, average training loss: 5808.20, base loss: 20517.26
[INFO 2017-06-26 12:54:43,612 main.py:50] epoch 4614, training loss: 5743.04, average training loss: 5808.14, base loss: 20517.09
[INFO 2017-06-26 12:54:43,970 main.py:50] epoch 4615, training loss: 5719.42, average training loss: 5808.00, base loss: 20517.09
[INFO 2017-06-26 12:54:44,332 main.py:50] epoch 4616, training loss: 5737.06, average training loss: 5807.85, base loss: 20517.25
[INFO 2017-06-26 12:54:44,692 main.py:50] epoch 4617, training loss: 5758.90, average training loss: 5807.78, base loss: 20517.19
[INFO 2017-06-26 12:54:45,052 main.py:50] epoch 4618, training loss: 5780.69, average training loss: 5807.71, base loss: 20517.26
[INFO 2017-06-26 12:54:45,413 main.py:50] epoch 4619, training loss: 5682.08, average training loss: 5807.50, base loss: 20517.13
[INFO 2017-06-26 12:54:45,773 main.py:50] epoch 4620, training loss: 5779.71, average training loss: 5807.47, base loss: 20517.09
[INFO 2017-06-26 12:54:46,132 main.py:50] epoch 4621, training loss: 5780.30, average training loss: 5807.39, base loss: 20516.87
[INFO 2017-06-26 12:54:46,492 main.py:50] epoch 4622, training loss: 5665.20, average training loss: 5807.17, base loss: 20516.60
[INFO 2017-06-26 12:54:46,853 main.py:50] epoch 4623, training loss: 5699.92, average training loss: 5807.00, base loss: 20516.63
[INFO 2017-06-26 12:54:47,213 main.py:50] epoch 4624, training loss: 5736.44, average training loss: 5806.91, base loss: 20516.83
[INFO 2017-06-26 12:54:47,573 main.py:50] epoch 4625, training loss: 5709.99, average training loss: 5806.76, base loss: 20516.66
[INFO 2017-06-26 12:54:47,934 main.py:50] epoch 4626, training loss: 5657.52, average training loss: 5806.57, base loss: 20516.64
[INFO 2017-06-26 12:54:48,294 main.py:50] epoch 4627, training loss: 5694.07, average training loss: 5806.40, base loss: 20516.65
[INFO 2017-06-26 12:54:48,655 main.py:50] epoch 4628, training loss: 5695.73, average training loss: 5806.25, base loss: 20516.75
[INFO 2017-06-26 12:54:49,014 main.py:50] epoch 4629, training loss: 5673.63, average training loss: 5806.07, base loss: 20516.89
[INFO 2017-06-26 12:54:49,375 main.py:50] epoch 4630, training loss: 5730.06, average training loss: 5805.93, base loss: 20517.09
[INFO 2017-06-26 12:54:49,734 main.py:50] epoch 4631, training loss: 5738.83, average training loss: 5805.86, base loss: 20517.49
[INFO 2017-06-26 12:54:50,093 main.py:50] epoch 4632, training loss: 5792.86, average training loss: 5805.79, base loss: 20517.40
[INFO 2017-06-26 12:54:50,455 main.py:50] epoch 4633, training loss: 5765.19, average training loss: 5805.74, base loss: 20517.59
[INFO 2017-06-26 12:54:50,814 main.py:50] epoch 4634, training loss: 5708.22, average training loss: 5805.64, base loss: 20517.67
[INFO 2017-06-26 12:54:51,173 main.py:50] epoch 4635, training loss: 5723.02, average training loss: 5805.51, base loss: 20517.67
[INFO 2017-06-26 12:54:51,532 main.py:50] epoch 4636, training loss: 5726.57, average training loss: 5805.43, base loss: 20517.71
[INFO 2017-06-26 12:54:51,890 main.py:50] epoch 4637, training loss: 5746.49, average training loss: 5805.32, base loss: 20517.79
[INFO 2017-06-26 12:54:52,248 main.py:50] epoch 4638, training loss: 5766.68, average training loss: 5805.15, base loss: 20518.10
[INFO 2017-06-26 12:54:52,608 main.py:50] epoch 4639, training loss: 5737.69, average training loss: 5805.16, base loss: 20518.32
[INFO 2017-06-26 12:54:52,967 main.py:50] epoch 4640, training loss: 5766.29, average training loss: 5804.99, base loss: 20517.85
[INFO 2017-06-26 12:54:53,327 main.py:50] epoch 4641, training loss: 5762.00, average training loss: 5804.84, base loss: 20518.16
[INFO 2017-06-26 12:54:53,689 main.py:50] epoch 4642, training loss: 5754.87, average training loss: 5804.59, base loss: 20517.73
[INFO 2017-06-26 12:54:54,049 main.py:50] epoch 4643, training loss: 5740.68, average training loss: 5804.49, base loss: 20517.96
[INFO 2017-06-26 12:54:54,407 main.py:50] epoch 4644, training loss: 5764.11, average training loss: 5804.28, base loss: 20517.58
[INFO 2017-06-26 12:54:54,767 main.py:50] epoch 4645, training loss: 5729.10, average training loss: 5804.04, base loss: 20517.48
[INFO 2017-06-26 12:54:55,127 main.py:50] epoch 4646, training loss: 5784.58, average training loss: 5803.92, base loss: 20517.57
[INFO 2017-06-26 12:54:55,486 main.py:50] epoch 4647, training loss: 5700.40, average training loss: 5803.57, base loss: 20517.11
[INFO 2017-06-26 12:54:55,846 main.py:50] epoch 4648, training loss: 5729.79, average training loss: 5803.42, base loss: 20517.03
[INFO 2017-06-26 12:54:56,204 main.py:50] epoch 4649, training loss: 5724.68, average training loss: 5803.18, base loss: 20516.66
[INFO 2017-06-26 12:54:56,564 main.py:50] epoch 4650, training loss: 5767.76, average training loss: 5803.02, base loss: 20516.28
[INFO 2017-06-26 12:54:56,922 main.py:50] epoch 4651, training loss: 5711.55, average training loss: 5802.75, base loss: 20516.17
[INFO 2017-06-26 12:54:57,281 main.py:50] epoch 4652, training loss: 5712.06, average training loss: 5802.63, base loss: 20516.15
[INFO 2017-06-26 12:54:57,640 main.py:50] epoch 4653, training loss: 5732.63, average training loss: 5802.42, base loss: 20515.80
[INFO 2017-06-26 12:54:58,000 main.py:50] epoch 4654, training loss: 5721.37, average training loss: 5802.29, base loss: 20515.92
[INFO 2017-06-26 12:54:58,360 main.py:50] epoch 4655, training loss: 5705.03, average training loss: 5802.15, base loss: 20516.31
[INFO 2017-06-26 12:54:58,720 main.py:50] epoch 4656, training loss: 5686.61, average training loss: 5801.96, base loss: 20516.53
[INFO 2017-06-26 12:54:59,080 main.py:50] epoch 4657, training loss: 5737.15, average training loss: 5801.78, base loss: 20516.26
[INFO 2017-06-26 12:54:59,440 main.py:50] epoch 4658, training loss: 5701.32, average training loss: 5801.56, base loss: 20516.00
[INFO 2017-06-26 12:54:59,799 main.py:50] epoch 4659, training loss: 5728.33, average training loss: 5801.41, base loss: 20515.84
[INFO 2017-06-26 12:55:00,159 main.py:50] epoch 4660, training loss: 5667.54, average training loss: 5801.21, base loss: 20515.44
[INFO 2017-06-26 12:55:00,519 main.py:50] epoch 4661, training loss: 5772.92, average training loss: 5801.09, base loss: 20515.71
[INFO 2017-06-26 12:55:00,878 main.py:50] epoch 4662, training loss: 5634.74, average training loss: 5800.89, base loss: 20515.63
[INFO 2017-06-26 12:55:01,238 main.py:50] epoch 4663, training loss: 5743.65, average training loss: 5800.72, base loss: 20515.52
[INFO 2017-06-26 12:55:01,598 main.py:50] epoch 4664, training loss: 5709.38, average training loss: 5800.53, base loss: 20515.81
[INFO 2017-06-26 12:55:01,958 main.py:50] epoch 4665, training loss: 5852.22, average training loss: 5800.58, base loss: 20515.99
[INFO 2017-06-26 12:55:02,318 main.py:50] epoch 4666, training loss: 5753.42, average training loss: 5800.52, base loss: 20516.00
[INFO 2017-06-26 12:55:02,677 main.py:50] epoch 4667, training loss: 5750.71, average training loss: 5800.40, base loss: 20515.81
[INFO 2017-06-26 12:55:03,036 main.py:50] epoch 4668, training loss: 5748.12, average training loss: 5800.33, base loss: 20515.76
[INFO 2017-06-26 12:55:03,395 main.py:50] epoch 4669, training loss: 5731.76, average training loss: 5800.22, base loss: 20515.67
[INFO 2017-06-26 12:55:03,755 main.py:50] epoch 4670, training loss: 5744.92, average training loss: 5800.13, base loss: 20515.49
[INFO 2017-06-26 12:55:04,114 main.py:50] epoch 4671, training loss: 5716.13, average training loss: 5800.03, base loss: 20515.33
[INFO 2017-06-26 12:55:04,473 main.py:50] epoch 4672, training loss: 5735.31, average training loss: 5799.87, base loss: 20515.06
[INFO 2017-06-26 12:55:04,832 main.py:50] epoch 4673, training loss: 5686.25, average training loss: 5799.66, base loss: 20515.04
[INFO 2017-06-26 12:55:05,191 main.py:50] epoch 4674, training loss: 5727.75, average training loss: 5799.53, base loss: 20515.30
[INFO 2017-06-26 12:55:05,551 main.py:50] epoch 4675, training loss: 5703.33, average training loss: 5799.34, base loss: 20515.04
[INFO 2017-06-26 12:55:05,911 main.py:50] epoch 4676, training loss: 5731.91, average training loss: 5799.25, base loss: 20515.15
[INFO 2017-06-26 12:55:06,270 main.py:50] epoch 4677, training loss: 5781.01, average training loss: 5799.09, base loss: 20515.52
[INFO 2017-06-26 12:55:06,630 main.py:50] epoch 4678, training loss: 5792.25, average training loss: 5798.95, base loss: 20515.25
[INFO 2017-06-26 12:55:06,990 main.py:50] epoch 4679, training loss: 5665.67, average training loss: 5798.65, base loss: 20514.72
[INFO 2017-06-26 12:55:07,351 main.py:50] epoch 4680, training loss: 5775.36, average training loss: 5798.55, base loss: 20515.04
[INFO 2017-06-26 12:55:07,712 main.py:50] epoch 4681, training loss: 5786.73, average training loss: 5798.42, base loss: 20515.00
[INFO 2017-06-26 12:55:08,073 main.py:50] epoch 4682, training loss: 5764.63, average training loss: 5798.38, base loss: 20515.33
[INFO 2017-06-26 12:55:08,434 main.py:50] epoch 4683, training loss: 5747.57, average training loss: 5798.34, base loss: 20515.50
[INFO 2017-06-26 12:55:08,792 main.py:50] epoch 4684, training loss: 5785.94, average training loss: 5798.19, base loss: 20515.22
[INFO 2017-06-26 12:55:09,152 main.py:50] epoch 4685, training loss: 5714.66, average training loss: 5798.05, base loss: 20515.07
[INFO 2017-06-26 12:55:09,512 main.py:50] epoch 4686, training loss: 5797.38, average training loss: 5797.97, base loss: 20514.75
[INFO 2017-06-26 12:55:09,887 main.py:50] epoch 4687, training loss: 5741.34, average training loss: 5797.83, base loss: 20514.86
[INFO 2017-06-26 12:55:10,249 main.py:50] epoch 4688, training loss: 5788.22, average training loss: 5797.73, base loss: 20514.68
[INFO 2017-06-26 12:55:10,609 main.py:50] epoch 4689, training loss: 5665.14, average training loss: 5797.49, base loss: 20514.84
[INFO 2017-06-26 12:55:10,968 main.py:50] epoch 4690, training loss: 5785.24, average training loss: 5797.42, base loss: 20514.75
[INFO 2017-06-26 12:55:11,328 main.py:50] epoch 4691, training loss: 5711.72, average training loss: 5797.29, base loss: 20514.58
[INFO 2017-06-26 12:55:11,687 main.py:50] epoch 4692, training loss: 5694.27, average training loss: 5797.15, base loss: 20514.67
[INFO 2017-06-26 12:55:12,049 main.py:50] epoch 4693, training loss: 5783.04, average training loss: 5797.07, base loss: 20514.83
[INFO 2017-06-26 12:55:12,408 main.py:50] epoch 4694, training loss: 5756.37, average training loss: 5797.00, base loss: 20514.66
[INFO 2017-06-26 12:55:12,768 main.py:50] epoch 4695, training loss: 5709.88, average training loss: 5796.95, base loss: 20514.45
[INFO 2017-06-26 12:55:13,128 main.py:50] epoch 4696, training loss: 5681.26, average training loss: 5796.82, base loss: 20514.35
[INFO 2017-06-26 12:55:13,488 main.py:50] epoch 4697, training loss: 5735.04, average training loss: 5796.67, base loss: 20514.16
[INFO 2017-06-26 12:55:13,850 main.py:50] epoch 4698, training loss: 5704.24, average training loss: 5796.54, base loss: 20514.15
[INFO 2017-06-26 12:55:14,209 main.py:50] epoch 4699, training loss: 5721.43, average training loss: 5796.45, base loss: 20514.31
[INFO 2017-06-26 12:55:14,209 main.py:52] epoch 4699, testing
[INFO 2017-06-26 12:55:15,676 main.py:103] average testing loss: 5765.94, base loss: 20534.94
[INFO 2017-06-26 12:55:15,677 main.py:76] current best accuracy: 5742.41
[INFO 2017-06-26 12:55:16,035 main.py:50] epoch 4700, training loss: 5761.87, average training loss: 5796.41, base loss: 20514.34
[INFO 2017-06-26 12:55:16,394 main.py:50] epoch 4701, training loss: 5713.52, average training loss: 5796.33, base loss: 20514.44
[INFO 2017-06-26 12:55:16,753 main.py:50] epoch 4702, training loss: 5721.59, average training loss: 5796.21, base loss: 20514.22
[INFO 2017-06-26 12:55:17,112 main.py:50] epoch 4703, training loss: 5713.27, average training loss: 5796.01, base loss: 20514.12
[INFO 2017-06-26 12:55:17,470 main.py:50] epoch 4704, training loss: 5756.05, average training loss: 5795.93, base loss: 20514.39
[INFO 2017-06-26 12:55:17,830 main.py:50] epoch 4705, training loss: 5732.50, average training loss: 5795.81, base loss: 20514.56
[INFO 2017-06-26 12:55:18,190 main.py:50] epoch 4706, training loss: 5722.26, average training loss: 5795.72, base loss: 20514.54
[INFO 2017-06-26 12:55:18,547 main.py:50] epoch 4707, training loss: 5713.79, average training loss: 5795.63, base loss: 20514.88
[INFO 2017-06-26 12:55:18,906 main.py:50] epoch 4708, training loss: 5684.20, average training loss: 5795.48, base loss: 20514.43
[INFO 2017-06-26 12:55:19,266 main.py:50] epoch 4709, training loss: 5722.37, average training loss: 5795.41, base loss: 20514.56
[INFO 2017-06-26 12:55:19,625 main.py:50] epoch 4710, training loss: 5739.48, average training loss: 5795.32, base loss: 20514.58
[INFO 2017-06-26 12:55:19,984 main.py:50] epoch 4711, training loss: 5744.24, average training loss: 5795.18, base loss: 20514.43
[INFO 2017-06-26 12:55:20,343 main.py:50] epoch 4712, training loss: 5696.75, average training loss: 5795.08, base loss: 20514.20
[INFO 2017-06-26 12:55:20,702 main.py:50] epoch 4713, training loss: 5775.50, average training loss: 5794.99, base loss: 20513.76
[INFO 2017-06-26 12:55:21,073 main.py:50] epoch 4714, training loss: 5729.53, average training loss: 5794.81, base loss: 20513.30
[INFO 2017-06-26 12:55:21,433 main.py:50] epoch 4715, training loss: 5701.23, average training loss: 5794.65, base loss: 20513.29
[INFO 2017-06-26 12:55:21,791 main.py:50] epoch 4716, training loss: 5702.50, average training loss: 5794.49, base loss: 20513.18
[INFO 2017-06-26 12:55:22,152 main.py:50] epoch 4717, training loss: 5659.23, average training loss: 5794.29, base loss: 20513.57
[INFO 2017-06-26 12:55:22,510 main.py:50] epoch 4718, training loss: 5659.78, average training loss: 5794.13, base loss: 20514.07
[INFO 2017-06-26 12:55:22,869 main.py:50] epoch 4719, training loss: 5684.66, average training loss: 5793.93, base loss: 20513.83
[INFO 2017-06-26 12:55:23,228 main.py:50] epoch 4720, training loss: 5708.84, average training loss: 5793.81, base loss: 20513.87
[INFO 2017-06-26 12:55:23,589 main.py:50] epoch 4721, training loss: 5626.22, average training loss: 5793.54, base loss: 20513.79
[INFO 2017-06-26 12:55:23,949 main.py:50] epoch 4722, training loss: 5724.64, average training loss: 5793.43, base loss: 20513.82
[INFO 2017-06-26 12:55:24,310 main.py:50] epoch 4723, training loss: 5688.16, average training loss: 5793.24, base loss: 20513.31
[INFO 2017-06-26 12:55:24,667 main.py:50] epoch 4724, training loss: 5735.39, average training loss: 5793.15, base loss: 20513.72
[INFO 2017-06-26 12:55:25,026 main.py:50] epoch 4725, training loss: 5684.67, average training loss: 5792.96, base loss: 20513.52
[INFO 2017-06-26 12:55:25,385 main.py:50] epoch 4726, training loss: 5677.85, average training loss: 5792.83, base loss: 20513.59
[INFO 2017-06-26 12:55:25,744 main.py:50] epoch 4727, training loss: 5689.84, average training loss: 5792.63, base loss: 20513.63
[INFO 2017-06-26 12:55:26,104 main.py:50] epoch 4728, training loss: 5666.47, average training loss: 5792.44, base loss: 20513.53
[INFO 2017-06-26 12:55:26,464 main.py:50] epoch 4729, training loss: 5711.23, average training loss: 5792.32, base loss: 20513.82
[INFO 2017-06-26 12:55:26,824 main.py:50] epoch 4730, training loss: 5702.61, average training loss: 5792.21, base loss: 20514.21
[INFO 2017-06-26 12:55:27,184 main.py:50] epoch 4731, training loss: 5757.33, average training loss: 5792.14, base loss: 20514.27
[INFO 2017-06-26 12:55:27,542 main.py:50] epoch 4732, training loss: 5709.92, average training loss: 5791.94, base loss: 20513.80
[INFO 2017-06-26 12:55:27,902 main.py:50] epoch 4733, training loss: 5793.99, average training loss: 5791.85, base loss: 20514.20
[INFO 2017-06-26 12:55:28,261 main.py:50] epoch 4734, training loss: 5697.11, average training loss: 5791.71, base loss: 20514.17
[INFO 2017-06-26 12:55:28,622 main.py:50] epoch 4735, training loss: 5769.15, average training loss: 5791.62, base loss: 20514.15
[INFO 2017-06-26 12:55:28,981 main.py:50] epoch 4736, training loss: 5744.97, average training loss: 5791.45, base loss: 20513.61
[INFO 2017-06-26 12:55:29,341 main.py:50] epoch 4737, training loss: 5765.24, average training loss: 5791.38, base loss: 20513.97
[INFO 2017-06-26 12:55:29,700 main.py:50] epoch 4738, training loss: 5710.11, average training loss: 5791.32, base loss: 20514.29
[INFO 2017-06-26 12:55:30,061 main.py:50] epoch 4739, training loss: 5760.59, average training loss: 5791.20, base loss: 20514.71
[INFO 2017-06-26 12:55:30,421 main.py:50] epoch 4740, training loss: 5757.94, average training loss: 5791.07, base loss: 20514.57
[INFO 2017-06-26 12:55:30,779 main.py:50] epoch 4741, training loss: 5738.47, average training loss: 5790.93, base loss: 20514.57
[INFO 2017-06-26 12:55:31,139 main.py:50] epoch 4742, training loss: 5766.07, average training loss: 5790.85, base loss: 20514.25
[INFO 2017-06-26 12:55:31,499 main.py:50] epoch 4743, training loss: 5777.48, average training loss: 5790.77, base loss: 20514.72
[INFO 2017-06-26 12:55:31,860 main.py:50] epoch 4744, training loss: 5796.52, average training loss: 5790.71, base loss: 20515.45
[INFO 2017-06-26 12:55:32,219 main.py:50] epoch 4745, training loss: 5760.54, average training loss: 5790.54, base loss: 20515.45
[INFO 2017-06-26 12:55:32,579 main.py:50] epoch 4746, training loss: 5712.08, average training loss: 5790.29, base loss: 20515.20
[INFO 2017-06-26 12:55:32,939 main.py:50] epoch 4747, training loss: 5754.24, average training loss: 5790.17, base loss: 20515.07
[INFO 2017-06-26 12:55:33,300 main.py:50] epoch 4748, training loss: 5766.80, average training loss: 5790.01, base loss: 20514.93
[INFO 2017-06-26 12:55:33,658 main.py:50] epoch 4749, training loss: 5686.38, average training loss: 5789.80, base loss: 20514.56
[INFO 2017-06-26 12:55:34,019 main.py:50] epoch 4750, training loss: 5769.69, average training loss: 5789.65, base loss: 20514.66
[INFO 2017-06-26 12:55:34,379 main.py:50] epoch 4751, training loss: 5683.21, average training loss: 5789.45, base loss: 20514.30
[INFO 2017-06-26 12:55:34,740 main.py:50] epoch 4752, training loss: 5729.53, average training loss: 5789.38, base loss: 20514.82
[INFO 2017-06-26 12:55:35,099 main.py:50] epoch 4753, training loss: 5727.97, average training loss: 5789.18, base loss: 20515.13
[INFO 2017-06-26 12:55:35,460 main.py:50] epoch 4754, training loss: 5723.49, average training loss: 5789.01, base loss: 20515.06
[INFO 2017-06-26 12:55:35,820 main.py:50] epoch 4755, training loss: 5740.73, average training loss: 5788.88, base loss: 20514.95
[INFO 2017-06-26 12:55:36,181 main.py:50] epoch 4756, training loss: 5695.79, average training loss: 5788.64, base loss: 20514.13
[INFO 2017-06-26 12:55:36,540 main.py:50] epoch 4757, training loss: 5851.48, average training loss: 5788.58, base loss: 20514.61
[INFO 2017-06-26 12:55:36,902 main.py:50] epoch 4758, training loss: 5746.38, average training loss: 5788.39, base loss: 20514.38
[INFO 2017-06-26 12:55:37,261 main.py:50] epoch 4759, training loss: 5726.43, average training loss: 5788.19, base loss: 20514.31
[INFO 2017-06-26 12:55:37,622 main.py:50] epoch 4760, training loss: 5715.96, average training loss: 5788.02, base loss: 20514.26
[INFO 2017-06-26 12:55:37,980 main.py:50] epoch 4761, training loss: 5664.78, average training loss: 5787.80, base loss: 20514.19
[INFO 2017-06-26 12:55:38,340 main.py:50] epoch 4762, training loss: 5793.48, average training loss: 5787.71, base loss: 20514.31
[INFO 2017-06-26 12:55:38,700 main.py:50] epoch 4763, training loss: 5719.68, average training loss: 5787.51, base loss: 20514.04
[INFO 2017-06-26 12:55:39,062 main.py:50] epoch 4764, training loss: 5766.09, average training loss: 5787.38, base loss: 20513.91
[INFO 2017-06-26 12:55:39,421 main.py:50] epoch 4765, training loss: 5663.91, average training loss: 5787.16, base loss: 20514.12
[INFO 2017-06-26 12:55:39,780 main.py:50] epoch 4766, training loss: 5713.21, average training loss: 5787.06, base loss: 20514.30
[INFO 2017-06-26 12:55:40,139 main.py:50] epoch 4767, training loss: 5671.15, average training loss: 5786.91, base loss: 20514.22
[INFO 2017-06-26 12:55:40,498 main.py:50] epoch 4768, training loss: 5719.55, average training loss: 5786.78, base loss: 20513.96
[INFO 2017-06-26 12:55:40,857 main.py:50] epoch 4769, training loss: 5727.47, average training loss: 5786.67, base loss: 20514.02
[INFO 2017-06-26 12:55:41,217 main.py:50] epoch 4770, training loss: 5766.06, average training loss: 5786.54, base loss: 20514.23
[INFO 2017-06-26 12:55:41,578 main.py:50] epoch 4771, training loss: 5732.02, average training loss: 5786.42, base loss: 20514.74
[INFO 2017-06-26 12:55:41,936 main.py:50] epoch 4772, training loss: 5764.27, average training loss: 5786.40, base loss: 20515.19
[INFO 2017-06-26 12:55:42,296 main.py:50] epoch 4773, training loss: 5727.45, average training loss: 5786.38, base loss: 20515.16
[INFO 2017-06-26 12:55:42,656 main.py:50] epoch 4774, training loss: 5774.73, average training loss: 5786.28, base loss: 20515.14
[INFO 2017-06-26 12:55:43,015 main.py:50] epoch 4775, training loss: 5721.86, average training loss: 5786.15, base loss: 20515.12
[INFO 2017-06-26 12:55:43,375 main.py:50] epoch 4776, training loss: 5691.54, average training loss: 5785.98, base loss: 20515.25
[INFO 2017-06-26 12:55:43,736 main.py:50] epoch 4777, training loss: 5732.71, average training loss: 5785.87, base loss: 20515.36
[INFO 2017-06-26 12:55:44,095 main.py:50] epoch 4778, training loss: 5678.39, average training loss: 5785.75, base loss: 20515.17
[INFO 2017-06-26 12:55:44,455 main.py:50] epoch 4779, training loss: 5693.06, average training loss: 5785.61, base loss: 20515.18
[INFO 2017-06-26 12:55:44,814 main.py:50] epoch 4780, training loss: 5732.69, average training loss: 5785.50, base loss: 20515.26
[INFO 2017-06-26 12:55:45,174 main.py:50] epoch 4781, training loss: 5747.63, average training loss: 5785.45, base loss: 20515.38
[INFO 2017-06-26 12:55:45,534 main.py:50] epoch 4782, training loss: 5682.44, average training loss: 5785.27, base loss: 20515.43
[INFO 2017-06-26 12:55:45,893 main.py:50] epoch 4783, training loss: 5766.06, average training loss: 5785.25, base loss: 20515.70
[INFO 2017-06-26 12:55:46,253 main.py:50] epoch 4784, training loss: 5624.75, average training loss: 5785.11, base loss: 20515.87
[INFO 2017-06-26 12:55:46,614 main.py:50] epoch 4785, training loss: 5705.93, average training loss: 5784.94, base loss: 20515.65
[INFO 2017-06-26 12:55:46,973 main.py:50] epoch 4786, training loss: 5679.96, average training loss: 5784.77, base loss: 20515.74
[INFO 2017-06-26 12:55:47,332 main.py:50] epoch 4787, training loss: 5680.47, average training loss: 5784.57, base loss: 20515.78
[INFO 2017-06-26 12:55:47,692 main.py:50] epoch 4788, training loss: 5780.96, average training loss: 5784.53, base loss: 20515.96
[INFO 2017-06-26 12:55:48,052 main.py:50] epoch 4789, training loss: 5664.90, average training loss: 5784.33, base loss: 20515.64
[INFO 2017-06-26 12:55:48,411 main.py:50] epoch 4790, training loss: 5740.28, average training loss: 5784.28, base loss: 20515.69
[INFO 2017-06-26 12:55:48,770 main.py:50] epoch 4791, training loss: 5694.21, average training loss: 5784.14, base loss: 20516.00
[INFO 2017-06-26 12:55:49,130 main.py:50] epoch 4792, training loss: 5726.90, average training loss: 5784.01, base loss: 20515.90
[INFO 2017-06-26 12:55:49,490 main.py:50] epoch 4793, training loss: 5709.05, average training loss: 5783.89, base loss: 20515.99
[INFO 2017-06-26 12:55:49,848 main.py:50] epoch 4794, training loss: 5675.11, average training loss: 5783.76, base loss: 20515.88
[INFO 2017-06-26 12:55:50,205 main.py:50] epoch 4795, training loss: 5736.05, average training loss: 5783.66, base loss: 20515.97
[INFO 2017-06-26 12:55:50,565 main.py:50] epoch 4796, training loss: 5720.00, average training loss: 5783.51, base loss: 20516.07
[INFO 2017-06-26 12:55:50,924 main.py:50] epoch 4797, training loss: 5732.49, average training loss: 5783.40, base loss: 20516.15
[INFO 2017-06-26 12:55:51,284 main.py:50] epoch 4798, training loss: 5733.03, average training loss: 5783.24, base loss: 20516.14
[INFO 2017-06-26 12:55:51,643 main.py:50] epoch 4799, training loss: 5699.07, average training loss: 5783.02, base loss: 20515.94
[INFO 2017-06-26 12:55:51,643 main.py:52] epoch 4799, testing
[INFO 2017-06-26 12:55:53,116 main.py:103] average testing loss: 5703.71, base loss: 20533.30
[INFO 2017-06-26 12:55:53,117 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 12:55:53,125 main.py:76] current best accuracy: 5703.71
[INFO 2017-06-26 12:55:53,485 main.py:50] epoch 4800, training loss: 5653.97, average training loss: 5782.85, base loss: 20515.84
[INFO 2017-06-26 12:55:53,843 main.py:50] epoch 4801, training loss: 5768.35, average training loss: 5782.77, base loss: 20516.15
[INFO 2017-06-26 12:55:54,203 main.py:50] epoch 4802, training loss: 5646.52, average training loss: 5782.50, base loss: 20516.03
[INFO 2017-06-26 12:55:54,563 main.py:50] epoch 4803, training loss: 5712.65, average training loss: 5782.30, base loss: 20516.02
[INFO 2017-06-26 12:55:54,924 main.py:50] epoch 4804, training loss: 5717.71, average training loss: 5782.18, base loss: 20515.78
[INFO 2017-06-26 12:55:55,283 main.py:50] epoch 4805, training loss: 5737.40, average training loss: 5782.01, base loss: 20515.55
[INFO 2017-06-26 12:55:55,644 main.py:50] epoch 4806, training loss: 5727.84, average training loss: 5781.91, base loss: 20515.19
[INFO 2017-06-26 12:55:56,003 main.py:50] epoch 4807, training loss: 5648.36, average training loss: 5781.66, base loss: 20515.17
[INFO 2017-06-26 12:55:56,363 main.py:50] epoch 4808, training loss: 5651.84, average training loss: 5781.50, base loss: 20515.03
[INFO 2017-06-26 12:55:56,723 main.py:50] epoch 4809, training loss: 5731.28, average training loss: 5781.29, base loss: 20515.00
[INFO 2017-06-26 12:55:57,084 main.py:50] epoch 4810, training loss: 5675.74, average training loss: 5781.12, base loss: 20514.56
[INFO 2017-06-26 12:55:57,445 main.py:50] epoch 4811, training loss: 5683.90, average training loss: 5780.79, base loss: 20514.32
[INFO 2017-06-26 12:55:57,806 main.py:50] epoch 4812, training loss: 5728.88, average training loss: 5780.64, base loss: 20514.53
[INFO 2017-06-26 12:55:58,167 main.py:50] epoch 4813, training loss: 5641.10, average training loss: 5780.40, base loss: 20514.63
[INFO 2017-06-26 12:55:58,528 main.py:50] epoch 4814, training loss: 5706.77, average training loss: 5780.27, base loss: 20514.60
[INFO 2017-06-26 12:55:58,889 main.py:50] epoch 4815, training loss: 5661.42, average training loss: 5780.07, base loss: 20514.82
[INFO 2017-06-26 12:55:59,250 main.py:50] epoch 4816, training loss: 5703.67, average training loss: 5779.91, base loss: 20514.89
[INFO 2017-06-26 12:55:59,609 main.py:50] epoch 4817, training loss: 5689.23, average training loss: 5779.73, base loss: 20515.20
[INFO 2017-06-26 12:55:59,969 main.py:50] epoch 4818, training loss: 5731.78, average training loss: 5779.59, base loss: 20515.53
[INFO 2017-06-26 12:56:00,329 main.py:50] epoch 4819, training loss: 5719.16, average training loss: 5779.47, base loss: 20515.37
[INFO 2017-06-26 12:56:00,690 main.py:50] epoch 4820, training loss: 5709.67, average training loss: 5779.31, base loss: 20515.39
[INFO 2017-06-26 12:56:01,050 main.py:50] epoch 4821, training loss: 5690.28, average training loss: 5779.19, base loss: 20515.74
[INFO 2017-06-26 12:56:01,411 main.py:50] epoch 4822, training loss: 5666.01, average training loss: 5778.93, base loss: 20515.16
[INFO 2017-06-26 12:56:01,771 main.py:50] epoch 4823, training loss: 5699.08, average training loss: 5778.89, base loss: 20515.31
[INFO 2017-06-26 12:56:02,130 main.py:50] epoch 4824, training loss: 5742.49, average training loss: 5778.79, base loss: 20515.14
[INFO 2017-06-26 12:56:02,491 main.py:50] epoch 4825, training loss: 5677.85, average training loss: 5778.61, base loss: 20515.01
[INFO 2017-06-26 12:56:02,850 main.py:50] epoch 4826, training loss: 5671.50, average training loss: 5778.36, base loss: 20514.54
[INFO 2017-06-26 12:56:03,209 main.py:50] epoch 4827, training loss: 5675.51, average training loss: 5778.22, base loss: 20514.22
[INFO 2017-06-26 12:56:03,569 main.py:50] epoch 4828, training loss: 5672.13, average training loss: 5778.03, base loss: 20514.06
[INFO 2017-06-26 12:56:03,929 main.py:50] epoch 4829, training loss: 5659.58, average training loss: 5777.90, base loss: 20513.92
[INFO 2017-06-26 12:56:04,290 main.py:50] epoch 4830, training loss: 5628.33, average training loss: 5777.70, base loss: 20513.83
[INFO 2017-06-26 12:56:04,651 main.py:50] epoch 4831, training loss: 5784.35, average training loss: 5777.68, base loss: 20513.69
[INFO 2017-06-26 12:56:05,011 main.py:50] epoch 4832, training loss: 5665.11, average training loss: 5777.60, base loss: 20513.69
[INFO 2017-06-26 12:56:05,387 main.py:50] epoch 4833, training loss: 5769.86, average training loss: 5777.55, base loss: 20513.75
[INFO 2017-06-26 12:56:05,747 main.py:50] epoch 4834, training loss: 5660.05, average training loss: 5777.42, base loss: 20513.56
[INFO 2017-06-26 12:56:06,107 main.py:50] epoch 4835, training loss: 5682.03, average training loss: 5777.34, base loss: 20513.53
[INFO 2017-06-26 12:56:06,468 main.py:50] epoch 4836, training loss: 5746.65, average training loss: 5777.27, base loss: 20513.73
[INFO 2017-06-26 12:56:06,827 main.py:50] epoch 4837, training loss: 5707.92, average training loss: 5777.15, base loss: 20513.90
[INFO 2017-06-26 12:56:07,188 main.py:50] epoch 4838, training loss: 5731.50, average training loss: 5777.01, base loss: 20513.57
[INFO 2017-06-26 12:56:07,548 main.py:50] epoch 4839, training loss: 5643.53, average training loss: 5776.84, base loss: 20513.08
[INFO 2017-06-26 12:56:07,907 main.py:50] epoch 4840, training loss: 5755.12, average training loss: 5776.78, base loss: 20513.26
[INFO 2017-06-26 12:56:08,269 main.py:50] epoch 4841, training loss: 5692.79, average training loss: 5776.74, base loss: 20513.33
[INFO 2017-06-26 12:56:08,630 main.py:50] epoch 4842, training loss: 5744.73, average training loss: 5776.63, base loss: 20513.36
[INFO 2017-06-26 12:56:08,990 main.py:50] epoch 4843, training loss: 5728.43, average training loss: 5776.50, base loss: 20513.40
[INFO 2017-06-26 12:56:09,351 main.py:50] epoch 4844, training loss: 5702.07, average training loss: 5776.34, base loss: 20513.45
[INFO 2017-06-26 12:56:09,712 main.py:50] epoch 4845, training loss: 5696.41, average training loss: 5776.25, base loss: 20513.26
[INFO 2017-06-26 12:56:10,073 main.py:50] epoch 4846, training loss: 5689.68, average training loss: 5776.13, base loss: 20513.61
[INFO 2017-06-26 12:56:10,433 main.py:50] epoch 4847, training loss: 5686.69, average training loss: 5776.02, base loss: 20513.67
[INFO 2017-06-26 12:56:10,794 main.py:50] epoch 4848, training loss: 5749.82, average training loss: 5775.87, base loss: 20513.70
[INFO 2017-06-26 12:56:11,154 main.py:50] epoch 4849, training loss: 5746.07, average training loss: 5775.81, base loss: 20513.52
[INFO 2017-06-26 12:56:11,515 main.py:50] epoch 4850, training loss: 5641.25, average training loss: 5775.58, base loss: 20513.21
[INFO 2017-06-26 12:56:11,874 main.py:50] epoch 4851, training loss: 5699.41, average training loss: 5775.49, base loss: 20513.23
[INFO 2017-06-26 12:56:12,235 main.py:50] epoch 4852, training loss: 5728.94, average training loss: 5775.48, base loss: 20513.74
[INFO 2017-06-26 12:56:12,595 main.py:50] epoch 4853, training loss: 5727.14, average training loss: 5775.37, base loss: 20513.85
[INFO 2017-06-26 12:56:12,955 main.py:50] epoch 4854, training loss: 5711.21, average training loss: 5775.30, base loss: 20514.05
[INFO 2017-06-26 12:56:13,316 main.py:50] epoch 4855, training loss: 5666.73, average training loss: 5775.15, base loss: 20514.48
[INFO 2017-06-26 12:56:13,676 main.py:50] epoch 4856, training loss: 5709.32, average training loss: 5775.04, base loss: 20514.72
[INFO 2017-06-26 12:56:14,036 main.py:50] epoch 4857, training loss: 5693.33, average training loss: 5774.91, base loss: 20514.53
[INFO 2017-06-26 12:56:14,396 main.py:50] epoch 4858, training loss: 5704.62, average training loss: 5774.83, base loss: 20514.43
[INFO 2017-06-26 12:56:14,756 main.py:50] epoch 4859, training loss: 5710.07, average training loss: 5774.83, base loss: 20514.65
[INFO 2017-06-26 12:56:15,117 main.py:50] epoch 4860, training loss: 5727.05, average training loss: 5774.68, base loss: 20514.53
[INFO 2017-06-26 12:56:15,477 main.py:50] epoch 4861, training loss: 5724.73, average training loss: 5774.56, base loss: 20514.57
[INFO 2017-06-26 12:56:15,836 main.py:50] epoch 4862, training loss: 5648.90, average training loss: 5774.40, base loss: 20514.05
[INFO 2017-06-26 12:56:16,196 main.py:50] epoch 4863, training loss: 5803.41, average training loss: 5774.33, base loss: 20514.63
[INFO 2017-06-26 12:56:16,556 main.py:50] epoch 4864, training loss: 5692.64, average training loss: 5774.20, base loss: 20514.73
[INFO 2017-06-26 12:56:16,916 main.py:50] epoch 4865, training loss: 5712.36, average training loss: 5774.05, base loss: 20514.96
[INFO 2017-06-26 12:56:17,276 main.py:50] epoch 4866, training loss: 5687.06, average training loss: 5773.88, base loss: 20514.61
[INFO 2017-06-26 12:56:17,635 main.py:50] epoch 4867, training loss: 5725.83, average training loss: 5773.70, base loss: 20514.72
[INFO 2017-06-26 12:56:17,995 main.py:50] epoch 4868, training loss: 5696.44, average training loss: 5773.56, base loss: 20514.57
[INFO 2017-06-26 12:56:18,355 main.py:50] epoch 4869, training loss: 5701.85, average training loss: 5773.45, base loss: 20514.52
[INFO 2017-06-26 12:56:18,715 main.py:50] epoch 4870, training loss: 5786.57, average training loss: 5773.41, base loss: 20514.69
[INFO 2017-06-26 12:56:19,075 main.py:50] epoch 4871, training loss: 5668.24, average training loss: 5773.22, base loss: 20514.83
[INFO 2017-06-26 12:56:19,435 main.py:50] epoch 4872, training loss: 5772.11, average training loss: 5773.21, base loss: 20514.84
[INFO 2017-06-26 12:56:19,796 main.py:50] epoch 4873, training loss: 5698.33, average training loss: 5772.98, base loss: 20514.73
[INFO 2017-06-26 12:56:20,155 main.py:50] epoch 4874, training loss: 5759.10, average training loss: 5772.90, base loss: 20514.70
[INFO 2017-06-26 12:56:20,515 main.py:50] epoch 4875, training loss: 5729.20, average training loss: 5772.62, base loss: 20514.71
[INFO 2017-06-26 12:56:20,875 main.py:50] epoch 4876, training loss: 5761.52, average training loss: 5772.55, base loss: 20514.96
[INFO 2017-06-26 12:56:21,237 main.py:50] epoch 4877, training loss: 5720.71, average training loss: 5772.37, base loss: 20514.77
[INFO 2017-06-26 12:56:21,604 main.py:50] epoch 4878, training loss: 5761.26, average training loss: 5772.37, base loss: 20514.71
[INFO 2017-06-26 12:56:21,964 main.py:50] epoch 4879, training loss: 5738.81, average training loss: 5772.30, base loss: 20514.50
[INFO 2017-06-26 12:56:22,323 main.py:50] epoch 4880, training loss: 5721.17, average training loss: 5772.24, base loss: 20514.03
[INFO 2017-06-26 12:56:22,685 main.py:50] epoch 4881, training loss: 5788.38, average training loss: 5772.15, base loss: 20514.14
[INFO 2017-06-26 12:56:23,046 main.py:50] epoch 4882, training loss: 5713.23, average training loss: 5772.04, base loss: 20514.18
[INFO 2017-06-26 12:56:23,406 main.py:50] epoch 4883, training loss: 5773.51, average training loss: 5771.99, base loss: 20514.41
[INFO 2017-06-26 12:56:23,765 main.py:50] epoch 4884, training loss: 5679.50, average training loss: 5771.80, base loss: 20514.55
[INFO 2017-06-26 12:56:24,125 main.py:50] epoch 4885, training loss: 5698.11, average training loss: 5771.67, base loss: 20513.96
[INFO 2017-06-26 12:56:24,485 main.py:50] epoch 4886, training loss: 5669.53, average training loss: 5771.55, base loss: 20514.10
[INFO 2017-06-26 12:56:24,846 main.py:50] epoch 4887, training loss: 5772.30, average training loss: 5771.49, base loss: 20514.09
[INFO 2017-06-26 12:56:25,205 main.py:50] epoch 4888, training loss: 5714.55, average training loss: 5771.38, base loss: 20514.03
[INFO 2017-06-26 12:56:25,565 main.py:50] epoch 4889, training loss: 5682.70, average training loss: 5771.25, base loss: 20513.96
[INFO 2017-06-26 12:56:25,927 main.py:50] epoch 4890, training loss: 5676.37, average training loss: 5771.13, base loss: 20513.91
[INFO 2017-06-26 12:56:26,288 main.py:50] epoch 4891, training loss: 5655.18, average training loss: 5770.97, base loss: 20513.79
[INFO 2017-06-26 12:56:26,647 main.py:50] epoch 4892, training loss: 5734.51, average training loss: 5770.88, base loss: 20513.69
[INFO 2017-06-26 12:56:27,007 main.py:50] epoch 4893, training loss: 5710.55, average training loss: 5770.73, base loss: 20513.40
[INFO 2017-06-26 12:56:27,366 main.py:50] epoch 4894, training loss: 5662.49, average training loss: 5770.65, base loss: 20513.62
[INFO 2017-06-26 12:56:27,727 main.py:50] epoch 4895, training loss: 5653.91, average training loss: 5770.52, base loss: 20513.63
[INFO 2017-06-26 12:56:28,087 main.py:50] epoch 4896, training loss: 5736.89, average training loss: 5770.46, base loss: 20513.71
[INFO 2017-06-26 12:56:28,448 main.py:50] epoch 4897, training loss: 5723.93, average training loss: 5770.34, base loss: 20513.43
[INFO 2017-06-26 12:56:28,809 main.py:50] epoch 4898, training loss: 5678.06, average training loss: 5770.19, base loss: 20512.95
[INFO 2017-06-26 12:56:29,170 main.py:50] epoch 4899, training loss: 5648.08, average training loss: 5770.01, base loss: 20512.57
[INFO 2017-06-26 12:56:29,170 main.py:52] epoch 4899, testing
[INFO 2017-06-26 12:56:30,642 main.py:103] average testing loss: 5666.22, base loss: 20492.83
[INFO 2017-06-26 12:56:30,643 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 12:56:30,649 main.py:76] current best accuracy: 5666.22
[INFO 2017-06-26 12:56:31,007 main.py:50] epoch 4900, training loss: 5690.63, average training loss: 5769.80, base loss: 20512.33
[INFO 2017-06-26 12:56:31,368 main.py:50] epoch 4901, training loss: 5648.15, average training loss: 5769.68, base loss: 20512.11
[INFO 2017-06-26 12:56:31,728 main.py:50] epoch 4902, training loss: 5650.58, average training loss: 5769.50, base loss: 20512.08
[INFO 2017-06-26 12:56:32,089 main.py:50] epoch 4903, training loss: 5685.22, average training loss: 5769.42, base loss: 20512.03
[INFO 2017-06-26 12:56:32,450 main.py:50] epoch 4904, training loss: 5673.65, average training loss: 5769.29, base loss: 20512.20
[INFO 2017-06-26 12:56:32,810 main.py:50] epoch 4905, training loss: 5683.98, average training loss: 5769.10, base loss: 20512.34
[INFO 2017-06-26 12:56:33,171 main.py:50] epoch 4906, training loss: 5720.27, average training loss: 5769.04, base loss: 20512.38
[INFO 2017-06-26 12:56:33,530 main.py:50] epoch 4907, training loss: 5637.31, average training loss: 5768.75, base loss: 20512.22
[INFO 2017-06-26 12:56:33,892 main.py:50] epoch 4908, training loss: 5672.03, average training loss: 5768.61, base loss: 20512.11
[INFO 2017-06-26 12:56:34,252 main.py:50] epoch 4909, training loss: 5656.44, average training loss: 5768.40, base loss: 20512.18
[INFO 2017-06-26 12:56:34,612 main.py:50] epoch 4910, training loss: 5667.74, average training loss: 5768.28, base loss: 20512.10
[INFO 2017-06-26 12:56:34,972 main.py:50] epoch 4911, training loss: 5658.57, average training loss: 5768.13, base loss: 20512.26
[INFO 2017-06-26 12:56:35,332 main.py:50] epoch 4912, training loss: 5709.94, average training loss: 5768.01, base loss: 20512.23
[INFO 2017-06-26 12:56:35,692 main.py:50] epoch 4913, training loss: 5644.96, average training loss: 5767.79, base loss: 20511.84
[INFO 2017-06-26 12:56:36,052 main.py:50] epoch 4914, training loss: 5629.50, average training loss: 5767.65, base loss: 20511.62
[INFO 2017-06-26 12:56:36,412 main.py:50] epoch 4915, training loss: 5708.48, average training loss: 5767.53, base loss: 20511.93
[INFO 2017-06-26 12:56:36,771 main.py:50] epoch 4916, training loss: 5696.94, average training loss: 5767.40, base loss: 20512.57
[INFO 2017-06-26 12:56:37,132 main.py:50] epoch 4917, training loss: 5744.95, average training loss: 5767.22, base loss: 20512.68
[INFO 2017-06-26 12:56:37,492 main.py:50] epoch 4918, training loss: 5716.90, average training loss: 5766.99, base loss: 20512.50
[INFO 2017-06-26 12:56:37,852 main.py:50] epoch 4919, training loss: 5710.83, average training loss: 5766.81, base loss: 20512.78
[INFO 2017-06-26 12:56:38,213 main.py:50] epoch 4920, training loss: 5689.75, average training loss: 5766.63, base loss: 20512.98
[INFO 2017-06-26 12:56:38,573 main.py:50] epoch 4921, training loss: 5680.89, average training loss: 5766.38, base loss: 20512.66
[INFO 2017-06-26 12:56:38,932 main.py:50] epoch 4922, training loss: 5616.92, average training loss: 5766.12, base loss: 20512.87
[INFO 2017-06-26 12:56:39,292 main.py:50] epoch 4923, training loss: 5679.18, average training loss: 5765.97, base loss: 20512.78
[INFO 2017-06-26 12:56:39,652 main.py:50] epoch 4924, training loss: 5703.31, average training loss: 5765.83, base loss: 20512.95
[INFO 2017-06-26 12:56:40,012 main.py:50] epoch 4925, training loss: 5598.70, average training loss: 5765.58, base loss: 20512.68
[INFO 2017-06-26 12:56:40,372 main.py:50] epoch 4926, training loss: 5651.38, average training loss: 5765.40, base loss: 20513.16
[INFO 2017-06-26 12:56:40,733 main.py:50] epoch 4927, training loss: 5665.07, average training loss: 5765.25, base loss: 20513.48
[INFO 2017-06-26 12:56:41,093 main.py:50] epoch 4928, training loss: 5607.47, average training loss: 5764.98, base loss: 20513.10
[INFO 2017-06-26 12:56:41,453 main.py:50] epoch 4929, training loss: 5667.82, average training loss: 5764.82, base loss: 20513.32
[INFO 2017-06-26 12:56:41,813 main.py:50] epoch 4930, training loss: 5769.74, average training loss: 5764.65, base loss: 20513.71
[INFO 2017-06-26 12:56:42,174 main.py:50] epoch 4931, training loss: 5649.12, average training loss: 5764.50, base loss: 20513.75
[INFO 2017-06-26 12:56:42,534 main.py:50] epoch 4932, training loss: 5680.65, average training loss: 5764.32, base loss: 20513.72
[INFO 2017-06-26 12:56:42,894 main.py:50] epoch 4933, training loss: 5704.35, average training loss: 5764.10, base loss: 20513.77
[INFO 2017-06-26 12:56:43,254 main.py:50] epoch 4934, training loss: 5697.07, average training loss: 5763.82, base loss: 20513.28
[INFO 2017-06-26 12:56:43,615 main.py:50] epoch 4935, training loss: 5716.29, average training loss: 5763.70, base loss: 20513.33
[INFO 2017-06-26 12:56:43,976 main.py:50] epoch 4936, training loss: 5703.60, average training loss: 5763.56, base loss: 20513.45
[INFO 2017-06-26 12:56:44,336 main.py:50] epoch 4937, training loss: 5709.65, average training loss: 5763.40, base loss: 20513.48
[INFO 2017-06-26 12:56:44,696 main.py:50] epoch 4938, training loss: 5699.00, average training loss: 5763.32, base loss: 20513.25
[INFO 2017-06-26 12:56:45,057 main.py:50] epoch 4939, training loss: 5666.28, average training loss: 5763.19, base loss: 20513.49
[INFO 2017-06-26 12:56:45,416 main.py:50] epoch 4940, training loss: 5646.71, average training loss: 5763.04, base loss: 20513.21
[INFO 2017-06-26 12:56:45,777 main.py:50] epoch 4941, training loss: 5674.31, average training loss: 5762.89, base loss: 20513.43
[INFO 2017-06-26 12:56:46,138 main.py:50] epoch 4942, training loss: 5690.72, average training loss: 5762.78, base loss: 20513.53
[INFO 2017-06-26 12:56:46,499 main.py:50] epoch 4943, training loss: 5613.94, average training loss: 5762.58, base loss: 20513.49
[INFO 2017-06-26 12:56:46,858 main.py:50] epoch 4944, training loss: 5652.67, average training loss: 5762.38, base loss: 20513.61
[INFO 2017-06-26 12:56:47,219 main.py:50] epoch 4945, training loss: 5639.30, average training loss: 5762.20, base loss: 20513.75
[INFO 2017-06-26 12:56:47,580 main.py:50] epoch 4946, training loss: 5694.02, average training loss: 5762.04, base loss: 20514.16
[INFO 2017-06-26 12:56:47,940 main.py:50] epoch 4947, training loss: 5669.12, average training loss: 5761.87, base loss: 20514.23
[INFO 2017-06-26 12:56:48,301 main.py:50] epoch 4948, training loss: 5709.72, average training loss: 5761.70, base loss: 20514.11
[INFO 2017-06-26 12:56:48,662 main.py:50] epoch 4949, training loss: 5728.75, average training loss: 5761.64, base loss: 20514.61
[INFO 2017-06-26 12:56:49,022 main.py:50] epoch 4950, training loss: 5619.55, average training loss: 5761.39, base loss: 20514.32
[INFO 2017-06-26 12:56:49,406 main.py:50] epoch 4951, training loss: 5641.77, average training loss: 5761.22, base loss: 20514.31
[INFO 2017-06-26 12:56:49,786 main.py:50] epoch 4952, training loss: 5762.82, average training loss: 5761.20, base loss: 20514.42
[INFO 2017-06-26 12:56:50,148 main.py:50] epoch 4953, training loss: 5726.02, average training loss: 5761.13, base loss: 20514.67
[INFO 2017-06-26 12:56:50,508 main.py:50] epoch 4954, training loss: 5650.38, average training loss: 5760.93, base loss: 20514.31
[INFO 2017-06-26 12:56:50,869 main.py:50] epoch 4955, training loss: 5751.76, average training loss: 5760.89, base loss: 20514.05
[INFO 2017-06-26 12:56:51,233 main.py:50] epoch 4956, training loss: 5703.25, average training loss: 5760.79, base loss: 20514.02
[INFO 2017-06-26 12:56:51,595 main.py:50] epoch 4957, training loss: 5705.99, average training loss: 5760.72, base loss: 20514.08
[INFO 2017-06-26 12:56:51,955 main.py:50] epoch 4958, training loss: 5714.37, average training loss: 5760.65, base loss: 20513.84
[INFO 2017-06-26 12:56:52,318 main.py:50] epoch 4959, training loss: 5730.35, average training loss: 5760.55, base loss: 20513.86
[INFO 2017-06-26 12:56:52,679 main.py:50] epoch 4960, training loss: 5743.42, average training loss: 5760.55, base loss: 20513.96
[INFO 2017-06-26 12:56:53,040 main.py:50] epoch 4961, training loss: 5677.33, average training loss: 5760.49, base loss: 20513.61
[INFO 2017-06-26 12:56:53,400 main.py:50] epoch 4962, training loss: 5662.03, average training loss: 5760.32, base loss: 20513.25
[INFO 2017-06-26 12:56:53,762 main.py:50] epoch 4963, training loss: 5582.62, average training loss: 5760.02, base loss: 20512.50
[INFO 2017-06-26 12:56:54,123 main.py:50] epoch 4964, training loss: 5670.29, average training loss: 5759.89, base loss: 20512.38
[INFO 2017-06-26 12:56:54,484 main.py:50] epoch 4965, training loss: 5641.95, average training loss: 5759.70, base loss: 20512.34
[INFO 2017-06-26 12:56:54,844 main.py:50] epoch 4966, training loss: 5670.75, average training loss: 5759.54, base loss: 20512.43
[INFO 2017-06-26 12:56:55,205 main.py:50] epoch 4967, training loss: 5584.78, average training loss: 5759.32, base loss: 20512.28
[INFO 2017-06-26 12:56:55,567 main.py:50] epoch 4968, training loss: 5583.42, average training loss: 5759.09, base loss: 20511.92
[INFO 2017-06-26 12:56:55,928 main.py:50] epoch 4969, training loss: 5763.00, average training loss: 5759.03, base loss: 20512.02
[INFO 2017-06-26 12:56:56,291 main.py:50] epoch 4970, training loss: 5738.92, average training loss: 5758.97, base loss: 20511.93
[INFO 2017-06-26 12:56:56,652 main.py:50] epoch 4971, training loss: 5699.59, average training loss: 5758.86, base loss: 20512.24
[INFO 2017-06-26 12:56:57,013 main.py:50] epoch 4972, training loss: 5665.46, average training loss: 5758.70, base loss: 20512.49
[INFO 2017-06-26 12:56:57,375 main.py:50] epoch 4973, training loss: 5625.46, average training loss: 5758.53, base loss: 20512.12
[INFO 2017-06-26 12:56:57,737 main.py:50] epoch 4974, training loss: 5712.64, average training loss: 5758.45, base loss: 20512.43
[INFO 2017-06-26 12:56:58,099 main.py:50] epoch 4975, training loss: 5724.95, average training loss: 5758.35, base loss: 20512.34
[INFO 2017-06-26 12:56:58,460 main.py:50] epoch 4976, training loss: 5637.73, average training loss: 5758.14, base loss: 20512.28
[INFO 2017-06-26 12:56:58,821 main.py:50] epoch 4977, training loss: 5718.08, average training loss: 5758.02, base loss: 20512.32
[INFO 2017-06-26 12:56:59,183 main.py:50] epoch 4978, training loss: 5650.34, average training loss: 5757.89, base loss: 20512.08
[INFO 2017-06-26 12:56:59,544 main.py:50] epoch 4979, training loss: 5688.44, average training loss: 5757.73, base loss: 20511.90
[INFO 2017-06-26 12:56:59,907 main.py:50] epoch 4980, training loss: 5695.68, average training loss: 5757.60, base loss: 20512.13
[INFO 2017-06-26 12:57:00,269 main.py:50] epoch 4981, training loss: 5669.95, average training loss: 5757.38, base loss: 20511.77
[INFO 2017-06-26 12:57:00,631 main.py:50] epoch 4982, training loss: 5713.92, average training loss: 5757.23, base loss: 20511.64
[INFO 2017-06-26 12:57:00,994 main.py:50] epoch 4983, training loss: 5675.18, average training loss: 5757.08, base loss: 20511.41
[INFO 2017-06-26 12:57:01,354 main.py:50] epoch 4984, training loss: 5702.45, average training loss: 5757.01, base loss: 20511.13
[INFO 2017-06-26 12:57:01,716 main.py:50] epoch 4985, training loss: 5709.13, average training loss: 5756.88, base loss: 20510.89
[INFO 2017-06-26 12:57:02,076 main.py:50] epoch 4986, training loss: 5714.93, average training loss: 5756.79, base loss: 20510.87
[INFO 2017-06-26 12:57:02,439 main.py:50] epoch 4987, training loss: 5684.27, average training loss: 5756.59, base loss: 20510.85
[INFO 2017-06-26 12:57:02,800 main.py:50] epoch 4988, training loss: 5741.79, average training loss: 5756.50, base loss: 20511.10
[INFO 2017-06-26 12:57:03,162 main.py:50] epoch 4989, training loss: 5727.97, average training loss: 5756.40, base loss: 20510.82
[INFO 2017-06-26 12:57:03,524 main.py:50] epoch 4990, training loss: 5696.49, average training loss: 5756.32, base loss: 20510.90
[INFO 2017-06-26 12:57:03,886 main.py:50] epoch 4991, training loss: 5723.32, average training loss: 5756.26, base loss: 20511.26
[INFO 2017-06-26 12:57:04,248 main.py:50] epoch 4992, training loss: 5694.26, average training loss: 5756.07, base loss: 20511.38
[INFO 2017-06-26 12:57:04,609 main.py:50] epoch 4993, training loss: 5662.60, average training loss: 5755.99, base loss: 20511.55
[INFO 2017-06-26 12:57:04,969 main.py:50] epoch 4994, training loss: 5673.44, average training loss: 5755.79, base loss: 20511.73
[INFO 2017-06-26 12:57:05,330 main.py:50] epoch 4995, training loss: 5708.29, average training loss: 5755.63, base loss: 20511.70
[INFO 2017-06-26 12:57:05,691 main.py:50] epoch 4996, training loss: 5698.74, average training loss: 5755.49, base loss: 20511.52
[INFO 2017-06-26 12:57:06,052 main.py:50] epoch 4997, training loss: 5761.35, average training loss: 5755.36, base loss: 20511.58
[INFO 2017-06-26 12:57:06,414 main.py:50] epoch 4998, training loss: 5701.57, average training loss: 5755.22, base loss: 20511.46
[INFO 2017-06-26 12:57:06,775 main.py:50] epoch 4999, training loss: 5749.78, average training loss: 5755.16, base loss: 20511.89
[INFO 2017-06-26 12:57:06,775 main.py:52] epoch 4999, testing
[INFO 2017-06-26 12:57:08,255 main.py:103] average testing loss: 5702.74, base loss: 20577.18
[INFO 2017-06-26 12:57:08,256 main.py:76] current best accuracy: 5666.22
[INFO 2017-06-26 12:57:08,615 main.py:50] epoch 5000, training loss: 5686.88, average training loss: 5754.97, base loss: 20511.81
[INFO 2017-06-26 12:57:08,974 main.py:50] epoch 5001, training loss: 5698.13, average training loss: 5754.87, base loss: 20512.00
[INFO 2017-06-26 12:57:09,333 main.py:50] epoch 5002, training loss: 5683.77, average training loss: 5754.75, base loss: 20512.07
[INFO 2017-06-26 12:57:09,693 main.py:50] epoch 5003, training loss: 5692.97, average training loss: 5754.64, base loss: 20512.22
[INFO 2017-06-26 12:57:10,053 main.py:50] epoch 5004, training loss: 5646.10, average training loss: 5754.51, base loss: 20512.42
[INFO 2017-06-26 12:57:10,414 main.py:50] epoch 5005, training loss: 5670.37, average training loss: 5754.42, base loss: 20512.62
[INFO 2017-06-26 12:57:10,774 main.py:50] epoch 5006, training loss: 5704.44, average training loss: 5754.30, base loss: 20512.02
[INFO 2017-06-26 12:57:11,133 main.py:50] epoch 5007, training loss: 5653.57, average training loss: 5754.12, base loss: 20512.26
[INFO 2017-06-26 12:57:11,493 main.py:50] epoch 5008, training loss: 5669.90, average training loss: 5754.07, base loss: 20512.56
[INFO 2017-06-26 12:57:11,853 main.py:50] epoch 5009, training loss: 5697.30, average training loss: 5753.98, base loss: 20512.54
[INFO 2017-06-26 12:57:12,213 main.py:50] epoch 5010, training loss: 5681.30, average training loss: 5753.83, base loss: 20512.62
[INFO 2017-06-26 12:57:12,573 main.py:50] epoch 5011, training loss: 5687.08, average training loss: 5753.72, base loss: 20512.56
[INFO 2017-06-26 12:57:12,934 main.py:50] epoch 5012, training loss: 5706.23, average training loss: 5753.66, base loss: 20512.89
[INFO 2017-06-26 12:57:13,293 main.py:50] epoch 5013, training loss: 5736.07, average training loss: 5753.61, base loss: 20513.12
[INFO 2017-06-26 12:57:13,652 main.py:50] epoch 5014, training loss: 5712.71, average training loss: 5753.54, base loss: 20512.84
[INFO 2017-06-26 12:57:14,012 main.py:50] epoch 5015, training loss: 5630.95, average training loss: 5753.37, base loss: 20512.77
[INFO 2017-06-26 12:57:14,373 main.py:50] epoch 5016, training loss: 5629.43, average training loss: 5753.24, base loss: 20512.76
[INFO 2017-06-26 12:57:14,733 main.py:50] epoch 5017, training loss: 5662.67, average training loss: 5753.06, base loss: 20512.77
[INFO 2017-06-26 12:57:15,091 main.py:50] epoch 5018, training loss: 5701.06, average training loss: 5752.87, base loss: 20512.63
[INFO 2017-06-26 12:57:15,451 main.py:50] epoch 5019, training loss: 5648.65, average training loss: 5752.63, base loss: 20512.68
[INFO 2017-06-26 12:57:15,811 main.py:50] epoch 5020, training loss: 5723.49, average training loss: 5752.53, base loss: 20512.55
[INFO 2017-06-26 12:57:16,172 main.py:50] epoch 5021, training loss: 5690.68, average training loss: 5752.37, base loss: 20512.95
[INFO 2017-06-26 12:57:16,532 main.py:50] epoch 5022, training loss: 5695.50, average training loss: 5752.27, base loss: 20513.00
[INFO 2017-06-26 12:57:16,891 main.py:50] epoch 5023, training loss: 5645.70, average training loss: 5752.11, base loss: 20512.94
[INFO 2017-06-26 12:57:17,252 main.py:50] epoch 5024, training loss: 5709.76, average training loss: 5751.98, base loss: 20512.67
[INFO 2017-06-26 12:57:17,611 main.py:50] epoch 5025, training loss: 5716.06, average training loss: 5751.82, base loss: 20512.70
[INFO 2017-06-26 12:57:17,972 main.py:50] epoch 5026, training loss: 5722.39, average training loss: 5751.71, base loss: 20512.37
[INFO 2017-06-26 12:57:18,331 main.py:50] epoch 5027, training loss: 5630.82, average training loss: 5751.50, base loss: 20512.05
[INFO 2017-06-26 12:57:18,688 main.py:50] epoch 5028, training loss: 5676.75, average training loss: 5751.31, base loss: 20512.00
[INFO 2017-06-26 12:57:19,047 main.py:50] epoch 5029, training loss: 5659.68, average training loss: 5751.12, base loss: 20512.49
[INFO 2017-06-26 12:57:19,406 main.py:50] epoch 5030, training loss: 5679.66, average training loss: 5751.05, base loss: 20512.31
[INFO 2017-06-26 12:57:19,764 main.py:50] epoch 5031, training loss: 5727.54, average training loss: 5751.00, base loss: 20512.50
[INFO 2017-06-26 12:57:20,123 main.py:50] epoch 5032, training loss: 5704.35, average training loss: 5750.92, base loss: 20512.61
[INFO 2017-06-26 12:57:20,482 main.py:50] epoch 5033, training loss: 5719.35, average training loss: 5750.89, base loss: 20512.56
[INFO 2017-06-26 12:57:20,841 main.py:50] epoch 5034, training loss: 5697.43, average training loss: 5750.69, base loss: 20512.34
[INFO 2017-06-26 12:57:21,199 main.py:50] epoch 5035, training loss: 5693.94, average training loss: 5750.62, base loss: 20512.21
[INFO 2017-06-26 12:57:21,558 main.py:50] epoch 5036, training loss: 5654.48, average training loss: 5750.40, base loss: 20511.88
[INFO 2017-06-26 12:57:21,918 main.py:50] epoch 5037, training loss: 5657.33, average training loss: 5750.26, base loss: 20511.53
[INFO 2017-06-26 12:57:22,277 main.py:50] epoch 5038, training loss: 5726.21, average training loss: 5750.11, base loss: 20511.59
[INFO 2017-06-26 12:57:22,634 main.py:50] epoch 5039, training loss: 5820.22, average training loss: 5750.13, base loss: 20512.09
[INFO 2017-06-26 12:57:22,993 main.py:50] epoch 5040, training loss: 5755.56, average training loss: 5750.04, base loss: 20512.19
[INFO 2017-06-26 12:57:23,352 main.py:50] epoch 5041, training loss: 5736.79, average training loss: 5750.02, base loss: 20512.21
[INFO 2017-06-26 12:57:23,711 main.py:50] epoch 5042, training loss: 5768.15, average training loss: 5749.98, base loss: 20512.18
[INFO 2017-06-26 12:57:24,069 main.py:50] epoch 5043, training loss: 5703.25, average training loss: 5749.83, base loss: 20512.03
[INFO 2017-06-26 12:57:24,428 main.py:50] epoch 5044, training loss: 5769.71, average training loss: 5749.68, base loss: 20512.14
[INFO 2017-06-26 12:57:24,787 main.py:50] epoch 5045, training loss: 5789.85, average training loss: 5749.64, base loss: 20512.23
[INFO 2017-06-26 12:57:25,145 main.py:50] epoch 5046, training loss: 5684.59, average training loss: 5749.43, base loss: 20512.15
[INFO 2017-06-26 12:57:25,504 main.py:50] epoch 5047, training loss: 5668.54, average training loss: 5749.28, base loss: 20512.22
[INFO 2017-06-26 12:57:25,862 main.py:50] epoch 5048, training loss: 5748.63, average training loss: 5749.14, base loss: 20512.45
[INFO 2017-06-26 12:57:26,220 main.py:50] epoch 5049, training loss: 5746.72, average training loss: 5749.08, base loss: 20512.42
[INFO 2017-06-26 12:57:26,579 main.py:50] epoch 5050, training loss: 5699.04, average training loss: 5748.91, base loss: 20512.34
[INFO 2017-06-26 12:57:26,937 main.py:50] epoch 5051, training loss: 5663.49, average training loss: 5748.76, base loss: 20512.53
[INFO 2017-06-26 12:57:27,297 main.py:50] epoch 5052, training loss: 5695.22, average training loss: 5748.56, base loss: 20512.67
[INFO 2017-06-26 12:57:27,655 main.py:50] epoch 5053, training loss: 5691.63, average training loss: 5748.33, base loss: 20512.16
[INFO 2017-06-26 12:57:28,013 main.py:50] epoch 5054, training loss: 5708.39, average training loss: 5748.16, base loss: 20512.36
[INFO 2017-06-26 12:57:28,372 main.py:50] epoch 5055, training loss: 5683.34, average training loss: 5747.95, base loss: 20512.46
[INFO 2017-06-26 12:57:28,730 main.py:50] epoch 5056, training loss: 5768.16, average training loss: 5747.92, base loss: 20513.24
[INFO 2017-06-26 12:57:29,088 main.py:50] epoch 5057, training loss: 5707.65, average training loss: 5747.69, base loss: 20513.03
[INFO 2017-06-26 12:57:29,447 main.py:50] epoch 5058, training loss: 5647.86, average training loss: 5747.42, base loss: 20513.53
[INFO 2017-06-26 12:57:29,806 main.py:50] epoch 5059, training loss: 5742.18, average training loss: 5747.22, base loss: 20513.51
[INFO 2017-06-26 12:57:30,166 main.py:50] epoch 5060, training loss: 5679.30, average training loss: 5747.06, base loss: 20513.62
[INFO 2017-06-26 12:57:30,523 main.py:50] epoch 5061, training loss: 5716.67, average training loss: 5746.82, base loss: 20513.25
[INFO 2017-06-26 12:57:30,882 main.py:50] epoch 5062, training loss: 5628.29, average training loss: 5746.57, base loss: 20512.99
[INFO 2017-06-26 12:57:31,240 main.py:50] epoch 5063, training loss: 5729.76, average training loss: 5746.40, base loss: 20513.11
[INFO 2017-06-26 12:57:31,598 main.py:50] epoch 5064, training loss: 5643.79, average training loss: 5746.19, base loss: 20512.60
[INFO 2017-06-26 12:57:31,956 main.py:50] epoch 5065, training loss: 5614.96, average training loss: 5745.94, base loss: 20512.58
[INFO 2017-06-26 12:57:32,315 main.py:50] epoch 5066, training loss: 5726.75, average training loss: 5745.75, base loss: 20512.40
[INFO 2017-06-26 12:57:32,673 main.py:50] epoch 5067, training loss: 5692.25, average training loss: 5745.62, base loss: 20512.33
[INFO 2017-06-26 12:57:33,032 main.py:50] epoch 5068, training loss: 5711.60, average training loss: 5745.41, base loss: 20511.92
[INFO 2017-06-26 12:57:33,392 main.py:50] epoch 5069, training loss: 5674.63, average training loss: 5745.28, base loss: 20511.67
[INFO 2017-06-26 12:57:33,752 main.py:50] epoch 5070, training loss: 5612.46, average training loss: 5745.06, base loss: 20511.68
[INFO 2017-06-26 12:57:34,110 main.py:50] epoch 5071, training loss: 5664.21, average training loss: 5744.90, base loss: 20511.60
[INFO 2017-06-26 12:57:34,482 main.py:50] epoch 5072, training loss: 5708.83, average training loss: 5744.78, base loss: 20511.80
[INFO 2017-06-26 12:57:34,839 main.py:50] epoch 5073, training loss: 5716.68, average training loss: 5744.69, base loss: 20512.11
[INFO 2017-06-26 12:57:35,197 main.py:50] epoch 5074, training loss: 5651.84, average training loss: 5744.52, base loss: 20512.05
[INFO 2017-06-26 12:57:35,557 main.py:50] epoch 5075, training loss: 5661.77, average training loss: 5744.35, base loss: 20512.09
[INFO 2017-06-26 12:57:35,915 main.py:50] epoch 5076, training loss: 5761.29, average training loss: 5744.33, base loss: 20512.18
[INFO 2017-06-26 12:57:36,273 main.py:50] epoch 5077, training loss: 5686.54, average training loss: 5744.19, base loss: 20512.12
[INFO 2017-06-26 12:57:36,632 main.py:50] epoch 5078, training loss: 5771.98, average training loss: 5744.18, base loss: 20512.04
[INFO 2017-06-26 12:57:36,991 main.py:50] epoch 5079, training loss: 5709.29, average training loss: 5744.07, base loss: 20511.94
[INFO 2017-06-26 12:57:37,350 main.py:50] epoch 5080, training loss: 5663.53, average training loss: 5743.87, base loss: 20511.93
[INFO 2017-06-26 12:57:37,708 main.py:50] epoch 5081, training loss: 5652.70, average training loss: 5743.65, base loss: 20511.95
[INFO 2017-06-26 12:57:38,068 main.py:50] epoch 5082, training loss: 5610.50, average training loss: 5743.43, base loss: 20511.44
[INFO 2017-06-26 12:57:38,427 main.py:50] epoch 5083, training loss: 5647.20, average training loss: 5743.31, base loss: 20511.27
[INFO 2017-06-26 12:57:38,784 main.py:50] epoch 5084, training loss: 5702.63, average training loss: 5743.24, base loss: 20511.51
[INFO 2017-06-26 12:57:39,143 main.py:50] epoch 5085, training loss: 5671.00, average training loss: 5743.11, base loss: 20511.86
[INFO 2017-06-26 12:57:39,502 main.py:50] epoch 5086, training loss: 5674.06, average training loss: 5743.01, base loss: 20511.91
[INFO 2017-06-26 12:57:39,862 main.py:50] epoch 5087, training loss: 5702.66, average training loss: 5742.84, base loss: 20511.70
[INFO 2017-06-26 12:57:40,220 main.py:50] epoch 5088, training loss: 5728.31, average training loss: 5742.75, base loss: 20511.43
[INFO 2017-06-26 12:57:40,580 main.py:50] epoch 5089, training loss: 5674.36, average training loss: 5742.58, base loss: 20511.41
[INFO 2017-06-26 12:57:40,939 main.py:50] epoch 5090, training loss: 5669.43, average training loss: 5742.43, base loss: 20511.26
[INFO 2017-06-26 12:57:41,298 main.py:50] epoch 5091, training loss: 5684.89, average training loss: 5742.28, base loss: 20511.00
[INFO 2017-06-26 12:57:41,657 main.py:50] epoch 5092, training loss: 5714.64, average training loss: 5742.14, base loss: 20511.39
[INFO 2017-06-26 12:57:42,016 main.py:50] epoch 5093, training loss: 5691.87, average training loss: 5741.96, base loss: 20511.02
[INFO 2017-06-26 12:57:42,375 main.py:50] epoch 5094, training loss: 5797.24, average training loss: 5741.84, base loss: 20511.18
[INFO 2017-06-26 12:57:42,733 main.py:50] epoch 5095, training loss: 5744.64, average training loss: 5741.75, base loss: 20511.40
[INFO 2017-06-26 12:57:43,091 main.py:50] epoch 5096, training loss: 5776.25, average training loss: 5741.67, base loss: 20511.35
[INFO 2017-06-26 12:57:43,450 main.py:50] epoch 5097, training loss: 5638.02, average training loss: 5741.44, base loss: 20511.04
[INFO 2017-06-26 12:57:43,808 main.py:50] epoch 5098, training loss: 5750.41, average training loss: 5741.39, base loss: 20511.23
[INFO 2017-06-26 12:57:44,167 main.py:50] epoch 5099, training loss: 5664.94, average training loss: 5741.21, base loss: 20511.30
[INFO 2017-06-26 12:57:44,168 main.py:52] epoch 5099, testing
[INFO 2017-06-26 12:57:45,631 main.py:103] average testing loss: 5705.56, base loss: 20559.99
[INFO 2017-06-26 12:57:45,631 main.py:76] current best accuracy: 5666.22
[INFO 2017-06-26 12:57:45,990 main.py:50] epoch 5100, training loss: 5688.79, average training loss: 5741.06, base loss: 20510.93
[INFO 2017-06-26 12:57:46,348 main.py:50] epoch 5101, training loss: 5739.64, average training loss: 5740.99, base loss: 20511.00
[INFO 2017-06-26 12:57:46,708 main.py:50] epoch 5102, training loss: 5644.62, average training loss: 5740.90, base loss: 20510.98
[INFO 2017-06-26 12:57:47,067 main.py:50] epoch 5103, training loss: 5761.13, average training loss: 5740.84, base loss: 20511.19
[INFO 2017-06-26 12:57:47,425 main.py:50] epoch 5104, training loss: 5690.93, average training loss: 5740.75, base loss: 20511.18
[INFO 2017-06-26 12:57:47,784 main.py:50] epoch 5105, training loss: 5640.62, average training loss: 5740.53, base loss: 20511.16
[INFO 2017-06-26 12:57:48,144 main.py:50] epoch 5106, training loss: 5693.78, average training loss: 5740.44, base loss: 20511.34
[INFO 2017-06-26 12:57:48,503 main.py:50] epoch 5107, training loss: 5680.08, average training loss: 5740.23, base loss: 20511.15
[INFO 2017-06-26 12:57:48,863 main.py:50] epoch 5108, training loss: 5666.44, average training loss: 5740.07, base loss: 20511.03
[INFO 2017-06-26 12:57:49,223 main.py:50] epoch 5109, training loss: 5719.54, average training loss: 5739.96, base loss: 20511.45
[INFO 2017-06-26 12:57:49,582 main.py:50] epoch 5110, training loss: 5682.49, average training loss: 5739.88, base loss: 20511.63
[INFO 2017-06-26 12:57:49,941 main.py:50] epoch 5111, training loss: 5707.26, average training loss: 5739.71, base loss: 20511.49
[INFO 2017-06-26 12:57:50,302 main.py:50] epoch 5112, training loss: 5746.45, average training loss: 5739.59, base loss: 20511.30
[INFO 2017-06-26 12:57:50,660 main.py:50] epoch 5113, training loss: 5677.86, average training loss: 5739.41, base loss: 20511.66
[INFO 2017-06-26 12:57:51,021 main.py:50] epoch 5114, training loss: 5733.33, average training loss: 5739.23, base loss: 20511.67
[INFO 2017-06-26 12:57:51,380 main.py:50] epoch 5115, training loss: 5653.60, average training loss: 5739.00, base loss: 20511.14
[INFO 2017-06-26 12:57:51,739 main.py:50] epoch 5116, training loss: 5779.84, average training loss: 5738.84, base loss: 20511.30
[INFO 2017-06-26 12:57:52,099 main.py:50] epoch 5117, training loss: 5723.12, average training loss: 5738.77, base loss: 20511.43
[INFO 2017-06-26 12:57:52,459 main.py:50] epoch 5118, training loss: 5750.56, average training loss: 5738.52, base loss: 20511.50
[INFO 2017-06-26 12:57:52,819 main.py:50] epoch 5119, training loss: 5802.86, average training loss: 5738.49, base loss: 20511.72
[INFO 2017-06-26 12:57:53,179 main.py:50] epoch 5120, training loss: 5769.41, average training loss: 5738.36, base loss: 20511.99
[INFO 2017-06-26 12:57:53,539 main.py:50] epoch 5121, training loss: 5705.70, average training loss: 5738.10, base loss: 20511.78
[INFO 2017-06-26 12:57:53,899 main.py:50] epoch 5122, training loss: 5637.58, average training loss: 5737.97, base loss: 20512.04
[INFO 2017-06-26 12:57:54,259 main.py:50] epoch 5123, training loss: 5749.12, average training loss: 5737.82, base loss: 20512.23
[INFO 2017-06-26 12:57:54,618 main.py:50] epoch 5124, training loss: 5705.33, average training loss: 5737.69, base loss: 20511.90
[INFO 2017-06-26 12:57:54,978 main.py:50] epoch 5125, training loss: 5722.09, average training loss: 5737.53, base loss: 20512.01
[INFO 2017-06-26 12:57:55,338 main.py:50] epoch 5126, training loss: 5713.47, average training loss: 5737.44, base loss: 20512.00
[INFO 2017-06-26 12:57:55,698 main.py:50] epoch 5127, training loss: 5680.43, average training loss: 5737.36, base loss: 20512.18
[INFO 2017-06-26 12:57:56,057 main.py:50] epoch 5128, training loss: 5673.35, average training loss: 5737.22, base loss: 20511.67
[INFO 2017-06-26 12:57:56,419 main.py:50] epoch 5129, training loss: 5749.90, average training loss: 5737.15, base loss: 20511.67
[INFO 2017-06-26 12:57:56,778 main.py:50] epoch 5130, training loss: 5665.16, average training loss: 5737.02, base loss: 20511.36
[INFO 2017-06-26 12:57:57,137 main.py:50] epoch 5131, training loss: 5653.47, average training loss: 5736.93, base loss: 20511.33
[INFO 2017-06-26 12:57:57,497 main.py:50] epoch 5132, training loss: 5677.16, average training loss: 5736.79, base loss: 20511.24
[INFO 2017-06-26 12:57:57,857 main.py:50] epoch 5133, training loss: 5708.42, average training loss: 5736.69, base loss: 20511.34
[INFO 2017-06-26 12:57:58,217 main.py:50] epoch 5134, training loss: 5676.86, average training loss: 5736.56, base loss: 20511.18
[INFO 2017-06-26 12:57:58,576 main.py:50] epoch 5135, training loss: 5733.66, average training loss: 5736.53, base loss: 20511.49
[INFO 2017-06-26 12:57:58,935 main.py:50] epoch 5136, training loss: 5757.22, average training loss: 5736.52, base loss: 20511.94
[INFO 2017-06-26 12:57:59,295 main.py:50] epoch 5137, training loss: 5646.35, average training loss: 5736.34, base loss: 20511.96
[INFO 2017-06-26 12:57:59,655 main.py:50] epoch 5138, training loss: 5672.92, average training loss: 5736.24, base loss: 20511.81
[INFO 2017-06-26 12:58:00,016 main.py:50] epoch 5139, training loss: 5727.87, average training loss: 5736.17, base loss: 20511.66
[INFO 2017-06-26 12:58:00,375 main.py:50] epoch 5140, training loss: 5788.02, average training loss: 5736.18, base loss: 20511.80
[INFO 2017-06-26 12:58:00,735 main.py:50] epoch 5141, training loss: 5682.20, average training loss: 5736.06, base loss: 20511.78
[INFO 2017-06-26 12:58:01,095 main.py:50] epoch 5142, training loss: 5673.24, average training loss: 5735.97, base loss: 20511.93
[INFO 2017-06-26 12:58:01,456 main.py:50] epoch 5143, training loss: 5764.29, average training loss: 5735.95, base loss: 20511.81
[INFO 2017-06-26 12:58:01,816 main.py:50] epoch 5144, training loss: 5721.95, average training loss: 5735.91, base loss: 20512.05
[INFO 2017-06-26 12:58:02,175 main.py:50] epoch 5145, training loss: 5707.71, average training loss: 5735.84, base loss: 20511.85
[INFO 2017-06-26 12:58:02,535 main.py:50] epoch 5146, training loss: 5629.38, average training loss: 5735.68, base loss: 20511.78
[INFO 2017-06-26 12:58:02,895 main.py:50] epoch 5147, training loss: 5723.10, average training loss: 5735.60, base loss: 20511.61
[INFO 2017-06-26 12:58:03,255 main.py:50] epoch 5148, training loss: 5702.80, average training loss: 5735.50, base loss: 20511.28
[INFO 2017-06-26 12:58:03,616 main.py:50] epoch 5149, training loss: 5715.29, average training loss: 5735.42, base loss: 20510.88
[INFO 2017-06-26 12:58:03,975 main.py:50] epoch 5150, training loss: 5737.44, average training loss: 5735.27, base loss: 20511.24
[INFO 2017-06-26 12:58:04,336 main.py:50] epoch 5151, training loss: 5765.07, average training loss: 5735.17, base loss: 20511.06
[INFO 2017-06-26 12:58:04,697 main.py:50] epoch 5152, training loss: 5749.92, average training loss: 5735.13, base loss: 20510.79
[INFO 2017-06-26 12:58:05,057 main.py:50] epoch 5153, training loss: 5736.01, average training loss: 5735.02, base loss: 20511.12
[INFO 2017-06-26 12:58:05,417 main.py:50] epoch 5154, training loss: 5633.51, average training loss: 5734.93, base loss: 20511.26
[INFO 2017-06-26 12:58:05,777 main.py:50] epoch 5155, training loss: 5804.03, average training loss: 5734.94, base loss: 20511.18
[INFO 2017-06-26 12:58:06,136 main.py:50] epoch 5156, training loss: 5678.04, average training loss: 5734.85, base loss: 20511.13
[INFO 2017-06-26 12:58:06,497 main.py:50] epoch 5157, training loss: 5727.66, average training loss: 5734.80, base loss: 20510.73
[INFO 2017-06-26 12:58:06,857 main.py:50] epoch 5158, training loss: 5680.24, average training loss: 5734.71, base loss: 20510.42
[INFO 2017-06-26 12:58:07,217 main.py:50] epoch 5159, training loss: 5689.89, average training loss: 5734.62, base loss: 20510.40
[INFO 2017-06-26 12:58:07,578 main.py:50] epoch 5160, training loss: 5677.85, average training loss: 5734.51, base loss: 20510.41
[INFO 2017-06-26 12:58:07,938 main.py:50] epoch 5161, training loss: 5631.34, average training loss: 5734.30, base loss: 20510.04
[INFO 2017-06-26 12:58:08,298 main.py:50] epoch 5162, training loss: 5618.23, average training loss: 5734.11, base loss: 20509.91
[INFO 2017-06-26 12:58:08,658 main.py:50] epoch 5163, training loss: 5725.19, average training loss: 5733.98, base loss: 20509.94
[INFO 2017-06-26 12:58:09,017 main.py:50] epoch 5164, training loss: 5695.90, average training loss: 5733.85, base loss: 20509.85
[INFO 2017-06-26 12:58:09,377 main.py:50] epoch 5165, training loss: 5689.76, average training loss: 5733.74, base loss: 20509.40
[INFO 2017-06-26 12:58:09,738 main.py:50] epoch 5166, training loss: 5676.02, average training loss: 5733.56, base loss: 20508.98
[INFO 2017-06-26 12:58:10,097 main.py:50] epoch 5167, training loss: 5679.12, average training loss: 5733.43, base loss: 20508.39
[INFO 2017-06-26 12:58:10,458 main.py:50] epoch 5168, training loss: 5643.97, average training loss: 5733.28, base loss: 20508.20
[INFO 2017-06-26 12:58:10,818 main.py:50] epoch 5169, training loss: 5654.41, average training loss: 5733.13, base loss: 20508.28
[INFO 2017-06-26 12:58:11,178 main.py:50] epoch 5170, training loss: 5660.13, average training loss: 5732.96, base loss: 20508.17
[INFO 2017-06-26 12:58:11,539 main.py:50] epoch 5171, training loss: 5755.69, average training loss: 5732.90, base loss: 20508.43
[INFO 2017-06-26 12:58:11,899 main.py:50] epoch 5172, training loss: 5700.56, average training loss: 5732.83, base loss: 20508.31
[INFO 2017-06-26 12:58:12,259 main.py:50] epoch 5173, training loss: 5687.99, average training loss: 5732.70, base loss: 20507.90
[INFO 2017-06-26 12:58:12,620 main.py:50] epoch 5174, training loss: 5758.84, average training loss: 5732.67, base loss: 20508.00
[INFO 2017-06-26 12:58:12,980 main.py:50] epoch 5175, training loss: 5653.40, average training loss: 5732.51, base loss: 20507.92
[INFO 2017-06-26 12:58:13,340 main.py:50] epoch 5176, training loss: 5655.60, average training loss: 5732.35, base loss: 20507.56
[INFO 2017-06-26 12:58:13,701 main.py:50] epoch 5177, training loss: 5612.27, average training loss: 5732.10, base loss: 20507.23
[INFO 2017-06-26 12:58:14,061 main.py:50] epoch 5178, training loss: 5679.88, average training loss: 5732.06, base loss: 20507.15
[INFO 2017-06-26 12:58:14,421 main.py:50] epoch 5179, training loss: 5639.30, average training loss: 5731.98, base loss: 20507.42
[INFO 2017-06-26 12:58:14,780 main.py:50] epoch 5180, training loss: 5567.51, average training loss: 5731.75, base loss: 20507.02
[INFO 2017-06-26 12:58:15,140 main.py:50] epoch 5181, training loss: 5583.22, average training loss: 5731.52, base loss: 20506.97
[INFO 2017-06-26 12:58:15,501 main.py:50] epoch 5182, training loss: 5638.87, average training loss: 5731.37, base loss: 20507.00
[INFO 2017-06-26 12:58:15,861 main.py:50] epoch 5183, training loss: 5760.01, average training loss: 5731.34, base loss: 20506.92
[INFO 2017-06-26 12:58:16,220 main.py:50] epoch 5184, training loss: 5607.35, average training loss: 5731.15, base loss: 20506.62
[INFO 2017-06-26 12:58:16,582 main.py:50] epoch 5185, training loss: 5723.43, average training loss: 5731.09, base loss: 20506.74
[INFO 2017-06-26 12:58:16,942 main.py:50] epoch 5186, training loss: 5675.89, average training loss: 5731.04, base loss: 20507.00
[INFO 2017-06-26 12:58:17,302 main.py:50] epoch 5187, training loss: 5668.16, average training loss: 5730.90, base loss: 20506.98
[INFO 2017-06-26 12:58:17,663 main.py:50] epoch 5188, training loss: 5631.03, average training loss: 5730.71, base loss: 20506.97
[INFO 2017-06-26 12:58:18,022 main.py:50] epoch 5189, training loss: 5696.24, average training loss: 5730.62, base loss: 20507.00
[INFO 2017-06-26 12:58:18,382 main.py:50] epoch 5190, training loss: 5623.74, average training loss: 5730.46, base loss: 20506.76
[INFO 2017-06-26 12:58:18,743 main.py:50] epoch 5191, training loss: 5678.54, average training loss: 5730.31, base loss: 20506.77
[INFO 2017-06-26 12:58:19,114 main.py:50] epoch 5192, training loss: 5616.12, average training loss: 5730.19, base loss: 20506.74
[INFO 2017-06-26 12:58:19,475 main.py:50] epoch 5193, training loss: 5692.88, average training loss: 5730.07, base loss: 20506.54
[INFO 2017-06-26 12:58:19,835 main.py:50] epoch 5194, training loss: 5645.34, average training loss: 5730.00, base loss: 20506.33
[INFO 2017-06-26 12:58:20,195 main.py:50] epoch 5195, training loss: 5691.28, average training loss: 5729.95, base loss: 20506.21
[INFO 2017-06-26 12:58:20,556 main.py:50] epoch 5196, training loss: 5617.14, average training loss: 5729.82, base loss: 20506.32
[INFO 2017-06-26 12:58:20,916 main.py:50] epoch 5197, training loss: 5715.84, average training loss: 5729.77, base loss: 20506.62
[INFO 2017-06-26 12:58:21,276 main.py:50] epoch 5198, training loss: 5731.90, average training loss: 5729.70, base loss: 20506.81
[INFO 2017-06-26 12:58:21,637 main.py:50] epoch 5199, training loss: 5684.77, average training loss: 5729.64, base loss: 20506.74
[INFO 2017-06-26 12:58:21,637 main.py:52] epoch 5199, testing
[INFO 2017-06-26 12:58:23,105 main.py:103] average testing loss: 5661.51, base loss: 20591.31
[INFO 2017-06-26 12:58:23,106 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 12:58:23,112 main.py:76] current best accuracy: 5661.51
[INFO 2017-06-26 12:58:23,471 main.py:50] epoch 5200, training loss: 5677.48, average training loss: 5729.57, base loss: 20506.90
[INFO 2017-06-26 12:58:23,831 main.py:50] epoch 5201, training loss: 5665.72, average training loss: 5729.53, base loss: 20506.87
[INFO 2017-06-26 12:58:24,190 main.py:50] epoch 5202, training loss: 5783.98, average training loss: 5729.63, base loss: 20506.87
[INFO 2017-06-26 12:58:24,549 main.py:50] epoch 5203, training loss: 5670.35, average training loss: 5729.52, base loss: 20507.03
[INFO 2017-06-26 12:58:24,908 main.py:50] epoch 5204, training loss: 5693.39, average training loss: 5729.44, base loss: 20507.11
[INFO 2017-06-26 12:58:25,267 main.py:50] epoch 5205, training loss: 5663.42, average training loss: 5729.36, base loss: 20507.06
[INFO 2017-06-26 12:58:25,627 main.py:50] epoch 5206, training loss: 5690.39, average training loss: 5729.35, base loss: 20507.22
[INFO 2017-06-26 12:58:25,984 main.py:50] epoch 5207, training loss: 5624.48, average training loss: 5729.20, base loss: 20507.26
[INFO 2017-06-26 12:58:26,344 main.py:50] epoch 5208, training loss: 5624.20, average training loss: 5729.03, base loss: 20507.16
[INFO 2017-06-26 12:58:26,702 main.py:50] epoch 5209, training loss: 5641.12, average training loss: 5728.95, base loss: 20507.11
[INFO 2017-06-26 12:58:27,062 main.py:50] epoch 5210, training loss: 5674.02, average training loss: 5728.91, base loss: 20506.81
[INFO 2017-06-26 12:58:27,420 main.py:50] epoch 5211, training loss: 5651.91, average training loss: 5728.77, base loss: 20506.56
[INFO 2017-06-26 12:58:27,778 main.py:50] epoch 5212, training loss: 5608.34, average training loss: 5728.68, base loss: 20506.41
[INFO 2017-06-26 12:58:28,139 main.py:50] epoch 5213, training loss: 5641.57, average training loss: 5728.55, base loss: 20506.35
[INFO 2017-06-26 12:58:28,498 main.py:50] epoch 5214, training loss: 5679.84, average training loss: 5728.43, base loss: 20506.44
[INFO 2017-06-26 12:58:28,857 main.py:50] epoch 5215, training loss: 5645.76, average training loss: 5728.25, base loss: 20506.39
[INFO 2017-06-26 12:58:29,216 main.py:50] epoch 5216, training loss: 5688.66, average training loss: 5728.14, base loss: 20506.51
[INFO 2017-06-26 12:58:29,576 main.py:50] epoch 5217, training loss: 5685.52, average training loss: 5728.00, base loss: 20506.43
[INFO 2017-06-26 12:58:29,934 main.py:50] epoch 5218, training loss: 5658.91, average training loss: 5727.85, base loss: 20506.50
[INFO 2017-06-26 12:58:30,293 main.py:50] epoch 5219, training loss: 5621.73, average training loss: 5727.68, base loss: 20506.31
[INFO 2017-06-26 12:58:30,652 main.py:50] epoch 5220, training loss: 5671.92, average training loss: 5727.49, base loss: 20506.08
[INFO 2017-06-26 12:58:31,011 main.py:50] epoch 5221, training loss: 5668.24, average training loss: 5727.37, base loss: 20505.79
[INFO 2017-06-26 12:58:31,370 main.py:50] epoch 5222, training loss: 5598.42, average training loss: 5727.11, base loss: 20505.78
[INFO 2017-06-26 12:58:31,730 main.py:50] epoch 5223, training loss: 5601.81, average training loss: 5726.95, base loss: 20505.53
[INFO 2017-06-26 12:58:32,089 main.py:50] epoch 5224, training loss: 5661.77, average training loss: 5726.78, base loss: 20505.17
[INFO 2017-06-26 12:58:32,449 main.py:50] epoch 5225, training loss: 5701.22, average training loss: 5726.70, base loss: 20505.10
[INFO 2017-06-26 12:58:32,808 main.py:50] epoch 5226, training loss: 5632.36, average training loss: 5726.57, base loss: 20504.89
[INFO 2017-06-26 12:58:33,167 main.py:50] epoch 5227, training loss: 5662.74, average training loss: 5726.46, base loss: 20504.86
[INFO 2017-06-26 12:58:33,527 main.py:50] epoch 5228, training loss: 5678.46, average training loss: 5726.33, base loss: 20505.03
[INFO 2017-06-26 12:58:33,886 main.py:50] epoch 5229, training loss: 5653.10, average training loss: 5726.20, base loss: 20505.11
[INFO 2017-06-26 12:58:34,245 main.py:50] epoch 5230, training loss: 5657.89, average training loss: 5726.07, base loss: 20504.64
[INFO 2017-06-26 12:58:34,604 main.py:50] epoch 5231, training loss: 5613.84, average training loss: 5725.88, base loss: 20504.59
[INFO 2017-06-26 12:58:34,963 main.py:50] epoch 5232, training loss: 5667.60, average training loss: 5725.79, base loss: 20504.84
[INFO 2017-06-26 12:58:35,322 main.py:50] epoch 5233, training loss: 5609.09, average training loss: 5725.69, base loss: 20505.19
[INFO 2017-06-26 12:58:35,681 main.py:50] epoch 5234, training loss: 5615.22, average training loss: 5725.52, base loss: 20505.19
[INFO 2017-06-26 12:58:36,041 main.py:50] epoch 5235, training loss: 5623.29, average training loss: 5725.41, base loss: 20505.27
[INFO 2017-06-26 12:58:36,400 main.py:50] epoch 5236, training loss: 5646.89, average training loss: 5725.28, base loss: 20505.36
[INFO 2017-06-26 12:58:36,760 main.py:50] epoch 5237, training loss: 5679.92, average training loss: 5725.18, base loss: 20505.67
[INFO 2017-06-26 12:58:37,118 main.py:50] epoch 5238, training loss: 5660.18, average training loss: 5725.04, base loss: 20505.73
[INFO 2017-06-26 12:58:37,477 main.py:50] epoch 5239, training loss: 5652.84, average training loss: 5724.89, base loss: 20505.93
[INFO 2017-06-26 12:58:37,836 main.py:50] epoch 5240, training loss: 5728.83, average training loss: 5724.85, base loss: 20505.78
[INFO 2017-06-26 12:58:38,194 main.py:50] epoch 5241, training loss: 5700.35, average training loss: 5724.77, base loss: 20506.18
[INFO 2017-06-26 12:58:38,554 main.py:50] epoch 5242, training loss: 5631.86, average training loss: 5724.63, base loss: 20506.15
[INFO 2017-06-26 12:58:38,911 main.py:50] epoch 5243, training loss: 5655.74, average training loss: 5724.42, base loss: 20506.04
[INFO 2017-06-26 12:58:39,270 main.py:50] epoch 5244, training loss: 5616.23, average training loss: 5724.19, base loss: 20505.78
[INFO 2017-06-26 12:58:39,628 main.py:50] epoch 5245, training loss: 5638.97, average training loss: 5724.10, base loss: 20505.80
[INFO 2017-06-26 12:58:39,987 main.py:50] epoch 5246, training loss: 5666.41, average training loss: 5723.96, base loss: 20506.00
[INFO 2017-06-26 12:58:40,346 main.py:50] epoch 5247, training loss: 5599.07, average training loss: 5723.79, base loss: 20506.27
[INFO 2017-06-26 12:58:40,704 main.py:50] epoch 5248, training loss: 5632.15, average training loss: 5723.53, base loss: 20506.55
[INFO 2017-06-26 12:58:41,063 main.py:50] epoch 5249, training loss: 5638.52, average training loss: 5723.37, base loss: 20506.46
[INFO 2017-06-26 12:58:41,421 main.py:50] epoch 5250, training loss: 5610.00, average training loss: 5723.14, base loss: 20506.44
[INFO 2017-06-26 12:58:41,779 main.py:50] epoch 5251, training loss: 5641.97, average training loss: 5722.95, base loss: 20506.20
[INFO 2017-06-26 12:58:42,140 main.py:50] epoch 5252, training loss: 5621.79, average training loss: 5722.84, base loss: 20506.70
[INFO 2017-06-26 12:58:42,497 main.py:50] epoch 5253, training loss: 5637.41, average training loss: 5722.65, base loss: 20506.82
[INFO 2017-06-26 12:58:42,856 main.py:50] epoch 5254, training loss: 5584.94, average training loss: 5722.46, base loss: 20507.01
[INFO 2017-06-26 12:58:43,215 main.py:50] epoch 5255, training loss: 5713.79, average training loss: 5722.36, base loss: 20507.06
[INFO 2017-06-26 12:58:43,574 main.py:50] epoch 5256, training loss: 5632.19, average training loss: 5722.22, base loss: 20507.27
[INFO 2017-06-26 12:58:43,933 main.py:50] epoch 5257, training loss: 5660.96, average training loss: 5722.14, base loss: 20507.12
[INFO 2017-06-26 12:58:44,292 main.py:50] epoch 5258, training loss: 5574.98, average training loss: 5721.89, base loss: 20507.12
[INFO 2017-06-26 12:58:44,651 main.py:50] epoch 5259, training loss: 5673.18, average training loss: 5721.74, base loss: 20507.27
[INFO 2017-06-26 12:58:45,009 main.py:50] epoch 5260, training loss: 5640.30, average training loss: 5721.57, base loss: 20507.52
[INFO 2017-06-26 12:58:45,369 main.py:50] epoch 5261, training loss: 5593.76, average training loss: 5721.42, base loss: 20507.60
[INFO 2017-06-26 12:58:45,729 main.py:50] epoch 5262, training loss: 5606.16, average training loss: 5721.21, base loss: 20507.90
[INFO 2017-06-26 12:58:46,088 main.py:50] epoch 5263, training loss: 5636.85, average training loss: 5721.08, base loss: 20508.17
[INFO 2017-06-26 12:58:46,447 main.py:50] epoch 5264, training loss: 5623.08, average training loss: 5720.92, base loss: 20507.80
[INFO 2017-06-26 12:58:46,806 main.py:50] epoch 5265, training loss: 5684.75, average training loss: 5720.83, base loss: 20508.05
[INFO 2017-06-26 12:58:47,165 main.py:50] epoch 5266, training loss: 5669.98, average training loss: 5720.69, base loss: 20507.96
[INFO 2017-06-26 12:58:47,523 main.py:50] epoch 5267, training loss: 5632.72, average training loss: 5720.56, base loss: 20508.02
[INFO 2017-06-26 12:58:47,882 main.py:50] epoch 5268, training loss: 5634.46, average training loss: 5720.41, base loss: 20508.27
[INFO 2017-06-26 12:58:48,241 main.py:50] epoch 5269, training loss: 5634.22, average training loss: 5720.28, base loss: 20508.39
[INFO 2017-06-26 12:58:48,599 main.py:50] epoch 5270, training loss: 5613.08, average training loss: 5720.08, base loss: 20508.16
[INFO 2017-06-26 12:58:48,959 main.py:50] epoch 5271, training loss: 5656.44, average training loss: 5720.00, base loss: 20508.67
[INFO 2017-06-26 12:58:49,316 main.py:50] epoch 5272, training loss: 5623.64, average training loss: 5719.87, base loss: 20508.94
[INFO 2017-06-26 12:58:49,676 main.py:50] epoch 5273, training loss: 5619.83, average training loss: 5719.78, base loss: 20509.00
[INFO 2017-06-26 12:58:50,035 main.py:50] epoch 5274, training loss: 5707.60, average training loss: 5719.66, base loss: 20509.06
[INFO 2017-06-26 12:58:50,394 main.py:50] epoch 5275, training loss: 5642.11, average training loss: 5719.55, base loss: 20509.04
[INFO 2017-06-26 12:58:50,752 main.py:50] epoch 5276, training loss: 5686.02, average training loss: 5719.50, base loss: 20509.52
[INFO 2017-06-26 12:58:51,111 main.py:50] epoch 5277, training loss: 5641.16, average training loss: 5719.39, base loss: 20509.39
[INFO 2017-06-26 12:58:51,470 main.py:50] epoch 5278, training loss: 5685.34, average training loss: 5719.28, base loss: 20508.85
[INFO 2017-06-26 12:58:51,828 main.py:50] epoch 5279, training loss: 5702.58, average training loss: 5719.18, base loss: 20508.78
[INFO 2017-06-26 12:58:52,186 main.py:50] epoch 5280, training loss: 5670.06, average training loss: 5719.10, base loss: 20508.50
[INFO 2017-06-26 12:58:52,547 main.py:50] epoch 5281, training loss: 5635.24, average training loss: 5718.93, base loss: 20508.50
[INFO 2017-06-26 12:58:52,905 main.py:50] epoch 5282, training loss: 5629.59, average training loss: 5718.82, base loss: 20508.44
[INFO 2017-06-26 12:58:53,265 main.py:50] epoch 5283, training loss: 5688.53, average training loss: 5718.77, base loss: 20508.24
[INFO 2017-06-26 12:58:53,624 main.py:50] epoch 5284, training loss: 5735.91, average training loss: 5718.79, base loss: 20508.59
[INFO 2017-06-26 12:58:53,982 main.py:50] epoch 5285, training loss: 5694.36, average training loss: 5718.72, base loss: 20508.11
[INFO 2017-06-26 12:58:54,340 main.py:50] epoch 5286, training loss: 5711.11, average training loss: 5718.73, base loss: 20508.14
[INFO 2017-06-26 12:58:54,699 main.py:50] epoch 5287, training loss: 5659.38, average training loss: 5718.62, base loss: 20508.41
[INFO 2017-06-26 12:58:55,059 main.py:50] epoch 5288, training loss: 5674.43, average training loss: 5718.50, base loss: 20508.56
[INFO 2017-06-26 12:58:55,418 main.py:50] epoch 5289, training loss: 5733.18, average training loss: 5718.55, base loss: 20508.47
[INFO 2017-06-26 12:58:55,777 main.py:50] epoch 5290, training loss: 5647.26, average training loss: 5718.44, base loss: 20508.17
[INFO 2017-06-26 12:58:56,136 main.py:50] epoch 5291, training loss: 5664.54, average training loss: 5718.34, base loss: 20508.49
[INFO 2017-06-26 12:58:56,495 main.py:50] epoch 5292, training loss: 5673.01, average training loss: 5718.27, base loss: 20508.69
[INFO 2017-06-26 12:58:56,853 main.py:50] epoch 5293, training loss: 5625.61, average training loss: 5718.11, base loss: 20508.75
[INFO 2017-06-26 12:58:57,212 main.py:50] epoch 5294, training loss: 5649.41, average training loss: 5718.04, base loss: 20508.94
[INFO 2017-06-26 12:58:57,572 main.py:50] epoch 5295, training loss: 5576.99, average training loss: 5717.81, base loss: 20508.72
[INFO 2017-06-26 12:58:57,931 main.py:50] epoch 5296, training loss: 5674.86, average training loss: 5717.77, base loss: 20508.79
[INFO 2017-06-26 12:58:58,289 main.py:50] epoch 5297, training loss: 5698.81, average training loss: 5717.75, base loss: 20508.85
[INFO 2017-06-26 12:58:58,647 main.py:50] epoch 5298, training loss: 5659.99, average training loss: 5717.60, base loss: 20508.89
[INFO 2017-06-26 12:58:59,007 main.py:50] epoch 5299, training loss: 5610.07, average training loss: 5717.49, base loss: 20509.07
[INFO 2017-06-26 12:58:59,007 main.py:52] epoch 5299, testing
[INFO 2017-06-26 12:59:00,495 main.py:103] average testing loss: 5635.09, base loss: 20471.35
[INFO 2017-06-26 12:59:00,496 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 12:59:00,502 main.py:76] current best accuracy: 5635.09
[INFO 2017-06-26 12:59:00,862 main.py:50] epoch 5300, training loss: 5664.56, average training loss: 5717.39, base loss: 20509.01
[INFO 2017-06-26 12:59:01,220 main.py:50] epoch 5301, training loss: 5613.69, average training loss: 5717.26, base loss: 20508.91
[INFO 2017-06-26 12:59:01,581 main.py:50] epoch 5302, training loss: 5647.60, average training loss: 5717.06, base loss: 20508.88
[INFO 2017-06-26 12:59:01,940 main.py:50] epoch 5303, training loss: 5677.54, average training loss: 5716.92, base loss: 20509.06
[INFO 2017-06-26 12:59:02,299 main.py:50] epoch 5304, training loss: 5695.40, average training loss: 5716.78, base loss: 20509.54
[INFO 2017-06-26 12:59:02,657 main.py:50] epoch 5305, training loss: 5686.80, average training loss: 5716.73, base loss: 20509.70
[INFO 2017-06-26 12:59:03,018 main.py:50] epoch 5306, training loss: 5644.71, average training loss: 5716.57, base loss: 20509.50
[INFO 2017-06-26 12:59:03,376 main.py:50] epoch 5307, training loss: 5643.02, average training loss: 5716.41, base loss: 20509.80
[INFO 2017-06-26 12:59:03,734 main.py:50] epoch 5308, training loss: 5628.56, average training loss: 5716.21, base loss: 20509.22
[INFO 2017-06-26 12:59:04,092 main.py:50] epoch 5309, training loss: 5709.12, average training loss: 5716.12, base loss: 20509.81
[INFO 2017-06-26 12:59:04,452 main.py:50] epoch 5310, training loss: 5738.11, average training loss: 5716.04, base loss: 20509.94
[INFO 2017-06-26 12:59:04,811 main.py:50] epoch 5311, training loss: 5689.16, average training loss: 5715.96, base loss: 20509.87
[INFO 2017-06-26 12:59:05,170 main.py:50] epoch 5312, training loss: 5600.21, average training loss: 5715.75, base loss: 20509.42
[INFO 2017-06-26 12:59:05,529 main.py:50] epoch 5313, training loss: 5733.98, average training loss: 5715.76, base loss: 20509.59
[INFO 2017-06-26 12:59:05,887 main.py:50] epoch 5314, training loss: 5629.07, average training loss: 5715.63, base loss: 20509.62
[INFO 2017-06-26 12:59:06,247 main.py:50] epoch 5315, training loss: 5651.22, average training loss: 5715.49, base loss: 20509.45
[INFO 2017-06-26 12:59:06,607 main.py:50] epoch 5316, training loss: 5713.24, average training loss: 5715.45, base loss: 20509.54
[INFO 2017-06-26 12:59:06,964 main.py:50] epoch 5317, training loss: 5641.23, average training loss: 5715.31, base loss: 20509.31
[INFO 2017-06-26 12:59:07,323 main.py:50] epoch 5318, training loss: 5602.88, average training loss: 5715.16, base loss: 20508.82
[INFO 2017-06-26 12:59:07,684 main.py:50] epoch 5319, training loss: 5699.04, average training loss: 5715.11, base loss: 20508.89
[INFO 2017-06-26 12:59:08,041 main.py:50] epoch 5320, training loss: 5647.88, average training loss: 5715.01, base loss: 20508.90
[INFO 2017-06-26 12:59:08,400 main.py:50] epoch 5321, training loss: 5647.64, average training loss: 5714.92, base loss: 20508.75
[INFO 2017-06-26 12:59:08,758 main.py:50] epoch 5322, training loss: 5647.25, average training loss: 5714.73, base loss: 20508.64
[INFO 2017-06-26 12:59:09,119 main.py:50] epoch 5323, training loss: 5652.80, average training loss: 5714.64, base loss: 20508.75
[INFO 2017-06-26 12:59:09,478 main.py:50] epoch 5324, training loss: 5621.38, average training loss: 5714.42, base loss: 20508.52
[INFO 2017-06-26 12:59:09,837 main.py:50] epoch 5325, training loss: 5598.58, average training loss: 5714.30, base loss: 20508.40
[INFO 2017-06-26 12:59:10,194 main.py:50] epoch 5326, training loss: 5649.33, average training loss: 5714.14, base loss: 20508.36
[INFO 2017-06-26 12:59:10,554 main.py:50] epoch 5327, training loss: 5682.93, average training loss: 5714.07, base loss: 20508.18
[INFO 2017-06-26 12:59:10,915 main.py:50] epoch 5328, training loss: 5642.16, average training loss: 5713.96, base loss: 20508.37
[INFO 2017-06-26 12:59:11,274 main.py:50] epoch 5329, training loss: 5705.94, average training loss: 5713.93, base loss: 20508.68
[INFO 2017-06-26 12:59:11,635 main.py:50] epoch 5330, training loss: 5691.67, average training loss: 5713.91, base loss: 20508.69
[INFO 2017-06-26 12:59:11,994 main.py:50] epoch 5331, training loss: 5665.43, average training loss: 5713.87, base loss: 20508.44
[INFO 2017-06-26 12:59:12,354 main.py:50] epoch 5332, training loss: 5678.59, average training loss: 5713.82, base loss: 20508.42
[INFO 2017-06-26 12:59:12,715 main.py:50] epoch 5333, training loss: 5724.81, average training loss: 5713.79, base loss: 20508.60
[INFO 2017-06-26 12:59:13,075 main.py:50] epoch 5334, training loss: 5607.88, average training loss: 5713.60, base loss: 20508.59
[INFO 2017-06-26 12:59:13,434 main.py:50] epoch 5335, training loss: 5709.92, average training loss: 5713.58, base loss: 20509.07
[INFO 2017-06-26 12:59:13,794 main.py:50] epoch 5336, training loss: 5623.17, average training loss: 5713.51, base loss: 20509.64
[INFO 2017-06-26 12:59:14,152 main.py:50] epoch 5337, training loss: 5728.29, average training loss: 5713.46, base loss: 20509.89
[INFO 2017-06-26 12:59:14,513 main.py:50] epoch 5338, training loss: 5690.08, average training loss: 5713.45, base loss: 20510.17
[INFO 2017-06-26 12:59:14,872 main.py:50] epoch 5339, training loss: 5644.24, average training loss: 5713.38, base loss: 20510.16
[INFO 2017-06-26 12:59:15,231 main.py:50] epoch 5340, training loss: 5655.63, average training loss: 5713.35, base loss: 20510.13
[INFO 2017-06-26 12:59:15,591 main.py:50] epoch 5341, training loss: 5666.63, average training loss: 5713.23, base loss: 20510.07
[INFO 2017-06-26 12:59:15,951 main.py:50] epoch 5342, training loss: 5648.58, average training loss: 5713.19, base loss: 20510.12
[INFO 2017-06-26 12:59:16,311 main.py:50] epoch 5343, training loss: 5675.89, average training loss: 5713.05, base loss: 20510.19
[INFO 2017-06-26 12:59:16,670 main.py:50] epoch 5344, training loss: 5640.98, average training loss: 5712.92, base loss: 20510.09
[INFO 2017-06-26 12:59:17,029 main.py:50] epoch 5345, training loss: 5596.36, average training loss: 5712.78, base loss: 20509.38
[INFO 2017-06-26 12:59:17,387 main.py:50] epoch 5346, training loss: 5670.79, average training loss: 5712.72, base loss: 20509.18
[INFO 2017-06-26 12:59:17,746 main.py:50] epoch 5347, training loss: 5724.47, average training loss: 5712.66, base loss: 20509.31
[INFO 2017-06-26 12:59:18,105 main.py:50] epoch 5348, training loss: 5697.68, average training loss: 5712.60, base loss: 20509.09
[INFO 2017-06-26 12:59:18,465 main.py:50] epoch 5349, training loss: 5640.73, average training loss: 5712.44, base loss: 20508.91
[INFO 2017-06-26 12:59:18,824 main.py:50] epoch 5350, training loss: 5677.91, average training loss: 5712.29, base loss: 20508.85
[INFO 2017-06-26 12:59:19,182 main.py:50] epoch 5351, training loss: 5665.84, average training loss: 5712.23, base loss: 20508.56
[INFO 2017-06-26 12:59:19,542 main.py:50] epoch 5352, training loss: 5589.77, average training loss: 5712.01, base loss: 20508.54
[INFO 2017-06-26 12:59:19,902 main.py:50] epoch 5353, training loss: 5729.46, average training loss: 5711.97, base loss: 20508.67
[INFO 2017-06-26 12:59:20,262 main.py:50] epoch 5354, training loss: 5650.03, average training loss: 5711.71, base loss: 20508.43
[INFO 2017-06-26 12:59:20,621 main.py:50] epoch 5355, training loss: 5719.11, average training loss: 5711.63, base loss: 20508.34
[INFO 2017-06-26 12:59:20,981 main.py:50] epoch 5356, training loss: 5637.71, average training loss: 5711.44, base loss: 20508.27
[INFO 2017-06-26 12:59:21,340 main.py:50] epoch 5357, training loss: 5622.80, average training loss: 5711.25, base loss: 20508.35
[INFO 2017-06-26 12:59:21,700 main.py:50] epoch 5358, training loss: 5663.57, average training loss: 5711.15, base loss: 20507.79
[INFO 2017-06-26 12:59:22,059 main.py:50] epoch 5359, training loss: 5637.82, average training loss: 5710.93, base loss: 20508.09
[INFO 2017-06-26 12:59:22,418 main.py:50] epoch 5360, training loss: 5653.73, average training loss: 5710.78, base loss: 20508.33
[INFO 2017-06-26 12:59:22,776 main.py:50] epoch 5361, training loss: 5626.68, average training loss: 5710.63, base loss: 20508.35
[INFO 2017-06-26 12:59:23,135 main.py:50] epoch 5362, training loss: 5721.70, average training loss: 5710.55, base loss: 20508.59
[INFO 2017-06-26 12:59:23,495 main.py:50] epoch 5363, training loss: 5614.09, average training loss: 5710.39, base loss: 20508.36
[INFO 2017-06-26 12:59:23,854 main.py:50] epoch 5364, training loss: 5635.28, average training loss: 5710.18, base loss: 20508.52
[INFO 2017-06-26 12:59:24,213 main.py:50] epoch 5365, training loss: 5639.99, average training loss: 5710.07, base loss: 20508.69
[INFO 2017-06-26 12:59:24,573 main.py:50] epoch 5366, training loss: 5611.31, average training loss: 5710.03, base loss: 20508.89
[INFO 2017-06-26 12:59:24,932 main.py:50] epoch 5367, training loss: 5606.69, average training loss: 5709.94, base loss: 20509.11
[INFO 2017-06-26 12:59:25,290 main.py:50] epoch 5368, training loss: 5679.77, average training loss: 5709.82, base loss: 20509.27
[INFO 2017-06-26 12:59:25,649 main.py:50] epoch 5369, training loss: 5713.30, average training loss: 5709.74, base loss: 20509.30
[INFO 2017-06-26 12:59:26,007 main.py:50] epoch 5370, training loss: 5629.65, average training loss: 5709.58, base loss: 20509.08
[INFO 2017-06-26 12:59:26,367 main.py:50] epoch 5371, training loss: 5680.42, average training loss: 5709.53, base loss: 20509.10
[INFO 2017-06-26 12:59:26,725 main.py:50] epoch 5372, training loss: 5731.87, average training loss: 5709.47, base loss: 20509.25
[INFO 2017-06-26 12:59:27,085 main.py:50] epoch 5373, training loss: 5693.89, average training loss: 5709.42, base loss: 20509.07
[INFO 2017-06-26 12:59:27,443 main.py:50] epoch 5374, training loss: 5633.87, average training loss: 5709.31, base loss: 20509.05
[INFO 2017-06-26 12:59:27,802 main.py:50] epoch 5375, training loss: 5713.24, average training loss: 5709.24, base loss: 20509.10
[INFO 2017-06-26 12:59:28,161 main.py:50] epoch 5376, training loss: 5741.45, average training loss: 5709.19, base loss: 20508.89
[INFO 2017-06-26 12:59:28,520 main.py:50] epoch 5377, training loss: 5617.05, average training loss: 5709.01, base loss: 20508.67
[INFO 2017-06-26 12:59:28,878 main.py:50] epoch 5378, training loss: 5698.81, average training loss: 5708.89, base loss: 20508.71
[INFO 2017-06-26 12:59:29,237 main.py:50] epoch 5379, training loss: 5637.99, average training loss: 5708.75, base loss: 20508.55
[INFO 2017-06-26 12:59:29,597 main.py:50] epoch 5380, training loss: 5639.71, average training loss: 5708.66, base loss: 20508.64
[INFO 2017-06-26 12:59:29,956 main.py:50] epoch 5381, training loss: 5626.82, average training loss: 5708.53, base loss: 20508.87
[INFO 2017-06-26 12:59:30,315 main.py:50] epoch 5382, training loss: 5634.99, average training loss: 5708.36, base loss: 20508.90
[INFO 2017-06-26 12:59:30,674 main.py:50] epoch 5383, training loss: 5600.16, average training loss: 5708.12, base loss: 20509.04
[INFO 2017-06-26 12:59:31,033 main.py:50] epoch 5384, training loss: 5651.22, average training loss: 5707.98, base loss: 20508.94
[INFO 2017-06-26 12:59:31,392 main.py:50] epoch 5385, training loss: 5653.83, average training loss: 5707.86, base loss: 20509.03
[INFO 2017-06-26 12:59:31,750 main.py:50] epoch 5386, training loss: 5680.39, average training loss: 5707.77, base loss: 20508.74
[INFO 2017-06-26 12:59:32,109 main.py:50] epoch 5387, training loss: 5725.45, average training loss: 5707.78, base loss: 20509.08
[INFO 2017-06-26 12:59:32,469 main.py:50] epoch 5388, training loss: 5660.53, average training loss: 5707.67, base loss: 20509.38
[INFO 2017-06-26 12:59:32,828 main.py:50] epoch 5389, training loss: 5641.34, average training loss: 5707.56, base loss: 20509.18
[INFO 2017-06-26 12:59:33,185 main.py:50] epoch 5390, training loss: 5609.68, average training loss: 5707.42, base loss: 20509.53
[INFO 2017-06-26 12:59:33,544 main.py:50] epoch 5391, training loss: 5608.51, average training loss: 5707.33, base loss: 20509.43
[INFO 2017-06-26 12:59:33,903 main.py:50] epoch 5392, training loss: 5663.52, average training loss: 5707.22, base loss: 20509.62
[INFO 2017-06-26 12:59:34,261 main.py:50] epoch 5393, training loss: 5631.90, average training loss: 5707.17, base loss: 20510.03
[INFO 2017-06-26 12:59:34,620 main.py:50] epoch 5394, training loss: 5590.75, average training loss: 5706.98, base loss: 20510.10
[INFO 2017-06-26 12:59:34,978 main.py:50] epoch 5395, training loss: 5700.10, average training loss: 5706.88, base loss: 20510.46
[INFO 2017-06-26 12:59:35,337 main.py:50] epoch 5396, training loss: 5664.73, average training loss: 5706.79, base loss: 20510.55
[INFO 2017-06-26 12:59:35,695 main.py:50] epoch 5397, training loss: 5637.46, average training loss: 5706.65, base loss: 20510.60
[INFO 2017-06-26 12:59:36,053 main.py:50] epoch 5398, training loss: 5659.07, average training loss: 5706.63, base loss: 20510.35
[INFO 2017-06-26 12:59:36,412 main.py:50] epoch 5399, training loss: 5612.67, average training loss: 5706.51, base loss: 20510.28
[INFO 2017-06-26 12:59:36,412 main.py:52] epoch 5399, testing
[INFO 2017-06-26 12:59:37,884 main.py:103] average testing loss: 5630.96, base loss: 20386.59
[INFO 2017-06-26 12:59:37,885 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 12:59:37,891 main.py:76] current best accuracy: 5630.96
[INFO 2017-06-26 12:59:38,249 main.py:50] epoch 5400, training loss: 5616.25, average training loss: 5706.33, base loss: 20510.03
[INFO 2017-06-26 12:59:38,608 main.py:50] epoch 5401, training loss: 5625.68, average training loss: 5706.22, base loss: 20510.04
[INFO 2017-06-26 12:59:38,966 main.py:50] epoch 5402, training loss: 5721.08, average training loss: 5706.17, base loss: 20510.20
[INFO 2017-06-26 12:59:39,325 main.py:50] epoch 5403, training loss: 5575.57, average training loss: 5706.02, base loss: 20510.02
[INFO 2017-06-26 12:59:39,684 main.py:50] epoch 5404, training loss: 5692.01, average training loss: 5705.99, base loss: 20509.89
[INFO 2017-06-26 12:59:40,042 main.py:50] epoch 5405, training loss: 5627.64, average training loss: 5705.92, base loss: 20510.00
[INFO 2017-06-26 12:59:40,401 main.py:50] epoch 5406, training loss: 5653.45, average training loss: 5705.82, base loss: 20510.21
[INFO 2017-06-26 12:59:40,760 main.py:50] epoch 5407, training loss: 5610.36, average training loss: 5705.68, base loss: 20510.13
[INFO 2017-06-26 12:59:41,118 main.py:50] epoch 5408, training loss: 5641.71, average training loss: 5705.47, base loss: 20510.06
[INFO 2017-06-26 12:59:41,478 main.py:50] epoch 5409, training loss: 5660.10, average training loss: 5705.40, base loss: 20509.98
[INFO 2017-06-26 12:59:41,837 main.py:50] epoch 5410, training loss: 5657.42, average training loss: 5705.37, base loss: 20510.21
[INFO 2017-06-26 12:59:42,196 main.py:50] epoch 5411, training loss: 5646.33, average training loss: 5705.32, base loss: 20510.37
[INFO 2017-06-26 12:59:42,555 main.py:50] epoch 5412, training loss: 5653.30, average training loss: 5705.24, base loss: 20510.02
[INFO 2017-06-26 12:59:42,914 main.py:50] epoch 5413, training loss: 5591.76, average training loss: 5705.17, base loss: 20510.28
[INFO 2017-06-26 12:59:43,272 main.py:50] epoch 5414, training loss: 5617.06, average training loss: 5705.06, base loss: 20510.19
[INFO 2017-06-26 12:59:43,631 main.py:50] epoch 5415, training loss: 5624.76, average training loss: 5705.01, base loss: 20510.18
[INFO 2017-06-26 12:59:43,990 main.py:50] epoch 5416, training loss: 5657.79, average training loss: 5704.94, base loss: 20510.15
[INFO 2017-06-26 12:59:44,361 main.py:50] epoch 5417, training loss: 5702.11, average training loss: 5704.92, base loss: 20510.43
[INFO 2017-06-26 12:59:44,719 main.py:50] epoch 5418, training loss: 5680.04, average training loss: 5704.83, base loss: 20510.33
[INFO 2017-06-26 12:59:45,076 main.py:50] epoch 5419, training loss: 5648.12, average training loss: 5704.74, base loss: 20510.63
[INFO 2017-06-26 12:59:45,434 main.py:50] epoch 5420, training loss: 5703.93, average training loss: 5704.73, base loss: 20510.86
[INFO 2017-06-26 12:59:45,793 main.py:50] epoch 5421, training loss: 5605.38, average training loss: 5704.61, base loss: 20511.06
[INFO 2017-06-26 12:59:46,153 main.py:50] epoch 5422, training loss: 5585.16, average training loss: 5704.52, base loss: 20511.25
[INFO 2017-06-26 12:59:46,511 main.py:50] epoch 5423, training loss: 5648.88, average training loss: 5704.43, base loss: 20511.51
[INFO 2017-06-26 12:59:46,868 main.py:50] epoch 5424, training loss: 5685.88, average training loss: 5704.34, base loss: 20511.31
[INFO 2017-06-26 12:59:47,225 main.py:50] epoch 5425, training loss: 5595.14, average training loss: 5704.20, base loss: 20511.32
[INFO 2017-06-26 12:59:47,585 main.py:50] epoch 5426, training loss: 5645.04, average training loss: 5704.09, base loss: 20511.60
[INFO 2017-06-26 12:59:47,942 main.py:50] epoch 5427, training loss: 5728.04, average training loss: 5704.07, base loss: 20511.59
[INFO 2017-06-26 12:59:48,301 main.py:50] epoch 5428, training loss: 5643.20, average training loss: 5703.97, base loss: 20511.51
[INFO 2017-06-26 12:59:48,658 main.py:50] epoch 5429, training loss: 5652.67, average training loss: 5703.79, base loss: 20511.14
[INFO 2017-06-26 12:59:49,016 main.py:50] epoch 5430, training loss: 5654.04, average training loss: 5703.68, base loss: 20511.03
[INFO 2017-06-26 12:59:49,376 main.py:50] epoch 5431, training loss: 5670.20, average training loss: 5703.59, base loss: 20511.07
[INFO 2017-06-26 12:59:49,735 main.py:50] epoch 5432, training loss: 5638.34, average training loss: 5703.42, base loss: 20511.03
[INFO 2017-06-26 12:59:50,092 main.py:50] epoch 5433, training loss: 5641.24, average training loss: 5703.22, base loss: 20510.73
[INFO 2017-06-26 12:59:50,450 main.py:50] epoch 5434, training loss: 5610.53, average training loss: 5703.11, base loss: 20510.51
[INFO 2017-06-26 12:59:50,809 main.py:50] epoch 5435, training loss: 5625.55, average training loss: 5702.94, base loss: 20510.21
[INFO 2017-06-26 12:59:51,168 main.py:50] epoch 5436, training loss: 5644.88, average training loss: 5702.80, base loss: 20510.52
[INFO 2017-06-26 12:59:51,525 main.py:50] epoch 5437, training loss: 5566.31, average training loss: 5702.52, base loss: 20510.67
[INFO 2017-06-26 12:59:51,882 main.py:50] epoch 5438, training loss: 5636.35, average training loss: 5702.34, base loss: 20510.45
[INFO 2017-06-26 12:59:52,240 main.py:50] epoch 5439, training loss: 5615.31, average training loss: 5702.19, base loss: 20510.48
[INFO 2017-06-26 12:59:52,599 main.py:50] epoch 5440, training loss: 5590.42, average training loss: 5702.02, base loss: 20510.39
[INFO 2017-06-26 12:59:52,958 main.py:50] epoch 5441, training loss: 5645.93, average training loss: 5701.86, base loss: 20510.37
[INFO 2017-06-26 12:59:53,316 main.py:50] epoch 5442, training loss: 5648.77, average training loss: 5701.75, base loss: 20510.54
[INFO 2017-06-26 12:59:53,674 main.py:50] epoch 5443, training loss: 5623.65, average training loss: 5701.55, base loss: 20510.24
[INFO 2017-06-26 12:59:54,032 main.py:50] epoch 5444, training loss: 5711.82, average training loss: 5701.52, base loss: 20510.44
[INFO 2017-06-26 12:59:54,391 main.py:50] epoch 5445, training loss: 5676.54, average training loss: 5701.42, base loss: 20510.37
[INFO 2017-06-26 12:59:54,751 main.py:50] epoch 5446, training loss: 5624.52, average training loss: 5701.29, base loss: 20510.44
[INFO 2017-06-26 12:59:55,108 main.py:50] epoch 5447, training loss: 5637.51, average training loss: 5701.11, base loss: 20510.09
[INFO 2017-06-26 12:59:55,466 main.py:50] epoch 5448, training loss: 5650.80, average training loss: 5701.01, base loss: 20509.76
[INFO 2017-06-26 12:59:55,824 main.py:50] epoch 5449, training loss: 5677.45, average training loss: 5700.87, base loss: 20509.71
[INFO 2017-06-26 12:59:56,182 main.py:50] epoch 5450, training loss: 5604.42, average training loss: 5700.64, base loss: 20509.20
[INFO 2017-06-26 12:59:56,541 main.py:50] epoch 5451, training loss: 5616.33, average training loss: 5700.45, base loss: 20509.23
[INFO 2017-06-26 12:59:56,899 main.py:50] epoch 5452, training loss: 5671.37, average training loss: 5700.38, base loss: 20509.10
[INFO 2017-06-26 12:59:57,258 main.py:50] epoch 5453, training loss: 5660.12, average training loss: 5700.31, base loss: 20508.90
[INFO 2017-06-26 12:59:57,619 main.py:50] epoch 5454, training loss: 5620.90, average training loss: 5700.22, base loss: 20508.89
[INFO 2017-06-26 12:59:57,978 main.py:50] epoch 5455, training loss: 5659.19, average training loss: 5700.15, base loss: 20508.81
[INFO 2017-06-26 12:59:58,335 main.py:50] epoch 5456, training loss: 5623.91, average training loss: 5700.02, base loss: 20508.75
[INFO 2017-06-26 12:59:58,693 main.py:50] epoch 5457, training loss: 5647.54, average training loss: 5699.90, base loss: 20508.90
[INFO 2017-06-26 12:59:59,050 main.py:50] epoch 5458, training loss: 5598.92, average training loss: 5699.78, base loss: 20509.05
[INFO 2017-06-26 12:59:59,409 main.py:50] epoch 5459, training loss: 5631.41, average training loss: 5699.69, base loss: 20509.05
[INFO 2017-06-26 12:59:59,766 main.py:50] epoch 5460, training loss: 5648.66, average training loss: 5699.63, base loss: 20509.05
[INFO 2017-06-26 13:00:00,124 main.py:50] epoch 5461, training loss: 5618.34, average training loss: 5699.48, base loss: 20508.93
[INFO 2017-06-26 13:00:00,484 main.py:50] epoch 5462, training loss: 5601.19, average training loss: 5699.39, base loss: 20509.03
[INFO 2017-06-26 13:00:00,844 main.py:50] epoch 5463, training loss: 5564.16, average training loss: 5699.22, base loss: 20508.89
[INFO 2017-06-26 13:00:01,203 main.py:50] epoch 5464, training loss: 5620.39, average training loss: 5699.13, base loss: 20508.75
[INFO 2017-06-26 13:00:01,562 main.py:50] epoch 5465, training loss: 5715.75, average training loss: 5699.13, base loss: 20508.77
[INFO 2017-06-26 13:00:01,921 main.py:50] epoch 5466, training loss: 5635.92, average training loss: 5699.05, base loss: 20508.94
[INFO 2017-06-26 13:00:02,281 main.py:50] epoch 5467, training loss: 5615.49, average training loss: 5698.93, base loss: 20509.10
[INFO 2017-06-26 13:00:02,640 main.py:50] epoch 5468, training loss: 5592.97, average training loss: 5698.77, base loss: 20508.76
[INFO 2017-06-26 13:00:03,000 main.py:50] epoch 5469, training loss: 5636.18, average training loss: 5698.67, base loss: 20508.97
[INFO 2017-06-26 13:00:03,359 main.py:50] epoch 5470, training loss: 5655.78, average training loss: 5698.55, base loss: 20508.68
[INFO 2017-06-26 13:00:03,717 main.py:50] epoch 5471, training loss: 5579.88, average training loss: 5698.33, base loss: 20508.91
[INFO 2017-06-26 13:00:04,076 main.py:50] epoch 5472, training loss: 5617.90, average training loss: 5698.22, base loss: 20509.41
[INFO 2017-06-26 13:00:04,435 main.py:50] epoch 5473, training loss: 5626.60, average training loss: 5697.94, base loss: 20509.45
[INFO 2017-06-26 13:00:04,794 main.py:50] epoch 5474, training loss: 5593.15, average training loss: 5697.71, base loss: 20509.67
[INFO 2017-06-26 13:00:05,152 main.py:50] epoch 5475, training loss: 5536.36, average training loss: 5697.51, base loss: 20509.72
[INFO 2017-06-26 13:00:05,511 main.py:50] epoch 5476, training loss: 5586.85, average training loss: 5697.31, base loss: 20510.08
[INFO 2017-06-26 13:00:05,869 main.py:50] epoch 5477, training loss: 5656.09, average training loss: 5697.21, base loss: 20509.80
[INFO 2017-06-26 13:00:06,226 main.py:50] epoch 5478, training loss: 5564.02, average training loss: 5696.95, base loss: 20509.71
[INFO 2017-06-26 13:00:06,583 main.py:50] epoch 5479, training loss: 5606.70, average training loss: 5696.82, base loss: 20509.57
[INFO 2017-06-26 13:00:06,941 main.py:50] epoch 5480, training loss: 5706.20, average training loss: 5696.66, base loss: 20509.82
[INFO 2017-06-26 13:00:07,300 main.py:50] epoch 5481, training loss: 5666.71, average training loss: 5696.58, base loss: 20509.97
[INFO 2017-06-26 13:00:07,659 main.py:50] epoch 5482, training loss: 5640.36, average training loss: 5696.48, base loss: 20509.83
[INFO 2017-06-26 13:00:08,018 main.py:50] epoch 5483, training loss: 5684.94, average training loss: 5696.40, base loss: 20509.98
[INFO 2017-06-26 13:00:08,379 main.py:50] epoch 5484, training loss: 5707.46, average training loss: 5696.37, base loss: 20510.28
[INFO 2017-06-26 13:00:08,736 main.py:50] epoch 5485, training loss: 5687.73, average training loss: 5696.32, base loss: 20510.13
[INFO 2017-06-26 13:00:09,095 main.py:50] epoch 5486, training loss: 5722.45, average training loss: 5696.29, base loss: 20510.73
[INFO 2017-06-26 13:00:09,454 main.py:50] epoch 5487, training loss: 5631.48, average training loss: 5696.13, base loss: 20510.73
[INFO 2017-06-26 13:00:09,812 main.py:50] epoch 5488, training loss: 5676.13, average training loss: 5696.08, base loss: 20510.44
[INFO 2017-06-26 13:00:10,172 main.py:50] epoch 5489, training loss: 5640.71, average training loss: 5695.96, base loss: 20510.61
[INFO 2017-06-26 13:00:10,531 main.py:50] epoch 5490, training loss: 5711.93, average training loss: 5695.93, base loss: 20510.27
[INFO 2017-06-26 13:00:10,890 main.py:50] epoch 5491, training loss: 5653.92, average training loss: 5695.92, base loss: 20509.95
[INFO 2017-06-26 13:00:11,249 main.py:50] epoch 5492, training loss: 5629.97, average training loss: 5695.81, base loss: 20509.83
[INFO 2017-06-26 13:00:11,607 main.py:50] epoch 5493, training loss: 5720.83, average training loss: 5695.76, base loss: 20509.22
[INFO 2017-06-26 13:00:11,965 main.py:50] epoch 5494, training loss: 5655.13, average training loss: 5695.63, base loss: 20508.95
[INFO 2017-06-26 13:00:12,326 main.py:50] epoch 5495, training loss: 5660.28, average training loss: 5695.61, base loss: 20509.10
[INFO 2017-06-26 13:00:12,684 main.py:50] epoch 5496, training loss: 5711.21, average training loss: 5695.55, base loss: 20509.15
[INFO 2017-06-26 13:00:13,044 main.py:50] epoch 5497, training loss: 5645.94, average training loss: 5695.50, base loss: 20509.47
[INFO 2017-06-26 13:00:13,403 main.py:50] epoch 5498, training loss: 5665.46, average training loss: 5695.40, base loss: 20509.68
[INFO 2017-06-26 13:00:13,762 main.py:50] epoch 5499, training loss: 5668.99, average training loss: 5695.37, base loss: 20510.29
[INFO 2017-06-26 13:00:13,762 main.py:52] epoch 5499, testing
[INFO 2017-06-26 13:00:15,226 main.py:103] average testing loss: 5633.10, base loss: 20584.69
[INFO 2017-06-26 13:00:15,226 main.py:76] current best accuracy: 5630.96
[INFO 2017-06-26 13:00:15,584 main.py:50] epoch 5500, training loss: 5557.74, average training loss: 5695.15, base loss: 20510.02
[INFO 2017-06-26 13:00:15,942 main.py:50] epoch 5501, training loss: 5613.44, average training loss: 5694.99, base loss: 20509.90
[INFO 2017-06-26 13:00:16,303 main.py:50] epoch 5502, training loss: 5610.41, average training loss: 5694.92, base loss: 20509.67
[INFO 2017-06-26 13:00:16,659 main.py:50] epoch 5503, training loss: 5645.31, average training loss: 5694.90, base loss: 20509.90
[INFO 2017-06-26 13:00:17,018 main.py:50] epoch 5504, training loss: 5613.44, average training loss: 5694.79, base loss: 20509.43
[INFO 2017-06-26 13:00:17,377 main.py:50] epoch 5505, training loss: 5686.37, average training loss: 5694.72, base loss: 20509.58
[INFO 2017-06-26 13:00:17,736 main.py:50] epoch 5506, training loss: 5664.15, average training loss: 5694.62, base loss: 20509.83
[INFO 2017-06-26 13:00:18,096 main.py:50] epoch 5507, training loss: 5645.88, average training loss: 5694.51, base loss: 20509.95
[INFO 2017-06-26 13:00:18,455 main.py:50] epoch 5508, training loss: 5603.02, average training loss: 5694.40, base loss: 20510.18
[INFO 2017-06-26 13:00:18,813 main.py:50] epoch 5509, training loss: 5592.55, average training loss: 5694.24, base loss: 20510.49
[INFO 2017-06-26 13:00:19,172 main.py:50] epoch 5510, training loss: 5630.07, average training loss: 5694.17, base loss: 20510.78
[INFO 2017-06-26 13:00:19,531 main.py:50] epoch 5511, training loss: 5636.74, average training loss: 5694.06, base loss: 20510.64
[INFO 2017-06-26 13:00:19,890 main.py:50] epoch 5512, training loss: 5574.56, average training loss: 5693.90, base loss: 20510.08
[INFO 2017-06-26 13:00:20,249 main.py:50] epoch 5513, training loss: 5652.57, average training loss: 5693.77, base loss: 20510.09
[INFO 2017-06-26 13:00:20,608 main.py:50] epoch 5514, training loss: 5563.99, average training loss: 5693.60, base loss: 20509.72
[INFO 2017-06-26 13:00:20,966 main.py:50] epoch 5515, training loss: 5641.55, average training loss: 5693.50, base loss: 20509.93
[INFO 2017-06-26 13:00:21,326 main.py:50] epoch 5516, training loss: 5571.50, average training loss: 5693.44, base loss: 20509.77
[INFO 2017-06-26 13:00:21,685 main.py:50] epoch 5517, training loss: 5611.72, average training loss: 5693.32, base loss: 20509.83
[INFO 2017-06-26 13:00:22,044 main.py:50] epoch 5518, training loss: 5608.63, average training loss: 5693.22, base loss: 20510.29
[INFO 2017-06-26 13:00:22,404 main.py:50] epoch 5519, training loss: 5643.44, average training loss: 5693.10, base loss: 20510.44
[INFO 2017-06-26 13:00:22,762 main.py:50] epoch 5520, training loss: 5637.33, average training loss: 5692.97, base loss: 20509.80
[INFO 2017-06-26 13:00:23,122 main.py:50] epoch 5521, training loss: 5655.72, average training loss: 5692.84, base loss: 20509.90
[INFO 2017-06-26 13:00:23,480 main.py:50] epoch 5522, training loss: 5641.17, average training loss: 5692.75, base loss: 20509.89
[INFO 2017-06-26 13:00:23,839 main.py:50] epoch 5523, training loss: 5561.65, average training loss: 5692.55, base loss: 20509.70
[INFO 2017-06-26 13:00:24,197 main.py:50] epoch 5524, training loss: 5669.32, average training loss: 5692.45, base loss: 20509.91
[INFO 2017-06-26 13:00:24,556 main.py:50] epoch 5525, training loss: 5553.85, average training loss: 5692.29, base loss: 20510.18
[INFO 2017-06-26 13:00:24,913 main.py:50] epoch 5526, training loss: 5602.56, average training loss: 5692.08, base loss: 20509.78
[INFO 2017-06-26 13:00:25,270 main.py:50] epoch 5527, training loss: 5653.07, average training loss: 5691.97, base loss: 20509.63
[INFO 2017-06-26 13:00:25,628 main.py:50] epoch 5528, training loss: 5595.81, average training loss: 5691.83, base loss: 20509.74
[INFO 2017-06-26 13:00:25,987 main.py:50] epoch 5529, training loss: 5652.27, average training loss: 5691.70, base loss: 20509.68
[INFO 2017-06-26 13:00:26,346 main.py:50] epoch 5530, training loss: 5607.41, average training loss: 5691.58, base loss: 20509.47
[INFO 2017-06-26 13:00:26,705 main.py:50] epoch 5531, training loss: 5679.47, average training loss: 5691.52, base loss: 20509.23
[INFO 2017-06-26 13:00:27,065 main.py:50] epoch 5532, training loss: 5617.66, average training loss: 5691.37, base loss: 20509.19
[INFO 2017-06-26 13:00:27,423 main.py:50] epoch 5533, training loss: 5605.08, average training loss: 5691.23, base loss: 20509.16
[INFO 2017-06-26 13:00:27,784 main.py:50] epoch 5534, training loss: 5670.63, average training loss: 5691.12, base loss: 20509.08
[INFO 2017-06-26 13:00:28,143 main.py:50] epoch 5535, training loss: 5619.56, average training loss: 5690.99, base loss: 20509.15
[INFO 2017-06-26 13:00:28,501 main.py:50] epoch 5536, training loss: 5625.89, average training loss: 5690.88, base loss: 20509.36
[INFO 2017-06-26 13:00:28,872 main.py:50] epoch 5537, training loss: 5612.59, average training loss: 5690.82, base loss: 20509.77
[INFO 2017-06-26 13:00:29,231 main.py:50] epoch 5538, training loss: 5627.28, average training loss: 5690.68, base loss: 20509.53
[INFO 2017-06-26 13:00:29,589 main.py:50] epoch 5539, training loss: 5606.03, average training loss: 5690.50, base loss: 20509.19
[INFO 2017-06-26 13:00:29,948 main.py:50] epoch 5540, training loss: 5619.19, average training loss: 5690.41, base loss: 20509.63
[INFO 2017-06-26 13:00:30,309 main.py:50] epoch 5541, training loss: 5612.02, average training loss: 5690.27, base loss: 20509.48
[INFO 2017-06-26 13:00:30,668 main.py:50] epoch 5542, training loss: 5613.99, average training loss: 5690.05, base loss: 20509.61
[INFO 2017-06-26 13:00:31,027 main.py:50] epoch 5543, training loss: 5627.37, average training loss: 5689.92, base loss: 20508.94
[INFO 2017-06-26 13:00:31,386 main.py:50] epoch 5544, training loss: 5628.54, average training loss: 5689.77, base loss: 20509.23
[INFO 2017-06-26 13:00:31,749 main.py:50] epoch 5545, training loss: 5660.99, average training loss: 5689.72, base loss: 20508.85
[INFO 2017-06-26 13:00:32,108 main.py:50] epoch 5546, training loss: 5634.37, average training loss: 5689.56, base loss: 20508.85
[INFO 2017-06-26 13:00:32,467 main.py:50] epoch 5547, training loss: 5597.55, average training loss: 5689.54, base loss: 20508.92
[INFO 2017-06-26 13:00:32,825 main.py:50] epoch 5548, training loss: 5602.98, average training loss: 5689.30, base loss: 20508.58
[INFO 2017-06-26 13:00:33,185 main.py:50] epoch 5549, training loss: 5614.51, average training loss: 5689.13, base loss: 20508.19
[INFO 2017-06-26 13:00:33,542 main.py:50] epoch 5550, training loss: 5629.59, average training loss: 5688.95, base loss: 20508.26
[INFO 2017-06-26 13:00:33,901 main.py:50] epoch 5551, training loss: 5670.18, average training loss: 5688.77, base loss: 20508.27
[INFO 2017-06-26 13:00:34,260 main.py:50] epoch 5552, training loss: 5560.37, average training loss: 5688.52, base loss: 20508.13
[INFO 2017-06-26 13:00:34,618 main.py:50] epoch 5553, training loss: 5649.90, average training loss: 5688.25, base loss: 20508.24
[INFO 2017-06-26 13:00:34,977 main.py:50] epoch 5554, training loss: 5606.84, average training loss: 5688.05, base loss: 20508.52
[INFO 2017-06-26 13:00:35,337 main.py:50] epoch 5555, training loss: 5669.67, average training loss: 5687.86, base loss: 20507.92
[INFO 2017-06-26 13:00:35,696 main.py:50] epoch 5556, training loss: 5631.90, average training loss: 5687.76, base loss: 20507.94
[INFO 2017-06-26 13:00:36,054 main.py:50] epoch 5557, training loss: 5670.47, average training loss: 5687.68, base loss: 20507.89
[INFO 2017-06-26 13:00:36,413 main.py:50] epoch 5558, training loss: 5669.75, average training loss: 5687.58, base loss: 20507.97
[INFO 2017-06-26 13:00:36,774 main.py:50] epoch 5559, training loss: 5683.38, average training loss: 5687.55, base loss: 20508.13
[INFO 2017-06-26 13:00:37,132 main.py:50] epoch 5560, training loss: 5636.67, average training loss: 5687.41, base loss: 20507.89
[INFO 2017-06-26 13:00:37,493 main.py:50] epoch 5561, training loss: 5645.94, average training loss: 5687.28, base loss: 20508.23
[INFO 2017-06-26 13:00:37,850 main.py:50] epoch 5562, training loss: 5687.59, average training loss: 5687.22, base loss: 20508.66
[INFO 2017-06-26 13:00:38,209 main.py:50] epoch 5563, training loss: 5604.63, average training loss: 5687.02, base loss: 20508.48
[INFO 2017-06-26 13:00:38,567 main.py:50] epoch 5564, training loss: 5591.20, average training loss: 5686.84, base loss: 20508.39
[INFO 2017-06-26 13:00:38,926 main.py:50] epoch 5565, training loss: 5574.05, average training loss: 5686.58, base loss: 20508.32
[INFO 2017-06-26 13:00:39,284 main.py:50] epoch 5566, training loss: 5676.43, average training loss: 5686.44, base loss: 20508.22
[INFO 2017-06-26 13:00:39,643 main.py:50] epoch 5567, training loss: 5660.68, average training loss: 5686.23, base loss: 20508.70
[INFO 2017-06-26 13:00:40,001 main.py:50] epoch 5568, training loss: 5624.44, average training loss: 5686.07, base loss: 20508.98
[INFO 2017-06-26 13:00:40,361 main.py:50] epoch 5569, training loss: 5651.36, average training loss: 5685.94, base loss: 20508.59
[INFO 2017-06-26 13:00:40,718 main.py:50] epoch 5570, training loss: 5641.89, average training loss: 5685.82, base loss: 20508.53
[INFO 2017-06-26 13:00:41,077 main.py:50] epoch 5571, training loss: 5635.55, average training loss: 5685.74, base loss: 20508.85
[INFO 2017-06-26 13:00:41,436 main.py:50] epoch 5572, training loss: 5629.34, average training loss: 5685.61, base loss: 20509.08
[INFO 2017-06-26 13:00:41,794 main.py:50] epoch 5573, training loss: 5624.53, average training loss: 5685.45, base loss: 20508.68
[INFO 2017-06-26 13:00:42,154 main.py:50] epoch 5574, training loss: 5619.23, average training loss: 5685.33, base loss: 20508.48
[INFO 2017-06-26 13:00:42,513 main.py:50] epoch 5575, training loss: 5666.54, average training loss: 5685.26, base loss: 20508.47
[INFO 2017-06-26 13:00:42,871 main.py:50] epoch 5576, training loss: 5730.46, average training loss: 5685.23, base loss: 20508.86
[INFO 2017-06-26 13:00:43,230 main.py:50] epoch 5577, training loss: 5642.08, average training loss: 5685.13, base loss: 20508.51
[INFO 2017-06-26 13:00:43,590 main.py:50] epoch 5578, training loss: 5680.80, average training loss: 5685.03, base loss: 20508.56
[INFO 2017-06-26 13:00:43,950 main.py:50] epoch 5579, training loss: 5601.29, average training loss: 5684.87, base loss: 20508.62
[INFO 2017-06-26 13:00:44,308 main.py:50] epoch 5580, training loss: 5732.46, average training loss: 5684.90, base loss: 20508.66
[INFO 2017-06-26 13:00:44,666 main.py:50] epoch 5581, training loss: 5678.69, average training loss: 5684.73, base loss: 20508.20
[INFO 2017-06-26 13:00:45,024 main.py:50] epoch 5582, training loss: 5732.56, average training loss: 5684.67, base loss: 20508.15
[INFO 2017-06-26 13:00:45,381 main.py:50] epoch 5583, training loss: 5727.06, average training loss: 5684.66, base loss: 20508.20
[INFO 2017-06-26 13:00:45,740 main.py:50] epoch 5584, training loss: 5661.68, average training loss: 5684.50, base loss: 20508.66
[INFO 2017-06-26 13:00:46,099 main.py:50] epoch 5585, training loss: 5741.07, average training loss: 5684.47, base loss: 20508.88
[INFO 2017-06-26 13:00:46,457 main.py:50] epoch 5586, training loss: 5616.57, average training loss: 5684.30, base loss: 20509.06
[INFO 2017-06-26 13:00:46,817 main.py:50] epoch 5587, training loss: 5685.65, average training loss: 5684.19, base loss: 20509.23
[INFO 2017-06-26 13:00:47,176 main.py:50] epoch 5588, training loss: 5718.33, average training loss: 5684.16, base loss: 20509.40
[INFO 2017-06-26 13:00:47,535 main.py:50] epoch 5589, training loss: 5751.10, average training loss: 5684.11, base loss: 20509.91
[INFO 2017-06-26 13:00:47,894 main.py:50] epoch 5590, training loss: 5661.36, average training loss: 5684.01, base loss: 20509.94
[INFO 2017-06-26 13:00:48,252 main.py:50] epoch 5591, training loss: 5636.91, average training loss: 5683.82, base loss: 20509.80
[INFO 2017-06-26 13:00:48,611 main.py:50] epoch 5592, training loss: 5623.27, average training loss: 5683.66, base loss: 20509.64
[INFO 2017-06-26 13:00:48,970 main.py:50] epoch 5593, training loss: 5607.81, average training loss: 5683.47, base loss: 20509.32
[INFO 2017-06-26 13:00:49,330 main.py:50] epoch 5594, training loss: 5634.89, average training loss: 5683.29, base loss: 20509.68
[INFO 2017-06-26 13:00:49,688 main.py:50] epoch 5595, training loss: 5681.38, average training loss: 5683.19, base loss: 20509.66
[INFO 2017-06-26 13:00:50,046 main.py:50] epoch 5596, training loss: 5632.99, average training loss: 5683.06, base loss: 20509.67
[INFO 2017-06-26 13:00:50,406 main.py:50] epoch 5597, training loss: 5621.00, average training loss: 5682.99, base loss: 20509.61
[INFO 2017-06-26 13:00:50,765 main.py:50] epoch 5598, training loss: 5729.34, average training loss: 5683.05, base loss: 20509.70
[INFO 2017-06-26 13:00:51,123 main.py:50] epoch 5599, training loss: 5648.31, average training loss: 5682.95, base loss: 20509.55
[INFO 2017-06-26 13:00:51,123 main.py:52] epoch 5599, testing
[INFO 2017-06-26 13:00:52,588 main.py:103] average testing loss: 5642.57, base loss: 20466.42
[INFO 2017-06-26 13:00:52,589 main.py:76] current best accuracy: 5630.96
[INFO 2017-06-26 13:00:52,950 main.py:50] epoch 5600, training loss: 5649.48, average training loss: 5682.80, base loss: 20509.57
[INFO 2017-06-26 13:00:53,308 main.py:50] epoch 5601, training loss: 5630.13, average training loss: 5682.74, base loss: 20509.41
[INFO 2017-06-26 13:00:53,667 main.py:50] epoch 5602, training loss: 5575.20, average training loss: 5682.58, base loss: 20509.18
[INFO 2017-06-26 13:00:54,025 main.py:50] epoch 5603, training loss: 5596.23, average training loss: 5682.47, base loss: 20509.49
[INFO 2017-06-26 13:00:54,384 main.py:50] epoch 5604, training loss: 5673.31, average training loss: 5682.38, base loss: 20509.63
[INFO 2017-06-26 13:00:54,743 main.py:50] epoch 5605, training loss: 5612.77, average training loss: 5682.29, base loss: 20510.00
[INFO 2017-06-26 13:00:55,103 main.py:50] epoch 5606, training loss: 5580.46, average training loss: 5682.12, base loss: 20510.01
[INFO 2017-06-26 13:00:55,462 main.py:50] epoch 5607, training loss: 5681.30, average training loss: 5682.12, base loss: 20510.09
[INFO 2017-06-26 13:00:55,821 main.py:50] epoch 5608, training loss: 5613.92, average training loss: 5681.97, base loss: 20509.71
[INFO 2017-06-26 13:00:56,182 main.py:50] epoch 5609, training loss: 5680.43, average training loss: 5681.95, base loss: 20509.81
[INFO 2017-06-26 13:00:56,540 main.py:50] epoch 5610, training loss: 5608.41, average training loss: 5681.83, base loss: 20509.81
[INFO 2017-06-26 13:00:56,900 main.py:50] epoch 5611, training loss: 5651.31, average training loss: 5681.73, base loss: 20509.91
[INFO 2017-06-26 13:00:57,260 main.py:50] epoch 5612, training loss: 5668.56, average training loss: 5681.64, base loss: 20509.88
[INFO 2017-06-26 13:00:57,620 main.py:50] epoch 5613, training loss: 5655.84, average training loss: 5681.54, base loss: 20510.09
[INFO 2017-06-26 13:00:57,978 main.py:50] epoch 5614, training loss: 5690.41, average training loss: 5681.49, base loss: 20510.01
[INFO 2017-06-26 13:00:58,338 main.py:50] epoch 5615, training loss: 5593.46, average training loss: 5681.36, base loss: 20509.82
[INFO 2017-06-26 13:00:58,698 main.py:50] epoch 5616, training loss: 5690.25, average training loss: 5681.31, base loss: 20509.99
[INFO 2017-06-26 13:00:59,056 main.py:50] epoch 5617, training loss: 5649.92, average training loss: 5681.20, base loss: 20510.21
[INFO 2017-06-26 13:00:59,416 main.py:50] epoch 5618, training loss: 5646.46, average training loss: 5681.07, base loss: 20510.16
[INFO 2017-06-26 13:00:59,775 main.py:50] epoch 5619, training loss: 5677.64, average training loss: 5681.07, base loss: 20510.07
[INFO 2017-06-26 13:01:00,135 main.py:50] epoch 5620, training loss: 5643.45, average training loss: 5680.93, base loss: 20509.66
[INFO 2017-06-26 13:01:00,493 main.py:50] epoch 5621, training loss: 5648.50, average training loss: 5680.80, base loss: 20509.83
[INFO 2017-06-26 13:01:00,852 main.py:50] epoch 5622, training loss: 5677.03, average training loss: 5680.81, base loss: 20510.29
[INFO 2017-06-26 13:01:01,210 main.py:50] epoch 5623, training loss: 5560.01, average training loss: 5680.67, base loss: 20509.87
[INFO 2017-06-26 13:01:01,571 main.py:50] epoch 5624, training loss: 5664.47, average training loss: 5680.60, base loss: 20509.78
[INFO 2017-06-26 13:01:01,930 main.py:50] epoch 5625, training loss: 5702.03, average training loss: 5680.59, base loss: 20510.10
[INFO 2017-06-26 13:01:02,289 main.py:50] epoch 5626, training loss: 5739.51, average training loss: 5680.67, base loss: 20510.44
[INFO 2017-06-26 13:01:02,647 main.py:50] epoch 5627, training loss: 5598.06, average training loss: 5680.58, base loss: 20510.14
[INFO 2017-06-26 13:01:03,007 main.py:50] epoch 5628, training loss: 5732.01, average training loss: 5680.61, base loss: 20510.50
[INFO 2017-06-26 13:01:03,366 main.py:50] epoch 5629, training loss: 5609.54, average training loss: 5680.55, base loss: 20510.10
[INFO 2017-06-26 13:01:03,725 main.py:50] epoch 5630, training loss: 5671.30, average training loss: 5680.49, base loss: 20509.70
[INFO 2017-06-26 13:01:04,084 main.py:50] epoch 5631, training loss: 5689.18, average training loss: 5680.44, base loss: 20509.63
[INFO 2017-06-26 13:01:04,443 main.py:50] epoch 5632, training loss: 5613.52, average training loss: 5680.26, base loss: 20509.48
[INFO 2017-06-26 13:01:04,800 main.py:50] epoch 5633, training loss: 5705.08, average training loss: 5680.20, base loss: 20509.49
[INFO 2017-06-26 13:01:05,158 main.py:50] epoch 5634, training loss: 5718.26, average training loss: 5680.21, base loss: 20509.85
[INFO 2017-06-26 13:01:05,516 main.py:50] epoch 5635, training loss: 5673.35, average training loss: 5680.16, base loss: 20509.64
[INFO 2017-06-26 13:01:05,873 main.py:50] epoch 5636, training loss: 5716.56, average training loss: 5680.15, base loss: 20510.12
[INFO 2017-06-26 13:01:06,231 main.py:50] epoch 5637, training loss: 5698.56, average training loss: 5680.10, base loss: 20510.16
[INFO 2017-06-26 13:01:06,589 main.py:50] epoch 5638, training loss: 5669.40, average training loss: 5680.01, base loss: 20509.81
[INFO 2017-06-26 13:01:06,948 main.py:50] epoch 5639, training loss: 5639.15, average training loss: 5679.91, base loss: 20509.97
[INFO 2017-06-26 13:01:07,308 main.py:50] epoch 5640, training loss: 5646.96, average training loss: 5679.79, base loss: 20510.67
[INFO 2017-06-26 13:01:07,667 main.py:50] epoch 5641, training loss: 5623.92, average training loss: 5679.65, base loss: 20510.74
[INFO 2017-06-26 13:01:08,025 main.py:50] epoch 5642, training loss: 5595.62, average training loss: 5679.49, base loss: 20510.87
[INFO 2017-06-26 13:01:08,383 main.py:50] epoch 5643, training loss: 5604.14, average training loss: 5679.35, base loss: 20510.95
[INFO 2017-06-26 13:01:08,742 main.py:50] epoch 5644, training loss: 5619.75, average training loss: 5679.21, base loss: 20511.02
[INFO 2017-06-26 13:01:09,101 main.py:50] epoch 5645, training loss: 5642.12, average training loss: 5679.12, base loss: 20510.88
[INFO 2017-06-26 13:01:09,459 main.py:50] epoch 5646, training loss: 5705.80, average training loss: 5679.04, base loss: 20510.99
[INFO 2017-06-26 13:01:09,818 main.py:50] epoch 5647, training loss: 5596.17, average training loss: 5678.94, base loss: 20511.31
[INFO 2017-06-26 13:01:10,177 main.py:50] epoch 5648, training loss: 5588.14, average training loss: 5678.80, base loss: 20510.89
[INFO 2017-06-26 13:01:10,535 main.py:50] epoch 5649, training loss: 5650.19, average training loss: 5678.72, base loss: 20511.03
[INFO 2017-06-26 13:01:10,894 main.py:50] epoch 5650, training loss: 5679.79, average training loss: 5678.63, base loss: 20511.38
[INFO 2017-06-26 13:01:11,253 main.py:50] epoch 5651, training loss: 5642.72, average training loss: 5678.57, base loss: 20511.68
[INFO 2017-06-26 13:01:11,613 main.py:50] epoch 5652, training loss: 5632.93, average training loss: 5678.49, base loss: 20511.61
[INFO 2017-06-26 13:01:11,972 main.py:50] epoch 5653, training loss: 5641.94, average training loss: 5678.40, base loss: 20511.94
[INFO 2017-06-26 13:01:12,329 main.py:50] epoch 5654, training loss: 5678.81, average training loss: 5678.35, base loss: 20511.86
[INFO 2017-06-26 13:01:12,689 main.py:50] epoch 5655, training loss: 5630.05, average training loss: 5678.28, base loss: 20511.74
[INFO 2017-06-26 13:01:13,048 main.py:50] epoch 5656, training loss: 5655.13, average training loss: 5678.25, base loss: 20511.82
[INFO 2017-06-26 13:01:13,419 main.py:50] epoch 5657, training loss: 5571.63, average training loss: 5678.08, base loss: 20512.01
[INFO 2017-06-26 13:01:13,777 main.py:50] epoch 5658, training loss: 5591.67, average training loss: 5677.97, base loss: 20512.43
[INFO 2017-06-26 13:01:14,136 main.py:50] epoch 5659, training loss: 5584.77, average training loss: 5677.83, base loss: 20512.47
[INFO 2017-06-26 13:01:14,496 main.py:50] epoch 5660, training loss: 5618.36, average training loss: 5677.78, base loss: 20512.49
[INFO 2017-06-26 13:01:14,855 main.py:50] epoch 5661, training loss: 5674.74, average training loss: 5677.68, base loss: 20512.65
[INFO 2017-06-26 13:01:15,212 main.py:50] epoch 5662, training loss: 5640.03, average training loss: 5677.69, base loss: 20513.07
[INFO 2017-06-26 13:01:15,571 main.py:50] epoch 5663, training loss: 5627.16, average training loss: 5677.57, base loss: 20513.21
[INFO 2017-06-26 13:01:15,931 main.py:50] epoch 5664, training loss: 5617.70, average training loss: 5677.48, base loss: 20513.07
[INFO 2017-06-26 13:01:16,290 main.py:50] epoch 5665, training loss: 5639.54, average training loss: 5677.27, base loss: 20512.98
[INFO 2017-06-26 13:01:16,648 main.py:50] epoch 5666, training loss: 5602.40, average training loss: 5677.11, base loss: 20512.98
[INFO 2017-06-26 13:01:17,007 main.py:50] epoch 5667, training loss: 5592.17, average training loss: 5676.96, base loss: 20513.25
[INFO 2017-06-26 13:01:17,366 main.py:50] epoch 5668, training loss: 5627.02, average training loss: 5676.83, base loss: 20513.23
[INFO 2017-06-26 13:01:17,724 main.py:50] epoch 5669, training loss: 5590.48, average training loss: 5676.69, base loss: 20512.99
[INFO 2017-06-26 13:01:18,082 main.py:50] epoch 5670, training loss: 5619.72, average training loss: 5676.57, base loss: 20513.36
[INFO 2017-06-26 13:01:18,441 main.py:50] epoch 5671, training loss: 5573.73, average training loss: 5676.43, base loss: 20513.33
[INFO 2017-06-26 13:01:18,799 main.py:50] epoch 5672, training loss: 5594.54, average training loss: 5676.29, base loss: 20513.56
[INFO 2017-06-26 13:01:19,158 main.py:50] epoch 5673, training loss: 5577.79, average training loss: 5676.18, base loss: 20513.59
[INFO 2017-06-26 13:01:19,517 main.py:50] epoch 5674, training loss: 5590.17, average training loss: 5676.04, base loss: 20513.46
[INFO 2017-06-26 13:01:19,876 main.py:50] epoch 5675, training loss: 5548.30, average training loss: 5675.88, base loss: 20513.44
[INFO 2017-06-26 13:01:20,234 main.py:50] epoch 5676, training loss: 5609.79, average training loss: 5675.76, base loss: 20513.30
[INFO 2017-06-26 13:01:20,594 main.py:50] epoch 5677, training loss: 5661.43, average training loss: 5675.64, base loss: 20513.28
[INFO 2017-06-26 13:01:20,952 main.py:50] epoch 5678, training loss: 5610.12, average training loss: 5675.46, base loss: 20513.31
[INFO 2017-06-26 13:01:21,311 main.py:50] epoch 5679, training loss: 5634.94, average training loss: 5675.43, base loss: 20513.55
[INFO 2017-06-26 13:01:21,669 main.py:50] epoch 5680, training loss: 5559.42, average training loss: 5675.21, base loss: 20513.12
[INFO 2017-06-26 13:01:22,027 main.py:50] epoch 5681, training loss: 5602.42, average training loss: 5675.03, base loss: 20513.34
[INFO 2017-06-26 13:01:22,386 main.py:50] epoch 5682, training loss: 5596.63, average training loss: 5674.86, base loss: 20513.16
[INFO 2017-06-26 13:01:22,747 main.py:50] epoch 5683, training loss: 5601.84, average training loss: 5674.72, base loss: 20512.96
[INFO 2017-06-26 13:01:23,106 main.py:50] epoch 5684, training loss: 5539.97, average training loss: 5674.47, base loss: 20512.65
[INFO 2017-06-26 13:01:23,465 main.py:50] epoch 5685, training loss: 5608.37, average training loss: 5674.36, base loss: 20513.00
[INFO 2017-06-26 13:01:23,824 main.py:50] epoch 5686, training loss: 5543.44, average training loss: 5674.11, base loss: 20512.97
[INFO 2017-06-26 13:01:24,184 main.py:50] epoch 5687, training loss: 5644.85, average training loss: 5674.01, base loss: 20513.08
[INFO 2017-06-26 13:01:24,543 main.py:50] epoch 5688, training loss: 5645.06, average training loss: 5673.87, base loss: 20513.04
[INFO 2017-06-26 13:01:24,901 main.py:50] epoch 5689, training loss: 5606.17, average training loss: 5673.81, base loss: 20512.44
[INFO 2017-06-26 13:01:25,260 main.py:50] epoch 5690, training loss: 5591.63, average training loss: 5673.62, base loss: 20512.72
[INFO 2017-06-26 13:01:25,619 main.py:50] epoch 5691, training loss: 5556.73, average training loss: 5673.46, base loss: 20512.89
[INFO 2017-06-26 13:01:25,978 main.py:50] epoch 5692, training loss: 5645.58, average training loss: 5673.41, base loss: 20513.00
[INFO 2017-06-26 13:01:26,335 main.py:50] epoch 5693, training loss: 5539.66, average training loss: 5673.17, base loss: 20512.37
[INFO 2017-06-26 13:01:26,694 main.py:50] epoch 5694, training loss: 5607.39, average training loss: 5673.02, base loss: 20512.44
[INFO 2017-06-26 13:01:27,057 main.py:50] epoch 5695, training loss: 5558.98, average training loss: 5672.87, base loss: 20512.74
[INFO 2017-06-26 13:01:27,416 main.py:50] epoch 5696, training loss: 5571.17, average training loss: 5672.76, base loss: 20512.66
[INFO 2017-06-26 13:01:27,777 main.py:50] epoch 5697, training loss: 5579.31, average training loss: 5672.60, base loss: 20512.67
[INFO 2017-06-26 13:01:28,136 main.py:50] epoch 5698, training loss: 5734.66, average training loss: 5672.63, base loss: 20512.72
[INFO 2017-06-26 13:01:28,493 main.py:50] epoch 5699, training loss: 5642.67, average training loss: 5672.56, base loss: 20512.66
[INFO 2017-06-26 13:01:28,493 main.py:52] epoch 5699, testing
[INFO 2017-06-26 13:01:29,956 main.py:103] average testing loss: 5574.58, base loss: 20503.84
[INFO 2017-06-26 13:01:29,957 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 13:01:29,963 main.py:76] current best accuracy: 5574.58
[INFO 2017-06-26 13:01:30,321 main.py:50] epoch 5700, training loss: 5588.82, average training loss: 5672.38, base loss: 20512.81
[INFO 2017-06-26 13:01:30,681 main.py:50] epoch 5701, training loss: 5516.25, average training loss: 5672.19, base loss: 20512.66
[INFO 2017-06-26 13:01:31,039 main.py:50] epoch 5702, training loss: 5620.81, average training loss: 5672.08, base loss: 20512.75
[INFO 2017-06-26 13:01:31,398 main.py:50] epoch 5703, training loss: 5674.74, average training loss: 5672.05, base loss: 20512.91
[INFO 2017-06-26 13:01:31,757 main.py:50] epoch 5704, training loss: 5664.90, average training loss: 5671.96, base loss: 20512.68
[INFO 2017-06-26 13:01:32,116 main.py:50] epoch 5705, training loss: 5673.78, average training loss: 5671.90, base loss: 20512.60
[INFO 2017-06-26 13:01:32,474 main.py:50] epoch 5706, training loss: 5632.22, average training loss: 5671.81, base loss: 20512.65
[INFO 2017-06-26 13:01:32,832 main.py:50] epoch 5707, training loss: 5638.00, average training loss: 5671.73, base loss: 20512.31
[INFO 2017-06-26 13:01:33,191 main.py:50] epoch 5708, training loss: 5604.11, average training loss: 5671.65, base loss: 20512.36
[INFO 2017-06-26 13:01:33,550 main.py:50] epoch 5709, training loss: 5655.07, average training loss: 5671.58, base loss: 20512.61
[INFO 2017-06-26 13:01:33,908 main.py:50] epoch 5710, training loss: 5621.05, average training loss: 5671.46, base loss: 20512.81
[INFO 2017-06-26 13:01:34,267 main.py:50] epoch 5711, training loss: 5667.96, average training loss: 5671.39, base loss: 20512.60
[INFO 2017-06-26 13:01:34,625 main.py:50] epoch 5712, training loss: 5644.56, average training loss: 5671.34, base loss: 20512.71
[INFO 2017-06-26 13:01:34,983 main.py:50] epoch 5713, training loss: 5686.35, average training loss: 5671.25, base loss: 20513.04
[INFO 2017-06-26 13:01:35,342 main.py:50] epoch 5714, training loss: 5668.02, average training loss: 5671.19, base loss: 20513.19
[INFO 2017-06-26 13:01:35,701 main.py:50] epoch 5715, training loss: 5689.92, average training loss: 5671.17, base loss: 20512.89
[INFO 2017-06-26 13:01:36,060 main.py:50] epoch 5716, training loss: 5686.70, average training loss: 5671.16, base loss: 20512.84
[INFO 2017-06-26 13:01:36,419 main.py:50] epoch 5717, training loss: 5676.26, average training loss: 5671.18, base loss: 20512.52
[INFO 2017-06-26 13:01:36,778 main.py:50] epoch 5718, training loss: 5699.93, average training loss: 5671.22, base loss: 20512.40
[INFO 2017-06-26 13:01:37,136 main.py:50] epoch 5719, training loss: 5737.64, average training loss: 5671.27, base loss: 20512.63
[INFO 2017-06-26 13:01:37,495 main.py:50] epoch 5720, training loss: 5729.76, average training loss: 5671.29, base loss: 20512.82
[INFO 2017-06-26 13:01:37,856 main.py:50] epoch 5721, training loss: 5637.27, average training loss: 5671.30, base loss: 20512.92
[INFO 2017-06-26 13:01:38,216 main.py:50] epoch 5722, training loss: 5681.82, average training loss: 5671.26, base loss: 20512.56
[INFO 2017-06-26 13:01:38,574 main.py:50] epoch 5723, training loss: 5612.66, average training loss: 5671.18, base loss: 20512.62
[INFO 2017-06-26 13:01:38,932 main.py:50] epoch 5724, training loss: 5569.26, average training loss: 5671.02, base loss: 20512.40
[INFO 2017-06-26 13:01:39,291 main.py:50] epoch 5725, training loss: 5678.92, average training loss: 5671.01, base loss: 20512.65
[INFO 2017-06-26 13:01:39,651 main.py:50] epoch 5726, training loss: 5650.45, average training loss: 5670.98, base loss: 20512.44
[INFO 2017-06-26 13:01:40,010 main.py:50] epoch 5727, training loss: 5697.82, average training loss: 5670.99, base loss: 20512.59
[INFO 2017-06-26 13:01:40,370 main.py:50] epoch 5728, training loss: 5600.20, average training loss: 5670.92, base loss: 20512.42
[INFO 2017-06-26 13:01:40,728 main.py:50] epoch 5729, training loss: 5648.88, average training loss: 5670.86, base loss: 20512.45
[INFO 2017-06-26 13:01:41,085 main.py:50] epoch 5730, training loss: 5663.47, average training loss: 5670.82, base loss: 20512.26
[INFO 2017-06-26 13:01:41,444 main.py:50] epoch 5731, training loss: 5643.90, average training loss: 5670.71, base loss: 20512.55
[INFO 2017-06-26 13:01:41,804 main.py:50] epoch 5732, training loss: 5716.35, average training loss: 5670.72, base loss: 20512.83
[INFO 2017-06-26 13:01:42,162 main.py:50] epoch 5733, training loss: 5614.05, average training loss: 5670.54, base loss: 20512.85
[INFO 2017-06-26 13:01:42,521 main.py:50] epoch 5734, training loss: 5604.60, average training loss: 5670.44, base loss: 20512.99
[INFO 2017-06-26 13:01:42,880 main.py:50] epoch 5735, training loss: 5642.98, average training loss: 5670.32, base loss: 20513.06
[INFO 2017-06-26 13:01:43,240 main.py:50] epoch 5736, training loss: 5575.26, average training loss: 5670.15, base loss: 20513.13
[INFO 2017-06-26 13:01:43,598 main.py:50] epoch 5737, training loss: 5715.24, average training loss: 5670.10, base loss: 20513.10
[INFO 2017-06-26 13:01:43,957 main.py:50] epoch 5738, training loss: 5666.36, average training loss: 5670.05, base loss: 20513.23
[INFO 2017-06-26 13:01:44,315 main.py:50] epoch 5739, training loss: 5681.49, average training loss: 5669.98, base loss: 20513.05
[INFO 2017-06-26 13:01:44,674 main.py:50] epoch 5740, training loss: 5705.96, average training loss: 5669.92, base loss: 20513.19
[INFO 2017-06-26 13:01:45,033 main.py:50] epoch 5741, training loss: 5630.13, average training loss: 5669.81, base loss: 20513.37
[INFO 2017-06-26 13:01:45,394 main.py:50] epoch 5742, training loss: 5662.28, average training loss: 5669.71, base loss: 20513.59
[INFO 2017-06-26 13:01:45,752 main.py:50] epoch 5743, training loss: 5594.57, average training loss: 5669.53, base loss: 20513.73
[INFO 2017-06-26 13:01:46,109 main.py:50] epoch 5744, training loss: 5623.00, average training loss: 5669.35, base loss: 20513.55
[INFO 2017-06-26 13:01:46,469 main.py:50] epoch 5745, training loss: 5621.76, average training loss: 5669.22, base loss: 20513.40
[INFO 2017-06-26 13:01:46,826 main.py:50] epoch 5746, training loss: 5581.50, average training loss: 5669.09, base loss: 20513.14
[INFO 2017-06-26 13:01:47,185 main.py:50] epoch 5747, training loss: 5671.29, average training loss: 5669.00, base loss: 20513.35
[INFO 2017-06-26 13:01:47,544 main.py:50] epoch 5748, training loss: 5617.56, average training loss: 5668.85, base loss: 20513.15
[INFO 2017-06-26 13:01:47,903 main.py:50] epoch 5749, training loss: 5626.83, average training loss: 5668.79, base loss: 20513.41
[INFO 2017-06-26 13:01:48,264 main.py:50] epoch 5750, training loss: 5675.48, average training loss: 5668.70, base loss: 20513.44
[INFO 2017-06-26 13:01:48,622 main.py:50] epoch 5751, training loss: 5642.25, average training loss: 5668.66, base loss: 20513.66
[INFO 2017-06-26 13:01:48,980 main.py:50] epoch 5752, training loss: 5622.90, average training loss: 5668.55, base loss: 20513.65
[INFO 2017-06-26 13:01:49,340 main.py:50] epoch 5753, training loss: 5689.05, average training loss: 5668.51, base loss: 20513.61
[INFO 2017-06-26 13:01:49,700 main.py:50] epoch 5754, training loss: 5617.95, average training loss: 5668.41, base loss: 20513.56
[INFO 2017-06-26 13:01:50,059 main.py:50] epoch 5755, training loss: 5624.25, average training loss: 5668.29, base loss: 20513.43
[INFO 2017-06-26 13:01:50,418 main.py:50] epoch 5756, training loss: 5574.47, average training loss: 5668.17, base loss: 20513.96
[INFO 2017-06-26 13:01:50,777 main.py:50] epoch 5757, training loss: 5609.39, average training loss: 5667.93, base loss: 20513.39
[INFO 2017-06-26 13:01:51,136 main.py:50] epoch 5758, training loss: 5573.45, average training loss: 5667.75, base loss: 20513.31
[INFO 2017-06-26 13:01:51,495 main.py:50] epoch 5759, training loss: 5616.85, average training loss: 5667.64, base loss: 20513.51
[INFO 2017-06-26 13:01:51,853 main.py:50] epoch 5760, training loss: 5553.73, average training loss: 5667.48, base loss: 20513.73
[INFO 2017-06-26 13:01:52,212 main.py:50] epoch 5761, training loss: 5620.33, average training loss: 5667.44, base loss: 20513.97
[INFO 2017-06-26 13:01:52,570 main.py:50] epoch 5762, training loss: 5660.37, average training loss: 5667.31, base loss: 20513.88
[INFO 2017-06-26 13:01:52,928 main.py:50] epoch 5763, training loss: 5564.04, average training loss: 5667.15, base loss: 20513.92
[INFO 2017-06-26 13:01:53,289 main.py:50] epoch 5764, training loss: 5604.84, average training loss: 5666.99, base loss: 20514.05
[INFO 2017-06-26 13:01:53,647 main.py:50] epoch 5765, training loss: 5600.77, average training loss: 5666.93, base loss: 20514.05
[INFO 2017-06-26 13:01:54,005 main.py:50] epoch 5766, training loss: 5552.74, average training loss: 5666.76, base loss: 20513.89
[INFO 2017-06-26 13:01:54,364 main.py:50] epoch 5767, training loss: 5584.66, average training loss: 5666.68, base loss: 20514.15
[INFO 2017-06-26 13:01:54,723 main.py:50] epoch 5768, training loss: 5601.77, average training loss: 5666.56, base loss: 20514.29
[INFO 2017-06-26 13:01:55,083 main.py:50] epoch 5769, training loss: 5597.57, average training loss: 5666.43, base loss: 20514.46
[INFO 2017-06-26 13:01:55,442 main.py:50] epoch 5770, training loss: 5557.95, average training loss: 5666.22, base loss: 20514.26
[INFO 2017-06-26 13:01:55,801 main.py:50] epoch 5771, training loss: 5553.81, average training loss: 5666.04, base loss: 20514.13
[INFO 2017-06-26 13:01:56,159 main.py:50] epoch 5772, training loss: 5604.21, average training loss: 5665.88, base loss: 20514.15
[INFO 2017-06-26 13:01:56,518 main.py:50] epoch 5773, training loss: 5539.94, average training loss: 5665.70, base loss: 20514.07
[INFO 2017-06-26 13:01:56,876 main.py:50] epoch 5774, training loss: 5579.87, average training loss: 5665.50, base loss: 20513.96
[INFO 2017-06-26 13:01:57,234 main.py:50] epoch 5775, training loss: 5576.56, average training loss: 5665.36, base loss: 20513.89
[INFO 2017-06-26 13:01:57,606 main.py:50] epoch 5776, training loss: 5555.22, average training loss: 5665.22, base loss: 20513.94
[INFO 2017-06-26 13:01:57,964 main.py:50] epoch 5777, training loss: 5560.38, average training loss: 5665.05, base loss: 20513.64
[INFO 2017-06-26 13:01:58,324 main.py:50] epoch 5778, training loss: 5598.53, average training loss: 5664.97, base loss: 20513.92
[INFO 2017-06-26 13:01:58,681 main.py:50] epoch 5779, training loss: 5573.02, average training loss: 5664.85, base loss: 20513.97
[INFO 2017-06-26 13:01:59,040 main.py:50] epoch 5780, training loss: 5596.24, average training loss: 5664.71, base loss: 20514.00
[INFO 2017-06-26 13:01:59,399 main.py:50] epoch 5781, training loss: 5616.33, average training loss: 5664.58, base loss: 20513.90
[INFO 2017-06-26 13:01:59,759 main.py:50] epoch 5782, training loss: 5565.63, average training loss: 5664.46, base loss: 20514.01
[INFO 2017-06-26 13:02:00,118 main.py:50] epoch 5783, training loss: 5580.73, average training loss: 5664.28, base loss: 20513.48
[INFO 2017-06-26 13:02:00,478 main.py:50] epoch 5784, training loss: 5562.46, average training loss: 5664.22, base loss: 20513.36
[INFO 2017-06-26 13:02:00,837 main.py:50] epoch 5785, training loss: 5599.41, average training loss: 5664.11, base loss: 20513.16
[INFO 2017-06-26 13:02:01,196 main.py:50] epoch 5786, training loss: 5588.21, average training loss: 5664.02, base loss: 20513.22
[INFO 2017-06-26 13:02:01,553 main.py:50] epoch 5787, training loss: 5609.52, average training loss: 5663.95, base loss: 20512.90
[INFO 2017-06-26 13:02:01,912 main.py:50] epoch 5788, training loss: 5626.73, average training loss: 5663.79, base loss: 20512.96
[INFO 2017-06-26 13:02:02,272 main.py:50] epoch 5789, training loss: 5564.22, average training loss: 5663.69, base loss: 20513.35
[INFO 2017-06-26 13:02:02,631 main.py:50] epoch 5790, training loss: 5619.50, average training loss: 5663.57, base loss: 20513.42
[INFO 2017-06-26 13:02:02,990 main.py:50] epoch 5791, training loss: 5580.97, average training loss: 5663.46, base loss: 20513.14
[INFO 2017-06-26 13:02:03,348 main.py:50] epoch 5792, training loss: 5625.15, average training loss: 5663.36, base loss: 20513.38
[INFO 2017-06-26 13:02:03,707 main.py:50] epoch 5793, training loss: 5616.63, average training loss: 5663.26, base loss: 20513.44
[INFO 2017-06-26 13:02:04,067 main.py:50] epoch 5794, training loss: 5626.85, average training loss: 5663.22, base loss: 20513.74
[INFO 2017-06-26 13:02:04,426 main.py:50] epoch 5795, training loss: 5671.59, average training loss: 5663.15, base loss: 20513.92
[INFO 2017-06-26 13:02:04,784 main.py:50] epoch 5796, training loss: 5642.93, average training loss: 5663.07, base loss: 20514.06
[INFO 2017-06-26 13:02:05,141 main.py:50] epoch 5797, training loss: 5639.46, average training loss: 5662.98, base loss: 20513.97
[INFO 2017-06-26 13:02:05,500 main.py:50] epoch 5798, training loss: 5517.00, average training loss: 5662.76, base loss: 20513.67
[INFO 2017-06-26 13:02:05,860 main.py:50] epoch 5799, training loss: 5636.91, average training loss: 5662.70, base loss: 20513.61
[INFO 2017-06-26 13:02:05,860 main.py:52] epoch 5799, testing
[INFO 2017-06-26 13:02:07,324 main.py:103] average testing loss: 5575.73, base loss: 20498.89
[INFO 2017-06-26 13:02:07,325 main.py:76] current best accuracy: 5574.58
[INFO 2017-06-26 13:02:07,683 main.py:50] epoch 5800, training loss: 5602.56, average training loss: 5662.65, base loss: 20513.60
[INFO 2017-06-26 13:02:08,042 main.py:50] epoch 5801, training loss: 5591.10, average training loss: 5662.47, base loss: 20513.79
[INFO 2017-06-26 13:02:08,401 main.py:50] epoch 5802, training loss: 5569.32, average training loss: 5662.40, base loss: 20513.94
[INFO 2017-06-26 13:02:08,760 main.py:50] epoch 5803, training loss: 5571.10, average training loss: 5662.25, base loss: 20513.72
[INFO 2017-06-26 13:02:09,119 main.py:50] epoch 5804, training loss: 5585.64, average training loss: 5662.12, base loss: 20513.69
[INFO 2017-06-26 13:02:09,479 main.py:50] epoch 5805, training loss: 5572.23, average training loss: 5661.96, base loss: 20513.69
[INFO 2017-06-26 13:02:09,838 main.py:50] epoch 5806, training loss: 5619.40, average training loss: 5661.85, base loss: 20513.69
[INFO 2017-06-26 13:02:10,196 main.py:50] epoch 5807, training loss: 5602.88, average training loss: 5661.80, base loss: 20514.17
[INFO 2017-06-26 13:02:10,555 main.py:50] epoch 5808, training loss: 5578.45, average training loss: 5661.73, base loss: 20514.10
[INFO 2017-06-26 13:02:10,914 main.py:50] epoch 5809, training loss: 5652.67, average training loss: 5661.65, base loss: 20514.13
[INFO 2017-06-26 13:02:11,273 main.py:50] epoch 5810, training loss: 5636.53, average training loss: 5661.61, base loss: 20514.46
[INFO 2017-06-26 13:02:11,630 main.py:50] epoch 5811, training loss: 5589.53, average training loss: 5661.52, base loss: 20514.66
[INFO 2017-06-26 13:02:11,989 main.py:50] epoch 5812, training loss: 5637.00, average training loss: 5661.43, base loss: 20514.41
[INFO 2017-06-26 13:02:12,348 main.py:50] epoch 5813, training loss: 5585.64, average training loss: 5661.37, base loss: 20514.39
[INFO 2017-06-26 13:02:12,707 main.py:50] epoch 5814, training loss: 5613.52, average training loss: 5661.28, base loss: 20513.95
[INFO 2017-06-26 13:02:13,065 main.py:50] epoch 5815, training loss: 5640.92, average training loss: 5661.26, base loss: 20513.92
[INFO 2017-06-26 13:02:13,426 main.py:50] epoch 5816, training loss: 5607.17, average training loss: 5661.16, base loss: 20513.90
[INFO 2017-06-26 13:02:13,784 main.py:50] epoch 5817, training loss: 5633.39, average training loss: 5661.10, base loss: 20513.76
[INFO 2017-06-26 13:02:14,143 main.py:50] epoch 5818, training loss: 5551.92, average training loss: 5660.92, base loss: 20513.32
[INFO 2017-06-26 13:02:14,502 main.py:50] epoch 5819, training loss: 5526.72, average training loss: 5660.73, base loss: 20513.50
[INFO 2017-06-26 13:02:14,861 main.py:50] epoch 5820, training loss: 5606.67, average training loss: 5660.63, base loss: 20513.25
[INFO 2017-06-26 13:02:15,220 main.py:50] epoch 5821, training loss: 5650.40, average training loss: 5660.59, base loss: 20513.23
[INFO 2017-06-26 13:02:15,579 main.py:50] epoch 5822, training loss: 5592.52, average training loss: 5660.52, base loss: 20513.55
[INFO 2017-06-26 13:02:15,936 main.py:50] epoch 5823, training loss: 5646.87, average training loss: 5660.46, base loss: 20513.84
[INFO 2017-06-26 13:02:16,296 main.py:50] epoch 5824, training loss: 5618.92, average training loss: 5660.34, base loss: 20513.91
[INFO 2017-06-26 13:02:16,655 main.py:50] epoch 5825, training loss: 5641.01, average training loss: 5660.30, base loss: 20513.84
[INFO 2017-06-26 13:02:17,014 main.py:50] epoch 5826, training loss: 5567.96, average training loss: 5660.20, base loss: 20513.99
[INFO 2017-06-26 13:02:17,373 main.py:50] epoch 5827, training loss: 5586.05, average training loss: 5660.11, base loss: 20514.31
[INFO 2017-06-26 13:02:17,732 main.py:50] epoch 5828, training loss: 5592.53, average training loss: 5660.03, base loss: 20514.39
[INFO 2017-06-26 13:02:18,092 main.py:50] epoch 5829, training loss: 5643.52, average training loss: 5660.01, base loss: 20514.72
[INFO 2017-06-26 13:02:18,451 main.py:50] epoch 5830, training loss: 5571.20, average training loss: 5659.96, base loss: 20514.86
[INFO 2017-06-26 13:02:18,810 main.py:50] epoch 5831, training loss: 5573.18, average training loss: 5659.75, base loss: 20514.86
[INFO 2017-06-26 13:02:19,167 main.py:50] epoch 5832, training loss: 5568.80, average training loss: 5659.65, base loss: 20515.00
[INFO 2017-06-26 13:02:19,527 main.py:50] epoch 5833, training loss: 5580.30, average training loss: 5659.46, base loss: 20515.27
[INFO 2017-06-26 13:02:19,886 main.py:50] epoch 5834, training loss: 5597.27, average training loss: 5659.40, base loss: 20515.38
[INFO 2017-06-26 13:02:20,245 main.py:50] epoch 5835, training loss: 5557.69, average training loss: 5659.27, base loss: 20515.52
[INFO 2017-06-26 13:02:20,604 main.py:50] epoch 5836, training loss: 5609.25, average training loss: 5659.14, base loss: 20515.64
[INFO 2017-06-26 13:02:20,963 main.py:50] epoch 5837, training loss: 5634.18, average training loss: 5659.06, base loss: 20515.56
[INFO 2017-06-26 13:02:21,320 main.py:50] epoch 5838, training loss: 5629.62, average training loss: 5658.96, base loss: 20515.79
[INFO 2017-06-26 13:02:21,679 main.py:50] epoch 5839, training loss: 5646.55, average training loss: 5658.96, base loss: 20516.22
[INFO 2017-06-26 13:02:22,039 main.py:50] epoch 5840, training loss: 5599.48, average training loss: 5658.81, base loss: 20515.98
[INFO 2017-06-26 13:02:22,398 main.py:50] epoch 5841, training loss: 5556.21, average training loss: 5658.67, base loss: 20515.76
[INFO 2017-06-26 13:02:22,757 main.py:50] epoch 5842, training loss: 5589.70, average training loss: 5658.52, base loss: 20515.89
[INFO 2017-06-26 13:02:23,115 main.py:50] epoch 5843, training loss: 5599.86, average training loss: 5658.39, base loss: 20515.83
[INFO 2017-06-26 13:02:23,474 main.py:50] epoch 5844, training loss: 5596.65, average training loss: 5658.28, base loss: 20515.67
[INFO 2017-06-26 13:02:23,833 main.py:50] epoch 5845, training loss: 5659.05, average training loss: 5658.24, base loss: 20515.72
[INFO 2017-06-26 13:02:24,192 main.py:50] epoch 5846, training loss: 5705.44, average training loss: 5658.26, base loss: 20515.91
[INFO 2017-06-26 13:02:24,552 main.py:50] epoch 5847, training loss: 5594.24, average training loss: 5658.17, base loss: 20515.82
[INFO 2017-06-26 13:02:24,910 main.py:50] epoch 5848, training loss: 5560.35, average training loss: 5657.98, base loss: 20515.55
[INFO 2017-06-26 13:02:25,269 main.py:50] epoch 5849, training loss: 5645.72, average training loss: 5657.88, base loss: 20515.56
[INFO 2017-06-26 13:02:25,628 main.py:50] epoch 5850, training loss: 5648.05, average training loss: 5657.89, base loss: 20515.93
[INFO 2017-06-26 13:02:25,987 main.py:50] epoch 5851, training loss: 5664.13, average training loss: 5657.85, base loss: 20516.03
[INFO 2017-06-26 13:02:26,346 main.py:50] epoch 5852, training loss: 5619.25, average training loss: 5657.74, base loss: 20516.14
[INFO 2017-06-26 13:02:26,705 main.py:50] epoch 5853, training loss: 5615.79, average training loss: 5657.63, base loss: 20515.98
[INFO 2017-06-26 13:02:27,063 main.py:50] epoch 5854, training loss: 5644.03, average training loss: 5657.56, base loss: 20515.55
[INFO 2017-06-26 13:02:27,422 main.py:50] epoch 5855, training loss: 5682.95, average training loss: 5657.58, base loss: 20515.52
[INFO 2017-06-26 13:02:27,782 main.py:50] epoch 5856, training loss: 5658.40, average training loss: 5657.53, base loss: 20515.28
[INFO 2017-06-26 13:02:28,141 main.py:50] epoch 5857, training loss: 5580.84, average training loss: 5657.41, base loss: 20515.15
[INFO 2017-06-26 13:02:28,498 main.py:50] epoch 5858, training loss: 5647.33, average training loss: 5657.36, base loss: 20515.23
[INFO 2017-06-26 13:02:28,856 main.py:50] epoch 5859, training loss: 5583.72, average training loss: 5657.23, base loss: 20515.52
[INFO 2017-06-26 13:02:29,215 main.py:50] epoch 5860, training loss: 5628.21, average training loss: 5657.13, base loss: 20515.50
[INFO 2017-06-26 13:02:29,574 main.py:50] epoch 5861, training loss: 5601.89, average training loss: 5657.01, base loss: 20515.65
[INFO 2017-06-26 13:02:29,934 main.py:50] epoch 5862, training loss: 5635.84, average training loss: 5657.00, base loss: 20515.99
[INFO 2017-06-26 13:02:30,294 main.py:50] epoch 5863, training loss: 5667.00, average training loss: 5656.86, base loss: 20515.95
[INFO 2017-06-26 13:02:30,653 main.py:50] epoch 5864, training loss: 5610.73, average training loss: 5656.78, base loss: 20515.92
[INFO 2017-06-26 13:02:31,012 main.py:50] epoch 5865, training loss: 5571.81, average training loss: 5656.64, base loss: 20515.34
[INFO 2017-06-26 13:02:31,371 main.py:50] epoch 5866, training loss: 5593.08, average training loss: 5656.54, base loss: 20515.72
[INFO 2017-06-26 13:02:31,729 main.py:50] epoch 5867, training loss: 5657.19, average training loss: 5656.47, base loss: 20515.76
[INFO 2017-06-26 13:02:32,089 main.py:50] epoch 5868, training loss: 5598.85, average training loss: 5656.38, base loss: 20515.98
[INFO 2017-06-26 13:02:32,448 main.py:50] epoch 5869, training loss: 5563.97, average training loss: 5656.24, base loss: 20516.16
[INFO 2017-06-26 13:02:32,807 main.py:50] epoch 5870, training loss: 5563.98, average training loss: 5656.02, base loss: 20516.44
[INFO 2017-06-26 13:02:33,165 main.py:50] epoch 5871, training loss: 5699.34, average training loss: 5656.05, base loss: 20516.55
[INFO 2017-06-26 13:02:33,523 main.py:50] epoch 5872, training loss: 5594.58, average training loss: 5655.87, base loss: 20517.06
[INFO 2017-06-26 13:02:33,884 main.py:50] epoch 5873, training loss: 5573.93, average training loss: 5655.75, base loss: 20517.28
[INFO 2017-06-26 13:02:34,242 main.py:50] epoch 5874, training loss: 5574.42, average training loss: 5655.56, base loss: 20517.41
[INFO 2017-06-26 13:02:34,602 main.py:50] epoch 5875, training loss: 5573.21, average training loss: 5655.41, base loss: 20517.13
[INFO 2017-06-26 13:02:34,961 main.py:50] epoch 5876, training loss: 5601.70, average training loss: 5655.25, base loss: 20517.24
[INFO 2017-06-26 13:02:35,320 main.py:50] epoch 5877, training loss: 5648.59, average training loss: 5655.17, base loss: 20517.32
[INFO 2017-06-26 13:02:35,679 main.py:50] epoch 5878, training loss: 5585.11, average training loss: 5655.00, base loss: 20517.49
[INFO 2017-06-26 13:02:36,037 main.py:50] epoch 5879, training loss: 5681.46, average training loss: 5654.94, base loss: 20517.85
[INFO 2017-06-26 13:02:36,395 main.py:50] epoch 5880, training loss: 5559.09, average training loss: 5654.78, base loss: 20518.46
[INFO 2017-06-26 13:02:36,754 main.py:50] epoch 5881, training loss: 5624.17, average training loss: 5654.61, base loss: 20518.51
[INFO 2017-06-26 13:02:37,112 main.py:50] epoch 5882, training loss: 5567.38, average training loss: 5654.47, base loss: 20518.58
[INFO 2017-06-26 13:02:37,472 main.py:50] epoch 5883, training loss: 5588.68, average training loss: 5654.28, base loss: 20518.38
[INFO 2017-06-26 13:02:37,833 main.py:50] epoch 5884, training loss: 5646.32, average training loss: 5654.25, base loss: 20518.13
[INFO 2017-06-26 13:02:38,192 main.py:50] epoch 5885, training loss: 5567.70, average training loss: 5654.12, base loss: 20518.28
[INFO 2017-06-26 13:02:38,551 main.py:50] epoch 5886, training loss: 5667.20, average training loss: 5654.12, base loss: 20518.45
[INFO 2017-06-26 13:02:38,910 main.py:50] epoch 5887, training loss: 5677.78, average training loss: 5654.02, base loss: 20518.68
[INFO 2017-06-26 13:02:39,269 main.py:50] epoch 5888, training loss: 5643.85, average training loss: 5653.95, base loss: 20518.62
[INFO 2017-06-26 13:02:39,629 main.py:50] epoch 5889, training loss: 5664.65, average training loss: 5653.93, base loss: 20518.72
[INFO 2017-06-26 13:02:39,989 main.py:50] epoch 5890, training loss: 5543.60, average training loss: 5653.80, base loss: 20518.72
[INFO 2017-06-26 13:02:40,348 main.py:50] epoch 5891, training loss: 5622.55, average training loss: 5653.77, base loss: 20519.10
[INFO 2017-06-26 13:02:40,706 main.py:50] epoch 5892, training loss: 5623.20, average training loss: 5653.66, base loss: 20518.96
[INFO 2017-06-26 13:02:41,065 main.py:50] epoch 5893, training loss: 5632.76, average training loss: 5653.58, base loss: 20519.52
[INFO 2017-06-26 13:02:41,426 main.py:50] epoch 5894, training loss: 5598.01, average training loss: 5653.51, base loss: 20519.76
[INFO 2017-06-26 13:02:41,783 main.py:50] epoch 5895, training loss: 5621.18, average training loss: 5653.48, base loss: 20519.94
[INFO 2017-06-26 13:02:42,154 main.py:50] epoch 5896, training loss: 5553.01, average training loss: 5653.30, base loss: 20519.71
[INFO 2017-06-26 13:02:42,514 main.py:50] epoch 5897, training loss: 5610.99, average training loss: 5653.19, base loss: 20519.68
[INFO 2017-06-26 13:02:42,873 main.py:50] epoch 5898, training loss: 5607.42, average training loss: 5653.11, base loss: 20520.23
[INFO 2017-06-26 13:02:43,231 main.py:50] epoch 5899, training loss: 5661.00, average training loss: 5653.13, base loss: 20520.32
[INFO 2017-06-26 13:02:43,232 main.py:52] epoch 5899, testing
[INFO 2017-06-26 13:02:44,701 main.py:103] average testing loss: 5598.17, base loss: 20611.83
[INFO 2017-06-26 13:02:44,702 main.py:76] current best accuracy: 5574.58
[INFO 2017-06-26 13:02:45,061 main.py:50] epoch 5900, training loss: 5631.29, average training loss: 5653.07, base loss: 20520.61
[INFO 2017-06-26 13:02:45,421 main.py:50] epoch 5901, training loss: 5639.53, average training loss: 5653.06, base loss: 20521.06
[INFO 2017-06-26 13:02:45,780 main.py:50] epoch 5902, training loss: 5565.51, average training loss: 5652.97, base loss: 20521.04
[INFO 2017-06-26 13:02:46,139 main.py:50] epoch 5903, training loss: 5625.78, average training loss: 5652.91, base loss: 20521.11
[INFO 2017-06-26 13:02:46,498 main.py:50] epoch 5904, training loss: 5622.25, average training loss: 5652.86, base loss: 20521.19
[INFO 2017-06-26 13:02:46,858 main.py:50] epoch 5905, training loss: 5644.38, average training loss: 5652.82, base loss: 20521.10
[INFO 2017-06-26 13:02:47,217 main.py:50] epoch 5906, training loss: 5542.77, average training loss: 5652.65, base loss: 20520.69
[INFO 2017-06-26 13:02:47,577 main.py:50] epoch 5907, training loss: 5625.41, average training loss: 5652.63, base loss: 20520.45
[INFO 2017-06-26 13:02:47,936 main.py:50] epoch 5908, training loss: 5585.28, average training loss: 5652.55, base loss: 20520.63
[INFO 2017-06-26 13:02:48,295 main.py:50] epoch 5909, training loss: 5646.70, average training loss: 5652.54, base loss: 20520.70
[INFO 2017-06-26 13:02:48,654 main.py:50] epoch 5910, training loss: 5572.75, average training loss: 5652.44, base loss: 20520.87
[INFO 2017-06-26 13:02:49,013 main.py:50] epoch 5911, training loss: 5582.19, average training loss: 5652.37, base loss: 20520.83
[INFO 2017-06-26 13:02:49,373 main.py:50] epoch 5912, training loss: 5583.74, average training loss: 5652.24, base loss: 20520.43
[INFO 2017-06-26 13:02:49,731 main.py:50] epoch 5913, training loss: 5547.11, average training loss: 5652.14, base loss: 20520.45
[INFO 2017-06-26 13:02:50,091 main.py:50] epoch 5914, training loss: 5592.97, average training loss: 5652.11, base loss: 20520.43
[INFO 2017-06-26 13:02:50,450 main.py:50] epoch 5915, training loss: 5622.43, average training loss: 5652.02, base loss: 20520.05
[INFO 2017-06-26 13:02:50,810 main.py:50] epoch 5916, training loss: 5655.44, average training loss: 5651.98, base loss: 20519.76
[INFO 2017-06-26 13:02:51,168 main.py:50] epoch 5917, training loss: 5548.81, average training loss: 5651.78, base loss: 20519.70
[INFO 2017-06-26 13:02:51,527 main.py:50] epoch 5918, training loss: 5580.21, average training loss: 5651.65, base loss: 20519.50
[INFO 2017-06-26 13:02:51,886 main.py:50] epoch 5919, training loss: 5575.55, average training loss: 5651.51, base loss: 20519.50
[INFO 2017-06-26 13:02:52,246 main.py:50] epoch 5920, training loss: 5644.65, average training loss: 5651.47, base loss: 20519.27
[INFO 2017-06-26 13:02:52,604 main.py:50] epoch 5921, training loss: 5604.07, average training loss: 5651.39, base loss: 20519.44
[INFO 2017-06-26 13:02:52,963 main.py:50] epoch 5922, training loss: 5590.26, average training loss: 5651.36, base loss: 20519.15
[INFO 2017-06-26 13:02:53,321 main.py:50] epoch 5923, training loss: 5584.74, average training loss: 5651.27, base loss: 20519.09
[INFO 2017-06-26 13:02:53,680 main.py:50] epoch 5924, training loss: 5642.87, average training loss: 5651.21, base loss: 20519.28
[INFO 2017-06-26 13:02:54,039 main.py:50] epoch 5925, training loss: 5566.58, average training loss: 5651.17, base loss: 20519.54
[INFO 2017-06-26 13:02:54,399 main.py:50] epoch 5926, training loss: 5527.56, average training loss: 5651.05, base loss: 20519.30
[INFO 2017-06-26 13:02:54,758 main.py:50] epoch 5927, training loss: 5572.02, average training loss: 5650.96, base loss: 20519.32
[INFO 2017-06-26 13:02:55,115 main.py:50] epoch 5928, training loss: 5628.41, average training loss: 5650.98, base loss: 20519.26
[INFO 2017-06-26 13:02:55,475 main.py:50] epoch 5929, training loss: 5560.73, average training loss: 5650.87, base loss: 20519.26
[INFO 2017-06-26 13:02:55,834 main.py:50] epoch 5930, training loss: 5590.30, average training loss: 5650.69, base loss: 20519.14
[INFO 2017-06-26 13:02:56,192 main.py:50] epoch 5931, training loss: 5562.26, average training loss: 5650.61, base loss: 20519.16
[INFO 2017-06-26 13:02:56,551 main.py:50] epoch 5932, training loss: 5652.81, average training loss: 5650.58, base loss: 20519.21
[INFO 2017-06-26 13:02:56,910 main.py:50] epoch 5933, training loss: 5575.77, average training loss: 5650.45, base loss: 20519.19
[INFO 2017-06-26 13:02:57,269 main.py:50] epoch 5934, training loss: 5593.00, average training loss: 5650.34, base loss: 20519.59
[INFO 2017-06-26 13:02:57,628 main.py:50] epoch 5935, training loss: 5605.41, average training loss: 5650.23, base loss: 20519.94
[INFO 2017-06-26 13:02:57,987 main.py:50] epoch 5936, training loss: 5609.98, average training loss: 5650.14, base loss: 20519.75
[INFO 2017-06-26 13:02:58,346 main.py:50] epoch 5937, training loss: 5631.47, average training loss: 5650.06, base loss: 20519.58
[INFO 2017-06-26 13:02:58,706 main.py:50] epoch 5938, training loss: 5572.04, average training loss: 5649.94, base loss: 20519.76
[INFO 2017-06-26 13:02:59,065 main.py:50] epoch 5939, training loss: 5644.09, average training loss: 5649.91, base loss: 20519.79
[INFO 2017-06-26 13:02:59,425 main.py:50] epoch 5940, training loss: 5594.11, average training loss: 5649.86, base loss: 20519.38
[INFO 2017-06-26 13:02:59,783 main.py:50] epoch 5941, training loss: 5682.66, average training loss: 5649.87, base loss: 20519.48
[INFO 2017-06-26 13:03:00,142 main.py:50] epoch 5942, training loss: 5594.60, average training loss: 5649.77, base loss: 20519.38
[INFO 2017-06-26 13:03:00,501 main.py:50] epoch 5943, training loss: 5640.68, average training loss: 5649.80, base loss: 20519.37
[INFO 2017-06-26 13:03:00,858 main.py:50] epoch 5944, training loss: 5589.44, average training loss: 5649.74, base loss: 20519.19
[INFO 2017-06-26 13:03:01,218 main.py:50] epoch 5945, training loss: 5637.34, average training loss: 5649.73, base loss: 20518.92
[INFO 2017-06-26 13:03:01,576 main.py:50] epoch 5946, training loss: 5589.36, average training loss: 5649.63, base loss: 20518.55
[INFO 2017-06-26 13:03:01,936 main.py:50] epoch 5947, training loss: 5574.40, average training loss: 5649.53, base loss: 20518.70
[INFO 2017-06-26 13:03:02,294 main.py:50] epoch 5948, training loss: 5589.73, average training loss: 5649.41, base loss: 20518.81
[INFO 2017-06-26 13:03:02,651 main.py:50] epoch 5949, training loss: 5644.39, average training loss: 5649.33, base loss: 20518.58
[INFO 2017-06-26 13:03:03,010 main.py:50] epoch 5950, training loss: 5612.39, average training loss: 5649.32, base loss: 20518.81
[INFO 2017-06-26 13:03:03,370 main.py:50] epoch 5951, training loss: 5572.18, average training loss: 5649.25, base loss: 20518.69
[INFO 2017-06-26 13:03:03,728 main.py:50] epoch 5952, training loss: 5593.62, average training loss: 5649.08, base loss: 20518.73
[INFO 2017-06-26 13:03:04,087 main.py:50] epoch 5953, training loss: 5605.09, average training loss: 5648.96, base loss: 20518.71
[INFO 2017-06-26 13:03:04,447 main.py:50] epoch 5954, training loss: 5573.76, average training loss: 5648.89, base loss: 20519.02
[INFO 2017-06-26 13:03:04,806 main.py:50] epoch 5955, training loss: 5568.31, average training loss: 5648.70, base loss: 20519.09
[INFO 2017-06-26 13:03:05,165 main.py:50] epoch 5956, training loss: 5520.62, average training loss: 5648.52, base loss: 20518.90
[INFO 2017-06-26 13:03:05,524 main.py:50] epoch 5957, training loss: 5574.07, average training loss: 5648.39, base loss: 20518.79
[INFO 2017-06-26 13:03:05,881 main.py:50] epoch 5958, training loss: 5557.28, average training loss: 5648.23, base loss: 20518.98
[INFO 2017-06-26 13:03:06,239 main.py:50] epoch 5959, training loss: 5498.52, average training loss: 5648.00, base loss: 20518.66
[INFO 2017-06-26 13:03:06,597 main.py:50] epoch 5960, training loss: 5553.55, average training loss: 5647.81, base loss: 20518.81
[INFO 2017-06-26 13:03:06,956 main.py:50] epoch 5961, training loss: 5544.05, average training loss: 5647.68, base loss: 20519.29
[INFO 2017-06-26 13:03:07,314 main.py:50] epoch 5962, training loss: 5555.22, average training loss: 5647.57, base loss: 20519.24
[INFO 2017-06-26 13:03:07,674 main.py:50] epoch 5963, training loss: 5517.98, average training loss: 5647.51, base loss: 20519.72
[INFO 2017-06-26 13:03:08,032 main.py:50] epoch 5964, training loss: 5599.55, average training loss: 5647.43, base loss: 20519.67
[INFO 2017-06-26 13:03:08,392 main.py:50] epoch 5965, training loss: 5525.01, average training loss: 5647.32, base loss: 20519.89
[INFO 2017-06-26 13:03:08,750 main.py:50] epoch 5966, training loss: 5555.76, average training loss: 5647.20, base loss: 20519.84
[INFO 2017-06-26 13:03:09,109 main.py:50] epoch 5967, training loss: 5539.66, average training loss: 5647.16, base loss: 20520.29
[INFO 2017-06-26 13:03:09,468 main.py:50] epoch 5968, training loss: 5599.93, average training loss: 5647.17, base loss: 20520.48
[INFO 2017-06-26 13:03:09,827 main.py:50] epoch 5969, training loss: 5597.28, average training loss: 5647.01, base loss: 20520.23
[INFO 2017-06-26 13:03:10,185 main.py:50] epoch 5970, training loss: 5695.03, average training loss: 5646.96, base loss: 20520.52
[INFO 2017-06-26 13:03:10,545 main.py:50] epoch 5971, training loss: 5552.71, average training loss: 5646.82, base loss: 20520.17
[INFO 2017-06-26 13:03:10,904 main.py:50] epoch 5972, training loss: 5648.67, average training loss: 5646.80, base loss: 20520.34
[INFO 2017-06-26 13:03:11,262 main.py:50] epoch 5973, training loss: 5556.16, average training loss: 5646.73, base loss: 20520.61
[INFO 2017-06-26 13:03:11,621 main.py:50] epoch 5974, training loss: 5598.88, average training loss: 5646.62, base loss: 20520.49
[INFO 2017-06-26 13:03:11,980 main.py:50] epoch 5975, training loss: 5538.94, average training loss: 5646.43, base loss: 20520.62
[INFO 2017-06-26 13:03:12,339 main.py:50] epoch 5976, training loss: 5565.49, average training loss: 5646.36, base loss: 20520.35
[INFO 2017-06-26 13:03:12,698 main.py:50] epoch 5977, training loss: 5585.40, average training loss: 5646.23, base loss: 20520.37
[INFO 2017-06-26 13:03:13,055 main.py:50] epoch 5978, training loss: 5565.29, average training loss: 5646.14, base loss: 20520.61
[INFO 2017-06-26 13:03:13,416 main.py:50] epoch 5979, training loss: 5565.34, average training loss: 5646.02, base loss: 20520.54
[INFO 2017-06-26 13:03:13,774 main.py:50] epoch 5980, training loss: 5548.98, average training loss: 5645.87, base loss: 20520.62
[INFO 2017-06-26 13:03:14,133 main.py:50] epoch 5981, training loss: 5545.92, average training loss: 5645.75, base loss: 20521.03
[INFO 2017-06-26 13:03:14,492 main.py:50] epoch 5982, training loss: 5578.28, average training loss: 5645.61, base loss: 20520.98
[INFO 2017-06-26 13:03:14,850 main.py:50] epoch 5983, training loss: 5556.13, average training loss: 5645.49, base loss: 20521.16
[INFO 2017-06-26 13:03:15,210 main.py:50] epoch 5984, training loss: 5601.79, average training loss: 5645.39, base loss: 20521.03
[INFO 2017-06-26 13:03:15,568 main.py:50] epoch 5985, training loss: 5573.91, average training loss: 5645.26, base loss: 20520.99
[INFO 2017-06-26 13:03:15,926 main.py:50] epoch 5986, training loss: 5578.93, average training loss: 5645.12, base loss: 20520.77
[INFO 2017-06-26 13:03:16,284 main.py:50] epoch 5987, training loss: 5546.20, average training loss: 5644.98, base loss: 20520.91
[INFO 2017-06-26 13:03:16,644 main.py:50] epoch 5988, training loss: 5515.03, average training loss: 5644.76, base loss: 20520.67
[INFO 2017-06-26 13:03:17,003 main.py:50] epoch 5989, training loss: 5604.44, average training loss: 5644.63, base loss: 20521.00
[INFO 2017-06-26 13:03:17,361 main.py:50] epoch 5990, training loss: 5574.69, average training loss: 5644.51, base loss: 20520.80
[INFO 2017-06-26 13:03:17,719 main.py:50] epoch 5991, training loss: 5633.64, average training loss: 5644.42, base loss: 20520.54
[INFO 2017-06-26 13:03:18,079 main.py:50] epoch 5992, training loss: 5578.45, average training loss: 5644.31, base loss: 20520.59
[INFO 2017-06-26 13:03:18,438 main.py:50] epoch 5993, training loss: 5545.93, average training loss: 5644.19, base loss: 20520.83
[INFO 2017-06-26 13:03:18,796 main.py:50] epoch 5994, training loss: 5556.10, average training loss: 5644.07, base loss: 20520.86
[INFO 2017-06-26 13:03:19,155 main.py:50] epoch 5995, training loss: 5547.55, average training loss: 5643.91, base loss: 20520.60
[INFO 2017-06-26 13:03:19,513 main.py:50] epoch 5996, training loss: 5610.81, average training loss: 5643.82, base loss: 20520.56
[INFO 2017-06-26 13:03:19,872 main.py:50] epoch 5997, training loss: 5635.26, average training loss: 5643.70, base loss: 20520.84
[INFO 2017-06-26 13:03:20,231 main.py:50] epoch 5998, training loss: 5603.42, average training loss: 5643.60, base loss: 20520.85
[INFO 2017-06-26 13:03:20,588 main.py:50] epoch 5999, training loss: 5583.57, average training loss: 5643.43, base loss: 20520.23
[INFO 2017-06-26 13:03:20,588 main.py:52] epoch 5999, testing
[INFO 2017-06-26 13:03:22,054 main.py:103] average testing loss: 5605.80, base loss: 20533.71
[INFO 2017-06-26 13:03:22,054 main.py:76] current best accuracy: 5574.58
[INFO 2017-06-26 13:03:22,413 main.py:50] epoch 6000, training loss: 5590.12, average training loss: 5643.34, base loss: 20520.19
[INFO 2017-06-26 13:03:22,773 main.py:50] epoch 6001, training loss: 5554.39, average training loss: 5643.19, base loss: 20520.26
[INFO 2017-06-26 13:03:23,131 main.py:50] epoch 6002, training loss: 5555.63, average training loss: 5643.06, base loss: 20519.95
[INFO 2017-06-26 13:03:23,491 main.py:50] epoch 6003, training loss: 5627.77, average training loss: 5643.00, base loss: 20520.27
[INFO 2017-06-26 13:03:23,863 main.py:50] epoch 6004, training loss: 5541.91, average training loss: 5642.89, base loss: 20520.42
[INFO 2017-06-26 13:03:24,223 main.py:50] epoch 6005, training loss: 5589.65, average training loss: 5642.81, base loss: 20520.53
[INFO 2017-06-26 13:03:24,583 main.py:50] epoch 6006, training loss: 5516.06, average training loss: 5642.63, base loss: 20520.97
[INFO 2017-06-26 13:03:24,944 main.py:50] epoch 6007, training loss: 5579.50, average training loss: 5642.55, base loss: 20521.05
[INFO 2017-06-26 13:03:25,302 main.py:50] epoch 6008, training loss: 5555.68, average training loss: 5642.44, base loss: 20520.77
[INFO 2017-06-26 13:03:25,661 main.py:50] epoch 6009, training loss: 5570.76, average training loss: 5642.31, base loss: 20520.91
[INFO 2017-06-26 13:03:26,021 main.py:50] epoch 6010, training loss: 5545.90, average training loss: 5642.18, base loss: 20520.63
[INFO 2017-06-26 13:03:26,380 main.py:50] epoch 6011, training loss: 5547.13, average training loss: 5642.04, base loss: 20520.88
[INFO 2017-06-26 13:03:26,740 main.py:50] epoch 6012, training loss: 5579.94, average training loss: 5641.91, base loss: 20520.83
[INFO 2017-06-26 13:03:27,100 main.py:50] epoch 6013, training loss: 5512.05, average training loss: 5641.69, base loss: 20520.40
[INFO 2017-06-26 13:03:27,462 main.py:50] epoch 6014, training loss: 5618.39, average training loss: 5641.59, base loss: 20520.69
[INFO 2017-06-26 13:03:27,823 main.py:50] epoch 6015, training loss: 5575.62, average training loss: 5641.54, base loss: 20520.79
[INFO 2017-06-26 13:03:28,183 main.py:50] epoch 6016, training loss: 5591.98, average training loss: 5641.50, base loss: 20521.10
[INFO 2017-06-26 13:03:28,541 main.py:50] epoch 6017, training loss: 5672.87, average training loss: 5641.51, base loss: 20521.28
[INFO 2017-06-26 13:03:28,903 main.py:50] epoch 6018, training loss: 5582.48, average training loss: 5641.39, base loss: 20521.01
[INFO 2017-06-26 13:03:29,261 main.py:50] epoch 6019, training loss: 5600.57, average training loss: 5641.34, base loss: 20520.99
[INFO 2017-06-26 13:03:29,621 main.py:50] epoch 6020, training loss: 5520.23, average training loss: 5641.14, base loss: 20521.17
[INFO 2017-06-26 13:03:29,980 main.py:50] epoch 6021, training loss: 5641.07, average training loss: 5641.09, base loss: 20520.94
[INFO 2017-06-26 13:03:30,340 main.py:50] epoch 6022, training loss: 5490.99, average training loss: 5640.88, base loss: 20520.47
[INFO 2017-06-26 13:03:30,699 main.py:50] epoch 6023, training loss: 5591.41, average training loss: 5640.83, base loss: 20520.55
[INFO 2017-06-26 13:03:31,059 main.py:50] epoch 6024, training loss: 5640.92, average training loss: 5640.76, base loss: 20521.02
[INFO 2017-06-26 13:03:31,419 main.py:50] epoch 6025, training loss: 5601.23, average training loss: 5640.65, base loss: 20520.91
[INFO 2017-06-26 13:03:31,780 main.py:50] epoch 6026, training loss: 5599.87, average training loss: 5640.52, base loss: 20521.25
[INFO 2017-06-26 13:03:32,138 main.py:50] epoch 6027, training loss: 5571.70, average training loss: 5640.46, base loss: 20521.20
[INFO 2017-06-26 13:03:32,498 main.py:50] epoch 6028, training loss: 5679.82, average training loss: 5640.47, base loss: 20521.63
[INFO 2017-06-26 13:03:32,859 main.py:50] epoch 6029, training loss: 5607.92, average training loss: 5640.42, base loss: 20520.92
[INFO 2017-06-26 13:03:33,219 main.py:50] epoch 6030, training loss: 5578.47, average training loss: 5640.31, base loss: 20520.96
[INFO 2017-06-26 13:03:33,579 main.py:50] epoch 6031, training loss: 5631.47, average training loss: 5640.22, base loss: 20520.70
[INFO 2017-06-26 13:03:33,939 main.py:50] epoch 6032, training loss: 5594.74, average training loss: 5640.11, base loss: 20520.51
[INFO 2017-06-26 13:03:34,298 main.py:50] epoch 6033, training loss: 5636.82, average training loss: 5640.03, base loss: 20520.61
[INFO 2017-06-26 13:03:34,658 main.py:50] epoch 6034, training loss: 5548.18, average training loss: 5639.88, base loss: 20520.89
[INFO 2017-06-26 13:03:35,017 main.py:50] epoch 6035, training loss: 5621.98, average training loss: 5639.81, base loss: 20521.35
[INFO 2017-06-26 13:03:35,377 main.py:50] epoch 6036, training loss: 5552.07, average training loss: 5639.70, base loss: 20521.42
[INFO 2017-06-26 13:03:35,736 main.py:50] epoch 6037, training loss: 5637.74, average training loss: 5639.68, base loss: 20521.74
[INFO 2017-06-26 13:03:36,097 main.py:50] epoch 6038, training loss: 5595.02, average training loss: 5639.55, base loss: 20521.81
[INFO 2017-06-26 13:03:36,456 main.py:50] epoch 6039, training loss: 5560.00, average training loss: 5639.29, base loss: 20521.41
[INFO 2017-06-26 13:03:36,816 main.py:50] epoch 6040, training loss: 5525.76, average training loss: 5639.06, base loss: 20521.45
[INFO 2017-06-26 13:03:37,176 main.py:50] epoch 6041, training loss: 5608.80, average training loss: 5638.93, base loss: 20521.57
[INFO 2017-06-26 13:03:37,535 main.py:50] epoch 6042, training loss: 5572.02, average training loss: 5638.74, base loss: 20521.13
[INFO 2017-06-26 13:03:37,896 main.py:50] epoch 6043, training loss: 5600.58, average training loss: 5638.64, base loss: 20521.05
[INFO 2017-06-26 13:03:38,254 main.py:50] epoch 6044, training loss: 5608.78, average training loss: 5638.47, base loss: 20520.96
[INFO 2017-06-26 13:03:38,613 main.py:50] epoch 6045, training loss: 5606.53, average training loss: 5638.29, base loss: 20520.78
[INFO 2017-06-26 13:03:38,975 main.py:50] epoch 6046, training loss: 5598.32, average training loss: 5638.20, base loss: 20520.72
[INFO 2017-06-26 13:03:39,334 main.py:50] epoch 6047, training loss: 5676.79, average training loss: 5638.21, base loss: 20520.68
[INFO 2017-06-26 13:03:39,693 main.py:50] epoch 6048, training loss: 5648.27, average training loss: 5638.11, base loss: 20520.18
[INFO 2017-06-26 13:03:40,052 main.py:50] epoch 6049, training loss: 5536.41, average training loss: 5637.90, base loss: 20520.20
[INFO 2017-06-26 13:03:40,412 main.py:50] epoch 6050, training loss: 5632.54, average training loss: 5637.84, base loss: 20520.61
[INFO 2017-06-26 13:03:40,773 main.py:50] epoch 6051, training loss: 5632.77, average training loss: 5637.81, base loss: 20520.49
[INFO 2017-06-26 13:03:41,131 main.py:50] epoch 6052, training loss: 5531.89, average training loss: 5637.64, base loss: 20520.55
[INFO 2017-06-26 13:03:41,491 main.py:50] epoch 6053, training loss: 5626.25, average training loss: 5637.58, base loss: 20520.84
[INFO 2017-06-26 13:03:41,850 main.py:50] epoch 6054, training loss: 5608.33, average training loss: 5637.48, base loss: 20520.88
[INFO 2017-06-26 13:03:42,211 main.py:50] epoch 6055, training loss: 5636.22, average training loss: 5637.43, base loss: 20521.04
[INFO 2017-06-26 13:03:42,570 main.py:50] epoch 6056, training loss: 5685.48, average training loss: 5637.35, base loss: 20520.90
[INFO 2017-06-26 13:03:42,929 main.py:50] epoch 6057, training loss: 5625.44, average training loss: 5637.26, base loss: 20521.12
[INFO 2017-06-26 13:03:43,290 main.py:50] epoch 6058, training loss: 5598.18, average training loss: 5637.21, base loss: 20520.74
[INFO 2017-06-26 13:03:43,650 main.py:50] epoch 6059, training loss: 5621.81, average training loss: 5637.09, base loss: 20520.88
[INFO 2017-06-26 13:03:44,009 main.py:50] epoch 6060, training loss: 5569.16, average training loss: 5636.98, base loss: 20520.57
[INFO 2017-06-26 13:03:44,369 main.py:50] epoch 6061, training loss: 5587.14, average training loss: 5636.85, base loss: 20520.55
[INFO 2017-06-26 13:03:44,730 main.py:50] epoch 6062, training loss: 5582.55, average training loss: 5636.81, base loss: 20520.62
[INFO 2017-06-26 13:03:45,090 main.py:50] epoch 6063, training loss: 5536.14, average training loss: 5636.62, base loss: 20520.43
[INFO 2017-06-26 13:03:45,451 main.py:50] epoch 6064, training loss: 5595.36, average training loss: 5636.57, base loss: 20520.79
[INFO 2017-06-26 13:03:45,810 main.py:50] epoch 6065, training loss: 5621.48, average training loss: 5636.57, base loss: 20520.89
[INFO 2017-06-26 13:03:46,170 main.py:50] epoch 6066, training loss: 5556.79, average training loss: 5636.40, base loss: 20520.84
[INFO 2017-06-26 13:03:46,529 main.py:50] epoch 6067, training loss: 5572.01, average training loss: 5636.28, base loss: 20520.82
[INFO 2017-06-26 13:03:46,889 main.py:50] epoch 6068, training loss: 5592.34, average training loss: 5636.16, base loss: 20521.01
[INFO 2017-06-26 13:03:47,249 main.py:50] epoch 6069, training loss: 5630.10, average training loss: 5636.12, base loss: 20521.03
[INFO 2017-06-26 13:03:47,609 main.py:50] epoch 6070, training loss: 5587.68, average training loss: 5636.09, base loss: 20520.62
[INFO 2017-06-26 13:03:47,968 main.py:50] epoch 6071, training loss: 5705.94, average training loss: 5636.14, base loss: 20520.78
[INFO 2017-06-26 13:03:48,327 main.py:50] epoch 6072, training loss: 5648.70, average training loss: 5636.08, base loss: 20520.56
[INFO 2017-06-26 13:03:48,686 main.py:50] epoch 6073, training loss: 5638.42, average training loss: 5636.00, base loss: 20520.18
[INFO 2017-06-26 13:03:49,046 main.py:50] epoch 6074, training loss: 5586.90, average training loss: 5635.93, base loss: 20520.43
[INFO 2017-06-26 13:03:49,407 main.py:50] epoch 6075, training loss: 5596.84, average training loss: 5635.87, base loss: 20520.45
[INFO 2017-06-26 13:03:49,766 main.py:50] epoch 6076, training loss: 5633.90, average training loss: 5635.74, base loss: 20520.43
[INFO 2017-06-26 13:03:50,125 main.py:50] epoch 6077, training loss: 5584.23, average training loss: 5635.64, base loss: 20520.69
[INFO 2017-06-26 13:03:50,486 main.py:50] epoch 6078, training loss: 5608.55, average training loss: 5635.48, base loss: 20520.78
[INFO 2017-06-26 13:03:50,845 main.py:50] epoch 6079, training loss: 5581.96, average training loss: 5635.35, base loss: 20520.57
[INFO 2017-06-26 13:03:51,206 main.py:50] epoch 6080, training loss: 5579.20, average training loss: 5635.26, base loss: 20520.58
[INFO 2017-06-26 13:03:51,566 main.py:50] epoch 6081, training loss: 5545.50, average training loss: 5635.16, base loss: 20520.55
[INFO 2017-06-26 13:03:51,925 main.py:50] epoch 6082, training loss: 5637.28, average training loss: 5635.18, base loss: 20520.35
[INFO 2017-06-26 13:03:52,284 main.py:50] epoch 6083, training loss: 5627.17, average training loss: 5635.16, base loss: 20520.63
[INFO 2017-06-26 13:03:52,646 main.py:50] epoch 6084, training loss: 5602.46, average training loss: 5635.06, base loss: 20520.44
[INFO 2017-06-26 13:03:53,005 main.py:50] epoch 6085, training loss: 5605.18, average training loss: 5635.00, base loss: 20519.97
[INFO 2017-06-26 13:03:53,365 main.py:50] epoch 6086, training loss: 5625.57, average training loss: 5634.95, base loss: 20520.10
[INFO 2017-06-26 13:03:53,725 main.py:50] epoch 6087, training loss: 5541.13, average training loss: 5634.79, base loss: 20519.91
[INFO 2017-06-26 13:03:54,085 main.py:50] epoch 6088, training loss: 5604.49, average training loss: 5634.66, base loss: 20519.67
[INFO 2017-06-26 13:03:54,445 main.py:50] epoch 6089, training loss: 5592.34, average training loss: 5634.58, base loss: 20519.52
[INFO 2017-06-26 13:03:54,805 main.py:50] epoch 6090, training loss: 5581.84, average training loss: 5634.49, base loss: 20519.31
[INFO 2017-06-26 13:03:55,165 main.py:50] epoch 6091, training loss: 5609.36, average training loss: 5634.42, base loss: 20519.60
[INFO 2017-06-26 13:03:55,525 main.py:50] epoch 6092, training loss: 5583.83, average training loss: 5634.29, base loss: 20519.44
[INFO 2017-06-26 13:03:55,885 main.py:50] epoch 6093, training loss: 5612.05, average training loss: 5634.21, base loss: 20520.00
[INFO 2017-06-26 13:03:56,246 main.py:50] epoch 6094, training loss: 5655.94, average training loss: 5634.07, base loss: 20520.09
[INFO 2017-06-26 13:03:56,606 main.py:50] epoch 6095, training loss: 5570.31, average training loss: 5633.89, base loss: 20519.86
[INFO 2017-06-26 13:03:56,965 main.py:50] epoch 6096, training loss: 5714.02, average training loss: 5633.83, base loss: 20520.00
[INFO 2017-06-26 13:03:57,324 main.py:50] epoch 6097, training loss: 5551.25, average training loss: 5633.74, base loss: 20520.24
[INFO 2017-06-26 13:03:57,684 main.py:50] epoch 6098, training loss: 5577.17, average training loss: 5633.57, base loss: 20520.25
[INFO 2017-06-26 13:03:58,045 main.py:50] epoch 6099, training loss: 5601.81, average training loss: 5633.51, base loss: 20520.31
[INFO 2017-06-26 13:03:58,045 main.py:52] epoch 6099, testing
[INFO 2017-06-26 13:03:59,516 main.py:103] average testing loss: 5574.36, base loss: 20545.16
[INFO 2017-06-26 13:03:59,516 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 13:03:59,522 main.py:76] current best accuracy: 5574.36
[INFO 2017-06-26 13:03:59,881 main.py:50] epoch 6100, training loss: 5613.95, average training loss: 5633.43, base loss: 20520.76
[INFO 2017-06-26 13:04:00,240 main.py:50] epoch 6101, training loss: 5550.87, average training loss: 5633.24, base loss: 20520.57
[INFO 2017-06-26 13:04:00,599 main.py:50] epoch 6102, training loss: 5544.07, average training loss: 5633.14, base loss: 20520.77
[INFO 2017-06-26 13:04:00,957 main.py:50] epoch 6103, training loss: 5532.69, average training loss: 5632.91, base loss: 20520.62
[INFO 2017-06-26 13:04:01,317 main.py:50] epoch 6104, training loss: 5592.53, average training loss: 5632.82, base loss: 20520.46
[INFO 2017-06-26 13:04:01,675 main.py:50] epoch 6105, training loss: 5538.52, average training loss: 5632.71, base loss: 20520.30
[INFO 2017-06-26 13:04:02,033 main.py:50] epoch 6106, training loss: 5576.25, average training loss: 5632.60, base loss: 20520.50
[INFO 2017-06-26 13:04:02,393 main.py:50] epoch 6107, training loss: 5584.18, average training loss: 5632.50, base loss: 20520.29
[INFO 2017-06-26 13:04:02,753 main.py:50] epoch 6108, training loss: 5557.76, average training loss: 5632.39, base loss: 20520.43
[INFO 2017-06-26 13:04:03,111 main.py:50] epoch 6109, training loss: 5616.55, average training loss: 5632.29, base loss: 20520.46
[INFO 2017-06-26 13:04:03,470 main.py:50] epoch 6110, training loss: 5518.79, average training loss: 5632.12, base loss: 20520.49
[INFO 2017-06-26 13:04:03,829 main.py:50] epoch 6111, training loss: 5561.54, average training loss: 5631.98, base loss: 20520.79
[INFO 2017-06-26 13:04:04,189 main.py:50] epoch 6112, training loss: 5547.31, average training loss: 5631.78, base loss: 20520.59
[INFO 2017-06-26 13:04:04,547 main.py:50] epoch 6113, training loss: 5583.03, average training loss: 5631.68, base loss: 20520.46
[INFO 2017-06-26 13:04:04,906 main.py:50] epoch 6114, training loss: 5516.28, average training loss: 5631.47, base loss: 20520.58
[INFO 2017-06-26 13:04:05,265 main.py:50] epoch 6115, training loss: 5597.17, average training loss: 5631.41, base loss: 20520.95
[INFO 2017-06-26 13:04:05,623 main.py:50] epoch 6116, training loss: 5552.35, average training loss: 5631.18, base loss: 20520.85
[INFO 2017-06-26 13:04:05,981 main.py:50] epoch 6117, training loss: 5530.69, average training loss: 5630.99, base loss: 20520.60
[INFO 2017-06-26 13:04:06,341 main.py:50] epoch 6118, training loss: 5586.22, average training loss: 5630.83, base loss: 20520.47
[INFO 2017-06-26 13:04:06,701 main.py:50] epoch 6119, training loss: 5530.78, average training loss: 5630.55, base loss: 20520.35
[INFO 2017-06-26 13:04:07,059 main.py:50] epoch 6120, training loss: 5559.45, average training loss: 5630.34, base loss: 20520.27
[INFO 2017-06-26 13:04:07,418 main.py:50] epoch 6121, training loss: 5533.69, average training loss: 5630.17, base loss: 20520.13
[INFO 2017-06-26 13:04:07,779 main.py:50] epoch 6122, training loss: 5583.28, average training loss: 5630.12, base loss: 20519.89
[INFO 2017-06-26 13:04:08,149 main.py:50] epoch 6123, training loss: 5618.26, average training loss: 5629.99, base loss: 20519.55
[INFO 2017-06-26 13:04:08,507 main.py:50] epoch 6124, training loss: 5568.78, average training loss: 5629.85, base loss: 20519.79
[INFO 2017-06-26 13:04:08,867 main.py:50] epoch 6125, training loss: 5583.09, average training loss: 5629.71, base loss: 20519.84
[INFO 2017-06-26 13:04:09,226 main.py:50] epoch 6126, training loss: 5568.32, average training loss: 5629.57, base loss: 20519.85
[INFO 2017-06-26 13:04:09,586 main.py:50] epoch 6127, training loss: 5518.95, average training loss: 5629.41, base loss: 20519.76
[INFO 2017-06-26 13:04:09,945 main.py:50] epoch 6128, training loss: 5525.23, average training loss: 5629.26, base loss: 20519.77
[INFO 2017-06-26 13:04:10,304 main.py:50] epoch 6129, training loss: 5588.13, average training loss: 5629.10, base loss: 20519.47
[INFO 2017-06-26 13:04:10,663 main.py:50] epoch 6130, training loss: 5571.11, average training loss: 5629.00, base loss: 20519.41
[INFO 2017-06-26 13:04:11,022 main.py:50] epoch 6131, training loss: 5504.32, average training loss: 5628.85, base loss: 20519.31
[INFO 2017-06-26 13:04:11,382 main.py:50] epoch 6132, training loss: 5547.66, average training loss: 5628.72, base loss: 20519.21
[INFO 2017-06-26 13:04:11,741 main.py:50] epoch 6133, training loss: 5550.30, average training loss: 5628.56, base loss: 20519.15
[INFO 2017-06-26 13:04:12,100 main.py:50] epoch 6134, training loss: 5645.87, average training loss: 5628.53, base loss: 20519.42
[INFO 2017-06-26 13:04:12,457 main.py:50] epoch 6135, training loss: 5571.12, average training loss: 5628.37, base loss: 20519.36
[INFO 2017-06-26 13:04:12,816 main.py:50] epoch 6136, training loss: 5588.44, average training loss: 5628.20, base loss: 20519.25
[INFO 2017-06-26 13:04:13,174 main.py:50] epoch 6137, training loss: 5546.02, average training loss: 5628.10, base loss: 20518.95
[INFO 2017-06-26 13:04:13,534 main.py:50] epoch 6138, training loss: 5614.00, average training loss: 5628.04, base loss: 20519.11
[INFO 2017-06-26 13:04:13,893 main.py:50] epoch 6139, training loss: 5595.99, average training loss: 5627.91, base loss: 20519.27
[INFO 2017-06-26 13:04:14,251 main.py:50] epoch 6140, training loss: 5667.46, average training loss: 5627.79, base loss: 20519.48
[INFO 2017-06-26 13:04:14,609 main.py:50] epoch 6141, training loss: 5550.37, average training loss: 5627.66, base loss: 20519.70
[INFO 2017-06-26 13:04:14,968 main.py:50] epoch 6142, training loss: 5568.65, average training loss: 5627.55, base loss: 20519.40
[INFO 2017-06-26 13:04:15,328 main.py:50] epoch 6143, training loss: 5630.86, average training loss: 5627.42, base loss: 20519.61
[INFO 2017-06-26 13:04:15,687 main.py:50] epoch 6144, training loss: 5526.52, average training loss: 5627.23, base loss: 20519.70
[INFO 2017-06-26 13:04:16,046 main.py:50] epoch 6145, training loss: 5665.59, average training loss: 5627.18, base loss: 20519.99
[INFO 2017-06-26 13:04:16,407 main.py:50] epoch 6146, training loss: 5561.78, average training loss: 5627.12, base loss: 20520.46
[INFO 2017-06-26 13:04:16,765 main.py:50] epoch 6147, training loss: 5556.85, average training loss: 5626.95, base loss: 20520.43
[INFO 2017-06-26 13:04:17,123 main.py:50] epoch 6148, training loss: 5561.43, average training loss: 5626.81, base loss: 20520.08
[INFO 2017-06-26 13:04:17,481 main.py:50] epoch 6149, training loss: 5499.95, average training loss: 5626.59, base loss: 20520.20
[INFO 2017-06-26 13:04:17,840 main.py:50] epoch 6150, training loss: 5539.15, average training loss: 5626.39, base loss: 20519.97
[INFO 2017-06-26 13:04:18,199 main.py:50] epoch 6151, training loss: 5554.54, average training loss: 5626.18, base loss: 20519.54
[INFO 2017-06-26 13:04:18,558 main.py:50] epoch 6152, training loss: 5599.74, average training loss: 5626.03, base loss: 20520.20
[INFO 2017-06-26 13:04:18,916 main.py:50] epoch 6153, training loss: 5544.96, average training loss: 5625.84, base loss: 20519.89
[INFO 2017-06-26 13:04:19,275 main.py:50] epoch 6154, training loss: 5608.77, average training loss: 5625.82, base loss: 20519.93
[INFO 2017-06-26 13:04:19,634 main.py:50] epoch 6155, training loss: 5582.68, average training loss: 5625.60, base loss: 20519.92
[INFO 2017-06-26 13:04:19,993 main.py:50] epoch 6156, training loss: 5528.58, average training loss: 5625.45, base loss: 20520.02
[INFO 2017-06-26 13:04:20,352 main.py:50] epoch 6157, training loss: 5603.75, average training loss: 5625.32, base loss: 20520.46
[INFO 2017-06-26 13:04:20,711 main.py:50] epoch 6158, training loss: 5541.30, average training loss: 5625.18, base loss: 20520.53
[INFO 2017-06-26 13:04:21,070 main.py:50] epoch 6159, training loss: 5535.87, average training loss: 5625.03, base loss: 20520.84
[INFO 2017-06-26 13:04:21,429 main.py:50] epoch 6160, training loss: 5538.69, average training loss: 5624.89, base loss: 20520.83
[INFO 2017-06-26 13:04:21,789 main.py:50] epoch 6161, training loss: 5570.67, average training loss: 5624.83, base loss: 20521.00
[INFO 2017-06-26 13:04:22,147 main.py:50] epoch 6162, training loss: 5562.72, average training loss: 5624.78, base loss: 20520.99
[INFO 2017-06-26 13:04:22,508 main.py:50] epoch 6163, training loss: 5583.24, average training loss: 5624.63, base loss: 20520.54
[INFO 2017-06-26 13:04:22,867 main.py:50] epoch 6164, training loss: 5561.60, average training loss: 5624.50, base loss: 20520.23
[INFO 2017-06-26 13:04:23,227 main.py:50] epoch 6165, training loss: 5562.20, average training loss: 5624.37, base loss: 20520.10
[INFO 2017-06-26 13:04:23,586 main.py:50] epoch 6166, training loss: 5543.31, average training loss: 5624.24, base loss: 20520.58
[INFO 2017-06-26 13:04:23,946 main.py:50] epoch 6167, training loss: 5590.46, average training loss: 5624.15, base loss: 20521.15
[INFO 2017-06-26 13:04:24,305 main.py:50] epoch 6168, training loss: 5486.93, average training loss: 5623.99, base loss: 20521.24
[INFO 2017-06-26 13:04:24,664 main.py:50] epoch 6169, training loss: 5622.49, average training loss: 5623.96, base loss: 20521.68
[INFO 2017-06-26 13:04:25,023 main.py:50] epoch 6170, training loss: 5571.98, average training loss: 5623.87, base loss: 20521.83
[INFO 2017-06-26 13:04:25,382 main.py:50] epoch 6171, training loss: 5563.72, average training loss: 5623.68, base loss: 20521.88
[INFO 2017-06-26 13:04:25,741 main.py:50] epoch 6172, training loss: 5647.99, average training loss: 5623.63, base loss: 20521.94
[INFO 2017-06-26 13:04:26,100 main.py:50] epoch 6173, training loss: 5542.45, average training loss: 5623.48, base loss: 20521.94
[INFO 2017-06-26 13:04:26,459 main.py:50] epoch 6174, training loss: 5607.80, average training loss: 5623.33, base loss: 20521.93
[INFO 2017-06-26 13:04:26,816 main.py:50] epoch 6175, training loss: 5575.32, average training loss: 5623.25, base loss: 20521.92
[INFO 2017-06-26 13:04:27,175 main.py:50] epoch 6176, training loss: 5603.54, average training loss: 5623.20, base loss: 20522.45
[INFO 2017-06-26 13:04:27,534 main.py:50] epoch 6177, training loss: 5550.85, average training loss: 5623.14, base loss: 20522.32
[INFO 2017-06-26 13:04:27,893 main.py:50] epoch 6178, training loss: 5587.40, average training loss: 5623.05, base loss: 20522.15
[INFO 2017-06-26 13:04:28,251 main.py:50] epoch 6179, training loss: 5599.93, average training loss: 5623.01, base loss: 20521.99
[INFO 2017-06-26 13:04:28,609 main.py:50] epoch 6180, training loss: 5552.73, average training loss: 5622.99, base loss: 20522.05
[INFO 2017-06-26 13:04:28,968 main.py:50] epoch 6181, training loss: 5583.16, average training loss: 5622.99, base loss: 20522.09
[INFO 2017-06-26 13:04:29,326 main.py:50] epoch 6182, training loss: 5565.56, average training loss: 5622.92, base loss: 20521.91
[INFO 2017-06-26 13:04:29,686 main.py:50] epoch 6183, training loss: 5557.20, average training loss: 5622.72, base loss: 20522.08
[INFO 2017-06-26 13:04:30,045 main.py:50] epoch 6184, training loss: 5579.96, average training loss: 5622.69, base loss: 20522.21
[INFO 2017-06-26 13:04:30,404 main.py:50] epoch 6185, training loss: 5616.31, average training loss: 5622.58, base loss: 20521.77
[INFO 2017-06-26 13:04:30,762 main.py:50] epoch 6186, training loss: 5597.55, average training loss: 5622.50, base loss: 20521.57
[INFO 2017-06-26 13:04:31,123 main.py:50] epoch 6187, training loss: 5579.75, average training loss: 5622.42, base loss: 20521.66
[INFO 2017-06-26 13:04:31,481 main.py:50] epoch 6188, training loss: 5642.31, average training loss: 5622.43, base loss: 20521.95
[INFO 2017-06-26 13:04:31,841 main.py:50] epoch 6189, training loss: 5678.80, average training loss: 5622.41, base loss: 20522.16
[INFO 2017-06-26 13:04:32,200 main.py:50] epoch 6190, training loss: 5618.18, average training loss: 5622.40, base loss: 20522.24
[INFO 2017-06-26 13:04:32,559 main.py:50] epoch 6191, training loss: 5568.82, average training loss: 5622.29, base loss: 20521.83
[INFO 2017-06-26 13:04:32,918 main.py:50] epoch 6192, training loss: 5667.25, average training loss: 5622.35, base loss: 20521.79
[INFO 2017-06-26 13:04:33,278 main.py:50] epoch 6193, training loss: 5609.19, average training loss: 5622.26, base loss: 20521.77
[INFO 2017-06-26 13:04:33,638 main.py:50] epoch 6194, training loss: 5565.19, average training loss: 5622.18, base loss: 20521.95
[INFO 2017-06-26 13:04:33,998 main.py:50] epoch 6195, training loss: 5577.10, average training loss: 5622.07, base loss: 20521.97
[INFO 2017-06-26 13:04:34,357 main.py:50] epoch 6196, training loss: 5573.94, average training loss: 5622.02, base loss: 20522.09
[INFO 2017-06-26 13:04:34,716 main.py:50] epoch 6197, training loss: 5584.92, average training loss: 5621.89, base loss: 20521.97
[INFO 2017-06-26 13:04:35,076 main.py:50] epoch 6198, training loss: 5602.90, average training loss: 5621.76, base loss: 20521.68
[INFO 2017-06-26 13:04:35,434 main.py:50] epoch 6199, training loss: 5566.56, average training loss: 5621.65, base loss: 20521.34
[INFO 2017-06-26 13:04:35,435 main.py:52] epoch 6199, testing
[INFO 2017-06-26 13:04:36,909 main.py:103] average testing loss: 5583.77, base loss: 20475.94
[INFO 2017-06-26 13:04:36,909 main.py:76] current best accuracy: 5574.36
[INFO 2017-06-26 13:04:37,268 main.py:50] epoch 6200, training loss: 5562.68, average training loss: 5621.53, base loss: 20521.43
[INFO 2017-06-26 13:04:37,628 main.py:50] epoch 6201, training loss: 5547.95, average training loss: 5621.41, base loss: 20521.28
[INFO 2017-06-26 13:04:37,987 main.py:50] epoch 6202, training loss: 5571.65, average training loss: 5621.20, base loss: 20521.12
[INFO 2017-06-26 13:04:38,345 main.py:50] epoch 6203, training loss: 5600.04, average training loss: 5621.13, base loss: 20521.36
[INFO 2017-06-26 13:04:38,704 main.py:50] epoch 6204, training loss: 5577.58, average training loss: 5621.02, base loss: 20521.45
[INFO 2017-06-26 13:04:39,064 main.py:50] epoch 6205, training loss: 5600.61, average training loss: 5620.95, base loss: 20521.79
[INFO 2017-06-26 13:04:39,423 main.py:50] epoch 6206, training loss: 5478.32, average training loss: 5620.74, base loss: 20521.33
[INFO 2017-06-26 13:04:39,782 main.py:50] epoch 6207, training loss: 5544.17, average training loss: 5620.66, base loss: 20521.20
[INFO 2017-06-26 13:04:40,142 main.py:50] epoch 6208, training loss: 5498.19, average training loss: 5620.53, base loss: 20521.36
[INFO 2017-06-26 13:04:40,499 main.py:50] epoch 6209, training loss: 5515.06, average training loss: 5620.41, base loss: 20521.43
[INFO 2017-06-26 13:04:40,859 main.py:50] epoch 6210, training loss: 5539.74, average training loss: 5620.27, base loss: 20521.59
[INFO 2017-06-26 13:04:41,219 main.py:50] epoch 6211, training loss: 5576.00, average training loss: 5620.20, base loss: 20521.92
[INFO 2017-06-26 13:04:41,578 main.py:50] epoch 6212, training loss: 5568.89, average training loss: 5620.16, base loss: 20522.06
[INFO 2017-06-26 13:04:41,938 main.py:50] epoch 6213, training loss: 5546.60, average training loss: 5620.06, base loss: 20522.18
[INFO 2017-06-26 13:04:42,297 main.py:50] epoch 6214, training loss: 5581.96, average training loss: 5619.97, base loss: 20522.05
[INFO 2017-06-26 13:04:42,656 main.py:50] epoch 6215, training loss: 5573.99, average training loss: 5619.89, base loss: 20521.76
[INFO 2017-06-26 13:04:43,014 main.py:50] epoch 6216, training loss: 5598.31, average training loss: 5619.80, base loss: 20521.66
[INFO 2017-06-26 13:04:43,373 main.py:50] epoch 6217, training loss: 5478.39, average training loss: 5619.60, base loss: 20521.39
[INFO 2017-06-26 13:04:43,732 main.py:50] epoch 6218, training loss: 5582.36, average training loss: 5619.52, base loss: 20521.46
[INFO 2017-06-26 13:04:44,091 main.py:50] epoch 6219, training loss: 5545.51, average training loss: 5619.44, base loss: 20521.75
[INFO 2017-06-26 13:04:44,451 main.py:50] epoch 6220, training loss: 5586.05, average training loss: 5619.36, base loss: 20521.74
[INFO 2017-06-26 13:04:44,810 main.py:50] epoch 6221, training loss: 5548.63, average training loss: 5619.24, base loss: 20521.92
[INFO 2017-06-26 13:04:45,170 main.py:50] epoch 6222, training loss: 5602.22, average training loss: 5619.24, base loss: 20521.89
[INFO 2017-06-26 13:04:45,529 main.py:50] epoch 6223, training loss: 5535.40, average training loss: 5619.18, base loss: 20521.79
[INFO 2017-06-26 13:04:45,888 main.py:50] epoch 6224, training loss: 5570.41, average training loss: 5619.08, base loss: 20522.08
[INFO 2017-06-26 13:04:46,246 main.py:50] epoch 6225, training loss: 5530.07, average training loss: 5618.91, base loss: 20522.15
[INFO 2017-06-26 13:04:46,606 main.py:50] epoch 6226, training loss: 5640.06, average training loss: 5618.92, base loss: 20522.32
[INFO 2017-06-26 13:04:46,964 main.py:50] epoch 6227, training loss: 5563.54, average training loss: 5618.82, base loss: 20522.44
[INFO 2017-06-26 13:04:47,323 main.py:50] epoch 6228, training loss: 5600.55, average training loss: 5618.74, base loss: 20522.38
[INFO 2017-06-26 13:04:47,681 main.py:50] epoch 6229, training loss: 5628.84, average training loss: 5618.72, base loss: 20522.25
[INFO 2017-06-26 13:04:48,041 main.py:50] epoch 6230, training loss: 5564.84, average training loss: 5618.63, base loss: 20522.51
[INFO 2017-06-26 13:04:48,400 main.py:50] epoch 6231, training loss: 5547.08, average training loss: 5618.56, base loss: 20522.25
[INFO 2017-06-26 13:04:48,759 main.py:50] epoch 6232, training loss: 5578.97, average training loss: 5618.47, base loss: 20521.84
[INFO 2017-06-26 13:04:49,117 main.py:50] epoch 6233, training loss: 5511.17, average training loss: 5618.37, base loss: 20521.66
[INFO 2017-06-26 13:04:49,476 main.py:50] epoch 6234, training loss: 5611.85, average training loss: 5618.37, base loss: 20521.54
[INFO 2017-06-26 13:04:49,836 main.py:50] epoch 6235, training loss: 5541.74, average training loss: 5618.29, base loss: 20521.28
[INFO 2017-06-26 13:04:50,195 main.py:50] epoch 6236, training loss: 5567.27, average training loss: 5618.21, base loss: 20521.18
[INFO 2017-06-26 13:04:50,554 main.py:50] epoch 6237, training loss: 5565.14, average training loss: 5618.09, base loss: 20521.14
[INFO 2017-06-26 13:04:50,913 main.py:50] epoch 6238, training loss: 5578.52, average training loss: 5618.01, base loss: 20521.15
[INFO 2017-06-26 13:04:51,272 main.py:50] epoch 6239, training loss: 5633.26, average training loss: 5617.99, base loss: 20521.29
[INFO 2017-06-26 13:04:51,631 main.py:50] epoch 6240, training loss: 5639.06, average training loss: 5617.90, base loss: 20521.45
[INFO 2017-06-26 13:04:51,990 main.py:50] epoch 6241, training loss: 5624.97, average training loss: 5617.83, base loss: 20521.27
[INFO 2017-06-26 13:04:52,349 main.py:50] epoch 6242, training loss: 5597.87, average training loss: 5617.79, base loss: 20521.40
[INFO 2017-06-26 13:04:52,719 main.py:50] epoch 6243, training loss: 5670.27, average training loss: 5617.81, base loss: 20521.51
[INFO 2017-06-26 13:04:53,078 main.py:50] epoch 6244, training loss: 5542.12, average training loss: 5617.73, base loss: 20521.63
[INFO 2017-06-26 13:04:53,439 main.py:50] epoch 6245, training loss: 5584.25, average training loss: 5617.68, base loss: 20521.48
[INFO 2017-06-26 13:04:53,797 main.py:50] epoch 6246, training loss: 5564.11, average training loss: 5617.58, base loss: 20520.82
[INFO 2017-06-26 13:04:54,157 main.py:50] epoch 6247, training loss: 5640.52, average training loss: 5617.62, base loss: 20520.82
[INFO 2017-06-26 13:04:54,516 main.py:50] epoch 6248, training loss: 5587.40, average training loss: 5617.57, base loss: 20520.27
[INFO 2017-06-26 13:04:54,875 main.py:50] epoch 6249, training loss: 5571.02, average training loss: 5617.51, base loss: 20520.33
[INFO 2017-06-26 13:04:55,234 main.py:50] epoch 6250, training loss: 5645.64, average training loss: 5617.54, base loss: 20520.06
[INFO 2017-06-26 13:04:55,595 main.py:50] epoch 6251, training loss: 5652.31, average training loss: 5617.55, base loss: 20520.61
[INFO 2017-06-26 13:04:55,954 main.py:50] epoch 6252, training loss: 5560.83, average training loss: 5617.49, base loss: 20520.18
[INFO 2017-06-26 13:04:56,312 main.py:50] epoch 6253, training loss: 5625.50, average training loss: 5617.48, base loss: 20519.86
[INFO 2017-06-26 13:04:56,671 main.py:50] epoch 6254, training loss: 5590.30, average training loss: 5617.48, base loss: 20519.69
[INFO 2017-06-26 13:04:57,031 main.py:50] epoch 6255, training loss: 5641.35, average training loss: 5617.41, base loss: 20519.88
[INFO 2017-06-26 13:04:57,389 main.py:50] epoch 6256, training loss: 5692.29, average training loss: 5617.47, base loss: 20519.94
[INFO 2017-06-26 13:04:57,749 main.py:50] epoch 6257, training loss: 5671.74, average training loss: 5617.48, base loss: 20520.12
[INFO 2017-06-26 13:04:58,107 main.py:50] epoch 6258, training loss: 5703.10, average training loss: 5617.61, base loss: 20520.37
[INFO 2017-06-26 13:04:58,465 main.py:50] epoch 6259, training loss: 5546.89, average training loss: 5617.48, base loss: 20520.29
[INFO 2017-06-26 13:04:58,825 main.py:50] epoch 6260, training loss: 5581.23, average training loss: 5617.43, base loss: 20520.34
[INFO 2017-06-26 13:04:59,185 main.py:50] epoch 6261, training loss: 5591.84, average training loss: 5617.42, base loss: 20520.18
[INFO 2017-06-26 13:04:59,544 main.py:50] epoch 6262, training loss: 5585.98, average training loss: 5617.40, base loss: 20520.13
[INFO 2017-06-26 13:04:59,903 main.py:50] epoch 6263, training loss: 5564.76, average training loss: 5617.33, base loss: 20520.11
[INFO 2017-06-26 13:05:00,261 main.py:50] epoch 6264, training loss: 5579.38, average training loss: 5617.29, base loss: 20520.24
[INFO 2017-06-26 13:05:00,621 main.py:50] epoch 6265, training loss: 5541.00, average training loss: 5617.14, base loss: 20520.18
[INFO 2017-06-26 13:05:00,980 main.py:50] epoch 6266, training loss: 5574.78, average training loss: 5617.05, base loss: 20520.20
[INFO 2017-06-26 13:05:01,338 main.py:50] epoch 6267, training loss: 5564.28, average training loss: 5616.98, base loss: 20520.02
[INFO 2017-06-26 13:05:01,698 main.py:50] epoch 6268, training loss: 5551.19, average training loss: 5616.90, base loss: 20519.58
[INFO 2017-06-26 13:05:02,056 main.py:50] epoch 6269, training loss: 5569.96, average training loss: 5616.83, base loss: 20519.82
[INFO 2017-06-26 13:05:02,415 main.py:50] epoch 6270, training loss: 5492.82, average training loss: 5616.71, base loss: 20519.83
[INFO 2017-06-26 13:05:02,774 main.py:50] epoch 6271, training loss: 5551.65, average training loss: 5616.61, base loss: 20519.66
[INFO 2017-06-26 13:05:03,132 main.py:50] epoch 6272, training loss: 5590.70, average training loss: 5616.57, base loss: 20519.63
[INFO 2017-06-26 13:05:03,492 main.py:50] epoch 6273, training loss: 5538.71, average training loss: 5616.49, base loss: 20519.72
[INFO 2017-06-26 13:05:03,850 main.py:50] epoch 6274, training loss: 5569.68, average training loss: 5616.36, base loss: 20519.28
[INFO 2017-06-26 13:05:04,209 main.py:50] epoch 6275, training loss: 5589.26, average training loss: 5616.30, base loss: 20519.68
[INFO 2017-06-26 13:05:04,569 main.py:50] epoch 6276, training loss: 5534.54, average training loss: 5616.15, base loss: 20519.04
[INFO 2017-06-26 13:05:04,928 main.py:50] epoch 6277, training loss: 5615.23, average training loss: 5616.13, base loss: 20518.81
[INFO 2017-06-26 13:05:05,286 main.py:50] epoch 6278, training loss: 5544.35, average training loss: 5615.98, base loss: 20518.97
[INFO 2017-06-26 13:05:05,645 main.py:50] epoch 6279, training loss: 5532.09, average training loss: 5615.81, base loss: 20518.81
[INFO 2017-06-26 13:05:06,003 main.py:50] epoch 6280, training loss: 5561.79, average training loss: 5615.71, base loss: 20518.87
[INFO 2017-06-26 13:05:06,362 main.py:50] epoch 6281, training loss: 5521.08, average training loss: 5615.59, base loss: 20518.94
[INFO 2017-06-26 13:05:06,720 main.py:50] epoch 6282, training loss: 5597.35, average training loss: 5615.56, base loss: 20519.15
[INFO 2017-06-26 13:05:07,079 main.py:50] epoch 6283, training loss: 5568.18, average training loss: 5615.44, base loss: 20519.37
[INFO 2017-06-26 13:05:07,438 main.py:50] epoch 6284, training loss: 5633.77, average training loss: 5615.34, base loss: 20519.49
[INFO 2017-06-26 13:05:07,797 main.py:50] epoch 6285, training loss: 5562.58, average training loss: 5615.20, base loss: 20519.47
[INFO 2017-06-26 13:05:08,154 main.py:50] epoch 6286, training loss: 5591.00, average training loss: 5615.08, base loss: 20519.71
[INFO 2017-06-26 13:05:08,513 main.py:50] epoch 6287, training loss: 5570.34, average training loss: 5615.00, base loss: 20519.70
[INFO 2017-06-26 13:05:08,873 main.py:50] epoch 6288, training loss: 5526.15, average training loss: 5614.85, base loss: 20519.51
[INFO 2017-06-26 13:05:09,232 main.py:50] epoch 6289, training loss: 5553.56, average training loss: 5614.67, base loss: 20519.15
[INFO 2017-06-26 13:05:09,589 main.py:50] epoch 6290, training loss: 5610.96, average training loss: 5614.63, base loss: 20519.24
[INFO 2017-06-26 13:05:09,948 main.py:50] epoch 6291, training loss: 5551.51, average training loss: 5614.52, base loss: 20519.10
[INFO 2017-06-26 13:05:10,308 main.py:50] epoch 6292, training loss: 5599.88, average training loss: 5614.45, base loss: 20519.00
[INFO 2017-06-26 13:05:10,667 main.py:50] epoch 6293, training loss: 5609.33, average training loss: 5614.43, base loss: 20518.99
[INFO 2017-06-26 13:05:11,024 main.py:50] epoch 6294, training loss: 5584.68, average training loss: 5614.36, base loss: 20519.37
[INFO 2017-06-26 13:05:11,383 main.py:50] epoch 6295, training loss: 5655.54, average training loss: 5614.44, base loss: 20519.39
[INFO 2017-06-26 13:05:11,741 main.py:50] epoch 6296, training loss: 5540.05, average training loss: 5614.31, base loss: 20518.91
[INFO 2017-06-26 13:05:12,101 main.py:50] epoch 6297, training loss: 5623.01, average training loss: 5614.23, base loss: 20518.87
[INFO 2017-06-26 13:05:12,460 main.py:50] epoch 6298, training loss: 5564.77, average training loss: 5614.14, base loss: 20518.87
[INFO 2017-06-26 13:05:12,817 main.py:50] epoch 6299, training loss: 5679.70, average training loss: 5614.21, base loss: 20518.38
[INFO 2017-06-26 13:05:12,817 main.py:52] epoch 6299, testing
[INFO 2017-06-26 13:05:14,283 main.py:103] average testing loss: 5589.12, base loss: 20501.12
[INFO 2017-06-26 13:05:14,283 main.py:76] current best accuracy: 5574.36
[INFO 2017-06-26 13:05:14,643 main.py:50] epoch 6300, training loss: 5548.68, average training loss: 5614.09, base loss: 20518.24
[INFO 2017-06-26 13:05:15,003 main.py:50] epoch 6301, training loss: 5598.21, average training loss: 5614.08, base loss: 20518.53
[INFO 2017-06-26 13:05:15,363 main.py:50] epoch 6302, training loss: 5574.97, average training loss: 5614.00, base loss: 20518.29
[INFO 2017-06-26 13:05:15,722 main.py:50] epoch 6303, training loss: 5547.24, average training loss: 5613.87, base loss: 20517.94
[INFO 2017-06-26 13:05:16,082 main.py:50] epoch 6304, training loss: 5669.46, average training loss: 5613.85, base loss: 20517.58
[INFO 2017-06-26 13:05:16,441 main.py:50] epoch 6305, training loss: 5560.03, average training loss: 5613.72, base loss: 20517.29
[INFO 2017-06-26 13:05:16,801 main.py:50] epoch 6306, training loss: 5551.22, average training loss: 5613.63, base loss: 20517.49
[INFO 2017-06-26 13:05:17,161 main.py:50] epoch 6307, training loss: 5596.71, average training loss: 5613.58, base loss: 20517.10
[INFO 2017-06-26 13:05:17,521 main.py:50] epoch 6308, training loss: 5565.73, average training loss: 5613.52, base loss: 20517.25
[INFO 2017-06-26 13:05:17,881 main.py:50] epoch 6309, training loss: 5626.45, average training loss: 5613.43, base loss: 20517.45
[INFO 2017-06-26 13:05:18,242 main.py:50] epoch 6310, training loss: 5530.22, average training loss: 5613.23, base loss: 20516.92
[INFO 2017-06-26 13:05:18,600 main.py:50] epoch 6311, training loss: 5602.04, average training loss: 5613.14, base loss: 20517.05
[INFO 2017-06-26 13:05:18,961 main.py:50] epoch 6312, training loss: 5550.52, average training loss: 5613.09, base loss: 20517.09
[INFO 2017-06-26 13:05:19,320 main.py:50] epoch 6313, training loss: 5609.85, average training loss: 5612.97, base loss: 20517.04
[INFO 2017-06-26 13:05:19,679 main.py:50] epoch 6314, training loss: 5573.57, average training loss: 5612.91, base loss: 20517.21
[INFO 2017-06-26 13:05:20,039 main.py:50] epoch 6315, training loss: 5600.12, average training loss: 5612.86, base loss: 20517.44
[INFO 2017-06-26 13:05:20,398 main.py:50] epoch 6316, training loss: 5538.00, average training loss: 5612.68, base loss: 20517.36
[INFO 2017-06-26 13:05:20,757 main.py:50] epoch 6317, training loss: 5560.16, average training loss: 5612.60, base loss: 20517.79
[INFO 2017-06-26 13:05:21,116 main.py:50] epoch 6318, training loss: 5611.52, average training loss: 5612.61, base loss: 20518.16
[INFO 2017-06-26 13:05:21,478 main.py:50] epoch 6319, training loss: 5550.54, average training loss: 5612.46, base loss: 20517.90
[INFO 2017-06-26 13:05:21,837 main.py:50] epoch 6320, training loss: 5591.75, average training loss: 5612.41, base loss: 20517.74
[INFO 2017-06-26 13:05:22,199 main.py:50] epoch 6321, training loss: 5521.99, average training loss: 5612.28, base loss: 20517.62
[INFO 2017-06-26 13:05:22,557 main.py:50] epoch 6322, training loss: 5593.91, average training loss: 5612.23, base loss: 20517.73
[INFO 2017-06-26 13:05:22,918 main.py:50] epoch 6323, training loss: 5521.34, average training loss: 5612.10, base loss: 20517.78
[INFO 2017-06-26 13:05:23,277 main.py:50] epoch 6324, training loss: 5627.47, average training loss: 5612.10, base loss: 20517.62
[INFO 2017-06-26 13:05:23,636 main.py:50] epoch 6325, training loss: 5542.14, average training loss: 5612.05, base loss: 20517.59
[INFO 2017-06-26 13:05:23,996 main.py:50] epoch 6326, training loss: 5525.52, average training loss: 5611.92, base loss: 20517.55
[INFO 2017-06-26 13:05:24,355 main.py:50] epoch 6327, training loss: 5528.42, average training loss: 5611.77, base loss: 20517.35
[INFO 2017-06-26 13:05:24,715 main.py:50] epoch 6328, training loss: 5532.37, average training loss: 5611.66, base loss: 20517.10
[INFO 2017-06-26 13:05:25,075 main.py:50] epoch 6329, training loss: 5557.42, average training loss: 5611.51, base loss: 20516.70
[INFO 2017-06-26 13:05:25,435 main.py:50] epoch 6330, training loss: 5562.90, average training loss: 5611.38, base loss: 20516.83
[INFO 2017-06-26 13:05:25,794 main.py:50] epoch 6331, training loss: 5554.50, average training loss: 5611.27, base loss: 20517.12
[INFO 2017-06-26 13:05:26,156 main.py:50] epoch 6332, training loss: 5518.45, average training loss: 5611.11, base loss: 20517.11
[INFO 2017-06-26 13:05:26,515 main.py:50] epoch 6333, training loss: 5515.72, average training loss: 5610.90, base loss: 20517.00
[INFO 2017-06-26 13:05:26,874 main.py:50] epoch 6334, training loss: 5602.49, average training loss: 5610.89, base loss: 20516.87
[INFO 2017-06-26 13:05:27,234 main.py:50] epoch 6335, training loss: 5515.99, average training loss: 5610.70, base loss: 20516.49
[INFO 2017-06-26 13:05:27,594 main.py:50] epoch 6336, training loss: 5611.06, average training loss: 5610.69, base loss: 20516.50
[INFO 2017-06-26 13:05:27,956 main.py:50] epoch 6337, training loss: 5523.53, average training loss: 5610.48, base loss: 20516.12
[INFO 2017-06-26 13:05:28,318 main.py:50] epoch 6338, training loss: 5533.55, average training loss: 5610.33, base loss: 20515.75
[INFO 2017-06-26 13:05:28,679 main.py:50] epoch 6339, training loss: 5614.67, average training loss: 5610.30, base loss: 20515.71
[INFO 2017-06-26 13:05:29,039 main.py:50] epoch 6340, training loss: 5562.29, average training loss: 5610.20, base loss: 20515.84
[INFO 2017-06-26 13:05:29,398 main.py:50] epoch 6341, training loss: 5591.43, average training loss: 5610.13, base loss: 20515.67
[INFO 2017-06-26 13:05:29,758 main.py:50] epoch 6342, training loss: 5548.25, average training loss: 5610.03, base loss: 20516.11
[INFO 2017-06-26 13:05:30,117 main.py:50] epoch 6343, training loss: 5625.04, average training loss: 5609.98, base loss: 20515.85
[INFO 2017-06-26 13:05:30,477 main.py:50] epoch 6344, training loss: 5577.05, average training loss: 5609.91, base loss: 20516.19
[INFO 2017-06-26 13:05:30,837 main.py:50] epoch 6345, training loss: 5575.82, average training loss: 5609.89, base loss: 20516.50
[INFO 2017-06-26 13:05:31,197 main.py:50] epoch 6346, training loss: 5553.23, average training loss: 5609.78, base loss: 20516.62
[INFO 2017-06-26 13:05:31,555 main.py:50] epoch 6347, training loss: 5601.07, average training loss: 5609.65, base loss: 20516.70
[INFO 2017-06-26 13:05:31,915 main.py:50] epoch 6348, training loss: 5535.87, average training loss: 5609.49, base loss: 20516.87
[INFO 2017-06-26 13:05:32,274 main.py:50] epoch 6349, training loss: 5519.26, average training loss: 5609.37, base loss: 20517.29
[INFO 2017-06-26 13:05:32,635 main.py:50] epoch 6350, training loss: 5559.36, average training loss: 5609.25, base loss: 20517.19
[INFO 2017-06-26 13:05:32,996 main.py:50] epoch 6351, training loss: 5541.60, average training loss: 5609.13, base loss: 20517.25
[INFO 2017-06-26 13:05:33,356 main.py:50] epoch 6352, training loss: 5583.99, average training loss: 5609.12, base loss: 20517.46
[INFO 2017-06-26 13:05:33,714 main.py:50] epoch 6353, training loss: 5583.21, average training loss: 5608.97, base loss: 20517.64
[INFO 2017-06-26 13:05:34,074 main.py:50] epoch 6354, training loss: 5499.39, average training loss: 5608.82, base loss: 20517.92
[INFO 2017-06-26 13:05:34,434 main.py:50] epoch 6355, training loss: 5581.18, average training loss: 5608.69, base loss: 20517.89
[INFO 2017-06-26 13:05:34,793 main.py:50] epoch 6356, training loss: 5605.73, average training loss: 5608.65, base loss: 20518.30
[INFO 2017-06-26 13:05:35,153 main.py:50] epoch 6357, training loss: 5540.85, average training loss: 5608.57, base loss: 20518.25
[INFO 2017-06-26 13:05:35,514 main.py:50] epoch 6358, training loss: 5561.44, average training loss: 5608.47, base loss: 20518.59
[INFO 2017-06-26 13:05:35,875 main.py:50] epoch 6359, training loss: 5596.49, average training loss: 5608.43, base loss: 20518.46
[INFO 2017-06-26 13:05:36,236 main.py:50] epoch 6360, training loss: 5571.48, average training loss: 5608.35, base loss: 20518.29
[INFO 2017-06-26 13:05:36,594 main.py:50] epoch 6361, training loss: 5563.46, average training loss: 5608.28, base loss: 20518.54
[INFO 2017-06-26 13:05:36,955 main.py:50] epoch 6362, training loss: 5576.33, average training loss: 5608.14, base loss: 20518.36
[INFO 2017-06-26 13:05:37,325 main.py:50] epoch 6363, training loss: 5505.67, average training loss: 5608.03, base loss: 20518.35
[INFO 2017-06-26 13:05:37,684 main.py:50] epoch 6364, training loss: 5572.14, average training loss: 5607.97, base loss: 20518.25
[INFO 2017-06-26 13:05:38,045 main.py:50] epoch 6365, training loss: 5606.65, average training loss: 5607.93, base loss: 20518.26
[INFO 2017-06-26 13:05:38,405 main.py:50] epoch 6366, training loss: 5508.70, average training loss: 5607.83, base loss: 20518.19
[INFO 2017-06-26 13:05:38,764 main.py:50] epoch 6367, training loss: 5572.88, average training loss: 5607.80, base loss: 20518.51
[INFO 2017-06-26 13:05:39,124 main.py:50] epoch 6368, training loss: 5572.73, average training loss: 5607.69, base loss: 20518.36
[INFO 2017-06-26 13:05:39,484 main.py:50] epoch 6369, training loss: 5594.65, average training loss: 5607.57, base loss: 20518.18
[INFO 2017-06-26 13:05:39,843 main.py:50] epoch 6370, training loss: 5561.39, average training loss: 5607.50, base loss: 20518.33
[INFO 2017-06-26 13:05:40,203 main.py:50] epoch 6371, training loss: 5507.30, average training loss: 5607.33, base loss: 20518.15
[INFO 2017-06-26 13:05:40,563 main.py:50] epoch 6372, training loss: 5494.09, average training loss: 5607.09, base loss: 20517.71
[INFO 2017-06-26 13:05:40,925 main.py:50] epoch 6373, training loss: 5542.80, average training loss: 5606.94, base loss: 20517.69
[INFO 2017-06-26 13:05:41,284 main.py:50] epoch 6374, training loss: 5568.33, average training loss: 5606.88, base loss: 20517.96
[INFO 2017-06-26 13:05:41,646 main.py:50] epoch 6375, training loss: 5528.33, average training loss: 5606.69, base loss: 20517.92
[INFO 2017-06-26 13:05:42,005 main.py:50] epoch 6376, training loss: 5522.48, average training loss: 5606.47, base loss: 20517.81
[INFO 2017-06-26 13:05:42,364 main.py:50] epoch 6377, training loss: 5557.15, average training loss: 5606.41, base loss: 20518.10
[INFO 2017-06-26 13:05:42,723 main.py:50] epoch 6378, training loss: 5498.19, average training loss: 5606.21, base loss: 20517.95
[INFO 2017-06-26 13:05:43,085 main.py:50] epoch 6379, training loss: 5498.60, average training loss: 5606.07, base loss: 20518.03
[INFO 2017-06-26 13:05:43,444 main.py:50] epoch 6380, training loss: 5535.76, average training loss: 5605.97, base loss: 20517.80
[INFO 2017-06-26 13:05:43,804 main.py:50] epoch 6381, training loss: 5485.32, average training loss: 5605.83, base loss: 20517.58
[INFO 2017-06-26 13:05:44,164 main.py:50] epoch 6382, training loss: 5546.93, average training loss: 5605.74, base loss: 20517.80
[INFO 2017-06-26 13:05:44,523 main.py:50] epoch 6383, training loss: 5530.81, average training loss: 5605.67, base loss: 20517.87
[INFO 2017-06-26 13:05:44,883 main.py:50] epoch 6384, training loss: 5595.74, average training loss: 5605.61, base loss: 20517.98
[INFO 2017-06-26 13:05:45,242 main.py:50] epoch 6385, training loss: 5534.82, average training loss: 5605.49, base loss: 20517.93
[INFO 2017-06-26 13:05:45,603 main.py:50] epoch 6386, training loss: 5552.93, average training loss: 5605.37, base loss: 20518.05
[INFO 2017-06-26 13:05:45,962 main.py:50] epoch 6387, training loss: 5546.12, average training loss: 5605.19, base loss: 20517.66
[INFO 2017-06-26 13:05:46,321 main.py:50] epoch 6388, training loss: 5522.38, average training loss: 5605.05, base loss: 20517.27
[INFO 2017-06-26 13:05:46,683 main.py:50] epoch 6389, training loss: 5549.98, average training loss: 5604.96, base loss: 20517.26
[INFO 2017-06-26 13:05:47,042 main.py:50] epoch 6390, training loss: 5577.28, average training loss: 5604.93, base loss: 20517.23
[INFO 2017-06-26 13:05:47,401 main.py:50] epoch 6391, training loss: 5525.84, average training loss: 5604.84, base loss: 20517.21
[INFO 2017-06-26 13:05:47,763 main.py:50] epoch 6392, training loss: 5578.68, average training loss: 5604.76, base loss: 20517.13
[INFO 2017-06-26 13:05:48,121 main.py:50] epoch 6393, training loss: 5549.97, average training loss: 5604.68, base loss: 20517.15
[INFO 2017-06-26 13:05:48,482 main.py:50] epoch 6394, training loss: 5545.51, average training loss: 5604.63, base loss: 20517.30
[INFO 2017-06-26 13:05:48,843 main.py:50] epoch 6395, training loss: 5572.22, average training loss: 5604.50, base loss: 20517.09
[INFO 2017-06-26 13:05:49,201 main.py:50] epoch 6396, training loss: 5544.39, average training loss: 5604.38, base loss: 20516.90
[INFO 2017-06-26 13:05:49,561 main.py:50] epoch 6397, training loss: 5531.04, average training loss: 5604.28, base loss: 20517.09
[INFO 2017-06-26 13:05:49,920 main.py:50] epoch 6398, training loss: 5572.59, average training loss: 5604.19, base loss: 20517.63
[INFO 2017-06-26 13:05:50,280 main.py:50] epoch 6399, training loss: 5564.97, average training loss: 5604.14, base loss: 20517.85
[INFO 2017-06-26 13:05:50,280 main.py:52] epoch 6399, testing
[INFO 2017-06-26 13:05:51,748 main.py:103] average testing loss: 5578.49, base loss: 20559.19
[INFO 2017-06-26 13:05:51,749 main.py:76] current best accuracy: 5574.36
[INFO 2017-06-26 13:05:52,108 main.py:50] epoch 6400, training loss: 5541.65, average training loss: 5604.07, base loss: 20518.15
[INFO 2017-06-26 13:05:52,466 main.py:50] epoch 6401, training loss: 5492.49, average training loss: 5603.93, base loss: 20517.95
[INFO 2017-06-26 13:05:52,824 main.py:50] epoch 6402, training loss: 5598.66, average training loss: 5603.81, base loss: 20517.90
[INFO 2017-06-26 13:05:53,184 main.py:50] epoch 6403, training loss: 5567.19, average training loss: 5603.80, base loss: 20518.06
[INFO 2017-06-26 13:05:53,542 main.py:50] epoch 6404, training loss: 5595.42, average training loss: 5603.71, base loss: 20518.27
[INFO 2017-06-26 13:05:53,901 main.py:50] epoch 6405, training loss: 5504.80, average training loss: 5603.58, base loss: 20518.11
[INFO 2017-06-26 13:05:54,260 main.py:50] epoch 6406, training loss: 5575.56, average training loss: 5603.51, base loss: 20517.86
[INFO 2017-06-26 13:05:54,619 main.py:50] epoch 6407, training loss: 5544.36, average training loss: 5603.44, base loss: 20517.95
[INFO 2017-06-26 13:05:54,979 main.py:50] epoch 6408, training loss: 5502.44, average training loss: 5603.30, base loss: 20517.89
[INFO 2017-06-26 13:05:55,337 main.py:50] epoch 6409, training loss: 5616.90, average training loss: 5603.26, base loss: 20518.13
[INFO 2017-06-26 13:05:55,695 main.py:50] epoch 6410, training loss: 5497.69, average training loss: 5603.10, base loss: 20518.03
[INFO 2017-06-26 13:05:56,054 main.py:50] epoch 6411, training loss: 5541.66, average training loss: 5602.99, base loss: 20518.04
[INFO 2017-06-26 13:05:56,413 main.py:50] epoch 6412, training loss: 5523.71, average training loss: 5602.86, base loss: 20517.96
[INFO 2017-06-26 13:05:56,772 main.py:50] epoch 6413, training loss: 5510.37, average training loss: 5602.78, base loss: 20518.16
[INFO 2017-06-26 13:05:57,131 main.py:50] epoch 6414, training loss: 5515.77, average training loss: 5602.68, base loss: 20518.20
[INFO 2017-06-26 13:05:57,490 main.py:50] epoch 6415, training loss: 5598.51, average training loss: 5602.65, base loss: 20518.23
[INFO 2017-06-26 13:05:57,849 main.py:50] epoch 6416, training loss: 5566.02, average training loss: 5602.56, base loss: 20518.39
[INFO 2017-06-26 13:05:58,208 main.py:50] epoch 6417, training loss: 5597.22, average training loss: 5602.46, base loss: 20518.49
[INFO 2017-06-26 13:05:58,567 main.py:50] epoch 6418, training loss: 5509.74, average training loss: 5602.29, base loss: 20518.57
[INFO 2017-06-26 13:05:58,926 main.py:50] epoch 6419, training loss: 5555.72, average training loss: 5602.20, base loss: 20518.21
[INFO 2017-06-26 13:05:59,287 main.py:50] epoch 6420, training loss: 5567.43, average training loss: 5602.06, base loss: 20517.57
[INFO 2017-06-26 13:05:59,646 main.py:50] epoch 6421, training loss: 5491.85, average training loss: 5601.95, base loss: 20517.54
[INFO 2017-06-26 13:06:00,005 main.py:50] epoch 6422, training loss: 5521.16, average training loss: 5601.88, base loss: 20517.74
[INFO 2017-06-26 13:06:00,364 main.py:50] epoch 6423, training loss: 5513.34, average training loss: 5601.75, base loss: 20517.48
[INFO 2017-06-26 13:06:00,723 main.py:50] epoch 6424, training loss: 5611.11, average training loss: 5601.67, base loss: 20517.91
[INFO 2017-06-26 13:06:01,082 main.py:50] epoch 6425, training loss: 5531.92, average training loss: 5601.61, base loss: 20517.84
[INFO 2017-06-26 13:06:01,440 main.py:50] epoch 6426, training loss: 5573.72, average training loss: 5601.54, base loss: 20517.29
[INFO 2017-06-26 13:06:01,800 main.py:50] epoch 6427, training loss: 5571.69, average training loss: 5601.38, base loss: 20517.58
[INFO 2017-06-26 13:06:02,159 main.py:50] epoch 6428, training loss: 5506.68, average training loss: 5601.24, base loss: 20517.43
[INFO 2017-06-26 13:06:02,519 main.py:50] epoch 6429, training loss: 5558.06, average training loss: 5601.15, base loss: 20518.04
[INFO 2017-06-26 13:06:02,878 main.py:50] epoch 6430, training loss: 5559.35, average training loss: 5601.05, base loss: 20518.50
[INFO 2017-06-26 13:06:03,236 main.py:50] epoch 6431, training loss: 5567.26, average training loss: 5600.95, base loss: 20518.34
[INFO 2017-06-26 13:06:03,594 main.py:50] epoch 6432, training loss: 5493.93, average training loss: 5600.81, base loss: 20518.04
[INFO 2017-06-26 13:06:03,953 main.py:50] epoch 6433, training loss: 5568.48, average training loss: 5600.73, base loss: 20518.25
[INFO 2017-06-26 13:06:04,312 main.py:50] epoch 6434, training loss: 5534.19, average training loss: 5600.66, base loss: 20518.49
[INFO 2017-06-26 13:06:04,670 main.py:50] epoch 6435, training loss: 5575.40, average training loss: 5600.61, base loss: 20518.69
[INFO 2017-06-26 13:06:05,028 main.py:50] epoch 6436, training loss: 5515.49, average training loss: 5600.48, base loss: 20518.39
[INFO 2017-06-26 13:06:05,386 main.py:50] epoch 6437, training loss: 5502.57, average training loss: 5600.41, base loss: 20518.21
[INFO 2017-06-26 13:06:05,745 main.py:50] epoch 6438, training loss: 5587.88, average training loss: 5600.37, base loss: 20518.65
[INFO 2017-06-26 13:06:06,103 main.py:50] epoch 6439, training loss: 5528.63, average training loss: 5600.28, base loss: 20518.26
[INFO 2017-06-26 13:06:06,462 main.py:50] epoch 6440, training loss: 5555.16, average training loss: 5600.24, base loss: 20518.41
[INFO 2017-06-26 13:06:06,822 main.py:50] epoch 6441, training loss: 5559.04, average training loss: 5600.16, base loss: 20518.60
[INFO 2017-06-26 13:06:07,181 main.py:50] epoch 6442, training loss: 5587.23, average training loss: 5600.10, base loss: 20518.48
[INFO 2017-06-26 13:06:07,539 main.py:50] epoch 6443, training loss: 5489.01, average training loss: 5599.96, base loss: 20518.57
[INFO 2017-06-26 13:06:07,900 main.py:50] epoch 6444, training loss: 5586.75, average training loss: 5599.84, base loss: 20518.26
[INFO 2017-06-26 13:06:08,258 main.py:50] epoch 6445, training loss: 5544.43, average training loss: 5599.70, base loss: 20518.07
[INFO 2017-06-26 13:06:08,616 main.py:50] epoch 6446, training loss: 5593.39, average training loss: 5599.67, base loss: 20517.96
[INFO 2017-06-26 13:06:08,976 main.py:50] epoch 6447, training loss: 5523.09, average training loss: 5599.56, base loss: 20518.17
[INFO 2017-06-26 13:06:09,335 main.py:50] epoch 6448, training loss: 5596.43, average training loss: 5599.50, base loss: 20518.70
[INFO 2017-06-26 13:06:09,694 main.py:50] epoch 6449, training loss: 5582.31, average training loss: 5599.41, base loss: 20518.95
[INFO 2017-06-26 13:06:10,053 main.py:50] epoch 6450, training loss: 5603.78, average training loss: 5599.41, base loss: 20519.05
[INFO 2017-06-26 13:06:10,412 main.py:50] epoch 6451, training loss: 5588.35, average training loss: 5599.38, base loss: 20519.25
[INFO 2017-06-26 13:06:10,771 main.py:50] epoch 6452, training loss: 5561.38, average training loss: 5599.27, base loss: 20519.80
[INFO 2017-06-26 13:06:11,130 main.py:50] epoch 6453, training loss: 5486.29, average training loss: 5599.10, base loss: 20519.88
[INFO 2017-06-26 13:06:11,489 main.py:50] epoch 6454, training loss: 5645.46, average training loss: 5599.12, base loss: 20520.11
[INFO 2017-06-26 13:06:11,847 main.py:50] epoch 6455, training loss: 5540.86, average training loss: 5599.00, base loss: 20520.22
[INFO 2017-06-26 13:06:12,206 main.py:50] epoch 6456, training loss: 5565.06, average training loss: 5598.94, base loss: 20519.70
[INFO 2017-06-26 13:06:12,566 main.py:50] epoch 6457, training loss: 5517.03, average training loss: 5598.81, base loss: 20519.54
[INFO 2017-06-26 13:06:12,924 main.py:50] epoch 6458, training loss: 5600.97, average training loss: 5598.82, base loss: 20519.35
[INFO 2017-06-26 13:06:13,283 main.py:50] epoch 6459, training loss: 5567.57, average training loss: 5598.75, base loss: 20519.25
[INFO 2017-06-26 13:06:13,642 main.py:50] epoch 6460, training loss: 5675.60, average training loss: 5598.78, base loss: 20519.61
[INFO 2017-06-26 13:06:14,001 main.py:50] epoch 6461, training loss: 5558.37, average training loss: 5598.72, base loss: 20519.59
[INFO 2017-06-26 13:06:14,360 main.py:50] epoch 6462, training loss: 5579.94, average training loss: 5598.70, base loss: 20519.58
[INFO 2017-06-26 13:06:14,719 main.py:50] epoch 6463, training loss: 5612.68, average training loss: 5598.75, base loss: 20519.80
[INFO 2017-06-26 13:06:15,078 main.py:50] epoch 6464, training loss: 5597.62, average training loss: 5598.72, base loss: 20519.92
[INFO 2017-06-26 13:06:15,437 main.py:50] epoch 6465, training loss: 5533.07, average training loss: 5598.54, base loss: 20519.69
[INFO 2017-06-26 13:06:15,796 main.py:50] epoch 6466, training loss: 5507.40, average training loss: 5598.41, base loss: 20519.69
[INFO 2017-06-26 13:06:16,154 main.py:50] epoch 6467, training loss: 5564.02, average training loss: 5598.36, base loss: 20519.69
[INFO 2017-06-26 13:06:16,515 main.py:50] epoch 6468, training loss: 5528.50, average training loss: 5598.30, base loss: 20519.99
[INFO 2017-06-26 13:06:16,875 main.py:50] epoch 6469, training loss: 5614.19, average training loss: 5598.27, base loss: 20519.76
[INFO 2017-06-26 13:06:17,234 main.py:50] epoch 6470, training loss: 5607.72, average training loss: 5598.23, base loss: 20519.85
[INFO 2017-06-26 13:06:17,593 main.py:50] epoch 6471, training loss: 5521.10, average training loss: 5598.17, base loss: 20519.81
[INFO 2017-06-26 13:06:17,952 main.py:50] epoch 6472, training loss: 5566.30, average training loss: 5598.12, base loss: 20519.70
[INFO 2017-06-26 13:06:18,311 main.py:50] epoch 6473, training loss: 5543.88, average training loss: 5598.03, base loss: 20519.59
[INFO 2017-06-26 13:06:18,671 main.py:50] epoch 6474, training loss: 5533.06, average training loss: 5597.97, base loss: 20519.52
[INFO 2017-06-26 13:06:19,030 main.py:50] epoch 6475, training loss: 5530.98, average training loss: 5597.97, base loss: 20519.48
[INFO 2017-06-26 13:06:19,388 main.py:50] epoch 6476, training loss: 5596.80, average training loss: 5597.98, base loss: 20519.58
[INFO 2017-06-26 13:06:19,748 main.py:50] epoch 6477, training loss: 5568.18, average training loss: 5597.89, base loss: 20520.18
[INFO 2017-06-26 13:06:20,106 main.py:50] epoch 6478, training loss: 5633.91, average training loss: 5597.96, base loss: 20520.24
[INFO 2017-06-26 13:06:20,465 main.py:50] epoch 6479, training loss: 5494.38, average training loss: 5597.85, base loss: 20520.61
[INFO 2017-06-26 13:06:20,824 main.py:50] epoch 6480, training loss: 5523.36, average training loss: 5597.66, base loss: 20520.24
[INFO 2017-06-26 13:06:21,184 main.py:50] epoch 6481, training loss: 5519.91, average training loss: 5597.52, base loss: 20519.92
[INFO 2017-06-26 13:06:21,542 main.py:50] epoch 6482, training loss: 5561.16, average training loss: 5597.44, base loss: 20519.84
[INFO 2017-06-26 13:06:21,913 main.py:50] epoch 6483, training loss: 5627.51, average training loss: 5597.38, base loss: 20519.68
[INFO 2017-06-26 13:06:22,272 main.py:50] epoch 6484, training loss: 5515.95, average training loss: 5597.19, base loss: 20519.43
[INFO 2017-06-26 13:06:22,631 main.py:50] epoch 6485, training loss: 5568.61, average training loss: 5597.07, base loss: 20519.84
[INFO 2017-06-26 13:06:22,991 main.py:50] epoch 6486, training loss: 5489.98, average training loss: 5596.84, base loss: 20519.38
[INFO 2017-06-26 13:06:23,350 main.py:50] epoch 6487, training loss: 5572.81, average training loss: 5596.78, base loss: 20519.31
[INFO 2017-06-26 13:06:23,710 main.py:50] epoch 6488, training loss: 5643.15, average training loss: 5596.75, base loss: 20519.61
[INFO 2017-06-26 13:06:24,070 main.py:50] epoch 6489, training loss: 5524.81, average training loss: 5596.63, base loss: 20519.42
[INFO 2017-06-26 13:06:24,428 main.py:50] epoch 6490, training loss: 5608.97, average training loss: 5596.53, base loss: 20519.62
[INFO 2017-06-26 13:06:24,788 main.py:50] epoch 6491, training loss: 5567.36, average training loss: 5596.44, base loss: 20519.84
[INFO 2017-06-26 13:06:25,147 main.py:50] epoch 6492, training loss: 5564.19, average training loss: 5596.37, base loss: 20520.01
[INFO 2017-06-26 13:06:25,506 main.py:50] epoch 6493, training loss: 5573.33, average training loss: 5596.23, base loss: 20520.00
[INFO 2017-06-26 13:06:25,865 main.py:50] epoch 6494, training loss: 5547.70, average training loss: 5596.12, base loss: 20519.95
[INFO 2017-06-26 13:06:26,224 main.py:50] epoch 6495, training loss: 5526.19, average training loss: 5595.99, base loss: 20519.81
[INFO 2017-06-26 13:06:26,584 main.py:50] epoch 6496, training loss: 5582.80, average training loss: 5595.86, base loss: 20519.85
[INFO 2017-06-26 13:06:26,943 main.py:50] epoch 6497, training loss: 5549.69, average training loss: 5595.76, base loss: 20520.01
[INFO 2017-06-26 13:06:27,302 main.py:50] epoch 6498, training loss: 5574.33, average training loss: 5595.67, base loss: 20519.84
[INFO 2017-06-26 13:06:27,661 main.py:50] epoch 6499, training loss: 5535.85, average training loss: 5595.54, base loss: 20519.49
[INFO 2017-06-26 13:06:27,662 main.py:52] epoch 6499, testing
[INFO 2017-06-26 13:06:29,135 main.py:103] average testing loss: 5543.72, base loss: 20507.22
[INFO 2017-06-26 13:06:29,135 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 13:06:29,141 main.py:76] current best accuracy: 5543.72
[INFO 2017-06-26 13:06:29,500 main.py:50] epoch 6500, training loss: 5589.74, average training loss: 5595.57, base loss: 20519.57
[INFO 2017-06-26 13:06:29,859 main.py:50] epoch 6501, training loss: 5607.72, average training loss: 5595.56, base loss: 20519.73
[INFO 2017-06-26 13:06:30,219 main.py:50] epoch 6502, training loss: 5565.56, average training loss: 5595.52, base loss: 20520.04
[INFO 2017-06-26 13:06:30,579 main.py:50] epoch 6503, training loss: 5559.13, average training loss: 5595.43, base loss: 20519.77
[INFO 2017-06-26 13:06:30,938 main.py:50] epoch 6504, training loss: 5587.94, average training loss: 5595.41, base loss: 20519.77
[INFO 2017-06-26 13:06:31,298 main.py:50] epoch 6505, training loss: 5621.66, average training loss: 5595.34, base loss: 20519.82
[INFO 2017-06-26 13:06:31,658 main.py:50] epoch 6506, training loss: 5620.05, average training loss: 5595.30, base loss: 20519.57
[INFO 2017-06-26 13:06:32,017 main.py:50] epoch 6507, training loss: 5502.15, average training loss: 5595.15, base loss: 20519.38
[INFO 2017-06-26 13:06:32,375 main.py:50] epoch 6508, training loss: 5489.89, average training loss: 5595.04, base loss: 20519.14
[INFO 2017-06-26 13:06:32,733 main.py:50] epoch 6509, training loss: 5579.39, average training loss: 5595.03, base loss: 20518.86
[INFO 2017-06-26 13:06:33,094 main.py:50] epoch 6510, training loss: 5568.91, average training loss: 5594.97, base loss: 20518.62
[INFO 2017-06-26 13:06:33,452 main.py:50] epoch 6511, training loss: 5542.83, average training loss: 5594.87, base loss: 20518.65
[INFO 2017-06-26 13:06:33,811 main.py:50] epoch 6512, training loss: 5594.15, average training loss: 5594.89, base loss: 20519.14
[INFO 2017-06-26 13:06:34,171 main.py:50] epoch 6513, training loss: 5546.63, average training loss: 5594.79, base loss: 20519.23
[INFO 2017-06-26 13:06:34,530 main.py:50] epoch 6514, training loss: 5572.58, average training loss: 5594.79, base loss: 20519.49
[INFO 2017-06-26 13:06:34,890 main.py:50] epoch 6515, training loss: 5569.72, average training loss: 5594.72, base loss: 20519.13
[INFO 2017-06-26 13:06:35,248 main.py:50] epoch 6516, training loss: 5562.39, average training loss: 5594.71, base loss: 20519.35
[INFO 2017-06-26 13:06:35,608 main.py:50] epoch 6517, training loss: 5581.69, average training loss: 5594.68, base loss: 20519.52
[INFO 2017-06-26 13:06:35,968 main.py:50] epoch 6518, training loss: 5552.89, average training loss: 5594.63, base loss: 20519.56
[INFO 2017-06-26 13:06:36,328 main.py:50] epoch 6519, training loss: 5507.48, average training loss: 5594.49, base loss: 20519.37
[INFO 2017-06-26 13:06:36,687 main.py:50] epoch 6520, training loss: 5521.04, average training loss: 5594.38, base loss: 20519.95
[INFO 2017-06-26 13:06:37,046 main.py:50] epoch 6521, training loss: 5554.71, average training loss: 5594.27, base loss: 20519.62
[INFO 2017-06-26 13:06:37,405 main.py:50] epoch 6522, training loss: 5544.92, average training loss: 5594.18, base loss: 20519.97
[INFO 2017-06-26 13:06:37,765 main.py:50] epoch 6523, training loss: 5609.38, average training loss: 5594.23, base loss: 20519.80
[INFO 2017-06-26 13:06:38,123 main.py:50] epoch 6524, training loss: 5527.40, average training loss: 5594.08, base loss: 20519.53
[INFO 2017-06-26 13:06:38,481 main.py:50] epoch 6525, training loss: 5562.64, average training loss: 5594.09, base loss: 20519.62
[INFO 2017-06-26 13:06:38,842 main.py:50] epoch 6526, training loss: 5542.87, average training loss: 5594.03, base loss: 20519.67
[INFO 2017-06-26 13:06:39,200 main.py:50] epoch 6527, training loss: 5566.63, average training loss: 5593.95, base loss: 20519.40
[INFO 2017-06-26 13:06:39,559 main.py:50] epoch 6528, training loss: 5593.78, average training loss: 5593.94, base loss: 20519.14
[INFO 2017-06-26 13:06:39,919 main.py:50] epoch 6529, training loss: 5590.94, average training loss: 5593.88, base loss: 20519.32
[INFO 2017-06-26 13:06:40,278 main.py:50] epoch 6530, training loss: 5529.87, average training loss: 5593.81, base loss: 20519.26
[INFO 2017-06-26 13:06:40,639 main.py:50] epoch 6531, training loss: 5545.48, average training loss: 5593.67, base loss: 20519.39
[INFO 2017-06-26 13:06:40,999 main.py:50] epoch 6532, training loss: 5561.95, average training loss: 5593.62, base loss: 20519.50
[INFO 2017-06-26 13:06:41,357 main.py:50] epoch 6533, training loss: 5540.84, average training loss: 5593.55, base loss: 20519.79
[INFO 2017-06-26 13:06:41,718 main.py:50] epoch 6534, training loss: 5532.80, average training loss: 5593.41, base loss: 20519.87
[INFO 2017-06-26 13:06:42,077 main.py:50] epoch 6535, training loss: 5607.73, average training loss: 5593.40, base loss: 20519.65
[INFO 2017-06-26 13:06:42,436 main.py:50] epoch 6536, training loss: 5530.61, average training loss: 5593.31, base loss: 20519.34
[INFO 2017-06-26 13:06:42,794 main.py:50] epoch 6537, training loss: 5581.15, average training loss: 5593.28, base loss: 20518.99
[INFO 2017-06-26 13:06:43,152 main.py:50] epoch 6538, training loss: 5552.43, average training loss: 5593.20, base loss: 20519.19
[INFO 2017-06-26 13:06:43,510 main.py:50] epoch 6539, training loss: 5553.55, average training loss: 5593.15, base loss: 20519.38
[INFO 2017-06-26 13:06:43,870 main.py:50] epoch 6540, training loss: 5557.61, average training loss: 5593.09, base loss: 20519.37
[INFO 2017-06-26 13:06:44,230 main.py:50] epoch 6541, training loss: 5597.73, average training loss: 5593.07, base loss: 20519.49
[INFO 2017-06-26 13:06:44,591 main.py:50] epoch 6542, training loss: 5563.62, average training loss: 5593.02, base loss: 20519.29
[INFO 2017-06-26 13:06:44,949 main.py:50] epoch 6543, training loss: 5536.94, average training loss: 5592.93, base loss: 20519.80
[INFO 2017-06-26 13:06:45,308 main.py:50] epoch 6544, training loss: 5573.77, average training loss: 5592.88, base loss: 20519.78
[INFO 2017-06-26 13:06:45,670 main.py:50] epoch 6545, training loss: 5551.78, average training loss: 5592.77, base loss: 20520.28
[INFO 2017-06-26 13:06:46,028 main.py:50] epoch 6546, training loss: 5548.26, average training loss: 5592.68, base loss: 20520.36
[INFO 2017-06-26 13:06:46,388 main.py:50] epoch 6547, training loss: 5534.24, average training loss: 5592.62, base loss: 20520.62
[INFO 2017-06-26 13:06:46,747 main.py:50] epoch 6548, training loss: 5596.23, average training loss: 5592.61, base loss: 20520.54
[INFO 2017-06-26 13:06:47,107 main.py:50] epoch 6549, training loss: 5590.72, average training loss: 5592.59, base loss: 20520.88
[INFO 2017-06-26 13:06:47,466 main.py:50] epoch 6550, training loss: 5612.74, average training loss: 5592.57, base loss: 20520.67
[INFO 2017-06-26 13:06:47,826 main.py:50] epoch 6551, training loss: 5499.74, average training loss: 5592.40, base loss: 20520.20
[INFO 2017-06-26 13:06:48,185 main.py:50] epoch 6552, training loss: 5632.38, average training loss: 5592.47, base loss: 20520.17
[INFO 2017-06-26 13:06:48,545 main.py:50] epoch 6553, training loss: 5525.07, average training loss: 5592.35, base loss: 20520.02
[INFO 2017-06-26 13:06:48,903 main.py:50] epoch 6554, training loss: 5578.49, average training loss: 5592.32, base loss: 20519.83
[INFO 2017-06-26 13:06:49,261 main.py:50] epoch 6555, training loss: 5616.99, average training loss: 5592.27, base loss: 20520.34
[INFO 2017-06-26 13:06:49,621 main.py:50] epoch 6556, training loss: 5544.84, average training loss: 5592.18, base loss: 20520.08
[INFO 2017-06-26 13:06:49,980 main.py:50] epoch 6557, training loss: 5488.84, average training loss: 5592.00, base loss: 20520.11
[INFO 2017-06-26 13:06:50,339 main.py:50] epoch 6558, training loss: 5509.04, average training loss: 5591.84, base loss: 20519.99
[INFO 2017-06-26 13:06:50,699 main.py:50] epoch 6559, training loss: 5567.83, average training loss: 5591.72, base loss: 20519.78
[INFO 2017-06-26 13:06:51,058 main.py:50] epoch 6560, training loss: 5497.33, average training loss: 5591.58, base loss: 20519.72
[INFO 2017-06-26 13:06:51,418 main.py:50] epoch 6561, training loss: 5516.86, average training loss: 5591.45, base loss: 20519.70
[INFO 2017-06-26 13:06:51,779 main.py:50] epoch 6562, training loss: 5555.07, average training loss: 5591.32, base loss: 20519.65
[INFO 2017-06-26 13:06:52,138 main.py:50] epoch 6563, training loss: 5486.98, average training loss: 5591.20, base loss: 20519.81
[INFO 2017-06-26 13:06:52,498 main.py:50] epoch 6564, training loss: 5502.84, average training loss: 5591.11, base loss: 20519.84
[INFO 2017-06-26 13:06:52,858 main.py:50] epoch 6565, training loss: 5568.45, average training loss: 5591.11, base loss: 20519.82
[INFO 2017-06-26 13:06:53,218 main.py:50] epoch 6566, training loss: 5594.40, average training loss: 5591.03, base loss: 20519.89
[INFO 2017-06-26 13:06:53,578 main.py:50] epoch 6567, training loss: 5585.29, average training loss: 5590.95, base loss: 20519.57
[INFO 2017-06-26 13:06:53,937 main.py:50] epoch 6568, training loss: 5523.77, average training loss: 5590.85, base loss: 20518.93
[INFO 2017-06-26 13:06:54,296 main.py:50] epoch 6569, training loss: 5476.20, average training loss: 5590.68, base loss: 20518.83
[INFO 2017-06-26 13:06:54,657 main.py:50] epoch 6570, training loss: 5516.64, average training loss: 5590.55, base loss: 20519.16
[INFO 2017-06-26 13:06:55,016 main.py:50] epoch 6571, training loss: 5554.34, average training loss: 5590.47, base loss: 20519.44
[INFO 2017-06-26 13:06:55,376 main.py:50] epoch 6572, training loss: 5503.69, average training loss: 5590.34, base loss: 20519.27
[INFO 2017-06-26 13:06:55,737 main.py:50] epoch 6573, training loss: 5437.70, average training loss: 5590.16, base loss: 20519.28
[INFO 2017-06-26 13:06:56,097 main.py:50] epoch 6574, training loss: 5462.11, average training loss: 5590.00, base loss: 20519.31
[INFO 2017-06-26 13:06:56,457 main.py:50] epoch 6575, training loss: 5474.68, average training loss: 5589.81, base loss: 20519.20
[INFO 2017-06-26 13:06:56,818 main.py:50] epoch 6576, training loss: 5488.96, average training loss: 5589.57, base loss: 20518.65
[INFO 2017-06-26 13:06:57,178 main.py:50] epoch 6577, training loss: 5503.19, average training loss: 5589.43, base loss: 20518.54
[INFO 2017-06-26 13:06:57,539 main.py:50] epoch 6578, training loss: 5598.96, average training loss: 5589.35, base loss: 20518.57
[INFO 2017-06-26 13:06:57,902 main.py:50] epoch 6579, training loss: 5547.67, average training loss: 5589.29, base loss: 20518.88
[INFO 2017-06-26 13:06:58,261 main.py:50] epoch 6580, training loss: 5501.85, average training loss: 5589.06, base loss: 20519.08
[INFO 2017-06-26 13:06:58,621 main.py:50] epoch 6581, training loss: 5586.48, average training loss: 5588.97, base loss: 20519.19
[INFO 2017-06-26 13:06:58,981 main.py:50] epoch 6582, training loss: 5617.69, average training loss: 5588.85, base loss: 20519.01
[INFO 2017-06-26 13:06:59,340 main.py:50] epoch 6583, training loss: 5577.29, average training loss: 5588.70, base loss: 20519.12
[INFO 2017-06-26 13:06:59,700 main.py:50] epoch 6584, training loss: 5590.23, average training loss: 5588.63, base loss: 20519.20
[INFO 2017-06-26 13:07:00,060 main.py:50] epoch 6585, training loss: 5593.99, average training loss: 5588.49, base loss: 20519.09
[INFO 2017-06-26 13:07:00,424 main.py:50] epoch 6586, training loss: 5561.05, average training loss: 5588.43, base loss: 20518.87
[INFO 2017-06-26 13:07:00,783 main.py:50] epoch 6587, training loss: 5478.54, average training loss: 5588.22, base loss: 20518.60
[INFO 2017-06-26 13:07:01,143 main.py:50] epoch 6588, training loss: 5539.50, average training loss: 5588.04, base loss: 20518.69
[INFO 2017-06-26 13:07:01,502 main.py:50] epoch 6589, training loss: 5569.78, average training loss: 5587.86, base loss: 20518.80
[INFO 2017-06-26 13:07:01,862 main.py:50] epoch 6590, training loss: 5613.28, average training loss: 5587.82, base loss: 20518.81
[INFO 2017-06-26 13:07:02,221 main.py:50] epoch 6591, training loss: 5599.26, average training loss: 5587.78, base loss: 20518.92
[INFO 2017-06-26 13:07:02,581 main.py:50] epoch 6592, training loss: 5559.25, average training loss: 5587.71, base loss: 20519.00
[INFO 2017-06-26 13:07:02,941 main.py:50] epoch 6593, training loss: 5549.94, average training loss: 5587.66, base loss: 20519.12
[INFO 2017-06-26 13:07:03,300 main.py:50] epoch 6594, training loss: 5525.66, average training loss: 5587.55, base loss: 20518.62
[INFO 2017-06-26 13:07:03,660 main.py:50] epoch 6595, training loss: 5547.39, average training loss: 5587.41, base loss: 20518.33
[INFO 2017-06-26 13:07:04,021 main.py:50] epoch 6596, training loss: 5535.88, average training loss: 5587.32, base loss: 20518.10
[INFO 2017-06-26 13:07:04,380 main.py:50] epoch 6597, training loss: 5526.74, average training loss: 5587.22, base loss: 20518.19
[INFO 2017-06-26 13:07:04,741 main.py:50] epoch 6598, training loss: 5526.40, average training loss: 5587.02, base loss: 20518.40
[INFO 2017-06-26 13:07:05,100 main.py:50] epoch 6599, training loss: 5493.63, average training loss: 5586.86, base loss: 20518.14
[INFO 2017-06-26 13:07:05,100 main.py:52] epoch 6599, testing
[INFO 2017-06-26 13:07:06,595 main.py:103] average testing loss: 5537.56, base loss: 20588.24
[INFO 2017-06-26 13:07:06,596 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 13:07:06,602 main.py:76] current best accuracy: 5537.56
[INFO 2017-06-26 13:07:06,961 main.py:50] epoch 6600, training loss: 5536.86, average training loss: 5586.75, base loss: 20518.00
[INFO 2017-06-26 13:07:07,320 main.py:50] epoch 6601, training loss: 5492.49, average training loss: 5586.61, base loss: 20518.12
[INFO 2017-06-26 13:07:07,679 main.py:50] epoch 6602, training loss: 5535.37, average training loss: 5586.57, base loss: 20518.24
[INFO 2017-06-26 13:07:08,037 main.py:50] epoch 6603, training loss: 5513.50, average training loss: 5586.49, base loss: 20518.22
[INFO 2017-06-26 13:07:08,396 main.py:50] epoch 6604, training loss: 5494.69, average training loss: 5586.31, base loss: 20518.04
[INFO 2017-06-26 13:07:08,755 main.py:50] epoch 6605, training loss: 5529.16, average training loss: 5586.23, base loss: 20517.96
[INFO 2017-06-26 13:07:09,115 main.py:50] epoch 6606, training loss: 5547.00, average training loss: 5586.19, base loss: 20517.43
[INFO 2017-06-26 13:07:09,474 main.py:50] epoch 6607, training loss: 5573.80, average training loss: 5586.09, base loss: 20517.57
[INFO 2017-06-26 13:07:09,833 main.py:50] epoch 6608, training loss: 5554.99, average training loss: 5586.03, base loss: 20517.97
[INFO 2017-06-26 13:07:10,192 main.py:50] epoch 6609, training loss: 5525.76, average training loss: 5585.87, base loss: 20517.80
[INFO 2017-06-26 13:07:10,551 main.py:50] epoch 6610, training loss: 5536.81, average training loss: 5585.80, base loss: 20518.16
[INFO 2017-06-26 13:07:10,909 main.py:50] epoch 6611, training loss: 5503.03, average training loss: 5585.65, base loss: 20518.25
[INFO 2017-06-26 13:07:11,268 main.py:50] epoch 6612, training loss: 5535.32, average training loss: 5585.52, base loss: 20518.14
[INFO 2017-06-26 13:07:11,628 main.py:50] epoch 6613, training loss: 5545.66, average training loss: 5585.41, base loss: 20517.87
[INFO 2017-06-26 13:07:11,986 main.py:50] epoch 6614, training loss: 5588.42, average training loss: 5585.31, base loss: 20518.24
[INFO 2017-06-26 13:07:12,344 main.py:50] epoch 6615, training loss: 5543.17, average training loss: 5585.26, base loss: 20518.75
[INFO 2017-06-26 13:07:12,703 main.py:50] epoch 6616, training loss: 5582.49, average training loss: 5585.15, base loss: 20518.38
[INFO 2017-06-26 13:07:13,062 main.py:50] epoch 6617, training loss: 5584.78, average training loss: 5585.09, base loss: 20518.49
[INFO 2017-06-26 13:07:13,421 main.py:50] epoch 6618, training loss: 5492.59, average training loss: 5584.93, base loss: 20518.44
[INFO 2017-06-26 13:07:13,781 main.py:50] epoch 6619, training loss: 5575.71, average training loss: 5584.83, base loss: 20518.43
[INFO 2017-06-26 13:07:14,139 main.py:50] epoch 6620, training loss: 5563.95, average training loss: 5584.75, base loss: 20519.04
[INFO 2017-06-26 13:07:14,498 main.py:50] epoch 6621, training loss: 5580.61, average training loss: 5584.68, base loss: 20519.27
[INFO 2017-06-26 13:07:14,856 main.py:50] epoch 6622, training loss: 5545.54, average training loss: 5584.55, base loss: 20519.13
[INFO 2017-06-26 13:07:15,213 main.py:50] epoch 6623, training loss: 5529.91, average training loss: 5584.52, base loss: 20519.10
[INFO 2017-06-26 13:07:15,572 main.py:50] epoch 6624, training loss: 5572.15, average training loss: 5584.43, base loss: 20519.04
[INFO 2017-06-26 13:07:15,933 main.py:50] epoch 6625, training loss: 5547.28, average training loss: 5584.27, base loss: 20518.64
[INFO 2017-06-26 13:07:16,294 main.py:50] epoch 6626, training loss: 5574.15, average training loss: 5584.11, base loss: 20518.45
[INFO 2017-06-26 13:07:16,652 main.py:50] epoch 6627, training loss: 5493.56, average training loss: 5584.00, base loss: 20518.37
[INFO 2017-06-26 13:07:17,012 main.py:50] epoch 6628, training loss: 5501.54, average training loss: 5583.77, base loss: 20517.88
[INFO 2017-06-26 13:07:17,370 main.py:50] epoch 6629, training loss: 5534.57, average training loss: 5583.70, base loss: 20518.17
[INFO 2017-06-26 13:07:17,727 main.py:50] epoch 6630, training loss: 5571.92, average training loss: 5583.60, base loss: 20518.67
[INFO 2017-06-26 13:07:18,086 main.py:50] epoch 6631, training loss: 5607.91, average training loss: 5583.52, base loss: 20518.35
[INFO 2017-06-26 13:07:18,443 main.py:50] epoch 6632, training loss: 5561.64, average training loss: 5583.47, base loss: 20518.63
[INFO 2017-06-26 13:07:18,801 main.py:50] epoch 6633, training loss: 5562.33, average training loss: 5583.32, base loss: 20518.70
[INFO 2017-06-26 13:07:19,161 main.py:50] epoch 6634, training loss: 5497.16, average training loss: 5583.10, base loss: 20518.58
[INFO 2017-06-26 13:07:19,519 main.py:50] epoch 6635, training loss: 5596.40, average training loss: 5583.02, base loss: 20518.92
[INFO 2017-06-26 13:07:19,878 main.py:50] epoch 6636, training loss: 5556.77, average training loss: 5582.87, base loss: 20518.66
[INFO 2017-06-26 13:07:20,236 main.py:50] epoch 6637, training loss: 5592.68, average training loss: 5582.76, base loss: 20518.15
[INFO 2017-06-26 13:07:20,595 main.py:50] epoch 6638, training loss: 5577.87, average training loss: 5582.67, base loss: 20518.47
[INFO 2017-06-26 13:07:20,952 main.py:50] epoch 6639, training loss: 5520.16, average training loss: 5582.55, base loss: 20518.21
[INFO 2017-06-26 13:07:21,312 main.py:50] epoch 6640, training loss: 5504.88, average training loss: 5582.41, base loss: 20517.59
[INFO 2017-06-26 13:07:21,670 main.py:50] epoch 6641, training loss: 5575.09, average training loss: 5582.36, base loss: 20517.49
[INFO 2017-06-26 13:07:22,029 main.py:50] epoch 6642, training loss: 5506.87, average training loss: 5582.27, base loss: 20517.51
[INFO 2017-06-26 13:07:22,386 main.py:50] epoch 6643, training loss: 5503.27, average training loss: 5582.17, base loss: 20517.64
[INFO 2017-06-26 13:07:22,746 main.py:50] epoch 6644, training loss: 5515.50, average training loss: 5582.06, base loss: 20517.48
[INFO 2017-06-26 13:07:23,104 main.py:50] epoch 6645, training loss: 5565.23, average training loss: 5581.99, base loss: 20517.84
[INFO 2017-06-26 13:07:23,463 main.py:50] epoch 6646, training loss: 5524.81, average training loss: 5581.81, base loss: 20517.46
[INFO 2017-06-26 13:07:23,822 main.py:50] epoch 6647, training loss: 5543.84, average training loss: 5581.75, base loss: 20517.39
[INFO 2017-06-26 13:07:24,181 main.py:50] epoch 6648, training loss: 5576.40, average training loss: 5581.74, base loss: 20518.02
[INFO 2017-06-26 13:07:24,539 main.py:50] epoch 6649, training loss: 5580.88, average training loss: 5581.67, base loss: 20517.77
[INFO 2017-06-26 13:07:24,899 main.py:50] epoch 6650, training loss: 5501.44, average training loss: 5581.49, base loss: 20517.64
[INFO 2017-06-26 13:07:25,257 main.py:50] epoch 6651, training loss: 5540.68, average training loss: 5581.39, base loss: 20517.18
[INFO 2017-06-26 13:07:25,617 main.py:50] epoch 6652, training loss: 5443.60, average training loss: 5581.20, base loss: 20517.28
[INFO 2017-06-26 13:07:25,976 main.py:50] epoch 6653, training loss: 5451.08, average training loss: 5581.01, base loss: 20517.07
[INFO 2017-06-26 13:07:26,335 main.py:50] epoch 6654, training loss: 5532.02, average training loss: 5580.87, base loss: 20517.01
[INFO 2017-06-26 13:07:26,694 main.py:50] epoch 6655, training loss: 5487.06, average training loss: 5580.72, base loss: 20516.96
[INFO 2017-06-26 13:07:27,053 main.py:50] epoch 6656, training loss: 5520.36, average training loss: 5580.59, base loss: 20516.93
[INFO 2017-06-26 13:07:27,411 main.py:50] epoch 6657, training loss: 5550.99, average training loss: 5580.57, base loss: 20516.89
[INFO 2017-06-26 13:07:27,770 main.py:50] epoch 6658, training loss: 5470.91, average training loss: 5580.45, base loss: 20516.57
[INFO 2017-06-26 13:07:28,127 main.py:50] epoch 6659, training loss: 5464.69, average training loss: 5580.33, base loss: 20516.53
[INFO 2017-06-26 13:07:28,486 main.py:50] epoch 6660, training loss: 5611.43, average training loss: 5580.32, base loss: 20516.93
[INFO 2017-06-26 13:07:28,845 main.py:50] epoch 6661, training loss: 5551.35, average training loss: 5580.20, base loss: 20516.65
[INFO 2017-06-26 13:07:29,204 main.py:50] epoch 6662, training loss: 5533.66, average training loss: 5580.09, base loss: 20516.37
[INFO 2017-06-26 13:07:29,564 main.py:50] epoch 6663, training loss: 5484.45, average training loss: 5579.95, base loss: 20516.29
[INFO 2017-06-26 13:07:29,924 main.py:50] epoch 6664, training loss: 5478.18, average training loss: 5579.81, base loss: 20516.04
[INFO 2017-06-26 13:07:30,282 main.py:50] epoch 6665, training loss: 5561.91, average training loss: 5579.73, base loss: 20516.19
[INFO 2017-06-26 13:07:30,641 main.py:50] epoch 6666, training loss: 5549.58, average training loss: 5579.68, base loss: 20516.24
[INFO 2017-06-26 13:07:31,000 main.py:50] epoch 6667, training loss: 5492.77, average training loss: 5579.58, base loss: 20516.39
[INFO 2017-06-26 13:07:31,359 main.py:50] epoch 6668, training loss: 5523.82, average training loss: 5579.47, base loss: 20516.63
[INFO 2017-06-26 13:07:31,718 main.py:50] epoch 6669, training loss: 5582.99, average training loss: 5579.47, base loss: 20516.61
[INFO 2017-06-26 13:07:32,077 main.py:50] epoch 6670, training loss: 5486.75, average training loss: 5579.33, base loss: 20516.43
[INFO 2017-06-26 13:07:32,435 main.py:50] epoch 6671, training loss: 5506.06, average training loss: 5579.27, base loss: 20516.39
[INFO 2017-06-26 13:07:32,795 main.py:50] epoch 6672, training loss: 5526.16, average training loss: 5579.20, base loss: 20516.26
[INFO 2017-06-26 13:07:33,155 main.py:50] epoch 6673, training loss: 5534.69, average training loss: 5579.15, base loss: 20516.39
[INFO 2017-06-26 13:07:33,515 main.py:50] epoch 6674, training loss: 5510.40, average training loss: 5579.07, base loss: 20516.38
[INFO 2017-06-26 13:07:33,875 main.py:50] epoch 6675, training loss: 5525.64, average training loss: 5579.05, base loss: 20516.18
[INFO 2017-06-26 13:07:34,233 main.py:50] epoch 6676, training loss: 5524.51, average training loss: 5578.97, base loss: 20516.30
[INFO 2017-06-26 13:07:34,591 main.py:50] epoch 6677, training loss: 5495.20, average training loss: 5578.80, base loss: 20515.77
[INFO 2017-06-26 13:07:34,949 main.py:50] epoch 6678, training loss: 5554.54, average training loss: 5578.74, base loss: 20515.83
[INFO 2017-06-26 13:07:35,308 main.py:50] epoch 6679, training loss: 5547.24, average training loss: 5578.66, base loss: 20516.33
[INFO 2017-06-26 13:07:35,667 main.py:50] epoch 6680, training loss: 5538.65, average training loss: 5578.64, base loss: 20516.50
[INFO 2017-06-26 13:07:36,027 main.py:50] epoch 6681, training loss: 5532.01, average training loss: 5578.57, base loss: 20516.16
[INFO 2017-06-26 13:07:36,384 main.py:50] epoch 6682, training loss: 5557.41, average training loss: 5578.53, base loss: 20516.38
[INFO 2017-06-26 13:07:36,743 main.py:50] epoch 6683, training loss: 5621.91, average training loss: 5578.55, base loss: 20516.76
[INFO 2017-06-26 13:07:37,101 main.py:50] epoch 6684, training loss: 5573.88, average training loss: 5578.58, base loss: 20517.24
[INFO 2017-06-26 13:07:37,459 main.py:50] epoch 6685, training loss: 5522.50, average training loss: 5578.49, base loss: 20517.09
[INFO 2017-06-26 13:07:37,819 main.py:50] epoch 6686, training loss: 5583.09, average training loss: 5578.53, base loss: 20517.34
[INFO 2017-06-26 13:07:38,178 main.py:50] epoch 6687, training loss: 5563.35, average training loss: 5578.45, base loss: 20517.23
[INFO 2017-06-26 13:07:38,537 main.py:50] epoch 6688, training loss: 5562.27, average training loss: 5578.37, base loss: 20517.15
[INFO 2017-06-26 13:07:38,897 main.py:50] epoch 6689, training loss: 5573.44, average training loss: 5578.34, base loss: 20517.72
[INFO 2017-06-26 13:07:39,256 main.py:50] epoch 6690, training loss: 5606.54, average training loss: 5578.35, base loss: 20517.67
[INFO 2017-06-26 13:07:39,615 main.py:50] epoch 6691, training loss: 5561.23, average training loss: 5578.36, base loss: 20517.71
[INFO 2017-06-26 13:07:39,974 main.py:50] epoch 6692, training loss: 5504.57, average training loss: 5578.22, base loss: 20517.73
[INFO 2017-06-26 13:07:40,333 main.py:50] epoch 6693, training loss: 5502.25, average training loss: 5578.18, base loss: 20517.80
[INFO 2017-06-26 13:07:40,693 main.py:50] epoch 6694, training loss: 5492.67, average training loss: 5578.06, base loss: 20517.81
[INFO 2017-06-26 13:07:41,052 main.py:50] epoch 6695, training loss: 5521.73, average training loss: 5578.03, base loss: 20517.88
[INFO 2017-06-26 13:07:41,411 main.py:50] epoch 6696, training loss: 5536.72, average training loss: 5577.99, base loss: 20518.20
[INFO 2017-06-26 13:07:41,771 main.py:50] epoch 6697, training loss: 5577.58, average training loss: 5577.99, base loss: 20518.37
[INFO 2017-06-26 13:07:42,130 main.py:50] epoch 6698, training loss: 5492.83, average training loss: 5577.75, base loss: 20518.45
[INFO 2017-06-26 13:07:42,488 main.py:50] epoch 6699, training loss: 5518.77, average training loss: 5577.62, base loss: 20518.47
[INFO 2017-06-26 13:07:42,488 main.py:52] epoch 6699, testing
[INFO 2017-06-26 13:07:43,957 main.py:103] average testing loss: 5520.36, base loss: 20563.42
[INFO 2017-06-26 13:07:43,957 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 13:07:43,967 main.py:76] current best accuracy: 5520.36
[INFO 2017-06-26 13:07:44,327 main.py:50] epoch 6700, training loss: 5568.96, average training loss: 5577.60, base loss: 20518.46
[INFO 2017-06-26 13:07:44,688 main.py:50] epoch 6701, training loss: 5487.48, average training loss: 5577.58, base loss: 20518.61
[INFO 2017-06-26 13:07:45,050 main.py:50] epoch 6702, training loss: 5538.59, average training loss: 5577.49, base loss: 20518.66
[INFO 2017-06-26 13:07:45,411 main.py:50] epoch 6703, training loss: 5605.81, average training loss: 5577.42, base loss: 20518.60
[INFO 2017-06-26 13:07:45,774 main.py:50] epoch 6704, training loss: 5586.14, average training loss: 5577.35, base loss: 20518.41
[INFO 2017-06-26 13:07:46,136 main.py:50] epoch 6705, training loss: 5513.98, average training loss: 5577.19, base loss: 20518.27
[INFO 2017-06-26 13:07:46,497 main.py:50] epoch 6706, training loss: 5490.72, average training loss: 5577.04, base loss: 20517.99
[INFO 2017-06-26 13:07:46,858 main.py:50] epoch 6707, training loss: 5540.04, average training loss: 5576.95, base loss: 20518.34
[INFO 2017-06-26 13:07:47,234 main.py:50] epoch 6708, training loss: 5570.41, average training loss: 5576.91, base loss: 20518.81
[INFO 2017-06-26 13:07:47,597 main.py:50] epoch 6709, training loss: 5581.26, average training loss: 5576.84, base loss: 20518.51
[INFO 2017-06-26 13:07:47,958 main.py:50] epoch 6710, training loss: 5555.45, average training loss: 5576.77, base loss: 20518.52
[INFO 2017-06-26 13:07:48,320 main.py:50] epoch 6711, training loss: 5509.52, average training loss: 5576.62, base loss: 20518.72
[INFO 2017-06-26 13:07:48,681 main.py:50] epoch 6712, training loss: 5525.76, average training loss: 5576.50, base loss: 20518.80
[INFO 2017-06-26 13:07:49,043 main.py:50] epoch 6713, training loss: 5495.52, average training loss: 5576.31, base loss: 20518.71
[INFO 2017-06-26 13:07:49,404 main.py:50] epoch 6714, training loss: 5522.15, average training loss: 5576.16, base loss: 20518.43
[INFO 2017-06-26 13:07:49,764 main.py:50] epoch 6715, training loss: 5431.64, average training loss: 5575.90, base loss: 20518.22
[INFO 2017-06-26 13:07:50,125 main.py:50] epoch 6716, training loss: 5534.05, average training loss: 5575.75, base loss: 20518.16
[INFO 2017-06-26 13:07:50,486 main.py:50] epoch 6717, training loss: 5526.74, average training loss: 5575.60, base loss: 20518.41
[INFO 2017-06-26 13:07:50,846 main.py:50] epoch 6718, training loss: 5494.15, average training loss: 5575.39, base loss: 20518.50
[INFO 2017-06-26 13:07:51,206 main.py:50] epoch 6719, training loss: 5548.32, average training loss: 5575.20, base loss: 20518.57
[INFO 2017-06-26 13:07:51,567 main.py:50] epoch 6720, training loss: 5558.31, average training loss: 5575.03, base loss: 20518.33
[INFO 2017-06-26 13:07:51,927 main.py:50] epoch 6721, training loss: 5538.06, average training loss: 5574.93, base loss: 20518.53
[INFO 2017-06-26 13:07:52,288 main.py:50] epoch 6722, training loss: 5557.82, average training loss: 5574.81, base loss: 20518.84
[INFO 2017-06-26 13:07:52,648 main.py:50] epoch 6723, training loss: 5523.80, average training loss: 5574.72, base loss: 20519.06
[INFO 2017-06-26 13:07:53,009 main.py:50] epoch 6724, training loss: 5527.96, average training loss: 5574.68, base loss: 20519.16
[INFO 2017-06-26 13:07:53,369 main.py:50] epoch 6725, training loss: 5510.50, average training loss: 5574.51, base loss: 20519.05
[INFO 2017-06-26 13:07:53,732 main.py:50] epoch 6726, training loss: 5485.46, average training loss: 5574.35, base loss: 20519.29
[INFO 2017-06-26 13:07:54,092 main.py:50] epoch 6727, training loss: 5646.36, average training loss: 5574.29, base loss: 20519.26
[INFO 2017-06-26 13:07:54,452 main.py:50] epoch 6728, training loss: 5539.78, average training loss: 5574.23, base loss: 20519.10
[INFO 2017-06-26 13:07:54,813 main.py:50] epoch 6729, training loss: 5593.34, average training loss: 5574.18, base loss: 20519.09
[INFO 2017-06-26 13:07:55,178 main.py:50] epoch 6730, training loss: 5541.38, average training loss: 5574.06, base loss: 20519.15
[INFO 2017-06-26 13:07:55,540 main.py:50] epoch 6731, training loss: 5511.40, average training loss: 5573.92, base loss: 20518.53
[INFO 2017-06-26 13:07:55,901 main.py:50] epoch 6732, training loss: 5622.80, average training loss: 5573.83, base loss: 20518.90
[INFO 2017-06-26 13:07:56,261 main.py:50] epoch 6733, training loss: 5531.92, average training loss: 5573.75, base loss: 20518.70
[INFO 2017-06-26 13:07:56,623 main.py:50] epoch 6734, training loss: 5532.05, average training loss: 5573.68, base loss: 20518.28
[INFO 2017-06-26 13:07:56,985 main.py:50] epoch 6735, training loss: 5612.99, average training loss: 5573.65, base loss: 20518.74
[INFO 2017-06-26 13:07:57,347 main.py:50] epoch 6736, training loss: 5615.30, average training loss: 5573.69, base loss: 20518.98
[INFO 2017-06-26 13:07:57,708 main.py:50] epoch 6737, training loss: 5569.00, average training loss: 5573.54, base loss: 20518.52
[INFO 2017-06-26 13:07:58,069 main.py:50] epoch 6738, training loss: 5581.29, average training loss: 5573.45, base loss: 20518.24
[INFO 2017-06-26 13:07:58,429 main.py:50] epoch 6739, training loss: 5567.67, average training loss: 5573.34, base loss: 20518.21
[INFO 2017-06-26 13:07:58,790 main.py:50] epoch 6740, training loss: 5672.09, average training loss: 5573.31, base loss: 20518.35
[INFO 2017-06-26 13:07:59,150 main.py:50] epoch 6741, training loss: 5524.61, average training loss: 5573.20, base loss: 20517.83
[INFO 2017-06-26 13:07:59,516 main.py:50] epoch 6742, training loss: 5564.25, average training loss: 5573.10, base loss: 20517.67
[INFO 2017-06-26 13:07:59,883 main.py:50] epoch 6743, training loss: 5604.70, average training loss: 5573.11, base loss: 20517.69
[INFO 2017-06-26 13:08:00,244 main.py:50] epoch 6744, training loss: 5569.04, average training loss: 5573.06, base loss: 20517.39
[INFO 2017-06-26 13:08:00,604 main.py:50] epoch 6745, training loss: 5533.36, average training loss: 5572.97, base loss: 20517.68
[INFO 2017-06-26 13:08:00,974 main.py:50] epoch 6746, training loss: 5588.81, average training loss: 5572.98, base loss: 20517.93
[INFO 2017-06-26 13:08:01,336 main.py:50] epoch 6747, training loss: 5544.25, average training loss: 5572.85, base loss: 20517.75
[INFO 2017-06-26 13:08:01,699 main.py:50] epoch 6748, training loss: 5533.50, average training loss: 5572.77, base loss: 20517.65
[INFO 2017-06-26 13:08:02,062 main.py:50] epoch 6749, training loss: 5627.72, average training loss: 5572.77, base loss: 20517.59
[INFO 2017-06-26 13:08:02,426 main.py:50] epoch 6750, training loss: 5602.71, average training loss: 5572.70, base loss: 20517.46
[INFO 2017-06-26 13:08:02,785 main.py:50] epoch 6751, training loss: 5586.79, average training loss: 5572.64, base loss: 20517.38
[INFO 2017-06-26 13:08:03,145 main.py:50] epoch 6752, training loss: 5635.92, average training loss: 5572.65, base loss: 20517.14
[INFO 2017-06-26 13:08:03,507 main.py:50] epoch 6753, training loss: 5564.26, average training loss: 5572.53, base loss: 20516.92
[INFO 2017-06-26 13:08:03,869 main.py:50] epoch 6754, training loss: 5553.22, average training loss: 5572.46, base loss: 20517.06
[INFO 2017-06-26 13:08:04,229 main.py:50] epoch 6755, training loss: 5642.08, average training loss: 5572.48, base loss: 20517.54
[INFO 2017-06-26 13:08:04,589 main.py:50] epoch 6756, training loss: 5609.81, average training loss: 5572.52, base loss: 20517.81
[INFO 2017-06-26 13:08:04,951 main.py:50] epoch 6757, training loss: 5528.42, average training loss: 5572.44, base loss: 20518.35
[INFO 2017-06-26 13:08:05,313 main.py:50] epoch 6758, training loss: 5569.02, average training loss: 5572.43, base loss: 20518.27
[INFO 2017-06-26 13:08:05,676 main.py:50] epoch 6759, training loss: 5528.53, average training loss: 5572.34, base loss: 20518.08
[INFO 2017-06-26 13:08:06,038 main.py:50] epoch 6760, training loss: 5632.08, average training loss: 5572.42, base loss: 20518.21
[INFO 2017-06-26 13:08:06,401 main.py:50] epoch 6761, training loss: 5644.09, average training loss: 5572.45, base loss: 20517.87
[INFO 2017-06-26 13:08:06,762 main.py:50] epoch 6762, training loss: 5606.76, average training loss: 5572.39, base loss: 20517.85
[INFO 2017-06-26 13:08:07,123 main.py:50] epoch 6763, training loss: 5600.55, average training loss: 5572.43, base loss: 20517.85
[INFO 2017-06-26 13:08:07,486 main.py:50] epoch 6764, training loss: 5549.73, average training loss: 5572.37, base loss: 20517.89
[INFO 2017-06-26 13:08:07,850 main.py:50] epoch 6765, training loss: 5549.21, average training loss: 5572.32, base loss: 20517.76
[INFO 2017-06-26 13:08:08,214 main.py:50] epoch 6766, training loss: 5554.02, average training loss: 5572.32, base loss: 20518.00
[INFO 2017-06-26 13:08:08,577 main.py:50] epoch 6767, training loss: 5487.04, average training loss: 5572.22, base loss: 20517.71
[INFO 2017-06-26 13:08:08,941 main.py:50] epoch 6768, training loss: 5564.00, average training loss: 5572.19, base loss: 20517.74
[INFO 2017-06-26 13:08:09,307 main.py:50] epoch 6769, training loss: 5507.45, average training loss: 5572.10, base loss: 20517.28
[INFO 2017-06-26 13:08:09,700 main.py:50] epoch 6770, training loss: 5517.71, average training loss: 5572.06, base loss: 20517.25
[INFO 2017-06-26 13:08:10,091 main.py:50] epoch 6771, training loss: 5528.37, average training loss: 5572.03, base loss: 20517.11
[INFO 2017-06-26 13:08:10,481 main.py:50] epoch 6772, training loss: 5539.86, average training loss: 5571.97, base loss: 20517.02
[INFO 2017-06-26 13:08:10,867 main.py:50] epoch 6773, training loss: 5525.77, average training loss: 5571.95, base loss: 20516.97
[INFO 2017-06-26 13:08:11,236 main.py:50] epoch 6774, training loss: 5565.19, average training loss: 5571.94, base loss: 20517.31
[INFO 2017-06-26 13:08:11,637 main.py:50] epoch 6775, training loss: 5530.11, average training loss: 5571.89, base loss: 20517.69
[INFO 2017-06-26 13:08:12,000 main.py:50] epoch 6776, training loss: 5500.87, average training loss: 5571.84, base loss: 20517.77
[INFO 2017-06-26 13:08:12,361 main.py:50] epoch 6777, training loss: 5568.79, average training loss: 5571.85, base loss: 20518.08
[INFO 2017-06-26 13:08:12,722 main.py:50] epoch 6778, training loss: 5572.41, average training loss: 5571.82, base loss: 20517.69
[INFO 2017-06-26 13:08:13,082 main.py:50] epoch 6779, training loss: 5579.15, average training loss: 5571.83, base loss: 20517.72
[INFO 2017-06-26 13:08:13,441 main.py:50] epoch 6780, training loss: 5534.81, average training loss: 5571.76, base loss: 20517.62
[INFO 2017-06-26 13:08:13,801 main.py:50] epoch 6781, training loss: 5515.98, average training loss: 5571.66, base loss: 20517.76
[INFO 2017-06-26 13:08:14,162 main.py:50] epoch 6782, training loss: 5494.19, average training loss: 5571.59, base loss: 20517.76
[INFO 2017-06-26 13:08:14,524 main.py:50] epoch 6783, training loss: 5554.05, average training loss: 5571.57, base loss: 20518.19
[INFO 2017-06-26 13:08:14,885 main.py:50] epoch 6784, training loss: 5591.82, average training loss: 5571.60, base loss: 20518.52
[INFO 2017-06-26 13:08:15,244 main.py:50] epoch 6785, training loss: 5482.10, average training loss: 5571.48, base loss: 20518.37
[INFO 2017-06-26 13:08:15,605 main.py:50] epoch 6786, training loss: 5545.72, average training loss: 5571.44, base loss: 20518.46
[INFO 2017-06-26 13:08:15,965 main.py:50] epoch 6787, training loss: 5556.53, average training loss: 5571.38, base loss: 20518.88
[INFO 2017-06-26 13:08:16,325 main.py:50] epoch 6788, training loss: 5576.30, average training loss: 5571.33, base loss: 20518.85
[INFO 2017-06-26 13:08:16,685 main.py:50] epoch 6789, training loss: 5526.29, average training loss: 5571.29, base loss: 20518.60
[INFO 2017-06-26 13:08:17,044 main.py:50] epoch 6790, training loss: 5457.92, average training loss: 5571.13, base loss: 20518.30
[INFO 2017-06-26 13:08:17,403 main.py:50] epoch 6791, training loss: 5607.70, average training loss: 5571.16, base loss: 20518.47
[INFO 2017-06-26 13:08:17,764 main.py:50] epoch 6792, training loss: 5562.65, average training loss: 5571.10, base loss: 20518.33
[INFO 2017-06-26 13:08:18,123 main.py:50] epoch 6793, training loss: 5523.14, average training loss: 5571.00, base loss: 20518.05
[INFO 2017-06-26 13:08:18,484 main.py:50] epoch 6794, training loss: 5510.87, average training loss: 5570.89, base loss: 20518.03
[INFO 2017-06-26 13:08:18,844 main.py:50] epoch 6795, training loss: 5479.82, average training loss: 5570.70, base loss: 20517.66
[INFO 2017-06-26 13:08:19,204 main.py:50] epoch 6796, training loss: 5521.28, average training loss: 5570.57, base loss: 20517.62
[INFO 2017-06-26 13:08:19,565 main.py:50] epoch 6797, training loss: 5546.58, average training loss: 5570.48, base loss: 20517.71
[INFO 2017-06-26 13:08:19,927 main.py:50] epoch 6798, training loss: 5492.45, average training loss: 5570.46, base loss: 20517.80
[INFO 2017-06-26 13:08:20,286 main.py:50] epoch 6799, training loss: 5535.34, average training loss: 5570.35, base loss: 20517.90
[INFO 2017-06-26 13:08:20,286 main.py:52] epoch 6799, testing
[INFO 2017-06-26 13:08:21,754 main.py:103] average testing loss: 5517.44, base loss: 20432.87
[INFO 2017-06-26 13:08:21,755 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 13:08:21,762 main.py:76] current best accuracy: 5517.44
[INFO 2017-06-26 13:08:22,121 main.py:50] epoch 6800, training loss: 5516.07, average training loss: 5570.27, base loss: 20517.71
[INFO 2017-06-26 13:08:22,480 main.py:50] epoch 6801, training loss: 5542.13, average training loss: 5570.22, base loss: 20517.52
[INFO 2017-06-26 13:08:22,840 main.py:50] epoch 6802, training loss: 5575.49, average training loss: 5570.23, base loss: 20517.36
[INFO 2017-06-26 13:08:23,202 main.py:50] epoch 6803, training loss: 5517.01, average training loss: 5570.17, base loss: 20517.64
[INFO 2017-06-26 13:08:23,562 main.py:50] epoch 6804, training loss: 5550.35, average training loss: 5570.14, base loss: 20518.05
[INFO 2017-06-26 13:08:23,921 main.py:50] epoch 6805, training loss: 5563.54, average training loss: 5570.13, base loss: 20518.18
[INFO 2017-06-26 13:08:24,281 main.py:50] epoch 6806, training loss: 5499.04, average training loss: 5570.01, base loss: 20517.99
[INFO 2017-06-26 13:08:24,641 main.py:50] epoch 6807, training loss: 5587.33, average training loss: 5569.99, base loss: 20518.17
[INFO 2017-06-26 13:08:25,002 main.py:50] epoch 6808, training loss: 5492.76, average training loss: 5569.91, base loss: 20518.39
[INFO 2017-06-26 13:08:25,362 main.py:50] epoch 6809, training loss: 5534.71, average training loss: 5569.79, base loss: 20518.16
[INFO 2017-06-26 13:08:25,722 main.py:50] epoch 6810, training loss: 5483.85, average training loss: 5569.64, base loss: 20517.60
[INFO 2017-06-26 13:08:26,081 main.py:50] epoch 6811, training loss: 5521.37, average training loss: 5569.57, base loss: 20517.44
[INFO 2017-06-26 13:08:26,443 main.py:50] epoch 6812, training loss: 5541.37, average training loss: 5569.47, base loss: 20517.90
[INFO 2017-06-26 13:08:26,803 main.py:50] epoch 6813, training loss: 5600.23, average training loss: 5569.49, base loss: 20518.06
[INFO 2017-06-26 13:08:27,163 main.py:50] epoch 6814, training loss: 5512.44, average training loss: 5569.38, base loss: 20518.40
[INFO 2017-06-26 13:08:27,525 main.py:50] epoch 6815, training loss: 5559.35, average training loss: 5569.30, base loss: 20518.28
[INFO 2017-06-26 13:08:27,888 main.py:50] epoch 6816, training loss: 5521.80, average training loss: 5569.22, base loss: 20518.31
[INFO 2017-06-26 13:08:28,250 main.py:50] epoch 6817, training loss: 5533.56, average training loss: 5569.12, base loss: 20518.49
[INFO 2017-06-26 13:08:28,645 main.py:50] epoch 6818, training loss: 5541.02, average training loss: 5569.11, base loss: 20518.58
[INFO 2017-06-26 13:08:29,007 main.py:50] epoch 6819, training loss: 5492.49, average training loss: 5569.07, base loss: 20518.71
[INFO 2017-06-26 13:08:29,368 main.py:50] epoch 6820, training loss: 5497.15, average training loss: 5568.96, base loss: 20518.75
[INFO 2017-06-26 13:08:29,729 main.py:50] epoch 6821, training loss: 5546.57, average training loss: 5568.86, base loss: 20518.66
[INFO 2017-06-26 13:08:30,097 main.py:50] epoch 6822, training loss: 5554.39, average training loss: 5568.82, base loss: 20518.43
[INFO 2017-06-26 13:08:30,456 main.py:50] epoch 6823, training loss: 5487.47, average training loss: 5568.66, base loss: 20518.19
[INFO 2017-06-26 13:08:30,816 main.py:50] epoch 6824, training loss: 5480.33, average training loss: 5568.52, base loss: 20518.22
[INFO 2017-06-26 13:08:31,177 main.py:50] epoch 6825, training loss: 5535.72, average training loss: 5568.42, base loss: 20518.18
[INFO 2017-06-26 13:08:31,539 main.py:50] epoch 6826, training loss: 5471.97, average training loss: 5568.32, base loss: 20518.09
[INFO 2017-06-26 13:08:31,913 main.py:50] epoch 6827, training loss: 5472.21, average training loss: 5568.21, base loss: 20517.83
[INFO 2017-06-26 13:08:32,274 main.py:50] epoch 6828, training loss: 5553.61, average training loss: 5568.17, base loss: 20517.94
[INFO 2017-06-26 13:08:32,634 main.py:50] epoch 6829, training loss: 5500.50, average training loss: 5568.03, base loss: 20517.85
[INFO 2017-06-26 13:08:32,993 main.py:50] epoch 6830, training loss: 5471.42, average training loss: 5567.93, base loss: 20517.55
[INFO 2017-06-26 13:08:33,352 main.py:50] epoch 6831, training loss: 5513.67, average training loss: 5567.87, base loss: 20517.97
[INFO 2017-06-26 13:08:33,712 main.py:50] epoch 6832, training loss: 5563.20, average training loss: 5567.86, base loss: 20518.12
[INFO 2017-06-26 13:08:34,071 main.py:50] epoch 6833, training loss: 5504.90, average training loss: 5567.79, base loss: 20517.89
[INFO 2017-06-26 13:08:34,430 main.py:50] epoch 6834, training loss: 5529.40, average training loss: 5567.72, base loss: 20517.93
[INFO 2017-06-26 13:08:34,790 main.py:50] epoch 6835, training loss: 5534.77, average training loss: 5567.70, base loss: 20517.84
[INFO 2017-06-26 13:08:35,151 main.py:50] epoch 6836, training loss: 5531.52, average training loss: 5567.62, base loss: 20517.49
[INFO 2017-06-26 13:08:35,511 main.py:50] epoch 6837, training loss: 5606.69, average training loss: 5567.59, base loss: 20517.64
[INFO 2017-06-26 13:08:35,872 main.py:50] epoch 6838, training loss: 5440.49, average training loss: 5567.40, base loss: 20517.24
[INFO 2017-06-26 13:08:36,233 main.py:50] epoch 6839, training loss: 5486.92, average training loss: 5567.24, base loss: 20516.60
[INFO 2017-06-26 13:08:36,593 main.py:50] epoch 6840, training loss: 5526.64, average training loss: 5567.17, base loss: 20516.90
[INFO 2017-06-26 13:08:36,955 main.py:50] epoch 6841, training loss: 5506.80, average training loss: 5567.12, base loss: 20517.12
[INFO 2017-06-26 13:08:37,316 main.py:50] epoch 6842, training loss: 5495.64, average training loss: 5567.03, base loss: 20516.68
[INFO 2017-06-26 13:08:37,677 main.py:50] epoch 6843, training loss: 5561.59, average training loss: 5566.99, base loss: 20516.44
[INFO 2017-06-26 13:08:38,038 main.py:50] epoch 6844, training loss: 5517.48, average training loss: 5566.91, base loss: 20516.17
[INFO 2017-06-26 13:08:38,399 main.py:50] epoch 6845, training loss: 5563.97, average training loss: 5566.81, base loss: 20516.33
[INFO 2017-06-26 13:08:38,760 main.py:50] epoch 6846, training loss: 5500.78, average training loss: 5566.61, base loss: 20516.21
[INFO 2017-06-26 13:08:39,121 main.py:50] epoch 6847, training loss: 5547.67, average training loss: 5566.56, base loss: 20516.59
[INFO 2017-06-26 13:08:39,481 main.py:50] epoch 6848, training loss: 5550.56, average training loss: 5566.55, base loss: 20516.86
[INFO 2017-06-26 13:08:39,842 main.py:50] epoch 6849, training loss: 5534.20, average training loss: 5566.44, base loss: 20516.99
[INFO 2017-06-26 13:08:40,203 main.py:50] epoch 6850, training loss: 5582.67, average training loss: 5566.37, base loss: 20517.13
[INFO 2017-06-26 13:08:40,564 main.py:50] epoch 6851, training loss: 5638.36, average training loss: 5566.35, base loss: 20517.35
[INFO 2017-06-26 13:08:40,926 main.py:50] epoch 6852, training loss: 5573.53, average training loss: 5566.30, base loss: 20517.44
[INFO 2017-06-26 13:08:41,287 main.py:50] epoch 6853, training loss: 5572.47, average training loss: 5566.26, base loss: 20517.07
[INFO 2017-06-26 13:08:41,648 main.py:50] epoch 6854, training loss: 5569.68, average training loss: 5566.19, base loss: 20517.30
[INFO 2017-06-26 13:08:42,043 main.py:50] epoch 6855, training loss: 5572.04, average training loss: 5566.07, base loss: 20517.32
[INFO 2017-06-26 13:08:42,428 main.py:50] epoch 6856, training loss: 5578.12, average training loss: 5565.99, base loss: 20517.53
[INFO 2017-06-26 13:08:42,804 main.py:50] epoch 6857, training loss: 5515.79, average training loss: 5565.93, base loss: 20517.59
[INFO 2017-06-26 13:08:43,166 main.py:50] epoch 6858, training loss: 5516.00, average training loss: 5565.80, base loss: 20517.59
[INFO 2017-06-26 13:08:43,527 main.py:50] epoch 6859, training loss: 5581.69, average training loss: 5565.80, base loss: 20517.33
[INFO 2017-06-26 13:08:43,889 main.py:50] epoch 6860, training loss: 5526.30, average training loss: 5565.69, base loss: 20517.68
[INFO 2017-06-26 13:08:44,249 main.py:50] epoch 6861, training loss: 5562.48, average training loss: 5565.65, base loss: 20517.57
[INFO 2017-06-26 13:08:44,609 main.py:50] epoch 6862, training loss: 5550.74, average training loss: 5565.57, base loss: 20517.64
[INFO 2017-06-26 13:08:44,970 main.py:50] epoch 6863, training loss: 5585.60, average training loss: 5565.49, base loss: 20516.99
[INFO 2017-06-26 13:08:45,331 main.py:50] epoch 6864, training loss: 5619.15, average training loss: 5565.50, base loss: 20516.94
[INFO 2017-06-26 13:08:45,692 main.py:50] epoch 6865, training loss: 5598.24, average training loss: 5565.52, base loss: 20517.73
[INFO 2017-06-26 13:08:46,053 main.py:50] epoch 6866, training loss: 5627.75, average training loss: 5565.56, base loss: 20517.53
[INFO 2017-06-26 13:08:46,414 main.py:50] epoch 6867, training loss: 5546.44, average training loss: 5565.45, base loss: 20517.40
[INFO 2017-06-26 13:08:46,776 main.py:50] epoch 6868, training loss: 5592.71, average training loss: 5565.44, base loss: 20517.21
[INFO 2017-06-26 13:08:47,138 main.py:50] epoch 6869, training loss: 5612.61, average training loss: 5565.49, base loss: 20517.43
[INFO 2017-06-26 13:08:47,499 main.py:50] epoch 6870, training loss: 5499.47, average training loss: 5565.42, base loss: 20517.22
[INFO 2017-06-26 13:08:47,861 main.py:50] epoch 6871, training loss: 5551.24, average training loss: 5565.28, base loss: 20516.90
[INFO 2017-06-26 13:08:48,254 main.py:50] epoch 6872, training loss: 5553.20, average training loss: 5565.24, base loss: 20516.69
[INFO 2017-06-26 13:08:48,631 main.py:50] epoch 6873, training loss: 5584.93, average training loss: 5565.25, base loss: 20516.70
[INFO 2017-06-26 13:08:48,993 main.py:50] epoch 6874, training loss: 5519.80, average training loss: 5565.19, base loss: 20516.69
[INFO 2017-06-26 13:08:49,355 main.py:50] epoch 6875, training loss: 5613.88, average training loss: 5565.23, base loss: 20516.65
[INFO 2017-06-26 13:08:49,727 main.py:50] epoch 6876, training loss: 5513.75, average training loss: 5565.14, base loss: 20516.33
[INFO 2017-06-26 13:08:50,091 main.py:50] epoch 6877, training loss: 5600.47, average training loss: 5565.10, base loss: 20516.32
[INFO 2017-06-26 13:08:50,453 main.py:50] epoch 6878, training loss: 5519.52, average training loss: 5565.03, base loss: 20516.30
[INFO 2017-06-26 13:08:50,815 main.py:50] epoch 6879, training loss: 5507.75, average training loss: 5564.86, base loss: 20516.09
[INFO 2017-06-26 13:08:51,176 main.py:50] epoch 6880, training loss: 5544.15, average training loss: 5564.84, base loss: 20515.90
[INFO 2017-06-26 13:08:51,537 main.py:50] epoch 6881, training loss: 5535.85, average training loss: 5564.75, base loss: 20515.59
[INFO 2017-06-26 13:08:51,897 main.py:50] epoch 6882, training loss: 5465.20, average training loss: 5564.65, base loss: 20515.43
[INFO 2017-06-26 13:08:52,258 main.py:50] epoch 6883, training loss: 5538.53, average training loss: 5564.60, base loss: 20515.64
[INFO 2017-06-26 13:08:52,618 main.py:50] epoch 6884, training loss: 5537.10, average training loss: 5564.49, base loss: 20516.27
[INFO 2017-06-26 13:08:52,979 main.py:50] epoch 6885, training loss: 5494.55, average training loss: 5564.42, base loss: 20515.93
[INFO 2017-06-26 13:08:53,339 main.py:50] epoch 6886, training loss: 5536.38, average training loss: 5564.29, base loss: 20515.62
[INFO 2017-06-26 13:08:53,701 main.py:50] epoch 6887, training loss: 5550.23, average training loss: 5564.16, base loss: 20515.53
[INFO 2017-06-26 13:08:54,062 main.py:50] epoch 6888, training loss: 5489.17, average training loss: 5564.01, base loss: 20515.49
[INFO 2017-06-26 13:08:54,423 main.py:50] epoch 6889, training loss: 5540.92, average training loss: 5563.88, base loss: 20515.65
[INFO 2017-06-26 13:08:54,784 main.py:50] epoch 6890, training loss: 5498.78, average training loss: 5563.84, base loss: 20515.81
[INFO 2017-06-26 13:08:55,144 main.py:50] epoch 6891, training loss: 5525.31, average training loss: 5563.74, base loss: 20515.46
[INFO 2017-06-26 13:08:55,505 main.py:50] epoch 6892, training loss: 5545.48, average training loss: 5563.66, base loss: 20515.75
[INFO 2017-06-26 13:08:55,865 main.py:50] epoch 6893, training loss: 5472.27, average training loss: 5563.50, base loss: 20515.62
[INFO 2017-06-26 13:08:56,225 main.py:50] epoch 6894, training loss: 5427.63, average training loss: 5563.33, base loss: 20515.09
[INFO 2017-06-26 13:08:56,586 main.py:50] epoch 6895, training loss: 5540.22, average training loss: 5563.25, base loss: 20514.76
[INFO 2017-06-26 13:08:56,946 main.py:50] epoch 6896, training loss: 5485.37, average training loss: 5563.18, base loss: 20514.92
[INFO 2017-06-26 13:08:57,307 main.py:50] epoch 6897, training loss: 5507.47, average training loss: 5563.08, base loss: 20514.88
[INFO 2017-06-26 13:08:57,668 main.py:50] epoch 6898, training loss: 5414.55, average training loss: 5562.89, base loss: 20514.54
[INFO 2017-06-26 13:08:58,029 main.py:50] epoch 6899, training loss: 5545.76, average training loss: 5562.77, base loss: 20514.49
[INFO 2017-06-26 13:08:58,029 main.py:52] epoch 6899, testing
[INFO 2017-06-26 13:08:59,501 main.py:103] average testing loss: 5514.30, base loss: 20546.60
[INFO 2017-06-26 13:08:59,502 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 13:08:59,508 main.py:76] current best accuracy: 5514.30
[INFO 2017-06-26 13:08:59,870 main.py:50] epoch 6900, training loss: 5514.42, average training loss: 5562.65, base loss: 20514.63
[INFO 2017-06-26 13:09:00,232 main.py:50] epoch 6901, training loss: 5578.32, average training loss: 5562.59, base loss: 20514.76
[INFO 2017-06-26 13:09:00,593 main.py:50] epoch 6902, training loss: 5483.48, average training loss: 5562.51, base loss: 20514.79
[INFO 2017-06-26 13:09:00,955 main.py:50] epoch 6903, training loss: 5543.08, average training loss: 5562.43, base loss: 20514.67
[INFO 2017-06-26 13:09:01,316 main.py:50] epoch 6904, training loss: 5460.55, average training loss: 5562.27, base loss: 20514.37
[INFO 2017-06-26 13:09:01,678 main.py:50] epoch 6905, training loss: 5492.47, average training loss: 5562.12, base loss: 20514.25
[INFO 2017-06-26 13:09:02,038 main.py:50] epoch 6906, training loss: 5490.85, average training loss: 5562.06, base loss: 20514.59
[INFO 2017-06-26 13:09:02,400 main.py:50] epoch 6907, training loss: 5489.01, average training loss: 5561.93, base loss: 20515.11
[INFO 2017-06-26 13:09:02,760 main.py:50] epoch 6908, training loss: 5591.43, average training loss: 5561.93, base loss: 20515.01
[INFO 2017-06-26 13:09:03,137 main.py:50] epoch 6909, training loss: 5532.21, average training loss: 5561.82, base loss: 20515.08
[INFO 2017-06-26 13:09:03,498 main.py:50] epoch 6910, training loss: 5527.64, average training loss: 5561.77, base loss: 20515.04
[INFO 2017-06-26 13:09:03,880 main.py:50] epoch 6911, training loss: 5522.68, average training loss: 5561.71, base loss: 20514.88
[INFO 2017-06-26 13:09:04,243 main.py:50] epoch 6912, training loss: 5538.17, average training loss: 5561.67, base loss: 20515.25
[INFO 2017-06-26 13:09:04,603 main.py:50] epoch 6913, training loss: 5458.96, average training loss: 5561.58, base loss: 20515.41
[INFO 2017-06-26 13:09:04,964 main.py:50] epoch 6914, training loss: 5535.79, average training loss: 5561.52, base loss: 20515.65
[INFO 2017-06-26 13:09:05,324 main.py:50] epoch 6915, training loss: 5524.71, average training loss: 5561.43, base loss: 20515.93
[INFO 2017-06-26 13:09:05,683 main.py:50] epoch 6916, training loss: 5497.67, average training loss: 5561.27, base loss: 20515.97
[INFO 2017-06-26 13:09:06,044 main.py:50] epoch 6917, training loss: 5452.80, average training loss: 5561.17, base loss: 20515.95
[INFO 2017-06-26 13:09:06,404 main.py:50] epoch 6918, training loss: 5450.73, average training loss: 5561.04, base loss: 20516.23
[INFO 2017-06-26 13:09:06,765 main.py:50] epoch 6919, training loss: 5510.24, average training loss: 5560.98, base loss: 20516.21
[INFO 2017-06-26 13:09:07,124 main.py:50] epoch 6920, training loss: 5470.90, average training loss: 5560.80, base loss: 20515.76
[INFO 2017-06-26 13:09:07,485 main.py:50] epoch 6921, training loss: 5519.28, average training loss: 5560.72, base loss: 20515.65
[INFO 2017-06-26 13:09:07,846 main.py:50] epoch 6922, training loss: 5487.90, average training loss: 5560.62, base loss: 20515.54
[INFO 2017-06-26 13:09:08,205 main.py:50] epoch 6923, training loss: 5555.09, average training loss: 5560.59, base loss: 20515.89
[INFO 2017-06-26 13:09:08,567 main.py:50] epoch 6924, training loss: 5472.31, average training loss: 5560.42, base loss: 20515.70
[INFO 2017-06-26 13:09:08,929 main.py:50] epoch 6925, training loss: 5470.27, average training loss: 5560.32, base loss: 20515.61
[INFO 2017-06-26 13:09:09,289 main.py:50] epoch 6926, training loss: 5513.49, average training loss: 5560.31, base loss: 20515.69
[INFO 2017-06-26 13:09:09,650 main.py:50] epoch 6927, training loss: 5463.29, average training loss: 5560.20, base loss: 20515.49
[INFO 2017-06-26 13:09:10,011 main.py:50] epoch 6928, training loss: 5480.96, average training loss: 5560.05, base loss: 20515.46
[INFO 2017-06-26 13:09:10,372 main.py:50] epoch 6929, training loss: 5573.32, average training loss: 5560.06, base loss: 20515.47
[INFO 2017-06-26 13:09:10,732 main.py:50] epoch 6930, training loss: 5621.62, average training loss: 5560.09, base loss: 20514.83
[INFO 2017-06-26 13:09:11,093 main.py:50] epoch 6931, training loss: 5536.93, average training loss: 5560.07, base loss: 20514.68
[INFO 2017-06-26 13:09:11,454 main.py:50] epoch 6932, training loss: 5543.19, average training loss: 5559.96, base loss: 20514.78
[INFO 2017-06-26 13:09:11,815 main.py:50] epoch 6933, training loss: 5544.82, average training loss: 5559.93, base loss: 20515.08
[INFO 2017-06-26 13:09:12,177 main.py:50] epoch 6934, training loss: 5535.55, average training loss: 5559.87, base loss: 20514.94
[INFO 2017-06-26 13:09:12,537 main.py:50] epoch 6935, training loss: 5506.06, average training loss: 5559.77, base loss: 20514.81
[INFO 2017-06-26 13:09:12,898 main.py:50] epoch 6936, training loss: 5548.61, average training loss: 5559.71, base loss: 20514.69
[INFO 2017-06-26 13:09:13,259 main.py:50] epoch 6937, training loss: 5526.46, average training loss: 5559.60, base loss: 20514.85
[INFO 2017-06-26 13:09:13,620 main.py:50] epoch 6938, training loss: 5542.54, average training loss: 5559.57, base loss: 20514.64
[INFO 2017-06-26 13:09:13,981 main.py:50] epoch 6939, training loss: 5587.47, average training loss: 5559.52, base loss: 20514.37
[INFO 2017-06-26 13:09:14,342 main.py:50] epoch 6940, training loss: 5492.46, average training loss: 5559.42, base loss: 20514.82
[INFO 2017-06-26 13:09:14,703 main.py:50] epoch 6941, training loss: 5500.90, average training loss: 5559.23, base loss: 20514.48
[INFO 2017-06-26 13:09:15,063 main.py:50] epoch 6942, training loss: 5560.84, average training loss: 5559.20, base loss: 20514.66
[INFO 2017-06-26 13:09:15,425 main.py:50] epoch 6943, training loss: 5569.14, average training loss: 5559.13, base loss: 20514.76
[INFO 2017-06-26 13:09:15,787 main.py:50] epoch 6944, training loss: 5571.47, average training loss: 5559.11, base loss: 20514.75
[INFO 2017-06-26 13:09:16,147 main.py:50] epoch 6945, training loss: 5519.82, average training loss: 5558.99, base loss: 20514.93
[INFO 2017-06-26 13:09:16,543 main.py:50] epoch 6946, training loss: 5542.60, average training loss: 5558.95, base loss: 20515.27
[INFO 2017-06-26 13:09:16,920 main.py:50] epoch 6947, training loss: 5595.04, average training loss: 5558.97, base loss: 20515.04
[INFO 2017-06-26 13:09:17,282 main.py:50] epoch 6948, training loss: 5527.63, average training loss: 5558.91, base loss: 20515.19
[INFO 2017-06-26 13:09:17,646 main.py:50] epoch 6949, training loss: 5510.02, average training loss: 5558.77, base loss: 20515.00
[INFO 2017-06-26 13:09:18,007 main.py:50] epoch 6950, training loss: 5527.81, average training loss: 5558.69, base loss: 20514.82
[INFO 2017-06-26 13:09:18,367 main.py:50] epoch 6951, training loss: 5558.25, average training loss: 5558.67, base loss: 20514.84
[INFO 2017-06-26 13:09:18,728 main.py:50] epoch 6952, training loss: 5530.81, average training loss: 5558.61, base loss: 20514.57
[INFO 2017-06-26 13:09:19,089 main.py:50] epoch 6953, training loss: 5561.74, average training loss: 5558.57, base loss: 20514.45
[INFO 2017-06-26 13:09:19,450 main.py:50] epoch 6954, training loss: 5542.33, average training loss: 5558.53, base loss: 20514.59
[INFO 2017-06-26 13:09:19,811 main.py:50] epoch 6955, training loss: 5566.80, average training loss: 5558.53, base loss: 20514.59
[INFO 2017-06-26 13:09:20,171 main.py:50] epoch 6956, training loss: 5553.03, average training loss: 5558.57, base loss: 20514.80
[INFO 2017-06-26 13:09:20,532 main.py:50] epoch 6957, training loss: 5540.82, average training loss: 5558.53, base loss: 20514.66
[INFO 2017-06-26 13:09:20,893 main.py:50] epoch 6958, training loss: 5511.17, average training loss: 5558.49, base loss: 20514.63
[INFO 2017-06-26 13:09:21,254 main.py:50] epoch 6959, training loss: 5488.71, average training loss: 5558.48, base loss: 20514.75
[INFO 2017-06-26 13:09:21,615 main.py:50] epoch 6960, training loss: 5527.15, average training loss: 5558.45, base loss: 20514.64
[INFO 2017-06-26 13:09:21,975 main.py:50] epoch 6961, training loss: 5503.32, average training loss: 5558.41, base loss: 20514.39
[INFO 2017-06-26 13:09:22,336 main.py:50] epoch 6962, training loss: 5548.04, average training loss: 5558.40, base loss: 20514.69
[INFO 2017-06-26 13:09:22,698 main.py:50] epoch 6963, training loss: 5522.52, average training loss: 5558.41, base loss: 20514.83
[INFO 2017-06-26 13:09:23,059 main.py:50] epoch 6964, training loss: 5514.07, average training loss: 5558.32, base loss: 20514.81
[INFO 2017-06-26 13:09:23,420 main.py:50] epoch 6965, training loss: 5530.99, average training loss: 5558.33, base loss: 20514.72
[INFO 2017-06-26 13:09:23,781 main.py:50] epoch 6966, training loss: 5514.20, average training loss: 5558.29, base loss: 20514.63
[INFO 2017-06-26 13:09:24,142 main.py:50] epoch 6967, training loss: 5525.09, average training loss: 5558.27, base loss: 20514.47
[INFO 2017-06-26 13:09:24,535 main.py:50] epoch 6968, training loss: 5542.01, average training loss: 5558.21, base loss: 20514.75
[INFO 2017-06-26 13:09:24,914 main.py:50] epoch 6969, training loss: 5596.85, average training loss: 5558.21, base loss: 20515.01
[INFO 2017-06-26 13:09:25,279 main.py:50] epoch 6970, training loss: 5457.41, average training loss: 5557.98, base loss: 20514.33
[INFO 2017-06-26 13:09:25,643 main.py:50] epoch 6971, training loss: 5533.89, average training loss: 5557.96, base loss: 20514.36
[INFO 2017-06-26 13:09:26,004 main.py:50] epoch 6972, training loss: 5552.62, average training loss: 5557.86, base loss: 20514.00
[INFO 2017-06-26 13:09:26,364 main.py:50] epoch 6973, training loss: 5506.13, average training loss: 5557.81, base loss: 20514.07
[INFO 2017-06-26 13:09:26,726 main.py:50] epoch 6974, training loss: 5512.76, average training loss: 5557.72, base loss: 20514.17
[INFO 2017-06-26 13:09:27,086 main.py:50] epoch 6975, training loss: 5480.90, average training loss: 5557.67, base loss: 20513.95
[INFO 2017-06-26 13:09:27,446 main.py:50] epoch 6976, training loss: 5494.52, average training loss: 5557.60, base loss: 20513.89
[INFO 2017-06-26 13:09:27,807 main.py:50] epoch 6977, training loss: 5417.49, average training loss: 5557.43, base loss: 20513.62
[INFO 2017-06-26 13:09:28,169 main.py:50] epoch 6978, training loss: 5520.82, average training loss: 5557.38, base loss: 20513.65
[INFO 2017-06-26 13:09:28,529 main.py:50] epoch 6979, training loss: 5447.46, average training loss: 5557.26, base loss: 20513.23
[INFO 2017-06-26 13:09:28,890 main.py:50] epoch 6980, training loss: 5492.34, average training loss: 5557.21, base loss: 20513.08
[INFO 2017-06-26 13:09:29,251 main.py:50] epoch 6981, training loss: 5494.76, average training loss: 5557.16, base loss: 20512.77
