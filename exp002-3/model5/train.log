[INFO 2017-06-26 13:50:51,487 main.py:170] Namespace(batch_size=32, display=False, image_dir='/home/yi/Downloads/mpii-64-one-2', image_size=64, init_model_path='', input_video_path='video', learning_rate=0.001, method='unsupervised', motion_range=6, num_channel=3, num_inputs=2, output_flow_path='flow', output_flow_video_path='flow_video', save_dir='./model', test=False, test_epoch=10, test_interval=100, test_video=False, train=True, train_epoch=10000)
[INFO 2017-06-26 13:50:55,433 main.py:50] epoch 0, training loss: 147147.05, average training loss: 147147.05, base loss: 10875.00
[INFO 2017-06-26 13:50:58,072 main.py:50] epoch 1, training loss: 123130.48, average training loss: 135138.76, base loss: 11064.56
[INFO 2017-06-26 13:51:00,965 main.py:50] epoch 2, training loss: 105075.74, average training loss: 125117.76, base loss: 10977.59
[INFO 2017-06-26 13:51:03,837 main.py:50] epoch 3, training loss: 91904.65, average training loss: 116814.48, base loss: 10882.21
[INFO 2017-06-26 13:51:06,750 main.py:50] epoch 4, training loss: 79493.88, average training loss: 109350.36, base loss: 10818.08
[INFO 2017-06-26 13:51:09,621 main.py:50] epoch 5, training loss: 69049.91, average training loss: 102633.62, base loss: 10759.22
[INFO 2017-06-26 13:51:12,502 main.py:50] epoch 6, training loss: 59911.92, average training loss: 96530.52, base loss: 10749.43
[INFO 2017-06-26 13:51:15,371 main.py:50] epoch 7, training loss: 52132.14, average training loss: 90980.72, base loss: 10738.84
[INFO 2017-06-26 13:51:18,249 main.py:50] epoch 8, training loss: 45801.11, average training loss: 85960.76, base loss: 10763.26
[INFO 2017-06-26 13:51:21,121 main.py:50] epoch 9, training loss: 40239.46, average training loss: 81388.63, base loss: 10816.96
[INFO 2017-06-26 13:51:23,994 main.py:50] epoch 10, training loss: 35438.87, average training loss: 77211.38, base loss: 10827.31
[INFO 2017-06-26 13:51:26,869 main.py:50] epoch 11, training loss: 32132.11, average training loss: 73454.78, base loss: 10821.95
[INFO 2017-06-26 13:51:29,738 main.py:50] epoch 12, training loss: 29275.82, average training loss: 70056.39, base loss: 10811.90
[INFO 2017-06-26 13:51:32,607 main.py:50] epoch 13, training loss: 26623.17, average training loss: 66954.02, base loss: 10781.16
[INFO 2017-06-26 13:51:35,481 main.py:50] epoch 14, training loss: 24808.49, average training loss: 64144.32, base loss: 10774.97
[INFO 2017-06-26 13:51:38,353 main.py:50] epoch 15, training loss: 23039.29, average training loss: 61575.26, base loss: 10766.39
[INFO 2017-06-26 13:51:41,236 main.py:50] epoch 16, training loss: 21264.52, average training loss: 59204.04, base loss: 10753.48
[INFO 2017-06-26 13:51:44,110 main.py:50] epoch 17, training loss: 20129.64, average training loss: 57033.24, base loss: 10742.79
[INFO 2017-06-26 13:51:46,986 main.py:50] epoch 18, training loss: 18971.11, average training loss: 55029.97, base loss: 10759.71
[INFO 2017-06-26 13:51:49,857 main.py:50] epoch 19, training loss: 17688.20, average training loss: 53162.88, base loss: 10746.38
[INFO 2017-06-26 13:51:52,735 main.py:50] epoch 20, training loss: 17102.00, average training loss: 51445.69, base loss: 10746.14
[INFO 2017-06-26 13:51:55,619 main.py:50] epoch 21, training loss: 17173.71, average training loss: 49887.88, base loss: 10771.75
[INFO 2017-06-26 13:51:58,499 main.py:50] epoch 22, training loss: 15817.62, average training loss: 48406.56, base loss: 10751.79
[INFO 2017-06-26 13:52:01,379 main.py:50] epoch 23, training loss: 15687.86, average training loss: 47043.28, base loss: 10767.27
[INFO 2017-06-26 13:52:04,253 main.py:50] epoch 24, training loss: 14276.52, average training loss: 45732.61, base loss: 10745.22
[INFO 2017-06-26 13:52:07,165 main.py:50] epoch 25, training loss: 14094.42, average training loss: 44515.76, base loss: 10743.47
[INFO 2017-06-26 13:52:10,044 main.py:50] epoch 26, training loss: 14113.95, average training loss: 43389.76, base loss: 10748.84
[INFO 2017-06-26 13:52:12,937 main.py:50] epoch 27, training loss: 13509.51, average training loss: 42322.61, base loss: 10742.84
[INFO 2017-06-26 13:52:15,815 main.py:50] epoch 28, training loss: 13337.84, average training loss: 41323.14, base loss: 10747.55
[INFO 2017-06-26 13:52:18,696 main.py:50] epoch 29, training loss: 12655.85, average training loss: 40367.56, base loss: 10746.05
[INFO 2017-06-26 13:52:21,574 main.py:50] epoch 30, training loss: 12584.04, average training loss: 39471.32, base loss: 10752.25
[INFO 2017-06-26 13:52:24,460 main.py:50] epoch 31, training loss: 12548.20, average training loss: 38629.97, base loss: 10761.53
[INFO 2017-06-26 13:52:27,344 main.py:50] epoch 32, training loss: 11684.01, average training loss: 37813.43, base loss: 10746.26
[INFO 2017-06-26 13:52:30,221 main.py:50] epoch 33, training loss: 12009.09, average training loss: 37054.48, base loss: 10757.96
[INFO 2017-06-26 13:52:33,098 main.py:50] epoch 34, training loss: 11280.83, average training loss: 36318.09, base loss: 10753.27
[INFO 2017-06-26 13:52:35,983 main.py:50] epoch 35, training loss: 11100.63, average training loss: 35617.60, base loss: 10746.31
[INFO 2017-06-26 13:52:38,904 main.py:50] epoch 36, training loss: 11338.57, average training loss: 34961.41, base loss: 10753.12
[INFO 2017-06-26 13:52:41,786 main.py:50] epoch 37, training loss: 11063.90, average training loss: 34332.53, base loss: 10754.03
[INFO 2017-06-26 13:52:44,682 main.py:50] epoch 38, training loss: 11150.52, average training loss: 33738.12, base loss: 10761.97
[INFO 2017-06-26 13:52:47,570 main.py:50] epoch 39, training loss: 10875.05, average training loss: 33166.54, base loss: 10766.68
[INFO 2017-06-26 13:52:50,491 main.py:50] epoch 40, training loss: 10440.35, average training loss: 32612.24, base loss: 10761.29
[INFO 2017-06-26 13:52:53,371 main.py:50] epoch 41, training loss: 10175.45, average training loss: 32078.03, base loss: 10751.63
[INFO 2017-06-26 13:52:56,265 main.py:50] epoch 42, training loss: 10479.35, average training loss: 31575.74, base loss: 10752.82
[INFO 2017-06-26 13:52:59,157 main.py:50] epoch 43, training loss: 10350.39, average training loss: 31093.35, base loss: 10752.12
[INFO 2017-06-26 13:53:02,055 main.py:50] epoch 44, training loss: 10467.96, average training loss: 30635.00, base loss: 10755.52
[INFO 2017-06-26 13:53:04,952 main.py:50] epoch 45, training loss: 10016.58, average training loss: 30186.78, base loss: 10749.11
[INFO 2017-06-26 13:53:07,863 main.py:50] epoch 46, training loss: 9876.64, average training loss: 29754.65, base loss: 10740.57
[INFO 2017-06-26 13:53:10,751 main.py:50] epoch 47, training loss: 10520.30, average training loss: 29353.93, base loss: 10748.91
[INFO 2017-06-26 13:53:13,642 main.py:50] epoch 48, training loss: 10154.82, average training loss: 28962.11, base loss: 10748.42
[INFO 2017-06-26 13:53:16,514 main.py:50] epoch 49, training loss: 10457.18, average training loss: 28592.01, base loss: 10756.51
[INFO 2017-06-26 13:53:19,416 main.py:50] epoch 50, training loss: 10472.78, average training loss: 28236.73, base loss: 10765.64
[INFO 2017-06-26 13:53:22,309 main.py:50] epoch 51, training loss: 10351.15, average training loss: 27892.78, base loss: 10772.67
[INFO 2017-06-26 13:53:25,199 main.py:50] epoch 52, training loss: 10075.17, average training loss: 27556.60, base loss: 10773.40
[INFO 2017-06-26 13:53:28,123 main.py:50] epoch 53, training loss: 9475.66, average training loss: 27221.77, base loss: 10761.99
[INFO 2017-06-26 13:53:31,039 main.py:50] epoch 54, training loss: 9838.95, average training loss: 26905.72, base loss: 10759.36
[INFO 2017-06-26 13:53:33,922 main.py:50] epoch 55, training loss: 10351.81, average training loss: 26610.11, base loss: 10768.76
[INFO 2017-06-26 13:53:36,822 main.py:50] epoch 56, training loss: 9763.98, average training loss: 26314.56, base loss: 10765.44
[INFO 2017-06-26 13:53:39,706 main.py:50] epoch 57, training loss: 9944.64, average training loss: 26032.32, base loss: 10766.59
[INFO 2017-06-26 13:53:42,598 main.py:50] epoch 58, training loss: 9937.16, average training loss: 25759.52, base loss: 10768.37
[INFO 2017-06-26 13:53:45,498 main.py:50] epoch 59, training loss: 9844.41, average training loss: 25494.27, base loss: 10768.20
[INFO 2017-06-26 13:53:48,385 main.py:50] epoch 60, training loss: 9577.23, average training loss: 25233.34, base loss: 10763.45
[INFO 2017-06-26 13:53:51,280 main.py:50] epoch 61, training loss: 9810.60, average training loss: 24984.58, base loss: 10764.81
[INFO 2017-06-26 13:53:54,172 main.py:50] epoch 62, training loss: 9648.82, average training loss: 24741.16, base loss: 10762.86
[INFO 2017-06-26 13:53:57,064 main.py:50] epoch 63, training loss: 9361.40, average training loss: 24500.85, base loss: 10755.65
[INFO 2017-06-26 13:53:59,956 main.py:50] epoch 64, training loss: 9528.91, average training loss: 24270.51, base loss: 10752.48
[INFO 2017-06-26 13:54:02,846 main.py:50] epoch 65, training loss: 9342.58, average training loss: 24044.33, base loss: 10745.76
[INFO 2017-06-26 13:54:05,744 main.py:50] epoch 66, training loss: 9699.30, average training loss: 23830.23, base loss: 10746.70
[INFO 2017-06-26 13:54:08,629 main.py:50] epoch 67, training loss: 9380.82, average training loss: 23617.74, base loss: 10742.32
[INFO 2017-06-26 13:54:11,524 main.py:50] epoch 68, training loss: 9685.28, average training loss: 23415.82, base loss: 10743.54
[INFO 2017-06-26 13:54:14,405 main.py:50] epoch 69, training loss: 9432.30, average training loss: 23216.05, base loss: 10741.07
[INFO 2017-06-26 13:54:17,294 main.py:50] epoch 70, training loss: 9407.54, average training loss: 23021.57, base loss: 10738.09
[INFO 2017-06-26 13:54:20,181 main.py:50] epoch 71, training loss: 9487.76, average training loss: 22833.60, base loss: 10736.35
[INFO 2017-06-26 13:54:23,088 main.py:50] epoch 72, training loss: 9726.70, average training loss: 22654.05, base loss: 10740.07
[INFO 2017-06-26 13:54:25,980 main.py:50] epoch 73, training loss: 9607.38, average training loss: 22477.74, base loss: 10740.59
[INFO 2017-06-26 13:54:28,876 main.py:50] epoch 74, training loss: 9304.55, average training loss: 22302.10, base loss: 10737.36
[INFO 2017-06-26 13:54:31,776 main.py:50] epoch 75, training loss: 9653.55, average training loss: 22135.67, base loss: 10740.53
[INFO 2017-06-26 13:54:34,652 main.py:50] epoch 76, training loss: 8926.29, average training loss: 21964.12, base loss: 10732.77
[INFO 2017-06-26 13:54:37,549 main.py:50] epoch 77, training loss: 8912.09, average training loss: 21796.79, base loss: 10724.87
[INFO 2017-06-26 13:54:40,444 main.py:50] epoch 78, training loss: 9499.54, average training loss: 21641.13, base loss: 10725.91
[INFO 2017-06-26 13:54:43,344 main.py:50] epoch 79, training loss: 9622.94, average training loss: 21490.90, base loss: 10729.20
[INFO 2017-06-26 13:54:46,255 main.py:50] epoch 80, training loss: 9278.85, average training loss: 21340.13, base loss: 10727.64
[INFO 2017-06-26 13:54:49,178 main.py:50] epoch 81, training loss: 9436.92, average training loss: 21194.97, base loss: 10728.55
[INFO 2017-06-26 13:54:52,073 main.py:50] epoch 82, training loss: 8784.62, average training loss: 21045.45, base loss: 10719.44
[INFO 2017-06-26 13:54:54,981 main.py:50] epoch 83, training loss: 9143.33, average training loss: 20903.76, base loss: 10716.85
[INFO 2017-06-26 13:54:57,868 main.py:50] epoch 84, training loss: 9144.33, average training loss: 20765.41, base loss: 10714.95
[INFO 2017-06-26 13:55:00,742 main.py:50] epoch 85, training loss: 8949.18, average training loss: 20628.01, base loss: 10710.41
[INFO 2017-06-26 13:55:03,670 main.py:50] epoch 86, training loss: 9344.24, average training loss: 20498.32, base loss: 10711.70
[INFO 2017-06-26 13:55:06,571 main.py:50] epoch 87, training loss: 9624.26, average training loss: 20374.75, base loss: 10718.02
[INFO 2017-06-26 13:55:09,498 main.py:50] epoch 88, training loss: 9222.53, average training loss: 20249.44, base loss: 10717.67
[INFO 2017-06-26 13:55:12,395 main.py:50] epoch 89, training loss: 9193.63, average training loss: 20126.60, base loss: 10718.26
[INFO 2017-06-26 13:55:15,291 main.py:50] epoch 90, training loss: 9248.25, average training loss: 20007.06, base loss: 10719.16
[INFO 2017-06-26 13:55:18,189 main.py:50] epoch 91, training loss: 9182.59, average training loss: 19889.40, base loss: 10719.89
[INFO 2017-06-26 13:55:21,079 main.py:50] epoch 92, training loss: 9241.98, average training loss: 19774.91, base loss: 10722.17
[INFO 2017-06-26 13:55:23,962 main.py:50] epoch 93, training loss: 8754.16, average training loss: 19657.67, base loss: 10717.57
[INFO 2017-06-26 13:55:26,845 main.py:50] epoch 94, training loss: 8695.07, average training loss: 19542.27, base loss: 10712.34
[INFO 2017-06-26 13:55:29,740 main.py:50] epoch 95, training loss: 9028.54, average training loss: 19432.75, base loss: 10711.59
[INFO 2017-06-26 13:55:32,624 main.py:50] epoch 96, training loss: 8921.26, average training loss: 19324.39, base loss: 10709.70
[INFO 2017-06-26 13:55:35,506 main.py:50] epoch 97, training loss: 9071.08, average training loss: 19219.76, base loss: 10710.72
[INFO 2017-06-26 13:55:38,393 main.py:50] epoch 98, training loss: 9054.46, average training loss: 19117.08, base loss: 10710.28
[INFO 2017-06-26 13:55:41,291 main.py:50] epoch 99, training loss: 8965.45, average training loss: 19015.57, base loss: 10709.88
[INFO 2017-06-26 13:55:41,291 main.py:52] epoch 99, testing
[INFO 2017-06-26 13:55:54,577 main.py:103] average testing loss: 9051.34, base loss: 10791.58
[INFO 2017-06-26 13:55:54,578 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 13:55:54,584 main.py:76] current best accuracy: 9051.34
[INFO 2017-06-26 13:55:57,463 main.py:50] epoch 100, training loss: 9064.85, average training loss: 18917.04, base loss: 10710.84
[INFO 2017-06-26 13:56:00,349 main.py:50] epoch 101, training loss: 9134.28, average training loss: 18821.14, base loss: 10713.71
[INFO 2017-06-26 13:56:03,239 main.py:50] epoch 102, training loss: 8576.34, average training loss: 18721.67, base loss: 10709.12
[INFO 2017-06-26 13:56:06,116 main.py:50] epoch 103, training loss: 9166.34, average training loss: 18629.79, base loss: 10713.11
[INFO 2017-06-26 13:56:08,997 main.py:50] epoch 104, training loss: 9071.48, average training loss: 18538.76, base loss: 10714.28
[INFO 2017-06-26 13:56:11,903 main.py:50] epoch 105, training loss: 8653.45, average training loss: 18445.50, base loss: 10711.26
[INFO 2017-06-26 13:56:14,787 main.py:50] epoch 106, training loss: 8901.48, average training loss: 18356.31, base loss: 10710.93
[INFO 2017-06-26 13:56:17,675 main.py:50] epoch 107, training loss: 8866.83, average training loss: 18268.44, base loss: 10711.44
[INFO 2017-06-26 13:56:20,592 main.py:50] epoch 108, training loss: 8872.66, average training loss: 18182.24, base loss: 10711.54
[INFO 2017-06-26 13:56:23,477 main.py:50] epoch 109, training loss: 8661.98, average training loss: 18095.69, base loss: 10709.81
[INFO 2017-06-26 13:56:26,388 main.py:50] epoch 110, training loss: 8925.89, average training loss: 18013.08, base loss: 10711.33
[INFO 2017-06-26 13:56:29,304 main.py:50] epoch 111, training loss: 9049.12, average training loss: 17933.05, base loss: 10714.67
[INFO 2017-06-26 13:56:32,186 main.py:50] epoch 112, training loss: 8399.11, average training loss: 17848.68, base loss: 10709.59
[INFO 2017-06-26 13:56:35,070 main.py:50] epoch 113, training loss: 8538.62, average training loss: 17767.01, base loss: 10708.00
[INFO 2017-06-26 13:56:37,941 main.py:50] epoch 114, training loss: 8510.71, average training loss: 17686.52, base loss: 10704.00
[INFO 2017-06-26 13:56:40,820 main.py:50] epoch 115, training loss: 8814.55, average training loss: 17610.04, base loss: 10704.85
[INFO 2017-06-26 13:56:43,728 main.py:50] epoch 116, training loss: 8385.87, average training loss: 17531.20, base loss: 10701.78
[INFO 2017-06-26 13:56:46,622 main.py:50] epoch 117, training loss: 8540.29, average training loss: 17455.00, base loss: 10699.44
[INFO 2017-06-26 13:56:49,530 main.py:50] epoch 118, training loss: 8391.55, average training loss: 17378.84, base loss: 10694.38
[INFO 2017-06-26 13:56:52,437 main.py:50] epoch 119, training loss: 8872.85, average training loss: 17307.96, base loss: 10696.36
[INFO 2017-06-26 13:56:55,321 main.py:50] epoch 120, training loss: 8769.33, average training loss: 17237.39, base loss: 10698.01
[INFO 2017-06-26 13:56:58,208 main.py:50] epoch 121, training loss: 8582.26, average training loss: 17166.45, base loss: 10696.04
[INFO 2017-06-26 13:57:01,120 main.py:50] epoch 122, training loss: 8596.44, average training loss: 17096.77, base loss: 10697.25
[INFO 2017-06-26 13:57:03,999 main.py:50] epoch 123, training loss: 8708.68, average training loss: 17029.13, base loss: 10698.40
[INFO 2017-06-26 13:57:06,885 main.py:50] epoch 124, training loss: 8398.70, average training loss: 16960.08, base loss: 10695.55
[INFO 2017-06-26 13:57:09,781 main.py:50] epoch 125, training loss: 8410.51, average training loss: 16892.23, base loss: 10694.91
[INFO 2017-06-26 13:57:12,660 main.py:50] epoch 126, training loss: 8575.71, average training loss: 16826.74, base loss: 10693.79
[INFO 2017-06-26 13:57:15,535 main.py:50] epoch 127, training loss: 8399.05, average training loss: 16760.90, base loss: 10692.63
[INFO 2017-06-26 13:57:18,432 main.py:50] epoch 128, training loss: 8900.03, average training loss: 16699.97, base loss: 10697.12
[INFO 2017-06-26 13:57:21,330 main.py:50] epoch 129, training loss: 8685.34, average training loss: 16638.32, base loss: 10698.01
[INFO 2017-06-26 13:57:24,238 main.py:50] epoch 130, training loss: 8492.82, average training loss: 16576.14, base loss: 10698.09
[INFO 2017-06-26 13:57:27,144 main.py:50] epoch 131, training loss: 8227.23, average training loss: 16512.89, base loss: 10694.69
[INFO 2017-06-26 13:57:30,039 main.py:50] epoch 132, training loss: 8216.79, average training loss: 16450.51, base loss: 10692.06
[INFO 2017-06-26 13:57:32,924 main.py:50] epoch 133, training loss: 8349.02, average training loss: 16390.05, base loss: 10691.60
[INFO 2017-06-26 13:57:35,823 main.py:50] epoch 134, training loss: 8204.40, average training loss: 16329.42, base loss: 10689.33
[INFO 2017-06-26 13:57:38,715 main.py:50] epoch 135, training loss: 8573.33, average training loss: 16272.39, base loss: 10690.92
[INFO 2017-06-26 13:57:41,607 main.py:50] epoch 136, training loss: 8205.98, average training loss: 16213.51, base loss: 10689.72
[INFO 2017-06-26 13:57:44,502 main.py:50] epoch 137, training loss: 8337.23, average training loss: 16156.43, base loss: 10688.14
[INFO 2017-06-26 13:57:47,432 main.py:50] epoch 138, training loss: 8131.12, average training loss: 16098.70, base loss: 10686.02
[INFO 2017-06-26 13:57:50,319 main.py:50] epoch 139, training loss: 8413.59, average training loss: 16043.80, base loss: 10686.59
[INFO 2017-06-26 13:57:53,235 main.py:50] epoch 140, training loss: 8375.25, average training loss: 15989.42, base loss: 10686.13
[INFO 2017-06-26 13:57:56,130 main.py:50] epoch 141, training loss: 8478.25, average training loss: 15936.52, base loss: 10687.81
[INFO 2017-06-26 13:57:59,021 main.py:50] epoch 142, training loss: 8595.15, average training loss: 15885.18, base loss: 10690.42
[INFO 2017-06-26 13:58:01,950 main.py:50] epoch 143, training loss: 8311.28, average training loss: 15832.59, base loss: 10688.79
[INFO 2017-06-26 13:58:04,855 main.py:50] epoch 144, training loss: 7979.99, average training loss: 15778.43, base loss: 10684.54
[INFO 2017-06-26 13:58:07,756 main.py:50] epoch 145, training loss: 8460.50, average training loss: 15728.31, base loss: 10684.99
[INFO 2017-06-26 13:58:10,654 main.py:50] epoch 146, training loss: 8259.87, average training loss: 15677.50, base loss: 10685.97
[INFO 2017-06-26 13:58:13,544 main.py:50] epoch 147, training loss: 8233.75, average training loss: 15627.21, base loss: 10685.59
[INFO 2017-06-26 13:58:16,432 main.py:50] epoch 148, training loss: 8495.21, average training loss: 15579.34, base loss: 10688.46
[INFO 2017-06-26 13:58:19,340 main.py:50] epoch 149, training loss: 8244.16, average training loss: 15530.44, base loss: 10687.74
[INFO 2017-06-26 13:58:22,260 main.py:50] epoch 150, training loss: 8396.55, average training loss: 15483.20, base loss: 10688.43
[INFO 2017-06-26 13:58:25,171 main.py:50] epoch 151, training loss: 8037.65, average training loss: 15434.21, base loss: 10687.56
[INFO 2017-06-26 13:58:28,066 main.py:50] epoch 152, training loss: 7945.48, average training loss: 15385.27, base loss: 10684.29
[INFO 2017-06-26 13:58:30,966 main.py:50] epoch 153, training loss: 8090.49, average training loss: 15337.90, base loss: 10683.85
[INFO 2017-06-26 13:58:33,878 main.py:50] epoch 154, training loss: 8323.90, average training loss: 15292.64, base loss: 10684.15
[INFO 2017-06-26 13:58:36,757 main.py:50] epoch 155, training loss: 8197.87, average training loss: 15247.17, base loss: 10684.09
[INFO 2017-06-26 13:58:39,665 main.py:50] epoch 156, training loss: 8303.33, average training loss: 15202.94, base loss: 10684.46
[INFO 2017-06-26 13:58:42,541 main.py:50] epoch 157, training loss: 8314.25, average training loss: 15159.34, base loss: 10686.16
[INFO 2017-06-26 13:58:45,426 main.py:50] epoch 158, training loss: 8274.45, average training loss: 15116.04, base loss: 10687.94
[INFO 2017-06-26 13:58:48,340 main.py:50] epoch 159, training loss: 8164.83, average training loss: 15072.59, base loss: 10688.83
[INFO 2017-06-26 13:58:51,244 main.py:50] epoch 160, training loss: 7675.08, average training loss: 15026.64, base loss: 10685.08
[INFO 2017-06-26 13:58:54,135 main.py:50] epoch 161, training loss: 8467.24, average training loss: 14986.15, base loss: 10687.30
[INFO 2017-06-26 13:58:57,021 main.py:50] epoch 162, training loss: 8210.16, average training loss: 14944.58, base loss: 10687.81
[INFO 2017-06-26 13:58:59,899 main.py:50] epoch 163, training loss: 8438.37, average training loss: 14904.91, base loss: 10690.41
[INFO 2017-06-26 13:59:02,787 main.py:50] epoch 164, training loss: 8390.00, average training loss: 14865.43, base loss: 10692.02
[INFO 2017-06-26 13:59:05,668 main.py:50] epoch 165, training loss: 7920.99, average training loss: 14823.59, base loss: 10689.75
[INFO 2017-06-26 13:59:08,547 main.py:50] epoch 166, training loss: 8209.52, average training loss: 14783.99, base loss: 10690.41
[INFO 2017-06-26 13:59:11,465 main.py:50] epoch 167, training loss: 7889.77, average training loss: 14742.95, base loss: 10689.37
[INFO 2017-06-26 13:59:14,338 main.py:50] epoch 168, training loss: 8112.91, average training loss: 14703.72, base loss: 10690.94
[INFO 2017-06-26 13:59:17,215 main.py:50] epoch 169, training loss: 8009.68, average training loss: 14664.34, base loss: 10690.00
[INFO 2017-06-26 13:59:20,104 main.py:50] epoch 170, training loss: 8100.35, average training loss: 14625.96, base loss: 10690.30
[INFO 2017-06-26 13:59:22,978 main.py:50] epoch 171, training loss: 7978.58, average training loss: 14587.31, base loss: 10689.33
[INFO 2017-06-26 13:59:25,852 main.py:50] epoch 172, training loss: 7924.42, average training loss: 14548.80, base loss: 10689.53
[INFO 2017-06-26 13:59:28,765 main.py:50] epoch 173, training loss: 8107.53, average training loss: 14511.78, base loss: 10691.17
[INFO 2017-06-26 13:59:31,650 main.py:50] epoch 174, training loss: 8074.01, average training loss: 14474.99, base loss: 10692.06
[INFO 2017-06-26 13:59:34,566 main.py:50] epoch 175, training loss: 7920.01, average training loss: 14437.75, base loss: 10691.59
[INFO 2017-06-26 13:59:37,448 main.py:50] epoch 176, training loss: 8384.76, average training loss: 14403.55, base loss: 10695.81
[INFO 2017-06-26 13:59:40,326 main.py:50] epoch 177, training loss: 7813.45, average training loss: 14366.53, base loss: 10694.16
[INFO 2017-06-26 13:59:43,208 main.py:50] epoch 178, training loss: 8009.27, average training loss: 14331.01, base loss: 10695.08
[INFO 2017-06-26 13:59:46,097 main.py:50] epoch 179, training loss: 7798.62, average training loss: 14294.72, base loss: 10693.08
[INFO 2017-06-26 13:59:48,977 main.py:50] epoch 180, training loss: 7896.07, average training loss: 14259.37, base loss: 10693.39
[INFO 2017-06-26 13:59:51,854 main.py:50] epoch 181, training loss: 7953.78, average training loss: 14224.72, base loss: 10693.83
[INFO 2017-06-26 13:59:54,741 main.py:50] epoch 182, training loss: 8027.41, average training loss: 14190.86, base loss: 10694.81
[INFO 2017-06-26 13:59:57,626 main.py:50] epoch 183, training loss: 7910.28, average training loss: 14156.72, base loss: 10695.35
[INFO 2017-06-26 14:00:00,544 main.py:50] epoch 184, training loss: 7641.61, average training loss: 14121.51, base loss: 10692.64
[INFO 2017-06-26 14:00:03,464 main.py:50] epoch 185, training loss: 7831.59, average training loss: 14087.69, base loss: 10692.46
[INFO 2017-06-26 14:00:06,349 main.py:50] epoch 186, training loss: 8113.70, average training loss: 14055.74, base loss: 10694.09
[INFO 2017-06-26 14:00:09,227 main.py:50] epoch 187, training loss: 8116.19, average training loss: 14024.15, base loss: 10697.14
[INFO 2017-06-26 14:00:12,145 main.py:50] epoch 188, training loss: 8004.77, average training loss: 13992.30, base loss: 10698.44
[INFO 2017-06-26 14:00:15,022 main.py:50] epoch 189, training loss: 7937.44, average training loss: 13960.43, base loss: 10699.52
[INFO 2017-06-26 14:00:17,897 main.py:50] epoch 190, training loss: 7683.09, average training loss: 13927.57, base loss: 10698.14
[INFO 2017-06-26 14:00:20,824 main.py:50] epoch 191, training loss: 7721.93, average training loss: 13895.25, base loss: 10697.43
[INFO 2017-06-26 14:00:23,712 main.py:50] epoch 192, training loss: 7659.80, average training loss: 13862.94, base loss: 10697.51
[INFO 2017-06-26 14:00:26,585 main.py:50] epoch 193, training loss: 7576.51, average training loss: 13830.53, base loss: 10697.00
[INFO 2017-06-26 14:00:29,475 main.py:50] epoch 194, training loss: 7869.42, average training loss: 13799.96, base loss: 10697.05
[INFO 2017-06-26 14:00:32,355 main.py:50] epoch 195, training loss: 7663.05, average training loss: 13768.65, base loss: 10696.66
[INFO 2017-06-26 14:00:35,275 main.py:50] epoch 196, training loss: 7472.36, average training loss: 13736.69, base loss: 10693.83
[INFO 2017-06-26 14:00:38,189 main.py:50] epoch 197, training loss: 7786.79, average training loss: 13706.64, base loss: 10694.31
[INFO 2017-06-26 14:00:41,089 main.py:50] epoch 198, training loss: 7576.90, average training loss: 13675.84, base loss: 10694.07
[INFO 2017-06-26 14:00:43,989 main.py:50] epoch 199, training loss: 7820.17, average training loss: 13646.56, base loss: 10694.09
[INFO 2017-06-26 14:00:43,989 main.py:52] epoch 199, testing
[INFO 2017-06-26 14:00:57,365 main.py:103] average testing loss: 7707.95, base loss: 10773.48
[INFO 2017-06-26 14:00:57,366 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 14:00:57,373 main.py:76] current best accuracy: 7707.95
[INFO 2017-06-26 14:01:00,248 main.py:50] epoch 200, training loss: 7732.24, average training loss: 13617.14, base loss: 10695.11
[INFO 2017-06-26 14:01:03,125 main.py:50] epoch 201, training loss: 7745.04, average training loss: 13588.07, base loss: 10695.14
[INFO 2017-06-26 14:01:06,019 main.py:50] epoch 202, training loss: 7545.54, average training loss: 13558.30, base loss: 10693.53
[INFO 2017-06-26 14:01:08,896 main.py:50] epoch 203, training loss: 7718.82, average training loss: 13529.68, base loss: 10694.78
[INFO 2017-06-26 14:01:11,790 main.py:50] epoch 204, training loss: 7917.33, average training loss: 13502.30, base loss: 10696.20
[INFO 2017-06-26 14:01:14,701 main.py:50] epoch 205, training loss: 7764.01, average training loss: 13474.44, base loss: 10697.31
[INFO 2017-06-26 14:01:17,586 main.py:50] epoch 206, training loss: 7382.94, average training loss: 13445.02, base loss: 10695.23
[INFO 2017-06-26 14:01:20,492 main.py:50] epoch 207, training loss: 7664.74, average training loss: 13417.23, base loss: 10695.50
[INFO 2017-06-26 14:01:23,391 main.py:50] epoch 208, training loss: 7763.77, average training loss: 13390.18, base loss: 10697.53
[INFO 2017-06-26 14:01:26,278 main.py:50] epoch 209, training loss: 7468.37, average training loss: 13361.98, base loss: 10695.42
[INFO 2017-06-26 14:01:29,166 main.py:50] epoch 210, training loss: 7438.59, average training loss: 13333.90, base loss: 10693.81
[INFO 2017-06-26 14:01:32,041 main.py:50] epoch 211, training loss: 8047.71, average training loss: 13308.97, base loss: 10696.50
[INFO 2017-06-26 14:01:34,952 main.py:50] epoch 212, training loss: 7546.32, average training loss: 13281.91, base loss: 10695.61
[INFO 2017-06-26 14:01:37,834 main.py:50] epoch 213, training loss: 7455.01, average training loss: 13254.69, base loss: 10693.97
[INFO 2017-06-26 14:01:40,729 main.py:50] epoch 214, training loss: 7418.80, average training loss: 13227.54, base loss: 10692.77
[INFO 2017-06-26 14:01:43,604 main.py:50] epoch 215, training loss: 7382.23, average training loss: 13200.48, base loss: 10691.40
[INFO 2017-06-26 14:01:46,481 main.py:50] epoch 216, training loss: 7315.73, average training loss: 13173.36, base loss: 10689.27
[INFO 2017-06-26 14:01:49,352 main.py:50] epoch 217, training loss: 7693.76, average training loss: 13148.23, base loss: 10689.78
[INFO 2017-06-26 14:01:52,360 main.py:50] epoch 218, training loss: 7229.62, average training loss: 13121.20, base loss: 10687.66
[INFO 2017-06-26 14:01:55,324 main.py:50] epoch 219, training loss: 7714.33, average training loss: 13096.62, base loss: 10688.48
[INFO 2017-06-26 14:01:58,216 main.py:50] epoch 220, training loss: 7656.51, average training loss: 13072.01, base loss: 10689.13
[INFO 2017-06-26 14:02:01,193 main.py:50] epoch 221, training loss: 7804.93, average training loss: 13048.28, base loss: 10691.84
[INFO 2017-06-26 14:02:04,291 main.py:50] epoch 222, training loss: 7498.65, average training loss: 13023.40, base loss: 10691.36
[INFO 2017-06-26 14:02:07,335 main.py:50] epoch 223, training loss: 7154.74, average training loss: 12997.20, base loss: 10688.51
[INFO 2017-06-26 14:02:10,527 main.py:50] epoch 224, training loss: 7325.20, average training loss: 12971.99, base loss: 10685.94
[INFO 2017-06-26 14:02:13,438 main.py:50] epoch 225, training loss: 7341.05, average training loss: 12947.07, base loss: 10683.67
[INFO 2017-06-26 14:02:16,452 main.py:50] epoch 226, training loss: 7517.90, average training loss: 12923.15, base loss: 10684.22
[INFO 2017-06-26 14:02:19,366 main.py:50] epoch 227, training loss: 7613.36, average training loss: 12899.87, base loss: 10685.75
[INFO 2017-06-26 14:02:22,544 main.py:50] epoch 228, training loss: 7610.44, average training loss: 12876.77, base loss: 10685.79
[INFO 2017-06-26 14:02:25,577 main.py:50] epoch 229, training loss: 7615.11, average training loss: 12853.89, base loss: 10687.51
[INFO 2017-06-26 14:02:28,574 main.py:50] epoch 230, training loss: 7251.89, average training loss: 12829.64, base loss: 10685.84
[INFO 2017-06-26 14:02:31,597 main.py:50] epoch 231, training loss: 7324.72, average training loss: 12805.91, base loss: 10685.00
[INFO 2017-06-26 14:02:34,545 main.py:50] epoch 232, training loss: 7163.21, average training loss: 12781.69, base loss: 10683.44
[INFO 2017-06-26 14:02:37,480 main.py:50] epoch 233, training loss: 7282.35, average training loss: 12758.19, base loss: 10681.99
[INFO 2017-06-26 14:02:40,461 main.py:50] epoch 234, training loss: 7326.72, average training loss: 12735.08, base loss: 10681.92
[INFO 2017-06-26 14:02:43,392 main.py:50] epoch 235, training loss: 7275.84, average training loss: 12711.95, base loss: 10681.93
[INFO 2017-06-26 14:02:46,281 main.py:50] epoch 236, training loss: 7664.24, average training loss: 12690.65, base loss: 10684.09
[INFO 2017-06-26 14:02:49,160 main.py:50] epoch 237, training loss: 7398.35, average training loss: 12668.41, base loss: 10684.57
[INFO 2017-06-26 14:02:52,031 main.py:50] epoch 238, training loss: 7297.46, average training loss: 12645.94, base loss: 10684.34
[INFO 2017-06-26 14:02:55,198 main.py:50] epoch 239, training loss: 7477.64, average training loss: 12624.41, base loss: 10686.18
[INFO 2017-06-26 14:02:58,107 main.py:50] epoch 240, training loss: 7793.72, average training loss: 12604.36, base loss: 10689.81
[INFO 2017-06-26 14:03:00,983 main.py:50] epoch 241, training loss: 7246.54, average training loss: 12582.22, base loss: 10688.99
[INFO 2017-06-26 14:03:03,874 main.py:50] epoch 242, training loss: 7462.40, average training loss: 12561.15, base loss: 10690.49
[INFO 2017-06-26 14:03:06,744 main.py:50] epoch 243, training loss: 7015.47, average training loss: 12538.42, base loss: 10687.99
[INFO 2017-06-26 14:03:09,673 main.py:50] epoch 244, training loss: 7306.68, average training loss: 12517.07, base loss: 10688.28
[INFO 2017-06-26 14:03:12,560 main.py:50] epoch 245, training loss: 7174.93, average training loss: 12495.35, base loss: 10687.83
[INFO 2017-06-26 14:03:15,455 main.py:50] epoch 246, training loss: 7320.11, average training loss: 12474.40, base loss: 10687.56
[INFO 2017-06-26 14:03:18,346 main.py:50] epoch 247, training loss: 7165.58, average training loss: 12453.00, base loss: 10686.44
[INFO 2017-06-26 14:03:21,288 main.py:50] epoch 248, training loss: 7308.26, average training loss: 12432.33, base loss: 10686.71
[INFO 2017-06-26 14:03:24,165 main.py:50] epoch 249, training loss: 7479.63, average training loss: 12412.52, base loss: 10689.08
[INFO 2017-06-26 14:03:27,038 main.py:50] epoch 250, training loss: 7446.18, average training loss: 12392.74, base loss: 10691.75
[INFO 2017-06-26 14:03:29,923 main.py:50] epoch 251, training loss: 7050.43, average training loss: 12371.54, base loss: 10690.75
[INFO 2017-06-26 14:03:32,800 main.py:50] epoch 252, training loss: 7194.29, average training loss: 12351.07, base loss: 10690.50
[INFO 2017-06-26 14:03:35,690 main.py:50] epoch 253, training loss: 7300.77, average training loss: 12331.19, base loss: 10691.52
[INFO 2017-06-26 14:03:38,599 main.py:50] epoch 254, training loss: 7436.25, average training loss: 12311.99, base loss: 10694.00
[INFO 2017-06-26 14:03:41,499 main.py:50] epoch 255, training loss: 7321.13, average training loss: 12292.50, base loss: 10695.40
[INFO 2017-06-26 14:03:44,417 main.py:50] epoch 256, training loss: 7029.67, average training loss: 12272.02, base loss: 10693.18
[INFO 2017-06-26 14:03:47,505 main.py:50] epoch 257, training loss: 7227.27, average training loss: 12252.47, base loss: 10693.71
[INFO 2017-06-26 14:03:50,396 main.py:50] epoch 258, training loss: 7265.52, average training loss: 12233.21, base loss: 10694.04
[INFO 2017-06-26 14:03:53,293 main.py:50] epoch 259, training loss: 7096.25, average training loss: 12213.46, base loss: 10693.96
[INFO 2017-06-26 14:03:56,187 main.py:50] epoch 260, training loss: 7170.06, average training loss: 12194.13, base loss: 10694.34
[INFO 2017-06-26 14:03:59,161 main.py:50] epoch 261, training loss: 6977.87, average training loss: 12174.22, base loss: 10693.47
[INFO 2017-06-26 14:04:02,046 main.py:50] epoch 262, training loss: 6985.16, average training loss: 12154.49, base loss: 10692.71
[INFO 2017-06-26 14:04:04,950 main.py:50] epoch 263, training loss: 6922.25, average training loss: 12134.67, base loss: 10690.98
[INFO 2017-06-26 14:04:07,846 main.py:50] epoch 264, training loss: 7205.57, average training loss: 12116.07, base loss: 10692.01
[INFO 2017-06-26 14:04:10,786 main.py:50] epoch 265, training loss: 7194.31, average training loss: 12097.57, base loss: 10692.72
[INFO 2017-06-26 14:04:13,665 main.py:50] epoch 266, training loss: 7155.98, average training loss: 12079.06, base loss: 10693.39
[INFO 2017-06-26 14:04:16,542 main.py:50] epoch 267, training loss: 7017.77, average training loss: 12060.18, base loss: 10693.57
[INFO 2017-06-26 14:04:19,437 main.py:50] epoch 268, training loss: 6996.59, average training loss: 12041.35, base loss: 10694.04
[INFO 2017-06-26 14:04:22,353 main.py:50] epoch 269, training loss: 6952.66, average training loss: 12022.51, base loss: 10693.74
[INFO 2017-06-26 14:04:25,225 main.py:50] epoch 270, training loss: 6995.56, average training loss: 12003.96, base loss: 10692.64
[INFO 2017-06-26 14:04:28,142 main.py:50] epoch 271, training loss: 6998.51, average training loss: 11985.55, base loss: 10692.51
[INFO 2017-06-26 14:04:31,026 main.py:50] epoch 272, training loss: 6848.15, average training loss: 11966.74, base loss: 10691.00
[INFO 2017-06-26 14:04:33,919 main.py:50] epoch 273, training loss: 6906.94, average training loss: 11948.27, base loss: 10690.46
[INFO 2017-06-26 14:04:36,790 main.py:50] epoch 274, training loss: 7019.79, average training loss: 11930.35, base loss: 10689.98
[INFO 2017-06-26 14:04:39,677 main.py:50] epoch 275, training loss: 6881.75, average training loss: 11912.06, base loss: 10689.86
[INFO 2017-06-26 14:04:42,561 main.py:50] epoch 276, training loss: 7145.06, average training loss: 11894.85, base loss: 10691.84
[INFO 2017-06-26 14:04:45,455 main.py:50] epoch 277, training loss: 7090.25, average training loss: 11877.56, base loss: 10693.11
[INFO 2017-06-26 14:04:48,347 main.py:50] epoch 278, training loss: 6823.95, average training loss: 11859.45, base loss: 10691.65
[INFO 2017-06-26 14:04:51,234 main.py:50] epoch 279, training loss: 7233.45, average training loss: 11842.93, base loss: 10693.45
[INFO 2017-06-26 14:04:54,132 main.py:50] epoch 280, training loss: 7017.94, average training loss: 11825.76, base loss: 10694.14
[INFO 2017-06-26 14:04:57,029 main.py:50] epoch 281, training loss: 7215.28, average training loss: 11809.41, base loss: 10696.75
[INFO 2017-06-26 14:04:59,937 main.py:50] epoch 282, training loss: 6800.24, average training loss: 11791.71, base loss: 10695.24
[INFO 2017-06-26 14:05:02,851 main.py:50] epoch 283, training loss: 7123.90, average training loss: 11775.27, base loss: 10696.20
[INFO 2017-06-26 14:05:05,742 main.py:50] epoch 284, training loss: 7015.94, average training loss: 11758.57, base loss: 10696.49
[INFO 2017-06-26 14:05:08,630 main.py:50] epoch 285, training loss: 6744.53, average training loss: 11741.04, base loss: 10694.32
[INFO 2017-06-26 14:05:11,528 main.py:50] epoch 286, training loss: 6968.94, average training loss: 11724.41, base loss: 10695.14
[INFO 2017-06-26 14:05:14,427 main.py:50] epoch 287, training loss: 6943.34, average training loss: 11707.81, base loss: 10696.15
[INFO 2017-06-26 14:05:17,301 main.py:50] epoch 288, training loss: 6803.73, average training loss: 11690.84, base loss: 10695.59
[INFO 2017-06-26 14:05:20,189 main.py:50] epoch 289, training loss: 6952.87, average training loss: 11674.51, base loss: 10696.74
[INFO 2017-06-26 14:05:23,072 main.py:50] epoch 290, training loss: 6787.06, average training loss: 11657.71, base loss: 10696.12
[INFO 2017-06-26 14:05:25,958 main.py:50] epoch 291, training loss: 6977.72, average training loss: 11641.68, base loss: 10696.82
[INFO 2017-06-26 14:05:28,843 main.py:50] epoch 292, training loss: 6645.14, average training loss: 11624.63, base loss: 10695.49
[INFO 2017-06-26 14:05:31,753 main.py:50] epoch 293, training loss: 6785.00, average training loss: 11608.17, base loss: 10694.79
[INFO 2017-06-26 14:05:34,633 main.py:50] epoch 294, training loss: 6764.61, average training loss: 11591.75, base loss: 10694.73
[INFO 2017-06-26 14:05:37,534 main.py:50] epoch 295, training loss: 6702.76, average training loss: 11575.23, base loss: 10693.74
[INFO 2017-06-26 14:05:40,432 main.py:50] epoch 296, training loss: 6838.11, average training loss: 11559.28, base loss: 10693.47
[INFO 2017-06-26 14:05:43,319 main.py:50] epoch 297, training loss: 6709.75, average training loss: 11543.01, base loss: 10691.98
[INFO 2017-06-26 14:05:46,201 main.py:50] epoch 298, training loss: 7035.70, average training loss: 11527.94, base loss: 10692.81
[INFO 2017-06-26 14:05:49,099 main.py:50] epoch 299, training loss: 6877.19, average training loss: 11512.43, base loss: 10693.42
[INFO 2017-06-26 14:05:49,100 main.py:52] epoch 299, testing
[INFO 2017-06-26 14:06:02,398 main.py:103] average testing loss: 6817.03, base loss: 10641.10
[INFO 2017-06-26 14:06:02,398 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 14:06:02,405 main.py:76] current best accuracy: 6817.03
[INFO 2017-06-26 14:06:05,285 main.py:50] epoch 300, training loss: 6784.31, average training loss: 11496.73, base loss: 10692.75
[INFO 2017-06-26 14:06:08,185 main.py:50] epoch 301, training loss: 6589.51, average training loss: 11480.48, base loss: 10691.60
[INFO 2017-06-26 14:06:11,200 main.py:50] epoch 302, training loss: 6678.81, average training loss: 11464.63, base loss: 10690.19
[INFO 2017-06-26 14:06:14,152 main.py:50] epoch 303, training loss: 6898.13, average training loss: 11449.61, base loss: 10691.38
[INFO 2017-06-26 14:06:17,021 main.py:50] epoch 304, training loss: 6918.22, average training loss: 11434.75, base loss: 10690.96
[INFO 2017-06-26 14:06:19,924 main.py:50] epoch 305, training loss: 6696.55, average training loss: 11419.27, base loss: 10690.37
[INFO 2017-06-26 14:06:22,801 main.py:50] epoch 306, training loss: 6744.67, average training loss: 11404.04, base loss: 10690.01
[INFO 2017-06-26 14:06:25,679 main.py:50] epoch 307, training loss: 6738.20, average training loss: 11388.89, base loss: 10690.07
[INFO 2017-06-26 14:06:28,642 main.py:50] epoch 308, training loss: 6875.77, average training loss: 11374.29, base loss: 10691.49
[INFO 2017-06-26 14:06:31,663 main.py:50] epoch 309, training loss: 6690.80, average training loss: 11359.18, base loss: 10690.78
[INFO 2017-06-26 14:06:34,756 main.py:50] epoch 310, training loss: 6853.38, average training loss: 11344.69, base loss: 10690.88
[INFO 2017-06-26 14:06:37,682 main.py:50] epoch 311, training loss: 7013.05, average training loss: 11330.81, base loss: 10691.84
[INFO 2017-06-26 14:06:40,555 main.py:50] epoch 312, training loss: 6590.05, average training loss: 11315.66, base loss: 10690.30
[INFO 2017-06-26 14:06:43,451 main.py:50] epoch 313, training loss: 6618.15, average training loss: 11300.70, base loss: 10689.83
[INFO 2017-06-26 14:06:46,335 main.py:50] epoch 314, training loss: 6768.02, average training loss: 11286.31, base loss: 10689.56
[INFO 2017-06-26 14:06:49,380 main.py:50] epoch 315, training loss: 6730.69, average training loss: 11271.89, base loss: 10689.57
[INFO 2017-06-26 14:06:52,269 main.py:50] epoch 316, training loss: 6526.00, average training loss: 11256.92, base loss: 10687.61
[INFO 2017-06-26 14:06:55,143 main.py:50] epoch 317, training loss: 6719.87, average training loss: 11242.65, base loss: 10687.14
[INFO 2017-06-26 14:06:58,014 main.py:50] epoch 318, training loss: 6671.44, average training loss: 11228.32, base loss: 10687.77
[INFO 2017-06-26 14:07:00,888 main.py:50] epoch 319, training loss: 6815.00, average training loss: 11214.53, base loss: 10688.31
[INFO 2017-06-26 14:07:03,768 main.py:50] epoch 320, training loss: 6715.02, average training loss: 11200.52, base loss: 10688.69
[INFO 2017-06-26 14:07:06,647 main.py:50] epoch 321, training loss: 6709.90, average training loss: 11186.57, base loss: 10688.54
[INFO 2017-06-26 14:07:09,517 main.py:50] epoch 322, training loss: 6698.53, average training loss: 11172.67, base loss: 10688.94
[INFO 2017-06-26 14:07:12,392 main.py:50] epoch 323, training loss: 6587.93, average training loss: 11158.52, base loss: 10687.84
[INFO 2017-06-26 14:07:15,267 main.py:50] epoch 324, training loss: 6533.54, average training loss: 11144.29, base loss: 10687.05
[INFO 2017-06-26 14:07:18,133 main.py:50] epoch 325, training loss: 6676.64, average training loss: 11130.59, base loss: 10687.04
[INFO 2017-06-26 14:07:21,020 main.py:50] epoch 326, training loss: 6541.11, average training loss: 11116.55, base loss: 10686.32
[INFO 2017-06-26 14:07:23,898 main.py:50] epoch 327, training loss: 6547.11, average training loss: 11102.62, base loss: 10685.45
[INFO 2017-06-26 14:07:26,784 main.py:50] epoch 328, training loss: 6581.07, average training loss: 11088.88, base loss: 10684.96
[INFO 2017-06-26 14:07:29,666 main.py:50] epoch 329, training loss: 6507.30, average training loss: 11075.00, base loss: 10684.00
[INFO 2017-06-26 14:07:32,540 main.py:50] epoch 330, training loss: 6454.50, average training loss: 11061.04, base loss: 10682.67
[INFO 2017-06-26 14:07:35,413 main.py:50] epoch 331, training loss: 6706.67, average training loss: 11047.92, base loss: 10682.42
[INFO 2017-06-26 14:07:38,313 main.py:50] epoch 332, training loss: 6554.30, average training loss: 11034.43, base loss: 10682.16
[INFO 2017-06-26 14:07:41,230 main.py:50] epoch 333, training loss: 6558.94, average training loss: 11021.03, base loss: 10681.58
[INFO 2017-06-26 14:07:44,098 main.py:50] epoch 334, training loss: 6627.54, average training loss: 11007.91, base loss: 10681.35
[INFO 2017-06-26 14:07:46,996 main.py:50] epoch 335, training loss: 6642.86, average training loss: 10994.92, base loss: 10681.99
[INFO 2017-06-26 14:07:49,898 main.py:50] epoch 336, training loss: 6731.66, average training loss: 10982.27, base loss: 10682.62
[INFO 2017-06-26 14:07:52,799 main.py:50] epoch 337, training loss: 6616.10, average training loss: 10969.35, base loss: 10682.34
[INFO 2017-06-26 14:07:55,689 main.py:50] epoch 338, training loss: 6790.07, average training loss: 10957.02, base loss: 10683.90
[INFO 2017-06-26 14:07:58,607 main.py:50] epoch 339, training loss: 6692.28, average training loss: 10944.48, base loss: 10685.15
[INFO 2017-06-26 14:08:01,480 main.py:50] epoch 340, training loss: 6627.33, average training loss: 10931.82, base loss: 10685.70
[INFO 2017-06-26 14:08:04,370 main.py:50] epoch 341, training loss: 6322.99, average training loss: 10918.34, base loss: 10684.16
[INFO 2017-06-26 14:08:07,252 main.py:50] epoch 342, training loss: 6568.24, average training loss: 10905.66, base loss: 10683.79
[INFO 2017-06-26 14:08:10,144 main.py:50] epoch 343, training loss: 6587.17, average training loss: 10893.11, base loss: 10683.82
[INFO 2017-06-26 14:08:13,026 main.py:50] epoch 344, training loss: 6581.61, average training loss: 10880.61, base loss: 10684.50
[INFO 2017-06-26 14:08:15,905 main.py:50] epoch 345, training loss: 6565.47, average training loss: 10868.14, base loss: 10685.06
[INFO 2017-06-26 14:08:18,783 main.py:50] epoch 346, training loss: 6417.05, average training loss: 10855.31, base loss: 10684.21
[INFO 2017-06-26 14:08:21,671 main.py:50] epoch 347, training loss: 6646.12, average training loss: 10843.22, base loss: 10684.65
[INFO 2017-06-26 14:08:24,552 main.py:50] epoch 348, training loss: 6678.13, average training loss: 10831.28, base loss: 10685.71
[INFO 2017-06-26 14:08:27,431 main.py:50] epoch 349, training loss: 6684.56, average training loss: 10819.44, base loss: 10686.57
[INFO 2017-06-26 14:08:30,316 main.py:50] epoch 350, training loss: 6564.11, average training loss: 10807.31, base loss: 10686.76
[INFO 2017-06-26 14:08:33,234 main.py:50] epoch 351, training loss: 6348.51, average training loss: 10794.64, base loss: 10685.70
[INFO 2017-06-26 14:08:36,148 main.py:50] epoch 352, training loss: 6686.42, average training loss: 10783.01, base loss: 10686.21
[INFO 2017-06-26 14:08:39,044 main.py:50] epoch 353, training loss: 6248.33, average training loss: 10770.20, base loss: 10684.26
[INFO 2017-06-26 14:08:41,930 main.py:50] epoch 354, training loss: 6439.38, average training loss: 10758.00, base loss: 10682.95
[INFO 2017-06-26 14:08:44,812 main.py:50] epoch 355, training loss: 6632.22, average training loss: 10746.41, base loss: 10683.61
[INFO 2017-06-26 14:08:47,713 main.py:50] epoch 356, training loss: 6515.11, average training loss: 10734.56, base loss: 10683.19
[INFO 2017-06-26 14:08:50,590 main.py:50] epoch 357, training loss: 6375.11, average training loss: 10722.38, base loss: 10682.80
[INFO 2017-06-26 14:08:53,486 main.py:50] epoch 358, training loss: 6712.72, average training loss: 10711.21, base loss: 10683.99
[INFO 2017-06-26 14:08:56,364 main.py:50] epoch 359, training loss: 6548.55, average training loss: 10699.65, base loss: 10683.99
[INFO 2017-06-26 14:08:59,245 main.py:50] epoch 360, training loss: 6507.63, average training loss: 10688.03, base loss: 10684.65
[INFO 2017-06-26 14:09:02,128 main.py:50] epoch 361, training loss: 6640.21, average training loss: 10676.85, base loss: 10685.72
[INFO 2017-06-26 14:09:05,023 main.py:50] epoch 362, training loss: 6470.63, average training loss: 10665.27, base loss: 10685.85
[INFO 2017-06-26 14:09:07,907 main.py:50] epoch 363, training loss: 6631.08, average training loss: 10654.18, base loss: 10687.39
[INFO 2017-06-26 14:09:10,803 main.py:50] epoch 364, training loss: 6421.70, average training loss: 10642.59, base loss: 10687.16
[INFO 2017-06-26 14:09:13,695 main.py:50] epoch 365, training loss: 6159.28, average training loss: 10630.34, base loss: 10685.26
[INFO 2017-06-26 14:09:16,621 main.py:50] epoch 366, training loss: 6202.59, average training loss: 10618.27, base loss: 10683.62
[INFO 2017-06-26 14:09:19,518 main.py:50] epoch 367, training loss: 6440.83, average training loss: 10606.92, base loss: 10683.56
[INFO 2017-06-26 14:09:22,409 main.py:50] epoch 368, training loss: 6630.64, average training loss: 10596.14, base loss: 10684.53
[INFO 2017-06-26 14:09:25,293 main.py:50] epoch 369, training loss: 6637.77, average training loss: 10585.45, base loss: 10685.12
[INFO 2017-06-26 14:09:28,218 main.py:50] epoch 370, training loss: 6446.35, average training loss: 10574.29, base loss: 10685.09
[INFO 2017-06-26 14:09:31,115 main.py:50] epoch 371, training loss: 6591.40, average training loss: 10563.58, base loss: 10685.49
[INFO 2017-06-26 14:09:33,989 main.py:50] epoch 372, training loss: 6366.00, average training loss: 10552.33, base loss: 10685.08
[INFO 2017-06-26 14:09:36,880 main.py:50] epoch 373, training loss: 6704.49, average training loss: 10542.04, base loss: 10686.85
[INFO 2017-06-26 14:09:39,771 main.py:50] epoch 374, training loss: 6542.62, average training loss: 10531.38, base loss: 10686.33
[INFO 2017-06-26 14:09:42,658 main.py:50] epoch 375, training loss: 6500.98, average training loss: 10520.66, base loss: 10686.77
[INFO 2017-06-26 14:09:45,569 main.py:50] epoch 376, training loss: 6508.75, average training loss: 10510.02, base loss: 10687.23
[INFO 2017-06-26 14:09:48,473 main.py:50] epoch 377, training loss: 6245.04, average training loss: 10498.73, base loss: 10686.53
[INFO 2017-06-26 14:09:51,356 main.py:50] epoch 378, training loss: 6601.13, average training loss: 10488.45, base loss: 10687.82
[INFO 2017-06-26 14:09:54,248 main.py:50] epoch 379, training loss: 6374.04, average training loss: 10477.62, base loss: 10687.22
[INFO 2017-06-26 14:09:57,136 main.py:50] epoch 380, training loss: 6729.84, average training loss: 10467.78, base loss: 10689.79
[INFO 2017-06-26 14:10:00,013 main.py:50] epoch 381, training loss: 6417.03, average training loss: 10457.18, base loss: 10690.23
[INFO 2017-06-26 14:10:02,908 main.py:50] epoch 382, training loss: 6218.48, average training loss: 10446.11, base loss: 10689.85
[INFO 2017-06-26 14:10:05,803 main.py:50] epoch 383, training loss: 6469.29, average training loss: 10435.76, base loss: 10690.08
[INFO 2017-06-26 14:10:08,683 main.py:50] epoch 384, training loss: 6260.65, average training loss: 10424.91, base loss: 10689.73
[INFO 2017-06-26 14:10:11,579 main.py:50] epoch 385, training loss: 6415.54, average training loss: 10414.53, base loss: 10690.00
[INFO 2017-06-26 14:10:14,485 main.py:50] epoch 386, training loss: 6352.13, average training loss: 10404.03, base loss: 10689.59
[INFO 2017-06-26 14:10:17,364 main.py:50] epoch 387, training loss: 6561.36, average training loss: 10394.12, base loss: 10690.74
[INFO 2017-06-26 14:10:20,276 main.py:50] epoch 388, training loss: 6400.38, average training loss: 10383.86, base loss: 10691.14
[INFO 2017-06-26 14:10:23,156 main.py:50] epoch 389, training loss: 6396.60, average training loss: 10373.63, base loss: 10691.59
[INFO 2017-06-26 14:10:26,037 main.py:50] epoch 390, training loss: 6262.93, average training loss: 10363.12, base loss: 10691.07
[INFO 2017-06-26 14:10:28,927 main.py:50] epoch 391, training loss: 6396.61, average training loss: 10353.00, base loss: 10691.53
[INFO 2017-06-26 14:10:31,842 main.py:50] epoch 392, training loss: 6070.36, average training loss: 10342.10, base loss: 10690.05
[INFO 2017-06-26 14:10:34,723 main.py:50] epoch 393, training loss: 6347.67, average training loss: 10331.97, base loss: 10690.40
[INFO 2017-06-26 14:10:37,595 main.py:50] epoch 394, training loss: 6223.03, average training loss: 10321.56, base loss: 10690.30
[INFO 2017-06-26 14:10:40,476 main.py:50] epoch 395, training loss: 6314.62, average training loss: 10311.45, base loss: 10691.15
[INFO 2017-06-26 14:10:43,351 main.py:50] epoch 396, training loss: 6327.38, average training loss: 10301.41, base loss: 10691.18
[INFO 2017-06-26 14:10:46,223 main.py:50] epoch 397, training loss: 6402.35, average training loss: 10291.61, base loss: 10691.78
[INFO 2017-06-26 14:10:49,102 main.py:50] epoch 398, training loss: 6464.29, average training loss: 10282.02, base loss: 10693.63
[INFO 2017-06-26 14:10:51,976 main.py:50] epoch 399, training loss: 6322.39, average training loss: 10272.12, base loss: 10693.49
[INFO 2017-06-26 14:10:51,976 main.py:52] epoch 399, testing
[INFO 2017-06-26 14:11:05,220 main.py:103] average testing loss: 6203.54, base loss: 10492.70
[INFO 2017-06-26 14:11:05,221 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 14:11:05,228 main.py:76] current best accuracy: 6203.54
[INFO 2017-06-26 14:11:08,101 main.py:50] epoch 400, training loss: 6141.41, average training loss: 10261.82, base loss: 10693.26
[INFO 2017-06-26 14:11:10,997 main.py:50] epoch 401, training loss: 6408.44, average training loss: 10252.24, base loss: 10693.71
[INFO 2017-06-26 14:11:13,911 main.py:50] epoch 402, training loss: 6088.14, average training loss: 10241.90, base loss: 10692.07
[INFO 2017-06-26 14:11:16,814 main.py:50] epoch 403, training loss: 6467.36, average training loss: 10232.56, base loss: 10692.90
[INFO 2017-06-26 14:11:19,698 main.py:50] epoch 404, training loss: 6349.91, average training loss: 10222.97, base loss: 10693.82
[INFO 2017-06-26 14:11:22,573 main.py:50] epoch 405, training loss: 6195.40, average training loss: 10213.05, base loss: 10693.58
[INFO 2017-06-26 14:11:25,463 main.py:50] epoch 406, training loss: 6445.74, average training loss: 10203.80, base loss: 10694.52
[INFO 2017-06-26 14:11:28,347 main.py:50] epoch 407, training loss: 6081.99, average training loss: 10193.69, base loss: 10693.89
[INFO 2017-06-26 14:11:31,259 main.py:50] epoch 408, training loss: 6212.36, average training loss: 10183.96, base loss: 10693.67
[INFO 2017-06-26 14:11:34,131 main.py:50] epoch 409, training loss: 6288.82, average training loss: 10174.46, base loss: 10694.26
[INFO 2017-06-26 14:11:37,022 main.py:50] epoch 410, training loss: 6264.00, average training loss: 10164.95, base loss: 10694.88
[INFO 2017-06-26 14:11:39,905 main.py:50] epoch 411, training loss: 6257.96, average training loss: 10155.46, base loss: 10695.19
[INFO 2017-06-26 14:11:42,774 main.py:50] epoch 412, training loss: 6332.01, average training loss: 10146.20, base loss: 10696.25
[INFO 2017-06-26 14:11:45,651 main.py:50] epoch 413, training loss: 6120.33, average training loss: 10136.48, base loss: 10694.99
[INFO 2017-06-26 14:11:48,532 main.py:50] epoch 414, training loss: 6213.30, average training loss: 10127.03, base loss: 10695.26
[INFO 2017-06-26 14:11:51,402 main.py:50] epoch 415, training loss: 6283.78, average training loss: 10117.79, base loss: 10696.30
[INFO 2017-06-26 14:11:54,286 main.py:50] epoch 416, training loss: 6279.74, average training loss: 10108.58, base loss: 10696.32
[INFO 2017-06-26 14:11:57,165 main.py:50] epoch 417, training loss: 5990.48, average training loss: 10098.73, base loss: 10695.30
[INFO 2017-06-26 14:12:00,073 main.py:50] epoch 418, training loss: 6211.89, average training loss: 10089.46, base loss: 10694.89
[INFO 2017-06-26 14:12:02,971 main.py:50] epoch 419, training loss: 6218.63, average training loss: 10080.24, base loss: 10695.16
[INFO 2017-06-26 14:12:05,847 main.py:50] epoch 420, training loss: 6213.67, average training loss: 10071.06, base loss: 10695.30
[INFO 2017-06-26 14:12:08,735 main.py:50] epoch 421, training loss: 6291.62, average training loss: 10062.10, base loss: 10695.47
[INFO 2017-06-26 14:12:11,617 main.py:50] epoch 422, training loss: 6488.35, average training loss: 10053.65, base loss: 10697.02
[INFO 2017-06-26 14:12:14,544 main.py:50] epoch 423, training loss: 6267.99, average training loss: 10044.72, base loss: 10697.44
[INFO 2017-06-26 14:12:17,420 main.py:50] epoch 424, training loss: 6069.64, average training loss: 10035.37, base loss: 10696.90
[INFO 2017-06-26 14:12:20,311 main.py:50] epoch 425, training loss: 6085.39, average training loss: 10026.10, base loss: 10696.03
[INFO 2017-06-26 14:12:23,200 main.py:50] epoch 426, training loss: 5719.58, average training loss: 10016.01, base loss: 10693.89
[INFO 2017-06-26 14:12:26,115 main.py:50] epoch 427, training loss: 6415.72, average training loss: 10007.60, base loss: 10695.04
[INFO 2017-06-26 14:12:28,999 main.py:50] epoch 428, training loss: 6392.17, average training loss: 9999.17, base loss: 10695.59
[INFO 2017-06-26 14:12:31,894 main.py:50] epoch 429, training loss: 6257.43, average training loss: 9990.47, base loss: 10695.58
[INFO 2017-06-26 14:12:34,792 main.py:50] epoch 430, training loss: 6081.61, average training loss: 9981.40, base loss: 10695.14
[INFO 2017-06-26 14:12:37,715 main.py:50] epoch 431, training loss: 6192.92, average training loss: 9972.63, base loss: 10694.75
[INFO 2017-06-26 14:12:40,598 main.py:50] epoch 432, training loss: 6013.73, average training loss: 9963.49, base loss: 10693.28
[INFO 2017-06-26 14:12:43,489 main.py:50] epoch 433, training loss: 6178.27, average training loss: 9954.77, base loss: 10692.92
[INFO 2017-06-26 14:12:46,401 main.py:50] epoch 434, training loss: 6102.38, average training loss: 9945.91, base loss: 10692.76
[INFO 2017-06-26 14:12:49,310 main.py:50] epoch 435, training loss: 6108.41, average training loss: 9937.11, base loss: 10691.96
[INFO 2017-06-26 14:12:52,197 main.py:50] epoch 436, training loss: 6103.03, average training loss: 9928.34, base loss: 10691.75
[INFO 2017-06-26 14:12:55,112 main.py:50] epoch 437, training loss: 6181.20, average training loss: 9919.78, base loss: 10691.64
[INFO 2017-06-26 14:12:58,027 main.py:50] epoch 438, training loss: 6186.34, average training loss: 9911.28, base loss: 10691.73
[INFO 2017-06-26 14:13:00,903 main.py:50] epoch 439, training loss: 6267.28, average training loss: 9902.99, base loss: 10692.29
[INFO 2017-06-26 14:13:03,814 main.py:50] epoch 440, training loss: 6017.11, average training loss: 9894.18, base loss: 10691.32
[INFO 2017-06-26 14:13:06,701 main.py:50] epoch 441, training loss: 6147.13, average training loss: 9885.70, base loss: 10691.58
[INFO 2017-06-26 14:13:09,596 main.py:50] epoch 442, training loss: 6257.13, average training loss: 9877.51, base loss: 10692.07
[INFO 2017-06-26 14:13:12,509 main.py:50] epoch 443, training loss: 6272.43, average training loss: 9869.39, base loss: 10693.07
[INFO 2017-06-26 14:13:15,397 main.py:50] epoch 444, training loss: 6069.19, average training loss: 9860.85, base loss: 10692.79
[INFO 2017-06-26 14:13:18,284 main.py:50] epoch 445, training loss: 6320.33, average training loss: 9852.92, base loss: 10694.02
[INFO 2017-06-26 14:13:21,163 main.py:50] epoch 446, training loss: 6006.62, average training loss: 9844.31, base loss: 10692.79
[INFO 2017-06-26 14:13:24,043 main.py:50] epoch 447, training loss: 6159.97, average training loss: 9836.09, base loss: 10692.98
[INFO 2017-06-26 14:13:26,928 main.py:50] epoch 448, training loss: 6180.42, average training loss: 9827.95, base loss: 10693.53
[INFO 2017-06-26 14:13:29,828 main.py:50] epoch 449, training loss: 6026.71, average training loss: 9819.50, base loss: 10693.23
[INFO 2017-06-26 14:13:32,709 main.py:50] epoch 450, training loss: 6056.76, average training loss: 9811.16, base loss: 10692.70
[INFO 2017-06-26 14:13:35,637 main.py:50] epoch 451, training loss: 6004.81, average training loss: 9802.73, base loss: 10692.12
[INFO 2017-06-26 14:13:38,530 main.py:50] epoch 452, training loss: 5927.45, average training loss: 9794.18, base loss: 10691.13
[INFO 2017-06-26 14:13:41,419 main.py:50] epoch 453, training loss: 5929.91, average training loss: 9785.67, base loss: 10690.53
[INFO 2017-06-26 14:13:44,310 main.py:50] epoch 454, training loss: 5827.23, average training loss: 9776.97, base loss: 10689.56
[INFO 2017-06-26 14:13:47,203 main.py:50] epoch 455, training loss: 6221.71, average training loss: 9769.17, base loss: 10690.40
[INFO 2017-06-26 14:13:50,070 main.py:50] epoch 456, training loss: 6135.63, average training loss: 9761.22, base loss: 10690.77
[INFO 2017-06-26 14:13:52,953 main.py:50] epoch 457, training loss: 6044.43, average training loss: 9753.11, base loss: 10690.78
[INFO 2017-06-26 14:13:55,845 main.py:50] epoch 458, training loss: 6249.53, average training loss: 9745.47, base loss: 10691.66
[INFO 2017-06-26 14:13:58,716 main.py:50] epoch 459, training loss: 6064.17, average training loss: 9737.47, base loss: 10691.80
[INFO 2017-06-26 14:14:01,637 main.py:50] epoch 460, training loss: 6187.56, average training loss: 9729.77, base loss: 10692.37
[INFO 2017-06-26 14:14:04,518 main.py:50] epoch 461, training loss: 5908.06, average training loss: 9721.50, base loss: 10691.72
[INFO 2017-06-26 14:14:07,401 main.py:50] epoch 462, training loss: 6235.81, average training loss: 9713.97, base loss: 10692.49
[INFO 2017-06-26 14:14:10,288 main.py:50] epoch 463, training loss: 5907.18, average training loss: 9705.76, base loss: 10692.37
[INFO 2017-06-26 14:14:13,171 main.py:50] epoch 464, training loss: 6085.94, average training loss: 9697.98, base loss: 10692.50
[INFO 2017-06-26 14:14:16,054 main.py:50] epoch 465, training loss: 5890.20, average training loss: 9689.81, base loss: 10691.88
[INFO 2017-06-26 14:14:18,936 main.py:50] epoch 466, training loss: 6153.95, average training loss: 9682.24, base loss: 10692.75
[INFO 2017-06-26 14:14:21,820 main.py:50] epoch 467, training loss: 5971.05, average training loss: 9674.31, base loss: 10692.31
[INFO 2017-06-26 14:14:24,701 main.py:50] epoch 468, training loss: 5987.22, average training loss: 9666.45, base loss: 10693.17
[INFO 2017-06-26 14:14:27,584 main.py:50] epoch 469, training loss: 5819.97, average training loss: 9658.26, base loss: 10691.83
[INFO 2017-06-26 14:14:30,467 main.py:50] epoch 470, training loss: 6090.95, average training loss: 9650.69, base loss: 10691.90
[INFO 2017-06-26 14:14:33,387 main.py:50] epoch 471, training loss: 6081.00, average training loss: 9643.12, base loss: 10692.13
[INFO 2017-06-26 14:14:36,296 main.py:50] epoch 472, training loss: 6022.43, average training loss: 9635.47, base loss: 10692.90
[INFO 2017-06-26 14:14:39,217 main.py:50] epoch 473, training loss: 6010.17, average training loss: 9627.82, base loss: 10693.01
[INFO 2017-06-26 14:14:42,093 main.py:50] epoch 474, training loss: 6000.38, average training loss: 9620.18, base loss: 10693.10
[INFO 2017-06-26 14:14:44,967 main.py:50] epoch 475, training loss: 6128.20, average training loss: 9612.85, base loss: 10693.62
[INFO 2017-06-26 14:14:47,857 main.py:50] epoch 476, training loss: 5902.64, average training loss: 9605.07, base loss: 10692.62
[INFO 2017-06-26 14:14:50,771 main.py:50] epoch 477, training loss: 6099.69, average training loss: 9597.74, base loss: 10693.54
[INFO 2017-06-26 14:14:53,647 main.py:50] epoch 478, training loss: 5789.10, average training loss: 9589.79, base loss: 10691.42
[INFO 2017-06-26 14:14:56,541 main.py:50] epoch 479, training loss: 5888.97, average training loss: 9582.08, base loss: 10690.62
[INFO 2017-06-26 14:14:59,421 main.py:50] epoch 480, training loss: 5991.94, average training loss: 9574.61, base loss: 10690.28
[INFO 2017-06-26 14:15:02,292 main.py:50] epoch 481, training loss: 6056.88, average training loss: 9567.31, base loss: 10690.62
[INFO 2017-06-26 14:15:05,170 main.py:50] epoch 482, training loss: 5928.81, average training loss: 9559.78, base loss: 10690.51
[INFO 2017-06-26 14:15:08,091 main.py:50] epoch 483, training loss: 5963.61, average training loss: 9552.35, base loss: 10690.78
[INFO 2017-06-26 14:15:10,971 main.py:50] epoch 484, training loss: 6012.05, average training loss: 9545.05, base loss: 10691.28
[INFO 2017-06-26 14:15:13,899 main.py:50] epoch 485, training loss: 5975.97, average training loss: 9537.71, base loss: 10691.36
[INFO 2017-06-26 14:15:16,822 main.py:50] epoch 486, training loss: 5925.70, average training loss: 9530.29, base loss: 10691.61
[INFO 2017-06-26 14:15:19,704 main.py:50] epoch 487, training loss: 5897.00, average training loss: 9522.85, base loss: 10691.55
[INFO 2017-06-26 14:15:22,591 main.py:50] epoch 488, training loss: 5831.16, average training loss: 9515.30, base loss: 10690.97
[INFO 2017-06-26 14:15:25,464 main.py:50] epoch 489, training loss: 5757.04, average training loss: 9507.63, base loss: 10689.87
[INFO 2017-06-26 14:15:28,332 main.py:50] epoch 490, training loss: 5899.80, average training loss: 9500.28, base loss: 10689.36
[INFO 2017-06-26 14:15:31,218 main.py:50] epoch 491, training loss: 5928.22, average training loss: 9493.02, base loss: 10689.54
[INFO 2017-06-26 14:15:34,107 main.py:50] epoch 492, training loss: 6032.67, average training loss: 9486.00, base loss: 10690.43
[INFO 2017-06-26 14:15:37,019 main.py:50] epoch 493, training loss: 6087.95, average training loss: 9479.12, base loss: 10691.15
[INFO 2017-06-26 14:15:39,896 main.py:50] epoch 494, training loss: 5913.36, average training loss: 9471.92, base loss: 10690.86
[INFO 2017-06-26 14:15:42,780 main.py:50] epoch 495, training loss: 6053.84, average training loss: 9465.03, base loss: 10691.29
[INFO 2017-06-26 14:15:45,670 main.py:50] epoch 496, training loss: 5904.06, average training loss: 9457.86, base loss: 10691.27
[INFO 2017-06-26 14:15:48,569 main.py:50] epoch 497, training loss: 5938.10, average training loss: 9450.79, base loss: 10691.24
[INFO 2017-06-26 14:15:51,446 main.py:50] epoch 498, training loss: 5891.63, average training loss: 9443.66, base loss: 10690.75
[INFO 2017-06-26 14:15:54,335 main.py:50] epoch 499, training loss: 6048.26, average training loss: 9436.87, base loss: 10690.85
[INFO 2017-06-26 14:15:54,335 main.py:52] epoch 499, testing
[INFO 2017-06-26 14:16:07,686 main.py:103] average testing loss: 5959.41, base loss: 10723.08
[INFO 2017-06-26 14:16:07,687 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 14:16:07,703 main.py:76] current best accuracy: 5959.41
[INFO 2017-06-26 14:16:10,598 main.py:50] epoch 500, training loss: 5891.07, average training loss: 9429.79, base loss: 10690.29
[INFO 2017-06-26 14:16:13,471 main.py:50] epoch 501, training loss: 5804.20, average training loss: 9422.57, base loss: 10689.69
[INFO 2017-06-26 14:16:16,364 main.py:50] epoch 502, training loss: 5805.79, average training loss: 9415.38, base loss: 10688.88
[INFO 2017-06-26 14:16:19,294 main.py:50] epoch 503, training loss: 6014.21, average training loss: 9408.63, base loss: 10689.57
[INFO 2017-06-26 14:16:22,172 main.py:50] epoch 504, training loss: 5845.13, average training loss: 9401.57, base loss: 10689.38
[INFO 2017-06-26 14:16:25,055 main.py:50] epoch 505, training loss: 5899.00, average training loss: 9394.65, base loss: 10689.62
[INFO 2017-06-26 14:16:27,973 main.py:50] epoch 506, training loss: 5887.86, average training loss: 9387.74, base loss: 10689.75
[INFO 2017-06-26 14:16:30,861 main.py:50] epoch 507, training loss: 5733.56, average training loss: 9380.54, base loss: 10688.69
[INFO 2017-06-26 14:16:33,734 main.py:50] epoch 508, training loss: 5700.55, average training loss: 9373.31, base loss: 10687.77
[INFO 2017-06-26 14:16:36,621 main.py:50] epoch 509, training loss: 5922.95, average training loss: 9366.55, base loss: 10688.31
[INFO 2017-06-26 14:16:39,513 main.py:50] epoch 510, training loss: 5874.73, average training loss: 9359.71, base loss: 10688.27
[INFO 2017-06-26 14:16:42,410 main.py:50] epoch 511, training loss: 6025.29, average training loss: 9353.20, base loss: 10688.94
[INFO 2017-06-26 14:16:45,300 main.py:50] epoch 512, training loss: 5857.57, average training loss: 9346.39, base loss: 10688.61
[INFO 2017-06-26 14:16:48,208 main.py:50] epoch 513, training loss: 5871.35, average training loss: 9339.63, base loss: 10688.88
[INFO 2017-06-26 14:16:51,128 main.py:50] epoch 514, training loss: 5934.61, average training loss: 9333.01, base loss: 10689.07
[INFO 2017-06-26 14:16:54,019 main.py:50] epoch 515, training loss: 5847.11, average training loss: 9326.26, base loss: 10689.05
[INFO 2017-06-26 14:16:56,892 main.py:50] epoch 516, training loss: 5683.08, average training loss: 9319.21, base loss: 10687.78
[INFO 2017-06-26 14:16:59,768 main.py:50] epoch 517, training loss: 5878.49, average training loss: 9312.57, base loss: 10687.54
[INFO 2017-06-26 14:17:02,660 main.py:50] epoch 518, training loss: 5772.35, average training loss: 9305.75, base loss: 10687.32
[INFO 2017-06-26 14:17:05,544 main.py:50] epoch 519, training loss: 5769.87, average training loss: 9298.95, base loss: 10686.86
[INFO 2017-06-26 14:17:08,420 main.py:50] epoch 520, training loss: 5922.22, average training loss: 9292.47, base loss: 10687.60
[INFO 2017-06-26 14:17:11,308 main.py:50] epoch 521, training loss: 5868.16, average training loss: 9285.91, base loss: 10687.61
[INFO 2017-06-26 14:17:14,191 main.py:50] epoch 522, training loss: 5726.65, average training loss: 9279.10, base loss: 10687.04
[INFO 2017-06-26 14:17:17,067 main.py:50] epoch 523, training loss: 5765.14, average training loss: 9272.40, base loss: 10686.52
[INFO 2017-06-26 14:17:19,951 main.py:50] epoch 524, training loss: 5699.23, average training loss: 9265.59, base loss: 10685.86
[INFO 2017-06-26 14:17:22,836 main.py:50] epoch 525, training loss: 5788.38, average training loss: 9258.98, base loss: 10685.81
[INFO 2017-06-26 14:17:25,727 main.py:50] epoch 526, training loss: 5841.21, average training loss: 9252.49, base loss: 10685.77
[INFO 2017-06-26 14:17:28,614 main.py:50] epoch 527, training loss: 5790.31, average training loss: 9245.94, base loss: 10685.54
[INFO 2017-06-26 14:17:31,499 main.py:50] epoch 528, training loss: 5773.30, average training loss: 9239.37, base loss: 10685.04
[INFO 2017-06-26 14:17:34,374 main.py:50] epoch 529, training loss: 5718.48, average training loss: 9232.73, base loss: 10684.91
[INFO 2017-06-26 14:17:37,256 main.py:50] epoch 530, training loss: 5662.46, average training loss: 9226.01, base loss: 10683.77
[INFO 2017-06-26 14:17:40,133 main.py:50] epoch 531, training loss: 5739.95, average training loss: 9219.45, base loss: 10683.64
[INFO 2017-06-26 14:17:43,023 main.py:50] epoch 532, training loss: 5674.58, average training loss: 9212.80, base loss: 10682.84
[INFO 2017-06-26 14:17:45,907 main.py:50] epoch 533, training loss: 5563.85, average training loss: 9205.97, base loss: 10682.08
[INFO 2017-06-26 14:17:48,783 main.py:50] epoch 534, training loss: 5584.36, average training loss: 9199.20, base loss: 10680.51
[INFO 2017-06-26 14:17:51,672 main.py:50] epoch 535, training loss: 5831.58, average training loss: 9192.92, base loss: 10680.63
[INFO 2017-06-26 14:17:54,576 main.py:50] epoch 536, training loss: 5828.89, average training loss: 9186.65, base loss: 10681.23
[INFO 2017-06-26 14:17:57,467 main.py:50] epoch 537, training loss: 6005.31, average training loss: 9180.74, base loss: 10682.29
[INFO 2017-06-26 14:18:00,345 main.py:50] epoch 538, training loss: 5844.11, average training loss: 9174.55, base loss: 10682.70
[INFO 2017-06-26 14:18:03,280 main.py:50] epoch 539, training loss: 5641.45, average training loss: 9168.01, base loss: 10681.94
[INFO 2017-06-26 14:18:06,156 main.py:50] epoch 540, training loss: 5844.85, average training loss: 9161.86, base loss: 10682.03
[INFO 2017-06-26 14:18:09,034 main.py:50] epoch 541, training loss: 5800.96, average training loss: 9155.66, base loss: 10681.62
[INFO 2017-06-26 14:18:11,914 main.py:50] epoch 542, training loss: 5612.85, average training loss: 9149.14, base loss: 10680.70
[INFO 2017-06-26 14:18:14,794 main.py:50] epoch 543, training loss: 5819.58, average training loss: 9143.02, base loss: 10681.30
[INFO 2017-06-26 14:18:17,691 main.py:50] epoch 544, training loss: 5800.34, average training loss: 9136.88, base loss: 10681.45
[INFO 2017-06-26 14:18:20,574 main.py:50] epoch 545, training loss: 5853.89, average training loss: 9130.87, base loss: 10681.66
[INFO 2017-06-26 14:18:23,488 main.py:50] epoch 546, training loss: 5833.92, average training loss: 9124.84, base loss: 10682.02
[INFO 2017-06-26 14:18:26,365 main.py:50] epoch 547, training loss: 5891.09, average training loss: 9118.94, base loss: 10682.89
[INFO 2017-06-26 14:18:29,254 main.py:50] epoch 548, training loss: 5826.38, average training loss: 9112.94, base loss: 10683.26
[INFO 2017-06-26 14:18:32,161 main.py:50] epoch 549, training loss: 5774.62, average training loss: 9106.88, base loss: 10683.49
[INFO 2017-06-26 14:18:35,038 main.py:50] epoch 550, training loss: 5969.88, average training loss: 9101.18, base loss: 10684.20
[INFO 2017-06-26 14:18:37,930 main.py:50] epoch 551, training loss: 5692.74, average training loss: 9095.01, base loss: 10683.86
[INFO 2017-06-26 14:18:40,812 main.py:50] epoch 552, training loss: 5916.92, average training loss: 9089.26, base loss: 10684.55
[INFO 2017-06-26 14:18:43,686 main.py:50] epoch 553, training loss: 5693.78, average training loss: 9083.13, base loss: 10684.14
[INFO 2017-06-26 14:18:46,561 main.py:50] epoch 554, training loss: 6001.19, average training loss: 9077.58, base loss: 10685.55
[INFO 2017-06-26 14:18:49,453 main.py:50] epoch 555, training loss: 5799.05, average training loss: 9071.68, base loss: 10685.23
[INFO 2017-06-26 14:18:52,337 main.py:50] epoch 556, training loss: 5855.78, average training loss: 9065.91, base loss: 10685.83
[INFO 2017-06-26 14:18:55,243 main.py:50] epoch 557, training loss: 5692.89, average training loss: 9059.86, base loss: 10685.61
[INFO 2017-06-26 14:18:58,155 main.py:50] epoch 558, training loss: 5775.30, average training loss: 9053.99, base loss: 10685.88
[INFO 2017-06-26 14:19:01,055 main.py:50] epoch 559, training loss: 5751.54, average training loss: 9048.09, base loss: 10686.11
[INFO 2017-06-26 14:19:03,939 main.py:50] epoch 560, training loss: 5715.08, average training loss: 9042.15, base loss: 10686.17
[INFO 2017-06-26 14:19:06,866 main.py:50] epoch 561, training loss: 5887.65, average training loss: 9036.54, base loss: 10686.45
[INFO 2017-06-26 14:19:09,770 main.py:50] epoch 562, training loss: 5740.95, average training loss: 9030.68, base loss: 10686.36
[INFO 2017-06-26 14:19:12,650 main.py:50] epoch 563, training loss: 5654.71, average training loss: 9024.70, base loss: 10685.81
[INFO 2017-06-26 14:19:15,527 main.py:50] epoch 564, training loss: 5580.55, average training loss: 9018.60, base loss: 10685.36
[INFO 2017-06-26 14:19:18,403 main.py:50] epoch 565, training loss: 5705.82, average training loss: 9012.75, base loss: 10685.26
[INFO 2017-06-26 14:19:21,317 main.py:50] epoch 566, training loss: 5602.32, average training loss: 9006.73, base loss: 10684.77
[INFO 2017-06-26 14:19:24,198 main.py:50] epoch 567, training loss: 5645.10, average training loss: 9000.81, base loss: 10684.48
[INFO 2017-06-26 14:19:27,069 main.py:50] epoch 568, training loss: 5681.29, average training loss: 8994.98, base loss: 10684.00
[INFO 2017-06-26 14:19:29,976 main.py:50] epoch 569, training loss: 5507.11, average training loss: 8988.86, base loss: 10682.87
[INFO 2017-06-26 14:19:32,887 main.py:50] epoch 570, training loss: 5634.95, average training loss: 8982.99, base loss: 10682.68
[INFO 2017-06-26 14:19:35,759 main.py:50] epoch 571, training loss: 5713.89, average training loss: 8977.27, base loss: 10683.09
[INFO 2017-06-26 14:19:38,645 main.py:50] epoch 572, training loss: 5746.11, average training loss: 8971.63, base loss: 10683.12
[INFO 2017-06-26 14:19:41,512 main.py:50] epoch 573, training loss: 5621.39, average training loss: 8965.80, base loss: 10682.67
[INFO 2017-06-26 14:19:44,380 main.py:50] epoch 574, training loss: 5734.74, average training loss: 8960.18, base loss: 10682.60
[INFO 2017-06-26 14:19:47,256 main.py:50] epoch 575, training loss: 5687.22, average training loss: 8954.50, base loss: 10683.10
[INFO 2017-06-26 14:19:50,129 main.py:50] epoch 576, training loss: 5605.56, average training loss: 8948.69, base loss: 10681.94
[INFO 2017-06-26 14:19:53,035 main.py:50] epoch 577, training loss: 5676.41, average training loss: 8943.03, base loss: 10681.63
[INFO 2017-06-26 14:19:55,918 main.py:50] epoch 578, training loss: 5775.72, average training loss: 8937.56, base loss: 10681.79
[INFO 2017-06-26 14:19:58,792 main.py:50] epoch 579, training loss: 5620.71, average training loss: 8931.84, base loss: 10680.90
[INFO 2017-06-26 14:20:01,667 main.py:50] epoch 580, training loss: 5699.57, average training loss: 8926.28, base loss: 10681.03
[INFO 2017-06-26 14:20:04,551 main.py:50] epoch 581, training loss: 5753.24, average training loss: 8920.83, base loss: 10681.02
[INFO 2017-06-26 14:20:07,435 main.py:50] epoch 582, training loss: 5687.90, average training loss: 8915.28, base loss: 10681.31
[INFO 2017-06-26 14:20:10,318 main.py:50] epoch 583, training loss: 5644.24, average training loss: 8909.68, base loss: 10681.39
[INFO 2017-06-26 14:20:13,200 main.py:50] epoch 584, training loss: 5723.66, average training loss: 8904.23, base loss: 10681.16
[INFO 2017-06-26 14:20:16,118 main.py:50] epoch 585, training loss: 5760.79, average training loss: 8898.87, base loss: 10681.62
[INFO 2017-06-26 14:20:19,032 main.py:50] epoch 586, training loss: 5670.00, average training loss: 8893.37, base loss: 10681.80
[INFO 2017-06-26 14:20:21,914 main.py:50] epoch 587, training loss: 5662.33, average training loss: 8887.87, base loss: 10681.76
[INFO 2017-06-26 14:20:24,793 main.py:50] epoch 588, training loss: 5650.41, average training loss: 8882.38, base loss: 10681.78
[INFO 2017-06-26 14:20:27,675 main.py:50] epoch 589, training loss: 5564.79, average training loss: 8876.75, base loss: 10681.41
[INFO 2017-06-26 14:20:30,566 main.py:50] epoch 590, training loss: 5847.25, average training loss: 8871.63, base loss: 10682.42
[INFO 2017-06-26 14:20:33,491 main.py:50] epoch 591, training loss: 5594.15, average training loss: 8866.09, base loss: 10681.90
[INFO 2017-06-26 14:20:36,369 main.py:50] epoch 592, training loss: 5705.57, average training loss: 8860.76, base loss: 10681.89
[INFO 2017-06-26 14:20:39,289 main.py:50] epoch 593, training loss: 5649.16, average training loss: 8855.35, base loss: 10681.45
[INFO 2017-06-26 14:20:42,172 main.py:50] epoch 594, training loss: 5665.24, average training loss: 8849.99, base loss: 10681.08
[INFO 2017-06-26 14:20:45,047 main.py:50] epoch 595, training loss: 5591.98, average training loss: 8844.53, base loss: 10680.25
[INFO 2017-06-26 14:20:47,937 main.py:50] epoch 596, training loss: 5666.68, average training loss: 8839.20, base loss: 10679.83
[INFO 2017-06-26 14:20:50,853 main.py:50] epoch 597, training loss: 5688.84, average training loss: 8833.94, base loss: 10679.50
[INFO 2017-06-26 14:20:53,776 main.py:50] epoch 598, training loss: 5640.79, average training loss: 8828.60, base loss: 10679.33
[INFO 2017-06-26 14:20:56,655 main.py:50] epoch 599, training loss: 5860.54, average training loss: 8823.66, base loss: 10679.41
[INFO 2017-06-26 14:20:56,655 main.py:52] epoch 599, testing
[INFO 2017-06-26 14:21:09,892 main.py:103] average testing loss: 5662.86, base loss: 10655.28
[INFO 2017-06-26 14:21:09,892 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 14:21:09,899 main.py:76] current best accuracy: 5662.86
[INFO 2017-06-26 14:21:12,773 main.py:50] epoch 600, training loss: 5663.41, average training loss: 8818.40, base loss: 10679.50
[INFO 2017-06-26 14:21:15,682 main.py:50] epoch 601, training loss: 5886.20, average training loss: 8813.53, base loss: 10680.18
[INFO 2017-06-26 14:21:18,567 main.py:50] epoch 602, training loss: 5905.08, average training loss: 8808.71, base loss: 10681.68
[INFO 2017-06-26 14:21:21,481 main.py:50] epoch 603, training loss: 5867.65, average training loss: 8803.84, base loss: 10681.01
[INFO 2017-06-26 14:21:24,359 main.py:50] epoch 604, training loss: 5462.80, average training loss: 8798.31, base loss: 10679.92
[INFO 2017-06-26 14:21:27,281 main.py:50] epoch 605, training loss: 5926.41, average training loss: 8793.57, base loss: 10679.83
[INFO 2017-06-26 14:21:30,168 main.py:50] epoch 606, training loss: 5895.15, average training loss: 8788.80, base loss: 10680.19
[INFO 2017-06-26 14:21:33,051 main.py:50] epoch 607, training loss: 5751.86, average training loss: 8783.80, base loss: 10680.13
[INFO 2017-06-26 14:21:35,936 main.py:50] epoch 608, training loss: 5794.34, average training loss: 8778.90, base loss: 10680.34
[INFO 2017-06-26 14:21:38,840 main.py:50] epoch 609, training loss: 5543.33, average training loss: 8773.59, base loss: 10679.69
[INFO 2017-06-26 14:21:41,725 main.py:50] epoch 610, training loss: 5971.96, average training loss: 8769.01, base loss: 10680.18
[INFO 2017-06-26 14:21:44,610 main.py:50] epoch 611, training loss: 5817.36, average training loss: 8764.18, base loss: 10680.80
[INFO 2017-06-26 14:21:47,500 main.py:50] epoch 612, training loss: 5726.37, average training loss: 8759.23, base loss: 10680.44
[INFO 2017-06-26 14:21:50,407 main.py:50] epoch 613, training loss: 5637.17, average training loss: 8754.14, base loss: 10680.59
[INFO 2017-06-26 14:21:53,298 main.py:50] epoch 614, training loss: 5639.40, average training loss: 8749.08, base loss: 10680.22
[INFO 2017-06-26 14:21:56,187 main.py:50] epoch 615, training loss: 5892.83, average training loss: 8744.44, base loss: 10680.99
[INFO 2017-06-26 14:21:59,065 main.py:50] epoch 616, training loss: 5655.93, average training loss: 8739.44, base loss: 10681.06
[INFO 2017-06-26 14:22:01,959 main.py:50] epoch 617, training loss: 5602.35, average training loss: 8734.36, base loss: 10680.41
[INFO 2017-06-26 14:22:04,841 main.py:50] epoch 618, training loss: 5436.03, average training loss: 8729.03, base loss: 10679.54
[INFO 2017-06-26 14:22:07,720 main.py:50] epoch 619, training loss: 5743.60, average training loss: 8724.22, base loss: 10679.18
[INFO 2017-06-26 14:22:10,597 main.py:50] epoch 620, training loss: 5639.26, average training loss: 8719.25, base loss: 10679.10
[INFO 2017-06-26 14:22:13,505 main.py:50] epoch 621, training loss: 5700.56, average training loss: 8714.40, base loss: 10679.34
[INFO 2017-06-26 14:22:16,391 main.py:50] epoch 622, training loss: 5717.74, average training loss: 8709.59, base loss: 10678.80
[INFO 2017-06-26 14:22:19,284 main.py:50] epoch 623, training loss: 5772.81, average training loss: 8704.88, base loss: 10679.27
[INFO 2017-06-26 14:22:22,169 main.py:50] epoch 624, training loss: 5622.63, average training loss: 8699.95, base loss: 10679.25
[INFO 2017-06-26 14:22:25,047 main.py:50] epoch 625, training loss: 5735.82, average training loss: 8695.21, base loss: 10679.58
[INFO 2017-06-26 14:22:27,938 main.py:50] epoch 626, training loss: 5533.05, average training loss: 8690.17, base loss: 10678.67
[INFO 2017-06-26 14:22:30,826 main.py:50] epoch 627, training loss: 5577.91, average training loss: 8685.21, base loss: 10678.59
[INFO 2017-06-26 14:22:33,727 main.py:50] epoch 628, training loss: 5844.24, average training loss: 8680.70, base loss: 10679.30
[INFO 2017-06-26 14:22:36,619 main.py:50] epoch 629, training loss: 5640.96, average training loss: 8675.87, base loss: 10679.29
[INFO 2017-06-26 14:22:39,517 main.py:50] epoch 630, training loss: 5520.99, average training loss: 8670.87, base loss: 10679.18
[INFO 2017-06-26 14:22:42,406 main.py:50] epoch 631, training loss: 5350.97, average training loss: 8665.62, base loss: 10678.08
[INFO 2017-06-26 14:22:45,320 main.py:50] epoch 632, training loss: 5643.07, average training loss: 8660.84, base loss: 10678.45
[INFO 2017-06-26 14:22:48,216 main.py:50] epoch 633, training loss: 5550.26, average training loss: 8655.94, base loss: 10678.13
[INFO 2017-06-26 14:22:51,138 main.py:50] epoch 634, training loss: 5531.95, average training loss: 8651.02, base loss: 10677.84
[INFO 2017-06-26 14:22:54,036 main.py:50] epoch 635, training loss: 5681.46, average training loss: 8646.35, base loss: 10677.90
[INFO 2017-06-26 14:22:56,926 main.py:50] epoch 636, training loss: 5471.61, average training loss: 8641.36, base loss: 10677.32
[INFO 2017-06-26 14:22:59,802 main.py:50] epoch 637, training loss: 5529.75, average training loss: 8636.49, base loss: 10677.28
[INFO 2017-06-26 14:23:02,689 main.py:50] epoch 638, training loss: 5538.30, average training loss: 8631.64, base loss: 10677.64
[INFO 2017-06-26 14:23:05,579 main.py:50] epoch 639, training loss: 5563.60, average training loss: 8626.85, base loss: 10677.27
[INFO 2017-06-26 14:23:08,460 main.py:50] epoch 640, training loss: 5545.71, average training loss: 8622.04, base loss: 10677.51
[INFO 2017-06-26 14:23:11,342 main.py:50] epoch 641, training loss: 5472.56, average training loss: 8617.13, base loss: 10677.38
[INFO 2017-06-26 14:23:14,232 main.py:50] epoch 642, training loss: 5552.12, average training loss: 8612.37, base loss: 10677.45
[INFO 2017-06-26 14:23:17,114 main.py:50] epoch 643, training loss: 5529.61, average training loss: 8607.58, base loss: 10677.51
[INFO 2017-06-26 14:23:19,999 main.py:50] epoch 644, training loss: 5604.98, average training loss: 8602.92, base loss: 10677.98
[INFO 2017-06-26 14:23:22,916 main.py:50] epoch 645, training loss: 5471.17, average training loss: 8598.08, base loss: 10677.32
[INFO 2017-06-26 14:23:25,831 main.py:50] epoch 646, training loss: 5587.88, average training loss: 8593.42, base loss: 10677.43
[INFO 2017-06-26 14:23:28,723 main.py:50] epoch 647, training loss: 5615.80, average training loss: 8588.83, base loss: 10677.93
[INFO 2017-06-26 14:23:31,640 main.py:50] epoch 648, training loss: 5635.94, average training loss: 8584.28, base loss: 10678.61
[INFO 2017-06-26 14:23:34,527 main.py:50] epoch 649, training loss: 5516.27, average training loss: 8579.56, base loss: 10678.58
[INFO 2017-06-26 14:23:37,413 main.py:50] epoch 650, training loss: 5468.60, average training loss: 8574.78, base loss: 10678.76
[INFO 2017-06-26 14:23:40,323 main.py:50] epoch 651, training loss: 5489.60, average training loss: 8570.05, base loss: 10678.38
[INFO 2017-06-26 14:23:43,221 main.py:50] epoch 652, training loss: 5435.29, average training loss: 8565.25, base loss: 10678.17
[INFO 2017-06-26 14:23:46,104 main.py:50] epoch 653, training loss: 5520.97, average training loss: 8560.59, base loss: 10678.61
[INFO 2017-06-26 14:23:48,985 main.py:50] epoch 654, training loss: 5648.21, average training loss: 8556.15, base loss: 10678.75
[INFO 2017-06-26 14:23:51,871 main.py:50] epoch 655, training loss: 5530.60, average training loss: 8551.53, base loss: 10679.08
[INFO 2017-06-26 14:23:54,754 main.py:50] epoch 656, training loss: 5509.00, average training loss: 8546.90, base loss: 10678.90
[INFO 2017-06-26 14:23:57,637 main.py:50] epoch 657, training loss: 5455.00, average training loss: 8542.20, base loss: 10678.63
[INFO 2017-06-26 14:24:00,513 main.py:50] epoch 658, training loss: 5669.19, average training loss: 8537.84, base loss: 10679.34
[INFO 2017-06-26 14:24:03,391 main.py:50] epoch 659, training loss: 5503.00, average training loss: 8533.25, base loss: 10679.49
[INFO 2017-06-26 14:24:06,272 main.py:50] epoch 660, training loss: 5487.32, average training loss: 8528.64, base loss: 10679.63
[INFO 2017-06-26 14:24:09,154 main.py:50] epoch 661, training loss: 5532.58, average training loss: 8524.11, base loss: 10679.87
[INFO 2017-06-26 14:24:12,053 main.py:50] epoch 662, training loss: 5407.66, average training loss: 8519.41, base loss: 10679.05
[INFO 2017-06-26 14:24:14,948 main.py:50] epoch 663, training loss: 5556.67, average training loss: 8514.95, base loss: 10679.12
[INFO 2017-06-26 14:24:17,830 main.py:50] epoch 664, training loss: 5457.13, average training loss: 8510.35, base loss: 10679.04
[INFO 2017-06-26 14:24:20,721 main.py:50] epoch 665, training loss: 5546.96, average training loss: 8505.90, base loss: 10679.06
[INFO 2017-06-26 14:24:23,598 main.py:50] epoch 666, training loss: 5420.67, average training loss: 8501.28, base loss: 10679.21
[INFO 2017-06-26 14:24:26,469 main.py:50] epoch 667, training loss: 5525.99, average training loss: 8496.82, base loss: 10679.89
[INFO 2017-06-26 14:24:29,353 main.py:50] epoch 668, training loss: 5489.02, average training loss: 8492.33, base loss: 10680.14
[INFO 2017-06-26 14:24:32,232 main.py:50] epoch 669, training loss: 5478.46, average training loss: 8487.83, base loss: 10680.33
[INFO 2017-06-26 14:24:35,107 main.py:50] epoch 670, training loss: 5620.88, average training loss: 8483.56, base loss: 10680.84
[INFO 2017-06-26 14:24:38,020 main.py:50] epoch 671, training loss: 5345.39, average training loss: 8478.89, base loss: 10680.16
[INFO 2017-06-26 14:24:40,937 main.py:50] epoch 672, training loss: 5498.54, average training loss: 8474.46, base loss: 10679.85
[INFO 2017-06-26 14:24:43,838 main.py:50] epoch 673, training loss: 5403.18, average training loss: 8469.90, base loss: 10679.70
[INFO 2017-06-26 14:24:46,727 main.py:50] epoch 674, training loss: 5353.44, average training loss: 8465.28, base loss: 10679.15
[INFO 2017-06-26 14:24:49,614 main.py:50] epoch 675, training loss: 5397.11, average training loss: 8460.74, base loss: 10679.14
[INFO 2017-06-26 14:24:52,504 main.py:50] epoch 676, training loss: 5403.99, average training loss: 8456.23, base loss: 10678.79
[INFO 2017-06-26 14:24:55,401 main.py:50] epoch 677, training loss: 5424.71, average training loss: 8451.76, base loss: 10678.31
[INFO 2017-06-26 14:24:58,280 main.py:50] epoch 678, training loss: 5477.31, average training loss: 8447.38, base loss: 10678.52
[INFO 2017-06-26 14:25:01,164 main.py:50] epoch 679, training loss: 5559.06, average training loss: 8443.13, base loss: 10679.21
[INFO 2017-06-26 14:25:04,077 main.py:50] epoch 680, training loss: 5645.61, average training loss: 8439.02, base loss: 10679.64
[INFO 2017-06-26 14:25:06,967 main.py:50] epoch 681, training loss: 5497.31, average training loss: 8434.71, base loss: 10679.33
[INFO 2017-06-26 14:25:09,847 main.py:50] epoch 682, training loss: 5789.75, average training loss: 8430.84, base loss: 10680.24
[INFO 2017-06-26 14:25:12,738 main.py:50] epoch 683, training loss: 5543.27, average training loss: 8426.61, base loss: 10680.31
[INFO 2017-06-26 14:25:15,621 main.py:50] epoch 684, training loss: 5555.45, average training loss: 8422.42, base loss: 10680.49
[INFO 2017-06-26 14:25:18,536 main.py:50] epoch 685, training loss: 5463.41, average training loss: 8418.11, base loss: 10680.33
[INFO 2017-06-26 14:25:21,459 main.py:50] epoch 686, training loss: 5500.15, average training loss: 8413.86, base loss: 10680.05
[INFO 2017-06-26 14:25:24,332 main.py:50] epoch 687, training loss: 5537.30, average training loss: 8409.68, base loss: 10679.99
[INFO 2017-06-26 14:25:27,203 main.py:50] epoch 688, training loss: 5400.08, average training loss: 8405.31, base loss: 10679.17
[INFO 2017-06-26 14:25:30,121 main.py:50] epoch 689, training loss: 5432.75, average training loss: 8401.01, base loss: 10679.07
[INFO 2017-06-26 14:25:32,999 main.py:50] epoch 690, training loss: 5388.58, average training loss: 8396.65, base loss: 10678.87
[INFO 2017-06-26 14:25:35,873 main.py:50] epoch 691, training loss: 5417.07, average training loss: 8392.34, base loss: 10678.71
[INFO 2017-06-26 14:25:38,768 main.py:50] epoch 692, training loss: 5504.67, average training loss: 8388.17, base loss: 10678.85
[INFO 2017-06-26 14:25:41,666 main.py:50] epoch 693, training loss: 5416.44, average training loss: 8383.89, base loss: 10679.03
[INFO 2017-06-26 14:25:44,548 main.py:50] epoch 694, training loss: 5449.71, average training loss: 8379.67, base loss: 10679.25
[INFO 2017-06-26 14:25:47,435 main.py:50] epoch 695, training loss: 5397.47, average training loss: 8375.38, base loss: 10679.33
[INFO 2017-06-26 14:25:50,346 main.py:50] epoch 696, training loss: 5313.17, average training loss: 8370.99, base loss: 10678.59
[INFO 2017-06-26 14:25:53,229 main.py:50] epoch 697, training loss: 5395.59, average training loss: 8366.73, base loss: 10678.28
[INFO 2017-06-26 14:25:56,127 main.py:50] epoch 698, training loss: 5526.19, average training loss: 8362.66, base loss: 10678.26
[INFO 2017-06-26 14:25:59,004 main.py:50] epoch 699, training loss: 5517.02, average training loss: 8358.60, base loss: 10678.57
[INFO 2017-06-26 14:25:59,004 main.py:52] epoch 699, testing
[INFO 2017-06-26 14:26:12,239 main.py:103] average testing loss: 5500.17, base loss: 10913.90
[INFO 2017-06-26 14:26:12,240 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 14:26:12,247 main.py:76] current best accuracy: 5500.17
[INFO 2017-06-26 14:26:15,126 main.py:50] epoch 700, training loss: 5414.29, average training loss: 8354.40, base loss: 10678.35
[INFO 2017-06-26 14:26:18,004 main.py:50] epoch 701, training loss: 5295.78, average training loss: 8350.04, base loss: 10677.51
[INFO 2017-06-26 14:26:20,894 main.py:50] epoch 702, training loss: 5411.60, average training loss: 8345.86, base loss: 10677.02
[INFO 2017-06-26 14:26:23,783 main.py:50] epoch 703, training loss: 5333.64, average training loss: 8341.58, base loss: 10676.61
[INFO 2017-06-26 14:26:26,696 main.py:50] epoch 704, training loss: 5543.82, average training loss: 8337.62, base loss: 10676.94
[INFO 2017-06-26 14:26:29,584 main.py:50] epoch 705, training loss: 5383.96, average training loss: 8333.43, base loss: 10676.81
[INFO 2017-06-26 14:26:32,461 main.py:50] epoch 706, training loss: 5322.22, average training loss: 8329.17, base loss: 10676.44
[INFO 2017-06-26 14:26:35,343 main.py:50] epoch 707, training loss: 5448.93, average training loss: 8325.10, base loss: 10676.59
[INFO 2017-06-26 14:26:38,223 main.py:50] epoch 708, training loss: 5417.69, average training loss: 8321.00, base loss: 10676.54
[INFO 2017-06-26 14:26:41,099 main.py:50] epoch 709, training loss: 5412.71, average training loss: 8316.91, base loss: 10676.46
[INFO 2017-06-26 14:26:43,997 main.py:50] epoch 710, training loss: 5453.80, average training loss: 8312.88, base loss: 10676.66
[INFO 2017-06-26 14:26:46,886 main.py:50] epoch 711, training loss: 5237.44, average training loss: 8308.56, base loss: 10676.10
[INFO 2017-06-26 14:26:49,803 main.py:50] epoch 712, training loss: 5542.18, average training loss: 8304.68, base loss: 10676.62
[INFO 2017-06-26 14:26:52,691 main.py:50] epoch 713, training loss: 5393.78, average training loss: 8300.60, base loss: 10676.75
[INFO 2017-06-26 14:26:55,595 main.py:50] epoch 714, training loss: 5414.14, average training loss: 8296.57, base loss: 10676.81
[INFO 2017-06-26 14:26:58,475 main.py:50] epoch 715, training loss: 5429.31, average training loss: 8292.56, base loss: 10677.00
[INFO 2017-06-26 14:27:01,351 main.py:50] epoch 716, training loss: 5274.80, average training loss: 8288.35, base loss: 10676.58
[INFO 2017-06-26 14:27:04,262 main.py:50] epoch 717, training loss: 5425.64, average training loss: 8284.37, base loss: 10676.53
[INFO 2017-06-26 14:27:07,137 main.py:50] epoch 718, training loss: 5418.50, average training loss: 8280.38, base loss: 10676.57
[INFO 2017-06-26 14:27:10,047 main.py:50] epoch 719, training loss: 5350.61, average training loss: 8276.31, base loss: 10676.28
[INFO 2017-06-26 14:27:12,937 main.py:50] epoch 720, training loss: 5351.81, average training loss: 8272.26, base loss: 10675.96
[INFO 2017-06-26 14:27:15,854 main.py:50] epoch 721, training loss: 5509.91, average training loss: 8268.43, base loss: 10676.59
[INFO 2017-06-26 14:27:18,730 main.py:50] epoch 722, training loss: 5483.39, average training loss: 8264.58, base loss: 10676.81
[INFO 2017-06-26 14:27:21,612 main.py:50] epoch 723, training loss: 5297.79, average training loss: 8260.48, base loss: 10676.69
[INFO 2017-06-26 14:27:24,503 main.py:50] epoch 724, training loss: 5356.69, average training loss: 8256.47, base loss: 10676.72
[INFO 2017-06-26 14:27:27,392 main.py:50] epoch 725, training loss: 5286.09, average training loss: 8252.38, base loss: 10676.45
[INFO 2017-06-26 14:27:30,281 main.py:50] epoch 726, training loss: 5316.53, average training loss: 8248.34, base loss: 10676.39
[INFO 2017-06-26 14:27:33,161 main.py:50] epoch 727, training loss: 5229.09, average training loss: 8244.20, base loss: 10675.82
[INFO 2017-06-26 14:27:36,084 main.py:50] epoch 728, training loss: 5255.61, average training loss: 8240.10, base loss: 10675.55
[INFO 2017-06-26 14:27:38,977 main.py:50] epoch 729, training loss: 5458.19, average training loss: 8236.29, base loss: 10675.86
[INFO 2017-06-26 14:27:41,865 main.py:50] epoch 730, training loss: 5384.30, average training loss: 8232.39, base loss: 10676.24
[INFO 2017-06-26 14:27:44,753 main.py:50] epoch 731, training loss: 5358.97, average training loss: 8228.46, base loss: 10675.83
[INFO 2017-06-26 14:27:47,665 main.py:50] epoch 732, training loss: 5368.50, average training loss: 8224.56, base loss: 10675.79
[INFO 2017-06-26 14:27:50,556 main.py:50] epoch 733, training loss: 5287.99, average training loss: 8220.56, base loss: 10675.30
[INFO 2017-06-26 14:27:53,483 main.py:50] epoch 734, training loss: 5314.64, average training loss: 8216.60, base loss: 10675.00
[INFO 2017-06-26 14:27:56,403 main.py:50] epoch 735, training loss: 5464.70, average training loss: 8212.86, base loss: 10675.61
[INFO 2017-06-26 14:27:59,286 main.py:50] epoch 736, training loss: 5455.45, average training loss: 8209.12, base loss: 10675.78
[INFO 2017-06-26 14:28:02,173 main.py:50] epoch 737, training loss: 5302.12, average training loss: 8205.18, base loss: 10675.64
[INFO 2017-06-26 14:28:05,066 main.py:50] epoch 738, training loss: 5359.33, average training loss: 8201.33, base loss: 10675.65
[INFO 2017-06-26 14:28:07,948 main.py:50] epoch 739, training loss: 5284.57, average training loss: 8197.39, base loss: 10675.53
[INFO 2017-06-26 14:28:10,866 main.py:50] epoch 740, training loss: 5505.62, average training loss: 8193.76, base loss: 10676.13
[INFO 2017-06-26 14:28:13,773 main.py:50] epoch 741, training loss: 5452.46, average training loss: 8190.06, base loss: 10676.29
[INFO 2017-06-26 14:28:16,660 main.py:50] epoch 742, training loss: 5334.80, average training loss: 8186.22, base loss: 10676.10
[INFO 2017-06-26 14:28:19,548 main.py:50] epoch 743, training loss: 5429.70, average training loss: 8182.52, base loss: 10676.37
[INFO 2017-06-26 14:28:22,463 main.py:50] epoch 744, training loss: 5284.06, average training loss: 8178.63, base loss: 10675.92
[INFO 2017-06-26 14:28:25,350 main.py:50] epoch 745, training loss: 5424.03, average training loss: 8174.93, base loss: 10675.90
[INFO 2017-06-26 14:28:28,242 main.py:50] epoch 746, training loss: 5306.48, average training loss: 8171.09, base loss: 10675.36
[INFO 2017-06-26 14:28:31,133 main.py:50] epoch 747, training loss: 5517.60, average training loss: 8167.55, base loss: 10676.09
[INFO 2017-06-26 14:28:34,030 main.py:50] epoch 748, training loss: 5368.06, average training loss: 8163.81, base loss: 10675.75
[INFO 2017-06-26 14:28:36,908 main.py:50] epoch 749, training loss: 5416.73, average training loss: 8160.15, base loss: 10675.92
[INFO 2017-06-26 14:28:39,787 main.py:50] epoch 750, training loss: 5346.68, average training loss: 8156.40, base loss: 10675.71
[INFO 2017-06-26 14:28:42,703 main.py:50] epoch 751, training loss: 5368.39, average training loss: 8152.69, base loss: 10675.67
[INFO 2017-06-26 14:28:45,605 main.py:50] epoch 752, training loss: 5294.87, average training loss: 8148.90, base loss: 10675.70
[INFO 2017-06-26 14:28:48,498 main.py:50] epoch 753, training loss: 5364.01, average training loss: 8145.20, base loss: 10675.47
[INFO 2017-06-26 14:28:51,373 main.py:50] epoch 754, training loss: 5341.50, average training loss: 8141.49, base loss: 10675.50
[INFO 2017-06-26 14:28:54,257 main.py:50] epoch 755, training loss: 5317.72, average training loss: 8137.75, base loss: 10675.12
[INFO 2017-06-26 14:28:57,133 main.py:50] epoch 756, training loss: 5281.23, average training loss: 8133.98, base loss: 10674.78
[INFO 2017-06-26 14:29:00,024 main.py:50] epoch 757, training loss: 5228.27, average training loss: 8130.15, base loss: 10674.18
[INFO 2017-06-26 14:29:02,937 main.py:50] epoch 758, training loss: 5351.45, average training loss: 8126.49, base loss: 10674.52
[INFO 2017-06-26 14:29:05,825 main.py:50] epoch 759, training loss: 5433.15, average training loss: 8122.94, base loss: 10674.43
[INFO 2017-06-26 14:29:08,744 main.py:50] epoch 760, training loss: 5272.20, average training loss: 8119.20, base loss: 10674.26
[INFO 2017-06-26 14:29:11,636 main.py:50] epoch 761, training loss: 5246.11, average training loss: 8115.43, base loss: 10673.81
[INFO 2017-06-26 14:29:14,535 main.py:50] epoch 762, training loss: 5227.73, average training loss: 8111.64, base loss: 10673.48
[INFO 2017-06-26 14:29:17,438 main.py:50] epoch 763, training loss: 5323.48, average training loss: 8107.99, base loss: 10673.73
[INFO 2017-06-26 14:29:20,326 main.py:50] epoch 764, training loss: 5275.00, average training loss: 8104.29, base loss: 10673.48
[INFO 2017-06-26 14:29:23,249 main.py:50] epoch 765, training loss: 5267.65, average training loss: 8100.59, base loss: 10673.14
[INFO 2017-06-26 14:29:26,133 main.py:50] epoch 766, training loss: 5248.35, average training loss: 8096.87, base loss: 10672.77
[INFO 2017-06-26 14:29:29,022 main.py:50] epoch 767, training loss: 5297.06, average training loss: 8093.22, base loss: 10672.95
[INFO 2017-06-26 14:29:31,906 main.py:50] epoch 768, training loss: 5555.90, average training loss: 8089.92, base loss: 10673.64
[INFO 2017-06-26 14:29:34,786 main.py:50] epoch 769, training loss: 5354.20, average training loss: 8086.37, base loss: 10673.60
[INFO 2017-06-26 14:29:37,675 main.py:50] epoch 770, training loss: 5459.66, average training loss: 8082.96, base loss: 10673.74
[INFO 2017-06-26 14:29:40,557 main.py:50] epoch 771, training loss: 5273.69, average training loss: 8079.32, base loss: 10673.33
[INFO 2017-06-26 14:29:43,437 main.py:50] epoch 772, training loss: 5386.00, average training loss: 8075.84, base loss: 10673.48
[INFO 2017-06-26 14:29:46,330 main.py:50] epoch 773, training loss: 5333.92, average training loss: 8072.30, base loss: 10673.32
[INFO 2017-06-26 14:29:49,228 main.py:50] epoch 774, training loss: 5377.04, average training loss: 8068.82, base loss: 10673.41
[INFO 2017-06-26 14:29:52,140 main.py:50] epoch 775, training loss: 5283.69, average training loss: 8065.23, base loss: 10673.07
[INFO 2017-06-26 14:29:55,047 main.py:50] epoch 776, training loss: 5387.99, average training loss: 8061.78, base loss: 10673.11
[INFO 2017-06-26 14:29:57,927 main.py:50] epoch 777, training loss: 5337.17, average training loss: 8058.28, base loss: 10673.12
[INFO 2017-06-26 14:30:00,813 main.py:50] epoch 778, training loss: 5298.28, average training loss: 8054.74, base loss: 10672.64
[INFO 2017-06-26 14:30:03,714 main.py:50] epoch 779, training loss: 5248.75, average training loss: 8051.14, base loss: 10672.57
[INFO 2017-06-26 14:30:06,611 main.py:50] epoch 780, training loss: 5287.49, average training loss: 8047.60, base loss: 10672.37
[INFO 2017-06-26 14:30:09,518 main.py:50] epoch 781, training loss: 5420.93, average training loss: 8044.24, base loss: 10672.34
[INFO 2017-06-26 14:30:12,427 main.py:50] epoch 782, training loss: 5458.20, average training loss: 8040.94, base loss: 10673.27
[INFO 2017-06-26 14:30:15,328 main.py:50] epoch 783, training loss: 5291.38, average training loss: 8037.43, base loss: 10672.94
[INFO 2017-06-26 14:30:18,207 main.py:50] epoch 784, training loss: 5304.63, average training loss: 8033.95, base loss: 10672.77
[INFO 2017-06-26 14:30:21,092 main.py:50] epoch 785, training loss: 5485.52, average training loss: 8030.71, base loss: 10673.38
[INFO 2017-06-26 14:30:23,995 main.py:50] epoch 786, training loss: 5335.83, average training loss: 8027.29, base loss: 10673.22
[INFO 2017-06-26 14:30:26,882 main.py:50] epoch 787, training loss: 5208.11, average training loss: 8023.71, base loss: 10672.66
[INFO 2017-06-26 14:30:29,764 main.py:50] epoch 788, training loss: 5318.86, average training loss: 8020.28, base loss: 10672.66
[INFO 2017-06-26 14:30:32,657 main.py:50] epoch 789, training loss: 5373.52, average training loss: 8016.93, base loss: 10673.01
[INFO 2017-06-26 14:30:35,533 main.py:50] epoch 790, training loss: 5331.74, average training loss: 8013.54, base loss: 10672.63
[INFO 2017-06-26 14:30:38,446 main.py:50] epoch 791, training loss: 5363.84, average training loss: 8010.19, base loss: 10673.34
[INFO 2017-06-26 14:30:41,332 main.py:50] epoch 792, training loss: 5397.43, average training loss: 8006.90, base loss: 10674.02
[INFO 2017-06-26 14:30:44,212 main.py:50] epoch 793, training loss: 5214.50, average training loss: 8003.38, base loss: 10673.53
[INFO 2017-06-26 14:30:47,104 main.py:50] epoch 794, training loss: 5256.95, average training loss: 7999.92, base loss: 10673.42
[INFO 2017-06-26 14:30:50,013 main.py:50] epoch 795, training loss: 5275.51, average training loss: 7996.50, base loss: 10673.56
[INFO 2017-06-26 14:30:52,915 main.py:50] epoch 796, training loss: 5380.76, average training loss: 7993.22, base loss: 10673.99
[INFO 2017-06-26 14:30:55,802 main.py:50] epoch 797, training loss: 5351.83, average training loss: 7989.91, base loss: 10674.07
[INFO 2017-06-26 14:30:58,692 main.py:50] epoch 798, training loss: 5282.05, average training loss: 7986.52, base loss: 10674.51
[INFO 2017-06-26 14:31:01,578 main.py:50] epoch 799, training loss: 5098.92, average training loss: 7982.91, base loss: 10673.61
[INFO 2017-06-26 14:31:01,578 main.py:52] epoch 799, testing
[INFO 2017-06-26 14:31:14,797 main.py:103] average testing loss: 5351.68, base loss: 10837.93
[INFO 2017-06-26 14:31:14,797 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 14:31:14,804 main.py:76] current best accuracy: 5351.68
[INFO 2017-06-26 14:31:17,710 main.py:50] epoch 800, training loss: 5282.46, average training loss: 7979.54, base loss: 10673.56
[INFO 2017-06-26 14:31:20,599 main.py:50] epoch 801, training loss: 5363.37, average training loss: 7976.28, base loss: 10673.79
[INFO 2017-06-26 14:31:23,470 main.py:50] epoch 802, training loss: 5306.88, average training loss: 7972.95, base loss: 10674.08
[INFO 2017-06-26 14:31:26,375 main.py:50] epoch 803, training loss: 5342.20, average training loss: 7969.68, base loss: 10674.12
[INFO 2017-06-26 14:31:29,257 main.py:50] epoch 804, training loss: 5322.24, average training loss: 7966.39, base loss: 10674.07
[INFO 2017-06-26 14:31:32,140 main.py:50] epoch 805, training loss: 5252.85, average training loss: 7963.03, base loss: 10674.13
[INFO 2017-06-26 14:31:35,051 main.py:50] epoch 806, training loss: 5243.13, average training loss: 7959.66, base loss: 10673.58
[INFO 2017-06-26 14:31:37,927 main.py:50] epoch 807, training loss: 5192.09, average training loss: 7956.23, base loss: 10672.82
[INFO 2017-06-26 14:31:40,847 main.py:50] epoch 808, training loss: 5345.54, average training loss: 7953.00, base loss: 10673.21
[INFO 2017-06-26 14:31:43,738 main.py:50] epoch 809, training loss: 5272.75, average training loss: 7949.69, base loss: 10673.32
[INFO 2017-06-26 14:31:46,626 main.py:50] epoch 810, training loss: 5197.32, average training loss: 7946.30, base loss: 10672.68
[INFO 2017-06-26 14:31:49,510 main.py:50] epoch 811, training loss: 5522.82, average training loss: 7943.32, base loss: 10673.33
[INFO 2017-06-26 14:31:52,408 main.py:50] epoch 812, training loss: 5280.79, average training loss: 7940.04, base loss: 10673.42
[INFO 2017-06-26 14:31:55,298 main.py:50] epoch 813, training loss: 5358.44, average training loss: 7936.87, base loss: 10673.24
[INFO 2017-06-26 14:31:58,175 main.py:50] epoch 814, training loss: 5321.44, average training loss: 7933.66, base loss: 10673.17
[INFO 2017-06-26 14:32:01,066 main.py:50] epoch 815, training loss: 5436.04, average training loss: 7930.60, base loss: 10673.69
[INFO 2017-06-26 14:32:03,950 main.py:50] epoch 816, training loss: 5243.98, average training loss: 7927.31, base loss: 10673.60
[INFO 2017-06-26 14:32:06,831 main.py:50] epoch 817, training loss: 5188.79, average training loss: 7923.96, base loss: 10672.73
[INFO 2017-06-26 14:32:09,712 main.py:50] epoch 818, training loss: 5274.20, average training loss: 7920.73, base loss: 10672.55
[INFO 2017-06-26 14:32:12,603 main.py:50] epoch 819, training loss: 5294.42, average training loss: 7917.52, base loss: 10672.52
[INFO 2017-06-26 14:32:15,500 main.py:50] epoch 820, training loss: 5184.86, average training loss: 7914.20, base loss: 10672.03
[INFO 2017-06-26 14:32:18,412 main.py:50] epoch 821, training loss: 5209.25, average training loss: 7910.91, base loss: 10671.80
[INFO 2017-06-26 14:32:21,293 main.py:50] epoch 822, training loss: 5212.65, average training loss: 7907.63, base loss: 10671.61
[INFO 2017-06-26 14:32:24,186 main.py:50] epoch 823, training loss: 5151.12, average training loss: 7904.28, base loss: 10671.08
[INFO 2017-06-26 14:32:27,078 main.py:50] epoch 824, training loss: 5202.73, average training loss: 7901.01, base loss: 10670.69
[INFO 2017-06-26 14:32:29,987 main.py:50] epoch 825, training loss: 5202.21, average training loss: 7897.74, base loss: 10670.64
[INFO 2017-06-26 14:32:32,873 main.py:50] epoch 826, training loss: 5123.90, average training loss: 7894.39, base loss: 10670.39
[INFO 2017-06-26 14:32:35,764 main.py:50] epoch 827, training loss: 5156.22, average training loss: 7891.08, base loss: 10670.21
[INFO 2017-06-26 14:32:38,638 main.py:50] epoch 828, training loss: 5114.65, average training loss: 7887.73, base loss: 10669.64
[INFO 2017-06-26 14:32:41,526 main.py:50] epoch 829, training loss: 5293.52, average training loss: 7884.60, base loss: 10669.56
[INFO 2017-06-26 14:32:44,421 main.py:50] epoch 830, training loss: 5200.71, average training loss: 7881.37, base loss: 10669.51
[INFO 2017-06-26 14:32:47,315 main.py:50] epoch 831, training loss: 5274.77, average training loss: 7878.24, base loss: 10669.67
[INFO 2017-06-26 14:32:50,204 main.py:50] epoch 832, training loss: 5294.27, average training loss: 7875.14, base loss: 10669.98
[INFO 2017-06-26 14:32:53,101 main.py:50] epoch 833, training loss: 5251.27, average training loss: 7871.99, base loss: 10670.18
[INFO 2017-06-26 14:32:56,000 main.py:50] epoch 834, training loss: 5228.52, average training loss: 7868.83, base loss: 10670.36
[INFO 2017-06-26 14:32:58,885 main.py:50] epoch 835, training loss: 5179.06, average training loss: 7865.61, base loss: 10669.91
[INFO 2017-06-26 14:33:01,778 main.py:50] epoch 836, training loss: 5105.75, average training loss: 7862.31, base loss: 10669.79
[INFO 2017-06-26 14:33:04,670 main.py:50] epoch 837, training loss: 5203.97, average training loss: 7859.14, base loss: 10669.82
[INFO 2017-06-26 14:33:07,566 main.py:50] epoch 838, training loss: 5199.18, average training loss: 7855.97, base loss: 10669.38
[INFO 2017-06-26 14:33:10,471 main.py:50] epoch 839, training loss: 5289.65, average training loss: 7852.92, base loss: 10669.72
[INFO 2017-06-26 14:33:13,356 main.py:50] epoch 840, training loss: 5329.61, average training loss: 7849.91, base loss: 10670.01
[INFO 2017-06-26 14:33:16,276 main.py:50] epoch 841, training loss: 5242.16, average training loss: 7846.82, base loss: 10669.70
[INFO 2017-06-26 14:33:19,202 main.py:50] epoch 842, training loss: 5370.26, average training loss: 7843.88, base loss: 10670.22
[INFO 2017-06-26 14:33:22,088 main.py:50] epoch 843, training loss: 5164.80, average training loss: 7840.71, base loss: 10669.76
[INFO 2017-06-26 14:33:24,983 main.py:50] epoch 844, training loss: 5141.38, average training loss: 7837.51, base loss: 10669.33
[INFO 2017-06-26 14:33:27,875 main.py:50] epoch 845, training loss: 5241.84, average training loss: 7834.44, base loss: 10669.47
[INFO 2017-06-26 14:33:30,800 main.py:50] epoch 846, training loss: 5263.30, average training loss: 7831.41, base loss: 10669.40
[INFO 2017-06-26 14:33:33,680 main.py:50] epoch 847, training loss: 5377.48, average training loss: 7828.51, base loss: 10669.80
[INFO 2017-06-26 14:33:36,581 main.py:50] epoch 848, training loss: 5260.40, average training loss: 7825.49, base loss: 10669.79
[INFO 2017-06-26 14:33:39,465 main.py:50] epoch 849, training loss: 5171.74, average training loss: 7822.37, base loss: 10669.55
[INFO 2017-06-26 14:33:42,371 main.py:50] epoch 850, training loss: 5349.00, average training loss: 7819.46, base loss: 10669.92
[INFO 2017-06-26 14:33:45,280 main.py:50] epoch 851, training loss: 5308.81, average training loss: 7816.51, base loss: 10670.44
[INFO 2017-06-26 14:33:48,165 main.py:50] epoch 852, training loss: 5266.33, average training loss: 7813.52, base loss: 10670.34
[INFO 2017-06-26 14:33:51,045 main.py:50] epoch 853, training loss: 5315.07, average training loss: 7810.60, base loss: 10670.14
[INFO 2017-06-26 14:33:53,926 main.py:50] epoch 854, training loss: 5267.12, average training loss: 7807.62, base loss: 10670.08
[INFO 2017-06-26 14:33:56,809 main.py:50] epoch 855, training loss: 5328.79, average training loss: 7804.73, base loss: 10670.33
[INFO 2017-06-26 14:33:59,690 main.py:50] epoch 856, training loss: 5263.67, average training loss: 7801.76, base loss: 10670.63
[INFO 2017-06-26 14:34:02,608 main.py:50] epoch 857, training loss: 5216.65, average training loss: 7798.75, base loss: 10670.53
[INFO 2017-06-26 14:34:05,496 main.py:50] epoch 858, training loss: 5265.92, average training loss: 7795.80, base loss: 10670.72
[INFO 2017-06-26 14:34:08,378 main.py:50] epoch 859, training loss: 5232.64, average training loss: 7792.82, base loss: 10670.64
[INFO 2017-06-26 14:34:11,266 main.py:50] epoch 860, training loss: 5372.27, average training loss: 7790.01, base loss: 10671.05
[INFO 2017-06-26 14:34:14,143 main.py:50] epoch 861, training loss: 5158.06, average training loss: 7786.96, base loss: 10670.92
[INFO 2017-06-26 14:34:17,039 main.py:50] epoch 862, training loss: 5299.26, average training loss: 7784.07, base loss: 10671.31
[INFO 2017-06-26 14:34:19,920 main.py:50] epoch 863, training loss: 5256.02, average training loss: 7781.15, base loss: 10671.52
[INFO 2017-06-26 14:34:22,796 main.py:50] epoch 864, training loss: 5290.24, average training loss: 7778.27, base loss: 10672.04
[INFO 2017-06-26 14:34:25,681 main.py:50] epoch 865, training loss: 5227.68, average training loss: 7775.32, base loss: 10671.90
[INFO 2017-06-26 14:34:28,597 main.py:50] epoch 866, training loss: 5186.82, average training loss: 7772.34, base loss: 10671.72
[INFO 2017-06-26 14:34:31,487 main.py:50] epoch 867, training loss: 5281.31, average training loss: 7769.47, base loss: 10672.38
[INFO 2017-06-26 14:34:34,392 main.py:50] epoch 868, training loss: 5320.67, average training loss: 7766.65, base loss: 10672.63
[INFO 2017-06-26 14:34:37,275 main.py:50] epoch 869, training loss: 5303.30, average training loss: 7763.82, base loss: 10673.14
[INFO 2017-06-26 14:34:40,173 main.py:50] epoch 870, training loss: 5499.25, average training loss: 7761.22, base loss: 10674.36
[INFO 2017-06-26 14:34:43,074 main.py:50] epoch 871, training loss: 5249.47, average training loss: 7758.34, base loss: 10674.55
[INFO 2017-06-26 14:34:45,957 main.py:50] epoch 872, training loss: 5170.71, average training loss: 7755.37, base loss: 10674.41
[INFO 2017-06-26 14:34:48,836 main.py:50] epoch 873, training loss: 5291.48, average training loss: 7752.55, base loss: 10674.34
[INFO 2017-06-26 14:34:51,721 main.py:50] epoch 874, training loss: 5519.37, average training loss: 7750.00, base loss: 10675.18
[INFO 2017-06-26 14:34:54,605 main.py:50] epoch 875, training loss: 5298.00, average training loss: 7747.20, base loss: 10674.91
[INFO 2017-06-26 14:34:57,494 main.py:50] epoch 876, training loss: 5094.37, average training loss: 7744.18, base loss: 10674.72
[INFO 2017-06-26 14:35:00,417 main.py:50] epoch 877, training loss: 5316.96, average training loss: 7741.41, base loss: 10674.91
[INFO 2017-06-26 14:35:03,298 main.py:50] epoch 878, training loss: 5229.89, average training loss: 7738.56, base loss: 10674.81
[INFO 2017-06-26 14:35:06,174 main.py:50] epoch 879, training loss: 5273.42, average training loss: 7735.75, base loss: 10674.70
[INFO 2017-06-26 14:35:09,054 main.py:50] epoch 880, training loss: 5242.45, average training loss: 7732.92, base loss: 10675.10
[INFO 2017-06-26 14:35:11,955 main.py:50] epoch 881, training loss: 5247.49, average training loss: 7730.11, base loss: 10675.50
[INFO 2017-06-26 14:35:14,868 main.py:50] epoch 882, training loss: 5270.71, average training loss: 7727.32, base loss: 10675.79
[INFO 2017-06-26 14:35:17,747 main.py:50] epoch 883, training loss: 5237.67, average training loss: 7724.51, base loss: 10676.21
[INFO 2017-06-26 14:35:20,632 main.py:50] epoch 884, training loss: 5291.33, average training loss: 7721.76, base loss: 10676.41
[INFO 2017-06-26 14:35:23,524 main.py:50] epoch 885, training loss: 5240.01, average training loss: 7718.95, base loss: 10676.34
[INFO 2017-06-26 14:35:26,404 main.py:50] epoch 886, training loss: 5326.97, average training loss: 7716.26, base loss: 10676.91
[INFO 2017-06-26 14:35:29,303 main.py:50] epoch 887, training loss: 5166.20, average training loss: 7713.39, base loss: 10676.77
[INFO 2017-06-26 14:35:32,189 main.py:50] epoch 888, training loss: 5208.23, average training loss: 7710.57, base loss: 10677.08
[INFO 2017-06-26 14:35:35,063 main.py:50] epoch 889, training loss: 5120.06, average training loss: 7707.66, base loss: 10677.11
[INFO 2017-06-26 14:35:37,951 main.py:50] epoch 890, training loss: 5144.46, average training loss: 7704.78, base loss: 10676.70
[INFO 2017-06-26 14:35:40,836 main.py:50] epoch 891, training loss: 5015.43, average training loss: 7701.77, base loss: 10676.39
[INFO 2017-06-26 14:35:43,711 main.py:50] epoch 892, training loss: 5403.64, average training loss: 7699.19, base loss: 10677.25
[INFO 2017-06-26 14:35:46,593 main.py:50] epoch 893, training loss: 5071.82, average training loss: 7696.25, base loss: 10676.73
[INFO 2017-06-26 14:35:49,490 main.py:50] epoch 894, training loss: 5220.73, average training loss: 7693.49, base loss: 10676.90
[INFO 2017-06-26 14:35:52,367 main.py:50] epoch 895, training loss: 5228.47, average training loss: 7690.74, base loss: 10676.89
[INFO 2017-06-26 14:35:55,253 main.py:50] epoch 896, training loss: 5143.31, average training loss: 7687.90, base loss: 10676.56
[INFO 2017-06-26 14:35:58,156 main.py:50] epoch 897, training loss: 5177.01, average training loss: 7685.10, base loss: 10676.72
[INFO 2017-06-26 14:36:01,032 main.py:50] epoch 898, training loss: 5175.97, average training loss: 7682.31, base loss: 10677.05
[INFO 2017-06-26 14:36:03,924 main.py:50] epoch 899, training loss: 5143.37, average training loss: 7679.49, base loss: 10677.38
[INFO 2017-06-26 14:36:03,924 main.py:52] epoch 899, testing
[INFO 2017-06-26 14:36:17,300 main.py:103] average testing loss: 5140.46, base loss: 10571.86
[INFO 2017-06-26 14:36:17,301 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 14:36:17,308 main.py:76] current best accuracy: 5140.46
[INFO 2017-06-26 14:36:20,190 main.py:50] epoch 900, training loss: 5066.64, average training loss: 7676.59, base loss: 10677.05
[INFO 2017-06-26 14:36:23,072 main.py:50] epoch 901, training loss: 5127.65, average training loss: 7673.76, base loss: 10676.67
[INFO 2017-06-26 14:36:25,983 main.py:50] epoch 902, training loss: 5336.89, average training loss: 7671.17, base loss: 10677.09
[INFO 2017-06-26 14:36:28,856 main.py:50] epoch 903, training loss: 5309.08, average training loss: 7668.56, base loss: 10677.87
[INFO 2017-06-26 14:36:31,740 main.py:50] epoch 904, training loss: 5276.33, average training loss: 7665.92, base loss: 10677.91
[INFO 2017-06-26 14:36:34,624 main.py:50] epoch 905, training loss: 5185.10, average training loss: 7663.18, base loss: 10678.13
[INFO 2017-06-26 14:36:37,536 main.py:50] epoch 906, training loss: 5206.29, average training loss: 7660.47, base loss: 10678.20
[INFO 2017-06-26 14:36:40,425 main.py:50] epoch 907, training loss: 5155.18, average training loss: 7657.71, base loss: 10678.22
[INFO 2017-06-26 14:36:43,321 main.py:50] epoch 908, training loss: 5253.08, average training loss: 7655.07, base loss: 10678.69
[INFO 2017-06-26 14:36:46,201 main.py:50] epoch 909, training loss: 5245.64, average training loss: 7652.42, base loss: 10678.92
[INFO 2017-06-26 14:36:49,079 main.py:50] epoch 910, training loss: 5116.96, average training loss: 7649.64, base loss: 10678.96
[INFO 2017-06-26 14:36:51,958 main.py:50] epoch 911, training loss: 5231.08, average training loss: 7646.98, base loss: 10679.29
[INFO 2017-06-26 14:36:54,839 main.py:50] epoch 912, training loss: 5194.65, average training loss: 7644.30, base loss: 10679.72
[INFO 2017-06-26 14:36:57,725 main.py:50] epoch 913, training loss: 5161.02, average training loss: 7641.58, base loss: 10680.11
[INFO 2017-06-26 14:37:00,608 main.py:50] epoch 914, training loss: 5084.05, average training loss: 7638.79, base loss: 10679.89
[INFO 2017-06-26 14:37:03,521 main.py:50] epoch 915, training loss: 5048.74, average training loss: 7635.96, base loss: 10679.57
[INFO 2017-06-26 14:37:06,408 main.py:50] epoch 916, training loss: 5109.58, average training loss: 7633.20, base loss: 10679.46
[INFO 2017-06-26 14:37:09,294 main.py:50] epoch 917, training loss: 5185.81, average training loss: 7630.54, base loss: 10679.49
[INFO 2017-06-26 14:37:12,177 main.py:50] epoch 918, training loss: 5108.62, average training loss: 7627.79, base loss: 10679.40
[INFO 2017-06-26 14:37:15,079 main.py:50] epoch 919, training loss: 5262.07, average training loss: 7625.22, base loss: 10679.80
[INFO 2017-06-26 14:37:17,964 main.py:50] epoch 920, training loss: 5097.33, average training loss: 7622.48, base loss: 10679.47
[INFO 2017-06-26 14:37:20,864 main.py:50] epoch 921, training loss: 5115.31, average training loss: 7619.76, base loss: 10679.24
[INFO 2017-06-26 14:37:23,769 main.py:50] epoch 922, training loss: 5002.29, average training loss: 7616.92, base loss: 10678.79
[INFO 2017-06-26 14:37:26,650 main.py:50] epoch 923, training loss: 5147.44, average training loss: 7614.25, base loss: 10678.77
[INFO 2017-06-26 14:37:29,542 main.py:50] epoch 924, training loss: 5009.37, average training loss: 7611.43, base loss: 10678.59
[INFO 2017-06-26 14:37:32,448 main.py:50] epoch 925, training loss: 5126.71, average training loss: 7608.75, base loss: 10678.51
[INFO 2017-06-26 14:37:35,330 main.py:50] epoch 926, training loss: 5059.09, average training loss: 7606.00, base loss: 10678.42
[INFO 2017-06-26 14:37:38,249 main.py:50] epoch 927, training loss: 5230.76, average training loss: 7603.44, base loss: 10678.66
[INFO 2017-06-26 14:37:41,132 main.py:50] epoch 928, training loss: 5109.30, average training loss: 7600.76, base loss: 10678.51
[INFO 2017-06-26 14:37:44,016 main.py:50] epoch 929, training loss: 4920.25, average training loss: 7597.87, base loss: 10677.77
[INFO 2017-06-26 14:37:46,890 main.py:50] epoch 930, training loss: 5226.25, average training loss: 7595.33, base loss: 10678.24
[INFO 2017-06-26 14:37:49,776 main.py:50] epoch 931, training loss: 5179.75, average training loss: 7592.73, base loss: 10678.78
[INFO 2017-06-26 14:37:52,691 main.py:50] epoch 932, training loss: 5285.14, average training loss: 7590.26, base loss: 10679.53
[INFO 2017-06-26 14:37:55,617 main.py:50] epoch 933, training loss: 5148.53, average training loss: 7587.65, base loss: 10679.15
[INFO 2017-06-26 14:37:58,513 main.py:50] epoch 934, training loss: 5168.11, average training loss: 7585.06, base loss: 10679.19
[INFO 2017-06-26 14:38:01,419 main.py:50] epoch 935, training loss: 5138.89, average training loss: 7582.45, base loss: 10678.98
[INFO 2017-06-26 14:38:04,306 main.py:50] epoch 936, training loss: 5210.92, average training loss: 7579.91, base loss: 10679.08
[INFO 2017-06-26 14:38:07,180 main.py:50] epoch 937, training loss: 5043.44, average training loss: 7577.21, base loss: 10678.88
[INFO 2017-06-26 14:38:10,064 main.py:50] epoch 938, training loss: 5158.27, average training loss: 7574.63, base loss: 10678.57
[INFO 2017-06-26 14:38:12,953 main.py:50] epoch 939, training loss: 5260.89, average training loss: 7572.17, base loss: 10678.99
[INFO 2017-06-26 14:38:15,837 main.py:50] epoch 940, training loss: 5144.08, average training loss: 7569.59, base loss: 10679.34
[INFO 2017-06-26 14:38:18,738 main.py:50] epoch 941, training loss: 5225.99, average training loss: 7567.10, base loss: 10679.04
[INFO 2017-06-26 14:38:21,630 main.py:50] epoch 942, training loss: 4937.44, average training loss: 7564.32, base loss: 10678.32
[INFO 2017-06-26 14:38:24,530 main.py:50] epoch 943, training loss: 5342.58, average training loss: 7561.96, base loss: 10678.56
[INFO 2017-06-26 14:38:27,424 main.py:50] epoch 944, training loss: 5079.17, average training loss: 7559.33, base loss: 10678.29
[INFO 2017-06-26 14:38:30,341 main.py:50] epoch 945, training loss: 5257.45, average training loss: 7556.90, base loss: 10678.48
[INFO 2017-06-26 14:38:33,249 main.py:50] epoch 946, training loss: 5213.98, average training loss: 7554.43, base loss: 10679.10
[INFO 2017-06-26 14:38:36,136 main.py:50] epoch 947, training loss: 5252.10, average training loss: 7552.00, base loss: 10679.37
[INFO 2017-06-26 14:38:39,013 main.py:50] epoch 948, training loss: 5203.65, average training loss: 7549.52, base loss: 10679.91
[INFO 2017-06-26 14:38:41,901 main.py:50] epoch 949, training loss: 5338.88, average training loss: 7547.20, base loss: 10680.43
[INFO 2017-06-26 14:38:44,780 main.py:50] epoch 950, training loss: 5122.77, average training loss: 7544.65, base loss: 10680.50
[INFO 2017-06-26 14:38:47,663 main.py:50] epoch 951, training loss: 5226.66, average training loss: 7542.21, base loss: 10680.71
[INFO 2017-06-26 14:38:50,582 main.py:50] epoch 952, training loss: 5125.27, average training loss: 7539.68, base loss: 10680.54
[INFO 2017-06-26 14:38:53,464 main.py:50] epoch 953, training loss: 5149.94, average training loss: 7537.17, base loss: 10680.66
[INFO 2017-06-26 14:38:56,356 main.py:50] epoch 954, training loss: 5178.72, average training loss: 7534.70, base loss: 10680.82
[INFO 2017-06-26 14:38:59,238 main.py:50] epoch 955, training loss: 5292.69, average training loss: 7532.36, base loss: 10681.47
[INFO 2017-06-26 14:39:02,123 main.py:50] epoch 956, training loss: 5087.70, average training loss: 7529.80, base loss: 10681.51
[INFO 2017-06-26 14:39:05,005 main.py:50] epoch 957, training loss: 5184.91, average training loss: 7527.35, base loss: 10681.79
[INFO 2017-06-26 14:39:07,896 main.py:50] epoch 958, training loss: 5184.70, average training loss: 7524.91, base loss: 10681.95
[INFO 2017-06-26 14:39:10,776 main.py:50] epoch 959, training loss: 5103.20, average training loss: 7522.39, base loss: 10681.95
[INFO 2017-06-26 14:39:13,656 main.py:50] epoch 960, training loss: 5026.12, average training loss: 7519.79, base loss: 10681.62
[INFO 2017-06-26 14:39:16,542 main.py:50] epoch 961, training loss: 5095.21, average training loss: 7517.27, base loss: 10681.73
[INFO 2017-06-26 14:39:19,434 main.py:50] epoch 962, training loss: 5027.79, average training loss: 7514.69, base loss: 10681.51
[INFO 2017-06-26 14:39:22,326 main.py:50] epoch 963, training loss: 5297.80, average training loss: 7512.39, base loss: 10682.29
[INFO 2017-06-26 14:39:25,218 main.py:50] epoch 964, training loss: 5048.91, average training loss: 7509.83, base loss: 10682.20
[INFO 2017-06-26 14:39:28,127 main.py:50] epoch 965, training loss: 5195.37, average training loss: 7507.44, base loss: 10682.09
[INFO 2017-06-26 14:39:31,010 main.py:50] epoch 966, training loss: 5106.29, average training loss: 7504.95, base loss: 10681.94
[INFO 2017-06-26 14:39:33,895 main.py:50] epoch 967, training loss: 5297.41, average training loss: 7502.67, base loss: 10682.43
[INFO 2017-06-26 14:39:36,797 main.py:50] epoch 968, training loss: 5219.21, average training loss: 7500.32, base loss: 10682.80
[INFO 2017-06-26 14:39:39,708 main.py:50] epoch 969, training loss: 5203.52, average training loss: 7497.95, base loss: 10683.20
[INFO 2017-06-26 14:39:42,588 main.py:50] epoch 970, training loss: 5207.01, average training loss: 7495.59, base loss: 10683.37
[INFO 2017-06-26 14:39:45,472 main.py:50] epoch 971, training loss: 5096.91, average training loss: 7493.12, base loss: 10682.93
[INFO 2017-06-26 14:39:48,359 main.py:50] epoch 972, training loss: 5152.52, average training loss: 7490.72, base loss: 10682.90
[INFO 2017-06-26 14:39:51,240 main.py:50] epoch 973, training loss: 5246.20, average training loss: 7488.41, base loss: 10683.15
[INFO 2017-06-26 14:39:54,132 main.py:50] epoch 974, training loss: 5211.34, average training loss: 7486.08, base loss: 10683.67
[INFO 2017-06-26 14:39:57,022 main.py:50] epoch 975, training loss: 5159.98, average training loss: 7483.69, base loss: 10683.94
[INFO 2017-06-26 14:39:59,917 main.py:50] epoch 976, training loss: 5057.78, average training loss: 7481.21, base loss: 10683.96
[INFO 2017-06-26 14:40:02,807 main.py:50] epoch 977, training loss: 5091.13, average training loss: 7478.77, base loss: 10684.15
[INFO 2017-06-26 14:40:05,707 main.py:50] epoch 978, training loss: 5042.37, average training loss: 7476.28, base loss: 10684.10
[INFO 2017-06-26 14:40:08,592 main.py:50] epoch 979, training loss: 5052.48, average training loss: 7473.81, base loss: 10684.09
[INFO 2017-06-26 14:40:11,484 main.py:50] epoch 980, training loss: 5038.97, average training loss: 7471.32, base loss: 10684.11
[INFO 2017-06-26 14:40:14,372 main.py:50] epoch 981, training loss: 5099.25, average training loss: 7468.91, base loss: 10684.51
[INFO 2017-06-26 14:40:17,256 main.py:50] epoch 982, training loss: 5169.66, average training loss: 7466.57, base loss: 10684.82
[INFO 2017-06-26 14:40:20,146 main.py:50] epoch 983, training loss: 5104.65, average training loss: 7464.17, base loss: 10685.01
[INFO 2017-06-26 14:40:23,038 main.py:50] epoch 984, training loss: 4909.27, average training loss: 7461.57, base loss: 10684.02
[INFO 2017-06-26 14:40:25,933 main.py:50] epoch 985, training loss: 5281.93, average training loss: 7459.36, base loss: 10684.44
[INFO 2017-06-26 14:40:28,820 main.py:50] epoch 986, training loss: 5006.50, average training loss: 7456.88, base loss: 10683.99
[INFO 2017-06-26 14:40:31,725 main.py:50] epoch 987, training loss: 5156.75, average training loss: 7454.55, base loss: 10684.47
[INFO 2017-06-26 14:40:34,629 main.py:50] epoch 988, training loss: 5227.83, average training loss: 7452.30, base loss: 10685.13
[INFO 2017-06-26 14:40:37,529 main.py:50] epoch 989, training loss: 5027.00, average training loss: 7449.85, base loss: 10684.89
[INFO 2017-06-26 14:40:40,440 main.py:50] epoch 990, training loss: 4965.28, average training loss: 7447.34, base loss: 10684.57
[INFO 2017-06-26 14:40:43,362 main.py:50] epoch 991, training loss: 5147.53, average training loss: 7445.02, base loss: 10684.70
[INFO 2017-06-26 14:40:46,280 main.py:50] epoch 992, training loss: 5126.40, average training loss: 7442.69, base loss: 10684.93
[INFO 2017-06-26 14:40:49,171 main.py:50] epoch 993, training loss: 5121.16, average training loss: 7440.35, base loss: 10684.41
[INFO 2017-06-26 14:40:52,060 main.py:50] epoch 994, training loss: 5175.30, average training loss: 7438.08, base loss: 10684.62
[INFO 2017-06-26 14:40:54,964 main.py:50] epoch 995, training loss: 5044.25, average training loss: 7435.67, base loss: 10684.62
[INFO 2017-06-26 14:40:57,859 main.py:50] epoch 996, training loss: 4989.35, average training loss: 7433.22, base loss: 10683.96
[INFO 2017-06-26 14:41:00,746 main.py:50] epoch 997, training loss: 5012.64, average training loss: 7430.79, base loss: 10683.78
[INFO 2017-06-26 14:41:03,652 main.py:50] epoch 998, training loss: 5176.20, average training loss: 7428.54, base loss: 10683.94
[INFO 2017-06-26 14:41:06,575 main.py:50] epoch 999, training loss: 4926.83, average training loss: 7426.04, base loss: 10683.57
[INFO 2017-06-26 14:41:06,576 main.py:52] epoch 999, testing
[INFO 2017-06-26 14:41:19,908 main.py:103] average testing loss: 5037.71, base loss: 10604.66
[INFO 2017-06-26 14:41:19,909 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 14:41:19,915 main.py:76] current best accuracy: 5037.71
[INFO 2017-06-26 14:41:22,822 main.py:50] epoch 1000, training loss: 5072.28, average training loss: 7283.96, base loss: 10683.29
[INFO 2017-06-26 14:41:25,731 main.py:50] epoch 1001, training loss: 5109.84, average training loss: 7165.94, base loss: 10682.99
[INFO 2017-06-26 14:41:28,618 main.py:50] epoch 1002, training loss: 5143.00, average training loss: 7066.01, base loss: 10683.48
[INFO 2017-06-26 14:41:31,508 main.py:50] epoch 1003, training loss: 4974.52, average training loss: 6979.08, base loss: 10683.48
[INFO 2017-06-26 14:41:34,405 main.py:50] epoch 1004, training loss: 4997.76, average training loss: 6904.58, base loss: 10683.45
[INFO 2017-06-26 14:41:37,303 main.py:50] epoch 1005, training loss: 5020.47, average training loss: 6840.55, base loss: 10683.38
[INFO 2017-06-26 14:41:40,200 main.py:50] epoch 1006, training loss: 5077.51, average training loss: 6785.72, base loss: 10683.35
[INFO 2017-06-26 14:41:43,089 main.py:50] epoch 1007, training loss: 4965.15, average training loss: 6738.55, base loss: 10683.07
[INFO 2017-06-26 14:41:45,990 main.py:50] epoch 1008, training loss: 4939.77, average training loss: 6697.69, base loss: 10682.45
[INFO 2017-06-26 14:41:48,891 main.py:50] epoch 1009, training loss: 4962.73, average training loss: 6662.41, base loss: 10681.62
[INFO 2017-06-26 14:41:51,787 main.py:50] epoch 1010, training loss: 5121.04, average training loss: 6632.09, base loss: 10681.76
[INFO 2017-06-26 14:41:54,664 main.py:50] epoch 1011, training loss: 5053.06, average training loss: 6605.02, base loss: 10681.46
[INFO 2017-06-26 14:41:57,551 main.py:50] epoch 1012, training loss: 5100.38, average training loss: 6580.84, base loss: 10681.50
[INFO 2017-06-26 14:42:00,477 main.py:50] epoch 1013, training loss: 4969.83, average training loss: 6559.19, base loss: 10681.51
[INFO 2017-06-26 14:42:03,379 main.py:50] epoch 1014, training loss: 5204.14, average training loss: 6539.58, base loss: 10682.13
[INFO 2017-06-26 14:42:06,281 main.py:50] epoch 1015, training loss: 5071.63, average training loss: 6521.62, base loss: 10682.22
[INFO 2017-06-26 14:42:09,171 main.py:50] epoch 1016, training loss: 5230.21, average training loss: 6505.58, base loss: 10683.15
[INFO 2017-06-26 14:42:12,051 main.py:50] epoch 1017, training loss: 5024.12, average training loss: 6490.48, base loss: 10683.04
[INFO 2017-06-26 14:42:14,934 main.py:50] epoch 1018, training loss: 4994.08, average training loss: 6476.50, base loss: 10682.13
[INFO 2017-06-26 14:42:17,821 main.py:50] epoch 1019, training loss: 5175.25, average training loss: 6463.99, base loss: 10682.45
[INFO 2017-06-26 14:42:20,743 main.py:50] epoch 1020, training loss: 5123.52, average training loss: 6452.01, base loss: 10682.41
[INFO 2017-06-26 14:42:23,625 main.py:50] epoch 1021, training loss: 5123.39, average training loss: 6439.96, base loss: 10681.83
[INFO 2017-06-26 14:42:26,556 main.py:50] epoch 1022, training loss: 5272.80, average training loss: 6429.41, base loss: 10682.94
[INFO 2017-06-26 14:42:29,444 main.py:50] epoch 1023, training loss: 5026.75, average training loss: 6418.75, base loss: 10682.15
[INFO 2017-06-26 14:42:32,352 main.py:50] epoch 1024, training loss: 5048.59, average training loss: 6409.52, base loss: 10683.11
[INFO 2017-06-26 14:42:35,232 main.py:50] epoch 1025, training loss: 5059.51, average training loss: 6400.49, base loss: 10683.15
[INFO 2017-06-26 14:42:38,117 main.py:50] epoch 1026, training loss: 4910.88, average training loss: 6391.28, base loss: 10682.32
[INFO 2017-06-26 14:42:41,020 main.py:50] epoch 1027, training loss: 5111.86, average training loss: 6382.89, base loss: 10682.74
[INFO 2017-06-26 14:42:43,950 main.py:50] epoch 1028, training loss: 5136.86, average training loss: 6374.69, base loss: 10682.80
[INFO 2017-06-26 14:42:46,837 main.py:50] epoch 1029, training loss: 5060.92, average training loss: 6367.09, base loss: 10682.95
[INFO 2017-06-26 14:42:49,722 main.py:50] epoch 1030, training loss: 5004.13, average training loss: 6359.51, base loss: 10682.50
[INFO 2017-06-26 14:42:52,646 main.py:50] epoch 1031, training loss: 4988.83, average training loss: 6351.95, base loss: 10681.76
[INFO 2017-06-26 14:42:55,574 main.py:50] epoch 1032, training loss: 4971.83, average training loss: 6345.24, base loss: 10681.84
[INFO 2017-06-26 14:42:58,469 main.py:50] epoch 1033, training loss: 4939.83, average training loss: 6338.17, base loss: 10680.95
[INFO 2017-06-26 14:43:01,388 main.py:50] epoch 1034, training loss: 4946.49, average training loss: 6331.84, base loss: 10680.71
[INFO 2017-06-26 14:43:04,296 main.py:50] epoch 1035, training loss: 4922.47, average training loss: 6325.66, base loss: 10680.66
[INFO 2017-06-26 14:43:07,200 main.py:50] epoch 1036, training loss: 4925.11, average training loss: 6319.24, base loss: 10680.17
[INFO 2017-06-26 14:43:10,091 main.py:50] epoch 1037, training loss: 5042.66, average training loss: 6313.22, base loss: 10680.09
[INFO 2017-06-26 14:43:13,017 main.py:50] epoch 1038, training loss: 5006.19, average training loss: 6307.08, base loss: 10679.73
[INFO 2017-06-26 14:43:15,914 main.py:50] epoch 1039, training loss: 4991.81, average training loss: 6301.20, base loss: 10679.43
[INFO 2017-06-26 14:43:18,806 main.py:50] epoch 1040, training loss: 5080.19, average training loss: 6295.84, base loss: 10679.67
[INFO 2017-06-26 14:43:21,735 main.py:50] epoch 1041, training loss: 4877.26, average training loss: 6290.54, base loss: 10679.26
[INFO 2017-06-26 14:43:24,617 main.py:50] epoch 1042, training loss: 4977.11, average training loss: 6285.04, base loss: 10679.07
[INFO 2017-06-26 14:43:27,519 main.py:50] epoch 1043, training loss: 5041.71, average training loss: 6279.73, base loss: 10679.20
[INFO 2017-06-26 14:43:30,404 main.py:50] epoch 1044, training loss: 5085.14, average training loss: 6274.34, base loss: 10679.48
[INFO 2017-06-26 14:43:33,326 main.py:50] epoch 1045, training loss: 5071.60, average training loss: 6269.40, base loss: 10679.69
[INFO 2017-06-26 14:43:36,220 main.py:50] epoch 1046, training loss: 5169.17, average training loss: 6264.69, base loss: 10680.54
[INFO 2017-06-26 14:43:39,131 main.py:50] epoch 1047, training loss: 5050.16, average training loss: 6259.22, base loss: 10680.22
[INFO 2017-06-26 14:43:42,026 main.py:50] epoch 1048, training loss: 4979.41, average training loss: 6254.05, base loss: 10680.03
[INFO 2017-06-26 14:43:44,950 main.py:50] epoch 1049, training loss: 4848.16, average training loss: 6248.44, base loss: 10679.19
[INFO 2017-06-26 14:43:47,848 main.py:50] epoch 1050, training loss: 4947.99, average training loss: 6242.91, base loss: 10678.43
[INFO 2017-06-26 14:43:50,755 main.py:50] epoch 1051, training loss: 4970.95, average training loss: 6237.53, base loss: 10677.73
[INFO 2017-06-26 14:43:53,669 main.py:50] epoch 1052, training loss: 4888.75, average training loss: 6232.35, base loss: 10677.31
[INFO 2017-06-26 14:43:56,581 main.py:50] epoch 1053, training loss: 5084.20, average training loss: 6227.95, base loss: 10678.39
[INFO 2017-06-26 14:43:59,472 main.py:50] epoch 1054, training loss: 5032.98, average training loss: 6223.15, base loss: 10678.36
[INFO 2017-06-26 14:44:02,372 main.py:50] epoch 1055, training loss: 4944.84, average training loss: 6217.74, base loss: 10677.35
[INFO 2017-06-26 14:44:05,249 main.py:50] epoch 1056, training loss: 5279.64, average training loss: 6213.26, base loss: 10678.01
[INFO 2017-06-26 14:44:08,157 main.py:50] epoch 1057, training loss: 4917.92, average training loss: 6208.23, base loss: 10677.60
[INFO 2017-06-26 14:44:11,064 main.py:50] epoch 1058, training loss: 5011.48, average training loss: 6203.30, base loss: 10677.78
[INFO 2017-06-26 14:44:13,948 main.py:50] epoch 1059, training loss: 4883.31, average training loss: 6198.34, base loss: 10677.08
[INFO 2017-06-26 14:44:16,854 main.py:50] epoch 1060, training loss: 4902.94, average training loss: 6193.67, base loss: 10676.92
[INFO 2017-06-26 14:44:19,792 main.py:50] epoch 1061, training loss: 5031.27, average training loss: 6188.89, base loss: 10676.36
[INFO 2017-06-26 14:44:22,694 main.py:50] epoch 1062, training loss: 4928.35, average training loss: 6184.17, base loss: 10676.21
[INFO 2017-06-26 14:44:25,596 main.py:50] epoch 1063, training loss: 5090.45, average training loss: 6179.90, base loss: 10676.25
[INFO 2017-06-26 14:44:28,486 main.py:50] epoch 1064, training loss: 4892.85, average training loss: 6175.26, base loss: 10675.87
[INFO 2017-06-26 14:44:31,384 main.py:50] epoch 1065, training loss: 5154.19, average training loss: 6171.07, base loss: 10676.18
[INFO 2017-06-26 14:44:34,285 main.py:50] epoch 1066, training loss: 4979.28, average training loss: 6166.35, base loss: 10675.79
[INFO 2017-06-26 14:44:37,196 main.py:50] epoch 1067, training loss: 5110.46, average training loss: 6162.08, base loss: 10676.25
[INFO 2017-06-26 14:44:40,101 main.py:50] epoch 1068, training loss: 4925.36, average training loss: 6157.32, base loss: 10675.75
[INFO 2017-06-26 14:44:43,003 main.py:50] epoch 1069, training loss: 4936.67, average training loss: 6152.83, base loss: 10675.70
[INFO 2017-06-26 14:44:45,908 main.py:50] epoch 1070, training loss: 4935.69, average training loss: 6148.36, base loss: 10675.52
[INFO 2017-06-26 14:44:48,797 main.py:50] epoch 1071, training loss: 5149.43, average training loss: 6144.02, base loss: 10675.98
[INFO 2017-06-26 14:44:51,697 main.py:50] epoch 1072, training loss: 4951.79, average training loss: 6139.24, base loss: 10675.01
[INFO 2017-06-26 14:44:54,602 main.py:50] epoch 1073, training loss: 5135.92, average training loss: 6134.77, base loss: 10675.14
[INFO 2017-06-26 14:44:57,506 main.py:50] epoch 1074, training loss: 4951.77, average training loss: 6130.42, base loss: 10675.50
[INFO 2017-06-26 14:45:00,431 main.py:50] epoch 1075, training loss: 4994.16, average training loss: 6125.76, base loss: 10675.02
[INFO 2017-06-26 14:45:03,340 main.py:50] epoch 1076, training loss: 4986.18, average training loss: 6121.82, base loss: 10675.59
[INFO 2017-06-26 14:45:06,249 main.py:50] epoch 1077, training loss: 4873.99, average training loss: 6117.78, base loss: 10675.43
[INFO 2017-06-26 14:45:09,165 main.py:50] epoch 1078, training loss: 5066.44, average training loss: 6113.35, base loss: 10675.40
[INFO 2017-06-26 14:45:12,078 main.py:50] epoch 1079, training loss: 5061.70, average training loss: 6108.79, base loss: 10675.49
[INFO 2017-06-26 14:45:14,990 main.py:50] epoch 1080, training loss: 5095.25, average training loss: 6104.60, base loss: 10676.12
[INFO 2017-06-26 14:45:17,879 main.py:50] epoch 1081, training loss: 5002.17, average training loss: 6100.17, base loss: 10675.87
[INFO 2017-06-26 14:45:20,791 main.py:50] epoch 1082, training loss: 4972.00, average training loss: 6096.36, base loss: 10676.24
[INFO 2017-06-26 14:45:23,724 main.py:50] epoch 1083, training loss: 4985.81, average training loss: 6092.20, base loss: 10676.14
[INFO 2017-06-26 14:45:26,610 main.py:50] epoch 1084, training loss: 4984.08, average training loss: 6088.04, base loss: 10676.24
[INFO 2017-06-26 14:45:29,503 main.py:50] epoch 1085, training loss: 4957.13, average training loss: 6084.05, base loss: 10676.02
[INFO 2017-06-26 14:45:32,432 main.py:50] epoch 1086, training loss: 5104.52, average training loss: 6079.81, base loss: 10676.23
[INFO 2017-06-26 14:45:35,331 main.py:50] epoch 1087, training loss: 5073.00, average training loss: 6075.25, base loss: 10675.84
[INFO 2017-06-26 14:45:38,247 main.py:50] epoch 1088, training loss: 5023.47, average training loss: 6071.06, base loss: 10676.21
[INFO 2017-06-26 14:45:41,147 main.py:50] epoch 1089, training loss: 4999.66, average training loss: 6066.86, base loss: 10676.19
[INFO 2017-06-26 14:45:44,032 main.py:50] epoch 1090, training loss: 4999.09, average training loss: 6062.61, base loss: 10676.14
[INFO 2017-06-26 14:45:46,942 main.py:50] epoch 1091, training loss: 4932.59, average training loss: 6058.36, base loss: 10676.05
[INFO 2017-06-26 14:45:49,852 main.py:50] epoch 1092, training loss: 5005.85, average training loss: 6054.13, base loss: 10676.17
[INFO 2017-06-26 14:45:52,749 main.py:50] epoch 1093, training loss: 5020.57, average training loss: 6050.39, base loss: 10676.84
[INFO 2017-06-26 14:45:55,662 main.py:50] epoch 1094, training loss: 5112.57, average training loss: 6046.81, base loss: 10677.73
[INFO 2017-06-26 14:45:58,575 main.py:50] epoch 1095, training loss: 4943.61, average training loss: 6042.73, base loss: 10677.77
[INFO 2017-06-26 14:46:01,456 main.py:50] epoch 1096, training loss: 5094.17, average training loss: 6038.90, base loss: 10678.09
[INFO 2017-06-26 14:46:04,367 main.py:50] epoch 1097, training loss: 4944.80, average training loss: 6034.77, base loss: 10677.79
[INFO 2017-06-26 14:46:07,266 main.py:50] epoch 1098, training loss: 5027.85, average training loss: 6030.75, base loss: 10677.66
[INFO 2017-06-26 14:46:10,176 main.py:50] epoch 1099, training loss: 4912.47, average training loss: 6026.69, base loss: 10677.27
[INFO 2017-06-26 14:46:10,176 main.py:52] epoch 1099, testing
[INFO 2017-06-26 14:46:23,620 main.py:103] average testing loss: 4953.37, base loss: 10690.98
[INFO 2017-06-26 14:46:23,620 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 14:46:23,627 main.py:76] current best accuracy: 4953.37
[INFO 2017-06-26 14:46:26,537 main.py:50] epoch 1100, training loss: 4935.43, average training loss: 6022.56, base loss: 10676.90
[INFO 2017-06-26 14:46:29,437 main.py:50] epoch 1101, training loss: 4970.82, average training loss: 6018.40, base loss: 10676.75
[INFO 2017-06-26 14:46:32,335 main.py:50] epoch 1102, training loss: 4947.92, average training loss: 6014.77, base loss: 10677.17
[INFO 2017-06-26 14:46:35,226 main.py:50] epoch 1103, training loss: 4970.54, average training loss: 6010.58, base loss: 10676.64
[INFO 2017-06-26 14:46:38,126 main.py:50] epoch 1104, training loss: 5134.35, average training loss: 6006.64, base loss: 10677.00
[INFO 2017-06-26 14:46:41,028 main.py:50] epoch 1105, training loss: 5042.57, average training loss: 6003.03, base loss: 10677.42
[INFO 2017-06-26 14:46:43,945 main.py:50] epoch 1106, training loss: 4897.81, average training loss: 5999.02, base loss: 10676.83
[INFO 2017-06-26 14:46:46,846 main.py:50] epoch 1107, training loss: 5034.63, average training loss: 5995.19, base loss: 10676.94
[INFO 2017-06-26 14:46:49,766 main.py:50] epoch 1108, training loss: 5023.21, average training loss: 5991.34, base loss: 10677.00
[INFO 2017-06-26 14:46:52,676 main.py:50] epoch 1109, training loss: 5121.42, average training loss: 5987.80, base loss: 10677.46
[INFO 2017-06-26 14:46:55,584 main.py:50] epoch 1110, training loss: 5007.78, average training loss: 5983.88, base loss: 10677.64
[INFO 2017-06-26 14:46:58,467 main.py:50] epoch 1111, training loss: 5118.59, average training loss: 5979.95, base loss: 10677.78
[INFO 2017-06-26 14:47:01,372 main.py:50] epoch 1112, training loss: 4839.71, average training loss: 5976.39, base loss: 10677.75
[INFO 2017-06-26 14:47:04,292 main.py:50] epoch 1113, training loss: 5077.07, average training loss: 5972.93, base loss: 10678.13
[INFO 2017-06-26 14:47:07,206 main.py:50] epoch 1114, training loss: 4933.72, average training loss: 5969.35, base loss: 10678.57
[INFO 2017-06-26 14:47:10,118 main.py:50] epoch 1115, training loss: 5002.45, average training loss: 5965.54, base loss: 10678.66
[INFO 2017-06-26 14:47:13,022 main.py:50] epoch 1116, training loss: 4856.62, average training loss: 5962.01, base loss: 10678.97
[INFO 2017-06-26 14:47:15,916 main.py:50] epoch 1117, training loss: 4986.64, average training loss: 5958.46, base loss: 10679.33
[INFO 2017-06-26 14:47:18,812 main.py:50] epoch 1118, training loss: 4940.39, average training loss: 5955.01, base loss: 10680.06
[INFO 2017-06-26 14:47:21,715 main.py:50] epoch 1119, training loss: 4954.82, average training loss: 5951.09, base loss: 10679.89
[INFO 2017-06-26 14:47:24,601 main.py:50] epoch 1120, training loss: 4956.06, average training loss: 5947.28, base loss: 10679.60
[INFO 2017-06-26 14:47:27,496 main.py:50] epoch 1121, training loss: 5010.88, average training loss: 5943.71, base loss: 10680.08
[INFO 2017-06-26 14:47:30,379 main.py:50] epoch 1122, training loss: 4849.10, average training loss: 5939.96, base loss: 10679.33
[INFO 2017-06-26 14:47:33,267 main.py:50] epoch 1123, training loss: 4960.06, average training loss: 5936.21, base loss: 10679.28
[INFO 2017-06-26 14:47:36,172 main.py:50] epoch 1124, training loss: 4972.68, average training loss: 5932.78, base loss: 10680.08
[INFO 2017-06-26 14:47:39,069 main.py:50] epoch 1125, training loss: 5102.01, average training loss: 5929.48, base loss: 10680.47
[INFO 2017-06-26 14:47:41,956 main.py:50] epoch 1126, training loss: 5037.04, average training loss: 5925.94, base loss: 10680.81
[INFO 2017-06-26 14:47:44,880 main.py:50] epoch 1127, training loss: 4920.44, average training loss: 5922.46, base loss: 10680.77
[INFO 2017-06-26 14:47:47,774 main.py:50] epoch 1128, training loss: 4970.70, average training loss: 5918.53, base loss: 10680.15
[INFO 2017-06-26 14:47:50,677 main.py:50] epoch 1129, training loss: 5063.76, average training loss: 5914.91, base loss: 10680.68
[INFO 2017-06-26 14:47:53,572 main.py:50] epoch 1130, training loss: 4961.10, average training loss: 5911.38, base loss: 10680.04
[INFO 2017-06-26 14:47:56,461 main.py:50] epoch 1131, training loss: 4914.19, average training loss: 5908.06, base loss: 10680.33
[INFO 2017-06-26 14:47:59,348 main.py:50] epoch 1132, training loss: 4967.88, average training loss: 5904.81, base loss: 10680.35
[INFO 2017-06-26 14:48:02,252 main.py:50] epoch 1133, training loss: 5034.24, average training loss: 5901.50, base loss: 10680.41
[INFO 2017-06-26 14:48:05,154 main.py:50] epoch 1134, training loss: 4969.05, average training loss: 5898.26, base loss: 10680.75
[INFO 2017-06-26 14:48:08,050 main.py:50] epoch 1135, training loss: 4930.36, average training loss: 5894.62, base loss: 10680.46
[INFO 2017-06-26 14:48:10,953 main.py:50] epoch 1136, training loss: 4949.49, average training loss: 5891.36, base loss: 10680.24
[INFO 2017-06-26 14:48:13,860 main.py:50] epoch 1137, training loss: 4959.88, average training loss: 5887.99, base loss: 10680.28
[INFO 2017-06-26 14:48:16,771 main.py:50] epoch 1138, training loss: 4779.74, average training loss: 5884.64, base loss: 10679.84
[INFO 2017-06-26 14:48:19,664 main.py:50] epoch 1139, training loss: 4900.75, average training loss: 5881.12, base loss: 10679.28
[INFO 2017-06-26 14:48:22,567 main.py:50] epoch 1140, training loss: 5051.61, average training loss: 5877.80, base loss: 10679.74
[INFO 2017-06-26 14:48:25,457 main.py:50] epoch 1141, training loss: 4877.40, average training loss: 5874.20, base loss: 10679.07
[INFO 2017-06-26 14:48:28,360 main.py:50] epoch 1142, training loss: 5076.27, average training loss: 5870.68, base loss: 10678.87
[INFO 2017-06-26 14:48:31,278 main.py:50] epoch 1143, training loss: 4993.12, average training loss: 5867.36, base loss: 10679.13
[INFO 2017-06-26 14:48:34,212 main.py:50] epoch 1144, training loss: 4952.62, average training loss: 5864.33, base loss: 10679.77
[INFO 2017-06-26 14:48:37,112 main.py:50] epoch 1145, training loss: 4925.26, average training loss: 5860.80, base loss: 10679.61
[INFO 2017-06-26 14:48:40,033 main.py:50] epoch 1146, training loss: 4925.78, average training loss: 5857.46, base loss: 10679.56
[INFO 2017-06-26 14:48:42,930 main.py:50] epoch 1147, training loss: 5041.51, average training loss: 5854.27, base loss: 10679.94
[INFO 2017-06-26 14:48:45,818 main.py:50] epoch 1148, training loss: 4952.58, average training loss: 5850.73, base loss: 10679.62
[INFO 2017-06-26 14:48:48,706 main.py:50] epoch 1149, training loss: 4969.59, average training loss: 5847.45, base loss: 10679.97
[INFO 2017-06-26 14:48:51,623 main.py:50] epoch 1150, training loss: 4908.71, average training loss: 5843.97, base loss: 10679.76
[INFO 2017-06-26 14:48:54,513 main.py:50] epoch 1151, training loss: 4868.00, average training loss: 5840.80, base loss: 10679.52
[INFO 2017-06-26 14:48:57,405 main.py:50] epoch 1152, training loss: 4883.80, average training loss: 5837.74, base loss: 10679.62
[INFO 2017-06-26 14:49:00,326 main.py:50] epoch 1153, training loss: 5013.51, average training loss: 5834.66, base loss: 10680.21
[INFO 2017-06-26 14:49:03,245 main.py:50] epoch 1154, training loss: 5099.78, average training loss: 5831.43, base loss: 10680.06
[INFO 2017-06-26 14:49:06,150 main.py:50] epoch 1155, training loss: 5095.81, average training loss: 5828.33, base loss: 10680.67
[INFO 2017-06-26 14:49:09,056 main.py:50] epoch 1156, training loss: 4921.80, average training loss: 5824.95, base loss: 10680.51
[INFO 2017-06-26 14:49:11,986 main.py:50] epoch 1157, training loss: 4907.23, average training loss: 5821.54, base loss: 10680.06
[INFO 2017-06-26 14:49:14,887 main.py:50] epoch 1158, training loss: 4875.86, average training loss: 5818.15, base loss: 10679.34
[INFO 2017-06-26 14:49:17,768 main.py:50] epoch 1159, training loss: 4854.52, average training loss: 5814.83, base loss: 10679.12
[INFO 2017-06-26 14:49:20,660 main.py:50] epoch 1160, training loss: 4878.65, average training loss: 5812.04, base loss: 10679.27
[INFO 2017-06-26 14:49:23,548 main.py:50] epoch 1161, training loss: 4961.01, average training loss: 5808.53, base loss: 10679.14
[INFO 2017-06-26 14:49:26,439 main.py:50] epoch 1162, training loss: 4871.37, average training loss: 5805.19, base loss: 10678.59
[INFO 2017-06-26 14:49:29,349 main.py:50] epoch 1163, training loss: 4932.82, average training loss: 5801.69, base loss: 10678.58
[INFO 2017-06-26 14:49:32,246 main.py:50] epoch 1164, training loss: 4869.77, average training loss: 5798.17, base loss: 10678.07
[INFO 2017-06-26 14:49:35,131 main.py:50] epoch 1165, training loss: 4878.80, average training loss: 5795.13, base loss: 10678.22
[INFO 2017-06-26 14:49:38,037 main.py:50] epoch 1166, training loss: 4970.27, average training loss: 5791.89, base loss: 10678.13
[INFO 2017-06-26 14:49:40,965 main.py:50] epoch 1167, training loss: 4880.12, average training loss: 5788.88, base loss: 10678.03
[INFO 2017-06-26 14:49:43,861 main.py:50] epoch 1168, training loss: 4932.92, average training loss: 5785.70, base loss: 10677.88
[INFO 2017-06-26 14:49:46,761 main.py:50] epoch 1169, training loss: 4894.00, average training loss: 5782.58, base loss: 10677.97
[INFO 2017-06-26 14:49:49,664 main.py:50] epoch 1170, training loss: 4906.22, average training loss: 5779.39, base loss: 10678.14
[INFO 2017-06-26 14:49:52,575 main.py:50] epoch 1171, training loss: 4812.84, average training loss: 5776.22, base loss: 10678.21
[INFO 2017-06-26 14:49:55,481 main.py:50] epoch 1172, training loss: 4895.29, average training loss: 5773.19, base loss: 10678.21
[INFO 2017-06-26 14:49:58,374 main.py:50] epoch 1173, training loss: 4794.50, average training loss: 5769.88, base loss: 10677.37
[INFO 2017-06-26 14:50:01,249 main.py:50] epoch 1174, training loss: 4793.17, average training loss: 5766.60, base loss: 10676.54
[INFO 2017-06-26 14:50:04,151 main.py:50] epoch 1175, training loss: 4826.90, average training loss: 5763.50, base loss: 10676.21
[INFO 2017-06-26 14:50:07,044 main.py:50] epoch 1176, training loss: 4944.90, average training loss: 5760.06, base loss: 10675.96
[INFO 2017-06-26 14:50:09,948 main.py:50] epoch 1177, training loss: 4947.22, average training loss: 5757.20, base loss: 10676.20
[INFO 2017-06-26 14:50:12,865 main.py:50] epoch 1178, training loss: 4943.05, average training loss: 5754.13, base loss: 10676.15
[INFO 2017-06-26 14:50:15,771 main.py:50] epoch 1179, training loss: 4914.37, average training loss: 5751.25, base loss: 10676.45
[INFO 2017-06-26 14:50:18,675 main.py:50] epoch 1180, training loss: 4841.32, average training loss: 5748.19, base loss: 10676.39
[INFO 2017-06-26 14:50:21,607 main.py:50] epoch 1181, training loss: 4935.12, average training loss: 5745.17, base loss: 10675.95
[INFO 2017-06-26 14:50:24,496 main.py:50] epoch 1182, training loss: 4863.02, average training loss: 5742.01, base loss: 10675.87
[INFO 2017-06-26 14:50:27,387 main.py:50] epoch 1183, training loss: 4780.98, average training loss: 5738.88, base loss: 10675.53
[INFO 2017-06-26 14:50:30,278 main.py:50] epoch 1184, training loss: 5011.50, average training loss: 5736.25, base loss: 10676.20
[INFO 2017-06-26 14:50:33,179 main.py:50] epoch 1185, training loss: 4807.88, average training loss: 5733.23, base loss: 10675.80
[INFO 2017-06-26 14:50:36,059 main.py:50] epoch 1186, training loss: 4934.89, average training loss: 5730.05, base loss: 10675.74
[INFO 2017-06-26 14:50:38,946 main.py:50] epoch 1187, training loss: 5034.30, average training loss: 5726.97, base loss: 10675.54
[INFO 2017-06-26 14:50:41,831 main.py:50] epoch 1188, training loss: 4895.39, average training loss: 5723.86, base loss: 10675.28
[INFO 2017-06-26 14:50:44,718 main.py:50] epoch 1189, training loss: 4889.61, average training loss: 5720.81, base loss: 10674.75
[INFO 2017-06-26 14:50:47,620 main.py:50] epoch 1190, training loss: 4948.21, average training loss: 5718.07, base loss: 10674.81
[INFO 2017-06-26 14:50:50,523 main.py:50] epoch 1191, training loss: 4956.17, average training loss: 5715.31, base loss: 10675.16
[INFO 2017-06-26 14:50:53,406 main.py:50] epoch 1192, training loss: 4959.40, average training loss: 5712.61, base loss: 10674.92
[INFO 2017-06-26 14:50:56,288 main.py:50] epoch 1193, training loss: 4859.45, average training loss: 5709.89, base loss: 10674.92
[INFO 2017-06-26 14:50:59,162 main.py:50] epoch 1194, training loss: 4970.69, average training loss: 5706.99, base loss: 10675.04
[INFO 2017-06-26 14:51:02,036 main.py:50] epoch 1195, training loss: 4942.83, average training loss: 5704.27, base loss: 10675.11
[INFO 2017-06-26 14:51:04,931 main.py:50] epoch 1196, training loss: 4911.90, average training loss: 5701.71, base loss: 10675.60
[INFO 2017-06-26 14:51:07,819 main.py:50] epoch 1197, training loss: 4961.73, average training loss: 5698.89, base loss: 10675.59
[INFO 2017-06-26 14:51:10,726 main.py:50] epoch 1198, training loss: 5028.64, average training loss: 5696.34, base loss: 10675.59
[INFO 2017-06-26 14:51:13,644 main.py:50] epoch 1199, training loss: 4878.81, average training loss: 5693.40, base loss: 10675.25
[INFO 2017-06-26 14:51:13,644 main.py:52] epoch 1199, testing
[INFO 2017-06-26 14:51:27,017 main.py:103] average testing loss: 4867.80, base loss: 10473.39
[INFO 2017-06-26 14:51:27,017 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 14:51:27,024 main.py:76] current best accuracy: 4867.80
[INFO 2017-06-26 14:51:29,910 main.py:50] epoch 1200, training loss: 4903.67, average training loss: 5690.57, base loss: 10675.07
[INFO 2017-06-26 14:51:32,828 main.py:50] epoch 1201, training loss: 4884.88, average training loss: 5687.71, base loss: 10674.92
[INFO 2017-06-26 14:51:35,723 main.py:50] epoch 1202, training loss: 4903.72, average training loss: 5685.07, base loss: 10675.24
[INFO 2017-06-26 14:51:38,631 main.py:50] epoch 1203, training loss: 4956.74, average training loss: 5682.30, base loss: 10675.00
[INFO 2017-06-26 14:51:41,540 main.py:50] epoch 1204, training loss: 4939.83, average training loss: 5679.33, base loss: 10674.91
[INFO 2017-06-26 14:51:44,434 main.py:50] epoch 1205, training loss: 4976.87, average training loss: 5676.54, base loss: 10674.94
[INFO 2017-06-26 14:51:47,328 main.py:50] epoch 1206, training loss: 4836.87, average training loss: 5673.99, base loss: 10675.23
[INFO 2017-06-26 14:51:50,233 main.py:50] epoch 1207, training loss: 4851.26, average training loss: 5671.18, base loss: 10675.08
[INFO 2017-06-26 14:51:53,139 main.py:50] epoch 1208, training loss: 5011.92, average training loss: 5668.43, base loss: 10675.14
[INFO 2017-06-26 14:51:56,035 main.py:50] epoch 1209, training loss: 4819.25, average training loss: 5665.78, base loss: 10675.15
[INFO 2017-06-26 14:51:58,931 main.py:50] epoch 1210, training loss: 4809.24, average training loss: 5663.15, base loss: 10675.31
[INFO 2017-06-26 14:52:01,830 main.py:50] epoch 1211, training loss: 4991.44, average training loss: 5660.09, base loss: 10675.05
[INFO 2017-06-26 14:52:04,755 main.py:50] epoch 1212, training loss: 4816.66, average training loss: 5657.36, base loss: 10675.00
[INFO 2017-06-26 14:52:07,649 main.py:50] epoch 1213, training loss: 4883.77, average training loss: 5654.79, base loss: 10675.48
[INFO 2017-06-26 14:52:10,533 main.py:50] epoch 1214, training loss: 4718.88, average training loss: 5652.09, base loss: 10674.90
[INFO 2017-06-26 14:52:13,424 main.py:50] epoch 1215, training loss: 4930.77, average training loss: 5649.64, base loss: 10674.95
[INFO 2017-06-26 14:52:16,317 main.py:50] epoch 1216, training loss: 5079.48, average training loss: 5647.41, base loss: 10675.83
[INFO 2017-06-26 14:52:19,199 main.py:50] epoch 1217, training loss: 4847.59, average training loss: 5644.56, base loss: 10675.36
[INFO 2017-06-26 14:52:22,087 main.py:50] epoch 1218, training loss: 4917.07, average training loss: 5642.25, base loss: 10675.81
[INFO 2017-06-26 14:52:25,004 main.py:50] epoch 1219, training loss: 4868.23, average training loss: 5639.40, base loss: 10675.42
[INFO 2017-06-26 14:52:27,903 main.py:50] epoch 1220, training loss: 5007.56, average training loss: 5636.75, base loss: 10674.99
[INFO 2017-06-26 14:52:30,787 main.py:50] epoch 1221, training loss: 4840.47, average training loss: 5633.79, base loss: 10674.26
[INFO 2017-06-26 14:52:33,697 main.py:50] epoch 1222, training loss: 4948.10, average training loss: 5631.24, base loss: 10673.88
[INFO 2017-06-26 14:52:36,588 main.py:50] epoch 1223, training loss: 4903.26, average training loss: 5628.99, base loss: 10674.65
[INFO 2017-06-26 14:52:39,473 main.py:50] epoch 1224, training loss: 4881.01, average training loss: 5626.54, base loss: 10675.08
[INFO 2017-06-26 14:52:42,360 main.py:50] epoch 1225, training loss: 5107.22, average training loss: 5624.31, base loss: 10676.40
[INFO 2017-06-26 14:52:45,234 main.py:50] epoch 1226, training loss: 4961.23, average training loss: 5621.75, base loss: 10676.35
[INFO 2017-06-26 14:52:48,117 main.py:50] epoch 1227, training loss: 4886.21, average training loss: 5619.02, base loss: 10676.04
[INFO 2017-06-26 14:52:51,005 main.py:50] epoch 1228, training loss: 4922.22, average training loss: 5616.33, base loss: 10675.80
[INFO 2017-06-26 14:52:53,889 main.py:50] epoch 1229, training loss: 4898.45, average training loss: 5613.62, base loss: 10675.25
[INFO 2017-06-26 14:52:56,776 main.py:50] epoch 1230, training loss: 4907.23, average training loss: 5611.27, base loss: 10675.53
[INFO 2017-06-26 14:52:59,678 main.py:50] epoch 1231, training loss: 4917.11, average training loss: 5608.87, base loss: 10675.80
[INFO 2017-06-26 14:53:02,561 main.py:50] epoch 1232, training loss: 4920.52, average training loss: 5606.62, base loss: 10676.61
[INFO 2017-06-26 14:53:05,442 main.py:50] epoch 1233, training loss: 4899.81, average training loss: 5604.24, base loss: 10676.93
[INFO 2017-06-26 14:53:08,313 main.py:50] epoch 1234, training loss: 4912.49, average training loss: 5601.83, base loss: 10677.11
[INFO 2017-06-26 14:53:11,209 main.py:50] epoch 1235, training loss: 4829.91, average training loss: 5599.38, base loss: 10676.90
[INFO 2017-06-26 14:53:14,122 main.py:50] epoch 1236, training loss: 4820.38, average training loss: 5596.54, base loss: 10675.90
[INFO 2017-06-26 14:53:17,024 main.py:50] epoch 1237, training loss: 4935.67, average training loss: 5594.07, base loss: 10675.80
[INFO 2017-06-26 14:53:19,907 main.py:50] epoch 1238, training loss: 5147.95, average training loss: 5591.92, base loss: 10676.75
[INFO 2017-06-26 14:53:22,788 main.py:50] epoch 1239, training loss: 4905.43, average training loss: 5589.35, base loss: 10675.82
[INFO 2017-06-26 14:53:25,677 main.py:50] epoch 1240, training loss: 4851.52, average training loss: 5586.41, base loss: 10674.94
[INFO 2017-06-26 14:53:28,599 main.py:50] epoch 1241, training loss: 4988.07, average training loss: 5584.15, base loss: 10675.56
[INFO 2017-06-26 14:53:31,477 main.py:50] epoch 1242, training loss: 4770.76, average training loss: 5581.46, base loss: 10674.74
[INFO 2017-06-26 14:53:34,356 main.py:50] epoch 1243, training loss: 4998.39, average training loss: 5579.44, base loss: 10675.60
[INFO 2017-06-26 14:53:37,248 main.py:50] epoch 1244, training loss: 4701.84, average training loss: 5576.84, base loss: 10674.79
[INFO 2017-06-26 14:53:40,143 main.py:50] epoch 1245, training loss: 4749.67, average training loss: 5574.41, base loss: 10674.39
[INFO 2017-06-26 14:53:43,019 main.py:50] epoch 1246, training loss: 4734.94, average training loss: 5571.83, base loss: 10673.40
[INFO 2017-06-26 14:53:45,902 main.py:50] epoch 1247, training loss: 4956.16, average training loss: 5569.62, base loss: 10673.57
[INFO 2017-06-26 14:53:48,796 main.py:50] epoch 1248, training loss: 4960.34, average training loss: 5567.27, base loss: 10673.64
[INFO 2017-06-26 14:53:51,681 main.py:50] epoch 1249, training loss: 4768.17, average training loss: 5564.56, base loss: 10672.19
[INFO 2017-06-26 14:53:54,563 main.py:50] epoch 1250, training loss: 4928.47, average training loss: 5562.04, base loss: 10671.64
[INFO 2017-06-26 14:53:57,481 main.py:50] epoch 1251, training loss: 4775.18, average training loss: 5559.77, base loss: 10671.43
[INFO 2017-06-26 14:54:00,373 main.py:50] epoch 1252, training loss: 4966.71, average training loss: 5557.54, base loss: 10671.78
[INFO 2017-06-26 14:54:03,278 main.py:50] epoch 1253, training loss: 4777.01, average training loss: 5555.01, base loss: 10671.28
[INFO 2017-06-26 14:54:06,172 main.py:50] epoch 1254, training loss: 4837.68, average training loss: 5552.42, base loss: 10670.79
[INFO 2017-06-26 14:54:09,074 main.py:50] epoch 1255, training loss: 4943.60, average training loss: 5550.04, base loss: 10670.72
[INFO 2017-06-26 14:54:11,968 main.py:50] epoch 1256, training loss: 4913.15, average training loss: 5547.92, base loss: 10671.31
[INFO 2017-06-26 14:54:14,845 main.py:50] epoch 1257, training loss: 4769.31, average training loss: 5545.46, base loss: 10670.80
[INFO 2017-06-26 14:54:17,743 main.py:50] epoch 1258, training loss: 4794.87, average training loss: 5542.99, base loss: 10670.68
[INFO 2017-06-26 14:54:20,631 main.py:50] epoch 1259, training loss: 4887.85, average training loss: 5540.79, base loss: 10670.93
[INFO 2017-06-26 14:54:23,514 main.py:50] epoch 1260, training loss: 4961.04, average training loss: 5538.58, base loss: 10671.05
[INFO 2017-06-26 14:54:26,393 main.py:50] epoch 1261, training loss: 4881.55, average training loss: 5536.48, base loss: 10671.38
[INFO 2017-06-26 14:54:29,286 main.py:50] epoch 1262, training loss: 4931.87, average training loss: 5534.43, base loss: 10672.07
[INFO 2017-06-26 14:54:32,208 main.py:50] epoch 1263, training loss: 4803.01, average training loss: 5532.31, base loss: 10672.11
[INFO 2017-06-26 14:54:35,123 main.py:50] epoch 1264, training loss: 4798.11, average training loss: 5529.90, base loss: 10671.43
[INFO 2017-06-26 14:54:38,007 main.py:50] epoch 1265, training loss: 4861.24, average training loss: 5527.57, base loss: 10670.89
[INFO 2017-06-26 14:54:40,884 main.py:50] epoch 1266, training loss: 4832.49, average training loss: 5525.24, base loss: 10670.62
[INFO 2017-06-26 14:54:43,802 main.py:50] epoch 1267, training loss: 4902.92, average training loss: 5523.13, base loss: 10670.59
[INFO 2017-06-26 14:54:46,704 main.py:50] epoch 1268, training loss: 4947.00, average training loss: 5521.08, base loss: 10670.62
[INFO 2017-06-26 14:54:49,608 main.py:50] epoch 1269, training loss: 4808.14, average training loss: 5518.93, base loss: 10670.72
[INFO 2017-06-26 14:54:52,499 main.py:50] epoch 1270, training loss: 4945.63, average training loss: 5516.88, base loss: 10671.28
[INFO 2017-06-26 14:54:55,400 main.py:50] epoch 1271, training loss: 4753.71, average training loss: 5514.64, base loss: 10671.01
[INFO 2017-06-26 14:54:58,299 main.py:50] epoch 1272, training loss: 4847.34, average training loss: 5512.64, base loss: 10671.12
[INFO 2017-06-26 14:55:01,178 main.py:50] epoch 1273, training loss: 5022.78, average training loss: 5510.75, base loss: 10671.81
[INFO 2017-06-26 14:55:04,095 main.py:50] epoch 1274, training loss: 4910.94, average training loss: 5508.65, base loss: 10672.10
[INFO 2017-06-26 14:55:06,994 main.py:50] epoch 1275, training loss: 5016.22, average training loss: 5506.78, base loss: 10672.79
[INFO 2017-06-26 14:55:09,887 main.py:50] epoch 1276, training loss: 4888.94, average training loss: 5504.52, base loss: 10672.24
[INFO 2017-06-26 14:55:12,770 main.py:50] epoch 1277, training loss: 4963.45, average training loss: 5502.40, base loss: 10672.09
[INFO 2017-06-26 14:55:15,655 main.py:50] epoch 1278, training loss: 5018.59, average training loss: 5500.59, base loss: 10672.98
[INFO 2017-06-26 14:55:18,562 main.py:50] epoch 1279, training loss: 4875.02, average training loss: 5498.23, base loss: 10672.50
[INFO 2017-06-26 14:55:21,452 main.py:50] epoch 1280, training loss: 4785.27, average training loss: 5496.00, base loss: 10672.19
[INFO 2017-06-26 14:55:24,359 main.py:50] epoch 1281, training loss: 4753.11, average training loss: 5493.54, base loss: 10670.82
[INFO 2017-06-26 14:55:27,235 main.py:50] epoch 1282, training loss: 5053.93, average training loss: 5491.79, base loss: 10671.63
[INFO 2017-06-26 14:55:30,149 main.py:50] epoch 1283, training loss: 4999.00, average training loss: 5489.67, base loss: 10671.69
[INFO 2017-06-26 14:55:33,030 main.py:50] epoch 1284, training loss: 4928.93, average training loss: 5487.58, base loss: 10671.57
[INFO 2017-06-26 14:55:35,915 main.py:50] epoch 1285, training loss: 4930.77, average training loss: 5485.77, base loss: 10672.23
[INFO 2017-06-26 14:55:38,813 main.py:50] epoch 1286, training loss: 4821.51, average training loss: 5483.62, base loss: 10671.90
[INFO 2017-06-26 14:55:41,711 main.py:50] epoch 1287, training loss: 4885.83, average training loss: 5481.56, base loss: 10671.80
[INFO 2017-06-26 14:55:44,599 main.py:50] epoch 1288, training loss: 4977.11, average training loss: 5479.74, base loss: 10672.38
[INFO 2017-06-26 14:55:47,485 main.py:50] epoch 1289, training loss: 4837.60, average training loss: 5477.62, base loss: 10671.66
[INFO 2017-06-26 14:55:50,381 main.py:50] epoch 1290, training loss: 4817.24, average training loss: 5475.65, base loss: 10671.61
[INFO 2017-06-26 14:55:53,281 main.py:50] epoch 1291, training loss: 5035.38, average training loss: 5473.71, base loss: 10671.82
[INFO 2017-06-26 14:55:56,206 main.py:50] epoch 1292, training loss: 4916.22, average training loss: 5471.98, base loss: 10672.55
[INFO 2017-06-26 14:55:59,104 main.py:50] epoch 1293, training loss: 4959.70, average training loss: 5470.15, base loss: 10673.39
[INFO 2017-06-26 14:56:01,987 main.py:50] epoch 1294, training loss: 4835.36, average training loss: 5468.22, base loss: 10673.44
[INFO 2017-06-26 14:56:04,887 main.py:50] epoch 1295, training loss: 4971.36, average training loss: 5466.49, base loss: 10674.37
[INFO 2017-06-26 14:56:07,768 main.py:50] epoch 1296, training loss: 4955.42, average training loss: 5464.61, base loss: 10675.06
[INFO 2017-06-26 14:56:10,670 main.py:50] epoch 1297, training loss: 4928.67, average training loss: 5462.83, base loss: 10675.34
[INFO 2017-06-26 14:56:13,579 main.py:50] epoch 1298, training loss: 4796.92, average training loss: 5460.59, base loss: 10674.79
[INFO 2017-06-26 14:56:16,475 main.py:50] epoch 1299, training loss: 4969.65, average training loss: 5458.68, base loss: 10674.70
[INFO 2017-06-26 14:56:16,475 main.py:52] epoch 1299, testing
[INFO 2017-06-26 14:56:29,856 main.py:103] average testing loss: 4919.21, base loss: 10744.59
[INFO 2017-06-26 14:56:29,857 main.py:76] current best accuracy: 4867.80
[INFO 2017-06-26 14:56:32,742 main.py:50] epoch 1300, training loss: 4784.28, average training loss: 5456.68, base loss: 10674.63
[INFO 2017-06-26 14:56:35,656 main.py:50] epoch 1301, training loss: 4779.66, average training loss: 5454.87, base loss: 10674.33
[INFO 2017-06-26 14:56:38,576 main.py:50] epoch 1302, training loss: 4830.17, average training loss: 5453.02, base loss: 10674.56
[INFO 2017-06-26 14:56:41,478 main.py:50] epoch 1303, training loss: 4780.75, average training loss: 5450.91, base loss: 10673.72
[INFO 2017-06-26 14:56:44,366 main.py:50] epoch 1304, training loss: 5034.52, average training loss: 5449.02, base loss: 10674.18
[INFO 2017-06-26 14:56:47,265 main.py:50] epoch 1305, training loss: 4834.46, average training loss: 5447.16, base loss: 10674.16
[INFO 2017-06-26 14:56:50,154 main.py:50] epoch 1306, training loss: 4839.08, average training loss: 5445.26, base loss: 10673.97
[INFO 2017-06-26 14:56:53,055 main.py:50] epoch 1307, training loss: 5009.14, average training loss: 5443.53, base loss: 10674.34
[INFO 2017-06-26 14:56:55,931 main.py:50] epoch 1308, training loss: 4815.94, average training loss: 5441.47, base loss: 10673.88
[INFO 2017-06-26 14:56:58,824 main.py:50] epoch 1309, training loss: 4882.25, average training loss: 5439.66, base loss: 10674.03
[INFO 2017-06-26 14:57:01,752 main.py:50] epoch 1310, training loss: 4829.86, average training loss: 5437.63, base loss: 10674.23
[INFO 2017-06-26 14:57:04,642 main.py:50] epoch 1311, training loss: 4821.05, average training loss: 5435.44, base loss: 10673.95
[INFO 2017-06-26 14:57:07,551 main.py:50] epoch 1312, training loss: 4806.39, average training loss: 5433.66, base loss: 10674.11
[INFO 2017-06-26 14:57:10,443 main.py:50] epoch 1313, training loss: 4829.15, average training loss: 5431.87, base loss: 10674.30
[INFO 2017-06-26 14:57:13,340 main.py:50] epoch 1314, training loss: 4891.75, average training loss: 5429.99, base loss: 10674.43
[INFO 2017-06-26 14:57:16,227 main.py:50] epoch 1315, training loss: 4879.30, average training loss: 5428.14, base loss: 10674.49
[INFO 2017-06-26 14:57:19,147 main.py:50] epoch 1316, training loss: 4818.64, average training loss: 5426.43, base loss: 10675.04
[INFO 2017-06-26 14:57:22,040 main.py:50] epoch 1317, training loss: 4817.88, average training loss: 5424.53, base loss: 10674.81
[INFO 2017-06-26 14:57:24,928 main.py:50] epoch 1318, training loss: 4818.71, average training loss: 5422.68, base loss: 10674.43
[INFO 2017-06-26 14:57:27,841 main.py:50] epoch 1319, training loss: 4772.09, average training loss: 5420.64, base loss: 10673.97
[INFO 2017-06-26 14:57:30,757 main.py:50] epoch 1320, training loss: 4840.95, average training loss: 5418.76, base loss: 10673.50
[INFO 2017-06-26 14:57:33,659 main.py:50] epoch 1321, training loss: 4743.25, average training loss: 5416.80, base loss: 10673.13
[INFO 2017-06-26 14:57:36,548 main.py:50] epoch 1322, training loss: 4892.46, average training loss: 5414.99, base loss: 10673.16
[INFO 2017-06-26 14:57:39,463 main.py:50] epoch 1323, training loss: 4802.47, average training loss: 5413.20, base loss: 10673.61
[INFO 2017-06-26 14:57:42,343 main.py:50] epoch 1324, training loss: 4919.86, average training loss: 5411.59, base loss: 10673.79
[INFO 2017-06-26 14:57:45,229 main.py:50] epoch 1325, training loss: 5041.39, average training loss: 5409.96, base loss: 10674.63
[INFO 2017-06-26 14:57:48,111 main.py:50] epoch 1326, training loss: 4747.40, average training loss: 5408.16, base loss: 10674.47
[INFO 2017-06-26 14:57:50,999 main.py:50] epoch 1327, training loss: 4728.21, average training loss: 5406.34, base loss: 10674.33
[INFO 2017-06-26 14:57:53,917 main.py:50] epoch 1328, training loss: 4754.83, average training loss: 5404.52, base loss: 10673.90
[INFO 2017-06-26 14:57:56,810 main.py:50] epoch 1329, training loss: 4862.87, average training loss: 5402.87, base loss: 10674.13
[INFO 2017-06-26 14:57:59,724 main.py:50] epoch 1330, training loss: 4774.00, average training loss: 5401.19, base loss: 10674.51
[INFO 2017-06-26 14:58:02,624 main.py:50] epoch 1331, training loss: 4855.80, average training loss: 5399.34, base loss: 10674.77
[INFO 2017-06-26 14:58:05,538 main.py:50] epoch 1332, training loss: 4888.94, average training loss: 5397.68, base loss: 10675.25
[INFO 2017-06-26 14:58:08,449 main.py:50] epoch 1333, training loss: 4845.83, average training loss: 5395.96, base loss: 10675.60
[INFO 2017-06-26 14:58:11,376 main.py:50] epoch 1334, training loss: 4872.66, average training loss: 5394.21, base loss: 10675.78
[INFO 2017-06-26 14:58:14,266 main.py:50] epoch 1335, training loss: 4819.03, average training loss: 5392.38, base loss: 10675.86
[INFO 2017-06-26 14:58:17,158 main.py:50] epoch 1336, training loss: 4917.12, average training loss: 5390.57, base loss: 10676.01
[INFO 2017-06-26 14:58:20,076 main.py:50] epoch 1337, training loss: 4829.96, average training loss: 5388.78, base loss: 10676.01
[INFO 2017-06-26 14:58:22,987 main.py:50] epoch 1338, training loss: 4927.16, average training loss: 5386.92, base loss: 10675.74
[INFO 2017-06-26 14:58:25,865 main.py:50] epoch 1339, training loss: 4892.10, average training loss: 5385.12, base loss: 10675.93
[INFO 2017-06-26 14:58:28,754 main.py:50] epoch 1340, training loss: 4993.93, average training loss: 5383.49, base loss: 10676.52
[INFO 2017-06-26 14:58:31,634 main.py:50] epoch 1341, training loss: 4844.95, average training loss: 5382.01, base loss: 10676.94
[INFO 2017-06-26 14:58:34,509 main.py:50] epoch 1342, training loss: 4884.29, average training loss: 5380.32, base loss: 10677.19
[INFO 2017-06-26 14:58:37,418 main.py:50] epoch 1343, training loss: 4704.55, average training loss: 5378.44, base loss: 10676.60
[INFO 2017-06-26 14:58:40,312 main.py:50] epoch 1344, training loss: 4744.33, average training loss: 5376.61, base loss: 10676.28
[INFO 2017-06-26 14:58:43,186 main.py:50] epoch 1345, training loss: 4788.88, average training loss: 5374.83, base loss: 10676.13
[INFO 2017-06-26 14:58:46,087 main.py:50] epoch 1346, training loss: 4898.59, average training loss: 5373.31, base loss: 10676.71
[INFO 2017-06-26 14:58:48,971 main.py:50] epoch 1347, training loss: 4695.28, average training loss: 5371.36, base loss: 10676.01
[INFO 2017-06-26 14:58:51,862 main.py:50] epoch 1348, training loss: 4867.09, average training loss: 5369.55, base loss: 10675.96
[INFO 2017-06-26 14:58:54,762 main.py:50] epoch 1349, training loss: 4858.90, average training loss: 5367.72, base loss: 10675.68
[INFO 2017-06-26 14:58:57,661 main.py:50] epoch 1350, training loss: 4757.89, average training loss: 5365.92, base loss: 10675.58
[INFO 2017-06-26 14:59:00,534 main.py:50] epoch 1351, training loss: 4813.23, average training loss: 5364.38, base loss: 10676.31
[INFO 2017-06-26 14:59:03,451 main.py:50] epoch 1352, training loss: 4714.13, average training loss: 5362.41, base loss: 10675.87
[INFO 2017-06-26 14:59:06,384 main.py:50] epoch 1353, training loss: 4903.88, average training loss: 5361.06, base loss: 10676.88
[INFO 2017-06-26 14:59:09,299 main.py:50] epoch 1354, training loss: 4761.65, average training loss: 5359.39, base loss: 10677.38
[INFO 2017-06-26 14:59:12,199 main.py:50] epoch 1355, training loss: 4817.75, average training loss: 5357.57, base loss: 10677.18
[INFO 2017-06-26 14:59:15,086 main.py:50] epoch 1356, training loss: 4775.88, average training loss: 5355.83, base loss: 10677.45
[INFO 2017-06-26 14:59:18,004 main.py:50] epoch 1357, training loss: 4999.58, average training loss: 5354.46, base loss: 10678.25
[INFO 2017-06-26 14:59:20,916 main.py:50] epoch 1358, training loss: 4719.67, average training loss: 5352.46, base loss: 10677.32
[INFO 2017-06-26 14:59:23,829 main.py:50] epoch 1359, training loss: 4738.63, average training loss: 5350.65, base loss: 10677.44
[INFO 2017-06-26 14:59:26,705 main.py:50] epoch 1360, training loss: 4777.47, average training loss: 5348.92, base loss: 10677.20
[INFO 2017-06-26 14:59:29,621 main.py:50] epoch 1361, training loss: 4846.94, average training loss: 5347.13, base loss: 10676.70
[INFO 2017-06-26 14:59:32,541 main.py:50] epoch 1362, training loss: 4734.46, average training loss: 5345.39, base loss: 10676.58
[INFO 2017-06-26 14:59:35,424 main.py:50] epoch 1363, training loss: 4732.69, average training loss: 5343.50, base loss: 10676.00
[INFO 2017-06-26 14:59:38,313 main.py:50] epoch 1364, training loss: 4894.93, average training loss: 5341.97, base loss: 10676.32
[INFO 2017-06-26 14:59:41,215 main.py:50] epoch 1365, training loss: 4790.49, average training loss: 5340.60, base loss: 10677.16
[INFO 2017-06-26 14:59:44,091 main.py:50] epoch 1366, training loss: 4710.97, average training loss: 5339.11, base loss: 10677.59
[INFO 2017-06-26 14:59:47,001 main.py:50] epoch 1367, training loss: 4750.98, average training loss: 5337.42, base loss: 10677.69
[INFO 2017-06-26 14:59:49,921 main.py:50] epoch 1368, training loss: 4710.22, average training loss: 5335.50, base loss: 10677.18
[INFO 2017-06-26 14:59:52,825 main.py:50] epoch 1369, training loss: 4872.86, average training loss: 5333.73, base loss: 10676.99
[INFO 2017-06-26 14:59:55,732 main.py:50] epoch 1370, training loss: 4676.70, average training loss: 5331.96, base loss: 10676.58
[INFO 2017-06-26 14:59:58,637 main.py:50] epoch 1371, training loss: 4792.82, average training loss: 5330.17, base loss: 10676.69
[INFO 2017-06-26 15:00:01,553 main.py:50] epoch 1372, training loss: 4847.21, average training loss: 5328.65, base loss: 10677.33
[INFO 2017-06-26 15:00:04,452 main.py:50] epoch 1373, training loss: 4886.77, average training loss: 5326.83, base loss: 10676.89
[INFO 2017-06-26 15:00:07,370 main.py:50] epoch 1374, training loss: 4742.60, average training loss: 5325.03, base loss: 10676.96
[INFO 2017-06-26 15:00:10,287 main.py:50] epoch 1375, training loss: 4870.85, average training loss: 5323.40, base loss: 10677.38
[INFO 2017-06-26 15:00:13,199 main.py:50] epoch 1376, training loss: 4812.53, average training loss: 5321.70, base loss: 10677.31
[INFO 2017-06-26 15:00:16,093 main.py:50] epoch 1377, training loss: 4768.31, average training loss: 5320.23, base loss: 10677.56
[INFO 2017-06-26 15:00:19,003 main.py:50] epoch 1378, training loss: 4783.97, average training loss: 5318.41, base loss: 10676.91
[INFO 2017-06-26 15:00:21,889 main.py:50] epoch 1379, training loss: 4657.31, average training loss: 5316.69, base loss: 10676.61
[INFO 2017-06-26 15:00:24,815 main.py:50] epoch 1380, training loss: 4673.07, average training loss: 5314.64, base loss: 10675.24
[INFO 2017-06-26 15:00:27,728 main.py:50] epoch 1381, training loss: 4774.12, average training loss: 5312.99, base loss: 10674.96
[INFO 2017-06-26 15:00:30,639 main.py:50] epoch 1382, training loss: 4921.97, average training loss: 5311.70, base loss: 10675.55
[INFO 2017-06-26 15:00:33,559 main.py:50] epoch 1383, training loss: 4731.87, average training loss: 5309.96, base loss: 10674.97
[INFO 2017-06-26 15:00:36,465 main.py:50] epoch 1384, training loss: 4828.82, average training loss: 5308.53, base loss: 10675.37
[INFO 2017-06-26 15:00:39,509 main.py:50] epoch 1385, training loss: 4703.38, average training loss: 5306.81, base loss: 10675.28
[INFO 2017-06-26 15:00:42,479 main.py:50] epoch 1386, training loss: 4811.64, average training loss: 5305.27, base loss: 10675.25
[INFO 2017-06-26 15:00:45,352 main.py:50] epoch 1387, training loss: 4793.67, average training loss: 5303.51, base loss: 10674.88
[INFO 2017-06-26 15:00:48,255 main.py:50] epoch 1388, training loss: 4715.18, average training loss: 5301.82, base loss: 10674.66
[INFO 2017-06-26 15:00:51,144 main.py:50] epoch 1389, training loss: 4710.03, average training loss: 5300.13, base loss: 10674.51
[INFO 2017-06-26 15:00:54,017 main.py:50] epoch 1390, training loss: 4746.38, average training loss: 5298.62, base loss: 10674.98
[INFO 2017-06-26 15:00:56,998 main.py:50] epoch 1391, training loss: 4791.22, average training loss: 5297.01, base loss: 10674.77
[INFO 2017-06-26 15:00:59,941 main.py:50] epoch 1392, training loss: 4770.28, average training loss: 5295.71, base loss: 10675.54
[INFO 2017-06-26 15:01:02,826 main.py:50] epoch 1393, training loss: 4889.76, average training loss: 5294.25, base loss: 10675.66
[INFO 2017-06-26 15:01:05,702 main.py:50] epoch 1394, training loss: 4740.04, average training loss: 5292.77, base loss: 10675.65
[INFO 2017-06-26 15:01:08,573 main.py:50] epoch 1395, training loss: 4622.95, average training loss: 5291.08, base loss: 10674.55
[INFO 2017-06-26 15:01:11,447 main.py:50] epoch 1396, training loss: 4679.34, average training loss: 5289.43, base loss: 10674.21
[INFO 2017-06-26 15:01:14,360 main.py:50] epoch 1397, training loss: 4612.27, average training loss: 5287.64, base loss: 10673.28
[INFO 2017-06-26 15:01:17,244 main.py:50] epoch 1398, training loss: 4650.05, average training loss: 5285.83, base loss: 10672.48
[INFO 2017-06-26 15:01:20,113 main.py:50] epoch 1399, training loss: 4739.24, average training loss: 5284.24, base loss: 10672.32
[INFO 2017-06-26 15:01:20,113 main.py:52] epoch 1399, testing
[INFO 2017-06-26 15:01:33,620 main.py:103] average testing loss: 4753.54, base loss: 10602.63
[INFO 2017-06-26 15:01:33,620 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 15:01:33,627 main.py:76] current best accuracy: 4753.54
[INFO 2017-06-26 15:01:36,502 main.py:50] epoch 1400, training loss: 4772.48, average training loss: 5282.88, base loss: 10672.44
[INFO 2017-06-26 15:01:39,371 main.py:50] epoch 1401, training loss: 4755.13, average training loss: 5281.22, base loss: 10672.32
[INFO 2017-06-26 15:01:42,258 main.py:50] epoch 1402, training loss: 4735.82, average training loss: 5279.87, base loss: 10672.59
[INFO 2017-06-26 15:01:45,242 main.py:50] epoch 1403, training loss: 4844.54, average training loss: 5278.25, base loss: 10672.49
[INFO 2017-06-26 15:01:48,111 main.py:50] epoch 1404, training loss: 4681.82, average training loss: 5276.58, base loss: 10671.78
[INFO 2017-06-26 15:01:50,974 main.py:50] epoch 1405, training loss: 4920.31, average training loss: 5275.30, base loss: 10672.28
[INFO 2017-06-26 15:01:53,848 main.py:50] epoch 1406, training loss: 4700.76, average training loss: 5273.56, base loss: 10671.82
[INFO 2017-06-26 15:01:56,911 main.py:50] epoch 1407, training loss: 4790.98, average training loss: 5272.27, base loss: 10672.17
[INFO 2017-06-26 15:01:59,821 main.py:50] epoch 1408, training loss: 4774.16, average training loss: 5270.83, base loss: 10672.02
[INFO 2017-06-26 15:02:02,699 main.py:50] epoch 1409, training loss: 4758.15, average training loss: 5269.30, base loss: 10671.68
[INFO 2017-06-26 15:02:05,824 main.py:50] epoch 1410, training loss: 4705.77, average training loss: 5267.74, base loss: 10671.46
[INFO 2017-06-26 15:02:08,740 main.py:50] epoch 1411, training loss: 4881.52, average training loss: 5266.36, base loss: 10671.69
[INFO 2017-06-26 15:02:11,646 main.py:50] epoch 1412, training loss: 4702.23, average training loss: 5264.73, base loss: 10670.82
[INFO 2017-06-26 15:02:14,526 main.py:50] epoch 1413, training loss: 4913.45, average training loss: 5263.53, base loss: 10671.50
[INFO 2017-06-26 15:02:17,397 main.py:50] epoch 1414, training loss: 4723.51, average training loss: 5262.04, base loss: 10671.30
[INFO 2017-06-26 15:02:20,272 main.py:50] epoch 1415, training loss: 4654.10, average training loss: 5260.41, base loss: 10670.76
[INFO 2017-06-26 15:02:23,146 main.py:50] epoch 1416, training loss: 4776.84, average training loss: 5258.91, base loss: 10670.77
[INFO 2017-06-26 15:02:26,012 main.py:50] epoch 1417, training loss: 4737.83, average training loss: 5257.65, base loss: 10670.80
[INFO 2017-06-26 15:02:28,925 main.py:50] epoch 1418, training loss: 4959.38, average training loss: 5256.40, base loss: 10671.59
[INFO 2017-06-26 15:02:31,884 main.py:50] epoch 1419, training loss: 4737.92, average training loss: 5254.92, base loss: 10671.28
[INFO 2017-06-26 15:02:34,756 main.py:50] epoch 1420, training loss: 4723.54, average training loss: 5253.43, base loss: 10671.33
[INFO 2017-06-26 15:02:37,663 main.py:50] epoch 1421, training loss: 4900.08, average training loss: 5252.04, base loss: 10671.58
[INFO 2017-06-26 15:02:40,532 main.py:50] epoch 1422, training loss: 4707.67, average training loss: 5250.26, base loss: 10670.62
[INFO 2017-06-26 15:02:43,593 main.py:50] epoch 1423, training loss: 4797.96, average training loss: 5248.79, base loss: 10670.50
[INFO 2017-06-26 15:02:46,471 main.py:50] epoch 1424, training loss: 4762.93, average training loss: 5247.48, base loss: 10670.42
[INFO 2017-06-26 15:02:49,338 main.py:50] epoch 1425, training loss: 4776.09, average training loss: 5246.17, base loss: 10670.61
[INFO 2017-06-26 15:02:52,208 main.py:50] epoch 1426, training loss: 4741.68, average training loss: 5245.19, base loss: 10671.65
[INFO 2017-06-26 15:02:55,230 main.py:50] epoch 1427, training loss: 4700.11, average training loss: 5243.48, base loss: 10670.90
[INFO 2017-06-26 15:02:58,133 main.py:50] epoch 1428, training loss: 4913.88, average training loss: 5242.00, base loss: 10671.17
[INFO 2017-06-26 15:03:01,001 main.py:50] epoch 1429, training loss: 4838.89, average training loss: 5240.58, base loss: 10671.39
[INFO 2017-06-26 15:03:03,974 main.py:50] epoch 1430, training loss: 4604.96, average training loss: 5239.10, base loss: 10670.82
[INFO 2017-06-26 15:03:06,904 main.py:50] epoch 1431, training loss: 4877.63, average training loss: 5237.79, base loss: 10671.47
[INFO 2017-06-26 15:03:09,806 main.py:50] epoch 1432, training loss: 4789.70, average training loss: 5236.56, base loss: 10672.15
[INFO 2017-06-26 15:03:12,679 main.py:50] epoch 1433, training loss: 4780.46, average training loss: 5235.17, base loss: 10672.42
[INFO 2017-06-26 15:03:15,670 main.py:50] epoch 1434, training loss: 4916.57, average training loss: 5233.98, base loss: 10672.97
[INFO 2017-06-26 15:03:18,539 main.py:50] epoch 1435, training loss: 4750.91, average training loss: 5232.62, base loss: 10672.84
[INFO 2017-06-26 15:03:21,421 main.py:50] epoch 1436, training loss: 4683.46, average training loss: 5231.20, base loss: 10672.62
[INFO 2017-06-26 15:03:24,353 main.py:50] epoch 1437, training loss: 4578.51, average training loss: 5229.60, base loss: 10671.90
[INFO 2017-06-26 15:03:27,263 main.py:50] epoch 1438, training loss: 4653.78, average training loss: 5228.07, base loss: 10671.30
[INFO 2017-06-26 15:03:30,137 main.py:50] epoch 1439, training loss: 4860.03, average training loss: 5226.66, base loss: 10671.26
[INFO 2017-06-26 15:03:33,007 main.py:50] epoch 1440, training loss: 4714.06, average training loss: 5225.36, base loss: 10671.57
[INFO 2017-06-26 15:03:35,887 main.py:50] epoch 1441, training loss: 4691.90, average training loss: 5223.90, base loss: 10671.28
[INFO 2017-06-26 15:03:38,762 main.py:50] epoch 1442, training loss: 4669.33, average training loss: 5222.32, base loss: 10670.94
[INFO 2017-06-26 15:03:41,824 main.py:50] epoch 1443, training loss: 4765.20, average training loss: 5220.81, base loss: 10670.71
[INFO 2017-06-26 15:03:44,699 main.py:50] epoch 1444, training loss: 4751.54, average training loss: 5219.49, base loss: 10670.64
[INFO 2017-06-26 15:03:47,623 main.py:50] epoch 1445, training loss: 4629.86, average training loss: 5217.80, base loss: 10669.46
[INFO 2017-06-26 15:03:50,494 main.py:50] epoch 1446, training loss: 4918.13, average training loss: 5216.71, base loss: 10670.15
[INFO 2017-06-26 15:03:53,398 main.py:50] epoch 1447, training loss: 4744.88, average training loss: 5215.30, base loss: 10669.49
[INFO 2017-06-26 15:03:56,274 main.py:50] epoch 1448, training loss: 4803.30, average training loss: 5213.92, base loss: 10669.63
[INFO 2017-06-26 15:03:59,245 main.py:50] epoch 1449, training loss: 4688.09, average training loss: 5212.58, base loss: 10669.22
[INFO 2017-06-26 15:04:02,132 main.py:50] epoch 1450, training loss: 4692.41, average training loss: 5211.22, base loss: 10669.18
[INFO 2017-06-26 15:04:05,104 main.py:50] epoch 1451, training loss: 5046.20, average training loss: 5210.26, base loss: 10670.31
[INFO 2017-06-26 15:04:07,999 main.py:50] epoch 1452, training loss: 4892.59, average training loss: 5209.22, base loss: 10671.07
[INFO 2017-06-26 15:04:10,869 main.py:50] epoch 1453, training loss: 4784.62, average training loss: 5208.08, base loss: 10671.37
[INFO 2017-06-26 15:04:13,818 main.py:50] epoch 1454, training loss: 4732.70, average training loss: 5206.98, base loss: 10671.52
[INFO 2017-06-26 15:04:16,707 main.py:50] epoch 1455, training loss: 4718.58, average training loss: 5205.48, base loss: 10670.73
[INFO 2017-06-26 15:04:19,580 main.py:50] epoch 1456, training loss: 4780.86, average training loss: 5204.13, base loss: 10670.71
[INFO 2017-06-26 15:04:22,447 main.py:50] epoch 1457, training loss: 4801.70, average training loss: 5202.88, base loss: 10670.95
[INFO 2017-06-26 15:04:25,393 main.py:50] epoch 1458, training loss: 4845.97, average training loss: 5201.48, base loss: 10670.66
[INFO 2017-06-26 15:04:28,369 main.py:50] epoch 1459, training loss: 4759.87, average training loss: 5200.17, base loss: 10670.50
[INFO 2017-06-26 15:04:31,287 main.py:50] epoch 1460, training loss: 4917.55, average training loss: 5198.90, base loss: 10670.88
[INFO 2017-06-26 15:04:34,213 main.py:50] epoch 1461, training loss: 4723.65, average training loss: 5197.72, base loss: 10671.19
[INFO 2017-06-26 15:04:37,090 main.py:50] epoch 1462, training loss: 4923.78, average training loss: 5196.41, base loss: 10671.14
[INFO 2017-06-26 15:04:40,012 main.py:50] epoch 1463, training loss: 4675.61, average training loss: 5195.18, base loss: 10670.43
[INFO 2017-06-26 15:04:42,894 main.py:50] epoch 1464, training loss: 4929.20, average training loss: 5194.02, base loss: 10670.64
[INFO 2017-06-26 15:04:45,766 main.py:50] epoch 1465, training loss: 4739.41, average training loss: 5192.87, base loss: 10670.51
[INFO 2017-06-26 15:04:48,645 main.py:50] epoch 1466, training loss: 4777.06, average training loss: 5191.49, base loss: 10669.86
[INFO 2017-06-26 15:04:51,586 main.py:50] epoch 1467, training loss: 4717.59, average training loss: 5190.24, base loss: 10669.94
[INFO 2017-06-26 15:04:54,456 main.py:50] epoch 1468, training loss: 4714.73, average training loss: 5188.97, base loss: 10669.49
[INFO 2017-06-26 15:04:57,364 main.py:50] epoch 1469, training loss: 4773.43, average training loss: 5187.92, base loss: 10670.06
[INFO 2017-06-26 15:05:00,256 main.py:50] epoch 1470, training loss: 4704.66, average training loss: 5186.53, base loss: 10669.88
[INFO 2017-06-26 15:05:03,131 main.py:50] epoch 1471, training loss: 4681.87, average training loss: 5185.13, base loss: 10669.67
[INFO 2017-06-26 15:05:06,015 main.py:50] epoch 1472, training loss: 4823.13, average training loss: 5183.94, base loss: 10669.38
[INFO 2017-06-26 15:05:08,898 main.py:50] epoch 1473, training loss: 4650.27, average training loss: 5182.58, base loss: 10669.25
[INFO 2017-06-26 15:05:11,821 main.py:50] epoch 1474, training loss: 4843.78, average training loss: 5181.42, base loss: 10669.58
[INFO 2017-06-26 15:05:14,710 main.py:50] epoch 1475, training loss: 4659.20, average training loss: 5179.95, base loss: 10669.11
[INFO 2017-06-26 15:05:17,620 main.py:50] epoch 1476, training loss: 4633.14, average training loss: 5178.68, base loss: 10669.36
[INFO 2017-06-26 15:05:20,524 main.py:50] epoch 1477, training loss: 4816.48, average training loss: 5177.40, base loss: 10669.45
[INFO 2017-06-26 15:05:23,403 main.py:50] epoch 1478, training loss: 4710.18, average training loss: 5176.32, base loss: 10670.27
[INFO 2017-06-26 15:05:26,268 main.py:50] epoch 1479, training loss: 4767.16, average training loss: 5175.20, base loss: 10670.81
[INFO 2017-06-26 15:05:29,188 main.py:50] epoch 1480, training loss: 4697.27, average training loss: 5173.90, base loss: 10670.75
[INFO 2017-06-26 15:05:32,069 main.py:50] epoch 1481, training loss: 4716.53, average training loss: 5172.56, base loss: 10670.43
[INFO 2017-06-26 15:05:34,943 main.py:50] epoch 1482, training loss: 4888.31, average training loss: 5171.52, base loss: 10671.11
[INFO 2017-06-26 15:05:37,818 main.py:50] epoch 1483, training loss: 4695.14, average training loss: 5170.25, base loss: 10670.77
[INFO 2017-06-26 15:05:40,698 main.py:50] epoch 1484, training loss: 4831.80, average training loss: 5169.07, base loss: 10670.61
[INFO 2017-06-26 15:05:43,573 main.py:50] epoch 1485, training loss: 4642.89, average training loss: 5167.74, base loss: 10669.99
[INFO 2017-06-26 15:05:46,445 main.py:50] epoch 1486, training loss: 4724.64, average training loss: 5166.54, base loss: 10669.96
[INFO 2017-06-26 15:05:49,327 main.py:50] epoch 1487, training loss: 4867.31, average training loss: 5165.51, base loss: 10670.72
[INFO 2017-06-26 15:05:52,199 main.py:50] epoch 1488, training loss: 4744.46, average training loss: 5164.42, base loss: 10670.23
[INFO 2017-06-26 15:05:55,117 main.py:50] epoch 1489, training loss: 4826.45, average training loss: 5163.49, base loss: 10670.92
[INFO 2017-06-26 15:05:58,027 main.py:50] epoch 1490, training loss: 4588.43, average training loss: 5162.18, base loss: 10670.55
[INFO 2017-06-26 15:06:00,927 main.py:50] epoch 1491, training loss: 4974.24, average training loss: 5161.23, base loss: 10671.04
[INFO 2017-06-26 15:06:03,801 main.py:50] epoch 1492, training loss: 4719.35, average training loss: 5159.91, base loss: 10670.29
[INFO 2017-06-26 15:06:06,680 main.py:50] epoch 1493, training loss: 4697.45, average training loss: 5158.52, base loss: 10669.59
[INFO 2017-06-26 15:06:09,590 main.py:50] epoch 1494, training loss: 4727.14, average training loss: 5157.34, base loss: 10669.68
[INFO 2017-06-26 15:06:12,468 main.py:50] epoch 1495, training loss: 4788.96, average training loss: 5156.07, base loss: 10669.36
[INFO 2017-06-26 15:06:15,364 main.py:50] epoch 1496, training loss: 4774.30, average training loss: 5154.94, base loss: 10669.21
[INFO 2017-06-26 15:06:18,311 main.py:50] epoch 1497, training loss: 4888.56, average training loss: 5153.89, base loss: 10669.80
[INFO 2017-06-26 15:06:21,181 main.py:50] epoch 1498, training loss: 4669.41, average training loss: 5152.67, base loss: 10669.91
[INFO 2017-06-26 15:06:24,094 main.py:50] epoch 1499, training loss: 4754.10, average training loss: 5151.37, base loss: 10669.94
[INFO 2017-06-26 15:06:24,094 main.py:52] epoch 1499, testing
[INFO 2017-06-26 15:06:37,706 main.py:103] average testing loss: 4698.52, base loss: 10612.74
[INFO 2017-06-26 15:06:37,707 main.py:72] model save to ./model/final.pth
[INFO 2017-06-26 15:06:37,714 main.py:76] current best accuracy: 4698.52
[INFO 2017-06-26 15:06:40,638 main.py:50] epoch 1500, training loss: 4602.34, average training loss: 5150.09, base loss: 10669.54
[INFO 2017-06-26 15:06:43,568 main.py:50] epoch 1501, training loss: 4697.73, average training loss: 5148.98, base loss: 10669.41
[INFO 2017-06-26 15:06:46,540 main.py:50] epoch 1502, training loss: 4714.30, average training loss: 5147.89, base loss: 10669.70
[INFO 2017-06-26 15:06:49,507 main.py:50] epoch 1503, training loss: 4915.88, average training loss: 5146.79, base loss: 10670.36
[INFO 2017-06-26 15:06:52,449 main.py:50] epoch 1504, training loss: 4820.29, average training loss: 5145.77, base loss: 10670.61
[INFO 2017-06-26 15:06:55,332 main.py:50] epoch 1505, training loss: 4715.14, average training loss: 5144.58, base loss: 10670.05
[INFO 2017-06-26 15:06:58,215 main.py:50] epoch 1506, training loss: 4998.93, average training loss: 5143.69, base loss: 10670.69
[INFO 2017-06-26 15:07:01,130 main.py:50] epoch 1507, training loss: 4750.54, average training loss: 5142.71, base loss: 10671.24
[INFO 2017-06-26 15:07:04,008 main.py:50] epoch 1508, training loss: 4820.23, average training loss: 5141.83, base loss: 10671.80
[INFO 2017-06-26 15:07:06,881 main.py:50] epoch 1509, training loss: 4719.23, average training loss: 5140.63, base loss: 10671.48
[INFO 2017-06-26 15:07:09,775 main.py:50] epoch 1510, training loss: 4692.43, average training loss: 5139.44, base loss: 10671.33
[INFO 2017-06-26 15:07:12,652 main.py:50] epoch 1511, training loss: 4772.86, average training loss: 5138.19, base loss: 10671.23
[INFO 2017-06-26 15:07:15,523 main.py:50] epoch 1512, training loss: 4886.91, average training loss: 5137.22, base loss: 10671.70
[INFO 2017-06-26 15:07:18,422 main.py:50] epoch 1513, training loss: 4789.35, average training loss: 5136.14, base loss: 10671.57
[INFO 2017-06-26 15:07:21,296 main.py:50] epoch 1514, training loss: 4788.36, average training loss: 5134.99, base loss: 10671.30
[INFO 2017-06-26 15:07:24,218 main.py:50] epoch 1515, training loss: 5021.61, average training loss: 5134.17, base loss: 10672.14
[INFO 2017-06-26 15:07:27,091 main.py:50] epoch 1516, training loss: 4925.46, average training loss: 5133.41, base loss: 10673.10
[INFO 2017-06-26 15:07:29,976 main.py:50] epoch 1517, training loss: 4789.33, average training loss: 5132.32, base loss: 10673.34
[INFO 2017-06-26 15:07:32,898 main.py:50] epoch 1518, training loss: 4709.03, average training loss: 5131.26, base loss: 10673.26
[INFO 2017-06-26 15:07:35,767 main.py:50] epoch 1519, training loss: 4615.41, average training loss: 5130.10, base loss: 10673.28
[INFO 2017-06-26 15:07:38,643 main.py:50] epoch 1520, training loss: 4607.73, average training loss: 5128.79, base loss: 10672.42
[INFO 2017-06-26 15:07:41,515 main.py:50] epoch 1521, training loss: 4652.01, average training loss: 5127.57, base loss: 10671.84
[INFO 2017-06-26 15:07:44,431 main.py:50] epoch 1522, training loss: 4756.72, average training loss: 5126.60, base loss: 10672.34
[INFO 2017-06-26 15:07:47,301 main.py:50] epoch 1523, training loss: 4758.49, average training loss: 5125.59, base loss: 10672.68
[INFO 2017-06-26 15:07:50,177 main.py:50] epoch 1524, training loss: 4764.49, average training loss: 5124.66, base loss: 10673.02
[INFO 2017-06-26 15:07:53,049 main.py:50] epoch 1525, training loss: 4705.09, average training loss: 5123.58, base loss: 10672.97
[INFO 2017-06-26 15:07:55,927 main.py:50] epoch 1526, training loss: 4635.19, average training loss: 5122.37, base loss: 10672.50
[INFO 2017-06-26 15:07:58,802 main.py:50] epoch 1527, training loss: 4633.41, average training loss: 5121.21, base loss: 10672.62
[INFO 2017-06-26 15:08:01,675 main.py:50] epoch 1528, training loss: 4787.28, average training loss: 5120.23, base loss: 10672.78
[INFO 2017-06-26 15:08:04,554 main.py:50] epoch 1529, training loss: 4714.16, average training loss: 5119.22, base loss: 10672.87
[INFO 2017-06-26 15:08:07,430 main.py:50] epoch 1530, training loss: 4668.12, average training loss: 5118.23, base loss: 10673.60
[INFO 2017-06-26 15:08:10,300 main.py:50] epoch 1531, training loss: 4630.03, average training loss: 5117.12, base loss: 10673.61
[INFO 2017-06-26 15:08:13,175 main.py:50] epoch 1532, training loss: 4791.56, average training loss: 5116.24, base loss: 10674.29
[INFO 2017-06-26 15:08:16,047 main.py:50] epoch 1533, training loss: 4748.05, average training loss: 5115.42, base loss: 10674.81
[INFO 2017-06-26 15:08:18,945 main.py:50] epoch 1534, training loss: 4717.01, average training loss: 5114.55, base loss: 10675.42
[INFO 2017-06-26 15:08:21,861 main.py:50] epoch 1535, training loss: 4796.06, average training loss: 5113.52, base loss: 10675.58
[INFO 2017-06-26 15:08:24,739 main.py:50] epoch 1536, training loss: 4673.40, average training loss: 5112.36, base loss: 10674.96
[INFO 2017-06-26 15:08:27,611 main.py:50] epoch 1537, training loss: 4629.44, average training loss: 5110.99, base loss: 10674.09
[INFO 2017-06-26 15:08:30,527 main.py:50] epoch 1538, training loss: 4755.60, average training loss: 5109.90, base loss: 10674.28
[INFO 2017-06-26 15:08:33,407 main.py:50] epoch 1539, training loss: 4603.27, average training loss: 5108.86, base loss: 10674.07
[INFO 2017-06-26 15:08:36,279 main.py:50] epoch 1540, training loss: 4782.32, average training loss: 5107.80, base loss: 10674.34
[INFO 2017-06-26 15:08:39,159 main.py:50] epoch 1541, training loss: 4671.64, average training loss: 5106.67, base loss: 10674.34
[INFO 2017-06-26 15:08:42,082 main.py:50] epoch 1542, training loss: 4639.04, average training loss: 5105.69, base loss: 10674.49
[INFO 2017-06-26 15:08:44,999 main.py:50] epoch 1543, training loss: 4693.75, average training loss: 5104.57, base loss: 10674.42
[INFO 2017-06-26 15:08:47,880 main.py:50] epoch 1544, training loss: 4743.09, average training loss: 5103.51, base loss: 10674.79
[INFO 2017-06-26 15:08:50,757 main.py:50] epoch 1545, training loss: 4701.81, average training loss: 5102.36, base loss: 10674.48
[INFO 2017-06-26 15:08:53,631 main.py:50] epoch 1546, training loss: 4629.88, average training loss: 5101.15, base loss: 10674.13
[INFO 2017-06-26 15:08:56,511 main.py:50] epoch 1547, training loss: 4661.48, average training loss: 5099.92, base loss: 10673.56
[INFO 2017-06-26 15:08:59,390 main.py:50] epoch 1548, training loss: 4643.34, average training loss: 5098.74, base loss: 10673.42
[INFO 2017-06-26 15:09:02,268 main.py:50] epoch 1549, training loss: 4689.60, average training loss: 5097.66, base loss: 10673.01
[INFO 2017-06-26 15:09:05,145 main.py:50] epoch 1550, training loss: 4690.11, average training loss: 5096.38, base loss: 10672.78
[INFO 2017-06-26 15:09:08,026 main.py:50] epoch 1551, training loss: 4684.36, average training loss: 5095.37, base loss: 10672.86
[INFO 2017-06-26 15:09:10,907 main.py:50] epoch 1552, training loss: 4589.06, average training loss: 5094.04, base loss: 10672.44
[INFO 2017-06-26 15:09:13,790 main.py:50] epoch 1553, training loss: 4578.57, average training loss: 5092.93, base loss: 10672.54
[INFO 2017-06-26 15:09:16,668 main.py:50] epoch 1554, training loss: 4577.13, average training loss: 5091.50, base loss: 10671.35
[INFO 2017-06-26 15:09:19,542 main.py:50] epoch 1555, training loss: 4714.39, average training loss: 5090.42, base loss: 10671.57
[INFO 2017-06-26 15:09:22,424 main.py:50] epoch 1556, training loss: 4648.63, average training loss: 5089.21, base loss: 10670.78
[INFO 2017-06-26 15:09:25,310 main.py:50] epoch 1557, training loss: 4716.03, average training loss: 5088.23, base loss: 10670.92
[INFO 2017-06-26 15:09:28,210 main.py:50] epoch 1558, training loss: 4819.89, average training loss: 5087.28, base loss: 10671.20
[INFO 2017-06-26 15:09:31,105 main.py:50] epoch 1559, training loss: 4769.37, average training loss: 5086.29, base loss: 10671.35
[INFO 2017-06-26 15:09:34,025 main.py:50] epoch 1560, training loss: 4752.23, average training loss: 5085.33, base loss: 10671.63
[INFO 2017-06-26 15:09:36,944 main.py:50] epoch 1561, training loss: 4705.38, average training loss: 5084.15, base loss: 10671.31
[INFO 2017-06-26 15:09:39,826 main.py:50] epoch 1562, training loss: 4709.02, average training loss: 5083.12, base loss: 10671.49
[INFO 2017-06-26 15:09:42,722 main.py:50] epoch 1563, training loss: 4637.58, average training loss: 5082.10, base loss: 10671.80
[INFO 2017-06-26 15:09:45,613 main.py:50] epoch 1564, training loss: 4777.12, average training loss: 5081.30, base loss: 10672.40
[INFO 2017-06-26 15:09:48,513 main.py:50] epoch 1565, training loss: 4716.49, average training loss: 5080.31, base loss: 10672.15
[INFO 2017-06-26 15:09:51,392 main.py:50] epoch 1566, training loss: 4782.32, average training loss: 5079.49, base loss: 10672.73
[INFO 2017-06-26 15:09:54,288 main.py:50] epoch 1567, training loss: 4735.16, average training loss: 5078.58, base loss: 10673.03
[INFO 2017-06-26 15:09:57,212 main.py:50] epoch 1568, training loss: 4702.88, average training loss: 5077.60, base loss: 10673.25
[INFO 2017-06-26 15:10:00,117 main.py:50] epoch 1569, training loss: 4642.63, average training loss: 5076.74, base loss: 10673.78
[INFO 2017-06-26 15:10:02,991 main.py:50] epoch 1570, training loss: 4640.14, average training loss: 5075.74, base loss: 10673.78
[INFO 2017-06-26 15:10:05,885 main.py:50] epoch 1571, training loss: 4741.67, average training loss: 5074.77, base loss: 10673.99
[INFO 2017-06-26 15:10:08,764 main.py:50] epoch 1572, training loss: 4710.79, average training loss: 5073.73, base loss: 10673.93
[INFO 2017-06-26 15:10:11,647 main.py:50] epoch 1573, training loss: 4649.50, average training loss: 5072.76, base loss: 10674.13
[INFO 2017-06-26 15:10:14,542 main.py:50] epoch 1574, training loss: 4655.10, average training loss: 5071.68, base loss: 10674.18
[INFO 2017-06-26 15:10:17,430 main.py:50] epoch 1575, training loss: 4747.59, average training loss: 5070.74, base loss: 10674.02
[INFO 2017-06-26 15:10:20,329 main.py:50] epoch 1576, training loss: 4662.69, average training loss: 5069.80, base loss: 10674.44
[INFO 2017-06-26 15:10:23,228 main.py:50] epoch 1577, training loss: 4744.75, average training loss: 5068.87, base loss: 10674.71
[INFO 2017-06-26 15:10:26,149 main.py:50] epoch 1578, training loss: 4668.53, average training loss: 5067.76, base loss: 10674.36
[INFO 2017-06-26 15:10:29,029 main.py:50] epoch 1579, training loss: 4744.85, average training loss: 5066.88, base loss: 10675.10
[INFO 2017-06-26 15:10:31,948 main.py:50] epoch 1580, training loss: 4756.11, average training loss: 5065.94, base loss: 10675.35
[INFO 2017-06-26 15:10:34,843 main.py:50] epoch 1581, training loss: 4652.73, average training loss: 5064.84, base loss: 10675.36
[INFO 2017-06-26 15:10:37,730 main.py:50] epoch 1582, training loss: 4695.79, average training loss: 5063.85, base loss: 10675.25
[INFO 2017-06-26 15:10:40,624 main.py:50] epoch 1583, training loss: 4668.42, average training loss: 5062.87, base loss: 10675.32
[INFO 2017-06-26 15:10:43,516 main.py:50] epoch 1584, training loss: 4711.05, average training loss: 5061.86, base loss: 10675.65
[INFO 2017-06-26 15:10:46,432 main.py:50] epoch 1585, training loss: 4814.09, average training loss: 5060.91, base loss: 10675.53
[INFO 2017-06-26 15:10:49,317 main.py:50] epoch 1586, training loss: 4637.88, average training loss: 5059.88, base loss: 10675.37
[INFO 2017-06-26 15:10:52,205 main.py:50] epoch 1587, training loss: 4686.63, average training loss: 5058.91, base loss: 10675.40
[INFO 2017-06-26 15:10:55,116 main.py:50] epoch 1588, training loss: 4656.96, average training loss: 5057.91, base loss: 10675.27
[INFO 2017-06-26 15:10:58,030 main.py:50] epoch 1589, training loss: 4625.63, average training loss: 5056.97, base loss: 10675.26
