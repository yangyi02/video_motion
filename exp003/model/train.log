[INFO 2017-06-26 21:57:44,252 main.py:126] Namespace(batch_size=32, display=False, image_size=64, init_model_path='', learning_rate=0.001, method='unsupervised', motion_range=2, num_channel=3, num_inputs=3, save_dir='./model', test=False, test_dir='/home/yi/Downloads/youtube-64', test_epoch=10, test_interval=100, train=True, train_dir='/home/yi/Downloads/youtube-64', train_epoch=10000)
[INFO 2017-06-26 21:57:46,674 main.py:47] epoch 0, training loss: 136978.06, average training loss: 136978.06, base loss: 5911.12
[INFO 2017-06-26 21:57:47,014 main.py:47] epoch 1, training loss: 102540.10, average training loss: 119759.08, base loss: 5495.64
[INFO 2017-06-26 21:57:47,353 main.py:47] epoch 2, training loss: 86730.22, average training loss: 108749.46, base loss: 5189.93
[INFO 2017-06-26 21:57:47,682 main.py:47] epoch 3, training loss: 70706.01, average training loss: 99238.60, base loss: 5163.89
[INFO 2017-06-26 21:57:48,016 main.py:47] epoch 4, training loss: 63653.66, average training loss: 92121.61, base loss: 5818.80
[INFO 2017-06-26 21:57:48,357 main.py:47] epoch 5, training loss: 54491.24, average training loss: 85849.88, base loss: 6132.15
[INFO 2017-06-26 21:57:48,692 main.py:47] epoch 6, training loss: 43954.29, average training loss: 79864.80, base loss: 5973.23
[INFO 2017-06-26 21:57:49,027 main.py:47] epoch 7, training loss: 40748.89, average training loss: 74975.31, base loss: 6173.06
[INFO 2017-06-26 21:57:49,360 main.py:47] epoch 8, training loss: 32081.64, average training loss: 70209.35, base loss: 5929.57
[INFO 2017-06-26 21:57:49,687 main.py:47] epoch 9, training loss: 27400.52, average training loss: 65928.46, base loss: 5828.80
[INFO 2017-06-26 21:57:50,023 main.py:47] epoch 10, training loss: 23244.60, average training loss: 62048.11, base loss: 5693.71
[INFO 2017-06-26 21:57:50,365 main.py:47] epoch 11, training loss: 20329.45, average training loss: 58571.56, base loss: 5566.44
[INFO 2017-06-26 21:57:50,705 main.py:47] epoch 12, training loss: 18189.53, average training loss: 55465.25, base loss: 5467.14
[INFO 2017-06-26 21:57:51,041 main.py:47] epoch 13, training loss: 16174.25, average training loss: 52658.75, base loss: 5379.93
[INFO 2017-06-26 21:57:51,380 main.py:47] epoch 14, training loss: 14280.30, average training loss: 50100.18, base loss: 5283.93
[INFO 2017-06-26 21:57:51,715 main.py:47] epoch 15, training loss: 12661.64, average training loss: 47760.27, base loss: 5253.21
[INFO 2017-06-26 21:57:52,047 main.py:47] epoch 16, training loss: 11225.30, average training loss: 45611.16, base loss: 5178.96
[INFO 2017-06-26 21:57:52,382 main.py:47] epoch 17, training loss: 10312.34, average training loss: 43650.11, base loss: 5159.64
[INFO 2017-06-26 21:57:52,713 main.py:47] epoch 18, training loss: 8979.72, average training loss: 41825.36, base loss: 5101.95
[INFO 2017-06-26 21:57:53,053 main.py:47] epoch 19, training loss: 8930.47, average training loss: 40180.61, base loss: 5070.08
[INFO 2017-06-26 21:57:53,382 main.py:47] epoch 20, training loss: 8176.30, average training loss: 38656.60, base loss: 5044.23
[INFO 2017-06-26 21:57:53,716 main.py:47] epoch 21, training loss: 7646.60, average training loss: 37247.05, base loss: 5021.64
[INFO 2017-06-26 21:57:54,056 main.py:47] epoch 22, training loss: 7618.15, average training loss: 35958.84, base loss: 5016.57
[INFO 2017-06-26 21:57:54,396 main.py:47] epoch 23, training loss: 6873.19, average training loss: 34746.94, base loss: 5009.39
[INFO 2017-06-26 21:57:54,745 main.py:47] epoch 24, training loss: 7250.32, average training loss: 33647.07, base loss: 5042.24
[INFO 2017-06-26 21:57:55,105 main.py:47] epoch 25, training loss: 6338.10, average training loss: 32596.73, base loss: 5031.76
[INFO 2017-06-26 21:57:55,470 main.py:47] epoch 26, training loss: 9049.55, average training loss: 31724.61, base loss: 5132.02
[INFO 2017-06-26 21:57:55,835 main.py:47] epoch 27, training loss: 8997.68, average training loss: 30912.93, base loss: 5226.77
[INFO 2017-06-26 21:57:56,208 main.py:47] epoch 28, training loss: 5370.86, average training loss: 30032.17, base loss: 5198.74
[INFO 2017-06-26 21:57:56,583 main.py:47] epoch 29, training loss: 5390.46, average training loss: 29210.78, base loss: 5175.27
[INFO 2017-06-26 21:57:56,958 main.py:47] epoch 30, training loss: 5416.43, average training loss: 28443.22, base loss: 5159.28
[INFO 2017-06-26 21:57:57,343 main.py:47] epoch 31, training loss: 4963.89, average training loss: 27709.49, base loss: 5132.78
[INFO 2017-06-26 21:57:57,734 main.py:47] epoch 32, training loss: 6209.62, average training loss: 27057.98, base loss: 5150.74
[INFO 2017-06-26 21:57:58,144 main.py:47] epoch 33, training loss: 5238.79, average training loss: 26416.24, base loss: 5139.00
[INFO 2017-06-26 21:57:58,549 main.py:47] epoch 34, training loss: 4216.13, average training loss: 25781.95, base loss: 5097.11
[INFO 2017-06-26 21:57:58,952 main.py:47] epoch 35, training loss: 4498.42, average training loss: 25190.74, base loss: 5068.92
[INFO 2017-06-26 21:57:59,355 main.py:47] epoch 36, training loss: 4944.48, average training loss: 24643.55, base loss: 5055.29
[INFO 2017-06-26 21:57:59,759 main.py:47] epoch 37, training loss: 4710.77, average training loss: 24119.00, base loss: 5036.21
[INFO 2017-06-26 21:58:00,159 main.py:47] epoch 38, training loss: 4453.57, average training loss: 23614.76, base loss: 5013.26
[INFO 2017-06-26 21:58:00,560 main.py:47] epoch 39, training loss: 4359.47, average training loss: 23133.38, base loss: 4988.69
[INFO 2017-06-26 21:58:00,962 main.py:47] epoch 40, training loss: 5129.29, average training loss: 22694.25, base loss: 4985.44
[INFO 2017-06-26 21:58:01,365 main.py:47] epoch 41, training loss: 4434.38, average training loss: 22259.49, base loss: 4964.95
[INFO 2017-06-26 21:58:01,767 main.py:47] epoch 42, training loss: 8197.88, average training loss: 21932.48, base loss: 5033.56
[INFO 2017-06-26 21:58:02,169 main.py:47] epoch 43, training loss: 4648.61, average training loss: 21539.66, base loss: 5018.96
[INFO 2017-06-26 21:58:02,569 main.py:47] epoch 44, training loss: 5214.93, average training loss: 21176.89, base loss: 5018.50
[INFO 2017-06-26 21:58:02,968 main.py:47] epoch 45, training loss: 4509.59, average training loss: 20814.56, base loss: 5002.30
[INFO 2017-06-26 21:58:03,364 main.py:47] epoch 46, training loss: 4988.28, average training loss: 20477.83, base loss: 4998.12
[INFO 2017-06-26 21:58:03,767 main.py:47] epoch 47, training loss: 4071.59, average training loss: 20136.03, base loss: 4973.39
[INFO 2017-06-26 21:58:04,166 main.py:47] epoch 48, training loss: 4406.22, average training loss: 19815.02, base loss: 4957.35
[INFO 2017-06-26 21:58:04,565 main.py:47] epoch 49, training loss: 4923.95, average training loss: 19517.19, base loss: 4952.75
[INFO 2017-06-26 21:58:04,968 main.py:47] epoch 50, training loss: 4199.20, average training loss: 19216.84, base loss: 4934.17
[INFO 2017-06-26 21:58:05,371 main.py:47] epoch 51, training loss: 4652.21, average training loss: 18936.75, base loss: 4925.56
[INFO 2017-06-26 21:58:05,770 main.py:47] epoch 52, training loss: 4416.26, average training loss: 18662.78, base loss: 4912.87
[INFO 2017-06-26 21:58:06,174 main.py:47] epoch 53, training loss: 5636.88, average training loss: 18421.56, base loss: 4923.44
[INFO 2017-06-26 21:58:06,576 main.py:47] epoch 54, training loss: 4819.31, average training loss: 18174.25, base loss: 4918.54
[INFO 2017-06-26 21:58:06,984 main.py:47] epoch 55, training loss: 5240.68, average training loss: 17943.29, base loss: 4921.82
[INFO 2017-06-26 21:58:07,383 main.py:47] epoch 56, training loss: 5034.83, average training loss: 17716.83, base loss: 4921.51
[INFO 2017-06-26 21:58:07,783 main.py:47] epoch 57, training loss: 4772.92, average training loss: 17493.66, base loss: 4916.96
[INFO 2017-06-26 21:58:08,187 main.py:47] epoch 58, training loss: 4540.54, average training loss: 17274.11, base loss: 4908.44
[INFO 2017-06-26 21:58:08,590 main.py:47] epoch 59, training loss: 5204.29, average training loss: 17072.95, base loss: 4911.18
[INFO 2017-06-26 21:58:08,991 main.py:47] epoch 60, training loss: 4831.21, average training loss: 16872.26, base loss: 4908.11
[INFO 2017-06-26 21:58:09,395 main.py:47] epoch 61, training loss: 5428.20, average training loss: 16687.68, base loss: 4915.49
[INFO 2017-06-26 21:58:09,799 main.py:47] epoch 62, training loss: 4968.48, average training loss: 16501.66, base loss: 4914.86
[INFO 2017-06-26 21:58:10,200 main.py:47] epoch 63, training loss: 5151.87, average training loss: 16324.32, base loss: 4916.70
[INFO 2017-06-26 21:58:10,600 main.py:47] epoch 64, training loss: 5315.11, average training loss: 16154.95, base loss: 4921.41
[INFO 2017-06-26 21:58:11,004 main.py:47] epoch 65, training loss: 4565.58, average training loss: 15979.35, base loss: 4914.51
[INFO 2017-06-26 21:58:11,408 main.py:47] epoch 66, training loss: 4163.25, average training loss: 15802.99, base loss: 4901.26
[INFO 2017-06-26 21:58:11,812 main.py:47] epoch 67, training loss: 4583.93, average training loss: 15638.01, base loss: 4895.18
[INFO 2017-06-26 21:58:12,215 main.py:47] epoch 68, training loss: 4849.81, average training loss: 15481.66, base loss: 4893.09
[INFO 2017-06-26 21:58:12,617 main.py:47] epoch 69, training loss: 4945.81, average training loss: 15331.14, base loss: 4892.96
[INFO 2017-06-26 21:58:13,016 main.py:47] epoch 70, training loss: 4395.53, average training loss: 15177.12, base loss: 4884.57
[INFO 2017-06-26 21:58:13,416 main.py:47] epoch 71, training loss: 4410.41, average training loss: 15027.58, base loss: 4876.79
[INFO 2017-06-26 21:58:13,818 main.py:47] epoch 72, training loss: 4185.57, average training loss: 14879.06, base loss: 4864.99
[INFO 2017-06-26 21:58:14,224 main.py:47] epoch 73, training loss: 4663.33, average training loss: 14741.01, base loss: 4861.52
[INFO 2017-06-26 21:58:14,625 main.py:47] epoch 74, training loss: 4810.94, average training loss: 14608.61, base loss: 4860.30
[INFO 2017-06-26 21:58:15,036 main.py:47] epoch 75, training loss: 4692.11, average training loss: 14478.13, base loss: 4857.93
[INFO 2017-06-26 21:58:15,436 main.py:47] epoch 76, training loss: 4738.63, average training loss: 14351.64, base loss: 4855.61
[INFO 2017-06-26 21:58:15,840 main.py:47] epoch 77, training loss: 4518.89, average training loss: 14225.58, base loss: 4850.56
[INFO 2017-06-26 21:58:16,247 main.py:47] epoch 78, training loss: 4959.00, average training loss: 14108.29, base loss: 4851.32
[INFO 2017-06-26 21:58:16,654 main.py:47] epoch 79, training loss: 4152.94, average training loss: 13983.84, base loss: 4841.69
[INFO 2017-06-26 21:58:17,056 main.py:47] epoch 80, training loss: 3845.46, average training loss: 13858.68, base loss: 4827.78
[INFO 2017-06-26 21:58:17,458 main.py:47] epoch 81, training loss: 4508.58, average training loss: 13744.65, base loss: 4823.08
[INFO 2017-06-26 21:58:17,860 main.py:47] epoch 82, training loss: 4979.19, average training loss: 13639.04, base loss: 4825.65
[INFO 2017-06-26 21:58:18,265 main.py:47] epoch 83, training loss: 5101.52, average training loss: 13537.41, base loss: 4828.37
[INFO 2017-06-26 21:58:18,665 main.py:47] epoch 84, training loss: 4733.78, average training loss: 13433.84, base loss: 4827.14
[INFO 2017-06-26 21:58:19,064 main.py:47] epoch 85, training loss: 4463.98, average training loss: 13329.53, base loss: 4822.30
[INFO 2017-06-26 21:58:19,467 main.py:47] epoch 86, training loss: 4447.97, average training loss: 13227.45, base loss: 4817.97
[INFO 2017-06-26 21:58:19,869 main.py:47] epoch 87, training loss: 3687.68, average training loss: 13119.04, base loss: 4803.94
[INFO 2017-06-26 21:58:20,267 main.py:47] epoch 88, training loss: 4565.82, average training loss: 13022.94, base loss: 4800.12
[INFO 2017-06-26 21:58:20,669 main.py:47] epoch 89, training loss: 4283.76, average training loss: 12925.84, base loss: 4794.57
[INFO 2017-06-26 21:58:21,067 main.py:47] epoch 90, training loss: 4307.39, average training loss: 12831.13, base loss: 4788.75
[INFO 2017-06-26 21:58:21,468 main.py:47] epoch 91, training loss: 4917.04, average training loss: 12745.10, base loss: 4789.36
[INFO 2017-06-26 21:58:21,867 main.py:47] epoch 92, training loss: 5568.03, average training loss: 12667.93, base loss: 4798.61
[INFO 2017-06-26 21:58:22,266 main.py:47] epoch 93, training loss: 4447.72, average training loss: 12580.48, base loss: 4793.92
[INFO 2017-06-26 21:58:22,669 main.py:47] epoch 94, training loss: 3993.26, average training loss: 12490.09, base loss: 4785.25
[INFO 2017-06-26 21:58:23,071 main.py:47] epoch 95, training loss: 5525.32, average training loss: 12417.54, base loss: 4794.23
[INFO 2017-06-26 21:58:23,470 main.py:47] epoch 96, training loss: 4206.66, average training loss: 12332.89, base loss: 4787.64
[INFO 2017-06-26 21:58:23,868 main.py:47] epoch 97, training loss: 4512.00, average training loss: 12253.09, base loss: 4785.32
[INFO 2017-06-26 21:58:24,267 main.py:47] epoch 98, training loss: 4237.84, average training loss: 12172.13, base loss: 4779.96
[INFO 2017-06-26 21:58:24,669 main.py:47] epoch 99, training loss: 4230.14, average training loss: 12092.71, base loss: 4774.85
[INFO 2017-06-26 21:58:24,669 main.py:49] epoch 99, testing
[INFO 2017-06-26 21:58:26,329 main.py:102] average testing loss: 4534.61, base loss: 4556.61
[INFO 2017-06-26 21:58:26,329 main.py:103] improve_loss: 22.00, improve_percent: 0.00
[INFO 2017-06-26 21:58:26,330 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 21:58:26,342 main.py:73] current best improved percent: 0.00
[INFO 2017-06-26 21:58:26,739 main.py:47] epoch 100, training loss: 4528.52, average training loss: 12017.81, base loss: 4773.02
[INFO 2017-06-26 21:58:27,135 main.py:47] epoch 101, training loss: 4529.48, average training loss: 11944.40, base loss: 4770.88
[INFO 2017-06-26 21:58:27,536 main.py:47] epoch 102, training loss: 3658.64, average training loss: 11863.95, base loss: 4759.31
[INFO 2017-06-26 21:58:27,934 main.py:47] epoch 103, training loss: 4273.47, average training loss: 11790.97, base loss: 4755.16
[INFO 2017-06-26 21:58:28,337 main.py:47] epoch 104, training loss: 4446.79, average training loss: 11721.02, base loss: 4752.24
[INFO 2017-06-26 21:58:28,741 main.py:47] epoch 105, training loss: 4137.21, average training loss: 11649.48, base loss: 4746.36
[INFO 2017-06-26 21:58:29,143 main.py:47] epoch 106, training loss: 4083.65, average training loss: 11578.77, base loss: 4740.39
[INFO 2017-06-26 21:58:29,544 main.py:47] epoch 107, training loss: 4350.21, average training loss: 11511.84, base loss: 4737.20
[INFO 2017-06-26 21:58:29,948 main.py:47] epoch 108, training loss: 4645.90, average training loss: 11448.85, base loss: 4735.88
[INFO 2017-06-26 21:58:30,347 main.py:47] epoch 109, training loss: 3774.08, average training loss: 11379.08, base loss: 4727.04
[INFO 2017-06-26 21:58:30,755 main.py:47] epoch 110, training loss: 3312.34, average training loss: 11306.40, base loss: 4713.27
[INFO 2017-06-26 21:58:31,160 main.py:47] epoch 111, training loss: 4483.76, average training loss: 11245.49, base loss: 4711.34
[INFO 2017-06-26 21:58:31,562 main.py:47] epoch 112, training loss: 8034.74, average training loss: 11217.07, base loss: 4741.07
[INFO 2017-06-26 21:58:31,969 main.py:47] epoch 113, training loss: 4782.44, average training loss: 11160.63, base loss: 4741.54
[INFO 2017-06-26 21:58:32,374 main.py:47] epoch 114, training loss: 4731.28, average training loss: 11104.72, base loss: 4741.54
[INFO 2017-06-26 21:58:32,774 main.py:47] epoch 115, training loss: 4111.91, average training loss: 11044.44, base loss: 4736.59
[INFO 2017-06-26 21:58:33,172 main.py:47] epoch 116, training loss: 4117.11, average training loss: 10985.23, base loss: 4732.23
[INFO 2017-06-26 21:58:33,571 main.py:47] epoch 117, training loss: 4755.44, average training loss: 10932.44, base loss: 4732.49
[INFO 2017-06-26 21:58:33,970 main.py:47] epoch 118, training loss: 7848.60, average training loss: 10906.52, base loss: 4758.98
[INFO 2017-06-26 21:58:34,370 main.py:47] epoch 119, training loss: 4615.52, average training loss: 10854.10, base loss: 4758.04
[INFO 2017-06-26 21:58:34,770 main.py:47] epoch 120, training loss: 4522.19, average training loss: 10801.77, base loss: 4757.03
[INFO 2017-06-26 21:58:35,168 main.py:47] epoch 121, training loss: 3639.36, average training loss: 10743.06, base loss: 4747.89
[INFO 2017-06-26 21:58:35,569 main.py:47] epoch 122, training loss: 4095.68, average training loss: 10689.02, base loss: 4742.30
[INFO 2017-06-26 21:58:35,967 main.py:47] epoch 123, training loss: 3659.35, average training loss: 10632.32, base loss: 4733.92
[INFO 2017-06-26 21:58:36,365 main.py:47] epoch 124, training loss: 4720.58, average training loss: 10585.03, base loss: 4734.41
[INFO 2017-06-26 21:58:36,763 main.py:47] epoch 125, training loss: 4762.68, average training loss: 10538.82, base loss: 4735.17
[INFO 2017-06-26 21:58:37,165 main.py:47] epoch 126, training loss: 4306.03, average training loss: 10489.74, base loss: 4732.49
[INFO 2017-06-26 21:58:37,564 main.py:47] epoch 127, training loss: 4685.86, average training loss: 10444.40, base loss: 4733.25
[INFO 2017-06-26 21:58:37,962 main.py:47] epoch 128, training loss: 3577.02, average training loss: 10391.17, base loss: 4723.77
[INFO 2017-06-26 21:58:38,366 main.py:47] epoch 129, training loss: 4388.33, average training loss: 10344.99, base loss: 4721.71
[INFO 2017-06-26 21:58:38,772 main.py:47] epoch 130, training loss: 5405.43, average training loss: 10307.28, base loss: 4728.01
[INFO 2017-06-26 21:58:39,176 main.py:47] epoch 131, training loss: 5000.99, average training loss: 10267.08, base loss: 4731.60
[INFO 2017-06-26 21:58:39,582 main.py:47] epoch 132, training loss: 3877.29, average training loss: 10219.04, base loss: 4725.75
[INFO 2017-06-26 21:58:39,989 main.py:47] epoch 133, training loss: 4675.49, average training loss: 10177.67, base loss: 4726.27
[INFO 2017-06-26 21:58:40,387 main.py:47] epoch 134, training loss: 4057.69, average training loss: 10132.34, base loss: 4721.43
[INFO 2017-06-26 21:58:40,787 main.py:47] epoch 135, training loss: 4497.14, average training loss: 10090.90, base loss: 4720.26
[INFO 2017-06-26 21:58:41,190 main.py:47] epoch 136, training loss: 4583.17, average training loss: 10050.70, base loss: 4720.38
[INFO 2017-06-26 21:58:41,595 main.py:47] epoch 137, training loss: 3734.19, average training loss: 10004.93, base loss: 4713.18
[INFO 2017-06-26 21:58:41,997 main.py:47] epoch 138, training loss: 3812.96, average training loss: 9960.38, base loss: 4707.03
[INFO 2017-06-26 21:58:42,405 main.py:47] epoch 139, training loss: 3692.72, average training loss: 9915.61, base loss: 4699.82
[INFO 2017-06-26 21:58:42,813 main.py:47] epoch 140, training loss: 3944.96, average training loss: 9873.27, base loss: 4694.98
[INFO 2017-06-26 21:58:43,217 main.py:47] epoch 141, training loss: 4234.68, average training loss: 9833.56, base loss: 4691.85
[INFO 2017-06-26 21:58:43,619 main.py:47] epoch 142, training loss: 3891.50, average training loss: 9792.01, base loss: 4686.47
[INFO 2017-06-26 21:58:44,022 main.py:47] epoch 143, training loss: 3778.96, average training loss: 9750.25, base loss: 4680.35
[INFO 2017-06-26 21:58:44,425 main.py:47] epoch 144, training loss: 4303.07, average training loss: 9712.68, base loss: 4678.86
[INFO 2017-06-26 21:58:44,828 main.py:47] epoch 145, training loss: 4615.24, average training loss: 9677.77, base loss: 4679.47
[INFO 2017-06-26 21:58:45,236 main.py:47] epoch 146, training loss: 7968.35, average training loss: 9666.14, base loss: 4702.46
[INFO 2017-06-26 21:58:45,636 main.py:47] epoch 147, training loss: 4204.46, average training loss: 9629.24, base loss: 4699.98
[INFO 2017-06-26 21:58:46,036 main.py:47] epoch 148, training loss: 3876.04, average training loss: 9590.62, base loss: 4694.58
[INFO 2017-06-26 21:58:46,439 main.py:47] epoch 149, training loss: 4033.39, average training loss: 9553.58, base loss: 4690.60
[INFO 2017-06-26 21:58:46,860 main.py:47] epoch 150, training loss: 4054.03, average training loss: 9517.16, base loss: 4686.71
[INFO 2017-06-26 21:58:47,262 main.py:47] epoch 151, training loss: 4854.67, average training loss: 9486.48, base loss: 4688.36
[INFO 2017-06-26 21:58:47,677 main.py:47] epoch 152, training loss: 4019.77, average training loss: 9450.75, base loss: 4684.27
[INFO 2017-06-26 21:58:48,081 main.py:47] epoch 153, training loss: 4099.91, average training loss: 9416.01, base loss: 4681.37
[INFO 2017-06-26 21:58:48,485 main.py:47] epoch 154, training loss: 3837.83, average training loss: 9380.02, base loss: 4676.18
[INFO 2017-06-26 21:58:48,890 main.py:47] epoch 155, training loss: 4183.21, average training loss: 9346.70, base loss: 4673.85
[INFO 2017-06-26 21:58:49,293 main.py:47] epoch 156, training loss: 3917.26, average training loss: 9312.12, base loss: 4670.05
[INFO 2017-06-26 21:58:49,697 main.py:47] epoch 157, training loss: 4426.16, average training loss: 9281.20, base loss: 4669.10
[INFO 2017-06-26 21:58:50,101 main.py:47] epoch 158, training loss: 8374.48, average training loss: 9275.50, base loss: 4693.19
[INFO 2017-06-26 21:58:50,505 main.py:47] epoch 159, training loss: 4358.59, average training loss: 9244.76, base loss: 4692.32
[INFO 2017-06-26 21:58:50,908 main.py:47] epoch 160, training loss: 4735.23, average training loss: 9216.76, base loss: 4693.98
[INFO 2017-06-26 21:58:51,312 main.py:47] epoch 161, training loss: 3461.94, average training loss: 9181.23, base loss: 4686.65
[INFO 2017-06-26 21:58:51,716 main.py:47] epoch 162, training loss: 4244.80, average training loss: 9150.95, base loss: 4684.45
[INFO 2017-06-26 21:58:52,121 main.py:47] epoch 163, training loss: 4425.68, average training loss: 9122.13, base loss: 4683.78
[INFO 2017-06-26 21:58:52,530 main.py:47] epoch 164, training loss: 4195.96, average training loss: 9092.28, base loss: 4681.74
[INFO 2017-06-26 21:58:52,934 main.py:47] epoch 165, training loss: 3940.91, average training loss: 9061.25, base loss: 4677.93
[INFO 2017-06-26 21:58:53,339 main.py:47] epoch 166, training loss: 4065.34, average training loss: 9031.33, base loss: 4675.32
[INFO 2017-06-26 21:58:53,750 main.py:47] epoch 167, training loss: 3738.34, average training loss: 8999.82, base loss: 4670.78
[INFO 2017-06-26 21:58:54,154 main.py:47] epoch 168, training loss: 4959.49, average training loss: 8975.92, base loss: 4674.21
[INFO 2017-06-26 21:58:54,556 main.py:47] epoch 169, training loss: 4238.79, average training loss: 8948.05, base loss: 4672.80
[INFO 2017-06-26 21:58:54,960 main.py:47] epoch 170, training loss: 4642.61, average training loss: 8922.87, base loss: 4673.46
[INFO 2017-06-26 21:58:55,363 main.py:47] epoch 171, training loss: 3924.80, average training loss: 8893.82, base loss: 4669.49
[INFO 2017-06-26 21:58:55,768 main.py:47] epoch 172, training loss: 4234.00, average training loss: 8866.88, base loss: 4667.66
[INFO 2017-06-26 21:58:56,171 main.py:47] epoch 173, training loss: 4325.01, average training loss: 8840.78, base loss: 4666.70
[INFO 2017-06-26 21:58:56,573 main.py:47] epoch 174, training loss: 3929.78, average training loss: 8812.71, base loss: 4662.93
[INFO 2017-06-26 21:58:56,976 main.py:47] epoch 175, training loss: 3802.83, average training loss: 8784.25, base loss: 4658.40
[INFO 2017-06-26 21:58:57,380 main.py:47] epoch 176, training loss: 4455.23, average training loss: 8759.79, base loss: 4658.06
[INFO 2017-06-26 21:58:57,784 main.py:47] epoch 177, training loss: 4047.38, average training loss: 8733.32, base loss: 4655.91
[INFO 2017-06-26 21:58:58,190 main.py:47] epoch 178, training loss: 3484.85, average training loss: 8704.00, base loss: 4649.76
[INFO 2017-06-26 21:58:58,593 main.py:47] epoch 179, training loss: 4621.16, average training loss: 8681.31, base loss: 4651.09
[INFO 2017-06-26 21:58:59,000 main.py:47] epoch 180, training loss: 4075.01, average training loss: 8655.86, base loss: 4648.87
[INFO 2017-06-26 21:58:59,410 main.py:47] epoch 181, training loss: 3968.77, average training loss: 8630.11, base loss: 4645.94
[INFO 2017-06-26 21:58:59,814 main.py:47] epoch 182, training loss: 3785.91, average training loss: 8603.64, base loss: 4641.25
[INFO 2017-06-26 21:59:00,218 main.py:47] epoch 183, training loss: 7602.48, average training loss: 8598.20, base loss: 4658.18
[INFO 2017-06-26 21:59:00,621 main.py:47] epoch 184, training loss: 4276.56, average training loss: 8574.84, base loss: 4657.09
[INFO 2017-06-26 21:59:01,024 main.py:47] epoch 185, training loss: 3659.31, average training loss: 8548.41, base loss: 4653.03
[INFO 2017-06-26 21:59:01,433 main.py:47] epoch 186, training loss: 3863.25, average training loss: 8523.36, base loss: 4649.62
[INFO 2017-06-26 21:59:01,835 main.py:47] epoch 187, training loss: 3935.51, average training loss: 8498.95, base loss: 4646.77
[INFO 2017-06-26 21:59:02,238 main.py:47] epoch 188, training loss: 7817.80, average training loss: 8495.35, base loss: 4664.37
[INFO 2017-06-26 21:59:02,641 main.py:47] epoch 189, training loss: 3546.11, average training loss: 8469.30, base loss: 4658.88
[INFO 2017-06-26 21:59:03,043 main.py:47] epoch 190, training loss: 3698.28, average training loss: 8444.32, base loss: 4654.48
[INFO 2017-06-26 21:59:03,446 main.py:47] epoch 191, training loss: 4025.17, average training loss: 8421.31, base loss: 4651.77
[INFO 2017-06-26 21:59:03,848 main.py:47] epoch 192, training loss: 4900.14, average training loss: 8403.06, base loss: 4653.88
[INFO 2017-06-26 21:59:04,252 main.py:47] epoch 193, training loss: 4278.52, average training loss: 8381.80, base loss: 4652.53
[INFO 2017-06-26 21:59:04,654 main.py:47] epoch 194, training loss: 4722.54, average training loss: 8363.04, base loss: 4654.02
[INFO 2017-06-26 21:59:05,057 main.py:47] epoch 195, training loss: 4299.01, average training loss: 8342.30, base loss: 4653.29
[INFO 2017-06-26 21:59:05,459 main.py:47] epoch 196, training loss: 4274.05, average training loss: 8321.65, base loss: 4652.52
[INFO 2017-06-26 21:59:05,862 main.py:47] epoch 197, training loss: 5173.26, average training loss: 8305.75, base loss: 4656.53
[INFO 2017-06-26 21:59:06,265 main.py:47] epoch 198, training loss: 3964.82, average training loss: 8283.93, base loss: 4653.54
[INFO 2017-06-26 21:59:06,668 main.py:47] epoch 199, training loss: 4214.91, average training loss: 8263.59, base loss: 4651.77
[INFO 2017-06-26 21:59:06,668 main.py:49] epoch 199, testing
[INFO 2017-06-26 21:59:08,309 main.py:102] average testing loss: 4593.13, base loss: 4804.00
[INFO 2017-06-26 21:59:08,309 main.py:103] improve_loss: 210.86, improve_percent: 0.04
[INFO 2017-06-26 21:59:08,310 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 21:59:08,322 main.py:73] current best improved percent: 0.04
[INFO 2017-06-26 21:59:08,731 main.py:47] epoch 200, training loss: 4562.90, average training loss: 8245.18, base loss: 4652.05
[INFO 2017-06-26 21:59:09,134 main.py:47] epoch 201, training loss: 4704.33, average training loss: 8227.65, base loss: 4653.42
[INFO 2017-06-26 21:59:09,538 main.py:47] epoch 202, training loss: 4824.74, average training loss: 8210.89, base loss: 4655.09
[INFO 2017-06-26 21:59:09,948 main.py:47] epoch 203, training loss: 3982.19, average training loss: 8190.16, base loss: 4652.28
[INFO 2017-06-26 21:59:10,350 main.py:47] epoch 204, training loss: 3789.18, average training loss: 8168.69, base loss: 4649.13
[INFO 2017-06-26 21:59:10,755 main.py:47] epoch 205, training loss: 4396.32, average training loss: 8150.38, base loss: 4649.19
[INFO 2017-06-26 21:59:11,158 main.py:47] epoch 206, training loss: 3500.96, average training loss: 8127.92, base loss: 4643.90
[INFO 2017-06-26 21:59:11,561 main.py:47] epoch 207, training loss: 4221.82, average training loss: 8109.14, base loss: 4642.95
[INFO 2017-06-26 21:59:11,965 main.py:47] epoch 208, training loss: 4021.20, average training loss: 8089.58, base loss: 4640.93
[INFO 2017-06-26 21:59:12,372 main.py:47] epoch 209, training loss: 4326.04, average training loss: 8071.66, base loss: 4640.56
[INFO 2017-06-26 21:59:12,778 main.py:47] epoch 210, training loss: 4534.87, average training loss: 8054.89, base loss: 4641.51
[INFO 2017-06-26 21:59:13,181 main.py:47] epoch 211, training loss: 4286.96, average training loss: 8037.12, base loss: 4640.89
[INFO 2017-06-26 21:59:13,583 main.py:47] epoch 212, training loss: 3922.20, average training loss: 8017.80, base loss: 4638.46
[INFO 2017-06-26 21:59:13,987 main.py:47] epoch 213, training loss: 3676.01, average training loss: 7997.51, base loss: 4634.46
[INFO 2017-06-26 21:59:14,394 main.py:47] epoch 214, training loss: 4617.56, average training loss: 7981.79, base loss: 4635.94
[INFO 2017-06-26 21:59:14,797 main.py:47] epoch 215, training loss: 4007.23, average training loss: 7963.39, base loss: 4634.14
[INFO 2017-06-26 21:59:15,199 main.py:47] epoch 216, training loss: 4293.58, average training loss: 7946.48, base loss: 4634.14
[INFO 2017-06-26 21:59:15,603 main.py:47] epoch 217, training loss: 4611.65, average training loss: 7931.18, base loss: 4635.31
[INFO 2017-06-26 21:59:16,007 main.py:47] epoch 218, training loss: 3919.99, average training loss: 7912.87, base loss: 4633.13
[INFO 2017-06-26 21:59:16,410 main.py:47] epoch 219, training loss: 4043.81, average training loss: 7895.28, base loss: 4631.24
[INFO 2017-06-26 21:59:16,815 main.py:47] epoch 220, training loss: 4149.06, average training loss: 7878.33, base loss: 4630.62
[INFO 2017-06-26 21:59:17,218 main.py:47] epoch 221, training loss: 3838.38, average training loss: 7860.13, base loss: 4627.47
[INFO 2017-06-26 21:59:17,619 main.py:47] epoch 222, training loss: 4422.84, average training loss: 7844.72, base loss: 4627.51
[INFO 2017-06-26 21:59:18,021 main.py:47] epoch 223, training loss: 4108.00, average training loss: 7828.03, base loss: 4626.07
[INFO 2017-06-26 21:59:18,427 main.py:47] epoch 224, training loss: 4695.51, average training loss: 7814.11, base loss: 4627.75
[INFO 2017-06-26 21:59:18,829 main.py:47] epoch 225, training loss: 3821.91, average training loss: 7796.45, base loss: 4624.89
[INFO 2017-06-26 21:59:19,242 main.py:47] epoch 226, training loss: 4008.74, average training loss: 7779.76, base loss: 4623.24
[INFO 2017-06-26 21:59:19,643 main.py:47] epoch 227, training loss: 4033.01, average training loss: 7763.33, base loss: 4621.76
[INFO 2017-06-26 21:59:20,046 main.py:47] epoch 228, training loss: 4327.40, average training loss: 7748.32, base loss: 4621.97
[INFO 2017-06-26 21:59:20,448 main.py:47] epoch 229, training loss: 4471.46, average training loss: 7734.08, base loss: 4622.89
[INFO 2017-06-26 21:59:20,853 main.py:47] epoch 230, training loss: 4903.00, average training loss: 7721.82, base loss: 4624.96
[INFO 2017-06-26 21:59:21,262 main.py:47] epoch 231, training loss: 7699.90, average training loss: 7721.73, base loss: 4639.21
[INFO 2017-06-26 21:59:21,664 main.py:47] epoch 232, training loss: 3682.45, average training loss: 7704.39, base loss: 4635.60
[INFO 2017-06-26 21:59:22,066 main.py:47] epoch 233, training loss: 7274.36, average training loss: 7702.55, base loss: 4647.64
[INFO 2017-06-26 21:59:22,468 main.py:47] epoch 234, training loss: 4154.71, average training loss: 7687.46, base loss: 4646.71
[INFO 2017-06-26 21:59:22,871 main.py:47] epoch 235, training loss: 4153.93, average training loss: 7672.48, base loss: 4645.94
[INFO 2017-06-26 21:59:23,273 main.py:47] epoch 236, training loss: 3967.48, average training loss: 7656.85, base loss: 4644.21
[INFO 2017-06-26 21:59:23,675 main.py:47] epoch 237, training loss: 3597.53, average training loss: 7639.79, base loss: 4640.59
[INFO 2017-06-26 21:59:24,078 main.py:47] epoch 238, training loss: 3907.29, average training loss: 7624.18, base loss: 4638.45
[INFO 2017-06-26 21:59:24,481 main.py:47] epoch 239, training loss: 3426.00, average training loss: 7606.69, base loss: 4633.96
[INFO 2017-06-26 21:59:24,883 main.py:47] epoch 240, training loss: 4340.78, average training loss: 7593.13, base loss: 4633.89
[INFO 2017-06-26 21:59:25,287 main.py:47] epoch 241, training loss: 4186.69, average training loss: 7579.06, base loss: 4632.82
[INFO 2017-06-26 21:59:25,690 main.py:47] epoch 242, training loss: 4319.47, average training loss: 7565.64, base loss: 4632.43
[INFO 2017-06-26 21:59:26,097 main.py:47] epoch 243, training loss: 3967.60, average training loss: 7550.90, base loss: 4630.97
[INFO 2017-06-26 21:59:26,502 main.py:47] epoch 244, training loss: 4661.55, average training loss: 7539.10, base loss: 4632.55
[INFO 2017-06-26 21:59:26,904 main.py:47] epoch 245, training loss: 3907.49, average training loss: 7524.34, base loss: 4630.44
[INFO 2017-06-26 21:59:27,308 main.py:47] epoch 246, training loss: 4121.46, average training loss: 7510.56, base loss: 4628.74
[INFO 2017-06-26 21:59:27,711 main.py:47] epoch 247, training loss: 4116.29, average training loss: 7496.88, base loss: 4628.15
[INFO 2017-06-26 21:59:28,113 main.py:47] epoch 248, training loss: 7858.28, average training loss: 7498.33, base loss: 4642.26
[INFO 2017-06-26 21:59:28,522 main.py:47] epoch 249, training loss: 4019.89, average training loss: 7484.42, base loss: 4640.39
[INFO 2017-06-26 21:59:28,925 main.py:47] epoch 250, training loss: 7490.94, average training loss: 7484.44, base loss: 4652.11
[INFO 2017-06-26 21:59:29,329 main.py:47] epoch 251, training loss: 4806.15, average training loss: 7473.81, base loss: 4654.34
[INFO 2017-06-26 21:59:29,735 main.py:47] epoch 252, training loss: 4080.52, average training loss: 7460.40, base loss: 4653.12
[INFO 2017-06-26 21:59:30,144 main.py:47] epoch 253, training loss: 7350.50, average training loss: 7459.97, base loss: 4664.79
[INFO 2017-06-26 21:59:30,546 main.py:47] epoch 254, training loss: 4106.00, average training loss: 7446.82, base loss: 4664.24
[INFO 2017-06-26 21:59:30,949 main.py:47] epoch 255, training loss: 4019.72, average training loss: 7433.43, base loss: 4662.58
[INFO 2017-06-26 21:59:31,351 main.py:47] epoch 256, training loss: 4043.19, average training loss: 7420.24, base loss: 4661.68
[INFO 2017-06-26 21:59:31,757 main.py:47] epoch 257, training loss: 4085.68, average training loss: 7407.31, base loss: 4660.43
[INFO 2017-06-26 21:59:32,159 main.py:47] epoch 258, training loss: 3551.71, average training loss: 7392.43, base loss: 4657.03
[INFO 2017-06-26 21:59:32,567 main.py:47] epoch 259, training loss: 3648.01, average training loss: 7378.02, base loss: 4653.97
[INFO 2017-06-26 21:59:32,969 main.py:47] epoch 260, training loss: 4582.35, average training loss: 7367.31, base loss: 4655.23
[INFO 2017-06-26 21:59:33,370 main.py:47] epoch 261, training loss: 4117.24, average training loss: 7354.91, base loss: 4654.52
[INFO 2017-06-26 21:59:33,774 main.py:47] epoch 262, training loss: 4351.97, average training loss: 7343.49, base loss: 4655.14
[INFO 2017-06-26 21:59:34,176 main.py:47] epoch 263, training loss: 3915.92, average training loss: 7330.51, base loss: 4653.51
[INFO 2017-06-26 21:59:34,585 main.py:47] epoch 264, training loss: 4556.99, average training loss: 7320.04, base loss: 4654.71
[INFO 2017-06-26 21:59:34,988 main.py:47] epoch 265, training loss: 4096.14, average training loss: 7307.92, base loss: 4653.63
[INFO 2017-06-26 21:59:35,396 main.py:47] epoch 266, training loss: 4799.71, average training loss: 7298.53, base loss: 4655.68
[INFO 2017-06-26 21:59:35,810 main.py:47] epoch 267, training loss: 4099.50, average training loss: 7286.59, base loss: 4654.61
[INFO 2017-06-26 21:59:36,213 main.py:47] epoch 268, training loss: 4363.86, average training loss: 7275.72, base loss: 4655.35
[INFO 2017-06-26 21:59:36,616 main.py:47] epoch 269, training loss: 4248.91, average training loss: 7264.51, base loss: 4654.98
[INFO 2017-06-26 21:59:37,019 main.py:47] epoch 270, training loss: 4575.98, average training loss: 7254.59, base loss: 4656.12
[INFO 2017-06-26 21:59:37,423 main.py:47] epoch 271, training loss: 3706.70, average training loss: 7241.55, base loss: 4653.68
[INFO 2017-06-26 21:59:37,826 main.py:47] epoch 272, training loss: 4259.15, average training loss: 7230.63, base loss: 4653.27
[INFO 2017-06-26 21:59:38,234 main.py:47] epoch 273, training loss: 3803.89, average training loss: 7218.12, base loss: 4651.18
[INFO 2017-06-26 21:59:38,637 main.py:47] epoch 274, training loss: 3708.38, average training loss: 7205.36, base loss: 4647.93
[INFO 2017-06-26 21:59:39,041 main.py:47] epoch 275, training loss: 4640.58, average training loss: 7196.06, base loss: 4648.67
[INFO 2017-06-26 21:59:39,442 main.py:47] epoch 276, training loss: 4580.06, average training loss: 7186.62, base loss: 4649.86
[INFO 2017-06-26 21:59:39,847 main.py:47] epoch 277, training loss: 4563.42, average training loss: 7177.18, base loss: 4650.75
[INFO 2017-06-26 21:59:40,250 main.py:47] epoch 278, training loss: 4394.44, average training loss: 7167.21, base loss: 4651.36
[INFO 2017-06-26 21:59:40,656 main.py:47] epoch 279, training loss: 4811.92, average training loss: 7158.80, base loss: 4653.39
[INFO 2017-06-26 21:59:41,059 main.py:47] epoch 280, training loss: 4089.29, average training loss: 7147.87, base loss: 4652.35
[INFO 2017-06-26 21:59:41,463 main.py:47] epoch 281, training loss: 4133.29, average training loss: 7137.18, base loss: 4651.53
[INFO 2017-06-26 21:59:41,868 main.py:47] epoch 282, training loss: 4308.29, average training loss: 7127.19, base loss: 4651.29
[INFO 2017-06-26 21:59:42,276 main.py:47] epoch 283, training loss: 3751.47, average training loss: 7115.30, base loss: 4648.78
[INFO 2017-06-26 21:59:42,678 main.py:47] epoch 284, training loss: 4771.85, average training loss: 7107.08, base loss: 4650.23
[INFO 2017-06-26 21:59:43,082 main.py:47] epoch 285, training loss: 4567.67, average training loss: 7098.20, base loss: 4651.54
[INFO 2017-06-26 21:59:43,484 main.py:47] epoch 286, training loss: 3879.42, average training loss: 7086.99, base loss: 4649.85
[INFO 2017-06-26 21:59:43,886 main.py:47] epoch 287, training loss: 4169.89, average training loss: 7076.86, base loss: 4649.62
[INFO 2017-06-26 21:59:44,289 main.py:47] epoch 288, training loss: 3545.36, average training loss: 7064.64, base loss: 4646.48
[INFO 2017-06-26 21:59:44,693 main.py:47] epoch 289, training loss: 3960.15, average training loss: 7053.93, base loss: 4645.03
[INFO 2017-06-26 21:59:45,095 main.py:47] epoch 290, training loss: 3830.44, average training loss: 7042.85, base loss: 4643.18
[INFO 2017-06-26 21:59:45,497 main.py:47] epoch 291, training loss: 7901.50, average training loss: 7045.79, base loss: 4655.28
[INFO 2017-06-26 21:59:45,899 main.py:47] epoch 292, training loss: 3568.72, average training loss: 7033.93, base loss: 4652.03
[INFO 2017-06-26 21:59:46,302 main.py:47] epoch 293, training loss: 4685.29, average training loss: 7025.94, base loss: 4653.58
[INFO 2017-06-26 21:59:46,704 main.py:47] epoch 294, training loss: 3905.85, average training loss: 7015.36, base loss: 4652.42
[INFO 2017-06-26 21:59:47,106 main.py:47] epoch 295, training loss: 4363.46, average training loss: 7006.40, base loss: 4653.22
[INFO 2017-06-26 21:59:47,510 main.py:47] epoch 296, training loss: 4433.05, average training loss: 6997.74, base loss: 4653.97
[INFO 2017-06-26 21:59:47,912 main.py:47] epoch 297, training loss: 3881.44, average training loss: 6987.28, base loss: 4651.97
[INFO 2017-06-26 21:59:48,317 main.py:47] epoch 298, training loss: 4102.08, average training loss: 6977.63, base loss: 4651.09
[INFO 2017-06-26 21:59:48,719 main.py:47] epoch 299, training loss: 3632.67, average training loss: 6966.48, base loss: 4648.27
[INFO 2017-06-26 21:59:48,720 main.py:49] epoch 299, testing
[INFO 2017-06-26 21:59:50,361 main.py:102] average testing loss: 4779.53, base loss: 5036.68
[INFO 2017-06-26 21:59:50,361 main.py:103] improve_loss: 257.15, improve_percent: 0.05
[INFO 2017-06-26 21:59:50,362 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 21:59:50,374 main.py:73] current best improved percent: 0.05
[INFO 2017-06-26 21:59:50,783 main.py:47] epoch 300, training loss: 4272.73, average training loss: 6957.53, base loss: 4648.44
[INFO 2017-06-26 21:59:51,192 main.py:47] epoch 301, training loss: 4027.32, average training loss: 6947.83, base loss: 4647.09
[INFO 2017-06-26 21:59:51,595 main.py:47] epoch 302, training loss: 3397.39, average training loss: 6936.11, base loss: 4643.65
[INFO 2017-06-26 21:59:51,999 main.py:47] epoch 303, training loss: 4428.49, average training loss: 6927.86, base loss: 4644.03
[INFO 2017-06-26 21:59:52,401 main.py:47] epoch 304, training loss: 4214.12, average training loss: 6918.97, base loss: 4643.64
[INFO 2017-06-26 21:59:52,808 main.py:47] epoch 305, training loss: 4870.93, average training loss: 6912.27, base loss: 4646.01
[INFO 2017-06-26 21:59:53,211 main.py:47] epoch 306, training loss: 4498.28, average training loss: 6904.41, base loss: 4646.23
[INFO 2017-06-26 21:59:53,620 main.py:47] epoch 307, training loss: 4252.58, average training loss: 6895.80, base loss: 4646.19
[INFO 2017-06-26 21:59:54,026 main.py:47] epoch 308, training loss: 4075.72, average training loss: 6886.67, base loss: 4645.09
[INFO 2017-06-26 21:59:54,432 main.py:47] epoch 309, training loss: 4145.24, average training loss: 6877.83, base loss: 4644.61
[INFO 2017-06-26 21:59:54,839 main.py:47] epoch 310, training loss: 3908.20, average training loss: 6868.28, base loss: 4643.07
[INFO 2017-06-26 21:59:55,242 main.py:47] epoch 311, training loss: 4572.49, average training loss: 6860.92, base loss: 4644.31
[INFO 2017-06-26 21:59:55,647 main.py:47] epoch 312, training loss: 3902.29, average training loss: 6851.47, base loss: 4642.87
[INFO 2017-06-26 21:59:56,050 main.py:47] epoch 313, training loss: 3755.58, average training loss: 6841.61, base loss: 4640.83
[INFO 2017-06-26 21:59:56,452 main.py:47] epoch 314, training loss: 4068.84, average training loss: 6832.81, base loss: 4640.05
[INFO 2017-06-26 21:59:56,855 main.py:47] epoch 315, training loss: 4208.03, average training loss: 6824.50, base loss: 4640.09
[INFO 2017-06-26 21:59:57,258 main.py:47] epoch 316, training loss: 4317.92, average training loss: 6816.60, base loss: 4640.22
[INFO 2017-06-26 21:59:57,663 main.py:47] epoch 317, training loss: 4878.34, average training loss: 6810.50, base loss: 4642.69
[INFO 2017-06-26 21:59:58,065 main.py:47] epoch 318, training loss: 3272.23, average training loss: 6799.41, base loss: 4638.45
[INFO 2017-06-26 21:59:58,472 main.py:47] epoch 319, training loss: 3540.19, average training loss: 6789.22, base loss: 4636.08
[INFO 2017-06-26 21:59:58,874 main.py:47] epoch 320, training loss: 3991.56, average training loss: 6780.51, base loss: 4634.98
[INFO 2017-06-26 21:59:59,277 main.py:47] epoch 321, training loss: 3574.74, average training loss: 6770.55, base loss: 4632.33
[INFO 2017-06-26 21:59:59,680 main.py:47] epoch 322, training loss: 4067.83, average training loss: 6762.18, base loss: 4631.32
[INFO 2017-06-26 22:00:00,083 main.py:47] epoch 323, training loss: 3956.43, average training loss: 6753.53, base loss: 4629.95
[INFO 2017-06-26 22:00:00,491 main.py:47] epoch 324, training loss: 4277.59, average training loss: 6745.91, base loss: 4629.85
[INFO 2017-06-26 22:00:00,893 main.py:47] epoch 325, training loss: 4005.86, average training loss: 6737.50, base loss: 4628.98
[INFO 2017-06-26 22:00:01,295 main.py:47] epoch 326, training loss: 3324.83, average training loss: 6727.07, base loss: 4625.44
[INFO 2017-06-26 22:00:01,697 main.py:47] epoch 327, training loss: 3973.79, average training loss: 6718.67, base loss: 4624.35
[INFO 2017-06-26 22:00:02,105 main.py:47] epoch 328, training loss: 3920.98, average training loss: 6710.17, base loss: 4622.96
[INFO 2017-06-26 22:00:02,507 main.py:47] epoch 329, training loss: 3246.29, average training loss: 6699.67, base loss: 4619.06
[INFO 2017-06-26 22:00:02,909 main.py:47] epoch 330, training loss: 3929.20, average training loss: 6691.30, base loss: 4617.86
[INFO 2017-06-26 22:00:03,312 main.py:47] epoch 331, training loss: 4455.37, average training loss: 6684.57, base loss: 4618.89
[INFO 2017-06-26 22:00:03,714 main.py:47] epoch 332, training loss: 4047.45, average training loss: 6676.65, base loss: 4618.41
[INFO 2017-06-26 22:00:04,121 main.py:47] epoch 333, training loss: 4710.68, average training loss: 6670.76, base loss: 4619.78
[INFO 2017-06-26 22:00:04,524 main.py:47] epoch 334, training loss: 7105.03, average training loss: 6672.06, base loss: 4627.92
[INFO 2017-06-26 22:00:04,930 main.py:47] epoch 335, training loss: 4032.81, average training loss: 6664.20, base loss: 4627.50
[INFO 2017-06-26 22:00:05,333 main.py:47] epoch 336, training loss: 3898.59, average training loss: 6656.00, base loss: 4625.78
[INFO 2017-06-26 22:00:05,737 main.py:47] epoch 337, training loss: 3397.98, average training loss: 6646.36, base loss: 4622.56
[INFO 2017-06-26 22:00:06,141 main.py:47] epoch 338, training loss: 3853.66, average training loss: 6638.12, base loss: 4621.19
[INFO 2017-06-26 22:00:06,544 main.py:47] epoch 339, training loss: 4855.12, average training loss: 6632.87, base loss: 4623.27
[INFO 2017-06-26 22:00:06,949 main.py:47] epoch 340, training loss: 3738.74, average training loss: 6624.39, base loss: 4621.33
[INFO 2017-06-26 22:00:07,350 main.py:47] epoch 341, training loss: 3667.26, average training loss: 6615.74, base loss: 4619.38
[INFO 2017-06-26 22:00:07,758 main.py:47] epoch 342, training loss: 4063.65, average training loss: 6608.30, base loss: 4618.34
[INFO 2017-06-26 22:00:08,163 main.py:47] epoch 343, training loss: 4263.04, average training loss: 6601.48, base loss: 4618.33
[INFO 2017-06-26 22:00:08,567 main.py:47] epoch 344, training loss: 3665.02, average training loss: 6592.97, base loss: 4616.80
[INFO 2017-06-26 22:00:08,970 main.py:47] epoch 345, training loss: 4006.54, average training loss: 6585.50, base loss: 4616.12
[INFO 2017-06-26 22:00:09,371 main.py:47] epoch 346, training loss: 4139.44, average training loss: 6578.45, base loss: 4615.74
[INFO 2017-06-26 22:00:09,773 main.py:47] epoch 347, training loss: 3290.51, average training loss: 6569.00, base loss: 4612.70
[INFO 2017-06-26 22:00:10,176 main.py:47] epoch 348, training loss: 4055.40, average training loss: 6561.80, base loss: 4612.56
[INFO 2017-06-26 22:00:10,578 main.py:47] epoch 349, training loss: 4172.28, average training loss: 6554.97, base loss: 4612.45
[INFO 2017-06-26 22:00:10,986 main.py:47] epoch 350, training loss: 3587.03, average training loss: 6546.51, base loss: 4610.16
[INFO 2017-06-26 22:00:11,390 main.py:47] epoch 351, training loss: 3742.17, average training loss: 6538.55, base loss: 4608.84
[INFO 2017-06-26 22:00:11,793 main.py:47] epoch 352, training loss: 3844.93, average training loss: 6530.92, base loss: 4607.65
[INFO 2017-06-26 22:00:12,196 main.py:47] epoch 353, training loss: 3868.57, average training loss: 6523.40, base loss: 4606.47
[INFO 2017-06-26 22:00:12,599 main.py:47] epoch 354, training loss: 3967.73, average training loss: 6516.20, base loss: 4605.95
[INFO 2017-06-26 22:00:13,002 main.py:47] epoch 355, training loss: 3896.24, average training loss: 6508.84, base loss: 4605.23
[INFO 2017-06-26 22:00:13,405 main.py:47] epoch 356, training loss: 4017.41, average training loss: 6501.86, base loss: 4604.54
[INFO 2017-06-26 22:00:13,808 main.py:47] epoch 357, training loss: 4994.57, average training loss: 6497.65, base loss: 4607.39
[INFO 2017-06-26 22:00:14,210 main.py:47] epoch 358, training loss: 4155.93, average training loss: 6491.12, base loss: 4607.23
[INFO 2017-06-26 22:00:14,613 main.py:47] epoch 359, training loss: 3895.63, average training loss: 6483.92, base loss: 4606.26
[INFO 2017-06-26 22:00:15,016 main.py:47] epoch 360, training loss: 3715.10, average training loss: 6476.25, base loss: 4604.21
[INFO 2017-06-26 22:00:15,418 main.py:47] epoch 361, training loss: 4439.28, average training loss: 6470.62, base loss: 4605.37
[INFO 2017-06-26 22:00:15,823 main.py:47] epoch 362, training loss: 7504.02, average training loss: 6473.47, base loss: 4614.54
[INFO 2017-06-26 22:00:16,227 main.py:47] epoch 363, training loss: 3975.63, average training loss: 6466.60, base loss: 4614.20
[INFO 2017-06-26 22:00:16,629 main.py:47] epoch 364, training loss: 3511.87, average training loss: 6458.51, base loss: 4611.93
[INFO 2017-06-26 22:00:17,032 main.py:47] epoch 365, training loss: 3926.38, average training loss: 6451.59, base loss: 4610.99
[INFO 2017-06-26 22:00:17,436 main.py:47] epoch 366, training loss: 3529.98, average training loss: 6443.63, base loss: 4608.76
[INFO 2017-06-26 22:00:17,842 main.py:47] epoch 367, training loss: 3985.50, average training loss: 6436.95, base loss: 4608.39
[INFO 2017-06-26 22:00:18,244 main.py:47] epoch 368, training loss: 7200.21, average training loss: 6439.02, base loss: 4616.19
[INFO 2017-06-26 22:00:18,650 main.py:47] epoch 369, training loss: 3727.34, average training loss: 6431.69, base loss: 4614.66
[INFO 2017-06-26 22:00:19,053 main.py:47] epoch 370, training loss: 3829.65, average training loss: 6424.67, base loss: 4613.65
[INFO 2017-06-26 22:00:19,455 main.py:47] epoch 371, training loss: 3590.17, average training loss: 6417.06, base loss: 4611.69
[INFO 2017-06-26 22:00:19,857 main.py:47] epoch 372, training loss: 3854.54, average training loss: 6410.19, base loss: 4610.57
[INFO 2017-06-26 22:00:20,261 main.py:47] epoch 373, training loss: 4340.28, average training loss: 6404.65, base loss: 4611.58
[INFO 2017-06-26 22:00:20,664 main.py:47] epoch 374, training loss: 3564.09, average training loss: 6397.08, base loss: 4609.64
[INFO 2017-06-26 22:00:21,068 main.py:47] epoch 375, training loss: 3607.62, average training loss: 6389.66, base loss: 4607.61
[INFO 2017-06-26 22:00:21,474 main.py:47] epoch 376, training loss: 3590.48, average training loss: 6382.23, base loss: 4605.69
[INFO 2017-06-26 22:00:21,877 main.py:47] epoch 377, training loss: 3471.92, average training loss: 6374.53, base loss: 4602.74
[INFO 2017-06-26 22:00:22,284 main.py:47] epoch 378, training loss: 3865.74, average training loss: 6367.91, base loss: 4601.77
[INFO 2017-06-26 22:00:22,687 main.py:47] epoch 379, training loss: 3403.36, average training loss: 6360.11, base loss: 4599.51
[INFO 2017-06-26 22:00:23,090 main.py:47] epoch 380, training loss: 3800.33, average training loss: 6353.39, base loss: 4597.99
[INFO 2017-06-26 22:00:23,496 main.py:47] epoch 381, training loss: 4262.34, average training loss: 6347.92, base loss: 4598.29
[INFO 2017-06-26 22:00:23,900 main.py:47] epoch 382, training loss: 4865.31, average training loss: 6344.05, base loss: 4600.65
[INFO 2017-06-26 22:00:24,303 main.py:47] epoch 383, training loss: 3680.43, average training loss: 6337.11, base loss: 4599.01
[INFO 2017-06-26 22:00:24,714 main.py:47] epoch 384, training loss: 4624.79, average training loss: 6332.66, base loss: 4600.36
[INFO 2017-06-26 22:00:25,118 main.py:47] epoch 385, training loss: 3425.18, average training loss: 6325.13, base loss: 4597.59
[INFO 2017-06-26 22:00:25,520 main.py:47] epoch 386, training loss: 4140.16, average training loss: 6319.49, base loss: 4597.21
[INFO 2017-06-26 22:00:25,924 main.py:47] epoch 387, training loss: 4485.16, average training loss: 6314.76, base loss: 4598.22
[INFO 2017-06-26 22:00:26,327 main.py:47] epoch 388, training loss: 3850.70, average training loss: 6308.42, base loss: 4597.40
[INFO 2017-06-26 22:00:26,731 main.py:47] epoch 389, training loss: 3380.74, average training loss: 6300.92, base loss: 4594.78
[INFO 2017-06-26 22:00:27,134 main.py:47] epoch 390, training loss: 3871.67, average training loss: 6294.70, base loss: 4593.73
[INFO 2017-06-26 22:00:27,541 main.py:47] epoch 391, training loss: 4436.52, average training loss: 6289.96, base loss: 4594.59
[INFO 2017-06-26 22:00:27,945 main.py:47] epoch 392, training loss: 4410.96, average training loss: 6285.18, base loss: 4595.19
[INFO 2017-06-26 22:00:28,348 main.py:47] epoch 393, training loss: 3872.21, average training loss: 6279.06, base loss: 4594.91
[INFO 2017-06-26 22:00:28,751 main.py:47] epoch 394, training loss: 4483.73, average training loss: 6274.51, base loss: 4596.14
[INFO 2017-06-26 22:00:29,154 main.py:47] epoch 395, training loss: 4244.99, average training loss: 6269.39, base loss: 4596.51
[INFO 2017-06-26 22:00:29,556 main.py:47] epoch 396, training loss: 4162.16, average training loss: 6264.08, base loss: 4597.19
[INFO 2017-06-26 22:00:29,959 main.py:47] epoch 397, training loss: 4269.38, average training loss: 6259.07, base loss: 4597.64
[INFO 2017-06-26 22:00:30,362 main.py:47] epoch 398, training loss: 4560.40, average training loss: 6254.81, base loss: 4598.95
[INFO 2017-06-26 22:00:30,765 main.py:47] epoch 399, training loss: 7936.88, average training loss: 6259.02, base loss: 4608.13
[INFO 2017-06-26 22:00:30,765 main.py:49] epoch 399, testing
[INFO 2017-06-26 22:00:32,407 main.py:102] average testing loss: 3933.22, base loss: 4344.85
[INFO 2017-06-26 22:00:32,408 main.py:103] improve_loss: 411.63, improve_percent: 0.09
[INFO 2017-06-26 22:00:32,408 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 22:00:32,421 main.py:73] current best improved percent: 0.09
[INFO 2017-06-26 22:00:32,827 main.py:47] epoch 400, training loss: 3520.04, average training loss: 6252.19, base loss: 4606.18
[INFO 2017-06-26 22:00:33,228 main.py:47] epoch 401, training loss: 3991.97, average training loss: 6246.56, base loss: 4605.74
[INFO 2017-06-26 22:00:33,632 main.py:47] epoch 402, training loss: 3664.68, average training loss: 6240.16, base loss: 4604.68
[INFO 2017-06-26 22:00:34,038 main.py:47] epoch 403, training loss: 4073.75, average training loss: 6234.79, base loss: 4604.40
[INFO 2017-06-26 22:00:34,440 main.py:47] epoch 404, training loss: 3589.82, average training loss: 6228.26, base loss: 4602.85
[INFO 2017-06-26 22:00:34,847 main.py:47] epoch 405, training loss: 4181.36, average training loss: 6223.22, base loss: 4603.03
[INFO 2017-06-26 22:00:35,255 main.py:47] epoch 406, training loss: 4061.25, average training loss: 6217.91, base loss: 4602.77
[INFO 2017-06-26 22:00:35,657 main.py:47] epoch 407, training loss: 7622.48, average training loss: 6221.35, base loss: 4611.27
[INFO 2017-06-26 22:00:36,061 main.py:47] epoch 408, training loss: 3692.21, average training loss: 6215.17, base loss: 4609.95
[INFO 2017-06-26 22:00:36,470 main.py:47] epoch 409, training loss: 3416.22, average training loss: 6208.34, base loss: 4607.98
[INFO 2017-06-26 22:00:36,873 main.py:47] epoch 410, training loss: 4136.50, average training loss: 6203.30, base loss: 4608.02
[INFO 2017-06-26 22:00:37,276 main.py:47] epoch 411, training loss: 7311.56, average training loss: 6205.99, base loss: 4615.68
[INFO 2017-06-26 22:00:37,678 main.py:47] epoch 412, training loss: 4702.41, average training loss: 6202.35, base loss: 4617.74
[INFO 2017-06-26 22:00:38,082 main.py:47] epoch 413, training loss: 7091.29, average training loss: 6204.50, base loss: 4624.39
[INFO 2017-06-26 22:00:38,485 main.py:47] epoch 414, training loss: 4151.86, average training loss: 6199.55, base loss: 4624.54
[INFO 2017-06-26 22:00:38,888 main.py:47] epoch 415, training loss: 3780.66, average training loss: 6193.74, base loss: 4623.56
[INFO 2017-06-26 22:00:39,290 main.py:47] epoch 416, training loss: 3851.44, average training loss: 6188.12, base loss: 4622.78
[INFO 2017-06-26 22:00:39,693 main.py:47] epoch 417, training loss: 3951.39, average training loss: 6182.77, base loss: 4622.24
[INFO 2017-06-26 22:00:40,096 main.py:47] epoch 418, training loss: 3445.90, average training loss: 6176.24, base loss: 4620.21
[INFO 2017-06-26 22:00:40,499 main.py:47] epoch 419, training loss: 3734.94, average training loss: 6170.42, base loss: 4619.27
[INFO 2017-06-26 22:00:40,904 main.py:47] epoch 420, training loss: 3316.36, average training loss: 6163.65, base loss: 4616.82
[INFO 2017-06-26 22:00:41,308 main.py:47] epoch 421, training loss: 3775.62, average training loss: 6157.99, base loss: 4615.98
[INFO 2017-06-26 22:00:41,711 main.py:47] epoch 422, training loss: 3485.02, average training loss: 6151.67, base loss: 4614.13
[INFO 2017-06-26 22:00:42,114 main.py:47] epoch 423, training loss: 4087.98, average training loss: 6146.80, base loss: 4614.29
[INFO 2017-06-26 22:00:42,516 main.py:47] epoch 424, training loss: 3856.70, average training loss: 6141.41, base loss: 4613.54
[INFO 2017-06-26 22:00:42,922 main.py:47] epoch 425, training loss: 4099.73, average training loss: 6136.62, base loss: 4613.19
[INFO 2017-06-26 22:00:43,324 main.py:47] epoch 426, training loss: 4143.13, average training loss: 6131.95, base loss: 4613.64
[INFO 2017-06-26 22:00:43,727 main.py:47] epoch 427, training loss: 3335.99, average training loss: 6125.42, base loss: 4611.30
[INFO 2017-06-26 22:00:44,129 main.py:47] epoch 428, training loss: 4540.77, average training loss: 6121.72, base loss: 4612.55
[INFO 2017-06-26 22:00:44,531 main.py:47] epoch 429, training loss: 4448.08, average training loss: 6117.83, base loss: 4613.94
[INFO 2017-06-26 22:00:44,933 main.py:47] epoch 430, training loss: 3509.20, average training loss: 6111.78, base loss: 4612.13
[INFO 2017-06-26 22:00:45,336 main.py:47] epoch 431, training loss: 4094.71, average training loss: 6107.11, base loss: 4612.20
[INFO 2017-06-26 22:00:45,742 main.py:47] epoch 432, training loss: 4206.77, average training loss: 6102.72, base loss: 4612.48
[INFO 2017-06-26 22:00:46,147 main.py:47] epoch 433, training loss: 3791.47, average training loss: 6097.40, base loss: 4611.49
[INFO 2017-06-26 22:00:46,550 main.py:47] epoch 434, training loss: 3669.67, average training loss: 6091.82, base loss: 4609.98
[INFO 2017-06-26 22:00:46,954 main.py:47] epoch 435, training loss: 3831.37, average training loss: 6086.63, base loss: 4609.32
[INFO 2017-06-26 22:00:47,357 main.py:47] epoch 436, training loss: 4698.02, average training loss: 6083.45, base loss: 4610.69
[INFO 2017-06-26 22:00:47,760 main.py:47] epoch 437, training loss: 3362.34, average training loss: 6077.24, base loss: 4608.56
[INFO 2017-06-26 22:00:48,163 main.py:47] epoch 438, training loss: 3547.01, average training loss: 6071.48, base loss: 4606.59
[INFO 2017-06-26 22:00:48,566 main.py:47] epoch 439, training loss: 3673.74, average training loss: 6066.03, base loss: 4605.30
[INFO 2017-06-26 22:00:48,968 main.py:47] epoch 440, training loss: 3526.75, average training loss: 6060.27, base loss: 4603.60
[INFO 2017-06-26 22:00:49,378 main.py:47] epoch 441, training loss: 3030.87, average training loss: 6053.42, base loss: 4600.62
[INFO 2017-06-26 22:00:49,781 main.py:47] epoch 442, training loss: 3743.90, average training loss: 6048.20, base loss: 4599.59
[INFO 2017-06-26 22:00:50,184 main.py:47] epoch 443, training loss: 3601.55, average training loss: 6042.69, base loss: 4598.41
[INFO 2017-06-26 22:00:50,590 main.py:47] epoch 444, training loss: 3558.41, average training loss: 6037.11, base loss: 4596.93
[INFO 2017-06-26 22:00:50,994 main.py:47] epoch 445, training loss: 4078.39, average training loss: 6032.72, base loss: 4596.77
[INFO 2017-06-26 22:00:51,397 main.py:47] epoch 446, training loss: 3759.52, average training loss: 6027.63, base loss: 4595.95
[INFO 2017-06-26 22:00:51,799 main.py:47] epoch 447, training loss: 3938.18, average training loss: 6022.97, base loss: 4595.62
[INFO 2017-06-26 22:00:52,202 main.py:47] epoch 448, training loss: 4505.43, average training loss: 6019.59, base loss: 4596.64
[INFO 2017-06-26 22:00:52,605 main.py:47] epoch 449, training loss: 4209.24, average training loss: 6015.57, base loss: 4597.20
[INFO 2017-06-26 22:00:53,011 main.py:47] epoch 450, training loss: 4080.46, average training loss: 6011.27, base loss: 4597.13
[INFO 2017-06-26 22:00:53,414 main.py:47] epoch 451, training loss: 4167.69, average training loss: 6007.20, base loss: 4597.68
[INFO 2017-06-26 22:00:53,817 main.py:47] epoch 452, training loss: 4631.74, average training loss: 6004.16, base loss: 4599.40
[INFO 2017-06-26 22:00:54,221 main.py:47] epoch 453, training loss: 3860.40, average training loss: 5999.44, base loss: 4599.15
[INFO 2017-06-26 22:00:54,624 main.py:47] epoch 454, training loss: 4221.44, average training loss: 5995.53, base loss: 4599.38
[INFO 2017-06-26 22:00:55,027 main.py:47] epoch 455, training loss: 4004.71, average training loss: 5991.16, base loss: 4599.38
[INFO 2017-06-26 22:00:55,430 main.py:47] epoch 456, training loss: 4255.81, average training loss: 5987.37, base loss: 4599.76
[INFO 2017-06-26 22:00:55,833 main.py:47] epoch 457, training loss: 4566.16, average training loss: 5984.26, base loss: 4601.03
[INFO 2017-06-26 22:00:56,236 main.py:47] epoch 458, training loss: 3590.45, average training loss: 5979.05, base loss: 4599.91
[INFO 2017-06-26 22:00:56,639 main.py:47] epoch 459, training loss: 3923.24, average training loss: 5974.58, base loss: 4599.75
[INFO 2017-06-26 22:00:57,042 main.py:47] epoch 460, training loss: 7586.03, average training loss: 5978.07, base loss: 4607.40
[INFO 2017-06-26 22:00:57,445 main.py:47] epoch 461, training loss: 4063.76, average training loss: 5973.93, base loss: 4606.84
[INFO 2017-06-26 22:00:57,848 main.py:47] epoch 462, training loss: 3829.24, average training loss: 5969.30, base loss: 4606.38
[INFO 2017-06-26 22:00:58,256 main.py:47] epoch 463, training loss: 4087.57, average training loss: 5965.24, base loss: 4606.49
[INFO 2017-06-26 22:00:58,659 main.py:47] epoch 464, training loss: 3498.13, average training loss: 5959.94, base loss: 4605.14
[INFO 2017-06-26 22:00:59,062 main.py:47] epoch 465, training loss: 3887.63, average training loss: 5955.49, base loss: 4604.37
[INFO 2017-06-26 22:00:59,464 main.py:47] epoch 466, training loss: 4700.38, average training loss: 5952.80, base loss: 4606.47
[INFO 2017-06-26 22:00:59,868 main.py:47] epoch 467, training loss: 4752.39, average training loss: 5950.24, base loss: 4608.48
[INFO 2017-06-26 22:01:00,275 main.py:47] epoch 468, training loss: 4087.51, average training loss: 5946.27, base loss: 4608.30
[INFO 2017-06-26 22:01:00,678 main.py:47] epoch 469, training loss: 3744.23, average training loss: 5941.58, base loss: 4607.55
[INFO 2017-06-26 22:01:01,081 main.py:47] epoch 470, training loss: 3797.15, average training loss: 5937.03, base loss: 4606.81
[INFO 2017-06-26 22:01:01,484 main.py:47] epoch 471, training loss: 4019.27, average training loss: 5932.97, base loss: 4606.81
[INFO 2017-06-26 22:01:01,887 main.py:47] epoch 472, training loss: 3714.63, average training loss: 5928.28, base loss: 4605.66
[INFO 2017-06-26 22:01:02,292 main.py:47] epoch 473, training loss: 3649.96, average training loss: 5923.47, base loss: 4604.46
[INFO 2017-06-26 22:01:02,696 main.py:47] epoch 474, training loss: 3943.48, average training loss: 5919.30, base loss: 4604.31
[INFO 2017-06-26 22:01:03,099 main.py:47] epoch 475, training loss: 7850.08, average training loss: 5923.36, base loss: 4611.77
[INFO 2017-06-26 22:01:03,502 main.py:47] epoch 476, training loss: 3455.62, average training loss: 5918.18, base loss: 4610.31
[INFO 2017-06-26 22:01:03,908 main.py:47] epoch 477, training loss: 3791.09, average training loss: 5913.73, base loss: 4609.36
[INFO 2017-06-26 22:01:04,311 main.py:47] epoch 478, training loss: 3784.76, average training loss: 5909.29, base loss: 4608.55
[INFO 2017-06-26 22:01:04,714 main.py:47] epoch 479, training loss: 4111.06, average training loss: 5905.54, base loss: 4608.33
[INFO 2017-06-26 22:01:05,117 main.py:47] epoch 480, training loss: 4669.35, average training loss: 5902.97, base loss: 4610.12
[INFO 2017-06-26 22:01:05,520 main.py:47] epoch 481, training loss: 3821.09, average training loss: 5898.65, base loss: 4609.54
[INFO 2017-06-26 22:01:05,921 main.py:47] epoch 482, training loss: 4048.50, average training loss: 5894.82, base loss: 4610.25
[INFO 2017-06-26 22:01:06,325 main.py:47] epoch 483, training loss: 3865.55, average training loss: 5890.63, base loss: 4609.62
[INFO 2017-06-26 22:01:06,732 main.py:47] epoch 484, training loss: 3989.12, average training loss: 5886.71, base loss: 4609.27
[INFO 2017-06-26 22:01:07,136 main.py:47] epoch 485, training loss: 4238.59, average training loss: 5883.32, base loss: 4610.01
[INFO 2017-06-26 22:01:07,538 main.py:47] epoch 486, training loss: 4034.89, average training loss: 5879.52, base loss: 4610.19
[INFO 2017-06-26 22:01:07,940 main.py:47] epoch 487, training loss: 3381.45, average training loss: 5874.40, base loss: 4608.06
[INFO 2017-06-26 22:01:08,342 main.py:47] epoch 488, training loss: 3596.03, average training loss: 5869.74, base loss: 4606.94
[INFO 2017-06-26 22:01:08,745 main.py:47] epoch 489, training loss: 4301.41, average training loss: 5866.54, base loss: 4607.21
[INFO 2017-06-26 22:01:09,154 main.py:47] epoch 490, training loss: 3984.62, average training loss: 5862.71, base loss: 4606.90
[INFO 2017-06-26 22:01:09,557 main.py:47] epoch 491, training loss: 4573.00, average training loss: 5860.09, base loss: 4608.11
[INFO 2017-06-26 22:01:09,960 main.py:47] epoch 492, training loss: 4326.37, average training loss: 5856.98, base loss: 4608.56
[INFO 2017-06-26 22:01:10,363 main.py:47] epoch 493, training loss: 4233.18, average training loss: 5853.69, base loss: 4609.49
[INFO 2017-06-26 22:01:10,765 main.py:47] epoch 494, training loss: 3797.57, average training loss: 5849.54, base loss: 4608.63
[INFO 2017-06-26 22:01:11,171 main.py:47] epoch 495, training loss: 4613.54, average training loss: 5847.05, base loss: 4610.02
[INFO 2017-06-26 22:01:11,575 main.py:47] epoch 496, training loss: 3448.33, average training loss: 5842.22, base loss: 4608.06
[INFO 2017-06-26 22:01:11,978 main.py:47] epoch 497, training loss: 3996.76, average training loss: 5838.51, base loss: 4607.34
[INFO 2017-06-26 22:01:12,381 main.py:47] epoch 498, training loss: 3270.85, average training loss: 5833.37, base loss: 4605.43
[INFO 2017-06-26 22:01:12,784 main.py:47] epoch 499, training loss: 4187.26, average training loss: 5830.08, base loss: 4605.75
[INFO 2017-06-26 22:01:12,784 main.py:49] epoch 499, testing
[INFO 2017-06-26 22:01:14,449 main.py:102] average testing loss: 3988.68, base loss: 4431.81
[INFO 2017-06-26 22:01:14,449 main.py:103] improve_loss: 443.14, improve_percent: 0.10
[INFO 2017-06-26 22:01:14,449 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 22:01:14,462 main.py:73] current best improved percent: 0.10
[INFO 2017-06-26 22:01:14,868 main.py:47] epoch 500, training loss: 4162.25, average training loss: 5826.75, base loss: 4605.89
[INFO 2017-06-26 22:01:15,269 main.py:47] epoch 501, training loss: 4410.73, average training loss: 5823.93, base loss: 4606.76
[INFO 2017-06-26 22:01:15,672 main.py:47] epoch 502, training loss: 4274.73, average training loss: 5820.85, base loss: 4607.18
[INFO 2017-06-26 22:01:16,078 main.py:47] epoch 503, training loss: 3405.61, average training loss: 5816.05, base loss: 4605.17
[INFO 2017-06-26 22:01:16,486 main.py:47] epoch 504, training loss: 4093.39, average training loss: 5812.64, base loss: 4605.25
[INFO 2017-06-26 22:01:16,890 main.py:47] epoch 505, training loss: 7275.81, average training loss: 5815.53, base loss: 4611.82
[INFO 2017-06-26 22:01:17,292 main.py:47] epoch 506, training loss: 3320.05, average training loss: 5810.61, base loss: 4609.81
[INFO 2017-06-26 22:01:17,697 main.py:47] epoch 507, training loss: 3210.42, average training loss: 5805.49, base loss: 4607.44
[INFO 2017-06-26 22:01:18,103 main.py:47] epoch 508, training loss: 4099.47, average training loss: 5802.14, base loss: 4607.33
[INFO 2017-06-26 22:01:18,507 main.py:47] epoch 509, training loss: 4695.00, average training loss: 5799.97, base loss: 4608.53
[INFO 2017-06-26 22:01:18,914 main.py:47] epoch 510, training loss: 4561.19, average training loss: 5797.55, base loss: 4609.70
[INFO 2017-06-26 22:01:19,316 main.py:47] epoch 511, training loss: 4107.93, average training loss: 5794.25, base loss: 4609.60
[INFO 2017-06-26 22:01:19,720 main.py:47] epoch 512, training loss: 3335.17, average training loss: 5789.45, base loss: 4607.95
[INFO 2017-06-26 22:01:20,123 main.py:47] epoch 513, training loss: 3638.52, average training loss: 5785.27, base loss: 4607.29
[INFO 2017-06-26 22:01:20,526 main.py:47] epoch 514, training loss: 3844.37, average training loss: 5781.50, base loss: 4606.44
[INFO 2017-06-26 22:01:20,928 main.py:47] epoch 515, training loss: 4174.89, average training loss: 5778.39, base loss: 4606.30
[INFO 2017-06-26 22:01:21,333 main.py:47] epoch 516, training loss: 3712.52, average training loss: 5774.39, base loss: 4605.44
[INFO 2017-06-26 22:01:21,735 main.py:47] epoch 517, training loss: 3451.97, average training loss: 5769.91, base loss: 4603.87
[INFO 2017-06-26 22:01:22,137 main.py:47] epoch 518, training loss: 3776.58, average training loss: 5766.07, base loss: 4603.39
[INFO 2017-06-26 22:01:22,539 main.py:47] epoch 519, training loss: 3772.19, average training loss: 5762.23, base loss: 4602.76
[INFO 2017-06-26 22:01:22,941 main.py:47] epoch 520, training loss: 4731.19, average training loss: 5760.25, base loss: 4604.19
[INFO 2017-06-26 22:01:23,347 main.py:47] epoch 521, training loss: 4607.41, average training loss: 5758.04, base loss: 4605.64
[INFO 2017-06-26 22:01:23,748 main.py:47] epoch 522, training loss: 4344.57, average training loss: 5755.34, base loss: 4606.15
[INFO 2017-06-26 22:01:24,152 main.py:47] epoch 523, training loss: 4067.86, average training loss: 5752.12, base loss: 4606.64
[INFO 2017-06-26 22:01:24,558 main.py:47] epoch 524, training loss: 4232.57, average training loss: 5749.23, base loss: 4607.32
[INFO 2017-06-26 22:01:24,961 main.py:47] epoch 525, training loss: 3882.36, average training loss: 5745.68, base loss: 4606.45
[INFO 2017-06-26 22:01:25,365 main.py:47] epoch 526, training loss: 4069.26, average training loss: 5742.50, base loss: 4606.49
[INFO 2017-06-26 22:01:25,766 main.py:47] epoch 527, training loss: 3974.69, average training loss: 5739.15, base loss: 4605.76
[INFO 2017-06-26 22:01:26,169 main.py:47] epoch 528, training loss: 3949.66, average training loss: 5735.77, base loss: 4605.51
[INFO 2017-06-26 22:01:26,578 main.py:47] epoch 529, training loss: 3986.78, average training loss: 5732.47, base loss: 4605.16
[INFO 2017-06-26 22:01:26,981 main.py:47] epoch 530, training loss: 3740.55, average training loss: 5728.71, base loss: 4604.34
[INFO 2017-06-26 22:01:27,385 main.py:47] epoch 531, training loss: 3753.39, average training loss: 5725.00, base loss: 4603.34
[INFO 2017-06-26 22:01:27,788 main.py:47] epoch 532, training loss: 3786.49, average training loss: 5721.36, base loss: 4602.73
[INFO 2017-06-26 22:01:28,190 main.py:47] epoch 533, training loss: 3399.24, average training loss: 5717.02, base loss: 4601.15
[INFO 2017-06-26 22:01:28,593 main.py:47] epoch 534, training loss: 3936.42, average training loss: 5713.69, base loss: 4600.69
[INFO 2017-06-26 22:01:28,996 main.py:47] epoch 535, training loss: 4602.01, average training loss: 5711.61, base loss: 4601.64
[INFO 2017-06-26 22:01:29,399 main.py:47] epoch 536, training loss: 4261.67, average training loss: 5708.91, base loss: 4601.78
[INFO 2017-06-26 22:01:29,801 main.py:47] epoch 537, training loss: 4155.25, average training loss: 5706.03, base loss: 4602.03
[INFO 2017-06-26 22:01:30,204 main.py:47] epoch 538, training loss: 3581.32, average training loss: 5702.08, base loss: 4600.53
[INFO 2017-06-26 22:01:30,606 main.py:47] epoch 539, training loss: 4504.62, average training loss: 5699.87, base loss: 4601.31
[INFO 2017-06-26 22:01:31,009 main.py:47] epoch 540, training loss: 4690.49, average training loss: 5698.00, base loss: 4602.76
[INFO 2017-06-26 22:01:31,412 main.py:47] epoch 541, training loss: 4385.33, average training loss: 5695.58, base loss: 4603.41
[INFO 2017-06-26 22:01:31,815 main.py:47] epoch 542, training loss: 4298.56, average training loss: 5693.01, base loss: 4604.30
[INFO 2017-06-26 22:01:32,218 main.py:47] epoch 543, training loss: 4012.98, average training loss: 5689.92, base loss: 4603.80
[INFO 2017-06-26 22:01:32,621 main.py:47] epoch 544, training loss: 4497.97, average training loss: 5687.73, base loss: 4605.09
[INFO 2017-06-26 22:01:33,024 main.py:47] epoch 545, training loss: 3732.65, average training loss: 5684.15, base loss: 4604.40
[INFO 2017-06-26 22:01:33,426 main.py:47] epoch 546, training loss: 3391.56, average training loss: 5679.96, base loss: 4602.61
[INFO 2017-06-26 22:01:33,829 main.py:47] epoch 547, training loss: 3608.93, average training loss: 5676.18, base loss: 4601.30
[INFO 2017-06-26 22:01:34,234 main.py:47] epoch 548, training loss: 3788.95, average training loss: 5672.74, base loss: 4600.79
[INFO 2017-06-26 22:01:34,637 main.py:47] epoch 549, training loss: 3315.48, average training loss: 5668.46, base loss: 4598.70
[INFO 2017-06-26 22:01:35,039 main.py:47] epoch 550, training loss: 4468.41, average training loss: 5666.28, base loss: 4599.45
[INFO 2017-06-26 22:01:35,442 main.py:47] epoch 551, training loss: 7507.06, average training loss: 5669.61, base loss: 4605.96
[INFO 2017-06-26 22:01:35,845 main.py:47] epoch 552, training loss: 4067.03, average training loss: 5666.72, base loss: 4605.86
[INFO 2017-06-26 22:01:36,249 main.py:47] epoch 553, training loss: 4457.65, average training loss: 5664.53, base loss: 4607.01
[INFO 2017-06-26 22:01:36,651 main.py:47] epoch 554, training loss: 3511.76, average training loss: 5660.65, base loss: 4605.59
[INFO 2017-06-26 22:01:37,057 main.py:47] epoch 555, training loss: 3783.67, average training loss: 5657.28, base loss: 4604.57
[INFO 2017-06-26 22:01:37,459 main.py:47] epoch 556, training loss: 3853.51, average training loss: 5654.04, base loss: 4603.73
[INFO 2017-06-26 22:01:37,862 main.py:47] epoch 557, training loss: 4050.63, average training loss: 5651.17, base loss: 4603.73
[INFO 2017-06-26 22:01:38,264 main.py:47] epoch 558, training loss: 4500.00, average training loss: 5649.11, base loss: 4605.15
[INFO 2017-06-26 22:01:38,672 main.py:47] epoch 559, training loss: 3910.53, average training loss: 5646.00, base loss: 4604.64
[INFO 2017-06-26 22:01:39,073 main.py:47] epoch 560, training loss: 3731.41, average training loss: 5642.59, base loss: 4603.97
[INFO 2017-06-26 22:01:39,475 main.py:47] epoch 561, training loss: 3586.22, average training loss: 5638.93, base loss: 4603.18
[INFO 2017-06-26 22:01:39,881 main.py:47] epoch 562, training loss: 3691.36, average training loss: 5635.47, base loss: 4602.61
[INFO 2017-06-26 22:01:40,286 main.py:47] epoch 563, training loss: 4238.12, average training loss: 5632.99, base loss: 4602.84
[INFO 2017-06-26 22:01:40,689 main.py:47] epoch 564, training loss: 3835.17, average training loss: 5629.81, base loss: 4602.28
[INFO 2017-06-26 22:01:41,092 main.py:47] epoch 565, training loss: 4439.71, average training loss: 5627.71, base loss: 4602.98
[INFO 2017-06-26 22:01:41,495 main.py:47] epoch 566, training loss: 3693.23, average training loss: 5624.30, base loss: 4602.18
[INFO 2017-06-26 22:01:41,898 main.py:47] epoch 567, training loss: 3920.18, average training loss: 5621.30, base loss: 4601.90
[INFO 2017-06-26 22:01:42,300 main.py:47] epoch 568, training loss: 4054.74, average training loss: 5618.54, base loss: 4601.93
[INFO 2017-06-26 22:01:42,703 main.py:47] epoch 569, training loss: 3512.24, average training loss: 5614.85, base loss: 4600.59
[INFO 2017-06-26 22:01:43,105 main.py:47] epoch 570, training loss: 4010.47, average training loss: 5612.04, base loss: 4600.67
[INFO 2017-06-26 22:01:43,508 main.py:47] epoch 571, training loss: 4531.56, average training loss: 5610.15, base loss: 4601.78
[INFO 2017-06-26 22:01:43,911 main.py:47] epoch 572, training loss: 4486.20, average training loss: 5608.19, base loss: 4603.09
[INFO 2017-06-26 22:01:44,315 main.py:47] epoch 573, training loss: 3543.70, average training loss: 5604.59, base loss: 4602.24
[INFO 2017-06-26 22:01:44,717 main.py:47] epoch 574, training loss: 4219.75, average training loss: 5602.18, base loss: 4602.84
[INFO 2017-06-26 22:01:45,119 main.py:47] epoch 575, training loss: 4005.88, average training loss: 5599.41, base loss: 4602.84
[INFO 2017-06-26 22:01:45,520 main.py:47] epoch 576, training loss: 3969.84, average training loss: 5596.59, base loss: 4602.56
[INFO 2017-06-26 22:01:45,923 main.py:47] epoch 577, training loss: 3295.17, average training loss: 5592.61, base loss: 4600.81
[INFO 2017-06-26 22:01:46,330 main.py:47] epoch 578, training loss: 4588.77, average training loss: 5590.87, base loss: 4602.03
[INFO 2017-06-26 22:01:46,735 main.py:47] epoch 579, training loss: 4647.63, average training loss: 5589.25, base loss: 4603.66
[INFO 2017-06-26 22:01:47,140 main.py:47] epoch 580, training loss: 3588.17, average training loss: 5585.80, base loss: 4602.47
[INFO 2017-06-26 22:01:47,542 main.py:47] epoch 581, training loss: 4437.57, average training loss: 5583.83, base loss: 4603.47
[INFO 2017-06-26 22:01:47,945 main.py:47] epoch 582, training loss: 7349.98, average training loss: 5586.86, base loss: 4608.76
[INFO 2017-06-26 22:01:48,354 main.py:47] epoch 583, training loss: 4285.27, average training loss: 5584.63, base loss: 4609.82
[INFO 2017-06-26 22:01:48,749 main.py:47] epoch 584, training loss: 7982.28, average training loss: 5588.73, base loss: 4617.06
[INFO 2017-06-26 22:01:49,147 main.py:47] epoch 585, training loss: 3749.74, average training loss: 5585.59, base loss: 4616.45
[INFO 2017-06-26 22:01:49,547 main.py:47] epoch 586, training loss: 3606.06, average training loss: 5582.22, base loss: 4615.88
[INFO 2017-06-26 22:01:49,945 main.py:47] epoch 587, training loss: 3795.52, average training loss: 5579.18, base loss: 4615.42
[INFO 2017-06-26 22:01:50,343 main.py:47] epoch 588, training loss: 3959.42, average training loss: 5576.43, base loss: 4615.52
[INFO 2017-06-26 22:01:50,745 main.py:47] epoch 589, training loss: 4523.31, average training loss: 5574.64, base loss: 4616.82
[INFO 2017-06-26 22:01:51,143 main.py:47] epoch 590, training loss: 4006.29, average training loss: 5571.99, base loss: 4616.65
[INFO 2017-06-26 22:01:51,542 main.py:47] epoch 591, training loss: 3755.56, average training loss: 5568.92, base loss: 4616.26
[INFO 2017-06-26 22:01:51,942 main.py:47] epoch 592, training loss: 4791.06, average training loss: 5567.61, base loss: 4618.05
[INFO 2017-06-26 22:01:52,339 main.py:47] epoch 593, training loss: 4092.59, average training loss: 5565.13, base loss: 4618.56
[INFO 2017-06-26 22:01:52,737 main.py:47] epoch 594, training loss: 3935.51, average training loss: 5562.39, base loss: 4618.45
[INFO 2017-06-26 22:01:53,133 main.py:47] epoch 595, training loss: 4128.88, average training loss: 5559.98, base loss: 4618.60
[INFO 2017-06-26 22:01:53,530 main.py:47] epoch 596, training loss: 3877.13, average training loss: 5557.16, base loss: 4618.66
[INFO 2017-06-26 22:01:53,926 main.py:47] epoch 597, training loss: 3525.49, average training loss: 5553.77, base loss: 4617.50
[INFO 2017-06-26 22:01:54,322 main.py:47] epoch 598, training loss: 3830.90, average training loss: 5550.89, base loss: 4617.34
[INFO 2017-06-26 22:01:54,723 main.py:47] epoch 599, training loss: 4118.45, average training loss: 5548.50, base loss: 4617.71
[INFO 2017-06-26 22:01:54,723 main.py:49] epoch 599, testing
[INFO 2017-06-26 22:01:56,368 main.py:102] average testing loss: 3936.10, base loss: 4504.74
[INFO 2017-06-26 22:01:56,369 main.py:103] improve_loss: 568.64, improve_percent: 0.13
[INFO 2017-06-26 22:01:56,369 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 22:01:56,381 main.py:73] current best improved percent: 0.13
[INFO 2017-06-26 22:01:56,781 main.py:47] epoch 600, training loss: 4132.88, average training loss: 5546.15, base loss: 4618.22
[INFO 2017-06-26 22:01:57,183 main.py:47] epoch 601, training loss: 3950.53, average training loss: 5543.50, base loss: 4617.99
[INFO 2017-06-26 22:01:57,583 main.py:47] epoch 602, training loss: 3625.10, average training loss: 5540.32, base loss: 4617.21
[INFO 2017-06-26 22:01:57,996 main.py:47] epoch 603, training loss: 3930.59, average training loss: 5537.65, base loss: 4616.93
[INFO 2017-06-26 22:01:58,398 main.py:47] epoch 604, training loss: 4465.28, average training loss: 5535.88, base loss: 4618.13
[INFO 2017-06-26 22:01:58,801 main.py:47] epoch 605, training loss: 4247.25, average training loss: 5533.75, base loss: 4619.08
[INFO 2017-06-26 22:01:59,203 main.py:47] epoch 606, training loss: 4104.54, average training loss: 5531.40, base loss: 4619.50
[INFO 2017-06-26 22:01:59,605 main.py:47] epoch 607, training loss: 3959.18, average training loss: 5528.81, base loss: 4619.22
[INFO 2017-06-26 22:02:00,004 main.py:47] epoch 608, training loss: 4482.98, average training loss: 5527.09, base loss: 4620.65
[INFO 2017-06-26 22:02:00,404 main.py:47] epoch 609, training loss: 3733.25, average training loss: 5524.15, base loss: 4620.24
[INFO 2017-06-26 22:02:00,806 main.py:47] epoch 610, training loss: 3985.42, average training loss: 5521.63, base loss: 4620.42
[INFO 2017-06-26 22:02:01,204 main.py:47] epoch 611, training loss: 4020.86, average training loss: 5519.18, base loss: 4620.50
[INFO 2017-06-26 22:02:01,604 main.py:47] epoch 612, training loss: 3376.53, average training loss: 5515.69, base loss: 4618.99
[INFO 2017-06-26 22:02:02,009 main.py:47] epoch 613, training loss: 4515.96, average training loss: 5514.06, base loss: 4620.15
[INFO 2017-06-26 22:02:02,409 main.py:47] epoch 614, training loss: 3751.94, average training loss: 5511.19, base loss: 4620.02
[INFO 2017-06-26 22:02:02,812 main.py:47] epoch 615, training loss: 3736.84, average training loss: 5508.31, base loss: 4619.26
[INFO 2017-06-26 22:02:03,210 main.py:47] epoch 616, training loss: 3939.35, average training loss: 5505.77, base loss: 4619.34
[INFO 2017-06-26 22:02:03,610 main.py:47] epoch 617, training loss: 3262.91, average training loss: 5502.14, base loss: 4617.67
[INFO 2017-06-26 22:02:04,013 main.py:47] epoch 618, training loss: 3576.36, average training loss: 5499.03, base loss: 4616.76
[INFO 2017-06-26 22:02:04,413 main.py:47] epoch 619, training loss: 4083.54, average training loss: 5496.75, base loss: 4617.11
[INFO 2017-06-26 22:02:04,814 main.py:47] epoch 620, training loss: 3738.15, average training loss: 5493.91, base loss: 4616.64
[INFO 2017-06-26 22:02:05,213 main.py:47] epoch 621, training loss: 3892.96, average training loss: 5491.34, base loss: 4616.51
[INFO 2017-06-26 22:02:05,615 main.py:47] epoch 622, training loss: 3804.10, average training loss: 5488.63, base loss: 4615.68
[INFO 2017-06-26 22:02:06,015 main.py:47] epoch 623, training loss: 3864.27, average training loss: 5486.03, base loss: 4615.25
[INFO 2017-06-26 22:02:06,419 main.py:47] epoch 624, training loss: 3737.89, average training loss: 5483.23, base loss: 4614.51
[INFO 2017-06-26 22:02:06,824 main.py:47] epoch 625, training loss: 4890.30, average training loss: 5482.29, base loss: 4616.39
[INFO 2017-06-26 22:02:07,234 main.py:47] epoch 626, training loss: 3873.97, average training loss: 5479.72, base loss: 4615.71
[INFO 2017-06-26 22:02:07,635 main.py:47] epoch 627, training loss: 3692.14, average training loss: 5476.87, base loss: 4615.03
[INFO 2017-06-26 22:02:08,041 main.py:47] epoch 628, training loss: 3892.53, average training loss: 5474.36, base loss: 4615.00
[INFO 2017-06-26 22:02:08,452 main.py:47] epoch 629, training loss: 3850.84, average training loss: 5471.78, base loss: 4614.70
[INFO 2017-06-26 22:02:08,859 main.py:47] epoch 630, training loss: 3586.94, average training loss: 5468.79, base loss: 4614.24
[INFO 2017-06-26 22:02:09,264 main.py:47] epoch 631, training loss: 3943.67, average training loss: 5466.38, base loss: 4613.91
[INFO 2017-06-26 22:02:09,665 main.py:47] epoch 632, training loss: 4048.10, average training loss: 5464.14, base loss: 4614.30
[INFO 2017-06-26 22:02:10,070 main.py:47] epoch 633, training loss: 3756.23, average training loss: 5461.44, base loss: 4613.87
[INFO 2017-06-26 22:02:10,477 main.py:47] epoch 634, training loss: 3969.57, average training loss: 5459.09, base loss: 4614.08
[INFO 2017-06-26 22:02:10,888 main.py:47] epoch 635, training loss: 3912.44, average training loss: 5456.66, base loss: 4614.03
[INFO 2017-06-26 22:02:11,297 main.py:47] epoch 636, training loss: 3191.43, average training loss: 5453.11, base loss: 4612.42
[INFO 2017-06-26 22:02:11,698 main.py:47] epoch 637, training loss: 3576.28, average training loss: 5450.16, base loss: 4611.58
[INFO 2017-06-26 22:02:12,101 main.py:47] epoch 638, training loss: 7596.82, average training loss: 5453.52, base loss: 4617.15
[INFO 2017-06-26 22:02:12,509 main.py:47] epoch 639, training loss: 3725.81, average training loss: 5450.82, base loss: 4616.57
[INFO 2017-06-26 22:02:12,916 main.py:47] epoch 640, training loss: 3840.71, average training loss: 5448.31, base loss: 4616.57
[INFO 2017-06-26 22:02:13,324 main.py:47] epoch 641, training loss: 3684.90, average training loss: 5445.57, base loss: 4616.06
[INFO 2017-06-26 22:02:13,733 main.py:47] epoch 642, training loss: 4159.81, average training loss: 5443.57, base loss: 4616.42
[INFO 2017-06-26 22:02:14,135 main.py:47] epoch 643, training loss: 3944.24, average training loss: 5441.24, base loss: 4616.29
[INFO 2017-06-26 22:02:14,541 main.py:47] epoch 644, training loss: 4114.37, average training loss: 5439.18, base loss: 4616.60
[INFO 2017-06-26 22:02:14,948 main.py:47] epoch 645, training loss: 3832.31, average training loss: 5436.69, base loss: 4616.41
[INFO 2017-06-26 22:02:15,351 main.py:47] epoch 646, training loss: 3465.48, average training loss: 5433.65, base loss: 4615.52
[INFO 2017-06-26 22:02:15,752 main.py:47] epoch 647, training loss: 3642.20, average training loss: 5430.88, base loss: 4615.16
[INFO 2017-06-26 22:02:16,158 main.py:47] epoch 648, training loss: 4076.52, average training loss: 5428.80, base loss: 4615.23
[INFO 2017-06-26 22:02:16,565 main.py:47] epoch 649, training loss: 3648.18, average training loss: 5426.06, base loss: 4614.54
[INFO 2017-06-26 22:02:16,974 main.py:47] epoch 650, training loss: 3360.02, average training loss: 5422.88, base loss: 4613.34
[INFO 2017-06-26 22:02:17,384 main.py:47] epoch 651, training loss: 3443.50, average training loss: 5419.85, base loss: 4612.43
[INFO 2017-06-26 22:02:17,786 main.py:47] epoch 652, training loss: 4563.05, average training loss: 5418.53, base loss: 4613.62
[INFO 2017-06-26 22:02:18,191 main.py:47] epoch 653, training loss: 3528.86, average training loss: 5415.64, base loss: 4612.67
[INFO 2017-06-26 22:02:18,592 main.py:47] epoch 654, training loss: 3363.28, average training loss: 5412.51, base loss: 4611.33
[INFO 2017-06-26 22:02:18,992 main.py:47] epoch 655, training loss: 3954.20, average training loss: 5410.29, base loss: 4611.20
[INFO 2017-06-26 22:02:19,397 main.py:47] epoch 656, training loss: 3945.42, average training loss: 5408.06, base loss: 4610.86
[INFO 2017-06-26 22:02:19,798 main.py:47] epoch 657, training loss: 3902.12, average training loss: 5405.77, base loss: 4610.45
[INFO 2017-06-26 22:02:20,200 main.py:47] epoch 658, training loss: 3287.06, average training loss: 5402.55, base loss: 4608.82
[INFO 2017-06-26 22:02:20,602 main.py:47] epoch 659, training loss: 3334.02, average training loss: 5399.42, base loss: 4607.33
[INFO 2017-06-26 22:02:20,999 main.py:47] epoch 660, training loss: 4300.29, average training loss: 5397.76, base loss: 4607.81
[INFO 2017-06-26 22:02:21,403 main.py:47] epoch 661, training loss: 5142.41, average training loss: 5397.37, base loss: 4610.02
[INFO 2017-06-26 22:02:21,805 main.py:47] epoch 662, training loss: 3845.21, average training loss: 5395.03, base loss: 4609.66
[INFO 2017-06-26 22:02:22,202 main.py:47] epoch 663, training loss: 3906.16, average training loss: 5392.79, base loss: 4609.54
[INFO 2017-06-26 22:02:22,608 main.py:47] epoch 664, training loss: 4094.19, average training loss: 5390.84, base loss: 4610.21
[INFO 2017-06-26 22:02:23,007 main.py:47] epoch 665, training loss: 3983.00, average training loss: 5388.72, base loss: 4610.09
[INFO 2017-06-26 22:02:23,405 main.py:47] epoch 666, training loss: 4156.42, average training loss: 5386.87, base loss: 4610.69
[INFO 2017-06-26 22:02:23,807 main.py:47] epoch 667, training loss: 3771.15, average training loss: 5384.46, base loss: 4610.37
[INFO 2017-06-26 22:02:24,207 main.py:47] epoch 668, training loss: 3670.67, average training loss: 5381.89, base loss: 4609.41
[INFO 2017-06-26 22:02:24,610 main.py:47] epoch 669, training loss: 3753.67, average training loss: 5379.46, base loss: 4609.17
[INFO 2017-06-26 22:02:25,018 main.py:47] epoch 670, training loss: 3253.76, average training loss: 5376.30, base loss: 4607.80
[INFO 2017-06-26 22:02:25,419 main.py:47] epoch 671, training loss: 3982.80, average training loss: 5374.22, base loss: 4607.96
[INFO 2017-06-26 22:02:25,819 main.py:47] epoch 672, training loss: 3404.15, average training loss: 5371.30, base loss: 4606.76
[INFO 2017-06-26 22:02:26,220 main.py:47] epoch 673, training loss: 3912.16, average training loss: 5369.13, base loss: 4607.03
[INFO 2017-06-26 22:02:26,624 main.py:47] epoch 674, training loss: 4170.69, average training loss: 5367.35, base loss: 4607.51
[INFO 2017-06-26 22:02:27,029 main.py:47] epoch 675, training loss: 4064.47, average training loss: 5365.43, base loss: 4607.54
[INFO 2017-06-26 22:02:27,438 main.py:47] epoch 676, training loss: 3862.80, average training loss: 5363.21, base loss: 4607.41
[INFO 2017-06-26 22:02:27,850 main.py:47] epoch 677, training loss: 3876.19, average training loss: 5361.01, base loss: 4607.37
[INFO 2017-06-26 22:02:28,262 main.py:47] epoch 678, training loss: 4497.05, average training loss: 5359.74, base loss: 4608.11
[INFO 2017-06-26 22:02:28,666 main.py:47] epoch 679, training loss: 4103.00, average training loss: 5357.89, base loss: 4607.76
[INFO 2017-06-26 22:02:29,073 main.py:47] epoch 680, training loss: 4161.95, average training loss: 5356.14, base loss: 4608.26
[INFO 2017-06-26 22:02:29,478 main.py:47] epoch 681, training loss: 3202.54, average training loss: 5352.98, base loss: 4606.91
[INFO 2017-06-26 22:02:29,882 main.py:47] epoch 682, training loss: 4246.16, average training loss: 5351.36, base loss: 4607.56
[INFO 2017-06-26 22:02:30,286 main.py:47] epoch 683, training loss: 3876.94, average training loss: 5349.20, base loss: 4607.70
[INFO 2017-06-26 22:02:30,692 main.py:47] epoch 684, training loss: 4085.93, average training loss: 5347.36, base loss: 4607.87
[INFO 2017-06-26 22:02:31,090 main.py:47] epoch 685, training loss: 3483.42, average training loss: 5344.64, base loss: 4606.93
[INFO 2017-06-26 22:02:31,491 main.py:47] epoch 686, training loss: 3593.28, average training loss: 5342.09, base loss: 4606.49
[INFO 2017-06-26 22:02:31,896 main.py:47] epoch 687, training loss: 6915.27, average training loss: 5344.38, base loss: 4610.33
[INFO 2017-06-26 22:02:32,298 main.py:47] epoch 688, training loss: 3570.93, average training loss: 5341.81, base loss: 4609.83
[INFO 2017-06-26 22:02:32,697 main.py:47] epoch 689, training loss: 3784.42, average training loss: 5339.55, base loss: 4609.56
[INFO 2017-06-26 22:02:33,101 main.py:47] epoch 690, training loss: 3770.53, average training loss: 5337.28, base loss: 4609.43
[INFO 2017-06-26 22:02:33,503 main.py:47] epoch 691, training loss: 4146.39, average training loss: 5335.56, base loss: 4609.80
[INFO 2017-06-26 22:02:33,906 main.py:47] epoch 692, training loss: 3726.33, average training loss: 5333.24, base loss: 4609.43
[INFO 2017-06-26 22:02:34,308 main.py:47] epoch 693, training loss: 3391.57, average training loss: 5330.44, base loss: 4607.98
[INFO 2017-06-26 22:02:34,714 main.py:47] epoch 694, training loss: 3403.21, average training loss: 5327.66, base loss: 4606.98
[INFO 2017-06-26 22:02:35,117 main.py:47] epoch 695, training loss: 4110.18, average training loss: 5325.92, base loss: 4607.51
[INFO 2017-06-26 22:02:35,520 main.py:47] epoch 696, training loss: 4478.93, average training loss: 5324.70, base loss: 4608.54
[INFO 2017-06-26 22:02:35,922 main.py:47] epoch 697, training loss: 3445.04, average training loss: 5322.01, base loss: 4607.69
[INFO 2017-06-26 22:02:36,321 main.py:47] epoch 698, training loss: 3985.96, average training loss: 5320.10, base loss: 4607.96
[INFO 2017-06-26 22:02:36,721 main.py:47] epoch 699, training loss: 4085.16, average training loss: 5318.33, base loss: 4608.40
[INFO 2017-06-26 22:02:36,721 main.py:49] epoch 699, testing
[INFO 2017-06-26 22:02:38,382 main.py:102] average testing loss: 3781.31, base loss: 4344.26
[INFO 2017-06-26 22:02:38,382 main.py:103] improve_loss: 562.95, improve_percent: 0.13
[INFO 2017-06-26 22:02:38,383 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 22:02:38,395 main.py:73] current best improved percent: 0.13
[INFO 2017-06-26 22:02:38,797 main.py:47] epoch 700, training loss: 3690.83, average training loss: 5316.01, base loss: 4607.74
[INFO 2017-06-26 22:02:39,195 main.py:47] epoch 701, training loss: 3298.95, average training loss: 5313.14, base loss: 4606.69
[INFO 2017-06-26 22:02:39,599 main.py:47] epoch 702, training loss: 5028.71, average training loss: 5312.73, base loss: 4608.64
[INFO 2017-06-26 22:02:40,001 main.py:47] epoch 703, training loss: 7579.43, average training loss: 5315.95, base loss: 4614.24
[INFO 2017-06-26 22:02:40,400 main.py:47] epoch 704, training loss: 4034.45, average training loss: 5314.13, base loss: 4614.43
[INFO 2017-06-26 22:02:40,803 main.py:47] epoch 705, training loss: 4091.32, average training loss: 5312.40, base loss: 4614.92
[INFO 2017-06-26 22:02:41,200 main.py:47] epoch 706, training loss: 3861.39, average training loss: 5310.35, base loss: 4614.97
[INFO 2017-06-26 22:02:41,601 main.py:47] epoch 707, training loss: 4122.09, average training loss: 5308.67, base loss: 4615.56
[INFO 2017-06-26 22:02:41,999 main.py:47] epoch 708, training loss: 3969.01, average training loss: 5306.78, base loss: 4615.55
[INFO 2017-06-26 22:02:42,400 main.py:47] epoch 709, training loss: 3741.92, average training loss: 5304.58, base loss: 4615.37
[INFO 2017-06-26 22:02:42,801 main.py:47] epoch 710, training loss: 3935.44, average training loss: 5302.65, base loss: 4615.41
[INFO 2017-06-26 22:02:43,201 main.py:47] epoch 711, training loss: 3680.04, average training loss: 5300.37, base loss: 4614.54
[INFO 2017-06-26 22:02:43,605 main.py:47] epoch 712, training loss: 3483.99, average training loss: 5297.83, base loss: 4613.51
[INFO 2017-06-26 22:02:44,009 main.py:47] epoch 713, training loss: 3962.66, average training loss: 5295.96, base loss: 4613.38
[INFO 2017-06-26 22:02:44,415 main.py:47] epoch 714, training loss: 3471.02, average training loss: 5293.40, base loss: 4612.31
[INFO 2017-06-26 22:02:44,825 main.py:47] epoch 715, training loss: 3726.50, average training loss: 5291.21, base loss: 4611.88
[INFO 2017-06-26 22:02:45,229 main.py:47] epoch 716, training loss: 4049.87, average training loss: 5289.48, base loss: 4612.00
[INFO 2017-06-26 22:02:45,633 main.py:47] epoch 717, training loss: 4013.89, average training loss: 5287.71, base loss: 4612.35
[INFO 2017-06-26 22:02:46,036 main.py:47] epoch 718, training loss: 3646.59, average training loss: 5285.42, base loss: 4611.56
[INFO 2017-06-26 22:02:46,444 main.py:47] epoch 719, training loss: 3985.26, average training loss: 5283.62, base loss: 4611.88
[INFO 2017-06-26 22:02:46,863 main.py:47] epoch 720, training loss: 3948.05, average training loss: 5281.77, base loss: 4611.64
[INFO 2017-06-26 22:02:47,267 main.py:47] epoch 721, training loss: 3905.19, average training loss: 5279.86, base loss: 4611.81
[INFO 2017-06-26 22:02:47,672 main.py:47] epoch 722, training loss: 6907.01, average training loss: 5282.11, base loss: 4615.55
[INFO 2017-06-26 22:02:48,075 main.py:47] epoch 723, training loss: 3749.30, average training loss: 5279.99, base loss: 4615.66
[INFO 2017-06-26 22:02:48,478 main.py:47] epoch 724, training loss: 4012.82, average training loss: 5278.25, base loss: 4615.95
[INFO 2017-06-26 22:02:48,882 main.py:47] epoch 725, training loss: 3797.97, average training loss: 5276.21, base loss: 4615.90
[INFO 2017-06-26 22:02:49,290 main.py:47] epoch 726, training loss: 3897.41, average training loss: 5274.31, base loss: 4615.77
[INFO 2017-06-26 22:02:49,698 main.py:47] epoch 727, training loss: 3819.46, average training loss: 5272.31, base loss: 4615.32
[INFO 2017-06-26 22:02:50,102 main.py:47] epoch 728, training loss: 3718.59, average training loss: 5270.18, base loss: 4615.16
[INFO 2017-06-26 22:02:50,506 main.py:47] epoch 729, training loss: 3503.06, average training loss: 5267.76, base loss: 4614.10
[INFO 2017-06-26 22:02:50,910 main.py:47] epoch 730, training loss: 3484.18, average training loss: 5265.32, base loss: 4613.32
[INFO 2017-06-26 22:02:51,314 main.py:47] epoch 731, training loss: 3995.87, average training loss: 5263.59, base loss: 4613.47
[INFO 2017-06-26 22:02:51,715 main.py:47] epoch 732, training loss: 3632.61, average training loss: 5261.36, base loss: 4613.01
[INFO 2017-06-26 22:02:52,122 main.py:47] epoch 733, training loss: 3655.28, average training loss: 5259.17, base loss: 4612.51
[INFO 2017-06-26 22:02:52,530 main.py:47] epoch 734, training loss: 3928.61, average training loss: 5257.36, base loss: 4612.65
[INFO 2017-06-26 22:02:52,936 main.py:47] epoch 735, training loss: 3390.55, average training loss: 5254.83, base loss: 4611.92
[INFO 2017-06-26 22:02:53,335 main.py:47] epoch 736, training loss: 3461.08, average training loss: 5252.39, base loss: 4610.99
[INFO 2017-06-26 22:02:53,735 main.py:47] epoch 737, training loss: 3949.78, average training loss: 5250.63, base loss: 4610.99
[INFO 2017-06-26 22:02:54,142 main.py:47] epoch 738, training loss: 3594.80, average training loss: 5248.39, base loss: 4610.32
[INFO 2017-06-26 22:02:54,547 main.py:47] epoch 739, training loss: 4116.92, average training loss: 5246.86, base loss: 4610.82
[INFO 2017-06-26 22:02:54,956 main.py:47] epoch 740, training loss: 4378.70, average training loss: 5245.69, base loss: 4611.69
[INFO 2017-06-26 22:02:55,362 main.py:47] epoch 741, training loss: 4179.26, average training loss: 5244.25, base loss: 4612.46
[INFO 2017-06-26 22:02:55,765 main.py:47] epoch 742, training loss: 4031.25, average training loss: 5242.62, base loss: 4612.91
[INFO 2017-06-26 22:02:56,166 main.py:47] epoch 743, training loss: 7705.24, average training loss: 5245.93, base loss: 4618.24
[INFO 2017-06-26 22:02:56,570 main.py:47] epoch 744, training loss: 3845.95, average training loss: 5244.05, base loss: 4618.00
[INFO 2017-06-26 22:02:56,967 main.py:47] epoch 745, training loss: 3834.03, average training loss: 5242.16, base loss: 4617.61
[INFO 2017-06-26 22:02:57,370 main.py:47] epoch 746, training loss: 3638.85, average training loss: 5240.01, base loss: 4617.04
[INFO 2017-06-26 22:02:57,779 main.py:47] epoch 747, training loss: 3972.30, average training loss: 5238.31, base loss: 4616.86
[INFO 2017-06-26 22:02:58,183 main.py:47] epoch 748, training loss: 3811.43, average training loss: 5236.41, base loss: 4616.43
[INFO 2017-06-26 22:02:58,588 main.py:47] epoch 749, training loss: 3369.60, average training loss: 5233.92, base loss: 4615.50
[INFO 2017-06-26 22:02:58,997 main.py:47] epoch 750, training loss: 3175.75, average training loss: 5231.18, base loss: 4613.87
[INFO 2017-06-26 22:02:59,407 main.py:47] epoch 751, training loss: 3051.64, average training loss: 5228.28, base loss: 4612.07
[INFO 2017-06-26 22:02:59,808 main.py:47] epoch 752, training loss: 3893.14, average training loss: 5226.51, base loss: 4611.83
[INFO 2017-06-26 22:03:00,210 main.py:47] epoch 753, training loss: 3842.71, average training loss: 5224.67, base loss: 4611.61
[INFO 2017-06-26 22:03:00,615 main.py:47] epoch 754, training loss: 4054.91, average training loss: 5223.12, base loss: 4611.78
[INFO 2017-06-26 22:03:01,016 main.py:47] epoch 755, training loss: 3817.55, average training loss: 5221.26, base loss: 4611.30
[INFO 2017-06-26 22:03:01,421 main.py:47] epoch 756, training loss: 3587.34, average training loss: 5219.11, base loss: 4610.39
[INFO 2017-06-26 22:03:01,828 main.py:47] epoch 757, training loss: 4051.31, average training loss: 5217.57, base loss: 4610.30
[INFO 2017-06-26 22:03:02,231 main.py:47] epoch 758, training loss: 4078.40, average training loss: 5216.06, base loss: 4610.46
[INFO 2017-06-26 22:03:02,633 main.py:47] epoch 759, training loss: 3892.27, average training loss: 5214.32, base loss: 4610.40
[INFO 2017-06-26 22:03:03,034 main.py:47] epoch 760, training loss: 4159.75, average training loss: 5212.94, base loss: 4610.54
[INFO 2017-06-26 22:03:03,436 main.py:47] epoch 761, training loss: 7582.73, average training loss: 5216.05, base loss: 4615.12
[INFO 2017-06-26 22:03:03,833 main.py:47] epoch 762, training loss: 3674.62, average training loss: 5214.03, base loss: 4614.49
[INFO 2017-06-26 22:03:04,237 main.py:47] epoch 763, training loss: 3889.54, average training loss: 5212.29, base loss: 4614.21
[INFO 2017-06-26 22:03:04,639 main.py:47] epoch 764, training loss: 4101.85, average training loss: 5210.84, base loss: 4614.39
[INFO 2017-06-26 22:03:05,041 main.py:47] epoch 765, training loss: 3593.11, average training loss: 5208.73, base loss: 4613.54
[INFO 2017-06-26 22:03:05,440 main.py:47] epoch 766, training loss: 3923.48, average training loss: 5207.05, base loss: 4613.56
[INFO 2017-06-26 22:03:05,841 main.py:47] epoch 767, training loss: 4140.54, average training loss: 5205.67, base loss: 4613.71
[INFO 2017-06-26 22:03:06,243 main.py:47] epoch 768, training loss: 3687.77, average training loss: 5203.69, base loss: 4613.59
[INFO 2017-06-26 22:03:06,644 main.py:47] epoch 769, training loss: 3958.37, average training loss: 5202.07, base loss: 4613.69
[INFO 2017-06-26 22:03:07,044 main.py:47] epoch 770, training loss: 3511.58, average training loss: 5199.88, base loss: 4612.92
[INFO 2017-06-26 22:03:07,442 main.py:47] epoch 771, training loss: 4067.41, average training loss: 5198.41, base loss: 4613.69
[INFO 2017-06-26 22:03:07,847 main.py:47] epoch 772, training loss: 4430.94, average training loss: 5197.42, base loss: 4614.71
[INFO 2017-06-26 22:03:08,247 main.py:47] epoch 773, training loss: 3609.22, average training loss: 5195.37, base loss: 4614.24
[INFO 2017-06-26 22:03:08,651 main.py:47] epoch 774, training loss: 7242.32, average training loss: 5198.01, base loss: 4618.34
[INFO 2017-06-26 22:03:09,052 main.py:47] epoch 775, training loss: 4229.25, average training loss: 5196.76, base loss: 4618.81
[INFO 2017-06-26 22:03:09,453 main.py:47] epoch 776, training loss: 3954.34, average training loss: 5195.16, base loss: 4619.08
[INFO 2017-06-26 22:03:09,860 main.py:47] epoch 777, training loss: 3873.81, average training loss: 5193.47, base loss: 4619.05
[INFO 2017-06-26 22:03:10,267 main.py:47] epoch 778, training loss: 3918.37, average training loss: 5191.83, base loss: 4618.90
[INFO 2017-06-26 22:03:10,673 main.py:47] epoch 779, training loss: 3696.43, average training loss: 5189.91, base loss: 4618.71
[INFO 2017-06-26 22:03:11,081 main.py:47] epoch 780, training loss: 4774.25, average training loss: 5189.38, base loss: 4620.35
[INFO 2017-06-26 22:03:11,487 main.py:47] epoch 781, training loss: 4045.43, average training loss: 5187.92, base loss: 4620.43
[INFO 2017-06-26 22:03:11,893 main.py:47] epoch 782, training loss: 3592.34, average training loss: 5185.88, base loss: 4619.83
[INFO 2017-06-26 22:03:12,298 main.py:47] epoch 783, training loss: 3958.94, average training loss: 5184.31, base loss: 4620.07
[INFO 2017-06-26 22:03:12,700 main.py:47] epoch 784, training loss: 3437.18, average training loss: 5182.09, base loss: 4619.28
[INFO 2017-06-26 22:03:13,098 main.py:47] epoch 785, training loss: 3717.59, average training loss: 5180.22, base loss: 4619.13
[INFO 2017-06-26 22:03:13,507 main.py:47] epoch 786, training loss: 3606.52, average training loss: 5178.23, base loss: 4618.40
[INFO 2017-06-26 22:03:13,907 main.py:47] epoch 787, training loss: 3540.51, average training loss: 5176.15, base loss: 4617.77
[INFO 2017-06-26 22:03:14,311 main.py:47] epoch 788, training loss: 3731.24, average training loss: 5174.32, base loss: 4617.68
[INFO 2017-06-26 22:03:14,708 main.py:47] epoch 789, training loss: 3863.79, average training loss: 5172.66, base loss: 4617.57
[INFO 2017-06-26 22:03:15,107 main.py:47] epoch 790, training loss: 4198.15, average training loss: 5171.42, base loss: 4618.33
[INFO 2017-06-26 22:03:15,510 main.py:47] epoch 791, training loss: 3871.18, average training loss: 5169.78, base loss: 4618.36
[INFO 2017-06-26 22:03:15,914 main.py:47] epoch 792, training loss: 3365.26, average training loss: 5167.51, base loss: 4617.28
[INFO 2017-06-26 22:03:16,316 main.py:47] epoch 793, training loss: 3671.33, average training loss: 5165.62, base loss: 4616.67
[INFO 2017-06-26 22:03:16,716 main.py:47] epoch 794, training loss: 3679.90, average training loss: 5163.75, base loss: 4616.64
[INFO 2017-06-26 22:03:17,118 main.py:47] epoch 795, training loss: 3508.15, average training loss: 5161.67, base loss: 4616.01
[INFO 2017-06-26 22:03:17,521 main.py:47] epoch 796, training loss: 3314.28, average training loss: 5159.36, base loss: 4615.04
[INFO 2017-06-26 22:03:17,924 main.py:47] epoch 797, training loss: 4105.72, average training loss: 5158.04, base loss: 4615.22
[INFO 2017-06-26 22:03:18,325 main.py:47] epoch 798, training loss: 3776.11, average training loss: 5156.31, base loss: 4614.93
[INFO 2017-06-26 22:03:18,725 main.py:47] epoch 799, training loss: 3760.52, average training loss: 5154.56, base loss: 4614.71
[INFO 2017-06-26 22:03:18,725 main.py:49] epoch 799, testing
[INFO 2017-06-26 22:03:20,381 main.py:102] average testing loss: 4135.01, base loss: 4808.58
[INFO 2017-06-26 22:03:20,382 main.py:103] improve_loss: 673.57, improve_percent: 0.14
[INFO 2017-06-26 22:03:20,382 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 22:03:20,394 main.py:73] current best improved percent: 0.14
[INFO 2017-06-26 22:03:20,802 main.py:47] epoch 800, training loss: 3227.74, average training loss: 5152.16, base loss: 4613.22
[INFO 2017-06-26 22:03:21,207 main.py:47] epoch 801, training loss: 3387.70, average training loss: 5149.96, base loss: 4612.39
[INFO 2017-06-26 22:03:21,608 main.py:47] epoch 802, training loss: 3217.13, average training loss: 5147.55, base loss: 4611.33
[INFO 2017-06-26 22:03:22,005 main.py:47] epoch 803, training loss: 4319.97, average training loss: 5146.52, base loss: 4612.12
[INFO 2017-06-26 22:03:22,404 main.py:47] epoch 804, training loss: 3450.86, average training loss: 5144.41, base loss: 4611.31
[INFO 2017-06-26 22:03:22,807 main.py:47] epoch 805, training loss: 3854.40, average training loss: 5142.81, base loss: 4611.27
[INFO 2017-06-26 22:03:23,208 main.py:47] epoch 806, training loss: 4212.97, average training loss: 5141.66, base loss: 4611.78
[INFO 2017-06-26 22:03:23,607 main.py:47] epoch 807, training loss: 4068.27, average training loss: 5140.33, base loss: 4612.11
[INFO 2017-06-26 22:03:24,008 main.py:47] epoch 808, training loss: 3671.31, average training loss: 5138.52, base loss: 4611.90
[INFO 2017-06-26 22:03:24,409 main.py:47] epoch 809, training loss: 4360.12, average training loss: 5137.56, base loss: 4612.64
[INFO 2017-06-26 22:03:24,813 main.py:47] epoch 810, training loss: 3685.20, average training loss: 5135.76, base loss: 4612.47
[INFO 2017-06-26 22:03:25,210 main.py:47] epoch 811, training loss: 3588.10, average training loss: 5133.86, base loss: 4611.93
[INFO 2017-06-26 22:03:25,611 main.py:47] epoch 812, training loss: 3984.41, average training loss: 5132.44, base loss: 4612.03
[INFO 2017-06-26 22:03:26,016 main.py:47] epoch 813, training loss: 4444.81, average training loss: 5131.60, base loss: 4612.99
[INFO 2017-06-26 22:03:26,421 main.py:47] epoch 814, training loss: 3781.81, average training loss: 5129.94, base loss: 4612.84
[INFO 2017-06-26 22:03:26,822 main.py:47] epoch 815, training loss: 4030.57, average training loss: 5128.60, base loss: 4612.98
[INFO 2017-06-26 22:03:27,220 main.py:47] epoch 816, training loss: 4367.98, average training loss: 5127.67, base loss: 4613.75
[INFO 2017-06-26 22:03:27,622 main.py:47] epoch 817, training loss: 4013.07, average training loss: 5126.30, base loss: 4614.02
[INFO 2017-06-26 22:03:28,021 main.py:47] epoch 818, training loss: 3739.22, average training loss: 5124.61, base loss: 4613.64
[INFO 2017-06-26 22:03:28,427 main.py:47] epoch 819, training loss: 3556.56, average training loss: 5122.70, base loss: 4613.09
[INFO 2017-06-26 22:03:28,831 main.py:47] epoch 820, training loss: 3682.32, average training loss: 5120.94, base loss: 4612.88
[INFO 2017-06-26 22:03:29,233 main.py:47] epoch 821, training loss: 3343.55, average training loss: 5118.78, base loss: 4611.85
[INFO 2017-06-26 22:03:29,635 main.py:47] epoch 822, training loss: 3750.61, average training loss: 5117.12, base loss: 4611.68
[INFO 2017-06-26 22:03:30,035 main.py:47] epoch 823, training loss: 4157.84, average training loss: 5115.95, base loss: 4612.10
[INFO 2017-06-26 22:03:30,438 main.py:47] epoch 824, training loss: 3979.87, average training loss: 5114.58, base loss: 4612.23
[INFO 2017-06-26 22:03:30,840 main.py:47] epoch 825, training loss: 4155.76, average training loss: 5113.42, base loss: 4612.52
[INFO 2017-06-26 22:03:31,239 main.py:47] epoch 826, training loss: 3164.10, average training loss: 5111.06, base loss: 4611.40
[INFO 2017-06-26 22:03:31,640 main.py:47] epoch 827, training loss: 4128.65, average training loss: 5109.87, base loss: 4612.04
[INFO 2017-06-26 22:03:32,040 main.py:47] epoch 828, training loss: 3961.02, average training loss: 5108.49, base loss: 4611.92
[INFO 2017-06-26 22:03:32,449 main.py:47] epoch 829, training loss: 3512.76, average training loss: 5106.56, base loss: 4611.23
[INFO 2017-06-26 22:03:32,846 main.py:47] epoch 830, training loss: 3110.86, average training loss: 5104.16, base loss: 4609.99
[INFO 2017-06-26 22:03:33,245 main.py:47] epoch 831, training loss: 3821.35, average training loss: 5102.62, base loss: 4609.77
[INFO 2017-06-26 22:03:33,647 main.py:47] epoch 832, training loss: 3243.88, average training loss: 5100.39, base loss: 4608.76
[INFO 2017-06-26 22:03:34,053 main.py:47] epoch 833, training loss: 3837.04, average training loss: 5098.87, base loss: 4608.40
[INFO 2017-06-26 22:03:34,456 main.py:47] epoch 834, training loss: 4086.34, average training loss: 5097.66, base loss: 4608.88
[INFO 2017-06-26 22:03:34,860 main.py:47] epoch 835, training loss: 3798.83, average training loss: 5096.11, base loss: 4608.65
[INFO 2017-06-26 22:03:35,264 main.py:47] epoch 836, training loss: 3637.53, average training loss: 5094.37, base loss: 4608.33
[INFO 2017-06-26 22:03:35,679 main.py:47] epoch 837, training loss: 3623.16, average training loss: 5092.61, base loss: 4608.08
[INFO 2017-06-26 22:03:36,080 main.py:47] epoch 838, training loss: 7260.67, average training loss: 5095.19, base loss: 4612.26
[INFO 2017-06-26 22:03:36,480 main.py:47] epoch 839, training loss: 3800.56, average training loss: 5093.65, base loss: 4612.51
[INFO 2017-06-26 22:03:36,882 main.py:47] epoch 840, training loss: 4187.56, average training loss: 5092.58, base loss: 4612.97
[INFO 2017-06-26 22:03:37,285 main.py:47] epoch 841, training loss: 3965.98, average training loss: 5091.24, base loss: 4613.09
[INFO 2017-06-26 22:03:37,691 main.py:47] epoch 842, training loss: 3718.70, average training loss: 5089.61, base loss: 4612.72
[INFO 2017-06-26 22:03:38,094 main.py:47] epoch 843, training loss: 3528.96, average training loss: 5087.76, base loss: 4612.07
[INFO 2017-06-26 22:03:38,501 main.py:47] epoch 844, training loss: 4289.72, average training loss: 5086.82, base loss: 4613.07
[INFO 2017-06-26 22:03:38,903 main.py:47] epoch 845, training loss: 3654.66, average training loss: 5085.12, base loss: 4612.69
[INFO 2017-06-26 22:03:39,308 main.py:47] epoch 846, training loss: 3929.54, average training loss: 5083.76, base loss: 4612.70
[INFO 2017-06-26 22:03:39,711 main.py:47] epoch 847, training loss: 4040.45, average training loss: 5082.53, base loss: 4612.71
[INFO 2017-06-26 22:03:40,116 main.py:47] epoch 848, training loss: 3930.97, average training loss: 5081.17, base loss: 4612.91
[INFO 2017-06-26 22:03:40,518 main.py:47] epoch 849, training loss: 3767.76, average training loss: 5079.63, base loss: 4612.84
[INFO 2017-06-26 22:03:40,922 main.py:47] epoch 850, training loss: 3661.06, average training loss: 5077.96, base loss: 4612.66
[INFO 2017-06-26 22:03:41,321 main.py:47] epoch 851, training loss: 4456.62, average training loss: 5077.23, base loss: 4613.94
[INFO 2017-06-26 22:03:41,719 main.py:47] epoch 852, training loss: 3729.52, average training loss: 5075.65, base loss: 4613.57
[INFO 2017-06-26 22:03:42,119 main.py:47] epoch 853, training loss: 3969.14, average training loss: 5074.35, base loss: 4613.48
[INFO 2017-06-26 22:03:42,522 main.py:47] epoch 854, training loss: 3916.17, average training loss: 5073.00, base loss: 4613.62
[INFO 2017-06-26 22:03:42,923 main.py:47] epoch 855, training loss: 3689.79, average training loss: 5071.38, base loss: 4613.40
[INFO 2017-06-26 22:03:43,322 main.py:47] epoch 856, training loss: 3691.83, average training loss: 5069.77, base loss: 4613.11
[INFO 2017-06-26 22:03:43,728 main.py:47] epoch 857, training loss: 3124.54, average training loss: 5067.51, base loss: 4611.92
[INFO 2017-06-26 22:03:44,127 main.py:47] epoch 858, training loss: 4234.20, average training loss: 5066.54, base loss: 4612.26
[INFO 2017-06-26 22:03:44,533 main.py:47] epoch 859, training loss: 4314.87, average training loss: 5065.66, base loss: 4613.01
[INFO 2017-06-26 22:03:44,930 main.py:47] epoch 860, training loss: 3186.26, average training loss: 5063.48, base loss: 4611.94
[INFO 2017-06-26 22:03:45,327 main.py:47] epoch 861, training loss: 4086.29, average training loss: 5062.35, base loss: 4612.45
[INFO 2017-06-26 22:03:45,725 main.py:47] epoch 862, training loss: 4002.82, average training loss: 5061.12, base loss: 4612.79
[INFO 2017-06-26 22:03:46,127 main.py:47] epoch 863, training loss: 3743.58, average training loss: 5059.59, base loss: 4612.66
[INFO 2017-06-26 22:03:46,530 main.py:47] epoch 864, training loss: 3607.57, average training loss: 5057.92, base loss: 4612.13
[INFO 2017-06-26 22:03:46,932 main.py:47] epoch 865, training loss: 3923.48, average training loss: 5056.61, base loss: 4612.20
[INFO 2017-06-26 22:03:47,333 main.py:47] epoch 866, training loss: 3542.15, average training loss: 5054.86, base loss: 4612.11
[INFO 2017-06-26 22:03:47,732 main.py:47] epoch 867, training loss: 3501.67, average training loss: 5053.07, base loss: 4611.38
[INFO 2017-06-26 22:03:48,130 main.py:47] epoch 868, training loss: 3732.29, average training loss: 5051.55, base loss: 4611.21
[INFO 2017-06-26 22:03:48,526 main.py:47] epoch 869, training loss: 3647.65, average training loss: 5049.94, base loss: 4610.95
[INFO 2017-06-26 22:03:48,926 main.py:47] epoch 870, training loss: 3167.33, average training loss: 5047.77, base loss: 4609.97
[INFO 2017-06-26 22:03:49,335 main.py:47] epoch 871, training loss: 4015.62, average training loss: 5046.59, base loss: 4610.34
[INFO 2017-06-26 22:03:49,735 main.py:47] epoch 872, training loss: 3953.79, average training loss: 5045.34, base loss: 4610.32
[INFO 2017-06-26 22:03:50,135 main.py:47] epoch 873, training loss: 3253.48, average training loss: 5043.29, base loss: 4609.17
[INFO 2017-06-26 22:03:50,538 main.py:47] epoch 874, training loss: 3960.33, average training loss: 5042.05, base loss: 4609.52
[INFO 2017-06-26 22:03:50,937 main.py:47] epoch 875, training loss: 3734.08, average training loss: 5040.56, base loss: 4609.10
[INFO 2017-06-26 22:03:51,342 main.py:47] epoch 876, training loss: 3452.61, average training loss: 5038.75, base loss: 4608.39
[INFO 2017-06-26 22:03:51,745 main.py:47] epoch 877, training loss: 3213.74, average training loss: 5036.67, base loss: 4607.43
[INFO 2017-06-26 22:03:52,146 main.py:47] epoch 878, training loss: 7309.67, average training loss: 5039.25, base loss: 4611.23
[INFO 2017-06-26 22:03:52,545 main.py:47] epoch 879, training loss: 3989.83, average training loss: 5038.06, base loss: 4611.35
[INFO 2017-06-26 22:03:52,945 main.py:47] epoch 880, training loss: 3732.83, average training loss: 5036.58, base loss: 4611.14
[INFO 2017-06-26 22:03:53,344 main.py:47] epoch 881, training loss: 3738.01, average training loss: 5035.11, base loss: 4611.18
[INFO 2017-06-26 22:03:53,743 main.py:47] epoch 882, training loss: 3668.27, average training loss: 5033.56, base loss: 4611.03
[INFO 2017-06-26 22:03:54,148 main.py:47] epoch 883, training loss: 3550.91, average training loss: 5031.88, base loss: 4610.48
[INFO 2017-06-26 22:03:54,549 main.py:47] epoch 884, training loss: 3219.92, average training loss: 5029.84, base loss: 4609.21
[INFO 2017-06-26 22:03:54,956 main.py:47] epoch 885, training loss: 3825.45, average training loss: 5028.48, base loss: 4609.28
[INFO 2017-06-26 22:03:55,359 main.py:47] epoch 886, training loss: 3403.66, average training loss: 5026.64, base loss: 4608.26
[INFO 2017-06-26 22:03:55,761 main.py:47] epoch 887, training loss: 3738.41, average training loss: 5025.19, base loss: 4607.92
[INFO 2017-06-26 22:03:56,161 main.py:47] epoch 888, training loss: 4441.56, average training loss: 5024.54, base loss: 4608.74
[INFO 2017-06-26 22:03:56,566 main.py:47] epoch 889, training loss: 3887.17, average training loss: 5023.26, base loss: 4608.70
[INFO 2017-06-26 22:03:56,968 main.py:47] epoch 890, training loss: 7490.61, average training loss: 5026.03, base loss: 4612.72
[INFO 2017-06-26 22:03:57,372 main.py:47] epoch 891, training loss: 3691.50, average training loss: 5024.53, base loss: 4612.61
[INFO 2017-06-26 22:03:57,774 main.py:47] epoch 892, training loss: 3788.10, average training loss: 5023.15, base loss: 4612.20
[INFO 2017-06-26 22:03:58,177 main.py:47] epoch 893, training loss: 3704.73, average training loss: 5021.67, base loss: 4612.02
[INFO 2017-06-26 22:03:58,577 main.py:47] epoch 894, training loss: 3818.43, average training loss: 5020.33, base loss: 4611.90
[INFO 2017-06-26 22:03:58,979 main.py:47] epoch 895, training loss: 3637.87, average training loss: 5018.79, base loss: 4611.93
[INFO 2017-06-26 22:03:59,382 main.py:47] epoch 896, training loss: 3797.97, average training loss: 5017.42, base loss: 4611.95
[INFO 2017-06-26 22:03:59,785 main.py:47] epoch 897, training loss: 3809.27, average training loss: 5016.08, base loss: 4612.06
[INFO 2017-06-26 22:04:00,185 main.py:47] epoch 898, training loss: 3386.45, average training loss: 5014.27, base loss: 4611.21
[INFO 2017-06-26 22:04:00,587 main.py:47] epoch 899, training loss: 3720.83, average training loss: 5012.83, base loss: 4611.05
[INFO 2017-06-26 22:04:00,588 main.py:49] epoch 899, testing
[INFO 2017-06-26 22:04:02,247 main.py:102] average testing loss: 3557.86, base loss: 4250.11
[INFO 2017-06-26 22:04:02,248 main.py:103] improve_loss: 692.25, improve_percent: 0.16
[INFO 2017-06-26 22:04:02,248 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 22:04:02,261 main.py:73] current best improved percent: 0.16
[INFO 2017-06-26 22:04:02,664 main.py:47] epoch 900, training loss: 4164.79, average training loss: 5011.89, base loss: 4611.55
[INFO 2017-06-26 22:04:03,066 main.py:47] epoch 901, training loss: 3526.63, average training loss: 5010.24, base loss: 4611.11
[INFO 2017-06-26 22:04:03,466 main.py:47] epoch 902, training loss: 3841.69, average training loss: 5008.95, base loss: 4610.96
[INFO 2017-06-26 22:04:03,868 main.py:47] epoch 903, training loss: 3150.43, average training loss: 5006.89, base loss: 4609.68
[INFO 2017-06-26 22:04:04,271 main.py:47] epoch 904, training loss: 3770.38, average training loss: 5005.53, base loss: 4609.63
[INFO 2017-06-26 22:04:04,680 main.py:47] epoch 905, training loss: 4035.14, average training loss: 5004.45, base loss: 4610.03
[INFO 2017-06-26 22:04:05,081 main.py:47] epoch 906, training loss: 3939.90, average training loss: 5003.28, base loss: 4610.33
[INFO 2017-06-26 22:04:05,482 main.py:47] epoch 907, training loss: 3505.83, average training loss: 5001.63, base loss: 4609.70
[INFO 2017-06-26 22:04:05,882 main.py:47] epoch 908, training loss: 3015.16, average training loss: 4999.45, base loss: 4608.35
[INFO 2017-06-26 22:04:06,281 main.py:47] epoch 909, training loss: 3404.22, average training loss: 4997.69, base loss: 4607.65
[INFO 2017-06-26 22:04:06,688 main.py:47] epoch 910, training loss: 3494.87, average training loss: 4996.04, base loss: 4607.26
[INFO 2017-06-26 22:04:07,090 main.py:47] epoch 911, training loss: 3670.09, average training loss: 4994.59, base loss: 4607.10
[INFO 2017-06-26 22:04:07,492 main.py:47] epoch 912, training loss: 3801.61, average training loss: 4993.28, base loss: 4607.20
[INFO 2017-06-26 22:04:07,895 main.py:47] epoch 913, training loss: 3420.37, average training loss: 4991.56, base loss: 4606.90
[INFO 2017-06-26 22:04:08,300 main.py:47] epoch 914, training loss: 3992.64, average training loss: 4990.47, base loss: 4607.25
[INFO 2017-06-26 22:04:08,698 main.py:47] epoch 915, training loss: 3650.87, average training loss: 4989.01, base loss: 4607.16
[INFO 2017-06-26 22:04:09,101 main.py:47] epoch 916, training loss: 3339.39, average training loss: 4987.21, base loss: 4606.28
[INFO 2017-06-26 22:04:09,499 main.py:47] epoch 917, training loss: 3914.85, average training loss: 4986.04, base loss: 4606.46
[INFO 2017-06-26 22:04:09,897 main.py:47] epoch 918, training loss: 3277.67, average training loss: 4984.18, base loss: 4605.61
[INFO 2017-06-26 22:04:10,300 main.py:47] epoch 919, training loss: 3726.02, average training loss: 4982.81, base loss: 4605.74
[INFO 2017-06-26 22:04:10,713 main.py:47] epoch 920, training loss: 3009.60, average training loss: 4980.67, base loss: 4604.34
[INFO 2017-06-26 22:04:11,115 main.py:47] epoch 921, training loss: 4898.62, average training loss: 4980.58, base loss: 4605.62
[INFO 2017-06-26 22:04:11,516 main.py:47] epoch 922, training loss: 3421.10, average training loss: 4978.89, base loss: 4605.04
[INFO 2017-06-26 22:04:11,919 main.py:47] epoch 923, training loss: 3299.60, average training loss: 4977.08, base loss: 4604.32
[INFO 2017-06-26 22:04:12,323 main.py:47] epoch 924, training loss: 3526.55, average training loss: 4975.51, base loss: 4603.86
[INFO 2017-06-26 22:04:12,721 main.py:47] epoch 925, training loss: 3552.64, average training loss: 4973.97, base loss: 4603.28
[INFO 2017-06-26 22:04:13,129 main.py:47] epoch 926, training loss: 3693.99, average training loss: 4972.59, base loss: 4602.91
[INFO 2017-06-26 22:04:13,531 main.py:47] epoch 927, training loss: 3226.61, average training loss: 4970.71, base loss: 4601.75
[INFO 2017-06-26 22:04:13,937 main.py:47] epoch 928, training loss: 3741.33, average training loss: 4969.39, base loss: 4601.67
[INFO 2017-06-26 22:04:14,340 main.py:47] epoch 929, training loss: 3881.78, average training loss: 4968.22, base loss: 4601.47
[INFO 2017-06-26 22:04:14,742 main.py:47] epoch 930, training loss: 3379.63, average training loss: 4966.51, base loss: 4600.96
[INFO 2017-06-26 22:04:15,141 main.py:47] epoch 931, training loss: 3724.70, average training loss: 4965.18, base loss: 4600.99
[INFO 2017-06-26 22:04:15,543 main.py:47] epoch 932, training loss: 7641.00, average training loss: 4968.05, base loss: 4605.29
[INFO 2017-06-26 22:04:15,942 main.py:47] epoch 933, training loss: 3668.74, average training loss: 4966.65, base loss: 4604.86
[INFO 2017-06-26 22:04:16,345 main.py:47] epoch 934, training loss: 3992.41, average training loss: 4965.61, base loss: 4605.12
[INFO 2017-06-26 22:04:16,745 main.py:47] epoch 935, training loss: 3757.85, average training loss: 4964.32, base loss: 4605.10
[INFO 2017-06-26 22:04:17,146 main.py:47] epoch 936, training loss: 4215.80, average training loss: 4963.52, base loss: 4605.51
[INFO 2017-06-26 22:04:17,546 main.py:47] epoch 937, training loss: 3602.19, average training loss: 4962.07, base loss: 4605.17
[INFO 2017-06-26 22:04:17,947 main.py:47] epoch 938, training loss: 3740.24, average training loss: 4960.77, base loss: 4604.97
[INFO 2017-06-26 22:04:18,351 main.py:47] epoch 939, training loss: 3682.24, average training loss: 4959.41, base loss: 4604.63
[INFO 2017-06-26 22:04:18,750 main.py:47] epoch 940, training loss: 3861.62, average training loss: 4958.24, base loss: 4604.50
[INFO 2017-06-26 22:04:19,150 main.py:47] epoch 941, training loss: 4322.90, average training loss: 4957.57, base loss: 4605.20
[INFO 2017-06-26 22:04:19,549 main.py:47] epoch 942, training loss: 3716.25, average training loss: 4956.25, base loss: 4604.82
[INFO 2017-06-26 22:04:19,953 main.py:47] epoch 943, training loss: 3612.19, average training loss: 4954.83, base loss: 4604.58
[INFO 2017-06-26 22:04:20,356 main.py:47] epoch 944, training loss: 4256.69, average training loss: 4954.09, base loss: 4605.22
[INFO 2017-06-26 22:04:20,759 main.py:47] epoch 945, training loss: 2964.31, average training loss: 4951.99, base loss: 4604.19
[INFO 2017-06-26 22:04:21,160 main.py:47] epoch 946, training loss: 3850.33, average training loss: 4950.82, base loss: 4604.00
[INFO 2017-06-26 22:04:21,558 main.py:47] epoch 947, training loss: 3487.76, average training loss: 4949.28, base loss: 4603.43
[INFO 2017-06-26 22:04:21,961 main.py:47] epoch 948, training loss: 4347.15, average training loss: 4948.65, base loss: 4604.15
[INFO 2017-06-26 22:04:22,363 main.py:47] epoch 949, training loss: 3756.38, average training loss: 4947.39, base loss: 4604.04
[INFO 2017-06-26 22:04:22,763 main.py:47] epoch 950, training loss: 3685.79, average training loss: 4946.06, base loss: 4603.76
[INFO 2017-06-26 22:04:23,163 main.py:47] epoch 951, training loss: 3531.06, average training loss: 4944.58, base loss: 4603.34
[INFO 2017-06-26 22:04:23,564 main.py:47] epoch 952, training loss: 3386.63, average training loss: 4942.94, base loss: 4602.55
[INFO 2017-06-26 22:04:23,966 main.py:47] epoch 953, training loss: 3902.94, average training loss: 4941.85, base loss: 4602.66
[INFO 2017-06-26 22:04:24,378 main.py:47] epoch 954, training loss: 3652.01, average training loss: 4940.50, base loss: 4602.57
[INFO 2017-06-26 22:04:24,781 main.py:47] epoch 955, training loss: 3714.31, average training loss: 4939.22, base loss: 4602.51
[INFO 2017-06-26 22:04:25,183 main.py:47] epoch 956, training loss: 3561.44, average training loss: 4937.78, base loss: 4602.35
[INFO 2017-06-26 22:04:25,585 main.py:47] epoch 957, training loss: 4328.63, average training loss: 4937.14, base loss: 4603.02
[INFO 2017-06-26 22:04:25,989 main.py:47] epoch 958, training loss: 4570.17, average training loss: 4936.76, base loss: 4604.23
[INFO 2017-06-26 22:04:26,389 main.py:47] epoch 959, training loss: 3854.07, average training loss: 4935.63, base loss: 4604.27
[INFO 2017-06-26 22:04:26,790 main.py:47] epoch 960, training loss: 3616.90, average training loss: 4934.26, base loss: 4604.13
[INFO 2017-06-26 22:04:27,193 main.py:47] epoch 961, training loss: 3870.84, average training loss: 4933.16, base loss: 4604.19
[INFO 2017-06-26 22:04:27,592 main.py:47] epoch 962, training loss: 3311.22, average training loss: 4931.47, base loss: 4603.42
[INFO 2017-06-26 22:04:27,992 main.py:47] epoch 963, training loss: 3552.24, average training loss: 4930.04, base loss: 4603.06
[INFO 2017-06-26 22:04:28,390 main.py:47] epoch 964, training loss: 3379.71, average training loss: 4928.43, base loss: 4602.54
[INFO 2017-06-26 22:04:28,791 main.py:47] epoch 965, training loss: 3831.05, average training loss: 4927.30, base loss: 4602.52
[INFO 2017-06-26 22:04:29,191 main.py:47] epoch 966, training loss: 3412.50, average training loss: 4925.73, base loss: 4602.14
[INFO 2017-06-26 22:04:29,590 main.py:47] epoch 967, training loss: 3814.98, average training loss: 4924.58, base loss: 4601.97
[INFO 2017-06-26 22:04:29,996 main.py:47] epoch 968, training loss: 4181.93, average training loss: 4923.82, base loss: 4602.51
[INFO 2017-06-26 22:04:30,399 main.py:47] epoch 969, training loss: 3875.72, average training loss: 4922.74, base loss: 4602.65
[INFO 2017-06-26 22:04:30,801 main.py:47] epoch 970, training loss: 3865.36, average training loss: 4921.65, base loss: 4602.72
[INFO 2017-06-26 22:04:31,208 main.py:47] epoch 971, training loss: 3994.19, average training loss: 4920.69, base loss: 4603.15
[INFO 2017-06-26 22:04:31,611 main.py:47] epoch 972, training loss: 4434.25, average training loss: 4920.19, base loss: 4604.08
[INFO 2017-06-26 22:04:32,013 main.py:47] epoch 973, training loss: 3923.28, average training loss: 4919.17, base loss: 4604.36
[INFO 2017-06-26 22:04:32,414 main.py:47] epoch 974, training loss: 3597.61, average training loss: 4917.82, base loss: 4603.92
[INFO 2017-06-26 22:04:32,817 main.py:47] epoch 975, training loss: 3332.28, average training loss: 4916.19, base loss: 4603.14
[INFO 2017-06-26 22:04:33,216 main.py:47] epoch 976, training loss: 3273.60, average training loss: 4914.51, base loss: 4602.45
[INFO 2017-06-26 22:04:33,618 main.py:47] epoch 977, training loss: 3261.63, average training loss: 4912.82, base loss: 4601.45
[INFO 2017-06-26 22:04:34,020 main.py:47] epoch 978, training loss: 4527.71, average training loss: 4912.43, base loss: 4602.02
[INFO 2017-06-26 22:04:34,423 main.py:47] epoch 979, training loss: 3493.83, average training loss: 4910.98, base loss: 4601.48
[INFO 2017-06-26 22:04:34,825 main.py:47] epoch 980, training loss: 3950.50, average training loss: 4910.00, base loss: 4601.44
[INFO 2017-06-26 22:04:35,223 main.py:47] epoch 981, training loss: 4025.31, average training loss: 4909.10, base loss: 4601.78
[INFO 2017-06-26 22:04:35,621 main.py:47] epoch 982, training loss: 3707.63, average training loss: 4907.88, base loss: 4601.45
[INFO 2017-06-26 22:04:36,019 main.py:47] epoch 983, training loss: 4068.16, average training loss: 4907.02, base loss: 4601.37
[INFO 2017-06-26 22:04:36,423 main.py:47] epoch 984, training loss: 3961.49, average training loss: 4906.06, base loss: 4601.32
[INFO 2017-06-26 22:04:36,825 main.py:47] epoch 985, training loss: 3359.24, average training loss: 4904.49, base loss: 4600.46
[INFO 2017-06-26 22:04:37,229 main.py:47] epoch 986, training loss: 4189.55, average training loss: 4903.77, base loss: 4600.76
[INFO 2017-06-26 22:04:37,631 main.py:47] epoch 987, training loss: 3426.43, average training loss: 4902.27, base loss: 4600.12
[INFO 2017-06-26 22:04:38,034 main.py:47] epoch 988, training loss: 4255.42, average training loss: 4901.62, base loss: 4600.61
[INFO 2017-06-26 22:04:38,443 main.py:47] epoch 989, training loss: 3320.27, average training loss: 4900.02, base loss: 4599.92
[INFO 2017-06-26 22:04:38,852 main.py:47] epoch 990, training loss: 3886.18, average training loss: 4899.00, base loss: 4599.96
[INFO 2017-06-26 22:04:39,256 main.py:47] epoch 991, training loss: 4036.49, average training loss: 4898.13, base loss: 4600.30
[INFO 2017-06-26 22:04:39,659 main.py:47] epoch 992, training loss: 4310.04, average training loss: 4897.54, base loss: 4601.11
[INFO 2017-06-26 22:04:40,058 main.py:47] epoch 993, training loss: 3798.84, average training loss: 4896.43, base loss: 4600.80
[INFO 2017-06-26 22:04:40,455 main.py:47] epoch 994, training loss: 3350.68, average training loss: 4894.88, base loss: 4600.17
[INFO 2017-06-26 22:04:40,858 main.py:47] epoch 995, training loss: 3652.67, average training loss: 4893.63, base loss: 4599.92
[INFO 2017-06-26 22:04:41,257 main.py:47] epoch 996, training loss: 3424.67, average training loss: 4892.16, base loss: 4599.43
[INFO 2017-06-26 22:04:41,660 main.py:47] epoch 997, training loss: 4079.05, average training loss: 4891.34, base loss: 4599.83
[INFO 2017-06-26 22:04:42,059 main.py:47] epoch 998, training loss: 4351.72, average training loss: 4890.80, base loss: 4600.82
[INFO 2017-06-26 22:04:42,464 main.py:47] epoch 999, training loss: 4028.24, average training loss: 4889.94, base loss: 4601.04
[INFO 2017-06-26 22:04:42,464 main.py:49] epoch 999, testing
[INFO 2017-06-26 22:04:44,123 main.py:102] average testing loss: 3943.61, base loss: 4553.79
[INFO 2017-06-26 22:04:44,123 main.py:103] improve_loss: 610.17, improve_percent: 0.13
[INFO 2017-06-26 22:04:44,123 main.py:73] current best improved percent: 0.16
[INFO 2017-06-26 22:04:44,525 main.py:47] epoch 1000, training loss: 3207.49, average training loss: 4756.17, base loss: 4598.80
[INFO 2017-06-26 22:04:44,923 main.py:47] epoch 1001, training loss: 3685.80, average training loss: 4657.32, base loss: 4598.17
[INFO 2017-06-26 22:04:45,325 main.py:47] epoch 1002, training loss: 3705.77, average training loss: 4574.29, base loss: 4598.20
[INFO 2017-06-26 22:04:45,725 main.py:47] epoch 1003, training loss: 3842.62, average training loss: 4507.43, base loss: 4597.67
[INFO 2017-06-26 22:04:46,125 main.py:47] epoch 1004, training loss: 3549.60, average training loss: 4447.32, base loss: 4593.41
[INFO 2017-06-26 22:04:46,528 main.py:47] epoch 1005, training loss: 3779.27, average training loss: 4396.61, base loss: 4590.33
[INFO 2017-06-26 22:04:46,933 main.py:47] epoch 1006, training loss: 3492.58, average training loss: 4356.15, base loss: 4589.35
[INFO 2017-06-26 22:04:47,336 main.py:47] epoch 1007, training loss: 3471.34, average training loss: 4318.87, base loss: 4585.83
[INFO 2017-06-26 22:04:47,738 main.py:47] epoch 1008, training loss: 4219.01, average training loss: 4291.01, base loss: 4586.95
[INFO 2017-06-26 22:04:48,137 main.py:47] epoch 1009, training loss: 3577.54, average training loss: 4267.19, base loss: 4586.25
[INFO 2017-06-26 22:04:48,540 main.py:47] epoch 1010, training loss: 3615.91, average training loss: 4247.56, base loss: 4586.52
[INFO 2017-06-26 22:04:48,936 main.py:47] epoch 1011, training loss: 3752.65, average training loss: 4230.98, base loss: 4586.81
[INFO 2017-06-26 22:04:49,332 main.py:47] epoch 1012, training loss: 3340.94, average training loss: 4216.13, base loss: 4586.75
[INFO 2017-06-26 22:04:49,734 main.py:47] epoch 1013, training loss: 4444.63, average training loss: 4204.40, base loss: 4587.99
[INFO 2017-06-26 22:04:50,135 main.py:47] epoch 1014, training loss: 3969.40, average training loss: 4194.09, base loss: 4588.81
[INFO 2017-06-26 22:04:50,533 main.py:47] epoch 1015, training loss: 3390.68, average training loss: 4184.82, base loss: 4588.08
[INFO 2017-06-26 22:04:50,932 main.py:47] epoch 1016, training loss: 7190.36, average training loss: 4180.79, base loss: 4592.28
[INFO 2017-06-26 22:04:51,330 main.py:47] epoch 1017, training loss: 3603.05, average training loss: 4174.08, base loss: 4591.71
[INFO 2017-06-26 22:04:51,728 main.py:47] epoch 1018, training loss: 3221.70, average training loss: 4168.32, base loss: 4591.21
[INFO 2017-06-26 22:04:52,128 main.py:47] epoch 1019, training loss: 6724.11, average training loss: 4166.11, base loss: 4593.75
[INFO 2017-06-26 22:04:52,527 main.py:47] epoch 1020, training loss: 4111.79, average training loss: 4162.05, base loss: 4594.18
[INFO 2017-06-26 22:04:52,928 main.py:47] epoch 1021, training loss: 4147.45, average training loss: 4158.55, base loss: 4594.80
[INFO 2017-06-26 22:04:53,329 main.py:47] epoch 1022, training loss: 3390.32, average training loss: 4154.32, base loss: 4593.97
[INFO 2017-06-26 22:04:53,727 main.py:47] epoch 1023, training loss: 3385.94, average training loss: 4150.84, base loss: 4593.25
[INFO 2017-06-26 22:04:54,127 main.py:47] epoch 1024, training loss: 4092.55, average training loss: 4147.68, base loss: 4592.34
[INFO 2017-06-26 22:04:54,530 main.py:47] epoch 1025, training loss: 3763.22, average training loss: 4145.10, base loss: 4592.37
[INFO 2017-06-26 22:04:54,934 main.py:47] epoch 1026, training loss: 3699.44, average training loss: 4139.75, base loss: 4589.13
[INFO 2017-06-26 22:04:55,339 main.py:47] epoch 1027, training loss: 4005.31, average training loss: 4134.76, base loss: 4586.16
[INFO 2017-06-26 22:04:55,738 main.py:47] epoch 1028, training loss: 3625.68, average training loss: 4133.01, base loss: 4586.16
[INFO 2017-06-26 22:04:56,139 main.py:47] epoch 1029, training loss: 3733.25, average training loss: 4131.36, base loss: 4586.16
[INFO 2017-06-26 22:04:56,544 main.py:47] epoch 1030, training loss: 3644.86, average training loss: 4129.59, base loss: 4586.08
[INFO 2017-06-26 22:04:56,947 main.py:47] epoch 1031, training loss: 3875.20, average training loss: 4128.50, base loss: 4586.49
[INFO 2017-06-26 22:04:57,349 main.py:47] epoch 1032, training loss: 3574.57, average training loss: 4125.86, base loss: 4585.01
[INFO 2017-06-26 22:04:57,749 main.py:47] epoch 1033, training loss: 3908.93, average training loss: 4124.53, base loss: 4584.98
[INFO 2017-06-26 22:04:58,156 main.py:47] epoch 1034, training loss: 3793.97, average training loss: 4124.11, base loss: 4586.07
[INFO 2017-06-26 22:04:58,558 main.py:47] epoch 1035, training loss: 3407.05, average training loss: 4123.02, base loss: 4585.84
[INFO 2017-06-26 22:04:58,957 main.py:47] epoch 1036, training loss: 3534.71, average training loss: 4121.61, base loss: 4585.34
[INFO 2017-06-26 22:04:59,357 main.py:47] epoch 1037, training loss: 3584.56, average training loss: 4120.48, base loss: 4585.41
[INFO 2017-06-26 22:04:59,760 main.py:47] epoch 1038, training loss: 3478.31, average training loss: 4119.51, base loss: 4585.55
[INFO 2017-06-26 22:05:00,163 main.py:47] epoch 1039, training loss: 3856.35, average training loss: 4119.00, base loss: 4586.36
[INFO 2017-06-26 22:05:00,567 main.py:47] epoch 1040, training loss: 3241.04, average training loss: 4117.12, base loss: 4585.41
[INFO 2017-06-26 22:05:00,971 main.py:47] epoch 1041, training loss: 3708.94, average training loss: 4116.39, base loss: 4585.89
[INFO 2017-06-26 22:05:01,373 main.py:47] epoch 1042, training loss: 3366.53, average training loss: 4111.56, base loss: 4582.15
[INFO 2017-06-26 22:05:01,776 main.py:47] epoch 1043, training loss: 3571.12, average training loss: 4110.48, base loss: 4582.09
[INFO 2017-06-26 22:05:02,178 main.py:47] epoch 1044, training loss: 3426.54, average training loss: 4108.69, base loss: 4581.27
[INFO 2017-06-26 22:05:02,580 main.py:47] epoch 1045, training loss: 3601.11, average training loss: 4107.79, base loss: 4581.32
[INFO 2017-06-26 22:05:02,981 main.py:47] epoch 1046, training loss: 3292.95, average training loss: 4106.09, base loss: 4580.17
[INFO 2017-06-26 22:05:03,387 main.py:47] epoch 1047, training loss: 3218.67, average training loss: 4105.24, base loss: 4580.03
[INFO 2017-06-26 22:05:03,790 main.py:47] epoch 1048, training loss: 3186.71, average training loss: 4104.02, base loss: 4579.60
[INFO 2017-06-26 22:05:04,192 main.py:47] epoch 1049, training loss: 7436.81, average training loss: 4106.53, base loss: 4583.08
[INFO 2017-06-26 22:05:04,597 main.py:47] epoch 1050, training loss: 3565.12, average training loss: 4105.90, base loss: 4583.46
[INFO 2017-06-26 22:05:05,000 main.py:47] epoch 1051, training loss: 3528.38, average training loss: 4104.77, base loss: 4583.53
[INFO 2017-06-26 22:05:05,402 main.py:47] epoch 1052, training loss: 4031.18, average training loss: 4104.39, base loss: 4583.91
[INFO 2017-06-26 22:05:05,804 main.py:47] epoch 1053, training loss: 3525.52, average training loss: 4102.28, base loss: 4582.67
[INFO 2017-06-26 22:05:06,209 main.py:47] epoch 1054, training loss: 3573.39, average training loss: 4101.03, base loss: 4582.06
[INFO 2017-06-26 22:05:06,614 main.py:47] epoch 1055, training loss: 4130.91, average training loss: 4099.92, base loss: 4582.14
[INFO 2017-06-26 22:05:07,016 main.py:47] epoch 1056, training loss: 3810.00, average training loss: 4098.70, base loss: 4581.73
[INFO 2017-06-26 22:05:07,419 main.py:47] epoch 1057, training loss: 4172.37, average training loss: 4098.09, base loss: 4582.19
[INFO 2017-06-26 22:05:07,821 main.py:47] epoch 1058, training loss: 3065.92, average training loss: 4096.62, base loss: 4581.18
[INFO 2017-06-26 22:05:08,227 main.py:47] epoch 1059, training loss: 3641.16, average training loss: 4095.06, base loss: 4580.95
[INFO 2017-06-26 22:05:08,628 main.py:47] epoch 1060, training loss: 3595.65, average training loss: 4093.82, base loss: 4580.69
[INFO 2017-06-26 22:05:09,032 main.py:47] epoch 1061, training loss: 3281.54, average training loss: 4091.67, base loss: 4579.25
[INFO 2017-06-26 22:05:09,439 main.py:47] epoch 1062, training loss: 3606.65, average training loss: 4090.31, base loss: 4578.84
[INFO 2017-06-26 22:05:09,840 main.py:47] epoch 1063, training loss: 3531.37, average training loss: 4088.69, base loss: 4578.15
[INFO 2017-06-26 22:05:10,238 main.py:47] epoch 1064, training loss: 4271.32, average training loss: 4087.65, base loss: 4578.36
[INFO 2017-06-26 22:05:10,638 main.py:47] epoch 1065, training loss: 3659.95, average training loss: 4086.74, base loss: 4578.25
[INFO 2017-06-26 22:05:11,040 main.py:47] epoch 1066, training loss: 3560.52, average training loss: 4086.14, base loss: 4578.62
[INFO 2017-06-26 22:05:11,442 main.py:47] epoch 1067, training loss: 3640.38, average training loss: 4085.20, base loss: 4578.53
[INFO 2017-06-26 22:05:11,846 main.py:47] epoch 1068, training loss: 4085.46, average training loss: 4084.43, base loss: 4578.73
[INFO 2017-06-26 22:05:12,245 main.py:47] epoch 1069, training loss: 2905.75, average training loss: 4082.39, base loss: 4577.16
[INFO 2017-06-26 22:05:12,646 main.py:47] epoch 1070, training loss: 3239.26, average training loss: 4081.24, base loss: 4576.65
[INFO 2017-06-26 22:05:13,044 main.py:47] epoch 1071, training loss: 3554.77, average training loss: 4080.38, base loss: 4576.68
[INFO 2017-06-26 22:05:13,464 main.py:47] epoch 1072, training loss: 4183.14, average training loss: 4080.38, base loss: 4577.85
[INFO 2017-06-26 22:05:13,867 main.py:47] epoch 1073, training loss: 3670.87, average training loss: 4079.39, base loss: 4577.82
[INFO 2017-06-26 22:05:14,271 main.py:47] epoch 1074, training loss: 3580.38, average training loss: 4078.16, base loss: 4577.33
[INFO 2017-06-26 22:05:14,670 main.py:47] epoch 1075, training loss: 4056.80, average training loss: 4077.52, base loss: 4577.20
[INFO 2017-06-26 22:05:15,075 main.py:47] epoch 1076, training loss: 3434.00, average training loss: 4076.22, base loss: 4576.74
[INFO 2017-06-26 22:05:15,478 main.py:47] epoch 1077, training loss: 3865.08, average training loss: 4075.56, base loss: 4577.22
[INFO 2017-06-26 22:05:15,878 main.py:47] epoch 1078, training loss: 3633.19, average training loss: 4074.24, base loss: 4576.57
[INFO 2017-06-26 22:05:16,281 main.py:47] epoch 1079, training loss: 3575.66, average training loss: 4073.66, base loss: 4576.47
[INFO 2017-06-26 22:05:16,683 main.py:47] epoch 1080, training loss: 4002.17, average training loss: 4073.81, base loss: 4577.49
[INFO 2017-06-26 22:05:17,083 main.py:47] epoch 1081, training loss: 3442.17, average training loss: 4072.75, base loss: 4577.18
[INFO 2017-06-26 22:05:17,488 main.py:47] epoch 1082, training loss: 3364.27, average training loss: 4071.13, base loss: 4576.16
[INFO 2017-06-26 22:05:17,888 main.py:47] epoch 1083, training loss: 3853.17, average training loss: 4069.89, base loss: 4575.79
[INFO 2017-06-26 22:05:18,291 main.py:47] epoch 1084, training loss: 3736.27, average training loss: 4068.89, base loss: 4575.75
[INFO 2017-06-26 22:05:18,692 main.py:47] epoch 1085, training loss: 3598.38, average training loss: 4068.02, base loss: 4575.79
[INFO 2017-06-26 22:05:19,096 main.py:47] epoch 1086, training loss: 4239.80, average training loss: 4067.81, base loss: 4576.60
[INFO 2017-06-26 22:05:19,495 main.py:47] epoch 1087, training loss: 3720.81, average training loss: 4067.85, base loss: 4577.65
[INFO 2017-06-26 22:05:19,897 main.py:47] epoch 1088, training loss: 3758.59, average training loss: 4067.04, base loss: 4577.67
[INFO 2017-06-26 22:05:20,300 main.py:47] epoch 1089, training loss: 3399.85, average training loss: 4066.16, base loss: 4577.33
[INFO 2017-06-26 22:05:20,699 main.py:47] epoch 1090, training loss: 3969.63, average training loss: 4065.82, base loss: 4578.01
[INFO 2017-06-26 22:05:21,102 main.py:47] epoch 1091, training loss: 3604.34, average training loss: 4064.51, base loss: 4577.45
[INFO 2017-06-26 22:05:21,509 main.py:47] epoch 1092, training loss: 4149.11, average training loss: 4063.09, base loss: 4576.85
[INFO 2017-06-26 22:05:21,913 main.py:47] epoch 1093, training loss: 4180.24, average training loss: 4062.82, base loss: 4577.62
[INFO 2017-06-26 22:05:22,321 main.py:47] epoch 1094, training loss: 3724.23, average training loss: 4062.55, base loss: 4578.08
[INFO 2017-06-26 22:05:22,723 main.py:47] epoch 1095, training loss: 3235.88, average training loss: 4060.26, base loss: 4576.39
[INFO 2017-06-26 22:05:23,124 main.py:47] epoch 1096, training loss: 3418.71, average training loss: 4059.47, base loss: 4576.37
[INFO 2017-06-26 22:05:23,524 main.py:47] epoch 1097, training loss: 3647.61, average training loss: 4058.61, base loss: 4576.27
[INFO 2017-06-26 22:05:23,928 main.py:47] epoch 1098, training loss: 3871.13, average training loss: 4058.24, base loss: 4576.51
[INFO 2017-06-26 22:05:24,330 main.py:47] epoch 1099, training loss: 3875.55, average training loss: 4057.89, base loss: 4576.87
[INFO 2017-06-26 22:05:24,330 main.py:49] epoch 1099, testing
[INFO 2017-06-26 22:05:25,990 main.py:102] average testing loss: 4534.52, base loss: 5405.76
[INFO 2017-06-26 22:05:25,990 main.py:103] improve_loss: 871.24, improve_percent: 0.16
[INFO 2017-06-26 22:05:25,991 main.py:73] current best improved percent: 0.16
[INFO 2017-06-26 22:05:26,392 main.py:47] epoch 1100, training loss: 3659.36, average training loss: 4057.02, base loss: 4576.49
[INFO 2017-06-26 22:05:26,795 main.py:47] epoch 1101, training loss: 3848.35, average training loss: 4056.34, base loss: 4576.58
[INFO 2017-06-26 22:05:27,201 main.py:47] epoch 1102, training loss: 3529.00, average training loss: 4056.21, base loss: 4577.39
[INFO 2017-06-26 22:05:27,603 main.py:47] epoch 1103, training loss: 3650.20, average training loss: 4055.58, base loss: 4577.35
[INFO 2017-06-26 22:05:28,007 main.py:47] epoch 1104, training loss: 3231.93, average training loss: 4054.37, base loss: 4576.84
[INFO 2017-06-26 22:05:28,407 main.py:47] epoch 1105, training loss: 4070.09, average training loss: 4054.30, base loss: 4577.92
[INFO 2017-06-26 22:05:28,803 main.py:47] epoch 1106, training loss: 3000.28, average training loss: 4053.22, base loss: 4577.13
[INFO 2017-06-26 22:05:29,206 main.py:47] epoch 1107, training loss: 3266.65, average training loss: 4052.13, base loss: 4576.39
[INFO 2017-06-26 22:05:29,608 main.py:47] epoch 1108, training loss: 3137.33, average training loss: 4050.63, base loss: 4575.33
[INFO 2017-06-26 22:05:30,010 main.py:47] epoch 1109, training loss: 3123.74, average training loss: 4049.98, base loss: 4575.38
[INFO 2017-06-26 22:05:30,412 main.py:47] epoch 1110, training loss: 3338.27, average training loss: 4050.00, base loss: 4576.11
[INFO 2017-06-26 22:05:30,814 main.py:47] epoch 1111, training loss: 3962.29, average training loss: 4049.48, base loss: 4576.10
[INFO 2017-06-26 22:05:31,218 main.py:47] epoch 1112, training loss: 3743.54, average training loss: 4045.19, base loss: 4572.61
[INFO 2017-06-26 22:05:31,620 main.py:47] epoch 1113, training loss: 4079.96, average training loss: 4044.49, base loss: 4572.72
[INFO 2017-06-26 22:05:32,024 main.py:47] epoch 1114, training loss: 3510.20, average training loss: 4043.27, base loss: 4572.20
[INFO 2017-06-26 22:05:32,429 main.py:47] epoch 1115, training loss: 3423.78, average training loss: 4042.58, base loss: 4572.17
[INFO 2017-06-26 22:05:32,831 main.py:47] epoch 1116, training loss: 3584.02, average training loss: 4042.04, base loss: 4572.57
[INFO 2017-06-26 22:05:33,233 main.py:47] epoch 1117, training loss: 3533.31, average training loss: 4040.82, base loss: 4572.14
[INFO 2017-06-26 22:05:33,632 main.py:47] epoch 1118, training loss: 4222.94, average training loss: 4037.20, base loss: 4569.09
[INFO 2017-06-26 22:05:34,034 main.py:47] epoch 1119, training loss: 3745.99, average training loss: 4036.33, base loss: 4569.13
[INFO 2017-06-26 22:05:34,437 main.py:47] epoch 1120, training loss: 3353.37, average training loss: 4035.16, base loss: 4568.54
[INFO 2017-06-26 22:05:34,844 main.py:47] epoch 1121, training loss: 4151.55, average training loss: 4035.67, base loss: 4570.01
[INFO 2017-06-26 22:05:35,244 main.py:47] epoch 1122, training loss: 3401.21, average training loss: 4034.98, base loss: 4570.17
[INFO 2017-06-26 22:05:35,650 main.py:47] epoch 1123, training loss: 3837.91, average training loss: 4035.15, base loss: 4571.12
[INFO 2017-06-26 22:05:36,056 main.py:47] epoch 1124, training loss: 3154.84, average training loss: 4033.59, base loss: 4570.11
[INFO 2017-06-26 22:05:36,459 main.py:47] epoch 1125, training loss: 3507.89, average training loss: 4032.33, base loss: 4569.37
[INFO 2017-06-26 22:05:36,861 main.py:47] epoch 1126, training loss: 3303.37, average training loss: 4031.33, base loss: 4568.83
[INFO 2017-06-26 22:05:37,264 main.py:47] epoch 1127, training loss: 3326.81, average training loss: 4029.97, base loss: 4568.16
[INFO 2017-06-26 22:05:37,664 main.py:47] epoch 1128, training loss: 3813.40, average training loss: 4030.21, base loss: 4569.20
[INFO 2017-06-26 22:05:38,063 main.py:47] epoch 1129, training loss: 3507.58, average training loss: 4029.33, base loss: 4568.94
[INFO 2017-06-26 22:05:38,463 main.py:47] epoch 1130, training loss: 3742.56, average training loss: 4027.67, base loss: 4567.81
[INFO 2017-06-26 22:05:38,865 main.py:47] epoch 1131, training loss: 3758.89, average training loss: 4026.42, base loss: 4567.07
[INFO 2017-06-26 22:05:39,265 main.py:47] epoch 1132, training loss: 3515.97, average training loss: 4026.06, base loss: 4567.55
[INFO 2017-06-26 22:05:39,668 main.py:47] epoch 1133, training loss: 3360.94, average training loss: 4024.75, base loss: 4566.91
[INFO 2017-06-26 22:05:40,070 main.py:47] epoch 1134, training loss: 3542.34, average training loss: 4024.23, base loss: 4567.04
[INFO 2017-06-26 22:05:40,470 main.py:47] epoch 1135, training loss: 4267.71, average training loss: 4024.00, base loss: 4567.84
[INFO 2017-06-26 22:05:40,874 main.py:47] epoch 1136, training loss: 3186.88, average training loss: 4022.61, base loss: 4566.70
[INFO 2017-06-26 22:05:41,274 main.py:47] epoch 1137, training loss: 4005.25, average training loss: 4022.88, base loss: 4567.91
[INFO 2017-06-26 22:05:41,677 main.py:47] epoch 1138, training loss: 3637.61, average training loss: 4022.70, base loss: 4568.43
[INFO 2017-06-26 22:05:42,076 main.py:47] epoch 1139, training loss: 3539.33, average training loss: 4022.55, base loss: 4568.95
[INFO 2017-06-26 22:05:42,472 main.py:47] epoch 1140, training loss: 4224.03, average training loss: 4022.83, base loss: 4570.15
[INFO 2017-06-26 22:05:42,869 main.py:47] epoch 1141, training loss: 4888.59, average training loss: 4023.48, base loss: 4571.78
[INFO 2017-06-26 22:05:43,269 main.py:47] epoch 1142, training loss: 3501.78, average training loss: 4023.09, base loss: 4572.35
[INFO 2017-06-26 22:05:43,674 main.py:47] epoch 1143, training loss: 3856.16, average training loss: 4023.17, base loss: 4573.45
[INFO 2017-06-26 22:05:44,077 main.py:47] epoch 1144, training loss: 3497.29, average training loss: 4022.36, base loss: 4573.15
[INFO 2017-06-26 22:05:44,477 main.py:47] epoch 1145, training loss: 4221.55, average training loss: 4021.97, base loss: 4573.61
[INFO 2017-06-26 22:05:44,880 main.py:47] epoch 1146, training loss: 4043.20, average training loss: 4018.04, base loss: 4570.53
[INFO 2017-06-26 22:05:45,282 main.py:47] epoch 1147, training loss: 3630.44, average training loss: 4017.47, base loss: 4570.61
[INFO 2017-06-26 22:05:45,685 main.py:47] epoch 1148, training loss: 3122.95, average training loss: 4016.72, base loss: 4570.42
[INFO 2017-06-26 22:05:46,087 main.py:47] epoch 1149, training loss: 7358.05, average training loss: 4020.04, base loss: 4574.40
[INFO 2017-06-26 22:05:46,489 main.py:47] epoch 1150, training loss: 3816.74, average training loss: 4019.80, base loss: 4574.96
[INFO 2017-06-26 22:05:46,890 main.py:47] epoch 1151, training loss: 3611.23, average training loss: 4018.56, base loss: 4574.44
[INFO 2017-06-26 22:05:47,292 main.py:47] epoch 1152, training loss: 3861.99, average training loss: 4018.40, base loss: 4575.00
[INFO 2017-06-26 22:05:47,695 main.py:47] epoch 1153, training loss: 3569.02, average training loss: 4017.87, base loss: 4575.07
[INFO 2017-06-26 22:05:48,097 main.py:47] epoch 1154, training loss: 3944.64, average training loss: 4017.98, base loss: 4576.37
[INFO 2017-06-26 22:05:48,496 main.py:47] epoch 1155, training loss: 4432.16, average training loss: 4018.23, base loss: 4577.80
[INFO 2017-06-26 22:05:48,896 main.py:47] epoch 1156, training loss: 3793.43, average training loss: 4018.10, base loss: 4578.41
[INFO 2017-06-26 22:05:49,297 main.py:47] epoch 1157, training loss: 4252.31, average training loss: 4017.93, base loss: 4579.14
[INFO 2017-06-26 22:05:49,706 main.py:47] epoch 1158, training loss: 3677.43, average training loss: 4013.23, base loss: 4574.93
[INFO 2017-06-26 22:05:50,106 main.py:47] epoch 1159, training loss: 3184.48, average training loss: 4012.06, base loss: 4574.12
[INFO 2017-06-26 22:05:50,507 main.py:47] epoch 1160, training loss: 3634.28, average training loss: 4010.96, base loss: 4573.53
[INFO 2017-06-26 22:05:50,909 main.py:47] epoch 1161, training loss: 3544.24, average training loss: 4011.04, base loss: 4574.20
[INFO 2017-06-26 22:05:51,312 main.py:47] epoch 1162, training loss: 3617.56, average training loss: 4010.41, base loss: 4574.13
[INFO 2017-06-26 22:05:51,713 main.py:47] epoch 1163, training loss: 4303.69, average training loss: 4010.29, base loss: 4574.74
[INFO 2017-06-26 22:05:52,115 main.py:47] epoch 1164, training loss: 3787.93, average training loss: 4009.88, base loss: 4575.18
[INFO 2017-06-26 22:05:52,516 main.py:47] epoch 1165, training loss: 3930.16, average training loss: 4009.87, base loss: 4576.10
[INFO 2017-06-26 22:05:52,915 main.py:47] epoch 1166, training loss: 4426.29, average training loss: 4010.23, base loss: 4577.20
[INFO 2017-06-26 22:05:53,317 main.py:47] epoch 1167, training loss: 3362.25, average training loss: 4009.86, base loss: 4577.25
[INFO 2017-06-26 22:05:53,720 main.py:47] epoch 1168, training loss: 3426.10, average training loss: 4008.32, base loss: 4576.18
[INFO 2017-06-26 22:05:54,119 main.py:47] epoch 1169, training loss: 3206.02, average training loss: 4007.29, base loss: 4575.52
[INFO 2017-06-26 22:05:54,521 main.py:47] epoch 1170, training loss: 4527.73, average training loss: 4007.18, base loss: 4576.59
[INFO 2017-06-26 22:05:54,926 main.py:47] epoch 1171, training loss: 6464.80, average training loss: 4009.72, base loss: 4579.54
[INFO 2017-06-26 22:05:55,329 main.py:47] epoch 1172, training loss: 3829.27, average training loss: 4009.31, base loss: 4580.11
[INFO 2017-06-26 22:05:55,734 main.py:47] epoch 1173, training loss: 3764.71, average training loss: 4008.75, base loss: 4580.07
[INFO 2017-06-26 22:05:56,136 main.py:47] epoch 1174, training loss: 4070.64, average training loss: 4008.89, base loss: 4580.79
[INFO 2017-06-26 22:05:56,537 main.py:47] epoch 1175, training loss: 3456.25, average training loss: 4008.55, base loss: 4581.07
[INFO 2017-06-26 22:05:56,939 main.py:47] epoch 1176, training loss: 3713.29, average training loss: 4007.80, base loss: 4580.88
[INFO 2017-06-26 22:05:57,339 main.py:47] epoch 1177, training loss: 3464.91, average training loss: 4007.22, base loss: 4580.64
[INFO 2017-06-26 22:05:57,740 main.py:47] epoch 1178, training loss: 3594.95, average training loss: 4007.33, base loss: 4581.50
[INFO 2017-06-26 22:05:58,142 main.py:47] epoch 1179, training loss: 4013.41, average training loss: 4006.72, base loss: 4581.73
[INFO 2017-06-26 22:05:58,543 main.py:47] epoch 1180, training loss: 3749.44, average training loss: 4006.40, base loss: 4582.13
[INFO 2017-06-26 22:05:58,949 main.py:47] epoch 1181, training loss: 3634.98, average training loss: 4006.06, base loss: 4582.34
[INFO 2017-06-26 22:05:59,352 main.py:47] epoch 1182, training loss: 4163.78, average training loss: 4006.44, base loss: 4583.49
[INFO 2017-06-26 22:05:59,754 main.py:47] epoch 1183, training loss: 3465.07, average training loss: 4002.30, base loss: 4579.94
[INFO 2017-06-26 22:06:00,155 main.py:47] epoch 1184, training loss: 4036.67, average training loss: 4002.06, base loss: 4580.74
[INFO 2017-06-26 22:06:00,555 main.py:47] epoch 1185, training loss: 6969.43, average training loss: 4005.37, base loss: 4584.44
[INFO 2017-06-26 22:06:00,956 main.py:47] epoch 1186, training loss: 3224.13, average training loss: 4004.74, base loss: 4584.26
[INFO 2017-06-26 22:06:01,355 main.py:47] epoch 1187, training loss: 3557.77, average training loss: 4004.36, base loss: 4584.34
[INFO 2017-06-26 22:06:01,760 main.py:47] epoch 1188, training loss: 3366.76, average training loss: 3999.91, base loss: 4580.20
[INFO 2017-06-26 22:06:02,161 main.py:47] epoch 1189, training loss: 3783.17, average training loss: 4000.14, base loss: 4581.18
[INFO 2017-06-26 22:06:02,573 main.py:47] epoch 1190, training loss: 3785.38, average training loss: 4000.23, base loss: 4582.16
[INFO 2017-06-26 22:06:02,975 main.py:47] epoch 1191, training loss: 4064.56, average training loss: 4000.27, base loss: 4583.08
[INFO 2017-06-26 22:06:03,379 main.py:47] epoch 1192, training loss: 3042.01, average training loss: 3998.41, base loss: 4581.39
[INFO 2017-06-26 22:06:03,775 main.py:47] epoch 1193, training loss: 3286.89, average training loss: 3997.42, base loss: 4580.80
[INFO 2017-06-26 22:06:04,177 main.py:47] epoch 1194, training loss: 2950.16, average training loss: 3995.65, base loss: 4579.17
[INFO 2017-06-26 22:06:04,580 main.py:47] epoch 1195, training loss: 4162.43, average training loss: 3995.51, base loss: 4579.49
[INFO 2017-06-26 22:06:04,983 main.py:47] epoch 1196, training loss: 3793.41, average training loss: 3995.03, base loss: 4579.59
[INFO 2017-06-26 22:06:05,382 main.py:47] epoch 1197, training loss: 4016.47, average training loss: 3993.87, base loss: 4579.22
[INFO 2017-06-26 22:06:05,783 main.py:47] epoch 1198, training loss: 3593.17, average training loss: 3993.50, base loss: 4579.55
[INFO 2017-06-26 22:06:06,185 main.py:47] epoch 1199, training loss: 3654.30, average training loss: 3992.94, base loss: 4579.67
[INFO 2017-06-26 22:06:06,185 main.py:49] epoch 1199, testing
[INFO 2017-06-26 22:06:07,841 main.py:102] average testing loss: 3861.68, base loss: 4633.14
[INFO 2017-06-26 22:06:07,841 main.py:103] improve_loss: 771.46, improve_percent: 0.17
[INFO 2017-06-26 22:06:07,842 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 22:06:07,854 main.py:73] current best improved percent: 0.17
[INFO 2017-06-26 22:06:08,254 main.py:47] epoch 1200, training loss: 3638.06, average training loss: 3992.02, base loss: 4579.14
[INFO 2017-06-26 22:06:08,658 main.py:47] epoch 1201, training loss: 3997.87, average training loss: 3991.31, base loss: 4579.27
[INFO 2017-06-26 22:06:09,061 main.py:47] epoch 1202, training loss: 3366.27, average training loss: 3989.85, base loss: 4578.43
[INFO 2017-06-26 22:06:09,464 main.py:47] epoch 1203, training loss: 3291.55, average training loss: 3989.16, base loss: 4578.32
[INFO 2017-06-26 22:06:09,866 main.py:47] epoch 1204, training loss: 3585.99, average training loss: 3988.96, base loss: 4578.82
[INFO 2017-06-26 22:06:10,264 main.py:47] epoch 1205, training loss: 3330.48, average training loss: 3987.89, base loss: 4578.12
[INFO 2017-06-26 22:06:10,663 main.py:47] epoch 1206, training loss: 3409.09, average training loss: 3987.80, base loss: 4578.74
[INFO 2017-06-26 22:06:11,065 main.py:47] epoch 1207, training loss: 3698.23, average training loss: 3987.28, base loss: 4578.78
[INFO 2017-06-26 22:06:11,471 main.py:47] epoch 1208, training loss: 3546.11, average training loss: 3986.80, base loss: 4579.19
[INFO 2017-06-26 22:06:11,877 main.py:47] epoch 1209, training loss: 3840.84, average training loss: 3986.32, base loss: 4579.69
[INFO 2017-06-26 22:06:12,284 main.py:47] epoch 1210, training loss: 3372.44, average training loss: 3985.15, base loss: 4578.92
[INFO 2017-06-26 22:06:12,685 main.py:47] epoch 1211, training loss: 3408.15, average training loss: 3984.28, base loss: 4578.83
[INFO 2017-06-26 22:06:13,094 main.py:47] epoch 1212, training loss: 3824.11, average training loss: 3984.18, base loss: 4579.47
[INFO 2017-06-26 22:06:13,503 main.py:47] epoch 1213, training loss: 4119.95, average training loss: 3984.62, base loss: 4580.79
[INFO 2017-06-26 22:06:13,910 main.py:47] epoch 1214, training loss: 3841.29, average training loss: 3983.85, base loss: 4580.34
[INFO 2017-06-26 22:06:14,313 main.py:47] epoch 1215, training loss: 3276.82, average training loss: 3983.11, base loss: 4580.14
[INFO 2017-06-26 22:06:14,713 main.py:47] epoch 1216, training loss: 3440.18, average training loss: 3982.26, base loss: 4579.59
[INFO 2017-06-26 22:06:15,113 main.py:47] epoch 1217, training loss: 3470.13, average training loss: 3981.12, base loss: 4578.84
[INFO 2017-06-26 22:06:15,522 main.py:47] epoch 1218, training loss: 4399.50, average training loss: 3981.60, base loss: 4580.19
[INFO 2017-06-26 22:06:15,924 main.py:47] epoch 1219, training loss: 7171.47, average training loss: 3984.73, base loss: 4583.72
[INFO 2017-06-26 22:06:16,334 main.py:47] epoch 1220, training loss: 4063.74, average training loss: 3984.64, base loss: 4584.28
[INFO 2017-06-26 22:06:16,745 main.py:47] epoch 1221, training loss: 3405.06, average training loss: 3984.21, base loss: 4584.38
[INFO 2017-06-26 22:06:17,151 main.py:47] epoch 1222, training loss: 7433.85, average training loss: 3987.22, base loss: 4588.21
[INFO 2017-06-26 22:06:17,566 main.py:47] epoch 1223, training loss: 3399.99, average training loss: 3986.51, base loss: 4587.87
[INFO 2017-06-26 22:06:17,978 main.py:47] epoch 1224, training loss: 3562.62, average training loss: 3985.38, base loss: 4587.19
[INFO 2017-06-26 22:06:18,385 main.py:47] epoch 1225, training loss: 3587.26, average training loss: 3985.14, base loss: 4587.68
[INFO 2017-06-26 22:06:18,797 main.py:47] epoch 1226, training loss: 3798.02, average training loss: 3984.93, base loss: 4588.18
[INFO 2017-06-26 22:06:19,219 main.py:47] epoch 1227, training loss: 4266.00, average training loss: 3985.17, base loss: 4589.31
[INFO 2017-06-26 22:06:19,624 main.py:47] epoch 1228, training loss: 3334.60, average training loss: 3984.17, base loss: 4588.69
[INFO 2017-06-26 22:06:20,026 main.py:47] epoch 1229, training loss: 3724.87, average training loss: 3983.43, base loss: 4588.34
[INFO 2017-06-26 22:06:20,438 main.py:47] epoch 1230, training loss: 3444.84, average training loss: 3981.97, base loss: 4587.41
[INFO 2017-06-26 22:06:20,849 main.py:47] epoch 1231, training loss: 3648.87, average training loss: 3977.92, base loss: 4583.98
[INFO 2017-06-26 22:06:21,263 main.py:47] epoch 1232, training loss: 3431.14, average training loss: 3977.67, base loss: 4584.42
[INFO 2017-06-26 22:06:21,670 main.py:47] epoch 1233, training loss: 3839.00, average training loss: 3974.23, base loss: 4581.74
[INFO 2017-06-26 22:06:22,076 main.py:47] epoch 1234, training loss: 3701.55, average training loss: 3973.78, base loss: 4581.77
[INFO 2017-06-26 22:06:22,482 main.py:47] epoch 1235, training loss: 4041.97, average training loss: 3973.67, base loss: 4582.21
[INFO 2017-06-26 22:06:22,885 main.py:47] epoch 1236, training loss: 3497.25, average training loss: 3973.20, base loss: 4582.28
[INFO 2017-06-26 22:06:23,286 main.py:47] epoch 1237, training loss: 3813.70, average training loss: 3973.41, base loss: 4583.38
[INFO 2017-06-26 22:06:23,687 main.py:47] epoch 1238, training loss: 3915.35, average training loss: 3973.42, base loss: 4584.14
[INFO 2017-06-26 22:06:24,097 main.py:47] epoch 1239, training loss: 3664.22, average training loss: 3973.66, base loss: 4584.90
[INFO 2017-06-26 22:06:24,503 main.py:47] epoch 1240, training loss: 3907.73, average training loss: 3973.23, base loss: 4585.17
[INFO 2017-06-26 22:06:24,906 main.py:47] epoch 1241, training loss: 3598.73, average training loss: 3972.64, base loss: 4585.28
[INFO 2017-06-26 22:06:25,307 main.py:47] epoch 1242, training loss: 3624.97, average training loss: 3971.94, base loss: 4585.17
[INFO 2017-06-26 22:06:25,711 main.py:47] epoch 1243, training loss: 3665.31, average training loss: 3971.64, base loss: 4585.29
[INFO 2017-06-26 22:06:26,114 main.py:47] epoch 1244, training loss: 4186.42, average training loss: 3971.17, base loss: 4585.73
[INFO 2017-06-26 22:06:26,524 main.py:47] epoch 1245, training loss: 3528.22, average training loss: 3970.79, base loss: 4586.07
[INFO 2017-06-26 22:06:26,927 main.py:47] epoch 1246, training loss: 7417.95, average training loss: 3974.08, base loss: 4589.72
[INFO 2017-06-26 22:06:27,335 main.py:47] epoch 1247, training loss: 3219.67, average training loss: 3973.19, base loss: 4589.10
[INFO 2017-06-26 22:06:27,734 main.py:47] epoch 1248, training loss: 3760.19, average training loss: 3969.09, base loss: 4585.64
[INFO 2017-06-26 22:06:28,143 main.py:47] epoch 1249, training loss: 3498.81, average training loss: 3968.57, base loss: 4585.81
[INFO 2017-06-26 22:06:28,551 main.py:47] epoch 1250, training loss: 3497.37, average training loss: 3964.57, base loss: 4582.24
[INFO 2017-06-26 22:06:28,955 main.py:47] epoch 1251, training loss: 3475.25, average training loss: 3963.24, base loss: 4581.24
[INFO 2017-06-26 22:06:29,363 main.py:47] epoch 1252, training loss: 3283.02, average training loss: 3962.44, base loss: 4580.85
[INFO 2017-06-26 22:06:29,769 main.py:47] epoch 1253, training loss: 3469.58, average training loss: 3958.56, base loss: 4577.51
[INFO 2017-06-26 22:06:30,203 main.py:47] epoch 1254, training loss: 3264.21, average training loss: 3957.72, base loss: 4576.97
[INFO 2017-06-26 22:06:30,635 main.py:47] epoch 1255, training loss: 4481.73, average training loss: 3958.18, base loss: 4578.09
[INFO 2017-06-26 22:06:31,070 main.py:47] epoch 1256, training loss: 3303.36, average training loss: 3957.44, base loss: 4577.74
[INFO 2017-06-26 22:06:31,478 main.py:47] epoch 1257, training loss: 3599.37, average training loss: 3956.96, base loss: 4577.84
[INFO 2017-06-26 22:06:31,926 main.py:47] epoch 1258, training loss: 3570.47, average training loss: 3956.98, base loss: 4578.26
[INFO 2017-06-26 22:06:32,404 main.py:47] epoch 1259, training loss: 3952.12, average training loss: 3957.28, base loss: 4579.15
[INFO 2017-06-26 22:06:32,878 main.py:47] epoch 1260, training loss: 3507.14, average training loss: 3956.21, base loss: 4578.36
[INFO 2017-06-26 22:06:33,375 main.py:47] epoch 1261, training loss: 4230.29, average training loss: 3956.32, base loss: 4579.16
[INFO 2017-06-26 22:06:33,836 main.py:47] epoch 1262, training loss: 3835.54, average training loss: 3955.80, base loss: 4579.14
[INFO 2017-06-26 22:06:34,251 main.py:47] epoch 1263, training loss: 3086.30, average training loss: 3954.97, base loss: 4578.52
[INFO 2017-06-26 22:06:34,728 main.py:47] epoch 1264, training loss: 3782.54, average training loss: 3954.20, base loss: 4578.64
[INFO 2017-06-26 22:06:35,225 main.py:47] epoch 1265, training loss: 3554.53, average training loss: 3953.66, base loss: 4578.43
[INFO 2017-06-26 22:06:35,657 main.py:47] epoch 1266, training loss: 3796.14, average training loss: 3952.65, base loss: 4577.89
[INFO 2017-06-26 22:06:36,068 main.py:47] epoch 1267, training loss: 3074.91, average training loss: 3951.63, base loss: 4577.05
[INFO 2017-06-26 22:06:36,469 main.py:47] epoch 1268, training loss: 3127.45, average training loss: 3950.39, base loss: 4576.09
[INFO 2017-06-26 22:06:36,865 main.py:47] epoch 1269, training loss: 7349.73, average training loss: 3953.49, base loss: 4579.53
[INFO 2017-06-26 22:06:37,270 main.py:47] epoch 1270, training loss: 3431.73, average training loss: 3952.35, base loss: 4578.58
[INFO 2017-06-26 22:06:37,677 main.py:47] epoch 1271, training loss: 3019.53, average training loss: 3951.66, base loss: 4577.93
[INFO 2017-06-26 22:06:38,106 main.py:47] epoch 1272, training loss: 3627.41, average training loss: 3951.03, base loss: 4577.61
[INFO 2017-06-26 22:06:38,509 main.py:47] epoch 1273, training loss: 3248.24, average training loss: 3950.47, base loss: 4577.31
[INFO 2017-06-26 22:06:38,910 main.py:47] epoch 1274, training loss: 3061.44, average training loss: 3949.83, base loss: 4577.26
[INFO 2017-06-26 22:06:39,307 main.py:47] epoch 1275, training loss: 3367.42, average training loss: 3948.55, base loss: 4576.36
[INFO 2017-06-26 22:06:39,713 main.py:47] epoch 1276, training loss: 7429.57, average training loss: 3951.40, base loss: 4579.71
[INFO 2017-06-26 22:06:40,113 main.py:47] epoch 1277, training loss: 3439.95, average training loss: 3950.28, base loss: 4578.78
[INFO 2017-06-26 22:06:40,515 main.py:47] epoch 1278, training loss: 3305.39, average training loss: 3949.19, base loss: 4577.92
[INFO 2017-06-26 22:06:40,920 main.py:47] epoch 1279, training loss: 3253.80, average training loss: 3947.63, base loss: 4576.55
[INFO 2017-06-26 22:06:41,320 main.py:47] epoch 1280, training loss: 3183.59, average training loss: 3946.73, base loss: 4575.75
[INFO 2017-06-26 22:06:41,718 main.py:47] epoch 1281, training loss: 3179.25, average training loss: 3945.77, base loss: 4575.27
[INFO 2017-06-26 22:06:42,116 main.py:47] epoch 1282, training loss: 3350.22, average training loss: 3944.81, base loss: 4574.98
[INFO 2017-06-26 22:06:42,514 main.py:47] epoch 1283, training loss: 3499.44, average training loss: 3944.56, base loss: 4575.37
[INFO 2017-06-26 22:06:42,916 main.py:47] epoch 1284, training loss: 3341.64, average training loss: 3943.13, base loss: 4574.14
[INFO 2017-06-26 22:06:43,315 main.py:47] epoch 1285, training loss: 3712.68, average training loss: 3942.28, base loss: 4573.84
[INFO 2017-06-26 22:06:43,717 main.py:47] epoch 1286, training loss: 3846.84, average training loss: 3942.25, base loss: 4574.41
[INFO 2017-06-26 22:06:44,117 main.py:47] epoch 1287, training loss: 3460.28, average training loss: 3941.54, base loss: 4574.15
[INFO 2017-06-26 22:06:44,515 main.py:47] epoch 1288, training loss: 3707.97, average training loss: 3941.70, base loss: 4575.12
[INFO 2017-06-26 22:06:44,918 main.py:47] epoch 1289, training loss: 3756.77, average training loss: 3941.49, base loss: 4575.73
[INFO 2017-06-26 22:06:45,318 main.py:47] epoch 1290, training loss: 3779.66, average training loss: 3941.44, base loss: 4576.22
[INFO 2017-06-26 22:06:45,719 main.py:47] epoch 1291, training loss: 3215.51, average training loss: 3936.76, base loss: 4572.11
[INFO 2017-06-26 22:06:46,117 main.py:47] epoch 1292, training loss: 2870.59, average training loss: 3936.06, base loss: 4571.70
[INFO 2017-06-26 22:06:46,514 main.py:47] epoch 1293, training loss: 3406.53, average training loss: 3934.78, base loss: 4570.87
[INFO 2017-06-26 22:06:46,912 main.py:47] epoch 1294, training loss: 3489.64, average training loss: 3934.36, base loss: 4571.05
[INFO 2017-06-26 22:06:47,310 main.py:47] epoch 1295, training loss: 3550.28, average training loss: 3933.55, base loss: 4570.59
[INFO 2017-06-26 22:06:47,711 main.py:47] epoch 1296, training loss: 3783.89, average training loss: 3932.90, base loss: 4570.32
[INFO 2017-06-26 22:06:48,109 main.py:47] epoch 1297, training loss: 3559.77, average training loss: 3932.58, base loss: 4570.70
[INFO 2017-06-26 22:06:48,506 main.py:47] epoch 1298, training loss: 3926.90, average training loss: 3932.41, base loss: 4571.04
[INFO 2017-06-26 22:06:48,905 main.py:47] epoch 1299, training loss: 3372.44, average training loss: 3932.15, base loss: 4571.29
[INFO 2017-06-26 22:06:48,905 main.py:49] epoch 1299, testing
[INFO 2017-06-26 22:06:50,556 main.py:102] average testing loss: 3623.64, base loss: 4481.49
[INFO 2017-06-26 22:06:50,556 main.py:103] improve_loss: 857.85, improve_percent: 0.19
[INFO 2017-06-26 22:06:50,557 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 22:06:50,569 main.py:73] current best improved percent: 0.19
[INFO 2017-06-26 22:06:50,974 main.py:47] epoch 1300, training loss: 7306.57, average training loss: 3935.18, base loss: 4574.84
[INFO 2017-06-26 22:06:51,375 main.py:47] epoch 1301, training loss: 3330.63, average training loss: 3934.48, base loss: 4574.55
[INFO 2017-06-26 22:06:51,777 main.py:47] epoch 1302, training loss: 3714.58, average training loss: 3934.80, base loss: 4575.65
[INFO 2017-06-26 22:06:52,176 main.py:47] epoch 1303, training loss: 2808.36, average training loss: 3933.18, base loss: 4574.20
[INFO 2017-06-26 22:06:52,575 main.py:47] epoch 1304, training loss: 3406.96, average training loss: 3932.37, base loss: 4573.82
[INFO 2017-06-26 22:06:52,977 main.py:47] epoch 1305, training loss: 3860.76, average training loss: 3931.36, base loss: 4573.21
[INFO 2017-06-26 22:06:53,384 main.py:47] epoch 1306, training loss: 4033.19, average training loss: 3930.90, base loss: 4573.68
[INFO 2017-06-26 22:06:53,791 main.py:47] epoch 1307, training loss: 3457.90, average training loss: 3930.10, base loss: 4573.22
[INFO 2017-06-26 22:06:54,197 main.py:47] epoch 1308, training loss: 3400.77, average training loss: 3929.43, base loss: 4573.07
[INFO 2017-06-26 22:06:54,601 main.py:47] epoch 1309, training loss: 3774.22, average training loss: 3929.06, base loss: 4573.29
[INFO 2017-06-26 22:06:55,004 main.py:47] epoch 1310, training loss: 3374.31, average training loss: 3928.52, base loss: 4573.35
[INFO 2017-06-26 22:06:55,413 main.py:47] epoch 1311, training loss: 3753.82, average training loss: 3927.70, base loss: 4572.98
[INFO 2017-06-26 22:06:55,817 main.py:47] epoch 1312, training loss: 3873.32, average training loss: 3927.68, base loss: 4573.77
[INFO 2017-06-26 22:06:56,225 main.py:47] epoch 1313, training loss: 3558.28, average training loss: 3927.48, base loss: 4574.00
[INFO 2017-06-26 22:06:56,631 main.py:47] epoch 1314, training loss: 3613.00, average training loss: 3927.02, base loss: 4573.94
[INFO 2017-06-26 22:06:57,035 main.py:47] epoch 1315, training loss: 3838.13, average training loss: 3926.65, base loss: 4574.26
[INFO 2017-06-26 22:06:57,442 main.py:47] epoch 1316, training loss: 3698.05, average training loss: 3926.03, base loss: 4574.45
[INFO 2017-06-26 22:06:57,859 main.py:47] epoch 1317, training loss: 3094.44, average training loss: 3924.25, base loss: 4573.02
[INFO 2017-06-26 22:06:58,276 main.py:47] epoch 1318, training loss: 3420.68, average training loss: 3924.40, base loss: 4573.78
[INFO 2017-06-26 22:06:58,704 main.py:47] epoch 1319, training loss: 3804.48, average training loss: 3924.66, base loss: 4574.74
[INFO 2017-06-26 22:06:59,117 main.py:47] epoch 1320, training loss: 3227.92, average training loss: 3923.90, base loss: 4574.35
[INFO 2017-06-26 22:06:59,545 main.py:47] epoch 1321, training loss: 3731.80, average training loss: 3924.05, base loss: 4575.35
[INFO 2017-06-26 22:07:00,034 main.py:47] epoch 1322, training loss: 4468.01, average training loss: 3924.45, base loss: 4576.91
[INFO 2017-06-26 22:07:00,520 main.py:47] epoch 1323, training loss: 3945.79, average training loss: 3924.44, base loss: 4577.79
[INFO 2017-06-26 22:07:00,983 main.py:47] epoch 1324, training loss: 3437.49, average training loss: 3923.60, base loss: 4577.59
[INFO 2017-06-26 22:07:01,471 main.py:47] epoch 1325, training loss: 4053.25, average training loss: 3923.65, base loss: 4578.60
[INFO 2017-06-26 22:07:01,900 main.py:47] epoch 1326, training loss: 3795.72, average training loss: 3924.12, base loss: 4579.86
[INFO 2017-06-26 22:07:02,376 main.py:47] epoch 1327, training loss: 3583.83, average training loss: 3923.73, base loss: 4580.16
[INFO 2017-06-26 22:07:02,883 main.py:47] epoch 1328, training loss: 3373.34, average training loss: 3923.18, base loss: 4580.58
[INFO 2017-06-26 22:07:03,319 main.py:47] epoch 1329, training loss: 3598.03, average training loss: 3923.54, base loss: 4581.95
[INFO 2017-06-26 22:07:03,729 main.py:47] epoch 1330, training loss: 3771.48, average training loss: 3923.38, base loss: 4582.54
[INFO 2017-06-26 22:07:04,140 main.py:47] epoch 1331, training loss: 4153.47, average training loss: 3923.08, base loss: 4582.74
[INFO 2017-06-26 22:07:04,546 main.py:47] epoch 1332, training loss: 3588.75, average training loss: 3922.62, base loss: 4582.74
[INFO 2017-06-26 22:07:04,963 main.py:47] epoch 1333, training loss: 3259.84, average training loss: 3921.17, base loss: 4581.83
[INFO 2017-06-26 22:07:05,368 main.py:47] epoch 1334, training loss: 3075.33, average training loss: 3917.14, base loss: 4578.05
[INFO 2017-06-26 22:07:05,776 main.py:47] epoch 1335, training loss: 3361.10, average training loss: 3916.47, base loss: 4577.70
[INFO 2017-06-26 22:07:06,181 main.py:47] epoch 1336, training loss: 3670.26, average training loss: 3916.24, base loss: 4578.38
[INFO 2017-06-26 22:07:06,588 main.py:47] epoch 1337, training loss: 3721.92, average training loss: 3916.56, base loss: 4579.51
[INFO 2017-06-26 22:07:06,998 main.py:47] epoch 1338, training loss: 3345.93, average training loss: 3916.05, base loss: 4579.57
[INFO 2017-06-26 22:07:07,402 main.py:47] epoch 1339, training loss: 3838.47, average training loss: 3915.04, base loss: 4579.10
[INFO 2017-06-26 22:07:07,807 main.py:47] epoch 1340, training loss: 3321.14, average training loss: 3914.62, base loss: 4579.16
[INFO 2017-06-26 22:07:08,217 main.py:47] epoch 1341, training loss: 3275.22, average training loss: 3914.23, base loss: 4579.26
[INFO 2017-06-26 22:07:08,629 main.py:47] epoch 1342, training loss: 3399.24, average training loss: 3913.56, base loss: 4579.29
[INFO 2017-06-26 22:07:09,038 main.py:47] epoch 1343, training loss: 3439.70, average training loss: 3912.74, base loss: 4578.82
[INFO 2017-06-26 22:07:09,442 main.py:47] epoch 1344, training loss: 3719.90, average training loss: 3912.79, base loss: 4579.65
[INFO 2017-06-26 22:07:09,852 main.py:47] epoch 1345, training loss: 3834.25, average training loss: 3912.62, base loss: 4580.15
[INFO 2017-06-26 22:07:10,256 main.py:47] epoch 1346, training loss: 3779.92, average training loss: 3912.26, base loss: 4580.73
[INFO 2017-06-26 22:07:10,662 main.py:47] epoch 1347, training loss: 3806.18, average training loss: 3912.78, base loss: 4582.07
[INFO 2017-06-26 22:07:11,071 main.py:47] epoch 1348, training loss: 4339.72, average training loss: 3913.06, base loss: 4582.75
[INFO 2017-06-26 22:07:11,476 main.py:47] epoch 1349, training loss: 7474.84, average training loss: 3916.37, base loss: 4586.63
[INFO 2017-06-26 22:07:11,881 main.py:47] epoch 1350, training loss: 3489.68, average training loss: 3916.27, base loss: 4586.90
[INFO 2017-06-26 22:07:12,285 main.py:47] epoch 1351, training loss: 4193.93, average training loss: 3916.72, base loss: 4587.81
[INFO 2017-06-26 22:07:12,690 main.py:47] epoch 1352, training loss: 9873.84, average training loss: 3922.75, base loss: 4593.90
[INFO 2017-06-26 22:07:13,096 main.py:47] epoch 1353, training loss: 3173.04, average training loss: 3922.05, base loss: 4593.34
[INFO 2017-06-26 22:07:13,499 main.py:47] epoch 1354, training loss: 3405.83, average training loss: 3921.49, base loss: 4593.00
[INFO 2017-06-26 22:07:13,906 main.py:47] epoch 1355, training loss: 3502.62, average training loss: 3921.10, base loss: 4592.97
[INFO 2017-06-26 22:07:14,315 main.py:47] epoch 1356, training loss: 4004.73, average training loss: 3921.08, base loss: 4593.51
[INFO 2017-06-26 22:07:14,722 main.py:47] epoch 1357, training loss: 4004.81, average training loss: 3920.09, base loss: 4592.86
[INFO 2017-06-26 22:07:15,131 main.py:47] epoch 1358, training loss: 3741.47, average training loss: 3919.68, base loss: 4592.92
[INFO 2017-06-26 22:07:15,536 main.py:47] epoch 1359, training loss: 3095.85, average training loss: 3918.88, base loss: 4592.26
[INFO 2017-06-26 22:07:15,945 main.py:47] epoch 1360, training loss: 3326.56, average training loss: 3918.49, base loss: 4592.57
[INFO 2017-06-26 22:07:16,352 main.py:47] epoch 1361, training loss: 3163.45, average training loss: 3917.22, base loss: 4591.29
[INFO 2017-06-26 22:07:16,757 main.py:47] epoch 1362, training loss: 3761.05, average training loss: 3913.47, base loss: 4588.25
[INFO 2017-06-26 22:07:17,166 main.py:47] epoch 1363, training loss: 7162.50, average training loss: 3916.66, base loss: 4591.71
[INFO 2017-06-26 22:07:17,571 main.py:47] epoch 1364, training loss: 3522.05, average training loss: 3916.67, base loss: 4592.31
[INFO 2017-06-26 22:07:17,976 main.py:47] epoch 1365, training loss: 3401.17, average training loss: 3916.15, base loss: 4592.28
[INFO 2017-06-26 22:07:18,381 main.py:47] epoch 1366, training loss: 3803.62, average training loss: 3916.42, base loss: 4593.26
[INFO 2017-06-26 22:07:18,786 main.py:47] epoch 1367, training loss: 3726.69, average training loss: 3916.16, base loss: 4593.51
[INFO 2017-06-26 22:07:19,192 main.py:47] epoch 1368, training loss: 7459.63, average training loss: 3916.42, base loss: 4594.41
[INFO 2017-06-26 22:07:19,599 main.py:47] epoch 1369, training loss: 3604.66, average training loss: 3916.30, base loss: 4594.76
[INFO 2017-06-26 22:07:20,006 main.py:47] epoch 1370, training loss: 3512.49, average training loss: 3915.98, base loss: 4594.74
[INFO 2017-06-26 22:07:20,416 main.py:47] epoch 1371, training loss: 3545.93, average training loss: 3915.94, base loss: 4595.21
[INFO 2017-06-26 22:07:20,827 main.py:47] epoch 1372, training loss: 3594.98, average training loss: 3915.68, base loss: 4595.43
[INFO 2017-06-26 22:07:21,233 main.py:47] epoch 1373, training loss: 3544.29, average training loss: 3914.88, base loss: 4594.58
[INFO 2017-06-26 22:07:21,645 main.py:47] epoch 1374, training loss: 3448.92, average training loss: 3914.76, base loss: 4594.79
[INFO 2017-06-26 22:07:22,052 main.py:47] epoch 1375, training loss: 3949.75, average training loss: 3915.11, base loss: 4595.89
[INFO 2017-06-26 22:07:22,458 main.py:47] epoch 1376, training loss: 3174.51, average training loss: 3914.69, base loss: 4595.86
[INFO 2017-06-26 22:07:22,875 main.py:47] epoch 1377, training loss: 3202.21, average training loss: 3914.42, base loss: 4596.02
[INFO 2017-06-26 22:07:23,287 main.py:47] epoch 1378, training loss: 6915.79, average training loss: 3917.47, base loss: 4599.44
[INFO 2017-06-26 22:07:23,696 main.py:47] epoch 1379, training loss: 3278.51, average training loss: 3917.35, base loss: 4599.52
[INFO 2017-06-26 22:07:24,193 main.py:47] epoch 1380, training loss: 3339.47, average training loss: 3916.89, base loss: 4599.70
[INFO 2017-06-26 22:07:24,604 main.py:47] epoch 1381, training loss: 3056.27, average training loss: 3915.68, base loss: 4598.74
[INFO 2017-06-26 22:07:25,052 main.py:47] epoch 1382, training loss: 3601.89, average training loss: 3914.42, base loss: 4597.47
[INFO 2017-06-26 22:07:25,490 main.py:47] epoch 1383, training loss: 3748.94, average training loss: 3914.48, base loss: 4598.27
[INFO 2017-06-26 22:07:25,897 main.py:47] epoch 1384, training loss: 3457.68, average training loss: 3913.32, base loss: 4597.26
[INFO 2017-06-26 22:07:26,346 main.py:47] epoch 1385, training loss: 7423.19, average training loss: 3917.32, base loss: 4602.35
[INFO 2017-06-26 22:07:26,754 main.py:47] epoch 1386, training loss: 2881.61, average training loss: 3916.06, base loss: 4600.93
[INFO 2017-06-26 22:07:27,161 main.py:47] epoch 1387, training loss: 3812.45, average training loss: 3915.38, base loss: 4600.93
[INFO 2017-06-26 22:07:27,624 main.py:47] epoch 1388, training loss: 3195.29, average training loss: 3914.73, base loss: 4600.64
[INFO 2017-06-26 22:07:28,031 main.py:47] epoch 1389, training loss: 3419.05, average training loss: 3914.77, base loss: 4601.29
[INFO 2017-06-26 22:07:28,453 main.py:47] epoch 1390, training loss: 3234.02, average training loss: 3914.13, base loss: 4600.90
[INFO 2017-06-26 22:07:28,858 main.py:47] epoch 1391, training loss: 3654.44, average training loss: 3913.35, base loss: 4600.47
[INFO 2017-06-26 22:07:29,352 main.py:47] epoch 1392, training loss: 3405.11, average training loss: 3912.34, base loss: 4599.94
[INFO 2017-06-26 22:07:29,777 main.py:47] epoch 1393, training loss: 3567.66, average training loss: 3912.04, base loss: 4599.86
[INFO 2017-06-26 22:07:30,183 main.py:47] epoch 1394, training loss: 7134.95, average training loss: 3914.69, base loss: 4602.81
[INFO 2017-06-26 22:07:30,586 main.py:47] epoch 1395, training loss: 2785.98, average training loss: 3913.23, base loss: 4601.32
[INFO 2017-06-26 22:07:30,989 main.py:47] epoch 1396, training loss: 3655.07, average training loss: 3912.72, base loss: 4601.06
[INFO 2017-06-26 22:07:31,395 main.py:47] epoch 1397, training loss: 3288.99, average training loss: 3911.74, base loss: 4600.39
[INFO 2017-06-26 22:07:31,795 main.py:47] epoch 1398, training loss: 3574.09, average training loss: 3910.76, base loss: 4599.66
[INFO 2017-06-26 22:07:32,194 main.py:47] epoch 1399, training loss: 3204.19, average training loss: 3906.02, base loss: 4595.46
[INFO 2017-06-26 22:07:32,194 main.py:49] epoch 1399, testing
[INFO 2017-06-26 22:07:33,844 main.py:102] average testing loss: 3903.42, base loss: 4681.74
[INFO 2017-06-26 22:07:33,844 main.py:103] improve_loss: 778.32, improve_percent: 0.17
[INFO 2017-06-26 22:07:33,845 main.py:73] current best improved percent: 0.19
[INFO 2017-06-26 22:07:34,250 main.py:47] epoch 1400, training loss: 3834.19, average training loss: 3906.34, base loss: 4596.52
[INFO 2017-06-26 22:07:34,709 main.py:47] epoch 1401, training loss: 3787.60, average training loss: 3906.13, base loss: 4596.91
[INFO 2017-06-26 22:07:35,145 main.py:47] epoch 1402, training loss: 3959.09, average training loss: 3906.43, base loss: 4597.85
[INFO 2017-06-26 22:07:35,558 main.py:47] epoch 1403, training loss: 3248.52, average training loss: 3905.60, base loss: 4597.28
[INFO 2017-06-26 22:07:35,967 main.py:47] epoch 1404, training loss: 4198.53, average training loss: 3906.21, base loss: 4598.98
[INFO 2017-06-26 22:07:36,367 main.py:47] epoch 1405, training loss: 4140.69, average training loss: 3906.17, base loss: 4599.74
[INFO 2017-06-26 22:07:36,774 main.py:47] epoch 1406, training loss: 7113.33, average training loss: 3909.22, base loss: 4603.22
[INFO 2017-06-26 22:07:37,199 main.py:47] epoch 1407, training loss: 3699.81, average training loss: 3905.30, base loss: 4599.71
[INFO 2017-06-26 22:07:37,603 main.py:47] epoch 1408, training loss: 3030.62, average training loss: 3904.64, base loss: 4599.04
[INFO 2017-06-26 22:07:38,019 main.py:47] epoch 1409, training loss: 3485.72, average training loss: 3904.71, base loss: 4599.48
[INFO 2017-06-26 22:07:38,420 main.py:47] epoch 1410, training loss: 4728.96, average training loss: 3905.30, base loss: 4600.98
[INFO 2017-06-26 22:07:38,819 main.py:47] epoch 1411, training loss: 3394.97, average training loss: 3901.38, base loss: 4597.36
[INFO 2017-06-26 22:07:39,219 main.py:47] epoch 1412, training loss: 3216.48, average training loss: 3899.90, base loss: 4595.82
[INFO 2017-06-26 22:07:39,624 main.py:47] epoch 1413, training loss: 3433.60, average training loss: 3896.24, base loss: 4592.96
[INFO 2017-06-26 22:07:40,025 main.py:47] epoch 1414, training loss: 4139.16, average training loss: 3896.23, base loss: 4593.49
[INFO 2017-06-26 22:07:40,430 main.py:47] epoch 1415, training loss: 3412.74, average training loss: 3895.86, base loss: 4593.25
[INFO 2017-06-26 22:07:40,827 main.py:47] epoch 1416, training loss: 3885.84, average training loss: 3895.89, base loss: 4594.13
[INFO 2017-06-26 22:07:41,232 main.py:47] epoch 1417, training loss: 3345.84, average training loss: 3895.29, base loss: 4593.65
[INFO 2017-06-26 22:07:41,634 main.py:47] epoch 1418, training loss: 4124.99, average training loss: 3895.97, base loss: 4595.31
[INFO 2017-06-26 22:07:42,032 main.py:47] epoch 1419, training loss: 3612.65, average training loss: 3895.84, base loss: 4595.75
[INFO 2017-06-26 22:07:42,433 main.py:47] epoch 1420, training loss: 3208.40, average training loss: 3895.74, base loss: 4596.32
[INFO 2017-06-26 22:07:42,835 main.py:47] epoch 1421, training loss: 7268.68, average training loss: 3899.23, base loss: 4600.27
[INFO 2017-06-26 22:07:43,237 main.py:47] epoch 1422, training loss: 3667.44, average training loss: 3899.41, base loss: 4600.74
[INFO 2017-06-26 22:07:43,641 main.py:47] epoch 1423, training loss: 4150.00, average training loss: 3899.47, base loss: 4601.53
[INFO 2017-06-26 22:07:44,043 main.py:47] epoch 1424, training loss: 3637.44, average training loss: 3899.25, base loss: 4602.01
[INFO 2017-06-26 22:07:44,440 main.py:47] epoch 1425, training loss: 3920.75, average training loss: 3899.08, base loss: 4602.70
[INFO 2017-06-26 22:07:44,837 main.py:47] epoch 1426, training loss: 3873.39, average training loss: 3898.81, base loss: 4602.98
[INFO 2017-06-26 22:07:45,237 main.py:47] epoch 1427, training loss: 3829.55, average training loss: 3899.30, base loss: 4604.29
[INFO 2017-06-26 22:07:45,640 main.py:47] epoch 1428, training loss: 3111.31, average training loss: 3897.87, base loss: 4603.04
[INFO 2017-06-26 22:07:46,042 main.py:47] epoch 1429, training loss: 3457.94, average training loss: 3896.88, base loss: 4602.28
[INFO 2017-06-26 22:07:46,439 main.py:47] epoch 1430, training loss: 3203.98, average training loss: 3896.57, base loss: 4602.35
[INFO 2017-06-26 22:07:46,840 main.py:47] epoch 1431, training loss: 2986.64, average training loss: 3895.47, base loss: 4601.32
[INFO 2017-06-26 22:07:47,251 main.py:47] epoch 1432, training loss: 3227.99, average training loss: 3894.49, base loss: 4600.35
[INFO 2017-06-26 22:07:47,650 main.py:47] epoch 1433, training loss: 3349.01, average training loss: 3894.05, base loss: 4600.31
[INFO 2017-06-26 22:07:48,060 main.py:47] epoch 1434, training loss: 3805.28, average training loss: 3894.18, base loss: 4601.13
[INFO 2017-06-26 22:07:48,464 main.py:47] epoch 1435, training loss: 3515.02, average training loss: 3893.86, base loss: 4601.21
[INFO 2017-06-26 22:07:48,873 main.py:47] epoch 1436, training loss: 3361.56, average training loss: 3892.53, base loss: 4600.31
[INFO 2017-06-26 22:07:49,274 main.py:47] epoch 1437, training loss: 3515.18, average training loss: 3892.68, base loss: 4600.98
[INFO 2017-06-26 22:07:49,673 main.py:47] epoch 1438, training loss: 3466.51, average training loss: 3892.60, base loss: 4601.43
[INFO 2017-06-26 22:07:50,073 main.py:47] epoch 1439, training loss: 7330.57, average training loss: 3896.26, base loss: 4605.67
[INFO 2017-06-26 22:07:50,476 main.py:47] epoch 1440, training loss: 6631.22, average training loss: 3899.36, base loss: 4609.13
[INFO 2017-06-26 22:07:50,912 main.py:47] epoch 1441, training loss: 4388.37, average training loss: 3900.72, base loss: 4611.70
[INFO 2017-06-26 22:07:51,315 main.py:47] epoch 1442, training loss: 3777.26, average training loss: 3900.75, base loss: 4612.53
[INFO 2017-06-26 22:07:51,714 main.py:47] epoch 1443, training loss: 3061.19, average training loss: 3900.21, base loss: 4612.11
[INFO 2017-06-26 22:07:52,115 main.py:47] epoch 1444, training loss: 3165.78, average training loss: 3899.82, base loss: 4612.06
[INFO 2017-06-26 22:07:52,517 main.py:47] epoch 1445, training loss: 3139.63, average training loss: 3898.88, base loss: 4611.30
[INFO 2017-06-26 22:07:52,919 main.py:47] epoch 1446, training loss: 3988.36, average training loss: 3899.11, base loss: 4612.08
[INFO 2017-06-26 22:07:53,318 main.py:47] epoch 1447, training loss: 3520.80, average training loss: 3898.69, base loss: 4612.22
[INFO 2017-06-26 22:07:53,718 main.py:47] epoch 1448, training loss: 3414.99, average training loss: 3897.60, base loss: 4611.23
[INFO 2017-06-26 22:07:54,119 main.py:47] epoch 1449, training loss: 3876.92, average training loss: 3897.27, base loss: 4611.28
[INFO 2017-06-26 22:07:54,525 main.py:47] epoch 1450, training loss: 7656.36, average training loss: 3900.85, base loss: 4615.34
[INFO 2017-06-26 22:07:54,928 main.py:47] epoch 1451, training loss: 3834.60, average training loss: 3900.51, base loss: 4615.43
[INFO 2017-06-26 22:07:55,331 main.py:47] epoch 1452, training loss: 3435.41, average training loss: 3899.32, base loss: 4614.52
[INFO 2017-06-26 22:07:55,730 main.py:47] epoch 1453, training loss: 3562.75, average training loss: 3899.02, base loss: 4614.43
[INFO 2017-06-26 22:07:56,129 main.py:47] epoch 1454, training loss: 3565.36, average training loss: 3898.36, base loss: 4614.32
[INFO 2017-06-26 22:07:56,531 main.py:47] epoch 1455, training loss: 3780.24, average training loss: 3898.14, base loss: 4614.52
[INFO 2017-06-26 22:07:56,934 main.py:47] epoch 1456, training loss: 3738.95, average training loss: 3897.62, base loss: 4614.63
[INFO 2017-06-26 22:07:57,341 main.py:47] epoch 1457, training loss: 3342.34, average training loss: 3896.40, base loss: 4613.26
[INFO 2017-06-26 22:07:57,747 main.py:47] epoch 1458, training loss: 3654.28, average training loss: 3896.46, base loss: 4614.02
[INFO 2017-06-26 22:07:58,152 main.py:47] epoch 1459, training loss: 3192.58, average training loss: 3895.73, base loss: 4613.36
[INFO 2017-06-26 22:07:58,554 main.py:47] epoch 1460, training loss: 3535.35, average training loss: 3891.68, base loss: 4609.37
[INFO 2017-06-26 22:07:58,957 main.py:47] epoch 1461, training loss: 3367.73, average training loss: 3890.98, base loss: 4609.25
[INFO 2017-06-26 22:07:59,464 main.py:47] epoch 1462, training loss: 3291.30, average training loss: 3890.45, base loss: 4608.89
[INFO 2017-06-26 22:07:59,874 main.py:47] epoch 1463, training loss: 3434.02, average training loss: 3889.79, base loss: 4608.36
[INFO 2017-06-26 22:08:00,287 main.py:47] epoch 1464, training loss: 3393.36, average training loss: 3889.69, base loss: 4608.53
[INFO 2017-06-26 22:08:00,691 main.py:47] epoch 1465, training loss: 3970.87, average training loss: 3889.77, base loss: 4609.27
[INFO 2017-06-26 22:08:01,095 main.py:47] epoch 1466, training loss: 3578.39, average training loss: 3888.65, base loss: 4608.47
[INFO 2017-06-26 22:08:01,498 main.py:47] epoch 1467, training loss: 7144.85, average training loss: 3891.04, base loss: 4611.27
[INFO 2017-06-26 22:08:01,898 main.py:47] epoch 1468, training loss: 3651.37, average training loss: 3890.60, base loss: 4611.31
[INFO 2017-06-26 22:08:02,303 main.py:47] epoch 1469, training loss: 3790.54, average training loss: 3890.65, base loss: 4611.93
[INFO 2017-06-26 22:08:02,702 main.py:47] epoch 1470, training loss: 3449.60, average training loss: 3890.30, base loss: 4612.18
[INFO 2017-06-26 22:08:03,181 main.py:47] epoch 1471, training loss: 3245.98, average training loss: 3889.53, base loss: 4611.45
[INFO 2017-06-26 22:08:03,620 main.py:47] epoch 1472, training loss: 4675.65, average training loss: 3890.49, base loss: 4613.26
[INFO 2017-06-26 22:08:04,027 main.py:47] epoch 1473, training loss: 3774.13, average training loss: 3890.62, base loss: 4614.24
[INFO 2017-06-26 22:08:04,432 main.py:47] epoch 1474, training loss: 4049.79, average training loss: 3890.72, base loss: 4615.04
[INFO 2017-06-26 22:08:04,834 main.py:47] epoch 1475, training loss: 3169.05, average training loss: 3886.04, base loss: 4610.79
[INFO 2017-06-26 22:08:05,241 main.py:47] epoch 1476, training loss: 3486.86, average training loss: 3886.07, base loss: 4611.27
[INFO 2017-06-26 22:08:05,639 main.py:47] epoch 1477, training loss: 3524.10, average training loss: 3885.80, base loss: 4611.53
[INFO 2017-06-26 22:08:06,043 main.py:47] epoch 1478, training loss: 3791.33, average training loss: 3885.81, base loss: 4612.02
[INFO 2017-06-26 22:08:06,447 main.py:47] epoch 1479, training loss: 3819.64, average training loss: 3885.52, base loss: 4612.28
[INFO 2017-06-26 22:08:06,852 main.py:47] epoch 1480, training loss: 3227.46, average training loss: 3884.08, base loss: 4610.63
[INFO 2017-06-26 22:08:07,254 main.py:47] epoch 1481, training loss: 3628.77, average training loss: 3883.89, base loss: 4610.68
[INFO 2017-06-26 22:08:07,659 main.py:47] epoch 1482, training loss: 3286.24, average training loss: 3883.12, base loss: 4609.79
[INFO 2017-06-26 22:08:08,087 main.py:47] epoch 1483, training loss: 3970.12, average training loss: 3883.23, base loss: 4610.49
[INFO 2017-06-26 22:08:08,530 main.py:47] epoch 1484, training loss: 3183.45, average training loss: 3882.42, base loss: 4609.84
[INFO 2017-06-26 22:08:08,936 main.py:47] epoch 1485, training loss: 3419.70, average training loss: 3881.60, base loss: 4609.09
[INFO 2017-06-26 22:08:09,340 main.py:47] epoch 1486, training loss: 3476.28, average training loss: 3881.04, base loss: 4608.65
[INFO 2017-06-26 22:08:09,744 main.py:47] epoch 1487, training loss: 3776.49, average training loss: 3881.44, base loss: 4609.75
[INFO 2017-06-26 22:08:10,148 main.py:47] epoch 1488, training loss: 3755.71, average training loss: 3881.60, base loss: 4610.29
[INFO 2017-06-26 22:08:10,548 main.py:47] epoch 1489, training loss: 6963.67, average training loss: 3884.26, base loss: 4613.45
[INFO 2017-06-26 22:08:10,971 main.py:47] epoch 1490, training loss: 3188.19, average training loss: 3883.47, base loss: 4612.82
[INFO 2017-06-26 22:08:11,395 main.py:47] epoch 1491, training loss: 3362.51, average training loss: 3882.26, base loss: 4611.79
[INFO 2017-06-26 22:08:11,799 main.py:47] epoch 1492, training loss: 3225.59, average training loss: 3881.15, base loss: 4611.02
[INFO 2017-06-26 22:08:12,206 main.py:47] epoch 1493, training loss: 3617.92, average training loss: 3880.54, base loss: 4610.41
[INFO 2017-06-26 22:08:12,607 main.py:47] epoch 1494, training loss: 3561.80, average training loss: 3880.30, base loss: 4610.75
[INFO 2017-06-26 22:08:13,005 main.py:47] epoch 1495, training loss: 3629.95, average training loss: 3879.32, base loss: 4610.35
[INFO 2017-06-26 22:08:13,403 main.py:47] epoch 1496, training loss: 3450.94, average training loss: 3879.32, base loss: 4610.86
[INFO 2017-06-26 22:08:13,913 main.py:47] epoch 1497, training loss: 4187.85, average training loss: 3879.51, base loss: 4612.06
[INFO 2017-06-26 22:08:14,324 main.py:47] epoch 1498, training loss: 3292.77, average training loss: 3879.54, base loss: 4612.48
[INFO 2017-06-26 22:08:14,862 main.py:47] epoch 1499, training loss: 2994.75, average training loss: 3878.34, base loss: 4611.37
[INFO 2017-06-26 22:08:14,862 main.py:49] epoch 1499, testing
[INFO 2017-06-26 22:08:16,536 main.py:102] average testing loss: 3604.78, base loss: 4613.73
[INFO 2017-06-26 22:08:16,536 main.py:103] improve_loss: 1008.95, improve_percent: 0.22
[INFO 2017-06-26 22:08:16,537 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 22:08:16,549 main.py:73] current best improved percent: 0.22
[INFO 2017-06-26 22:08:17,053 main.py:47] epoch 1500, training loss: 3019.22, average training loss: 3877.20, base loss: 4610.28
[INFO 2017-06-26 22:08:17,468 main.py:47] epoch 1501, training loss: 4256.17, average training loss: 3877.05, base loss: 4610.74
[INFO 2017-06-26 22:08:17,880 main.py:47] epoch 1502, training loss: 3486.87, average training loss: 3876.26, base loss: 4610.47
[INFO 2017-06-26 22:08:18,284 main.py:47] epoch 1503, training loss: 3048.37, average training loss: 3875.90, base loss: 4610.75
[INFO 2017-06-26 22:08:18,687 main.py:47] epoch 1504, training loss: 3519.94, average training loss: 3875.33, base loss: 4610.25
[INFO 2017-06-26 22:08:19,090 main.py:47] epoch 1505, training loss: 3504.05, average training loss: 3871.55, base loss: 4606.66
[INFO 2017-06-26 22:08:19,494 main.py:47] epoch 1506, training loss: 3474.53, average training loss: 3871.71, base loss: 4607.44
[INFO 2017-06-26 22:08:19,922 main.py:47] epoch 1507, training loss: 3537.53, average training loss: 3872.04, base loss: 4608.42
[INFO 2017-06-26 22:08:20,328 main.py:47] epoch 1508, training loss: 7014.25, average training loss: 3874.95, base loss: 4611.55
[INFO 2017-06-26 22:08:20,742 main.py:47] epoch 1509, training loss: 3168.18, average training loss: 3873.42, base loss: 4610.15
[INFO 2017-06-26 22:08:21,147 main.py:47] epoch 1510, training loss: 3572.92, average training loss: 3872.44, base loss: 4609.40
[INFO 2017-06-26 22:08:21,552 main.py:47] epoch 1511, training loss: 3985.19, average training loss: 3872.31, base loss: 4609.95
[INFO 2017-06-26 22:08:21,975 main.py:47] epoch 1512, training loss: 3136.70, average training loss: 3872.11, base loss: 4610.24
[INFO 2017-06-26 22:08:22,390 main.py:47] epoch 1513, training loss: 3665.18, average training loss: 3872.14, base loss: 4610.52
[INFO 2017-06-26 22:08:22,802 main.py:47] epoch 1514, training loss: 3325.82, average training loss: 3871.62, base loss: 4610.45
[INFO 2017-06-26 22:08:23,215 main.py:47] epoch 1515, training loss: 3345.10, average training loss: 3870.79, base loss: 4610.23
[INFO 2017-06-26 22:08:23,630 main.py:47] epoch 1516, training loss: 3879.56, average training loss: 3870.96, base loss: 4611.16
[INFO 2017-06-26 22:08:24,049 main.py:47] epoch 1517, training loss: 3719.76, average training loss: 3871.23, base loss: 4612.01
[INFO 2017-06-26 22:08:24,529 main.py:47] epoch 1518, training loss: 3182.17, average training loss: 3870.63, base loss: 4611.52
[INFO 2017-06-26 22:08:25,015 main.py:47] epoch 1519, training loss: 3556.30, average training loss: 3870.42, base loss: 4611.68
[INFO 2017-06-26 22:08:25,505 main.py:47] epoch 1520, training loss: 4267.17, average training loss: 3869.95, base loss: 4611.72
[INFO 2017-06-26 22:08:25,975 main.py:47] epoch 1521, training loss: 3635.42, average training loss: 3868.98, base loss: 4610.78
[INFO 2017-06-26 22:08:26,422 main.py:47] epoch 1522, training loss: 3477.87, average training loss: 3868.12, base loss: 4610.33
[INFO 2017-06-26 22:08:26,875 main.py:47] epoch 1523, training loss: 3271.77, average training loss: 3867.32, base loss: 4609.42
[INFO 2017-06-26 22:08:27,368 main.py:47] epoch 1524, training loss: 3146.28, average training loss: 3866.23, base loss: 4608.51
[INFO 2017-06-26 22:08:27,812 main.py:47] epoch 1525, training loss: 2838.34, average training loss: 3865.19, base loss: 4607.64
[INFO 2017-06-26 22:08:28,240 main.py:47] epoch 1526, training loss: 3526.19, average training loss: 3864.65, base loss: 4607.31
[INFO 2017-06-26 22:08:28,643 main.py:47] epoch 1527, training loss: 3185.60, average training loss: 3863.86, base loss: 4607.11
[INFO 2017-06-26 22:08:29,044 main.py:47] epoch 1528, training loss: 3430.78, average training loss: 3863.34, base loss: 4607.17
[INFO 2017-06-26 22:08:29,445 main.py:47] epoch 1529, training loss: 3267.23, average training loss: 3862.62, base loss: 4606.63
[INFO 2017-06-26 22:08:29,847 main.py:47] epoch 1530, training loss: 3255.83, average training loss: 3862.13, base loss: 4606.51
[INFO 2017-06-26 22:08:30,284 main.py:47] epoch 1531, training loss: 3446.73, average training loss: 3861.83, base loss: 4606.52
[INFO 2017-06-26 22:08:30,689 main.py:47] epoch 1532, training loss: 4392.28, average training loss: 3862.43, base loss: 4607.95
[INFO 2017-06-26 22:08:31,092 main.py:47] epoch 1533, training loss: 3566.15, average training loss: 3862.60, base loss: 4608.76
[INFO 2017-06-26 22:08:31,500 main.py:47] epoch 1534, training loss: 3344.54, average training loss: 3862.01, base loss: 4608.52
[INFO 2017-06-26 22:08:31,901 main.py:47] epoch 1535, training loss: 3442.40, average training loss: 3860.85, base loss: 4607.69
[INFO 2017-06-26 22:08:32,298 main.py:47] epoch 1536, training loss: 3852.08, average training loss: 3860.44, base loss: 4607.92
[INFO 2017-06-26 22:08:32,696 main.py:47] epoch 1537, training loss: 3732.03, average training loss: 3860.02, base loss: 4608.06
[INFO 2017-06-26 22:08:33,096 main.py:47] epoch 1538, training loss: 3602.77, average training loss: 3860.04, base loss: 4608.69
[INFO 2017-06-26 22:08:33,499 main.py:47] epoch 1539, training loss: 3882.45, average training loss: 3859.41, base loss: 4609.07
[INFO 2017-06-26 22:08:33,903 main.py:47] epoch 1540, training loss: 3644.80, average training loss: 3858.37, base loss: 4608.43
[INFO 2017-06-26 22:08:34,304 main.py:47] epoch 1541, training loss: 3119.89, average training loss: 3857.10, base loss: 4607.16
[INFO 2017-06-26 22:08:34,707 main.py:47] epoch 1542, training loss: 3488.93, average training loss: 3856.29, base loss: 4606.65
[INFO 2017-06-26 22:08:35,111 main.py:47] epoch 1543, training loss: 3121.28, average training loss: 3855.40, base loss: 4606.04
[INFO 2017-06-26 22:08:35,512 main.py:47] epoch 1544, training loss: 3421.67, average training loss: 3854.33, base loss: 4605.01
[INFO 2017-06-26 22:08:35,917 main.py:47] epoch 1545, training loss: 3450.91, average training loss: 3854.04, base loss: 4605.18
[INFO 2017-06-26 22:08:36,322 main.py:47] epoch 1546, training loss: 3109.10, average training loss: 3853.76, base loss: 4605.42
[INFO 2017-06-26 22:08:36,724 main.py:47] epoch 1547, training loss: 3831.68, average training loss: 3853.98, base loss: 4606.45
[INFO 2017-06-26 22:08:37,136 main.py:47] epoch 1548, training loss: 3730.03, average training loss: 3853.93, base loss: 4606.92
[INFO 2017-06-26 22:08:37,537 main.py:47] epoch 1549, training loss: 3244.80, average training loss: 3853.85, base loss: 4607.30
[INFO 2017-06-26 22:08:37,936 main.py:47] epoch 1550, training loss: 3372.61, average training loss: 3852.76, base loss: 4606.39
[INFO 2017-06-26 22:08:38,342 main.py:47] epoch 1551, training loss: 3505.32, average training loss: 3848.76, base loss: 4602.59
[INFO 2017-06-26 22:08:38,739 main.py:47] epoch 1552, training loss: 3622.03, average training loss: 3848.31, base loss: 4602.80
[INFO 2017-06-26 22:08:39,142 main.py:47] epoch 1553, training loss: 3383.28, average training loss: 3847.24, base loss: 4601.52
[INFO 2017-06-26 22:08:39,556 main.py:47] epoch 1554, training loss: 3703.30, average training loss: 3847.43, base loss: 4602.40
[INFO 2017-06-26 22:08:39,959 main.py:47] epoch 1555, training loss: 3739.11, average training loss: 3847.38, base loss: 4602.97
[INFO 2017-06-26 22:08:40,363 main.py:47] epoch 1556, training loss: 3709.20, average training loss: 3847.24, base loss: 4603.51
[INFO 2017-06-26 22:08:40,763 main.py:47] epoch 1557, training loss: 2996.86, average training loss: 3846.19, base loss: 4602.32
[INFO 2017-06-26 22:08:41,161 main.py:47] epoch 1558, training loss: 3831.75, average training loss: 3845.52, base loss: 4601.98
[INFO 2017-06-26 22:08:41,573 main.py:47] epoch 1559, training loss: 3197.94, average training loss: 3844.81, base loss: 4601.32
[INFO 2017-06-26 22:08:41,977 main.py:47] epoch 1560, training loss: 3348.61, average training loss: 3844.42, base loss: 4601.04
[INFO 2017-06-26 22:08:42,380 main.py:47] epoch 1561, training loss: 3858.52, average training loss: 3844.70, base loss: 4601.87
[INFO 2017-06-26 22:08:42,802 main.py:47] epoch 1562, training loss: 3884.27, average training loss: 3844.89, base loss: 4602.53
[INFO 2017-06-26 22:08:43,233 main.py:47] epoch 1563, training loss: 3938.65, average training loss: 3844.59, base loss: 4602.90
[INFO 2017-06-26 22:08:43,640 main.py:47] epoch 1564, training loss: 3580.29, average training loss: 3844.33, base loss: 4603.25
[INFO 2017-06-26 22:08:44,063 main.py:47] epoch 1565, training loss: 3386.86, average training loss: 3843.28, base loss: 4602.54
[INFO 2017-06-26 22:08:44,479 main.py:47] epoch 1566, training loss: 3796.51, average training loss: 3843.38, base loss: 4603.40
[INFO 2017-06-26 22:08:44,887 main.py:47] epoch 1567, training loss: 3670.13, average training loss: 3843.13, base loss: 4603.60
[INFO 2017-06-26 22:08:45,288 main.py:47] epoch 1568, training loss: 3507.92, average training loss: 3842.59, base loss: 4603.64
[INFO 2017-06-26 22:08:45,687 main.py:47] epoch 1569, training loss: 4163.27, average training loss: 3843.24, base loss: 4605.30
[INFO 2017-06-26 22:08:46,098 main.py:47] epoch 1570, training loss: 3796.13, average training loss: 3843.02, base loss: 4605.65
[INFO 2017-06-26 22:08:46,502 main.py:47] epoch 1571, training loss: 3664.18, average training loss: 3842.16, base loss: 4605.10
[INFO 2017-06-26 22:08:46,905 main.py:47] epoch 1572, training loss: 3561.48, average training loss: 3841.23, base loss: 4604.26
[INFO 2017-06-26 22:08:47,311 main.py:47] epoch 1573, training loss: 3728.99, average training loss: 3841.42, base loss: 4605.00
[INFO 2017-06-26 22:08:47,709 main.py:47] epoch 1574, training loss: 3820.24, average training loss: 3841.02, base loss: 4605.00
[INFO 2017-06-26 22:08:48,113 main.py:47] epoch 1575, training loss: 3703.81, average training loss: 3840.72, base loss: 4605.24
[INFO 2017-06-26 22:08:48,516 main.py:47] epoch 1576, training loss: 3661.34, average training loss: 3840.41, base loss: 4605.66
[INFO 2017-06-26 22:08:48,920 main.py:47] epoch 1577, training loss: 2946.89, average training loss: 3840.06, base loss: 4605.51
[INFO 2017-06-26 22:08:49,323 main.py:47] epoch 1578, training loss: 3214.60, average training loss: 3838.68, base loss: 4604.03
[INFO 2017-06-26 22:08:49,725 main.py:47] epoch 1579, training loss: 3852.30, average training loss: 3837.89, base loss: 4603.62
[INFO 2017-06-26 22:08:50,125 main.py:47] epoch 1580, training loss: 3398.38, average training loss: 3837.70, base loss: 4603.53
[INFO 2017-06-26 22:08:50,525 main.py:47] epoch 1581, training loss: 3484.35, average training loss: 3836.75, base loss: 4602.75
[INFO 2017-06-26 22:08:50,929 main.py:47] epoch 1582, training loss: 2951.91, average training loss: 3832.35, base loss: 4598.60
[INFO 2017-06-26 22:08:51,331 main.py:47] epoch 1583, training loss: 3710.14, average training loss: 3831.77, base loss: 4598.11
[INFO 2017-06-26 22:08:51,733 main.py:47] epoch 1584, training loss: 3413.14, average training loss: 3827.20, base loss: 4593.50
[INFO 2017-06-26 22:08:52,137 main.py:47] epoch 1585, training loss: 3964.19, average training loss: 3827.42, base loss: 4594.46
[INFO 2017-06-26 22:08:52,542 main.py:47] epoch 1586, training loss: 3215.04, average training loss: 3827.03, base loss: 4594.08
[INFO 2017-06-26 22:08:52,944 main.py:47] epoch 1587, training loss: 4211.30, average training loss: 3827.44, base loss: 4595.22
[INFO 2017-06-26 22:08:53,347 main.py:47] epoch 1588, training loss: 3144.74, average training loss: 3826.63, base loss: 4594.46
[INFO 2017-06-26 22:08:53,747 main.py:47] epoch 1589, training loss: 3451.70, average training loss: 3825.56, base loss: 4593.68
[INFO 2017-06-26 22:08:54,145 main.py:47] epoch 1590, training loss: 3570.86, average training loss: 3825.12, base loss: 4593.38
[INFO 2017-06-26 22:08:54,544 main.py:47] epoch 1591, training loss: 3356.00, average training loss: 3824.72, base loss: 4593.25
[INFO 2017-06-26 22:08:54,944 main.py:47] epoch 1592, training loss: 3298.15, average training loss: 3823.23, base loss: 4591.64
[INFO 2017-06-26 22:08:55,343 main.py:47] epoch 1593, training loss: 3918.23, average training loss: 3823.05, base loss: 4591.89
[INFO 2017-06-26 22:08:55,745 main.py:47] epoch 1594, training loss: 3535.21, average training loss: 3822.65, base loss: 4591.89
[INFO 2017-06-26 22:08:56,148 main.py:47] epoch 1595, training loss: 3759.02, average training loss: 3822.28, base loss: 4592.14
[INFO 2017-06-26 22:08:56,549 main.py:47] epoch 1596, training loss: 3306.02, average training loss: 3821.71, base loss: 4591.64
[INFO 2017-06-26 22:08:56,947 main.py:47] epoch 1597, training loss: 3244.95, average training loss: 3821.43, base loss: 4591.96
[INFO 2017-06-26 22:08:57,350 main.py:47] epoch 1598, training loss: 3982.46, average training loss: 3821.58, base loss: 4592.70
[INFO 2017-06-26 22:08:57,752 main.py:47] epoch 1599, training loss: 3210.79, average training loss: 3820.68, base loss: 4591.69
[INFO 2017-06-26 22:08:57,752 main.py:49] epoch 1599, testing
[INFO 2017-06-26 22:08:59,413 main.py:102] average testing loss: 3853.26, base loss: 4808.29
[INFO 2017-06-26 22:08:59,413 main.py:103] improve_loss: 955.03, improve_percent: 0.20
[INFO 2017-06-26 22:08:59,414 main.py:73] current best improved percent: 0.22
[INFO 2017-06-26 22:08:59,813 main.py:47] epoch 1600, training loss: 3350.25, average training loss: 3819.89, base loss: 4591.10
[INFO 2017-06-26 22:09:00,216 main.py:47] epoch 1601, training loss: 6954.42, average training loss: 3822.90, base loss: 4594.46
[INFO 2017-06-26 22:09:00,619 main.py:47] epoch 1602, training loss: 3376.07, average training loss: 3822.65, base loss: 4594.75
[INFO 2017-06-26 22:09:01,019 main.py:47] epoch 1603, training loss: 3610.17, average training loss: 3822.33, base loss: 4594.99
[INFO 2017-06-26 22:09:01,423 main.py:47] epoch 1604, training loss: 3841.67, average training loss: 3821.70, base loss: 4594.80
[INFO 2017-06-26 22:09:01,825 main.py:47] epoch 1605, training loss: 3079.31, average training loss: 3820.54, base loss: 4593.55
[INFO 2017-06-26 22:09:02,234 main.py:47] epoch 1606, training loss: 3474.17, average training loss: 3819.91, base loss: 4593.08
[INFO 2017-06-26 22:09:02,633 main.py:47] epoch 1607, training loss: 3751.68, average training loss: 3819.70, base loss: 4593.21
[INFO 2017-06-26 22:09:03,036 main.py:47] epoch 1608, training loss: 3470.00, average training loss: 3818.69, base loss: 4592.18
[INFO 2017-06-26 22:09:03,439 main.py:47] epoch 1609, training loss: 3137.75, average training loss: 3818.09, base loss: 4591.63
[INFO 2017-06-26 22:09:03,839 main.py:47] epoch 1610, training loss: 3973.58, average training loss: 3818.08, base loss: 4591.97
[INFO 2017-06-26 22:09:04,241 main.py:47] epoch 1611, training loss: 3422.15, average training loss: 3817.48, base loss: 4591.49
[INFO 2017-06-26 22:09:04,643 main.py:47] epoch 1612, training loss: 3236.79, average training loss: 3817.34, base loss: 4591.79
[INFO 2017-06-26 22:09:05,043 main.py:47] epoch 1613, training loss: 3949.79, average training loss: 3816.77, base loss: 4591.50
[INFO 2017-06-26 22:09:05,447 main.py:47] epoch 1614, training loss: 3647.49, average training loss: 3816.67, base loss: 4591.56
[INFO 2017-06-26 22:09:05,852 main.py:47] epoch 1615, training loss: 3690.73, average training loss: 3816.62, base loss: 4592.00
[INFO 2017-06-26 22:09:06,257 main.py:47] epoch 1616, training loss: 7549.02, average training loss: 3820.23, base loss: 4596.39
[INFO 2017-06-26 22:09:06,664 main.py:47] epoch 1617, training loss: 3258.33, average training loss: 3820.23, base loss: 4596.97
[INFO 2017-06-26 22:09:07,108 main.py:47] epoch 1618, training loss: 3331.63, average training loss: 3819.98, base loss: 4597.04
[INFO 2017-06-26 22:09:07,532 main.py:47] epoch 1619, training loss: 3380.87, average training loss: 3819.28, base loss: 4596.66
[INFO 2017-06-26 22:09:07,941 main.py:47] epoch 1620, training loss: 3817.09, average training loss: 3819.36, base loss: 4597.31
[INFO 2017-06-26 22:09:08,349 main.py:47] epoch 1621, training loss: 3349.44, average training loss: 3818.82, base loss: 4596.93
[INFO 2017-06-26 22:09:08,776 main.py:47] epoch 1622, training loss: 3569.84, average training loss: 3818.58, base loss: 4597.41
[INFO 2017-06-26 22:09:09,179 main.py:47] epoch 1623, training loss: 3301.27, average training loss: 3818.02, base loss: 4597.23
[INFO 2017-06-26 22:09:09,583 main.py:47] epoch 1624, training loss: 3561.75, average training loss: 3817.84, base loss: 4597.38
[INFO 2017-06-26 22:09:09,990 main.py:47] epoch 1625, training loss: 3688.63, average training loss: 3816.64, base loss: 4596.24
[INFO 2017-06-26 22:09:10,392 main.py:47] epoch 1626, training loss: 3164.90, average training loss: 3815.93, base loss: 4595.89
[INFO 2017-06-26 22:09:10,796 main.py:47] epoch 1627, training loss: 3439.75, average training loss: 3815.68, base loss: 4595.90
[INFO 2017-06-26 22:09:11,199 main.py:47] epoch 1628, training loss: 4185.53, average training loss: 3815.97, base loss: 4596.93
[INFO 2017-06-26 22:09:11,602 main.py:47] epoch 1629, training loss: 3686.33, average training loss: 3815.81, base loss: 4597.18
[INFO 2017-06-26 22:09:12,010 main.py:47] epoch 1630, training loss: 7175.91, average training loss: 3819.40, base loss: 4600.96
[INFO 2017-06-26 22:09:12,419 main.py:47] epoch 1631, training loss: 3498.94, average training loss: 3818.95, base loss: 4601.04
[INFO 2017-06-26 22:09:12,829 main.py:47] epoch 1632, training loss: 3403.48, average training loss: 3818.31, base loss: 4600.61
[INFO 2017-06-26 22:09:13,236 main.py:47] epoch 1633, training loss: 3176.63, average training loss: 3817.73, base loss: 4600.45
[INFO 2017-06-26 22:09:13,644 main.py:47] epoch 1634, training loss: 3278.28, average training loss: 3817.04, base loss: 4599.57
[INFO 2017-06-26 22:09:14,047 main.py:47] epoch 1635, training loss: 3392.83, average training loss: 3816.52, base loss: 4599.10
[INFO 2017-06-26 22:09:14,548 main.py:47] epoch 1636, training loss: 3672.28, average training loss: 3817.00, base loss: 4600.12
[INFO 2017-06-26 22:09:14,961 main.py:47] epoch 1637, training loss: 4055.72, average training loss: 3817.48, base loss: 4601.20
[INFO 2017-06-26 22:09:15,367 main.py:47] epoch 1638, training loss: 3830.00, average training loss: 3813.71, base loss: 4597.81
[INFO 2017-06-26 22:09:15,766 main.py:47] epoch 1639, training loss: 3330.78, average training loss: 3813.32, base loss: 4597.50
[INFO 2017-06-26 22:09:16,183 main.py:47] epoch 1640, training loss: 3809.65, average training loss: 3813.29, base loss: 4597.80
[INFO 2017-06-26 22:09:16,592 main.py:47] epoch 1641, training loss: 3564.73, average training loss: 3813.16, base loss: 4598.12
[INFO 2017-06-26 22:09:16,992 main.py:47] epoch 1642, training loss: 6919.88, average training loss: 3815.93, base loss: 4600.95
[INFO 2017-06-26 22:09:17,391 main.py:47] epoch 1643, training loss: 3221.95, average training loss: 3815.20, base loss: 4600.46
[INFO 2017-06-26 22:09:17,802 main.py:47] epoch 1644, training loss: 3606.22, average training loss: 3814.69, base loss: 4600.12
[INFO 2017-06-26 22:09:18,211 main.py:47] epoch 1645, training loss: 3425.42, average training loss: 3814.29, base loss: 4599.74
[INFO 2017-06-26 22:09:18,623 main.py:47] epoch 1646, training loss: 3379.99, average training loss: 3814.20, base loss: 4600.10
[INFO 2017-06-26 22:09:19,027 main.py:47] epoch 1647, training loss: 4198.10, average training loss: 3814.76, base loss: 4601.24
[INFO 2017-06-26 22:09:19,429 main.py:47] epoch 1648, training loss: 3462.56, average training loss: 3814.14, base loss: 4600.98
[INFO 2017-06-26 22:09:19,830 main.py:47] epoch 1649, training loss: 7155.07, average training loss: 3817.65, base loss: 4604.90
[INFO 2017-06-26 22:09:20,287 main.py:47] epoch 1650, training loss: 3510.54, average training loss: 3817.80, base loss: 4605.50
[INFO 2017-06-26 22:09:20,726 main.py:47] epoch 1651, training loss: 3534.64, average training loss: 3817.89, base loss: 4606.07
[INFO 2017-06-26 22:09:21,132 main.py:47] epoch 1652, training loss: 3073.34, average training loss: 3816.40, base loss: 4604.61
[INFO 2017-06-26 22:09:21,637 main.py:47] epoch 1653, training loss: 3494.53, average training loss: 3816.37, base loss: 4605.39
[INFO 2017-06-26 22:09:22,051 main.py:47] epoch 1654, training loss: 3251.62, average training loss: 3816.26, base loss: 4605.81
[INFO 2017-06-26 22:09:22,557 main.py:47] epoch 1655, training loss: 3209.70, average training loss: 3815.51, base loss: 4605.52
[INFO 2017-06-26 22:09:22,976 main.py:47] epoch 1656, training loss: 3861.94, average training loss: 3815.43, base loss: 4606.09
[INFO 2017-06-26 22:09:23,403 main.py:47] epoch 1657, training loss: 3529.46, average training loss: 3815.06, base loss: 4606.55
[INFO 2017-06-26 22:09:23,806 main.py:47] epoch 1658, training loss: 3823.87, average training loss: 3815.59, base loss: 4607.85
[INFO 2017-06-26 22:09:24,225 main.py:47] epoch 1659, training loss: 3290.33, average training loss: 3815.55, base loss: 4608.38
[INFO 2017-06-26 22:09:24,629 main.py:47] epoch 1660, training loss: 3499.47, average training loss: 3814.75, base loss: 4607.88
[INFO 2017-06-26 22:09:25,028 main.py:47] epoch 1661, training loss: 3404.63, average training loss: 3813.01, base loss: 4606.05
[INFO 2017-06-26 22:09:25,431 main.py:47] epoch 1662, training loss: 3446.53, average training loss: 3812.61, base loss: 4606.02
[INFO 2017-06-26 22:09:25,833 main.py:47] epoch 1663, training loss: 3518.46, average training loss: 3812.22, base loss: 4605.76
[INFO 2017-06-26 22:09:26,228 main.py:47] epoch 1664, training loss: 3795.43, average training loss: 3811.93, base loss: 4605.61
[INFO 2017-06-26 22:09:26,631 main.py:47] epoch 1665, training loss: 3100.35, average training loss: 3811.04, base loss: 4604.97
[INFO 2017-06-26 22:09:27,033 main.py:47] epoch 1666, training loss: 3078.29, average training loss: 3809.96, base loss: 4603.64
[INFO 2017-06-26 22:09:27,429 main.py:47] epoch 1667, training loss: 3198.99, average training loss: 3809.39, base loss: 4603.14
[INFO 2017-06-26 22:09:27,830 main.py:47] epoch 1668, training loss: 3180.64, average training loss: 3808.90, base loss: 4603.32
[INFO 2017-06-26 22:09:28,226 main.py:47] epoch 1669, training loss: 3937.71, average training loss: 3809.09, base loss: 4603.71
[INFO 2017-06-26 22:09:28,629 main.py:47] epoch 1670, training loss: 4121.15, average training loss: 3809.95, base loss: 4605.58
[INFO 2017-06-26 22:09:29,034 main.py:47] epoch 1671, training loss: 3293.91, average training loss: 3809.27, base loss: 4605.02
[INFO 2017-06-26 22:09:29,438 main.py:47] epoch 1672, training loss: 3673.19, average training loss: 3809.53, base loss: 4605.97
[INFO 2017-06-26 22:09:29,843 main.py:47] epoch 1673, training loss: 4024.57, average training loss: 3809.65, base loss: 4606.56
[INFO 2017-06-26 22:09:30,245 main.py:47] epoch 1674, training loss: 3579.40, average training loss: 3809.06, base loss: 4606.08
[INFO 2017-06-26 22:09:30,649 main.py:47] epoch 1675, training loss: 3602.73, average training loss: 3808.59, base loss: 4606.05
[INFO 2017-06-26 22:09:31,051 main.py:47] epoch 1676, training loss: 3114.80, average training loss: 3807.85, base loss: 4605.26
[INFO 2017-06-26 22:09:31,453 main.py:47] epoch 1677, training loss: 4112.43, average training loss: 3808.08, base loss: 4606.11
[INFO 2017-06-26 22:09:31,858 main.py:47] epoch 1678, training loss: 3268.36, average training loss: 3806.85, base loss: 4605.12
[INFO 2017-06-26 22:09:32,267 main.py:47] epoch 1679, training loss: 3021.38, average training loss: 3805.77, base loss: 4604.58
[INFO 2017-06-26 22:09:32,726 main.py:47] epoch 1680, training loss: 3229.40, average training loss: 3804.84, base loss: 4603.68
[INFO 2017-06-26 22:09:33,164 main.py:47] epoch 1681, training loss: 3512.29, average training loss: 3805.15, base loss: 4604.37
[INFO 2017-06-26 22:09:33,568 main.py:47] epoch 1682, training loss: 3213.95, average training loss: 3804.12, base loss: 4603.44
[INFO 2017-06-26 22:09:34,031 main.py:47] epoch 1683, training loss: 3282.24, average training loss: 3803.52, base loss: 4602.58
[INFO 2017-06-26 22:09:34,470 main.py:47] epoch 1684, training loss: 3768.72, average training loss: 3803.20, base loss: 4602.89
[INFO 2017-06-26 22:09:34,880 main.py:47] epoch 1685, training loss: 3519.31, average training loss: 3803.24, base loss: 4603.36
[INFO 2017-06-26 22:09:35,375 main.py:47] epoch 1686, training loss: 3069.31, average training loss: 3802.72, base loss: 4602.68
[INFO 2017-06-26 22:09:35,805 main.py:47] epoch 1687, training loss: 3772.16, average training loss: 3799.57, base loss: 4600.26
[INFO 2017-06-26 22:09:36,298 main.py:47] epoch 1688, training loss: 3253.15, average training loss: 3799.26, base loss: 4600.09
[INFO 2017-06-26 22:09:36,743 main.py:47] epoch 1689, training loss: 3596.21, average training loss: 3799.07, base loss: 4600.30
[INFO 2017-06-26 22:09:37,167 main.py:47] epoch 1690, training loss: 3550.92, average training loss: 3798.85, base loss: 4600.33
[INFO 2017-06-26 22:09:37,580 main.py:47] epoch 1691, training loss: 3283.65, average training loss: 3797.99, base loss: 4599.43
[INFO 2017-06-26 22:09:38,000 main.py:47] epoch 1692, training loss: 3429.81, average training loss: 3797.69, base loss: 4599.71
[INFO 2017-06-26 22:09:38,409 main.py:47] epoch 1693, training loss: 3375.16, average training loss: 3797.67, base loss: 4600.51
[INFO 2017-06-26 22:09:38,812 main.py:47] epoch 1694, training loss: 3551.33, average training loss: 3797.82, base loss: 4601.15
[INFO 2017-06-26 22:09:39,213 main.py:47] epoch 1695, training loss: 7311.10, average training loss: 3801.02, base loss: 4604.53
[INFO 2017-06-26 22:09:39,609 main.py:47] epoch 1696, training loss: 3135.97, average training loss: 3799.68, base loss: 4603.08
[INFO 2017-06-26 22:09:40,014 main.py:47] epoch 1697, training loss: 3364.11, average training loss: 3799.60, base loss: 4603.51
[INFO 2017-06-26 22:09:40,416 main.py:47] epoch 1698, training loss: 3620.47, average training loss: 3799.23, base loss: 4603.56
[INFO 2017-06-26 22:09:40,813 main.py:47] epoch 1699, training loss: 3434.85, average training loss: 3798.58, base loss: 4603.22
[INFO 2017-06-26 22:09:40,813 main.py:49] epoch 1699, testing
[INFO 2017-06-26 22:09:42,475 main.py:102] average testing loss: 3963.71, base loss: 5046.49
[INFO 2017-06-26 22:09:42,475 main.py:103] improve_loss: 1082.78, improve_percent: 0.21
[INFO 2017-06-26 22:09:42,476 main.py:73] current best improved percent: 0.22
[INFO 2017-06-26 22:09:42,874 main.py:47] epoch 1700, training loss: 3401.46, average training loss: 3798.29, base loss: 4603.10
[INFO 2017-06-26 22:09:43,273 main.py:47] epoch 1701, training loss: 3697.59, average training loss: 3798.69, base loss: 4604.16
[INFO 2017-06-26 22:09:43,671 main.py:47] epoch 1702, training loss: 3717.85, average training loss: 3797.38, base loss: 4603.03
[INFO 2017-06-26 22:09:44,077 main.py:47] epoch 1703, training loss: 2932.91, average training loss: 3792.73, base loss: 4597.91
[INFO 2017-06-26 22:09:44,476 main.py:47] epoch 1704, training loss: 3536.80, average training loss: 3792.24, base loss: 4597.70
[INFO 2017-06-26 22:09:44,875 main.py:47] epoch 1705, training loss: 3045.46, average training loss: 3791.19, base loss: 4596.50
[INFO 2017-06-26 22:09:45,276 main.py:47] epoch 1706, training loss: 3355.79, average training loss: 3790.68, base loss: 4596.23
[INFO 2017-06-26 22:09:45,675 main.py:47] epoch 1707, training loss: 3653.91, average training loss: 3790.22, base loss: 4595.83
[INFO 2017-06-26 22:09:46,075 main.py:47] epoch 1708, training loss: 3220.90, average training loss: 3789.47, base loss: 4595.45
[INFO 2017-06-26 22:09:46,474 main.py:47] epoch 1709, training loss: 3662.09, average training loss: 3789.39, base loss: 4595.93
[INFO 2017-06-26 22:09:46,879 main.py:47] epoch 1710, training loss: 3583.99, average training loss: 3789.04, base loss: 4596.06
[INFO 2017-06-26 22:09:47,277 main.py:47] epoch 1711, training loss: 3759.94, average training loss: 3789.12, base loss: 4596.85
[INFO 2017-06-26 22:09:47,679 main.py:47] epoch 1712, training loss: 3012.45, average training loss: 3788.65, base loss: 4596.56
[INFO 2017-06-26 22:09:48,077 main.py:47] epoch 1713, training loss: 3732.25, average training loss: 3788.41, base loss: 4597.07
[INFO 2017-06-26 22:09:48,476 main.py:47] epoch 1714, training loss: 3836.16, average training loss: 3788.78, base loss: 4598.50
[INFO 2017-06-26 22:09:48,875 main.py:47] epoch 1715, training loss: 3396.73, average training loss: 3788.45, base loss: 4598.39
[INFO 2017-06-26 22:09:49,275 main.py:47] epoch 1716, training loss: 3500.24, average training loss: 3787.90, base loss: 4598.38
[INFO 2017-06-26 22:09:49,677 main.py:47] epoch 1717, training loss: 3714.83, average training loss: 3787.60, base loss: 4598.18
[INFO 2017-06-26 22:09:50,076 main.py:47] epoch 1718, training loss: 3879.43, average training loss: 3787.83, base loss: 4599.18
[INFO 2017-06-26 22:09:50,476 main.py:47] epoch 1719, training loss: 3683.61, average training loss: 3787.53, base loss: 4599.17
[INFO 2017-06-26 22:09:50,879 main.py:47] epoch 1720, training loss: 3003.45, average training loss: 3786.59, base loss: 4598.33
[INFO 2017-06-26 22:09:51,277 main.py:47] epoch 1721, training loss: 3644.50, average training loss: 3786.33, base loss: 4598.01
[INFO 2017-06-26 22:09:51,675 main.py:47] epoch 1722, training loss: 3268.15, average training loss: 3782.69, base loss: 4594.79
[INFO 2017-06-26 22:09:52,078 main.py:47] epoch 1723, training loss: 3644.73, average training loss: 3782.58, base loss: 4594.72
[INFO 2017-06-26 22:09:52,481 main.py:47] epoch 1724, training loss: 3296.91, average training loss: 3781.87, base loss: 4594.03
[INFO 2017-06-26 22:09:52,886 main.py:47] epoch 1725, training loss: 3614.48, average training loss: 3781.68, base loss: 4594.23
[INFO 2017-06-26 22:09:53,288 main.py:47] epoch 1726, training loss: 3196.41, average training loss: 3780.98, base loss: 4593.57
[INFO 2017-06-26 22:09:53,689 main.py:47] epoch 1727, training loss: 3391.64, average training loss: 3780.56, base loss: 4593.92
[INFO 2017-06-26 22:09:54,090 main.py:47] epoch 1728, training loss: 3515.53, average training loss: 3780.35, base loss: 4593.91
[INFO 2017-06-26 22:09:54,492 main.py:47] epoch 1729, training loss: 3400.10, average training loss: 3780.25, base loss: 4594.45
[INFO 2017-06-26 22:09:54,896 main.py:47] epoch 1730, training loss: 3095.04, average training loss: 3779.86, base loss: 4594.55
[INFO 2017-06-26 22:09:55,297 main.py:47] epoch 1731, training loss: 3609.79, average training loss: 3779.47, base loss: 4594.39
[INFO 2017-06-26 22:09:55,694 main.py:47] epoch 1732, training loss: 3214.72, average training loss: 3779.06, base loss: 4594.14
[INFO 2017-06-26 22:09:56,099 main.py:47] epoch 1733, training loss: 3413.34, average training loss: 3778.81, base loss: 4594.35
[INFO 2017-06-26 22:09:56,501 main.py:47] epoch 1734, training loss: 3983.17, average training loss: 3778.87, base loss: 4594.76
[INFO 2017-06-26 22:09:56,904 main.py:47] epoch 1735, training loss: 6967.79, average training loss: 3782.45, base loss: 4598.46
[INFO 2017-06-26 22:09:57,314 main.py:47] epoch 1736, training loss: 3597.69, average training loss: 3782.58, base loss: 4599.38
[INFO 2017-06-26 22:09:57,714 main.py:47] epoch 1737, training loss: 3535.55, average training loss: 3782.17, base loss: 4599.30
[INFO 2017-06-26 22:09:58,117 main.py:47] epoch 1738, training loss: 6746.92, average training loss: 3785.32, base loss: 4602.65
[INFO 2017-06-26 22:09:58,524 main.py:47] epoch 1739, training loss: 3774.55, average training loss: 3784.98, base loss: 4602.69
[INFO 2017-06-26 22:09:58,929 main.py:47] epoch 1740, training loss: 3078.35, average training loss: 3783.68, base loss: 4601.41
[INFO 2017-06-26 22:09:59,332 main.py:47] epoch 1741, training loss: 3580.21, average training loss: 3783.08, base loss: 4601.09
[INFO 2017-06-26 22:09:59,839 main.py:47] epoch 1742, training loss: 3775.42, average training loss: 3782.82, base loss: 4601.19
[INFO 2017-06-26 22:10:00,249 main.py:47] epoch 1743, training loss: 3663.21, average training loss: 3778.78, base loss: 4597.51
[INFO 2017-06-26 22:10:00,742 main.py:47] epoch 1744, training loss: 3780.09, average training loss: 3778.72, base loss: 4598.10
[INFO 2017-06-26 22:10:01,174 main.py:47] epoch 1745, training loss: 3835.03, average training loss: 3778.72, base loss: 4598.68
[INFO 2017-06-26 22:10:01,639 main.py:47] epoch 1746, training loss: 7169.44, average training loss: 3782.25, base loss: 4603.00
[INFO 2017-06-26 22:10:02,074 main.py:47] epoch 1747, training loss: 3183.74, average training loss: 3781.46, base loss: 4602.43
[INFO 2017-06-26 22:10:02,563 main.py:47] epoch 1748, training loss: 3031.27, average training loss: 3780.68, base loss: 4602.01
[INFO 2017-06-26 22:10:02,990 main.py:47] epoch 1749, training loss: 3697.38, average training loss: 3781.01, base loss: 4602.82
[INFO 2017-06-26 22:10:03,418 main.py:47] epoch 1750, training loss: 3480.11, average training loss: 3781.31, base loss: 4603.96
[INFO 2017-06-26 22:10:03,860 main.py:47] epoch 1751, training loss: 3329.04, average training loss: 3781.59, base loss: 4604.60
[INFO 2017-06-26 22:10:04,300 main.py:47] epoch 1752, training loss: 3318.89, average training loss: 3781.01, base loss: 4604.42
[INFO 2017-06-26 22:10:04,707 main.py:47] epoch 1753, training loss: 3280.93, average training loss: 3780.45, base loss: 4604.27
[INFO 2017-06-26 22:10:05,117 main.py:47] epoch 1754, training loss: 3021.02, average training loss: 3779.42, base loss: 4603.70
[INFO 2017-06-26 22:10:05,532 main.py:47] epoch 1755, training loss: 3617.47, average training loss: 3779.22, base loss: 4604.11
[INFO 2017-06-26 22:10:05,945 main.py:47] epoch 1756, training loss: 3486.26, average training loss: 3779.12, base loss: 4604.65
[INFO 2017-06-26 22:10:06,348 main.py:47] epoch 1757, training loss: 3087.95, average training loss: 3778.15, base loss: 4603.99
[INFO 2017-06-26 22:10:06,751 main.py:47] epoch 1758, training loss: 3319.07, average training loss: 3777.39, base loss: 4603.54
[INFO 2017-06-26 22:10:07,152 main.py:47] epoch 1759, training loss: 3344.05, average training loss: 3776.85, base loss: 4603.34
[INFO 2017-06-26 22:10:07,555 main.py:47] epoch 1760, training loss: 4237.73, average training loss: 3776.92, base loss: 4604.40
[INFO 2017-06-26 22:10:07,956 main.py:47] epoch 1761, training loss: 3628.59, average training loss: 3772.97, base loss: 4601.33
[INFO 2017-06-26 22:10:08,367 main.py:47] epoch 1762, training loss: 3600.50, average training loss: 3772.90, base loss: 4601.91
[INFO 2017-06-26 22:10:08,769 main.py:47] epoch 1763, training loss: 7352.32, average training loss: 3776.36, base loss: 4605.78
[INFO 2017-06-26 22:10:09,170 main.py:47] epoch 1764, training loss: 3292.40, average training loss: 3775.55, base loss: 4605.30
[INFO 2017-06-26 22:10:09,575 main.py:47] epoch 1765, training loss: 3903.65, average training loss: 3775.86, base loss: 4606.67
[INFO 2017-06-26 22:10:09,972 main.py:47] epoch 1766, training loss: 3799.73, average training loss: 3775.74, base loss: 4607.20
[INFO 2017-06-26 22:10:10,375 main.py:47] epoch 1767, training loss: 3480.91, average training loss: 3775.08, base loss: 4607.08
[INFO 2017-06-26 22:10:10,780 main.py:47] epoch 1768, training loss: 3633.03, average training loss: 3775.02, base loss: 4607.25
[INFO 2017-06-26 22:10:11,217 main.py:47] epoch 1769, training loss: 3789.26, average training loss: 3774.85, base loss: 4607.71
[INFO 2017-06-26 22:10:11,627 main.py:47] epoch 1770, training loss: 3628.64, average training loss: 3774.97, base loss: 4608.33
[INFO 2017-06-26 22:10:12,030 main.py:47] epoch 1771, training loss: 3119.28, average training loss: 3774.02, base loss: 4606.95
[INFO 2017-06-26 22:10:12,435 main.py:47] epoch 1772, training loss: 3586.99, average training loss: 3773.18, base loss: 4606.39
[INFO 2017-06-26 22:10:12,837 main.py:47] epoch 1773, training loss: 3722.88, average training loss: 3773.29, base loss: 4607.20
[INFO 2017-06-26 22:10:13,240 main.py:47] epoch 1774, training loss: 3644.48, average training loss: 3769.69, base loss: 4604.30
[INFO 2017-06-26 22:10:13,647 main.py:47] epoch 1775, training loss: 3764.81, average training loss: 3769.23, base loss: 4604.32
[INFO 2017-06-26 22:10:14,049 main.py:47] epoch 1776, training loss: 3611.99, average training loss: 3768.89, base loss: 4604.26
[INFO 2017-06-26 22:10:14,452 main.py:47] epoch 1777, training loss: 3358.83, average training loss: 3768.37, base loss: 4603.94
[INFO 2017-06-26 22:10:14,854 main.py:47] epoch 1778, training loss: 4048.68, average training loss: 3768.50, base loss: 4604.74
[INFO 2017-06-26 22:10:15,256 main.py:47] epoch 1779, training loss: 2909.07, average training loss: 3767.71, base loss: 4603.68
[INFO 2017-06-26 22:10:15,662 main.py:47] epoch 1780, training loss: 3457.47, average training loss: 3766.40, base loss: 4602.20
[INFO 2017-06-26 22:10:16,070 main.py:47] epoch 1781, training loss: 3511.25, average training loss: 3765.86, base loss: 4602.24
[INFO 2017-06-26 22:10:16,474 main.py:47] epoch 1782, training loss: 3217.12, average training loss: 3765.49, base loss: 4602.32
[INFO 2017-06-26 22:10:16,883 main.py:47] epoch 1783, training loss: 3332.86, average training loss: 3764.86, base loss: 4602.05
[INFO 2017-06-26 22:10:17,285 main.py:47] epoch 1784, training loss: 3264.84, average training loss: 3764.69, base loss: 4602.32
[INFO 2017-06-26 22:10:17,683 main.py:47] epoch 1785, training loss: 3404.87, average training loss: 3764.38, base loss: 4602.02
[INFO 2017-06-26 22:10:18,086 main.py:47] epoch 1786, training loss: 3604.01, average training loss: 3764.37, base loss: 4602.61
[INFO 2017-06-26 22:10:18,513 main.py:47] epoch 1787, training loss: 3620.61, average training loss: 3764.45, base loss: 4603.21
[INFO 2017-06-26 22:10:18,916 main.py:47] epoch 1788, training loss: 7137.00, average training loss: 3767.86, base loss: 4606.87
[INFO 2017-06-26 22:10:19,320 main.py:47] epoch 1789, training loss: 3471.21, average training loss: 3767.47, base loss: 4606.70
[INFO 2017-06-26 22:10:19,721 main.py:47] epoch 1790, training loss: 3779.71, average training loss: 3767.05, base loss: 4606.38
[INFO 2017-06-26 22:10:20,124 main.py:47] epoch 1791, training loss: 3664.10, average training loss: 3766.84, base loss: 4606.75
[INFO 2017-06-26 22:10:20,527 main.py:47] epoch 1792, training loss: 3336.28, average training loss: 3766.81, base loss: 4607.20
[INFO 2017-06-26 22:10:20,935 main.py:47] epoch 1793, training loss: 3454.79, average training loss: 3766.60, base loss: 4607.62
[INFO 2017-06-26 22:10:21,338 main.py:47] epoch 1794, training loss: 3857.65, average training loss: 3766.77, base loss: 4608.05
[INFO 2017-06-26 22:10:21,739 main.py:47] epoch 1795, training loss: 7062.75, average training loss: 3770.33, base loss: 4611.78
[INFO 2017-06-26 22:10:22,143 main.py:47] epoch 1796, training loss: 2922.15, average training loss: 3769.94, base loss: 4611.51
[INFO 2017-06-26 22:10:22,548 main.py:47] epoch 1797, training loss: 3655.38, average training loss: 3769.49, base loss: 4611.59
[INFO 2017-06-26 22:10:22,953 main.py:47] epoch 1798, training loss: 2992.75, average training loss: 3768.70, base loss: 4610.83
[INFO 2017-06-26 22:10:23,357 main.py:47] epoch 1799, training loss: 3093.32, average training loss: 3768.04, base loss: 4609.98
[INFO 2017-06-26 22:10:23,357 main.py:49] epoch 1799, testing
[INFO 2017-06-26 22:10:25,012 main.py:102] average testing loss: 3468.28, base loss: 4437.37
[INFO 2017-06-26 22:10:25,012 main.py:103] improve_loss: 969.10, improve_percent: 0.22
[INFO 2017-06-26 22:10:25,013 main.py:73] current best improved percent: 0.22
[INFO 2017-06-26 22:10:25,412 main.py:47] epoch 1800, training loss: 4105.52, average training loss: 3768.91, base loss: 4612.02
[INFO 2017-06-26 22:10:25,811 main.py:47] epoch 1801, training loss: 3765.10, average training loss: 3769.29, base loss: 4612.99
[INFO 2017-06-26 22:10:26,208 main.py:47] epoch 1802, training loss: 3686.68, average training loss: 3769.76, base loss: 4614.04
[INFO 2017-06-26 22:10:26,610 main.py:47] epoch 1803, training loss: 3245.47, average training loss: 3768.69, base loss: 4612.82
[INFO 2017-06-26 22:10:27,013 main.py:47] epoch 1804, training loss: 3661.49, average training loss: 3768.90, base loss: 4613.77
[INFO 2017-06-26 22:10:27,417 main.py:47] epoch 1805, training loss: 3465.54, average training loss: 3768.51, base loss: 4613.51
[INFO 2017-06-26 22:10:27,821 main.py:47] epoch 1806, training loss: 3298.87, average training loss: 3767.59, base loss: 4612.73
[INFO 2017-06-26 22:10:28,226 main.py:47] epoch 1807, training loss: 3309.98, average training loss: 3766.84, base loss: 4612.14
[INFO 2017-06-26 22:10:28,629 main.py:47] epoch 1808, training loss: 3410.05, average training loss: 3766.57, base loss: 4611.91
[INFO 2017-06-26 22:10:29,033 main.py:47] epoch 1809, training loss: 2938.90, average training loss: 3765.15, base loss: 4610.16
[INFO 2017-06-26 22:10:29,437 main.py:47] epoch 1810, training loss: 3263.64, average training loss: 3764.73, base loss: 4609.95
[INFO 2017-06-26 22:10:29,840 main.py:47] epoch 1811, training loss: 3755.47, average training loss: 3764.90, base loss: 4610.85
[INFO 2017-06-26 22:10:30,241 main.py:47] epoch 1812, training loss: 3443.37, average training loss: 3764.36, base loss: 4610.28
[INFO 2017-06-26 22:10:30,645 main.py:47] epoch 1813, training loss: 4026.09, average training loss: 3763.94, base loss: 4609.94
[INFO 2017-06-26 22:10:31,052 main.py:47] epoch 1814, training loss: 3554.54, average training loss: 3763.71, base loss: 4610.22
[INFO 2017-06-26 22:10:31,457 main.py:47] epoch 1815, training loss: 3768.10, average training loss: 3763.45, base loss: 4610.34
[INFO 2017-06-26 22:10:31,859 main.py:47] epoch 1816, training loss: 3532.78, average training loss: 3762.61, base loss: 4609.76
[INFO 2017-06-26 22:10:32,268 main.py:47] epoch 1817, training loss: 3616.08, average training loss: 3762.22, base loss: 4609.74
[INFO 2017-06-26 22:10:32,671 main.py:47] epoch 1818, training loss: 6910.28, average training loss: 3765.39, base loss: 4613.21
[INFO 2017-06-26 22:10:33,080 main.py:47] epoch 1819, training loss: 3569.49, average training loss: 3765.40, base loss: 4613.63
[INFO 2017-06-26 22:10:33,484 main.py:47] epoch 1820, training loss: 3198.03, average training loss: 3764.92, base loss: 4613.09
[INFO 2017-06-26 22:10:33,890 main.py:47] epoch 1821, training loss: 3228.52, average training loss: 3764.80, base loss: 4613.45
[INFO 2017-06-26 22:10:34,293 main.py:47] epoch 1822, training loss: 3342.12, average training loss: 3764.39, base loss: 4613.47
[INFO 2017-06-26 22:10:34,699 main.py:47] epoch 1823, training loss: 3615.35, average training loss: 3763.85, base loss: 4612.85
[INFO 2017-06-26 22:10:35,102 main.py:47] epoch 1824, training loss: 3372.05, average training loss: 3763.24, base loss: 4612.45
[INFO 2017-06-26 22:10:35,506 main.py:47] epoch 1825, training loss: 3307.53, average training loss: 3762.39, base loss: 4611.78
[INFO 2017-06-26 22:10:35,909 main.py:47] epoch 1826, training loss: 3971.10, average training loss: 3763.20, base loss: 4613.09
[INFO 2017-06-26 22:10:36,311 main.py:47] epoch 1827, training loss: 3204.54, average training loss: 3762.28, base loss: 4611.87
[INFO 2017-06-26 22:10:36,711 main.py:47] epoch 1828, training loss: 3774.03, average training loss: 3762.09, base loss: 4612.22
[INFO 2017-06-26 22:10:37,109 main.py:47] epoch 1829, training loss: 4045.35, average training loss: 3762.62, base loss: 4613.33
[INFO 2017-06-26 22:10:37,517 main.py:47] epoch 1830, training loss: 3782.07, average training loss: 3763.29, base loss: 4614.90
[INFO 2017-06-26 22:10:37,919 main.py:47] epoch 1831, training loss: 3211.51, average training loss: 3762.68, base loss: 4614.57
[INFO 2017-06-26 22:10:38,324 main.py:47] epoch 1832, training loss: 3467.76, average training loss: 3762.91, base loss: 4615.28
[INFO 2017-06-26 22:10:38,721 main.py:47] epoch 1833, training loss: 3259.83, average training loss: 3762.33, base loss: 4615.41
[INFO 2017-06-26 22:10:39,125 main.py:47] epoch 1834, training loss: 2955.77, average training loss: 3761.20, base loss: 4613.98
[INFO 2017-06-26 22:10:39,555 main.py:47] epoch 1835, training loss: 3202.93, average training loss: 3760.60, base loss: 4613.40
[INFO 2017-06-26 22:10:39,981 main.py:47] epoch 1836, training loss: 3937.67, average training loss: 3760.91, base loss: 4614.33
[INFO 2017-06-26 22:10:40,386 main.py:47] epoch 1837, training loss: 2946.11, average training loss: 3760.23, base loss: 4613.63
[INFO 2017-06-26 22:10:40,900 main.py:47] epoch 1838, training loss: 3346.83, average training loss: 3756.31, base loss: 4609.77
[INFO 2017-06-26 22:10:41,308 main.py:47] epoch 1839, training loss: 7111.46, average training loss: 3759.63, base loss: 4612.82
[INFO 2017-06-26 22:10:41,716 main.py:47] epoch 1840, training loss: 3705.76, average training loss: 3759.14, base loss: 4612.74
[INFO 2017-06-26 22:10:42,124 main.py:47] epoch 1841, training loss: 3575.81, average training loss: 3758.75, base loss: 4612.58
[INFO 2017-06-26 22:10:42,533 main.py:47] epoch 1842, training loss: 3425.12, average training loss: 3758.46, base loss: 4612.78
[INFO 2017-06-26 22:10:42,943 main.py:47] epoch 1843, training loss: 3042.12, average training loss: 3757.97, base loss: 4612.41
[INFO 2017-06-26 22:10:43,350 main.py:47] epoch 1844, training loss: 2940.24, average training loss: 3756.62, base loss: 4610.33
[INFO 2017-06-26 22:10:43,749 main.py:47] epoch 1845, training loss: 4275.38, average training loss: 3757.24, base loss: 4611.62
[INFO 2017-06-26 22:10:44,159 main.py:47] epoch 1846, training loss: 3222.04, average training loss: 3756.54, base loss: 4610.96
[INFO 2017-06-26 22:10:44,569 main.py:47] epoch 1847, training loss: 3589.57, average training loss: 3756.09, base loss: 4610.95
[INFO 2017-06-26 22:10:44,973 main.py:47] epoch 1848, training loss: 3014.01, average training loss: 3755.17, base loss: 4610.22
[INFO 2017-06-26 22:10:45,380 main.py:47] epoch 1849, training loss: 3825.33, average training loss: 3755.23, base loss: 4610.66
[INFO 2017-06-26 22:10:45,783 main.py:47] epoch 1850, training loss: 3144.18, average training loss: 3754.71, base loss: 4610.11
[INFO 2017-06-26 22:10:46,191 main.py:47] epoch 1851, training loss: 2997.54, average training loss: 3753.25, base loss: 4607.97
[INFO 2017-06-26 22:10:46,595 main.py:47] epoch 1852, training loss: 3066.45, average training loss: 3752.59, base loss: 4607.40
[INFO 2017-06-26 22:10:47,002 main.py:47] epoch 1853, training loss: 3627.92, average training loss: 3752.25, base loss: 4607.42
[INFO 2017-06-26 22:10:47,407 main.py:47] epoch 1854, training loss: 3380.05, average training loss: 3751.71, base loss: 4606.86
[INFO 2017-06-26 22:10:47,812 main.py:47] epoch 1855, training loss: 3834.08, average training loss: 3751.85, base loss: 4607.17
[INFO 2017-06-26 22:10:48,215 main.py:47] epoch 1856, training loss: 3477.81, average training loss: 3751.64, base loss: 4607.27
[INFO 2017-06-26 22:10:48,618 main.py:47] epoch 1857, training loss: 3773.83, average training loss: 3752.29, base loss: 4608.80
[INFO 2017-06-26 22:10:49,027 main.py:47] epoch 1858, training loss: 3713.27, average training loss: 3751.77, base loss: 4608.58
[INFO 2017-06-26 22:10:49,430 main.py:47] epoch 1859, training loss: 3293.67, average training loss: 3750.75, base loss: 4607.79
[INFO 2017-06-26 22:10:49,829 main.py:47] epoch 1860, training loss: 3341.57, average training loss: 3750.90, base loss: 4608.33
[INFO 2017-06-26 22:10:50,232 main.py:47] epoch 1861, training loss: 3774.02, average training loss: 3750.59, base loss: 4608.20
[INFO 2017-06-26 22:10:50,633 main.py:47] epoch 1862, training loss: 4041.94, average training loss: 3750.63, base loss: 4608.82
[INFO 2017-06-26 22:10:51,033 main.py:47] epoch 1863, training loss: 3237.72, average training loss: 3750.12, base loss: 4608.27
[INFO 2017-06-26 22:10:51,431 main.py:47] epoch 1864, training loss: 3772.15, average training loss: 3750.29, base loss: 4609.13
[INFO 2017-06-26 22:10:51,831 main.py:47] epoch 1865, training loss: 3860.12, average training loss: 3750.22, base loss: 4609.68
[INFO 2017-06-26 22:10:52,235 main.py:47] epoch 1866, training loss: 3182.87, average training loss: 3749.87, base loss: 4609.19
[INFO 2017-06-26 22:10:52,641 main.py:47] epoch 1867, training loss: 3510.78, average training loss: 3749.87, base loss: 4609.94
[INFO 2017-06-26 22:10:53,040 main.py:47] epoch 1868, training loss: 3943.94, average training loss: 3750.09, base loss: 4610.78
[INFO 2017-06-26 22:10:53,439 main.py:47] epoch 1869, training loss: 3392.83, average training loss: 3749.83, base loss: 4610.70
[INFO 2017-06-26 22:10:53,843 main.py:47] epoch 1870, training loss: 3450.72, average training loss: 3750.11, base loss: 4611.55
[INFO 2017-06-26 22:10:54,245 main.py:47] epoch 1871, training loss: 3771.88, average training loss: 3749.87, base loss: 4611.94
[INFO 2017-06-26 22:10:54,650 main.py:47] epoch 1872, training loss: 4086.64, average training loss: 3750.00, base loss: 4612.92
[INFO 2017-06-26 22:10:55,048 main.py:47] epoch 1873, training loss: 3073.92, average training loss: 3749.82, base loss: 4613.01
[INFO 2017-06-26 22:10:55,447 main.py:47] epoch 1874, training loss: 3300.44, average training loss: 3749.16, base loss: 4612.16
[INFO 2017-06-26 22:10:55,850 main.py:47] epoch 1875, training loss: 3647.00, average training loss: 3749.08, base loss: 4612.72
[INFO 2017-06-26 22:10:56,255 main.py:47] epoch 1876, training loss: 3157.12, average training loss: 3748.78, base loss: 4612.66
[INFO 2017-06-26 22:10:56,659 main.py:47] epoch 1877, training loss: 3430.47, average training loss: 3749.00, base loss: 4613.54
[INFO 2017-06-26 22:10:57,063 main.py:47] epoch 1878, training loss: 3934.74, average training loss: 3745.62, base loss: 4610.54
[INFO 2017-06-26 22:10:57,468 main.py:47] epoch 1879, training loss: 3556.05, average training loss: 3745.19, base loss: 4610.32
[INFO 2017-06-26 22:10:57,885 main.py:47] epoch 1880, training loss: 3034.54, average training loss: 3744.49, base loss: 4609.72
[INFO 2017-06-26 22:10:58,287 main.py:47] epoch 1881, training loss: 3601.41, average training loss: 3744.36, base loss: 4609.55
[INFO 2017-06-26 22:10:58,692 main.py:47] epoch 1882, training loss: 3486.46, average training loss: 3744.17, base loss: 4609.56
[INFO 2017-06-26 22:10:59,093 main.py:47] epoch 1883, training loss: 3189.62, average training loss: 3743.81, base loss: 4609.30
[INFO 2017-06-26 22:10:59,491 main.py:47] epoch 1884, training loss: 3259.93, average training loss: 3743.85, base loss: 4609.98
[INFO 2017-06-26 22:10:59,900 main.py:47] epoch 1885, training loss: 3462.21, average training loss: 3743.49, base loss: 4609.63
[INFO 2017-06-26 22:11:00,304 main.py:47] epoch 1886, training loss: 3174.66, average training loss: 3743.26, base loss: 4609.79
[INFO 2017-06-26 22:11:00,706 main.py:47] epoch 1887, training loss: 3815.19, average training loss: 3743.34, base loss: 4610.81
[INFO 2017-06-26 22:11:01,110 main.py:47] epoch 1888, training loss: 3047.24, average training loss: 3741.94, base loss: 4609.34
[INFO 2017-06-26 22:11:01,514 main.py:47] epoch 1889, training loss: 3773.69, average training loss: 3741.83, base loss: 4610.02
[INFO 2017-06-26 22:11:01,925 main.py:47] epoch 1890, training loss: 3388.12, average training loss: 3737.73, base loss: 4606.10
[INFO 2017-06-26 22:11:02,326 main.py:47] epoch 1891, training loss: 3336.83, average training loss: 3737.37, base loss: 4605.90
[INFO 2017-06-26 22:11:02,724 main.py:47] epoch 1892, training loss: 3723.12, average training loss: 3737.31, base loss: 4606.81
[INFO 2017-06-26 22:11:03,127 main.py:47] epoch 1893, training loss: 3490.34, average training loss: 3737.09, base loss: 4607.13
[INFO 2017-06-26 22:11:03,526 main.py:47] epoch 1894, training loss: 3316.41, average training loss: 3736.59, base loss: 4606.90
[INFO 2017-06-26 22:11:03,932 main.py:47] epoch 1895, training loss: 3733.53, average training loss: 3736.69, base loss: 4607.37
[INFO 2017-06-26 22:11:04,335 main.py:47] epoch 1896, training loss: 3519.03, average training loss: 3736.41, base loss: 4607.28
[INFO 2017-06-26 22:11:04,736 main.py:47] epoch 1897, training loss: 3245.84, average training loss: 3735.84, base loss: 4606.63
[INFO 2017-06-26 22:11:05,139 main.py:47] epoch 1898, training loss: 3144.66, average training loss: 3735.60, base loss: 4607.03
[INFO 2017-06-26 22:11:05,544 main.py:47] epoch 1899, training loss: 3075.76, average training loss: 3734.96, base loss: 4605.92
[INFO 2017-06-26 22:11:05,544 main.py:49] epoch 1899, testing
[INFO 2017-06-26 22:11:07,200 main.py:102] average testing loss: 3762.71, base loss: 4765.35
[INFO 2017-06-26 22:11:07,200 main.py:103] improve_loss: 1002.64, improve_percent: 0.21
[INFO 2017-06-26 22:11:07,201 main.py:73] current best improved percent: 0.22
[INFO 2017-06-26 22:11:07,605 main.py:47] epoch 1900, training loss: 3712.86, average training loss: 3734.50, base loss: 4605.98
[INFO 2017-06-26 22:11:08,005 main.py:47] epoch 1901, training loss: 3657.68, average training loss: 3734.64, base loss: 4606.77
[INFO 2017-06-26 22:11:08,412 main.py:47] epoch 1902, training loss: 3419.61, average training loss: 3734.21, base loss: 4606.61
[INFO 2017-06-26 22:11:08,819 main.py:47] epoch 1903, training loss: 3445.61, average training loss: 3734.51, base loss: 4607.71
[INFO 2017-06-26 22:11:09,220 main.py:47] epoch 1904, training loss: 3605.79, average training loss: 3734.34, base loss: 4607.76
[INFO 2017-06-26 22:11:09,620 main.py:47] epoch 1905, training loss: 3543.01, average training loss: 3733.85, base loss: 4607.20
[INFO 2017-06-26 22:11:10,024 main.py:47] epoch 1906, training loss: 3076.57, average training loss: 3732.99, base loss: 4606.12
[INFO 2017-06-26 22:11:10,427 main.py:47] epoch 1907, training loss: 3506.79, average training loss: 3732.99, base loss: 4606.54
[INFO 2017-06-26 22:11:10,831 main.py:47] epoch 1908, training loss: 3094.71, average training loss: 3733.07, base loss: 4607.07
[INFO 2017-06-26 22:11:11,236 main.py:47] epoch 1909, training loss: 3220.83, average training loss: 3732.89, base loss: 4607.13
[INFO 2017-06-26 22:11:11,640 main.py:47] epoch 1910, training loss: 4035.64, average training loss: 3733.43, base loss: 4608.11
[INFO 2017-06-26 22:11:12,044 main.py:47] epoch 1911, training loss: 3841.45, average training loss: 3733.60, base loss: 4608.75
[INFO 2017-06-26 22:11:12,448 main.py:47] epoch 1912, training loss: 3825.47, average training loss: 3733.62, base loss: 4609.34
[INFO 2017-06-26 22:11:12,849 main.py:47] epoch 1913, training loss: 3617.12, average training loss: 3733.82, base loss: 4609.63
[INFO 2017-06-26 22:11:13,250 main.py:47] epoch 1914, training loss: 3381.22, average training loss: 3733.21, base loss: 4609.00
[INFO 2017-06-26 22:11:13,657 main.py:47] epoch 1915, training loss: 3776.86, average training loss: 3733.33, base loss: 4609.51
[INFO 2017-06-26 22:11:14,060 main.py:47] epoch 1916, training loss: 3882.56, average training loss: 3733.88, base loss: 4611.02
[INFO 2017-06-26 22:11:14,459 main.py:47] epoch 1917, training loss: 2893.45, average training loss: 3732.85, base loss: 4609.65
[INFO 2017-06-26 22:11:14,865 main.py:47] epoch 1918, training loss: 3366.38, average training loss: 3732.94, base loss: 4610.14
[INFO 2017-06-26 22:11:15,264 main.py:47] epoch 1919, training loss: 3365.16, average training loss: 3732.58, base loss: 4609.64
[INFO 2017-06-26 22:11:15,664 main.py:47] epoch 1920, training loss: 4068.74, average training loss: 3733.64, base loss: 4611.64
[INFO 2017-06-26 22:11:16,067 main.py:47] epoch 1921, training loss: 3129.96, average training loss: 3731.87, base loss: 4609.86
[INFO 2017-06-26 22:11:16,468 main.py:47] epoch 1922, training loss: 3028.74, average training loss: 3731.48, base loss: 4609.72
[INFO 2017-06-26 22:11:16,870 main.py:47] epoch 1923, training loss: 3667.00, average training loss: 3731.85, base loss: 4610.57
[INFO 2017-06-26 22:11:17,273 main.py:47] epoch 1924, training loss: 3573.29, average training loss: 3731.90, base loss: 4611.08
[INFO 2017-06-26 22:11:17,699 main.py:47] epoch 1925, training loss: 3511.22, average training loss: 3731.85, base loss: 4611.55
[INFO 2017-06-26 22:11:18,104 main.py:47] epoch 1926, training loss: 3699.93, average training loss: 3731.86, base loss: 4612.05
[INFO 2017-06-26 22:11:18,509 main.py:47] epoch 1927, training loss: 3458.18, average training loss: 3732.09, base loss: 4612.93
[INFO 2017-06-26 22:11:18,913 main.py:47] epoch 1928, training loss: 3708.45, average training loss: 3732.06, base loss: 4612.94
[INFO 2017-06-26 22:11:19,316 main.py:47] epoch 1929, training loss: 3650.41, average training loss: 3731.83, base loss: 4613.34
[INFO 2017-06-26 22:11:19,717 main.py:47] epoch 1930, training loss: 3669.58, average training loss: 3732.12, base loss: 4614.01
[INFO 2017-06-26 22:11:20,118 main.py:47] epoch 1931, training loss: 3498.46, average training loss: 3731.89, base loss: 4613.99
[INFO 2017-06-26 22:11:20,520 main.py:47] epoch 1932, training loss: 3813.94, average training loss: 3728.06, base loss: 4610.35
[INFO 2017-06-26 22:11:20,923 main.py:47] epoch 1933, training loss: 3612.10, average training loss: 3728.01, base loss: 4610.63
[INFO 2017-06-26 22:11:21,322 main.py:47] epoch 1934, training loss: 3680.74, average training loss: 3727.70, base loss: 4610.53
[INFO 2017-06-26 22:11:21,727 main.py:47] epoch 1935, training loss: 3658.95, average training loss: 3727.60, base loss: 4610.80
[INFO 2017-06-26 22:11:22,132 main.py:47] epoch 1936, training loss: 3751.73, average training loss: 3727.13, base loss: 4610.55
[INFO 2017-06-26 22:11:22,531 main.py:47] epoch 1937, training loss: 3264.10, average training loss: 3726.79, base loss: 4610.34
[INFO 2017-06-26 22:11:22,933 main.py:47] epoch 1938, training loss: 3676.22, average training loss: 3726.73, base loss: 4610.38
[INFO 2017-06-26 22:11:23,331 main.py:47] epoch 1939, training loss: 3506.17, average training loss: 3726.55, base loss: 4610.87
[INFO 2017-06-26 22:11:23,733 main.py:47] epoch 1940, training loss: 3790.01, average training loss: 3726.48, base loss: 4611.48
[INFO 2017-06-26 22:11:24,132 main.py:47] epoch 1941, training loss: 3140.74, average training loss: 3725.30, base loss: 4610.23
[INFO 2017-06-26 22:11:24,534 main.py:47] epoch 1942, training loss: 3553.49, average training loss: 3725.14, base loss: 4610.46
[INFO 2017-06-26 22:11:24,937 main.py:47] epoch 1943, training loss: 3251.84, average training loss: 3724.78, base loss: 4610.11
[INFO 2017-06-26 22:11:25,337 main.py:47] epoch 1944, training loss: 3180.89, average training loss: 3723.70, base loss: 4609.24
[INFO 2017-06-26 22:11:25,736 main.py:47] epoch 1945, training loss: 3161.38, average training loss: 3723.90, base loss: 4609.54
[INFO 2017-06-26 22:11:26,137 main.py:47] epoch 1946, training loss: 3460.16, average training loss: 3723.51, base loss: 4609.52
[INFO 2017-06-26 22:11:26,540 main.py:47] epoch 1947, training loss: 2914.19, average training loss: 3722.93, base loss: 4609.21
[INFO 2017-06-26 22:11:26,941 main.py:47] epoch 1948, training loss: 3253.61, average training loss: 3721.84, base loss: 4607.92
[INFO 2017-06-26 22:11:27,343 main.py:47] epoch 1949, training loss: 2985.86, average training loss: 3721.07, base loss: 4607.08
[INFO 2017-06-26 22:11:27,747 main.py:47] epoch 1950, training loss: 3218.36, average training loss: 3720.60, base loss: 4606.78
[INFO 2017-06-26 22:11:28,149 main.py:47] epoch 1951, training loss: 3234.25, average training loss: 3720.31, base loss: 4606.60
[INFO 2017-06-26 22:11:28,552 main.py:47] epoch 1952, training loss: 3164.45, average training loss: 3720.08, base loss: 4606.90
[INFO 2017-06-26 22:11:28,959 main.py:47] epoch 1953, training loss: 3257.73, average training loss: 3719.44, base loss: 4606.17
[INFO 2017-06-26 22:11:29,363 main.py:47] epoch 1954, training loss: 3119.12, average training loss: 3718.91, base loss: 4605.68
[INFO 2017-06-26 22:11:29,762 main.py:47] epoch 1955, training loss: 3709.36, average training loss: 3718.90, base loss: 4606.16
[INFO 2017-06-26 22:11:30,161 main.py:47] epoch 1956, training loss: 3038.79, average training loss: 3718.38, base loss: 4605.65
[INFO 2017-06-26 22:11:30,564 main.py:47] epoch 1957, training loss: 3357.13, average training loss: 3717.41, base loss: 4604.62
[INFO 2017-06-26 22:11:30,962 main.py:47] epoch 1958, training loss: 3059.22, average training loss: 3715.90, base loss: 4602.72
[INFO 2017-06-26 22:11:31,361 main.py:47] epoch 1959, training loss: 3098.65, average training loss: 3715.14, base loss: 4602.29
[INFO 2017-06-26 22:11:31,759 main.py:47] epoch 1960, training loss: 3293.31, average training loss: 3714.82, base loss: 4602.14
[INFO 2017-06-26 22:11:32,162 main.py:47] epoch 1961, training loss: 3545.21, average training loss: 3714.49, base loss: 4602.43
[INFO 2017-06-26 22:11:32,564 main.py:47] epoch 1962, training loss: 3332.40, average training loss: 3714.51, base loss: 4602.92
[INFO 2017-06-26 22:11:32,963 main.py:47] epoch 1963, training loss: 3633.95, average training loss: 3714.59, base loss: 4603.58
[INFO 2017-06-26 22:11:33,366 main.py:47] epoch 1964, training loss: 3604.84, average training loss: 3714.82, base loss: 4604.36
[INFO 2017-06-26 22:11:33,769 main.py:47] epoch 1965, training loss: 3578.97, average training loss: 3714.57, base loss: 4604.18
[INFO 2017-06-26 22:11:34,171 main.py:47] epoch 1966, training loss: 3757.06, average training loss: 3714.91, base loss: 4604.97
[INFO 2017-06-26 22:11:34,578 main.py:47] epoch 1967, training loss: 2898.71, average training loss: 3714.00, base loss: 4603.96
[INFO 2017-06-26 22:11:34,988 main.py:47] epoch 1968, training loss: 2785.73, average training loss: 3712.60, base loss: 4602.17
[INFO 2017-06-26 22:11:35,388 main.py:47] epoch 1969, training loss: 3200.72, average training loss: 3711.92, base loss: 4601.41
[INFO 2017-06-26 22:11:35,787 main.py:47] epoch 1970, training loss: 3507.18, average training loss: 3711.57, base loss: 4601.24
[INFO 2017-06-26 22:11:36,189 main.py:47] epoch 1971, training loss: 3426.70, average training loss: 3711.00, base loss: 4600.55
[INFO 2017-06-26 22:11:36,593 main.py:47] epoch 1972, training loss: 2853.36, average training loss: 3709.42, base loss: 4598.54
[INFO 2017-06-26 22:11:36,998 main.py:47] epoch 1973, training loss: 3117.74, average training loss: 3708.61, base loss: 4597.45
[INFO 2017-06-26 22:11:37,401 main.py:47] epoch 1974, training loss: 3854.44, average training loss: 3708.87, base loss: 4598.45
[INFO 2017-06-26 22:11:37,803 main.py:47] epoch 1975, training loss: 3430.99, average training loss: 3708.97, base loss: 4598.99
[INFO 2017-06-26 22:11:38,206 main.py:47] epoch 1976, training loss: 6983.78, average training loss: 3712.68, base loss: 4602.94
[INFO 2017-06-26 22:11:38,602 main.py:47] epoch 1977, training loss: 3786.54, average training loss: 3713.20, base loss: 4604.25
[INFO 2017-06-26 22:11:39,001 main.py:47] epoch 1978, training loss: 3173.55, average training loss: 3711.85, base loss: 4602.90
[INFO 2017-06-26 22:11:39,403 main.py:47] epoch 1979, training loss: 4080.31, average training loss: 3712.44, base loss: 4604.65
[INFO 2017-06-26 22:11:39,802 main.py:47] epoch 1980, training loss: 3214.87, average training loss: 3711.70, base loss: 4604.10
[INFO 2017-06-26 22:11:40,205 main.py:47] epoch 1981, training loss: 3266.57, average training loss: 3710.94, base loss: 4603.45
[INFO 2017-06-26 22:11:40,604 main.py:47] epoch 1982, training loss: 3403.11, average training loss: 3710.64, base loss: 4603.86
[INFO 2017-06-26 22:11:41,002 main.py:47] epoch 1983, training loss: 3723.36, average training loss: 3710.29, base loss: 4604.50
[INFO 2017-06-26 22:11:41,403 main.py:47] epoch 1984, training loss: 7268.98, average training loss: 3713.60, base loss: 4608.59
[INFO 2017-06-26 22:11:41,806 main.py:47] epoch 1985, training loss: 3660.31, average training loss: 3713.90, base loss: 4609.73
[INFO 2017-06-26 22:11:42,208 main.py:47] epoch 1986, training loss: 3297.96, average training loss: 3713.01, base loss: 4609.49
[INFO 2017-06-26 22:11:42,610 main.py:47] epoch 1987, training loss: 3189.60, average training loss: 3712.77, base loss: 4609.88
[INFO 2017-06-26 22:11:43,007 main.py:47] epoch 1988, training loss: 3718.19, average training loss: 3712.23, base loss: 4610.07
[INFO 2017-06-26 22:11:43,409 main.py:47] epoch 1989, training loss: 3546.89, average training loss: 3712.46, base loss: 4610.44
[INFO 2017-06-26 22:11:43,810 main.py:47] epoch 1990, training loss: 3254.67, average training loss: 3711.83, base loss: 4609.94
[INFO 2017-06-26 22:11:44,213 main.py:47] epoch 1991, training loss: 3827.56, average training loss: 3711.62, base loss: 4610.25
[INFO 2017-06-26 22:11:44,615 main.py:47] epoch 1992, training loss: 3426.19, average training loss: 3710.74, base loss: 4609.28
[INFO 2017-06-26 22:11:45,015 main.py:47] epoch 1993, training loss: 3723.56, average training loss: 3710.66, base loss: 4609.89
[INFO 2017-06-26 22:11:45,417 main.py:47] epoch 1994, training loss: 3203.41, average training loss: 3710.51, base loss: 4609.87
[INFO 2017-06-26 22:11:45,823 main.py:47] epoch 1995, training loss: 3433.85, average training loss: 3710.30, base loss: 4610.16
[INFO 2017-06-26 22:11:46,225 main.py:47] epoch 1996, training loss: 2951.37, average training loss: 3709.82, base loss: 4609.87
[INFO 2017-06-26 22:11:46,625 main.py:47] epoch 1997, training loss: 3341.73, average training loss: 3709.08, base loss: 4609.25
[INFO 2017-06-26 22:11:47,038 main.py:47] epoch 1998, training loss: 3646.29, average training loss: 3708.38, base loss: 4608.46
[INFO 2017-06-26 22:11:47,437 main.py:47] epoch 1999, training loss: 3953.21, average training loss: 3708.30, base loss: 4608.99
[INFO 2017-06-26 22:11:47,438 main.py:49] epoch 1999, testing
[INFO 2017-06-26 22:11:49,097 main.py:102] average testing loss: 3530.13, base loss: 4730.98
[INFO 2017-06-26 22:11:49,097 main.py:103] improve_loss: 1200.85, improve_percent: 0.25
[INFO 2017-06-26 22:11:49,098 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 22:11:49,113 main.py:73] current best improved percent: 0.25
[INFO 2017-06-26 22:11:49,515 main.py:47] epoch 2000, training loss: 3646.26, average training loss: 3708.74, base loss: 4610.27
[INFO 2017-06-26 22:11:49,922 main.py:47] epoch 2001, training loss: 3302.99, average training loss: 3708.36, base loss: 4610.04
[INFO 2017-06-26 22:11:50,325 main.py:47] epoch 2002, training loss: 3381.68, average training loss: 3708.04, base loss: 4609.87
[INFO 2017-06-26 22:11:50,725 main.py:47] epoch 2003, training loss: 3688.66, average training loss: 3707.88, base loss: 4610.45
[INFO 2017-06-26 22:11:51,132 main.py:47] epoch 2004, training loss: 3819.12, average training loss: 3708.15, base loss: 4611.71
[INFO 2017-06-26 22:11:51,536 main.py:47] epoch 2005, training loss: 3245.78, average training loss: 3707.62, base loss: 4611.49
[INFO 2017-06-26 22:11:51,942 main.py:47] epoch 2006, training loss: 3138.80, average training loss: 3707.26, base loss: 4611.56
[INFO 2017-06-26 22:11:52,345 main.py:47] epoch 2007, training loss: 3379.85, average training loss: 3707.17, base loss: 4611.70
[INFO 2017-06-26 22:11:52,747 main.py:47] epoch 2008, training loss: 3513.16, average training loss: 3706.47, base loss: 4611.29
[INFO 2017-06-26 22:11:53,152 main.py:47] epoch 2009, training loss: 3087.57, average training loss: 3705.98, base loss: 4610.95
[INFO 2017-06-26 22:11:53,555 main.py:47] epoch 2010, training loss: 3321.12, average training loss: 3705.68, base loss: 4610.72
[INFO 2017-06-26 22:11:53,966 main.py:47] epoch 2011, training loss: 3285.84, average training loss: 3705.22, base loss: 4610.41
[INFO 2017-06-26 22:11:54,369 main.py:47] epoch 2012, training loss: 3453.45, average training loss: 3705.33, base loss: 4610.75
[INFO 2017-06-26 22:11:54,774 main.py:47] epoch 2013, training loss: 3706.14, average training loss: 3704.59, base loss: 4610.37
[INFO 2017-06-26 22:11:55,177 main.py:47] epoch 2014, training loss: 3233.58, average training loss: 3703.85, base loss: 4609.76
[INFO 2017-06-26 22:11:55,582 main.py:47] epoch 2015, training loss: 3715.31, average training loss: 3704.18, base loss: 4610.53
[INFO 2017-06-26 22:11:56,020 main.py:47] epoch 2016, training loss: 3167.12, average training loss: 3700.16, base loss: 4606.47
[INFO 2017-06-26 22:11:56,435 main.py:47] epoch 2017, training loss: 3090.36, average training loss: 3699.64, base loss: 4605.94
[INFO 2017-06-26 22:11:56,850 main.py:47] epoch 2018, training loss: 3401.94, average training loss: 3699.82, base loss: 4606.70
[INFO 2017-06-26 22:11:57,258 main.py:47] epoch 2019, training loss: 3079.14, average training loss: 3696.18, base loss: 4603.67
[INFO 2017-06-26 22:11:57,673 main.py:47] epoch 2020, training loss: 3544.57, average training loss: 3695.61, base loss: 4603.25
[INFO 2017-06-26 22:11:58,079 main.py:47] epoch 2021, training loss: 7290.26, average training loss: 3698.75, base loss: 4606.71
[INFO 2017-06-26 22:11:58,502 main.py:47] epoch 2022, training loss: 3921.25, average training loss: 3699.28, base loss: 4608.19
[INFO 2017-06-26 22:11:58,918 main.py:47] epoch 2023, training loss: 2970.60, average training loss: 3698.87, base loss: 4607.69
[INFO 2017-06-26 22:11:59,326 main.py:47] epoch 2024, training loss: 3653.94, average training loss: 3698.43, base loss: 4607.63
[INFO 2017-06-26 22:11:59,740 main.py:47] epoch 2025, training loss: 3555.59, average training loss: 3698.22, base loss: 4607.50
[INFO 2017-06-26 22:12:00,198 main.py:47] epoch 2026, training loss: 3013.93, average training loss: 3697.54, base loss: 4606.86
[INFO 2017-06-26 22:12:00,680 main.py:47] epoch 2027, training loss: 3489.54, average training loss: 3697.02, base loss: 4606.72
[INFO 2017-06-26 22:12:01,165 main.py:47] epoch 2028, training loss: 3810.65, average training loss: 3697.21, base loss: 4607.43
[INFO 2017-06-26 22:12:01,632 main.py:47] epoch 2029, training loss: 2948.50, average training loss: 3696.42, base loss: 4606.73
[INFO 2017-06-26 22:12:02,119 main.py:47] epoch 2030, training loss: 4351.76, average training loss: 3697.13, base loss: 4608.15
[INFO 2017-06-26 22:12:02,545 main.py:47] epoch 2031, training loss: 3427.37, average training loss: 3696.68, base loss: 4607.76
[INFO 2017-06-26 22:12:03,040 main.py:47] epoch 2032, training loss: 3733.89, average training loss: 3696.84, base loss: 4608.71
[INFO 2017-06-26 22:12:03,520 main.py:47] epoch 2033, training loss: 3697.48, average training loss: 3696.63, base loss: 4609.16
[INFO 2017-06-26 22:12:03,948 main.py:47] epoch 2034, training loss: 3424.87, average training loss: 3696.26, base loss: 4608.77
[INFO 2017-06-26 22:12:04,353 main.py:47] epoch 2035, training loss: 3134.69, average training loss: 3695.99, base loss: 4608.85
[INFO 2017-06-26 22:12:04,758 main.py:47] epoch 2036, training loss: 3468.85, average training loss: 3695.92, base loss: 4609.49
[INFO 2017-06-26 22:12:05,196 main.py:47] epoch 2037, training loss: 3137.05, average training loss: 3695.47, base loss: 4609.11
[INFO 2017-06-26 22:12:05,602 main.py:47] epoch 2038, training loss: 3336.33, average training loss: 3695.33, base loss: 4608.77
[INFO 2017-06-26 22:12:06,007 main.py:47] epoch 2039, training loss: 3647.53, average training loss: 3695.12, base loss: 4608.76
[INFO 2017-06-26 22:12:06,418 main.py:47] epoch 2040, training loss: 3416.99, average training loss: 3695.30, base loss: 4609.13
[INFO 2017-06-26 22:12:06,823 main.py:47] epoch 2041, training loss: 3263.74, average training loss: 3694.85, base loss: 4608.47
[INFO 2017-06-26 22:12:07,266 main.py:47] epoch 2042, training loss: 3826.33, average training loss: 3695.31, base loss: 4609.43
[INFO 2017-06-26 22:12:07,704 main.py:47] epoch 2043, training loss: 3505.60, average training loss: 3695.25, base loss: 4609.73
[INFO 2017-06-26 22:12:08,109 main.py:47] epoch 2044, training loss: 3468.88, average training loss: 3695.29, base loss: 4610.13
[INFO 2017-06-26 22:12:08,617 main.py:47] epoch 2045, training loss: 3293.57, average training loss: 3694.98, base loss: 4610.20
[INFO 2017-06-26 22:12:09,031 main.py:47] epoch 2046, training loss: 3496.16, average training loss: 3695.19, base loss: 4611.21
[INFO 2017-06-26 22:12:09,454 main.py:47] epoch 2047, training loss: 3903.10, average training loss: 3695.87, base loss: 4612.84
[INFO 2017-06-26 22:12:09,859 main.py:47] epoch 2048, training loss: 3238.95, average training loss: 3695.92, base loss: 4613.22
[INFO 2017-06-26 22:12:10,286 main.py:47] epoch 2049, training loss: 3290.48, average training loss: 3691.78, base loss: 4609.30
[INFO 2017-06-26 22:12:10,687 main.py:47] epoch 2050, training loss: 3233.46, average training loss: 3691.44, base loss: 4609.21
[INFO 2017-06-26 22:12:11,088 main.py:47] epoch 2051, training loss: 3527.44, average training loss: 3691.44, base loss: 4609.38
[INFO 2017-06-26 22:12:11,490 main.py:47] epoch 2052, training loss: 3515.48, average training loss: 3690.93, base loss: 4609.67
[INFO 2017-06-26 22:12:11,897 main.py:47] epoch 2053, training loss: 3189.06, average training loss: 3690.59, base loss: 4609.37
[INFO 2017-06-26 22:12:12,308 main.py:47] epoch 2054, training loss: 3479.59, average training loss: 3690.50, base loss: 4609.93
[INFO 2017-06-26 22:12:12,715 main.py:47] epoch 2055, training loss: 3398.74, average training loss: 3689.77, base loss: 4609.14
[INFO 2017-06-26 22:12:13,118 main.py:47] epoch 2056, training loss: 3124.50, average training loss: 3689.08, base loss: 4608.60
[INFO 2017-06-26 22:12:13,520 main.py:47] epoch 2057, training loss: 3634.70, average training loss: 3688.54, base loss: 4608.16
[INFO 2017-06-26 22:12:13,927 main.py:47] epoch 2058, training loss: 3614.69, average training loss: 3689.09, base loss: 4609.66
[INFO 2017-06-26 22:12:14,327 main.py:47] epoch 2059, training loss: 3142.12, average training loss: 3688.59, base loss: 4608.57
[INFO 2017-06-26 22:12:14,730 main.py:47] epoch 2060, training loss: 3204.25, average training loss: 3688.20, base loss: 4608.25
[INFO 2017-06-26 22:12:15,135 main.py:47] epoch 2061, training loss: 3745.51, average training loss: 3688.66, base loss: 4609.48
[INFO 2017-06-26 22:12:15,544 main.py:47] epoch 2062, training loss: 2835.02, average training loss: 3687.89, base loss: 4608.57
[INFO 2017-06-26 22:12:15,949 main.py:47] epoch 2063, training loss: 3654.24, average training loss: 3688.02, base loss: 4609.17
[INFO 2017-06-26 22:12:16,358 main.py:47] epoch 2064, training loss: 3608.93, average training loss: 3687.35, base loss: 4608.68
[INFO 2017-06-26 22:12:16,761 main.py:47] epoch 2065, training loss: 3318.85, average training loss: 3687.01, base loss: 4608.77
[INFO 2017-06-26 22:12:17,158 main.py:47] epoch 2066, training loss: 3250.18, average training loss: 3686.70, base loss: 4608.90
[INFO 2017-06-26 22:12:17,563 main.py:47] epoch 2067, training loss: 3390.19, average training loss: 3686.45, base loss: 4608.82
[INFO 2017-06-26 22:12:17,967 main.py:47] epoch 2068, training loss: 3241.09, average training loss: 3685.61, base loss: 4608.17
[INFO 2017-06-26 22:12:18,371 main.py:47] epoch 2069, training loss: 3453.43, average training loss: 3686.16, base loss: 4609.28
[INFO 2017-06-26 22:12:18,770 main.py:47] epoch 2070, training loss: 3038.78, average training loss: 3685.95, base loss: 4609.54
[INFO 2017-06-26 22:12:19,167 main.py:47] epoch 2071, training loss: 3420.36, average training loss: 3685.82, base loss: 4609.48
[INFO 2017-06-26 22:12:19,573 main.py:47] epoch 2072, training loss: 3179.67, average training loss: 3684.82, base loss: 4608.50
[INFO 2017-06-26 22:12:19,977 main.py:47] epoch 2073, training loss: 3367.49, average training loss: 3684.51, base loss: 4608.14
[INFO 2017-06-26 22:12:20,374 main.py:47] epoch 2074, training loss: 3366.02, average training loss: 3684.30, base loss: 4608.41
[INFO 2017-06-26 22:12:20,785 main.py:47] epoch 2075, training loss: 3422.64, average training loss: 3683.67, base loss: 4608.09
[INFO 2017-06-26 22:12:21,189 main.py:47] epoch 2076, training loss: 3700.10, average training loss: 3683.93, base loss: 4608.86
[INFO 2017-06-26 22:12:21,593 main.py:47] epoch 2077, training loss: 3302.09, average training loss: 3683.37, base loss: 4608.09
[INFO 2017-06-26 22:12:22,003 main.py:47] epoch 2078, training loss: 3225.55, average training loss: 3682.96, base loss: 4607.91
[INFO 2017-06-26 22:12:22,405 main.py:47] epoch 2079, training loss: 3446.30, average training loss: 3682.83, base loss: 4608.56
[INFO 2017-06-26 22:12:22,806 main.py:47] epoch 2080, training loss: 3006.84, average training loss: 3681.84, base loss: 4607.75
[INFO 2017-06-26 22:12:23,206 main.py:47] epoch 2081, training loss: 3691.46, average training loss: 3682.09, base loss: 4608.66
[INFO 2017-06-26 22:12:23,606 main.py:47] epoch 2082, training loss: 3804.48, average training loss: 3682.53, base loss: 4610.00
[INFO 2017-06-26 22:12:24,003 main.py:47] epoch 2083, training loss: 3165.17, average training loss: 3681.84, base loss: 4609.32
[INFO 2017-06-26 22:12:24,408 main.py:47] epoch 2084, training loss: 3258.87, average training loss: 3681.36, base loss: 4609.06
[INFO 2017-06-26 22:12:24,808 main.py:47] epoch 2085, training loss: 6948.29, average training loss: 3684.71, base loss: 4612.97
[INFO 2017-06-26 22:12:25,210 main.py:47] epoch 2086, training loss: 3255.15, average training loss: 3683.73, base loss: 4612.11
[INFO 2017-06-26 22:12:25,610 main.py:47] epoch 2087, training loss: 3279.32, average training loss: 3683.28, base loss: 4611.51
[INFO 2017-06-26 22:12:26,021 main.py:47] epoch 2088, training loss: 2823.46, average training loss: 3682.35, base loss: 4610.68
[INFO 2017-06-26 22:12:26,425 main.py:47] epoch 2089, training loss: 3063.22, average training loss: 3682.01, base loss: 4610.73
[INFO 2017-06-26 22:12:26,833 main.py:47] epoch 2090, training loss: 6881.98, average training loss: 3684.92, base loss: 4613.79
[INFO 2017-06-26 22:12:27,243 main.py:47] epoch 2091, training loss: 3353.09, average training loss: 3684.67, base loss: 4613.67
[INFO 2017-06-26 22:12:27,645 main.py:47] epoch 2092, training loss: 2957.75, average training loss: 3683.48, base loss: 4612.31
[INFO 2017-06-26 22:12:28,056 main.py:47] epoch 2093, training loss: 3513.33, average training loss: 3682.81, base loss: 4611.66
[INFO 2017-06-26 22:12:28,467 main.py:47] epoch 2094, training loss: 3683.59, average training loss: 3682.77, base loss: 4612.08
[INFO 2017-06-26 22:12:28,872 main.py:47] epoch 2095, training loss: 3480.73, average training loss: 3683.02, base loss: 4612.73
[INFO 2017-06-26 22:12:29,276 main.py:47] epoch 2096, training loss: 3418.25, average training loss: 3683.02, base loss: 4613.16
[INFO 2017-06-26 22:12:29,684 main.py:47] epoch 2097, training loss: 3067.22, average training loss: 3682.44, base loss: 4612.61
[INFO 2017-06-26 22:12:30,084 main.py:47] epoch 2098, training loss: 3498.86, average training loss: 3682.07, base loss: 4612.58
[INFO 2017-06-26 22:12:30,484 main.py:47] epoch 2099, training loss: 3136.38, average training loss: 3681.33, base loss: 4612.17
[INFO 2017-06-26 22:12:30,484 main.py:49] epoch 2099, testing
[INFO 2017-06-26 22:12:32,144 main.py:102] average testing loss: 3584.10, base loss: 4806.61
[INFO 2017-06-26 22:12:32,144 main.py:103] improve_loss: 1222.52, improve_percent: 0.25
[INFO 2017-06-26 22:12:32,144 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 22:12:32,157 main.py:73] current best improved percent: 0.25
[INFO 2017-06-26 22:12:32,565 main.py:47] epoch 2100, training loss: 3500.94, average training loss: 3681.17, base loss: 4612.52
[INFO 2017-06-26 22:12:32,972 main.py:47] epoch 2101, training loss: 3228.13, average training loss: 3680.55, base loss: 4612.24
[INFO 2017-06-26 22:12:33,377 main.py:47] epoch 2102, training loss: 3545.64, average training loss: 3680.56, base loss: 4612.71
[INFO 2017-06-26 22:12:33,781 main.py:47] epoch 2103, training loss: 3483.34, average training loss: 3680.40, base loss: 4612.88
[INFO 2017-06-26 22:12:34,186 main.py:47] epoch 2104, training loss: 3203.62, average training loss: 3680.37, base loss: 4613.08
[INFO 2017-06-26 22:12:34,589 main.py:47] epoch 2105, training loss: 7126.99, average training loss: 3683.43, base loss: 4616.08
[INFO 2017-06-26 22:12:34,993 main.py:47] epoch 2106, training loss: 3320.68, average training loss: 3683.75, base loss: 4617.32
[INFO 2017-06-26 22:12:35,395 main.py:47] epoch 2107, training loss: 3204.57, average training loss: 3683.68, base loss: 4617.90
[INFO 2017-06-26 22:12:35,798 main.py:47] epoch 2108, training loss: 3426.92, average training loss: 3683.97, base loss: 4618.87
[INFO 2017-06-26 22:12:36,202 main.py:47] epoch 2109, training loss: 3475.49, average training loss: 3684.33, base loss: 4619.47
[INFO 2017-06-26 22:12:36,604 main.py:47] epoch 2110, training loss: 3361.15, average training loss: 3684.35, base loss: 4619.87
[INFO 2017-06-26 22:12:37,007 main.py:47] epoch 2111, training loss: 3576.69, average training loss: 3683.96, base loss: 4620.19
[INFO 2017-06-26 22:12:37,409 main.py:47] epoch 2112, training loss: 3143.09, average training loss: 3683.36, base loss: 4619.48
[INFO 2017-06-26 22:12:37,809 main.py:47] epoch 2113, training loss: 3617.97, average training loss: 3682.90, base loss: 4619.40
[INFO 2017-06-26 22:12:38,212 main.py:47] epoch 2114, training loss: 3498.93, average training loss: 3682.89, base loss: 4619.74
[INFO 2017-06-26 22:12:38,618 main.py:47] epoch 2115, training loss: 3344.95, average training loss: 3682.81, base loss: 4620.05
[INFO 2017-06-26 22:12:39,024 main.py:47] epoch 2116, training loss: 3306.29, average training loss: 3682.53, base loss: 4619.73
[INFO 2017-06-26 22:12:39,436 main.py:47] epoch 2117, training loss: 6845.65, average training loss: 3685.85, base loss: 4623.24
[INFO 2017-06-26 22:12:39,839 main.py:47] epoch 2118, training loss: 6961.96, average training loss: 3688.58, base loss: 4626.31
[INFO 2017-06-26 22:12:40,242 main.py:47] epoch 2119, training loss: 3615.71, average training loss: 3688.45, base loss: 4626.43
[INFO 2017-06-26 22:12:40,638 main.py:47] epoch 2120, training loss: 3448.11, average training loss: 3688.55, base loss: 4627.01
[INFO 2017-06-26 22:12:41,042 main.py:47] epoch 2121, training loss: 3406.77, average training loss: 3687.80, base loss: 4626.28
[INFO 2017-06-26 22:12:41,442 main.py:47] epoch 2122, training loss: 3075.81, average training loss: 3687.48, base loss: 4626.03
[INFO 2017-06-26 22:12:41,845 main.py:47] epoch 2123, training loss: 3486.56, average training loss: 3687.13, base loss: 4626.13
[INFO 2017-06-26 22:12:42,244 main.py:47] epoch 2124, training loss: 3402.60, average training loss: 3687.38, base loss: 4626.80
[INFO 2017-06-26 22:12:42,648 main.py:47] epoch 2125, training loss: 3237.30, average training loss: 3687.10, base loss: 4626.80
[INFO 2017-06-26 22:12:43,050 main.py:47] epoch 2126, training loss: 3245.22, average training loss: 3687.05, base loss: 4627.33
[INFO 2017-06-26 22:12:43,449 main.py:47] epoch 2127, training loss: 2844.67, average training loss: 3686.56, base loss: 4626.65
[INFO 2017-06-26 22:12:43,851 main.py:47] epoch 2128, training loss: 3356.11, average training loss: 3686.11, base loss: 4626.55
[INFO 2017-06-26 22:12:44,255 main.py:47] epoch 2129, training loss: 7473.29, average training loss: 3690.07, base loss: 4631.35
[INFO 2017-06-26 22:12:44,654 main.py:47] epoch 2130, training loss: 3254.02, average training loss: 3689.58, base loss: 4631.31
[INFO 2017-06-26 22:12:45,059 main.py:47] epoch 2131, training loss: 3442.48, average training loss: 3689.27, base loss: 4631.46
[INFO 2017-06-26 22:12:45,458 main.py:47] epoch 2132, training loss: 3487.91, average training loss: 3689.24, base loss: 4631.61
[INFO 2017-06-26 22:12:45,859 main.py:47] epoch 2133, training loss: 3403.89, average training loss: 3689.28, base loss: 4631.92
[INFO 2017-06-26 22:12:46,261 main.py:47] epoch 2134, training loss: 3749.81, average training loss: 3689.49, base loss: 4632.79
[INFO 2017-06-26 22:12:46,665 main.py:47] epoch 2135, training loss: 3087.08, average training loss: 3688.31, base loss: 4631.33
[INFO 2017-06-26 22:12:47,073 main.py:47] epoch 2136, training loss: 3743.15, average training loss: 3688.87, base loss: 4632.98
[INFO 2017-06-26 22:12:47,476 main.py:47] epoch 2137, training loss: 3589.63, average training loss: 3688.45, base loss: 4633.00
[INFO 2017-06-26 22:12:47,876 main.py:47] epoch 2138, training loss: 7577.80, average training loss: 3692.39, base loss: 4637.56
[INFO 2017-06-26 22:12:48,276 main.py:47] epoch 2139, training loss: 3515.95, average training loss: 3692.37, base loss: 4638.11
[INFO 2017-06-26 22:12:48,675 main.py:47] epoch 2140, training loss: 3290.59, average training loss: 3691.43, base loss: 4637.35
[INFO 2017-06-26 22:12:49,074 main.py:47] epoch 2141, training loss: 3086.02, average training loss: 3689.63, base loss: 4635.31
[INFO 2017-06-26 22:12:49,477 main.py:47] epoch 2142, training loss: 3436.41, average training loss: 3689.57, base loss: 4635.60
[INFO 2017-06-26 22:12:49,879 main.py:47] epoch 2143, training loss: 3473.81, average training loss: 3689.18, base loss: 4635.45
[INFO 2017-06-26 22:12:50,280 main.py:47] epoch 2144, training loss: 3036.34, average training loss: 3688.72, base loss: 4634.95
[INFO 2017-06-26 22:12:50,679 main.py:47] epoch 2145, training loss: 3261.94, average training loss: 3687.76, base loss: 4634.08
[INFO 2017-06-26 22:12:51,081 main.py:47] epoch 2146, training loss: 7599.01, average training loss: 3691.32, base loss: 4638.20
[INFO 2017-06-26 22:12:51,483 main.py:47] epoch 2147, training loss: 3121.52, average training loss: 3690.81, base loss: 4637.79
[INFO 2017-06-26 22:12:51,889 main.py:47] epoch 2148, training loss: 3397.42, average training loss: 3691.08, base loss: 4638.64
[INFO 2017-06-26 22:12:52,290 main.py:47] epoch 2149, training loss: 2931.58, average training loss: 3686.66, base loss: 4634.30
[INFO 2017-06-26 22:12:52,695 main.py:47] epoch 2150, training loss: 2981.29, average training loss: 3685.82, base loss: 4633.49
[INFO 2017-06-26 22:12:53,100 main.py:47] epoch 2151, training loss: 2690.60, average training loss: 3684.90, base loss: 4632.38
[INFO 2017-06-26 22:12:53,503 main.py:47] epoch 2152, training loss: 3355.99, average training loss: 3684.40, base loss: 4632.02
[INFO 2017-06-26 22:12:53,905 main.py:47] epoch 2153, training loss: 3382.89, average training loss: 3684.21, base loss: 4632.14
[INFO 2017-06-26 22:12:54,309 main.py:47] epoch 2154, training loss: 3507.54, average training loss: 3683.77, base loss: 4631.70
[INFO 2017-06-26 22:12:54,714 main.py:47] epoch 2155, training loss: 3552.64, average training loss: 3682.89, base loss: 4630.75
[INFO 2017-06-26 22:12:55,120 main.py:47] epoch 2156, training loss: 6894.61, average training loss: 3685.99, base loss: 4634.21
[INFO 2017-06-26 22:12:55,522 main.py:47] epoch 2157, training loss: 3481.68, average training loss: 3685.22, base loss: 4633.57
[INFO 2017-06-26 22:12:55,924 main.py:47] epoch 2158, training loss: 3399.23, average training loss: 3684.95, base loss: 4633.65
[INFO 2017-06-26 22:12:56,328 main.py:47] epoch 2159, training loss: 3820.84, average training loss: 3685.58, base loss: 4635.04
[INFO 2017-06-26 22:12:56,728 main.py:47] epoch 2160, training loss: 2877.31, average training loss: 3684.82, base loss: 4634.21
[INFO 2017-06-26 22:12:57,126 main.py:47] epoch 2161, training loss: 3036.84, average training loss: 3684.32, base loss: 4633.77
[INFO 2017-06-26 22:12:57,524 main.py:47] epoch 2162, training loss: 3088.09, average training loss: 3683.79, base loss: 4633.51
[INFO 2017-06-26 22:12:57,924 main.py:47] epoch 2163, training loss: 3746.02, average training loss: 3683.23, base loss: 4633.58
[INFO 2017-06-26 22:12:58,330 main.py:47] epoch 2164, training loss: 3249.93, average training loss: 3682.69, base loss: 4632.87
[INFO 2017-06-26 22:12:58,732 main.py:47] epoch 2165, training loss: 3318.72, average training loss: 3682.08, base loss: 4632.39
[INFO 2017-06-26 22:12:59,132 main.py:47] epoch 2166, training loss: 3186.81, average training loss: 3680.84, base loss: 4630.90
[INFO 2017-06-26 22:12:59,534 main.py:47] epoch 2167, training loss: 3329.17, average training loss: 3680.81, base loss: 4631.14
[INFO 2017-06-26 22:12:59,935 main.py:47] epoch 2168, training loss: 3811.45, average training loss: 3681.19, base loss: 4632.41
[INFO 2017-06-26 22:13:00,335 main.py:47] epoch 2169, training loss: 3454.92, average training loss: 3681.44, base loss: 4633.15
[INFO 2017-06-26 22:13:00,735 main.py:47] epoch 2170, training loss: 3646.83, average training loss: 3680.56, base loss: 4632.36
[INFO 2017-06-26 22:13:01,144 main.py:47] epoch 2171, training loss: 3336.80, average training loss: 3677.43, base loss: 4629.76
[INFO 2017-06-26 22:13:01,546 main.py:47] epoch 2172, training loss: 3712.79, average training loss: 3677.32, base loss: 4629.78
[INFO 2017-06-26 22:13:01,947 main.py:47] epoch 2173, training loss: 3525.43, average training loss: 3677.08, base loss: 4629.89
[INFO 2017-06-26 22:13:02,346 main.py:47] epoch 2174, training loss: 3532.80, average training loss: 3676.54, base loss: 4629.93
[INFO 2017-06-26 22:13:02,744 main.py:47] epoch 2175, training loss: 3141.71, average training loss: 3676.23, base loss: 4629.97
[INFO 2017-06-26 22:13:03,146 main.py:47] epoch 2176, training loss: 3229.67, average training loss: 3675.74, base loss: 4629.97
[INFO 2017-06-26 22:13:03,544 main.py:47] epoch 2177, training loss: 3318.27, average training loss: 3675.59, base loss: 4630.45
[INFO 2017-06-26 22:13:03,942 main.py:47] epoch 2178, training loss: 3466.21, average training loss: 3675.47, base loss: 4630.57
[INFO 2017-06-26 22:13:04,344 main.py:47] epoch 2179, training loss: 3415.39, average training loss: 3674.87, base loss: 4630.05
[INFO 2017-06-26 22:13:04,742 main.py:47] epoch 2180, training loss: 3234.62, average training loss: 3674.35, base loss: 4629.53
[INFO 2017-06-26 22:13:05,140 main.py:47] epoch 2181, training loss: 3648.24, average training loss: 3674.37, base loss: 4630.07
[INFO 2017-06-26 22:13:05,539 main.py:47] epoch 2182, training loss: 3684.53, average training loss: 3673.89, base loss: 4630.22
[INFO 2017-06-26 22:13:05,942 main.py:47] epoch 2183, training loss: 3508.46, average training loss: 3673.93, base loss: 4630.73
[INFO 2017-06-26 22:13:06,345 main.py:47] epoch 2184, training loss: 3316.95, average training loss: 3673.21, base loss: 4630.10
[INFO 2017-06-26 22:13:06,747 main.py:47] epoch 2185, training loss: 3267.94, average training loss: 3669.51, base loss: 4626.93
[INFO 2017-06-26 22:13:07,150 main.py:47] epoch 2186, training loss: 3302.16, average training loss: 3669.59, base loss: 4627.48
[INFO 2017-06-26 22:13:07,552 main.py:47] epoch 2187, training loss: 3381.57, average training loss: 3669.41, base loss: 4627.74
[INFO 2017-06-26 22:13:07,951 main.py:47] epoch 2188, training loss: 3040.37, average training loss: 3669.08, base loss: 4627.83
[INFO 2017-06-26 22:13:08,353 main.py:47] epoch 2189, training loss: 3441.34, average training loss: 3668.74, base loss: 4627.72
[INFO 2017-06-26 22:13:08,751 main.py:47] epoch 2190, training loss: 3313.38, average training loss: 3668.27, base loss: 4627.36
[INFO 2017-06-26 22:13:09,151 main.py:47] epoch 2191, training loss: 3610.47, average training loss: 3667.82, base loss: 4627.05
[INFO 2017-06-26 22:13:09,550 main.py:47] epoch 2192, training loss: 2954.71, average training loss: 3667.73, base loss: 4627.58
[INFO 2017-06-26 22:13:09,958 main.py:47] epoch 2193, training loss: 3280.06, average training loss: 3667.72, base loss: 4628.08
[INFO 2017-06-26 22:13:10,360 main.py:47] epoch 2194, training loss: 2904.72, average training loss: 3667.68, base loss: 4628.43
[INFO 2017-06-26 22:13:10,759 main.py:47] epoch 2195, training loss: 3088.83, average training loss: 3666.60, base loss: 4627.43
[INFO 2017-06-26 22:13:11,162 main.py:47] epoch 2196, training loss: 3287.22, average training loss: 3666.10, base loss: 4627.08
[INFO 2017-06-26 22:13:11,564 main.py:47] epoch 2197, training loss: 3035.38, average training loss: 3665.12, base loss: 4625.80
[INFO 2017-06-26 22:13:11,964 main.py:47] epoch 2198, training loss: 3452.05, average training loss: 3664.98, base loss: 4625.92
[INFO 2017-06-26 22:13:12,366 main.py:47] epoch 2199, training loss: 2925.50, average training loss: 3664.25, base loss: 4625.20
[INFO 2017-06-26 22:13:12,366 main.py:49] epoch 2199, testing
[INFO 2017-06-26 22:13:14,032 main.py:102] average testing loss: 3372.84, base loss: 4429.37
[INFO 2017-06-26 22:13:14,033 main.py:103] improve_loss: 1056.53, improve_percent: 0.24
[INFO 2017-06-26 22:13:14,033 main.py:73] current best improved percent: 0.25
[INFO 2017-06-26 22:13:14,446 main.py:47] epoch 2200, training loss: 3614.22, average training loss: 3664.22, base loss: 4625.86
[INFO 2017-06-26 22:13:14,844 main.py:47] epoch 2201, training loss: 3216.41, average training loss: 3663.44, base loss: 4624.87
[INFO 2017-06-26 22:13:15,245 main.py:47] epoch 2202, training loss: 3294.30, average training loss: 3663.37, base loss: 4625.25
[INFO 2017-06-26 22:13:15,647 main.py:47] epoch 2203, training loss: 3244.80, average training loss: 3663.32, base loss: 4625.46
[INFO 2017-06-26 22:13:16,049 main.py:47] epoch 2204, training loss: 3555.39, average training loss: 3663.29, base loss: 4625.53
[INFO 2017-06-26 22:13:16,449 main.py:47] epoch 2205, training loss: 3706.91, average training loss: 3663.67, base loss: 4626.63
[INFO 2017-06-26 22:13:16,853 main.py:47] epoch 2206, training loss: 3176.83, average training loss: 3663.44, base loss: 4626.49
[INFO 2017-06-26 22:13:17,255 main.py:47] epoch 2207, training loss: 3430.60, average training loss: 3663.17, base loss: 4626.53
[INFO 2017-06-26 22:13:17,654 main.py:47] epoch 2208, training loss: 3146.29, average training loss: 3662.77, base loss: 4626.11
[INFO 2017-06-26 22:13:18,057 main.py:47] epoch 2209, training loss: 3308.65, average training loss: 3662.24, base loss: 4625.30
[INFO 2017-06-26 22:13:18,461 main.py:47] epoch 2210, training loss: 3260.18, average training loss: 3662.12, base loss: 4625.53
[INFO 2017-06-26 22:13:18,864 main.py:47] epoch 2211, training loss: 3079.98, average training loss: 3661.80, base loss: 4625.14
[INFO 2017-06-26 22:13:19,266 main.py:47] epoch 2212, training loss: 3659.39, average training loss: 3661.63, base loss: 4625.16
[INFO 2017-06-26 22:13:19,668 main.py:47] epoch 2213, training loss: 3207.78, average training loss: 3660.72, base loss: 4624.28
[INFO 2017-06-26 22:13:20,070 main.py:47] epoch 2214, training loss: 2921.45, average training loss: 3659.80, base loss: 4623.33
[INFO 2017-06-26 22:13:20,469 main.py:47] epoch 2215, training loss: 3155.20, average training loss: 3659.68, base loss: 4623.31
[INFO 2017-06-26 22:13:20,867 main.py:47] epoch 2216, training loss: 3293.88, average training loss: 3659.53, base loss: 4623.46
[INFO 2017-06-26 22:13:21,271 main.py:47] epoch 2217, training loss: 3073.98, average training loss: 3659.14, base loss: 4623.21
[INFO 2017-06-26 22:13:21,674 main.py:47] epoch 2218, training loss: 4086.08, average training loss: 3658.82, base loss: 4623.32
[INFO 2017-06-26 22:13:22,083 main.py:47] epoch 2219, training loss: 3566.35, average training loss: 3655.22, base loss: 4620.32
[INFO 2017-06-26 22:13:22,487 main.py:47] epoch 2220, training loss: 3747.25, average training loss: 3654.90, base loss: 4620.33
[INFO 2017-06-26 22:13:22,887 main.py:47] epoch 2221, training loss: 3163.84, average training loss: 3654.66, base loss: 4620.44
[INFO 2017-06-26 22:13:23,286 main.py:47] epoch 2222, training loss: 3638.48, average training loss: 3650.86, base loss: 4616.48
[INFO 2017-06-26 22:13:23,690 main.py:47] epoch 2223, training loss: 3267.55, average training loss: 3650.73, base loss: 4616.84
[INFO 2017-06-26 22:13:24,090 main.py:47] epoch 2224, training loss: 3622.27, average training loss: 3650.79, base loss: 4617.49
[INFO 2017-06-26 22:13:24,494 main.py:47] epoch 2225, training loss: 3538.22, average training loss: 3650.74, base loss: 4617.78
[INFO 2017-06-26 22:13:24,898 main.py:47] epoch 2226, training loss: 3661.02, average training loss: 3650.60, base loss: 4617.72
[INFO 2017-06-26 22:13:25,300 main.py:47] epoch 2227, training loss: 3735.65, average training loss: 3650.07, base loss: 4617.40
[INFO 2017-06-26 22:13:25,697 main.py:47] epoch 2228, training loss: 3557.57, average training loss: 3650.30, base loss: 4618.18
[INFO 2017-06-26 22:13:26,103 main.py:47] epoch 2229, training loss: 3248.42, average training loss: 3649.82, base loss: 4617.88
[INFO 2017-06-26 22:13:26,503 main.py:47] epoch 2230, training loss: 3368.17, average training loss: 3649.74, base loss: 4618.38
[INFO 2017-06-26 22:13:26,908 main.py:47] epoch 2231, training loss: 3523.49, average training loss: 3649.62, base loss: 4618.46
[INFO 2017-06-26 22:13:27,311 main.py:47] epoch 2232, training loss: 3615.28, average training loss: 3649.80, base loss: 4619.07
[INFO 2017-06-26 22:13:27,710 main.py:47] epoch 2233, training loss: 3127.51, average training loss: 3649.09, base loss: 4618.21
[INFO 2017-06-26 22:13:28,110 main.py:47] epoch 2234, training loss: 3644.59, average training loss: 3649.03, base loss: 4618.60
[INFO 2017-06-26 22:13:28,515 main.py:47] epoch 2235, training loss: 3268.01, average training loss: 3648.26, base loss: 4618.07
[INFO 2017-06-26 22:13:28,917 main.py:47] epoch 2236, training loss: 3191.59, average training loss: 3647.96, base loss: 4617.65
[INFO 2017-06-26 22:13:29,322 main.py:47] epoch 2237, training loss: 3495.98, average training loss: 3647.64, base loss: 4617.37
[INFO 2017-06-26 22:13:29,720 main.py:47] epoch 2238, training loss: 3107.30, average training loss: 3646.83, base loss: 4616.55
[INFO 2017-06-26 22:13:30,128 main.py:47] epoch 2239, training loss: 3178.43, average training loss: 3646.34, base loss: 4616.33
[INFO 2017-06-26 22:13:30,541 main.py:47] epoch 2240, training loss: 3238.11, average training loss: 3645.67, base loss: 4615.70
[INFO 2017-06-26 22:13:30,948 main.py:47] epoch 2241, training loss: 3481.22, average training loss: 3645.56, base loss: 4615.80
[INFO 2017-06-26 22:13:31,353 main.py:47] epoch 2242, training loss: 3298.18, average training loss: 3645.23, base loss: 4615.90
[INFO 2017-06-26 22:13:31,757 main.py:47] epoch 2243, training loss: 3280.62, average training loss: 3644.84, base loss: 4615.68
[INFO 2017-06-26 22:13:32,162 main.py:47] epoch 2244, training loss: 3526.78, average training loss: 3644.19, base loss: 4614.99
[INFO 2017-06-26 22:13:32,572 main.py:47] epoch 2245, training loss: 3130.19, average training loss: 3643.79, base loss: 4614.53
[INFO 2017-06-26 22:13:32,985 main.py:47] epoch 2246, training loss: 3401.48, average training loss: 3639.77, base loss: 4611.12
[INFO 2017-06-26 22:13:33,390 main.py:47] epoch 2247, training loss: 3551.01, average training loss: 3640.10, base loss: 4612.07
[INFO 2017-06-26 22:13:33,795 main.py:47] epoch 2248, training loss: 3429.38, average training loss: 3639.77, base loss: 4611.92
[INFO 2017-06-26 22:13:34,205 main.py:47] epoch 2249, training loss: 3437.37, average training loss: 3639.71, base loss: 4612.25
[INFO 2017-06-26 22:13:34,610 main.py:47] epoch 2250, training loss: 3528.67, average training loss: 3639.74, base loss: 4613.06
[INFO 2017-06-26 22:13:35,022 main.py:47] epoch 2251, training loss: 2841.77, average training loss: 3639.11, base loss: 4612.43
[INFO 2017-06-26 22:13:35,420 main.py:47] epoch 2252, training loss: 6646.31, average training loss: 3642.47, base loss: 4616.03
[INFO 2017-06-26 22:13:35,823 main.py:47] epoch 2253, training loss: 3790.50, average training loss: 3642.79, base loss: 4617.13
[INFO 2017-06-26 22:13:36,228 main.py:47] epoch 2254, training loss: 3698.21, average training loss: 3643.23, base loss: 4618.15
[INFO 2017-06-26 22:13:36,630 main.py:47] epoch 2255, training loss: 3350.14, average training loss: 3642.09, base loss: 4617.36
[INFO 2017-06-26 22:13:37,041 main.py:47] epoch 2256, training loss: 3778.66, average training loss: 3642.57, base loss: 4618.74
[INFO 2017-06-26 22:13:37,444 main.py:47] epoch 2257, training loss: 3388.32, average training loss: 3642.36, base loss: 4618.93
[INFO 2017-06-26 22:13:37,846 main.py:47] epoch 2258, training loss: 3101.58, average training loss: 3641.89, base loss: 4618.58
[INFO 2017-06-26 22:13:38,251 main.py:47] epoch 2259, training loss: 3260.76, average training loss: 3641.20, base loss: 4618.32
[INFO 2017-06-26 22:13:38,656 main.py:47] epoch 2260, training loss: 2958.12, average training loss: 3640.65, base loss: 4617.78
[INFO 2017-06-26 22:13:39,056 main.py:47] epoch 2261, training loss: 3105.47, average training loss: 3639.52, base loss: 4616.73
[INFO 2017-06-26 22:13:39,459 main.py:47] epoch 2262, training loss: 3800.20, average training loss: 3639.49, base loss: 4617.07
[INFO 2017-06-26 22:13:39,868 main.py:47] epoch 2263, training loss: 3178.11, average training loss: 3639.58, base loss: 4617.77
[INFO 2017-06-26 22:13:40,270 main.py:47] epoch 2264, training loss: 3131.82, average training loss: 3638.93, base loss: 4616.78
[INFO 2017-06-26 22:13:40,672 main.py:47] epoch 2265, training loss: 3425.09, average training loss: 3638.80, base loss: 4617.44
[INFO 2017-06-26 22:13:41,078 main.py:47] epoch 2266, training loss: 3427.10, average training loss: 3638.43, base loss: 4617.25
[INFO 2017-06-26 22:13:41,481 main.py:47] epoch 2267, training loss: 3061.87, average training loss: 3638.42, base loss: 4617.68
[INFO 2017-06-26 22:13:41,885 main.py:47] epoch 2268, training loss: 7067.33, average training loss: 3642.36, base loss: 4622.16
[INFO 2017-06-26 22:13:42,287 main.py:47] epoch 2269, training loss: 3275.82, average training loss: 3638.28, base loss: 4618.61
[INFO 2017-06-26 22:13:42,691 main.py:47] epoch 2270, training loss: 3242.95, average training loss: 3638.10, base loss: 4618.96
[INFO 2017-06-26 22:13:43,096 main.py:47] epoch 2271, training loss: 3006.42, average training loss: 3638.08, base loss: 4619.16
[INFO 2017-06-26 22:13:43,501 main.py:47] epoch 2272, training loss: 3170.06, average training loss: 3637.63, base loss: 4619.20
[INFO 2017-06-26 22:13:43,901 main.py:47] epoch 2273, training loss: 7044.64, average training loss: 3641.42, base loss: 4623.72
[INFO 2017-06-26 22:13:44,306 main.py:47] epoch 2274, training loss: 3608.16, average training loss: 3641.97, base loss: 4624.90
[INFO 2017-06-26 22:13:44,704 main.py:47] epoch 2275, training loss: 3604.45, average training loss: 3642.21, base loss: 4625.92
[INFO 2017-06-26 22:13:45,102 main.py:47] epoch 2276, training loss: 3486.38, average training loss: 3638.26, base loss: 4622.33
[INFO 2017-06-26 22:13:45,499 main.py:47] epoch 2277, training loss: 4003.97, average training loss: 3638.83, base loss: 4624.06
[INFO 2017-06-26 22:13:45,900 main.py:47] epoch 2278, training loss: 3134.05, average training loss: 3638.66, base loss: 4623.92
[INFO 2017-06-26 22:13:46,300 main.py:47] epoch 2279, training loss: 2985.14, average training loss: 3638.39, base loss: 4623.87
[INFO 2017-06-26 22:13:46,700 main.py:47] epoch 2280, training loss: 3720.38, average training loss: 3638.92, base loss: 4625.47
[INFO 2017-06-26 22:13:47,098 main.py:47] epoch 2281, training loss: 3228.31, average training loss: 3638.97, base loss: 4625.61
[INFO 2017-06-26 22:13:47,498 main.py:47] epoch 2282, training loss: 3113.67, average training loss: 3638.74, base loss: 4625.11
[INFO 2017-06-26 22:13:48,002 main.py:47] epoch 2283, training loss: 3077.10, average training loss: 3638.31, base loss: 4624.76
[INFO 2017-06-26 22:13:48,410 main.py:47] epoch 2284, training loss: 3389.23, average training loss: 3638.36, base loss: 4625.42
[INFO 2017-06-26 22:13:48,816 main.py:47] epoch 2285, training loss: 3283.18, average training loss: 3637.93, base loss: 4624.99
[INFO 2017-06-26 22:13:49,220 main.py:47] epoch 2286, training loss: 3366.88, average training loss: 3637.45, base loss: 4624.94
[INFO 2017-06-26 22:13:49,706 main.py:47] epoch 2287, training loss: 3067.90, average training loss: 3637.06, base loss: 4624.29
[INFO 2017-06-26 22:13:50,130 main.py:47] epoch 2288, training loss: 3715.97, average training loss: 3637.07, base loss: 4624.49
[INFO 2017-06-26 22:13:50,544 main.py:47] epoch 2289, training loss: 3556.95, average training loss: 3636.87, base loss: 4624.53
[INFO 2017-06-26 22:13:51,008 main.py:47] epoch 2290, training loss: 3297.13, average training loss: 3636.38, base loss: 4624.03
[INFO 2017-06-26 22:13:51,457 main.py:47] epoch 2291, training loss: 3498.99, average training loss: 3636.67, base loss: 4624.63
[INFO 2017-06-26 22:13:51,905 main.py:47] epoch 2292, training loss: 3808.21, average training loss: 3637.61, base loss: 4626.53
[INFO 2017-06-26 22:13:52,389 main.py:47] epoch 2293, training loss: 3608.46, average training loss: 3637.81, base loss: 4627.14
[INFO 2017-06-26 22:13:52,804 main.py:47] epoch 2294, training loss: 3179.88, average training loss: 3637.50, base loss: 4626.86
[INFO 2017-06-26 22:13:53,204 main.py:47] epoch 2295, training loss: 3424.88, average training loss: 3637.37, base loss: 4627.23
[INFO 2017-06-26 22:13:53,603 main.py:47] epoch 2296, training loss: 3593.85, average training loss: 3637.18, base loss: 4627.57
[INFO 2017-06-26 22:13:54,016 main.py:47] epoch 2297, training loss: 3047.73, average training loss: 3636.67, base loss: 4626.85
[INFO 2017-06-26 22:13:54,419 main.py:47] epoch 2298, training loss: 3353.87, average training loss: 3636.10, base loss: 4626.59
[INFO 2017-06-26 22:13:54,821 main.py:47] epoch 2299, training loss: 4024.34, average training loss: 3636.75, base loss: 4628.27
[INFO 2017-06-26 22:13:54,821 main.py:49] epoch 2299, testing
[INFO 2017-06-26 22:13:56,483 main.py:102] average testing loss: 3359.08, base loss: 4417.05
[INFO 2017-06-26 22:13:56,483 main.py:103] improve_loss: 1057.98, improve_percent: 0.24
[INFO 2017-06-26 22:13:56,483 main.py:73] current best improved percent: 0.25
[INFO 2017-06-26 22:13:56,884 main.py:47] epoch 2300, training loss: 3178.01, average training loss: 3632.62, base loss: 4624.26
[INFO 2017-06-26 22:13:57,285 main.py:47] epoch 2301, training loss: 3242.35, average training loss: 3632.53, base loss: 4624.65
[INFO 2017-06-26 22:13:57,683 main.py:47] epoch 2302, training loss: 2954.65, average training loss: 3631.77, base loss: 4623.60
[INFO 2017-06-26 22:13:58,086 main.py:47] epoch 2303, training loss: 3499.77, average training loss: 3632.46, base loss: 4624.84
[INFO 2017-06-26 22:13:58,489 main.py:47] epoch 2304, training loss: 3478.18, average training loss: 3632.54, base loss: 4625.44
[INFO 2017-06-26 22:13:58,893 main.py:47] epoch 2305, training loss: 2680.97, average training loss: 3631.36, base loss: 4624.04
[INFO 2017-06-26 22:13:59,299 main.py:47] epoch 2306, training loss: 3355.81, average training loss: 3630.68, base loss: 4623.25
[INFO 2017-06-26 22:13:59,697 main.py:47] epoch 2307, training loss: 3296.05, average training loss: 3630.52, base loss: 4623.62
[INFO 2017-06-26 22:14:00,097 main.py:47] epoch 2308, training loss: 3400.19, average training loss: 3630.52, base loss: 4624.00
[INFO 2017-06-26 22:14:00,504 main.py:47] epoch 2309, training loss: 3189.35, average training loss: 3629.93, base loss: 4623.50
[INFO 2017-06-26 22:14:00,906 main.py:47] epoch 2310, training loss: 3500.96, average training loss: 3630.06, base loss: 4624.36
[INFO 2017-06-26 22:14:01,309 main.py:47] epoch 2311, training loss: 3784.50, average training loss: 3630.09, base loss: 4624.87
[INFO 2017-06-26 22:14:01,713 main.py:47] epoch 2312, training loss: 2978.10, average training loss: 3629.19, base loss: 4623.52
[INFO 2017-06-26 22:14:02,117 main.py:47] epoch 2313, training loss: 3276.97, average training loss: 3628.91, base loss: 4623.59
[INFO 2017-06-26 22:14:02,520 main.py:47] epoch 2314, training loss: 3311.36, average training loss: 3628.61, base loss: 4623.59
[INFO 2017-06-26 22:14:02,922 main.py:47] epoch 2315, training loss: 3298.13, average training loss: 3628.07, base loss: 4623.12
[INFO 2017-06-26 22:14:03,326 main.py:47] epoch 2316, training loss: 3449.30, average training loss: 3627.82, base loss: 4623.16
[INFO 2017-06-26 22:14:03,729 main.py:47] epoch 2317, training loss: 7061.02, average training loss: 3631.79, base loss: 4627.44
[INFO 2017-06-26 22:14:04,132 main.py:47] epoch 2318, training loss: 3021.41, average training loss: 3631.39, base loss: 4627.24
[INFO 2017-06-26 22:14:04,531 main.py:47] epoch 2319, training loss: 3145.68, average training loss: 3630.73, base loss: 4626.47
[INFO 2017-06-26 22:14:04,935 main.py:47] epoch 2320, training loss: 3666.69, average training loss: 3631.17, base loss: 4627.59
[INFO 2017-06-26 22:14:05,337 main.py:47] epoch 2321, training loss: 10165.47, average training loss: 3637.60, base loss: 4633.89
[INFO 2017-06-26 22:14:05,744 main.py:47] epoch 2322, training loss: 3518.13, average training loss: 3636.65, base loss: 4632.83
[INFO 2017-06-26 22:14:06,144 main.py:47] epoch 2323, training loss: 3462.53, average training loss: 3636.17, base loss: 4632.52
[INFO 2017-06-26 22:14:06,548 main.py:47] epoch 2324, training loss: 3287.22, average training loss: 3636.02, base loss: 4632.53
[INFO 2017-06-26 22:14:06,954 main.py:47] epoch 2325, training loss: 3245.75, average training loss: 3635.21, base loss: 4631.49
[INFO 2017-06-26 22:14:07,353 main.py:47] epoch 2326, training loss: 3021.87, average training loss: 3634.44, base loss: 4630.67
[INFO 2017-06-26 22:14:07,756 main.py:47] epoch 2327, training loss: 3328.12, average training loss: 3634.18, base loss: 4630.36
[INFO 2017-06-26 22:14:08,159 main.py:47] epoch 2328, training loss: 3094.06, average training loss: 3633.90, base loss: 4629.69
[INFO 2017-06-26 22:14:08,562 main.py:47] epoch 2329, training loss: 2949.03, average training loss: 3633.25, base loss: 4628.72
[INFO 2017-06-26 22:14:08,967 main.py:47] epoch 2330, training loss: 3484.95, average training loss: 3632.97, base loss: 4628.76
[INFO 2017-06-26 22:14:09,370 main.py:47] epoch 2331, training loss: 3598.27, average training loss: 3632.41, base loss: 4628.07
[INFO 2017-06-26 22:14:09,769 main.py:47] epoch 2332, training loss: 3197.74, average training loss: 3632.02, base loss: 4628.01
[INFO 2017-06-26 22:14:10,168 main.py:47] epoch 2333, training loss: 3318.82, average training loss: 3632.08, base loss: 4628.46
[INFO 2017-06-26 22:14:10,566 main.py:47] epoch 2334, training loss: 7174.38, average training loss: 3636.18, base loss: 4633.62
[INFO 2017-06-26 22:14:10,965 main.py:47] epoch 2335, training loss: 3452.86, average training loss: 3636.27, base loss: 4634.29
[INFO 2017-06-26 22:14:11,366 main.py:47] epoch 2336, training loss: 3387.37, average training loss: 3635.99, base loss: 4634.11
[INFO 2017-06-26 22:14:11,778 main.py:47] epoch 2337, training loss: 3465.40, average training loss: 3635.73, base loss: 4634.28
[INFO 2017-06-26 22:14:12,178 main.py:47] epoch 2338, training loss: 3594.41, average training loss: 3635.98, base loss: 4635.11
[INFO 2017-06-26 22:14:12,578 main.py:47] epoch 2339, training loss: 3255.72, average training loss: 3635.40, base loss: 4634.85
[INFO 2017-06-26 22:14:12,978 main.py:47] epoch 2340, training loss: 2848.44, average training loss: 3634.92, base loss: 4634.04
[INFO 2017-06-26 22:14:13,376 main.py:47] epoch 2341, training loss: 3583.38, average training loss: 3635.23, base loss: 4634.92
[INFO 2017-06-26 22:14:13,774 main.py:47] epoch 2342, training loss: 3234.07, average training loss: 3635.07, base loss: 4634.95
[INFO 2017-06-26 22:14:14,172 main.py:47] epoch 2343, training loss: 3070.02, average training loss: 3634.70, base loss: 4634.59
[INFO 2017-06-26 22:14:14,577 main.py:47] epoch 2344, training loss: 3860.73, average training loss: 3634.84, base loss: 4634.96
[INFO 2017-06-26 22:14:14,975 main.py:47] epoch 2345, training loss: 3284.90, average training loss: 3634.29, base loss: 4634.47
[INFO 2017-06-26 22:14:15,374 main.py:47] epoch 2346, training loss: 3171.09, average training loss: 3633.68, base loss: 4633.93
[INFO 2017-06-26 22:14:15,777 main.py:47] epoch 2347, training loss: 3164.93, average training loss: 3633.04, base loss: 4633.08
[INFO 2017-06-26 22:14:16,181 main.py:47] epoch 2348, training loss: 6975.58, average training loss: 3635.67, base loss: 4635.97
[INFO 2017-06-26 22:14:16,578 main.py:47] epoch 2349, training loss: 3569.62, average training loss: 3631.77, base loss: 4632.33
[INFO 2017-06-26 22:14:16,979 main.py:47] epoch 2350, training loss: 3259.26, average training loss: 3631.54, base loss: 4632.72
[INFO 2017-06-26 22:14:17,379 main.py:47] epoch 2351, training loss: 3193.02, average training loss: 3630.54, base loss: 4631.91
[INFO 2017-06-26 22:14:17,779 main.py:47] epoch 2352, training loss: 3176.33, average training loss: 3623.84, base loss: 4625.85
[INFO 2017-06-26 22:14:18,182 main.py:47] epoch 2353, training loss: 2820.01, average training loss: 3623.49, base loss: 4625.91
[INFO 2017-06-26 22:14:18,584 main.py:47] epoch 2354, training loss: 3533.01, average training loss: 3623.61, base loss: 4626.67
[INFO 2017-06-26 22:14:18,983 main.py:47] epoch 2355, training loss: 3399.59, average training loss: 3623.51, base loss: 4626.90
[INFO 2017-06-26 22:14:19,386 main.py:47] epoch 2356, training loss: 3096.29, average training loss: 3622.60, base loss: 4625.88
[INFO 2017-06-26 22:14:19,784 main.py:47] epoch 2357, training loss: 3740.70, average training loss: 3622.34, base loss: 4626.19
[INFO 2017-06-26 22:14:20,183 main.py:47] epoch 2358, training loss: 3247.12, average training loss: 3621.84, base loss: 4626.04
[INFO 2017-06-26 22:14:20,582 main.py:47] epoch 2359, training loss: 3636.53, average training loss: 3622.39, base loss: 4627.51
[INFO 2017-06-26 22:14:20,983 main.py:47] epoch 2360, training loss: 3483.93, average training loss: 3622.54, base loss: 4627.97
[INFO 2017-06-26 22:14:21,384 main.py:47] epoch 2361, training loss: 3047.32, average training loss: 3622.43, base loss: 4628.11
[INFO 2017-06-26 22:14:21,783 main.py:47] epoch 2362, training loss: 3433.96, average training loss: 3622.10, base loss: 4627.94
[INFO 2017-06-26 22:14:22,185 main.py:47] epoch 2363, training loss: 3459.26, average training loss: 3618.40, base loss: 4624.66
[INFO 2017-06-26 22:14:22,589 main.py:47] epoch 2364, training loss: 3408.86, average training loss: 3618.28, base loss: 4624.93
[INFO 2017-06-26 22:14:22,990 main.py:47] epoch 2365, training loss: 3397.81, average training loss: 3618.28, base loss: 4625.13
[INFO 2017-06-26 22:14:23,393 main.py:47] epoch 2366, training loss: 3420.84, average training loss: 3617.90, base loss: 4624.36
[INFO 2017-06-26 22:14:23,796 main.py:47] epoch 2367, training loss: 3673.89, average training loss: 3617.84, base loss: 4624.63
[INFO 2017-06-26 22:14:24,199 main.py:47] epoch 2368, training loss: 3330.32, average training loss: 3613.72, base loss: 4620.59
[INFO 2017-06-26 22:14:24,597 main.py:47] epoch 2369, training loss: 3213.06, average training loss: 3613.32, base loss: 4620.34
[INFO 2017-06-26 22:14:24,997 main.py:47] epoch 2370, training loss: 3571.15, average training loss: 3613.38, base loss: 4620.61
[INFO 2017-06-26 22:14:25,396 main.py:47] epoch 2371, training loss: 3280.06, average training loss: 3613.12, base loss: 4620.56
[INFO 2017-06-26 22:14:25,794 main.py:47] epoch 2372, training loss: 3376.69, average training loss: 3612.90, base loss: 4620.81
[INFO 2017-06-26 22:14:26,197 main.py:47] epoch 2373, training loss: 3746.84, average training loss: 3613.10, base loss: 4622.03
[INFO 2017-06-26 22:14:26,598 main.py:47] epoch 2374, training loss: 3357.36, average training loss: 3613.01, base loss: 4622.56
[INFO 2017-06-26 22:14:27,005 main.py:47] epoch 2375, training loss: 3466.60, average training loss: 3612.53, base loss: 4622.37
[INFO 2017-06-26 22:14:27,408 main.py:47] epoch 2376, training loss: 3488.10, average training loss: 3612.84, base loss: 4623.17
[INFO 2017-06-26 22:14:27,807 main.py:47] epoch 2377, training loss: 3553.02, average training loss: 3613.19, base loss: 4624.20
[INFO 2017-06-26 22:14:28,206 main.py:47] epoch 2378, training loss: 2927.11, average training loss: 3609.20, base loss: 4619.95
[INFO 2017-06-26 22:14:28,606 main.py:47] epoch 2379, training loss: 2918.66, average training loss: 3608.84, base loss: 4620.00
[INFO 2017-06-26 22:14:29,008 main.py:47] epoch 2380, training loss: 3033.35, average training loss: 3608.54, base loss: 4619.49
[INFO 2017-06-26 22:14:29,410 main.py:47] epoch 2381, training loss: 3059.39, average training loss: 3608.54, base loss: 4619.49
[INFO 2017-06-26 22:14:29,810 main.py:47] epoch 2382, training loss: 3708.78, average training loss: 3608.65, base loss: 4620.08
[INFO 2017-06-26 22:14:30,213 main.py:47] epoch 2383, training loss: 6786.33, average training loss: 3611.68, base loss: 4623.32
[INFO 2017-06-26 22:14:30,615 main.py:47] epoch 2384, training loss: 3365.50, average training loss: 3611.59, base loss: 4623.86
[INFO 2017-06-26 22:14:31,016 main.py:47] epoch 2385, training loss: 4000.85, average training loss: 3608.17, base loss: 4620.70
[INFO 2017-06-26 22:14:31,419 main.py:47] epoch 2386, training loss: 3914.31, average training loss: 3609.20, base loss: 4623.10
[INFO 2017-06-26 22:14:31,819 main.py:47] epoch 2387, training loss: 3029.66, average training loss: 3608.42, base loss: 4621.80
[INFO 2017-06-26 22:14:32,220 main.py:47] epoch 2388, training loss: 3461.73, average training loss: 3608.68, base loss: 4622.72
[INFO 2017-06-26 22:14:32,623 main.py:47] epoch 2389, training loss: 3783.86, average training loss: 3609.05, base loss: 4623.75
[INFO 2017-06-26 22:14:33,024 main.py:47] epoch 2390, training loss: 3252.53, average training loss: 3609.07, base loss: 4624.22
[INFO 2017-06-26 22:14:33,428 main.py:47] epoch 2391, training loss: 6982.24, average training loss: 3612.40, base loss: 4627.86
[INFO 2017-06-26 22:14:33,832 main.py:47] epoch 2392, training loss: 3612.19, average training loss: 3612.60, base loss: 4628.41
[INFO 2017-06-26 22:14:34,234 main.py:47] epoch 2393, training loss: 3249.98, average training loss: 3612.29, base loss: 4628.20
[INFO 2017-06-26 22:14:34,632 main.py:47] epoch 2394, training loss: 3466.09, average training loss: 3608.62, base loss: 4624.90
[INFO 2017-06-26 22:14:35,034 main.py:47] epoch 2395, training loss: 3073.53, average training loss: 3608.90, base loss: 4625.47
[INFO 2017-06-26 22:14:35,444 main.py:47] epoch 2396, training loss: 3445.95, average training loss: 3608.69, base loss: 4625.48
[INFO 2017-06-26 22:14:35,854 main.py:47] epoch 2397, training loss: 3704.49, average training loss: 3609.11, base loss: 4626.23
[INFO 2017-06-26 22:14:36,255 main.py:47] epoch 2398, training loss: 3161.82, average training loss: 3608.70, base loss: 4625.98
[INFO 2017-06-26 22:14:36,659 main.py:47] epoch 2399, training loss: 3043.31, average training loss: 3608.54, base loss: 4625.59
[INFO 2017-06-26 22:14:36,659 main.py:49] epoch 2399, testing
[INFO 2017-06-26 22:14:38,319 main.py:102] average testing loss: 3749.77, base loss: 4940.86
[INFO 2017-06-26 22:14:38,319 main.py:103] improve_loss: 1191.08, improve_percent: 0.24
[INFO 2017-06-26 22:14:38,319 main.py:73] current best improved percent: 0.25
[INFO 2017-06-26 22:14:38,720 main.py:47] epoch 2400, training loss: 3140.10, average training loss: 3607.84, base loss: 4624.49
[INFO 2017-06-26 22:14:39,122 main.py:47] epoch 2401, training loss: 3175.54, average training loss: 3607.23, base loss: 4623.84
[INFO 2017-06-26 22:14:39,520 main.py:47] epoch 2402, training loss: 3298.02, average training loss: 3606.57, base loss: 4623.29
[INFO 2017-06-26 22:14:39,921 main.py:47] epoch 2403, training loss: 3394.90, average training loss: 3606.72, base loss: 4624.01
[INFO 2017-06-26 22:14:40,325 main.py:47] epoch 2404, training loss: 3245.58, average training loss: 3605.76, base loss: 4622.79
[INFO 2017-06-26 22:14:40,722 main.py:47] epoch 2405, training loss: 3344.23, average training loss: 3604.97, base loss: 4621.82
[INFO 2017-06-26 22:14:41,119 main.py:47] epoch 2406, training loss: 3136.07, average training loss: 3600.99, base loss: 4617.99
[INFO 2017-06-26 22:14:41,518 main.py:47] epoch 2407, training loss: 3238.49, average training loss: 3600.53, base loss: 4617.78
[INFO 2017-06-26 22:14:41,921 main.py:47] epoch 2408, training loss: 3210.83, average training loss: 3600.71, base loss: 4618.43
[INFO 2017-06-26 22:14:42,323 main.py:47] epoch 2409, training loss: 3091.12, average training loss: 3600.31, base loss: 4618.31
[INFO 2017-06-26 22:14:42,727 main.py:47] epoch 2410, training loss: 3164.35, average training loss: 3598.75, base loss: 4616.42
[INFO 2017-06-26 22:14:43,129 main.py:47] epoch 2411, training loss: 3138.82, average training loss: 3598.49, base loss: 4616.27
[INFO 2017-06-26 22:14:43,528 main.py:47] epoch 2412, training loss: 3013.92, average training loss: 3598.29, base loss: 4616.24
[INFO 2017-06-26 22:14:43,932 main.py:47] epoch 2413, training loss: 3175.70, average training loss: 3598.03, base loss: 4615.83
[INFO 2017-06-26 22:14:44,333 main.py:47] epoch 2414, training loss: 2735.89, average training loss: 3596.63, base loss: 4614.04
[INFO 2017-06-26 22:14:44,731 main.py:47] epoch 2415, training loss: 3072.49, average training loss: 3596.29, base loss: 4613.86
[INFO 2017-06-26 22:14:45,129 main.py:47] epoch 2416, training loss: 3139.10, average training loss: 3595.54, base loss: 4612.82
[INFO 2017-06-26 22:14:45,529 main.py:47] epoch 2417, training loss: 3475.52, average training loss: 3595.67, base loss: 4613.92
[INFO 2017-06-26 22:14:45,927 main.py:47] epoch 2418, training loss: 3429.51, average training loss: 3594.98, base loss: 4613.31
[INFO 2017-06-26 22:14:46,330 main.py:47] epoch 2419, training loss: 3049.58, average training loss: 3594.41, base loss: 4612.69
[INFO 2017-06-26 22:14:46,737 main.py:47] epoch 2420, training loss: 3422.29, average training loss: 3594.63, base loss: 4613.07
[INFO 2017-06-26 22:14:47,139 main.py:47] epoch 2421, training loss: 3584.99, average training loss: 3590.94, base loss: 4609.93
[INFO 2017-06-26 22:14:47,541 main.py:47] epoch 2422, training loss: 3018.35, average training loss: 3590.29, base loss: 4609.37
[INFO 2017-06-26 22:14:47,942 main.py:47] epoch 2423, training loss: 3195.48, average training loss: 3589.34, base loss: 4608.26
[INFO 2017-06-26 22:14:48,341 main.py:47] epoch 2424, training loss: 3141.65, average training loss: 3588.84, base loss: 4607.46
[INFO 2017-06-26 22:14:48,740 main.py:47] epoch 2425, training loss: 3176.77, average training loss: 3588.10, base loss: 4606.60
[INFO 2017-06-26 22:14:49,142 main.py:47] epoch 2426, training loss: 3142.05, average training loss: 3587.37, base loss: 4605.55
[INFO 2017-06-26 22:14:49,543 main.py:47] epoch 2427, training loss: 2877.98, average training loss: 3586.42, base loss: 4604.25
[INFO 2017-06-26 22:14:49,946 main.py:47] epoch 2428, training loss: 6807.86, average training loss: 3590.11, base loss: 4608.00
[INFO 2017-06-26 22:14:50,350 main.py:47] epoch 2429, training loss: 3594.42, average training loss: 3590.25, base loss: 4608.67
[INFO 2017-06-26 22:14:50,796 main.py:47] epoch 2430, training loss: 3790.26, average training loss: 3590.84, base loss: 4609.98
[INFO 2017-06-26 22:14:51,243 main.py:47] epoch 2431, training loss: 3239.99, average training loss: 3591.09, base loss: 4610.56
[INFO 2017-06-26 22:14:51,656 main.py:47] epoch 2432, training loss: 3469.23, average training loss: 3591.33, base loss: 4611.69
[INFO 2017-06-26 22:14:52,079 main.py:47] epoch 2433, training loss: 3147.26, average training loss: 3591.13, base loss: 4611.52
[INFO 2017-06-26 22:14:52,500 main.py:47] epoch 2434, training loss: 3329.09, average training loss: 3590.65, base loss: 4610.95
[INFO 2017-06-26 22:14:52,930 main.py:47] epoch 2435, training loss: 3153.22, average training loss: 3590.29, base loss: 4610.55
[INFO 2017-06-26 22:14:53,336 main.py:47] epoch 2436, training loss: 3375.75, average training loss: 3590.31, base loss: 4610.81
[INFO 2017-06-26 22:14:53,802 main.py:47] epoch 2437, training loss: 3672.34, average training loss: 3590.46, base loss: 4611.64
[INFO 2017-06-26 22:14:54,213 main.py:47] epoch 2438, training loss: 3534.50, average training loss: 3590.53, base loss: 4612.12
[INFO 2017-06-26 22:14:54,619 main.py:47] epoch 2439, training loss: 3021.39, average training loss: 3586.22, base loss: 4607.66
[INFO 2017-06-26 22:14:55,023 main.py:47] epoch 2440, training loss: 3394.64, average training loss: 3582.99, base loss: 4604.81
[INFO 2017-06-26 22:14:55,437 main.py:47] epoch 2441, training loss: 3428.26, average training loss: 3582.03, base loss: 4603.56
[INFO 2017-06-26 22:14:55,841 main.py:47] epoch 2442, training loss: 3345.98, average training loss: 3581.59, base loss: 4603.00
[INFO 2017-06-26 22:14:56,246 main.py:47] epoch 2443, training loss: 3117.32, average training loss: 3581.65, base loss: 4603.39
[INFO 2017-06-26 22:14:56,650 main.py:47] epoch 2444, training loss: 2931.07, average training loss: 3581.42, base loss: 4603.40
[INFO 2017-06-26 22:14:57,053 main.py:47] epoch 2445, training loss: 3126.82, average training loss: 3581.40, base loss: 4603.77
[INFO 2017-06-26 22:14:57,455 main.py:47] epoch 2446, training loss: 6826.48, average training loss: 3584.24, base loss: 4606.48
[INFO 2017-06-26 22:14:57,854 main.py:47] epoch 2447, training loss: 3276.77, average training loss: 3584.00, base loss: 4606.35
[INFO 2017-06-26 22:14:58,254 main.py:47] epoch 2448, training loss: 3904.88, average training loss: 3584.49, base loss: 4607.79
[INFO 2017-06-26 22:14:58,655 main.py:47] epoch 2449, training loss: 3298.48, average training loss: 3583.91, base loss: 4607.41
[INFO 2017-06-26 22:14:59,057 main.py:47] epoch 2450, training loss: 3096.09, average training loss: 3579.35, base loss: 4602.72
[INFO 2017-06-26 22:14:59,455 main.py:47] epoch 2451, training loss: 3587.77, average training loss: 3579.10, base loss: 4602.70
[INFO 2017-06-26 22:14:59,861 main.py:47] epoch 2452, training loss: 3527.71, average training loss: 3579.19, base loss: 4603.25
[INFO 2017-06-26 22:15:00,265 main.py:47] epoch 2453, training loss: 3043.54, average training loss: 3578.67, base loss: 4603.04
[INFO 2017-06-26 22:15:00,670 main.py:47] epoch 2454, training loss: 3533.71, average training loss: 3578.64, base loss: 4603.37
[INFO 2017-06-26 22:15:01,085 main.py:47] epoch 2455, training loss: 3099.18, average training loss: 3577.96, base loss: 4602.62
[INFO 2017-06-26 22:15:01,488 main.py:47] epoch 2456, training loss: 3390.49, average training loss: 3577.61, base loss: 4602.03
[INFO 2017-06-26 22:15:01,892 main.py:47] epoch 2457, training loss: 3440.32, average training loss: 3577.71, base loss: 4602.81
[INFO 2017-06-26 22:15:02,296 main.py:47] epoch 2458, training loss: 2783.13, average training loss: 3576.84, base loss: 4601.45
[INFO 2017-06-26 22:15:02,699 main.py:47] epoch 2459, training loss: 3186.68, average training loss: 3576.83, base loss: 4602.17
[INFO 2017-06-26 22:15:03,103 main.py:47] epoch 2460, training loss: 6867.90, average training loss: 3580.17, base loss: 4606.24
[INFO 2017-06-26 22:15:03,506 main.py:47] epoch 2461, training loss: 3473.10, average training loss: 3580.27, base loss: 4606.91
[INFO 2017-06-26 22:15:03,906 main.py:47] epoch 2462, training loss: 3404.36, average training loss: 3580.38, base loss: 4607.39
[INFO 2017-06-26 22:15:04,305 main.py:47] epoch 2463, training loss: 3250.99, average training loss: 3580.20, base loss: 4607.78
[INFO 2017-06-26 22:15:04,701 main.py:47] epoch 2464, training loss: 3324.15, average training loss: 3580.13, base loss: 4608.05
[INFO 2017-06-26 22:15:05,099 main.py:47] epoch 2465, training loss: 3198.30, average training loss: 3579.36, base loss: 4607.23
[INFO 2017-06-26 22:15:05,496 main.py:47] epoch 2466, training loss: 2877.25, average training loss: 3578.66, base loss: 4606.04
[INFO 2017-06-26 22:15:05,899 main.py:47] epoch 2467, training loss: 3761.07, average training loss: 3575.28, base loss: 4602.96
[INFO 2017-06-26 22:15:06,302 main.py:47] epoch 2468, training loss: 3290.59, average training loss: 3574.91, base loss: 4602.88
[INFO 2017-06-26 22:15:06,706 main.py:47] epoch 2469, training loss: 3144.21, average training loss: 3574.27, base loss: 4602.01
[INFO 2017-06-26 22:15:07,107 main.py:47] epoch 2470, training loss: 3223.56, average training loss: 3574.04, base loss: 4601.78
[INFO 2017-06-26 22:15:07,507 main.py:47] epoch 2471, training loss: 3647.11, average training loss: 3574.44, base loss: 4602.91
[INFO 2017-06-26 22:15:07,905 main.py:47] epoch 2472, training loss: 3296.66, average training loss: 3573.06, base loss: 4601.45
[INFO 2017-06-26 22:15:08,303 main.py:47] epoch 2473, training loss: 3285.08, average training loss: 3572.57, base loss: 4601.03
[INFO 2017-06-26 22:15:08,705 main.py:47] epoch 2474, training loss: 2889.37, average training loss: 3571.41, base loss: 4599.18
[INFO 2017-06-26 22:15:09,115 main.py:47] epoch 2475, training loss: 3116.31, average training loss: 3571.36, base loss: 4599.31
[INFO 2017-06-26 22:15:09,518 main.py:47] epoch 2476, training loss: 3284.68, average training loss: 3571.16, base loss: 4599.32
[INFO 2017-06-26 22:15:09,921 main.py:47] epoch 2477, training loss: 3465.86, average training loss: 3571.10, base loss: 4599.62
[INFO 2017-06-26 22:15:10,324 main.py:47] epoch 2478, training loss: 3514.23, average training loss: 3570.82, base loss: 4599.86
[INFO 2017-06-26 22:15:10,720 main.py:47] epoch 2479, training loss: 10596.71, average training loss: 3577.60, base loss: 4607.26
[INFO 2017-06-26 22:15:11,122 main.py:47] epoch 2480, training loss: 3290.69, average training loss: 3577.66, base loss: 4608.10
[INFO 2017-06-26 22:15:11,527 main.py:47] epoch 2481, training loss: 3537.90, average training loss: 3577.57, base loss: 4608.63
[INFO 2017-06-26 22:15:11,931 main.py:47] epoch 2482, training loss: 3036.09, average training loss: 3577.32, base loss: 4608.77
[INFO 2017-06-26 22:15:12,339 main.py:47] epoch 2483, training loss: 3012.34, average training loss: 3576.37, base loss: 4607.72
[INFO 2017-06-26 22:15:12,742 main.py:47] epoch 2484, training loss: 3530.67, average training loss: 3576.71, base loss: 4608.68
[INFO 2017-06-26 22:15:13,149 main.py:47] epoch 2485, training loss: 3257.50, average training loss: 3576.55, base loss: 4608.77
[INFO 2017-06-26 22:15:13,553 main.py:47] epoch 2486, training loss: 3477.26, average training loss: 3576.55, base loss: 4609.24
[INFO 2017-06-26 22:15:13,957 main.py:47] epoch 2487, training loss: 3695.64, average training loss: 3576.47, base loss: 4609.83
[INFO 2017-06-26 22:15:14,355 main.py:47] epoch 2488, training loss: 3012.27, average training loss: 3575.73, base loss: 4608.92
[INFO 2017-06-26 22:15:14,751 main.py:47] epoch 2489, training loss: 3435.64, average training loss: 3572.20, base loss: 4605.64
[INFO 2017-06-26 22:15:15,155 main.py:47] epoch 2490, training loss: 3041.66, average training loss: 3572.05, base loss: 4605.70
[INFO 2017-06-26 22:15:15,560 main.py:47] epoch 2491, training loss: 3262.90, average training loss: 3571.95, base loss: 4605.91
[INFO 2017-06-26 22:15:15,963 main.py:47] epoch 2492, training loss: 3053.61, average training loss: 3571.78, base loss: 4605.74
[INFO 2017-06-26 22:15:16,364 main.py:47] epoch 2493, training loss: 3572.50, average training loss: 3571.74, base loss: 4606.12
[INFO 2017-06-26 22:15:16,769 main.py:47] epoch 2494, training loss: 3305.47, average training loss: 3571.48, base loss: 4605.95
[INFO 2017-06-26 22:15:17,173 main.py:47] epoch 2495, training loss: 3186.49, average training loss: 3571.04, base loss: 4605.18
[INFO 2017-06-26 22:15:17,583 main.py:47] epoch 2496, training loss: 3447.01, average training loss: 3571.03, base loss: 4605.80
[INFO 2017-06-26 22:15:17,985 main.py:47] epoch 2497, training loss: 3519.17, average training loss: 3570.36, base loss: 4605.07
[INFO 2017-06-26 22:15:18,388 main.py:47] epoch 2498, training loss: 3297.49, average training loss: 3570.37, base loss: 4605.19
[INFO 2017-06-26 22:15:18,791 main.py:47] epoch 2499, training loss: 3451.80, average training loss: 3570.83, base loss: 4606.19
[INFO 2017-06-26 22:15:18,791 main.py:49] epoch 2499, testing
[INFO 2017-06-26 22:15:20,445 main.py:102] average testing loss: 3260.72, base loss: 4312.38
[INFO 2017-06-26 22:15:20,445 main.py:103] improve_loss: 1051.67, improve_percent: 0.24
[INFO 2017-06-26 22:15:20,446 main.py:73] current best improved percent: 0.25
[INFO 2017-06-26 22:15:20,852 main.py:47] epoch 2500, training loss: 3017.83, average training loss: 3570.82, base loss: 4606.63
[INFO 2017-06-26 22:15:21,262 main.py:47] epoch 2501, training loss: 3370.92, average training loss: 3569.94, base loss: 4605.71
[INFO 2017-06-26 22:15:21,667 main.py:47] epoch 2502, training loss: 3491.02, average training loss: 3569.94, base loss: 4606.06
[INFO 2017-06-26 22:15:22,072 main.py:47] epoch 2503, training loss: 3975.47, average training loss: 3570.87, base loss: 4607.80
[INFO 2017-06-26 22:15:22,473 main.py:47] epoch 2504, training loss: 3432.73, average training loss: 3570.78, base loss: 4608.49
[INFO 2017-06-26 22:15:22,877 main.py:47] epoch 2505, training loss: 3451.50, average training loss: 3570.73, base loss: 4608.72
[INFO 2017-06-26 22:15:23,289 main.py:47] epoch 2506, training loss: 3295.69, average training loss: 3570.55, base loss: 4608.72
[INFO 2017-06-26 22:15:23,694 main.py:47] epoch 2507, training loss: 10015.13, average training loss: 3577.03, base loss: 4615.29
[INFO 2017-06-26 22:15:24,101 main.py:47] epoch 2508, training loss: 2695.62, average training loss: 3572.71, base loss: 4610.96
[INFO 2017-06-26 22:15:24,499 main.py:47] epoch 2509, training loss: 3460.95, average training loss: 3573.00, base loss: 4611.80
[INFO 2017-06-26 22:15:24,900 main.py:47] epoch 2510, training loss: 2944.39, average training loss: 3572.37, base loss: 4611.25
[INFO 2017-06-26 22:15:25,301 main.py:47] epoch 2511, training loss: 3332.03, average training loss: 3571.72, base loss: 4610.51
[INFO 2017-06-26 22:15:25,700 main.py:47] epoch 2512, training loss: 7224.55, average training loss: 3575.81, base loss: 4615.06
[INFO 2017-06-26 22:15:26,102 main.py:47] epoch 2513, training loss: 3117.16, average training loss: 3575.26, base loss: 4614.60
[INFO 2017-06-26 22:15:26,508 main.py:47] epoch 2514, training loss: 3416.86, average training loss: 3575.35, base loss: 4615.21
[INFO 2017-06-26 22:15:26,999 main.py:47] epoch 2515, training loss: 3410.68, average training loss: 3575.42, base loss: 4615.44
[INFO 2017-06-26 22:15:27,424 main.py:47] epoch 2516, training loss: 3201.68, average training loss: 3574.74, base loss: 4614.41
[INFO 2017-06-26 22:15:27,837 main.py:47] epoch 2517, training loss: 3269.61, average training loss: 3574.29, base loss: 4614.34
[INFO 2017-06-26 22:15:28,240 main.py:47] epoch 2518, training loss: 3353.99, average training loss: 3574.46, base loss: 4615.35
[INFO 2017-06-26 22:15:28,661 main.py:47] epoch 2519, training loss: 3090.00, average training loss: 3574.00, base loss: 4614.87
[INFO 2017-06-26 22:15:29,092 main.py:47] epoch 2520, training loss: 3363.92, average training loss: 3573.09, base loss: 4613.84
[INFO 2017-06-26 22:15:29,496 main.py:47] epoch 2521, training loss: 2827.98, average training loss: 3572.28, base loss: 4612.79
[INFO 2017-06-26 22:15:29,902 main.py:47] epoch 2522, training loss: 7313.84, average training loss: 3576.12, base loss: 4617.21
[INFO 2017-06-26 22:15:30,310 main.py:47] epoch 2523, training loss: 4218.85, average training loss: 3577.07, base loss: 4618.88
[INFO 2017-06-26 22:15:30,726 main.py:47] epoch 2524, training loss: 3385.56, average training loss: 3577.31, base loss: 4619.57
[INFO 2017-06-26 22:15:31,136 main.py:47] epoch 2525, training loss: 3460.22, average training loss: 3577.93, base loss: 4620.87
[INFO 2017-06-26 22:15:31,540 main.py:47] epoch 2526, training loss: 2929.93, average training loss: 3577.33, base loss: 4620.30
[INFO 2017-06-26 22:15:31,950 main.py:47] epoch 2527, training loss: 2978.71, average training loss: 3577.13, base loss: 4619.95
[INFO 2017-06-26 22:15:32,353 main.py:47] epoch 2528, training loss: 3242.89, average training loss: 3576.94, base loss: 4619.71
[INFO 2017-06-26 22:15:32,759 main.py:47] epoch 2529, training loss: 3233.94, average training loss: 3576.90, base loss: 4620.01
[INFO 2017-06-26 22:15:33,164 main.py:47] epoch 2530, training loss: 2995.32, average training loss: 3576.64, base loss: 4619.82
[INFO 2017-06-26 22:15:33,567 main.py:47] epoch 2531, training loss: 3499.59, average training loss: 3576.70, base loss: 4620.73
[INFO 2017-06-26 22:15:33,970 main.py:47] epoch 2532, training loss: 3112.62, average training loss: 3575.42, base loss: 4619.30
[INFO 2017-06-26 22:15:34,373 main.py:47] epoch 2533, training loss: 3704.40, average training loss: 3575.56, base loss: 4619.72
[INFO 2017-06-26 22:15:34,775 main.py:47] epoch 2534, training loss: 3065.99, average training loss: 3575.28, base loss: 4619.26
[INFO 2017-06-26 22:15:35,178 main.py:47] epoch 2535, training loss: 3104.49, average training loss: 3574.94, base loss: 4618.81
[INFO 2017-06-26 22:15:35,581 main.py:47] epoch 2536, training loss: 3085.25, average training loss: 3574.17, base loss: 4617.81
[INFO 2017-06-26 22:15:35,987 main.py:47] epoch 2537, training loss: 3062.01, average training loss: 3573.50, base loss: 4616.91
[INFO 2017-06-26 22:15:36,386 main.py:47] epoch 2538, training loss: 3468.82, average training loss: 3573.37, base loss: 4617.08
[INFO 2017-06-26 22:15:36,783 main.py:47] epoch 2539, training loss: 3096.14, average training loss: 3572.58, base loss: 4615.55
[INFO 2017-06-26 22:15:37,181 main.py:47] epoch 2540, training loss: 3064.65, average training loss: 3572.00, base loss: 4614.32
[INFO 2017-06-26 22:15:37,583 main.py:47] epoch 2541, training loss: 3683.32, average training loss: 3572.57, base loss: 4615.82
[INFO 2017-06-26 22:15:37,986 main.py:47] epoch 2542, training loss: 3293.58, average training loss: 3572.37, base loss: 4615.42
[INFO 2017-06-26 22:15:38,390 main.py:47] epoch 2543, training loss: 3519.52, average training loss: 3572.77, base loss: 4616.27
[INFO 2017-06-26 22:15:38,788 main.py:47] epoch 2544, training loss: 3315.98, average training loss: 3572.66, base loss: 4616.53
[INFO 2017-06-26 22:15:39,191 main.py:47] epoch 2545, training loss: 3378.59, average training loss: 3572.59, base loss: 4616.73
[INFO 2017-06-26 22:15:39,594 main.py:47] epoch 2546, training loss: 3326.91, average training loss: 3572.81, base loss: 4617.51
[INFO 2017-06-26 22:15:39,997 main.py:47] epoch 2547, training loss: 3083.88, average training loss: 3572.06, base loss: 4616.87
[INFO 2017-06-26 22:15:40,398 main.py:47] epoch 2548, training loss: 3458.15, average training loss: 3571.79, base loss: 4616.61
[INFO 2017-06-26 22:15:40,802 main.py:47] epoch 2549, training loss: 3649.57, average training loss: 3572.19, base loss: 4617.85
[INFO 2017-06-26 22:15:41,205 main.py:47] epoch 2550, training loss: 3700.91, average training loss: 3572.52, base loss: 4618.85
[INFO 2017-06-26 22:15:41,603 main.py:47] epoch 2551, training loss: 3053.39, average training loss: 3572.07, base loss: 4618.21
[INFO 2017-06-26 22:15:42,009 main.py:47] epoch 2552, training loss: 3226.54, average training loss: 3571.67, base loss: 4617.69
[INFO 2017-06-26 22:15:42,418 main.py:47] epoch 2553, training loss: 3478.46, average training loss: 3571.77, base loss: 4618.17
[INFO 2017-06-26 22:15:42,817 main.py:47] epoch 2554, training loss: 3112.88, average training loss: 3571.18, base loss: 4617.72
[INFO 2017-06-26 22:15:43,223 main.py:47] epoch 2555, training loss: 6380.08, average training loss: 3573.82, base loss: 4620.34
[INFO 2017-06-26 22:15:43,628 main.py:47] epoch 2556, training loss: 3436.46, average training loss: 3573.55, base loss: 4620.27
[INFO 2017-06-26 22:15:44,039 main.py:47] epoch 2557, training loss: 3084.15, average training loss: 3573.63, base loss: 4620.81
[INFO 2017-06-26 22:15:44,444 main.py:47] epoch 2558, training loss: 3538.54, average training loss: 3573.34, base loss: 4620.58
[INFO 2017-06-26 22:15:44,846 main.py:47] epoch 2559, training loss: 3160.24, average training loss: 3573.30, base loss: 4621.31
[INFO 2017-06-26 22:15:45,246 main.py:47] epoch 2560, training loss: 2806.09, average training loss: 3572.76, base loss: 4621.04
[INFO 2017-06-26 22:15:45,647 main.py:47] epoch 2561, training loss: 3071.89, average training loss: 3571.97, base loss: 4620.08
[INFO 2017-06-26 22:15:46,053 main.py:47] epoch 2562, training loss: 2908.78, average training loss: 3571.00, base loss: 4618.62
[INFO 2017-06-26 22:15:46,463 main.py:47] epoch 2563, training loss: 3076.03, average training loss: 3570.14, base loss: 4617.59
[INFO 2017-06-26 22:15:46,864 main.py:47] epoch 2564, training loss: 3205.90, average training loss: 3569.76, base loss: 4617.25
[INFO 2017-06-26 22:15:47,265 main.py:47] epoch 2565, training loss: 3748.62, average training loss: 3570.12, base loss: 4618.01
[INFO 2017-06-26 22:15:47,668 main.py:47] epoch 2566, training loss: 2637.91, average training loss: 3568.96, base loss: 4616.44
[INFO 2017-06-26 22:15:48,072 main.py:47] epoch 2567, training loss: 3221.13, average training loss: 3568.52, base loss: 4616.16
[INFO 2017-06-26 22:15:48,474 main.py:47] epoch 2568, training loss: 3221.08, average training loss: 3568.23, base loss: 4615.69
[INFO 2017-06-26 22:15:48,872 main.py:47] epoch 2569, training loss: 3032.56, average training loss: 3567.10, base loss: 4614.03
[INFO 2017-06-26 22:15:49,280 main.py:47] epoch 2570, training loss: 3754.28, average training loss: 3567.06, base loss: 4614.42
[INFO 2017-06-26 22:15:49,687 main.py:47] epoch 2571, training loss: 3397.01, average training loss: 3566.79, base loss: 4614.29
[INFO 2017-06-26 22:15:50,091 main.py:47] epoch 2572, training loss: 3106.08, average training loss: 3566.33, base loss: 4613.78
[INFO 2017-06-26 22:15:50,505 main.py:47] epoch 2573, training loss: 3355.91, average training loss: 3565.96, base loss: 4613.39
[INFO 2017-06-26 22:15:50,908 main.py:47] epoch 2574, training loss: 3667.72, average training loss: 3565.81, base loss: 4613.37
[INFO 2017-06-26 22:15:51,308 main.py:47] epoch 2575, training loss: 3176.15, average training loss: 3565.28, base loss: 4612.72
[INFO 2017-06-26 22:15:51,707 main.py:47] epoch 2576, training loss: 3195.65, average training loss: 3564.81, base loss: 4611.93
[INFO 2017-06-26 22:15:52,107 main.py:47] epoch 2577, training loss: 3257.89, average training loss: 3565.13, base loss: 4612.90
[INFO 2017-06-26 22:15:52,511 main.py:47] epoch 2578, training loss: 2930.00, average training loss: 3564.84, base loss: 4612.76
[INFO 2017-06-26 22:15:52,918 main.py:47] epoch 2579, training loss: 3226.21, average training loss: 3564.22, base loss: 4611.88
[INFO 2017-06-26 22:15:53,321 main.py:47] epoch 2580, training loss: 6997.19, average training loss: 3567.81, base loss: 4616.30
[INFO 2017-06-26 22:15:53,725 main.py:47] epoch 2581, training loss: 3608.06, average training loss: 3567.94, base loss: 4616.97
[INFO 2017-06-26 22:15:54,130 main.py:47] epoch 2582, training loss: 3797.21, average training loss: 3568.78, base loss: 4619.05
[INFO 2017-06-26 22:15:54,532 main.py:47] epoch 2583, training loss: 3574.04, average training loss: 3568.65, base loss: 4619.08
[INFO 2017-06-26 22:15:54,935 main.py:47] epoch 2584, training loss: 3081.99, average training loss: 3568.32, base loss: 4618.94
[INFO 2017-06-26 22:15:55,345 main.py:47] epoch 2585, training loss: 3137.17, average training loss: 3567.49, base loss: 4618.00
[INFO 2017-06-26 22:15:55,747 main.py:47] epoch 2586, training loss: 6794.01, average training loss: 3571.07, base loss: 4622.24
[INFO 2017-06-26 22:15:56,152 main.py:47] epoch 2587, training loss: 3009.17, average training loss: 3569.87, base loss: 4620.83
[INFO 2017-06-26 22:15:56,556 main.py:47] epoch 2588, training loss: 3098.12, average training loss: 3569.82, base loss: 4620.87
[INFO 2017-06-26 22:15:56,956 main.py:47] epoch 2589, training loss: 3570.49, average training loss: 3569.94, base loss: 4621.37
[INFO 2017-06-26 22:15:57,356 main.py:47] epoch 2590, training loss: 3125.92, average training loss: 3569.49, base loss: 4621.26
[INFO 2017-06-26 22:15:57,759 main.py:47] epoch 2591, training loss: 3302.26, average training loss: 3569.44, base loss: 4621.58
[INFO 2017-06-26 22:15:58,168 main.py:47] epoch 2592, training loss: 3260.58, average training loss: 3569.40, base loss: 4621.74
[INFO 2017-06-26 22:15:58,573 main.py:47] epoch 2593, training loss: 3292.57, average training loss: 3568.78, base loss: 4620.93
[INFO 2017-06-26 22:15:58,973 main.py:47] epoch 2594, training loss: 3342.03, average training loss: 3568.58, base loss: 4621.19
[INFO 2017-06-26 22:15:59,377 main.py:47] epoch 2595, training loss: 2778.79, average training loss: 3567.60, base loss: 4619.66
[INFO 2017-06-26 22:15:59,785 main.py:47] epoch 2596, training loss: 6396.59, average training loss: 3570.69, base loss: 4622.85
[INFO 2017-06-26 22:16:00,187 main.py:47] epoch 2597, training loss: 3586.08, average training loss: 3571.03, base loss: 4623.66
[INFO 2017-06-26 22:16:00,591 main.py:47] epoch 2598, training loss: 3044.90, average training loss: 3570.10, base loss: 4622.31
[INFO 2017-06-26 22:16:00,994 main.py:47] epoch 2599, training loss: 2920.97, average training loss: 3569.81, base loss: 4622.34
[INFO 2017-06-26 22:16:00,994 main.py:49] epoch 2599, testing
[INFO 2017-06-26 22:16:02,654 main.py:102] average testing loss: 3650.04, base loss: 4825.38
[INFO 2017-06-26 22:16:02,654 main.py:103] improve_loss: 1175.34, improve_percent: 0.24
[INFO 2017-06-26 22:16:02,655 main.py:73] current best improved percent: 0.25
[INFO 2017-06-26 22:16:03,052 main.py:47] epoch 2600, training loss: 6712.15, average training loss: 3573.17, base loss: 4625.86
[INFO 2017-06-26 22:16:03,453 main.py:47] epoch 2601, training loss: 3454.05, average training loss: 3569.67, base loss: 4622.62
[INFO 2017-06-26 22:16:03,859 main.py:47] epoch 2602, training loss: 3105.04, average training loss: 3569.40, base loss: 4622.39
[INFO 2017-06-26 22:16:04,264 main.py:47] epoch 2603, training loss: 3901.53, average training loss: 3569.69, base loss: 4623.48
[INFO 2017-06-26 22:16:04,665 main.py:47] epoch 2604, training loss: 2980.49, average training loss: 3568.83, base loss: 4622.29
[INFO 2017-06-26 22:16:05,080 main.py:47] epoch 2605, training loss: 3481.38, average training loss: 3569.23, base loss: 4623.05
[INFO 2017-06-26 22:16:05,483 main.py:47] epoch 2606, training loss: 3642.18, average training loss: 3569.40, base loss: 4623.81
[INFO 2017-06-26 22:16:05,887 main.py:47] epoch 2607, training loss: 3408.91, average training loss: 3569.05, base loss: 4623.99
[INFO 2017-06-26 22:16:06,297 main.py:47] epoch 2608, training loss: 3284.87, average training loss: 3568.87, base loss: 4623.94
[INFO 2017-06-26 22:16:06,697 main.py:47] epoch 2609, training loss: 3090.07, average training loss: 3568.82, base loss: 4624.22
[INFO 2017-06-26 22:16:07,101 main.py:47] epoch 2610, training loss: 3423.65, average training loss: 3568.27, base loss: 4624.08
[INFO 2017-06-26 22:16:07,509 main.py:47] epoch 2611, training loss: 7026.79, average training loss: 3571.88, base loss: 4628.32
[INFO 2017-06-26 22:16:07,914 main.py:47] epoch 2612, training loss: 2815.07, average training loss: 3571.45, base loss: 4627.88
[INFO 2017-06-26 22:16:08,319 main.py:47] epoch 2613, training loss: 3186.87, average training loss: 3570.69, base loss: 4627.04
[INFO 2017-06-26 22:16:08,720 main.py:47] epoch 2614, training loss: 3332.00, average training loss: 3570.38, base loss: 4626.87
[INFO 2017-06-26 22:16:09,126 main.py:47] epoch 2615, training loss: 3022.07, average training loss: 3569.71, base loss: 4626.00
[INFO 2017-06-26 22:16:09,530 main.py:47] epoch 2616, training loss: 3388.67, average training loss: 3565.55, base loss: 4621.59
[INFO 2017-06-26 22:16:09,936 main.py:47] epoch 2617, training loss: 3011.96, average training loss: 3565.30, base loss: 4621.45
[INFO 2017-06-26 22:16:10,338 main.py:47] epoch 2618, training loss: 3444.31, average training loss: 3565.41, base loss: 4621.95
[INFO 2017-06-26 22:16:10,738 main.py:47] epoch 2619, training loss: 2767.02, average training loss: 3564.80, base loss: 4620.97
[INFO 2017-06-26 22:16:11,135 main.py:47] epoch 2620, training loss: 10158.82, average training loss: 3571.14, base loss: 4627.40
[INFO 2017-06-26 22:16:11,531 main.py:47] epoch 2621, training loss: 3208.21, average training loss: 3571.00, base loss: 4627.63
[INFO 2017-06-26 22:16:11,928 main.py:47] epoch 2622, training loss: 3290.70, average training loss: 3570.72, base loss: 4627.55
[INFO 2017-06-26 22:16:12,326 main.py:47] epoch 2623, training loss: 3444.05, average training loss: 3570.86, base loss: 4628.35
[INFO 2017-06-26 22:16:12,728 main.py:47] epoch 2624, training loss: 3500.16, average training loss: 3570.80, base loss: 4628.83
[INFO 2017-06-26 22:16:13,134 main.py:47] epoch 2625, training loss: 7008.08, average training loss: 3574.12, base loss: 4632.65
[INFO 2017-06-26 22:16:13,536 main.py:47] epoch 2626, training loss: 3387.05, average training loss: 3574.34, base loss: 4633.15
[INFO 2017-06-26 22:16:13,941 main.py:47] epoch 2627, training loss: 3815.68, average training loss: 3574.72, base loss: 4634.49
[INFO 2017-06-26 22:16:14,344 main.py:47] epoch 2628, training loss: 3488.93, average training loss: 3574.02, base loss: 4633.51
[INFO 2017-06-26 22:16:14,747 main.py:47] epoch 2629, training loss: 3435.00, average training loss: 3573.77, base loss: 4633.64
[INFO 2017-06-26 22:16:15,147 main.py:47] epoch 2630, training loss: 6780.60, average training loss: 3573.38, base loss: 4633.62
[INFO 2017-06-26 22:16:15,545 main.py:47] epoch 2631, training loss: 3264.18, average training loss: 3573.14, base loss: 4633.65
[INFO 2017-06-26 22:16:15,945 main.py:47] epoch 2632, training loss: 2858.75, average training loss: 3572.60, base loss: 4632.56
[INFO 2017-06-26 22:16:16,348 main.py:47] epoch 2633, training loss: 3297.22, average training loss: 3572.72, base loss: 4632.94
[INFO 2017-06-26 22:16:16,749 main.py:47] epoch 2634, training loss: 2821.27, average training loss: 3572.26, base loss: 4632.53
[INFO 2017-06-26 22:16:17,149 main.py:47] epoch 2635, training loss: 3402.54, average training loss: 3572.27, base loss: 4632.74
[INFO 2017-06-26 22:16:17,555 main.py:47] epoch 2636, training loss: 3417.82, average training loss: 3572.02, base loss: 4632.59
[INFO 2017-06-26 22:16:17,957 main.py:47] epoch 2637, training loss: 3377.40, average training loss: 3571.34, base loss: 4631.93
[INFO 2017-06-26 22:16:18,368 main.py:47] epoch 2638, training loss: 3652.10, average training loss: 3571.16, base loss: 4632.18
[INFO 2017-06-26 22:16:18,777 main.py:47] epoch 2639, training loss: 3327.75, average training loss: 3571.16, base loss: 4632.56
[INFO 2017-06-26 22:16:19,178 main.py:47] epoch 2640, training loss: 3624.63, average training loss: 3570.97, base loss: 4632.52
[INFO 2017-06-26 22:16:19,581 main.py:47] epoch 2641, training loss: 3537.07, average training loss: 3570.94, base loss: 4632.70
[INFO 2017-06-26 22:16:20,062 main.py:47] epoch 2642, training loss: 3475.01, average training loss: 3567.50, base loss: 4629.91
[INFO 2017-06-26 22:16:20,488 main.py:47] epoch 2643, training loss: 3397.90, average training loss: 3567.68, base loss: 4630.62
[INFO 2017-06-26 22:16:20,895 main.py:47] epoch 2644, training loss: 3356.88, average training loss: 3567.43, base loss: 4630.49
[INFO 2017-06-26 22:16:21,300 main.py:47] epoch 2645, training loss: 3002.84, average training loss: 3567.00, base loss: 4630.25
[INFO 2017-06-26 22:16:21,712 main.py:47] epoch 2646, training loss: 3505.03, average training loss: 3567.13, base loss: 4630.37
[INFO 2017-06-26 22:16:22,117 main.py:47] epoch 2647, training loss: 3704.28, average training loss: 3566.63, base loss: 4630.21
[INFO 2017-06-26 22:16:22,520 main.py:47] epoch 2648, training loss: 3267.14, average training loss: 3566.44, base loss: 4630.21
[INFO 2017-06-26 22:16:22,926 main.py:47] epoch 2649, training loss: 3453.74, average training loss: 3562.74, base loss: 4626.69
[INFO 2017-06-26 22:16:23,330 main.py:47] epoch 2650, training loss: 3160.18, average training loss: 3562.39, base loss: 4626.47
[INFO 2017-06-26 22:16:23,732 main.py:47] epoch 2651, training loss: 3183.97, average training loss: 3562.04, base loss: 4626.10
[INFO 2017-06-26 22:16:24,136 main.py:47] epoch 2652, training loss: 2810.79, average training loss: 3561.77, base loss: 4625.76
[INFO 2017-06-26 22:16:24,540 main.py:47] epoch 2653, training loss: 3032.82, average training loss: 3561.31, base loss: 4624.82
[INFO 2017-06-26 22:16:24,939 main.py:47] epoch 2654, training loss: 3149.56, average training loss: 3561.21, base loss: 4624.91
[INFO 2017-06-26 22:16:25,338 main.py:47] epoch 2655, training loss: 3385.26, average training loss: 3561.39, base loss: 4625.13
[INFO 2017-06-26 22:16:25,743 main.py:47] epoch 2656, training loss: 3310.17, average training loss: 3560.83, base loss: 4624.65
[INFO 2017-06-26 22:16:26,147 main.py:47] epoch 2657, training loss: 2972.90, average training loss: 3560.28, base loss: 4623.72
[INFO 2017-06-26 22:16:26,550 main.py:47] epoch 2658, training loss: 3520.79, average training loss: 3559.97, base loss: 4623.33
[INFO 2017-06-26 22:16:26,948 main.py:47] epoch 2659, training loss: 2792.13, average training loss: 3559.48, base loss: 4622.69
[INFO 2017-06-26 22:16:27,351 main.py:47] epoch 2660, training loss: 3542.80, average training loss: 3559.52, base loss: 4623.31
[INFO 2017-06-26 22:16:27,753 main.py:47] epoch 2661, training loss: 3455.02, average training loss: 3559.57, base loss: 4623.79
[INFO 2017-06-26 22:16:28,158 main.py:47] epoch 2662, training loss: 3012.67, average training loss: 3559.14, base loss: 4623.28
[INFO 2017-06-26 22:16:28,561 main.py:47] epoch 2663, training loss: 3367.24, average training loss: 3558.98, base loss: 4623.77
[INFO 2017-06-26 22:16:28,965 main.py:47] epoch 2664, training loss: 3579.93, average training loss: 3558.77, base loss: 4623.60
[INFO 2017-06-26 22:16:29,364 main.py:47] epoch 2665, training loss: 3482.48, average training loss: 3559.15, base loss: 4624.85
[INFO 2017-06-26 22:16:29,768 main.py:47] epoch 2666, training loss: 3685.13, average training loss: 3559.76, base loss: 4626.45
[INFO 2017-06-26 22:16:30,171 main.py:47] epoch 2667, training loss: 6294.90, average training loss: 3562.85, base loss: 4629.52
[INFO 2017-06-26 22:16:30,568 main.py:47] epoch 2668, training loss: 3777.67, average training loss: 3563.45, base loss: 4630.59
[INFO 2017-06-26 22:16:30,969 main.py:47] epoch 2669, training loss: 3331.76, average training loss: 3562.85, base loss: 4630.37
[INFO 2017-06-26 22:16:31,371 main.py:47] epoch 2670, training loss: 3022.45, average training loss: 3561.75, base loss: 4628.86
[INFO 2017-06-26 22:16:31,775 main.py:47] epoch 2671, training loss: 3382.65, average training loss: 3561.84, base loss: 4629.28
[INFO 2017-06-26 22:16:32,179 main.py:47] epoch 2672, training loss: 3769.93, average training loss: 3561.93, base loss: 4629.76
[INFO 2017-06-26 22:16:32,590 main.py:47] epoch 2673, training loss: 3567.95, average training loss: 3561.48, base loss: 4629.19
[INFO 2017-06-26 22:16:32,994 main.py:47] epoch 2674, training loss: 3410.38, average training loss: 3561.31, base loss: 4629.34
[INFO 2017-06-26 22:16:33,397 main.py:47] epoch 2675, training loss: 3216.42, average training loss: 3560.92, base loss: 4629.06
[INFO 2017-06-26 22:16:33,800 main.py:47] epoch 2676, training loss: 2963.22, average training loss: 3560.77, base loss: 4629.53
[INFO 2017-06-26 22:16:34,208 main.py:47] epoch 2677, training loss: 3500.19, average training loss: 3560.16, base loss: 4628.81
[INFO 2017-06-26 22:16:34,613 main.py:47] epoch 2678, training loss: 3211.85, average training loss: 3560.10, base loss: 4629.02
[INFO 2017-06-26 22:16:35,012 main.py:47] epoch 2679, training loss: 2867.36, average training loss: 3559.95, base loss: 4628.92
[INFO 2017-06-26 22:16:35,416 main.py:47] epoch 2680, training loss: 3263.13, average training loss: 3559.98, base loss: 4629.39
[INFO 2017-06-26 22:16:35,826 main.py:47] epoch 2681, training loss: 3338.78, average training loss: 3559.81, base loss: 4629.30
[INFO 2017-06-26 22:16:36,231 main.py:47] epoch 2682, training loss: 3616.91, average training loss: 3560.21, base loss: 4630.05
[INFO 2017-06-26 22:16:36,631 main.py:47] epoch 2683, training loss: 2999.23, average training loss: 3559.93, base loss: 4630.25
[INFO 2017-06-26 22:16:37,033 main.py:47] epoch 2684, training loss: 3356.59, average training loss: 3559.51, base loss: 4629.45
[INFO 2017-06-26 22:16:37,437 main.py:47] epoch 2685, training loss: 3394.00, average training loss: 3559.39, base loss: 4629.63
[INFO 2017-06-26 22:16:37,841 main.py:47] epoch 2686, training loss: 3448.08, average training loss: 3559.77, base loss: 4630.70
[INFO 2017-06-26 22:16:38,245 main.py:47] epoch 2687, training loss: 3091.29, average training loss: 3559.09, base loss: 4629.97
[INFO 2017-06-26 22:16:38,646 main.py:47] epoch 2688, training loss: 3266.59, average training loss: 3559.10, base loss: 4630.24
[INFO 2017-06-26 22:16:39,050 main.py:47] epoch 2689, training loss: 3285.99, average training loss: 3558.79, base loss: 4629.96
[INFO 2017-06-26 22:16:39,452 main.py:47] epoch 2690, training loss: 3377.46, average training loss: 3558.62, base loss: 4629.84
[INFO 2017-06-26 22:16:39,864 main.py:47] epoch 2691, training loss: 3221.11, average training loss: 3558.55, base loss: 4630.02
[INFO 2017-06-26 22:16:40,267 main.py:47] epoch 2692, training loss: 3435.69, average training loss: 3558.56, base loss: 4629.89
[INFO 2017-06-26 22:16:40,667 main.py:47] epoch 2693, training loss: 3095.76, average training loss: 3558.28, base loss: 4629.59
[INFO 2017-06-26 22:16:41,071 main.py:47] epoch 2694, training loss: 3690.78, average training loss: 3558.42, base loss: 4630.21
[INFO 2017-06-26 22:16:41,474 main.py:47] epoch 2695, training loss: 3307.12, average training loss: 3554.42, base loss: 4626.47
[INFO 2017-06-26 22:16:41,875 main.py:47] epoch 2696, training loss: 3526.72, average training loss: 3554.81, base loss: 4627.48
[INFO 2017-06-26 22:16:42,274 main.py:47] epoch 2697, training loss: 3238.08, average training loss: 3554.68, base loss: 4627.59
[INFO 2017-06-26 22:16:42,672 main.py:47] epoch 2698, training loss: 6508.60, average training loss: 3557.57, base loss: 4630.37
[INFO 2017-06-26 22:16:43,070 main.py:47] epoch 2699, training loss: 3007.30, average training loss: 3557.14, base loss: 4630.08
[INFO 2017-06-26 22:16:43,070 main.py:49] epoch 2699, testing
[INFO 2017-06-26 22:16:44,730 main.py:102] average testing loss: 3676.56, base loss: 4907.89
[INFO 2017-06-26 22:16:44,730 main.py:103] improve_loss: 1231.33, improve_percent: 0.25
[INFO 2017-06-26 22:16:44,730 main.py:73] current best improved percent: 0.25
[INFO 2017-06-26 22:16:45,135 main.py:47] epoch 2700, training loss: 3368.34, average training loss: 3557.11, base loss: 4630.76
[INFO 2017-06-26 22:16:45,542 main.py:47] epoch 2701, training loss: 3040.94, average training loss: 3556.45, base loss: 4629.71
[INFO 2017-06-26 22:16:45,942 main.py:47] epoch 2702, training loss: 2987.25, average training loss: 3555.72, base loss: 4628.87
[INFO 2017-06-26 22:16:46,343 main.py:47] epoch 2703, training loss: 3253.79, average training loss: 3556.04, base loss: 4629.93
[INFO 2017-06-26 22:16:46,743 main.py:47] epoch 2704, training loss: 3499.59, average training loss: 3556.00, base loss: 4630.47
[INFO 2017-06-26 22:16:47,147 main.py:47] epoch 2705, training loss: 3660.08, average training loss: 3556.62, base loss: 4632.01
[INFO 2017-06-26 22:16:47,557 main.py:47] epoch 2706, training loss: 3797.54, average training loss: 3557.06, base loss: 4633.20
[INFO 2017-06-26 22:16:47,954 main.py:47] epoch 2707, training loss: 3367.33, average training loss: 3556.77, base loss: 4633.07
[INFO 2017-06-26 22:16:48,359 main.py:47] epoch 2708, training loss: 3504.05, average training loss: 3557.06, base loss: 4633.82
[INFO 2017-06-26 22:16:48,763 main.py:47] epoch 2709, training loss: 3361.70, average training loss: 3556.76, base loss: 4633.46
[INFO 2017-06-26 22:16:49,172 main.py:47] epoch 2710, training loss: 3665.56, average training loss: 3556.84, base loss: 4634.06
[INFO 2017-06-26 22:16:49,581 main.py:47] epoch 2711, training loss: 3358.35, average training loss: 3556.44, base loss: 4633.83
[INFO 2017-06-26 22:16:49,985 main.py:47] epoch 2712, training loss: 3341.19, average training loss: 3556.77, base loss: 4634.88
[INFO 2017-06-26 22:16:50,384 main.py:47] epoch 2713, training loss: 6489.43, average training loss: 3559.52, base loss: 4637.70
[INFO 2017-06-26 22:16:50,789 main.py:47] epoch 2714, training loss: 2994.87, average training loss: 3558.68, base loss: 4636.49
[INFO 2017-06-26 22:16:51,193 main.py:47] epoch 2715, training loss: 3196.31, average training loss: 3558.48, base loss: 4636.76
[INFO 2017-06-26 22:16:51,598 main.py:47] epoch 2716, training loss: 3574.34, average training loss: 3558.56, base loss: 4636.88
[INFO 2017-06-26 22:16:52,000 main.py:47] epoch 2717, training loss: 2830.27, average training loss: 3557.67, base loss: 4635.93
[INFO 2017-06-26 22:16:52,402 main.py:47] epoch 2718, training loss: 3480.00, average training loss: 3557.27, base loss: 4635.82
[INFO 2017-06-26 22:16:52,806 main.py:47] epoch 2719, training loss: 3221.41, average training loss: 3556.81, base loss: 4635.40
[INFO 2017-06-26 22:16:53,208 main.py:47] epoch 2720, training loss: 3478.48, average training loss: 3557.28, base loss: 4636.53
[INFO 2017-06-26 22:16:53,611 main.py:47] epoch 2721, training loss: 7667.52, average training loss: 3561.31, base loss: 4641.96
[INFO 2017-06-26 22:16:54,014 main.py:47] epoch 2722, training loss: 3408.71, average training loss: 3561.45, base loss: 4642.57
[INFO 2017-06-26 22:16:54,417 main.py:47] epoch 2723, training loss: 3530.75, average training loss: 3561.33, base loss: 4642.34
[INFO 2017-06-26 22:16:54,820 main.py:47] epoch 2724, training loss: 3704.92, average training loss: 3561.74, base loss: 4643.59
[INFO 2017-06-26 22:16:55,224 main.py:47] epoch 2725, training loss: 3325.54, average training loss: 3561.45, base loss: 4643.34
[INFO 2017-06-26 22:16:55,627 main.py:47] epoch 2726, training loss: 3200.57, average training loss: 3561.46, base loss: 4643.89
[INFO 2017-06-26 22:16:56,029 main.py:47] epoch 2727, training loss: 3109.26, average training loss: 3561.17, base loss: 4643.13
[INFO 2017-06-26 22:16:56,433 main.py:47] epoch 2728, training loss: 3518.11, average training loss: 3561.18, base loss: 4643.64
[INFO 2017-06-26 22:16:56,835 main.py:47] epoch 2729, training loss: 6490.40, average training loss: 3564.27, base loss: 4647.13
[INFO 2017-06-26 22:16:57,238 main.py:47] epoch 2730, training loss: 3351.79, average training loss: 3564.52, base loss: 4647.61
[INFO 2017-06-26 22:16:57,641 main.py:47] epoch 2731, training loss: 3250.87, average training loss: 3564.17, base loss: 4647.56
[INFO 2017-06-26 22:16:58,040 main.py:47] epoch 2732, training loss: 3570.34, average training loss: 3564.52, base loss: 4648.38
[INFO 2017-06-26 22:16:58,444 main.py:47] epoch 2733, training loss: 3193.66, average training loss: 3564.30, base loss: 4648.08
[INFO 2017-06-26 22:16:58,841 main.py:47] epoch 2734, training loss: 3711.06, average training loss: 3564.03, base loss: 4648.33
[INFO 2017-06-26 22:16:59,238 main.py:47] epoch 2735, training loss: 3081.89, average training loss: 3560.14, base loss: 4644.53
[INFO 2017-06-26 22:16:59,635 main.py:47] epoch 2736, training loss: 3009.39, average training loss: 3559.55, base loss: 4643.90
[INFO 2017-06-26 22:17:00,034 main.py:47] epoch 2737, training loss: 2855.04, average training loss: 3558.87, base loss: 4643.22
[INFO 2017-06-26 22:17:00,434 main.py:47] epoch 2738, training loss: 3716.02, average training loss: 3555.84, base loss: 4640.68
[INFO 2017-06-26 22:17:00,834 main.py:47] epoch 2739, training loss: 3042.77, average training loss: 3555.11, base loss: 4639.66
[INFO 2017-06-26 22:17:01,237 main.py:47] epoch 2740, training loss: 3446.59, average training loss: 3555.48, base loss: 4640.30
[INFO 2017-06-26 22:17:01,638 main.py:47] epoch 2741, training loss: 3287.55, average training loss: 3555.19, base loss: 4639.77
[INFO 2017-06-26 22:17:02,040 main.py:47] epoch 2742, training loss: 3006.55, average training loss: 3554.42, base loss: 4638.83
[INFO 2017-06-26 22:17:02,443 main.py:47] epoch 2743, training loss: 2995.96, average training loss: 3553.75, base loss: 4637.84
[INFO 2017-06-26 22:17:02,846 main.py:47] epoch 2744, training loss: 2904.97, average training loss: 3552.88, base loss: 4636.45
[INFO 2017-06-26 22:17:03,248 main.py:47] epoch 2745, training loss: 3330.78, average training loss: 3552.37, base loss: 4636.27
[INFO 2017-06-26 22:17:03,650 main.py:47] epoch 2746, training loss: 3332.13, average training loss: 3548.53, base loss: 4632.41
[INFO 2017-06-26 22:17:04,052 main.py:47] epoch 2747, training loss: 2953.78, average training loss: 3548.30, base loss: 4632.50
[INFO 2017-06-26 22:17:04,449 main.py:47] epoch 2748, training loss: 3378.84, average training loss: 3548.65, base loss: 4633.38
[INFO 2017-06-26 22:17:04,851 main.py:47] epoch 2749, training loss: 3763.48, average training loss: 3548.72, base loss: 4633.69
[INFO 2017-06-26 22:17:05,253 main.py:47] epoch 2750, training loss: 3164.67, average training loss: 3548.40, base loss: 4633.29
[INFO 2017-06-26 22:17:05,654 main.py:47] epoch 2751, training loss: 2984.31, average training loss: 3548.06, base loss: 4633.13
[INFO 2017-06-26 22:17:06,057 main.py:47] epoch 2752, training loss: 3285.02, average training loss: 3548.02, base loss: 4633.47
[INFO 2017-06-26 22:17:06,460 main.py:47] epoch 2753, training loss: 2947.46, average training loss: 3547.69, base loss: 4632.95
[INFO 2017-06-26 22:17:06,863 main.py:47] epoch 2754, training loss: 3229.21, average training loss: 3547.90, base loss: 4633.06
[INFO 2017-06-26 22:17:07,265 main.py:47] epoch 2755, training loss: 4014.15, average training loss: 3548.30, base loss: 4634.21
[INFO 2017-06-26 22:17:07,671 main.py:47] epoch 2756, training loss: 3507.21, average training loss: 3548.32, base loss: 4634.75
[INFO 2017-06-26 22:17:08,072 main.py:47] epoch 2757, training loss: 3699.00, average training loss: 3548.93, base loss: 4635.81
[INFO 2017-06-26 22:17:08,475 main.py:47] epoch 2758, training loss: 3146.32, average training loss: 3548.75, base loss: 4635.78
[INFO 2017-06-26 22:17:08,879 main.py:47] epoch 2759, training loss: 3108.51, average training loss: 3548.52, base loss: 4635.65
[INFO 2017-06-26 22:17:09,284 main.py:47] epoch 2760, training loss: 3314.01, average training loss: 3547.60, base loss: 4634.24
[INFO 2017-06-26 22:17:09,682 main.py:47] epoch 2761, training loss: 3355.36, average training loss: 3547.32, base loss: 4633.43
[INFO 2017-06-26 22:17:10,085 main.py:47] epoch 2762, training loss: 3555.89, average training loss: 3547.28, base loss: 4633.66
[INFO 2017-06-26 22:17:10,484 main.py:47] epoch 2763, training loss: 3200.87, average training loss: 3543.13, base loss: 4629.70
[INFO 2017-06-26 22:17:10,887 main.py:47] epoch 2764, training loss: 3352.30, average training loss: 3543.19, base loss: 4629.93
[INFO 2017-06-26 22:17:11,288 main.py:47] epoch 2765, training loss: 3324.32, average training loss: 3542.61, base loss: 4629.04
[INFO 2017-06-26 22:17:11,686 main.py:47] epoch 2766, training loss: 2948.62, average training loss: 3541.76, base loss: 4627.77
[INFO 2017-06-26 22:17:12,089 main.py:47] epoch 2767, training loss: 3104.22, average training loss: 3541.38, base loss: 4627.38
[INFO 2017-06-26 22:17:12,492 main.py:47] epoch 2768, training loss: 2975.51, average training loss: 3540.72, base loss: 4626.80
[INFO 2017-06-26 22:17:12,895 main.py:47] epoch 2769, training loss: 3608.80, average training loss: 3540.54, base loss: 4626.67
[INFO 2017-06-26 22:17:13,293 main.py:47] epoch 2770, training loss: 2914.40, average training loss: 3539.83, base loss: 4625.86
[INFO 2017-06-26 22:17:13,693 main.py:47] epoch 2771, training loss: 3464.09, average training loss: 3540.17, base loss: 4627.05
[INFO 2017-06-26 22:17:14,093 main.py:47] epoch 2772, training loss: 3183.99, average training loss: 3539.77, base loss: 4626.38
[INFO 2017-06-26 22:17:14,495 main.py:47] epoch 2773, training loss: 3060.15, average training loss: 3539.11, base loss: 4625.38
[INFO 2017-06-26 22:17:14,897 main.py:47] epoch 2774, training loss: 3502.36, average training loss: 3538.96, base loss: 4625.57
[INFO 2017-06-26 22:17:15,300 main.py:47] epoch 2775, training loss: 3390.62, average training loss: 3538.59, base loss: 4625.39
[INFO 2017-06-26 22:17:15,699 main.py:47] epoch 2776, training loss: 3292.36, average training loss: 3538.27, base loss: 4624.92
[INFO 2017-06-26 22:17:16,098 main.py:47] epoch 2777, training loss: 3045.62, average training loss: 3537.96, base loss: 4624.73
[INFO 2017-06-26 22:17:16,498 main.py:47] epoch 2778, training loss: 2703.63, average training loss: 3536.61, base loss: 4622.81
[INFO 2017-06-26 22:17:16,896 main.py:47] epoch 2779, training loss: 3441.58, average training loss: 3537.14, base loss: 4624.08
[INFO 2017-06-26 22:17:17,299 main.py:47] epoch 2780, training loss: 3452.81, average training loss: 3537.14, base loss: 4624.32
[INFO 2017-06-26 22:17:17,705 main.py:47] epoch 2781, training loss: 3357.82, average training loss: 3536.99, base loss: 4624.11
[INFO 2017-06-26 22:17:18,107 main.py:47] epoch 2782, training loss: 3086.62, average training loss: 3536.86, base loss: 4623.97
[INFO 2017-06-26 22:17:18,511 main.py:47] epoch 2783, training loss: 3611.80, average training loss: 3537.13, base loss: 4624.42
[INFO 2017-06-26 22:17:18,913 main.py:47] epoch 2784, training loss: 3278.56, average training loss: 3537.15, base loss: 4624.80
[INFO 2017-06-26 22:17:19,315 main.py:47] epoch 2785, training loss: 3221.09, average training loss: 3536.96, base loss: 4624.87
[INFO 2017-06-26 22:17:19,712 main.py:47] epoch 2786, training loss: 3638.36, average training loss: 3537.00, base loss: 4625.43
[INFO 2017-06-26 22:17:20,111 main.py:47] epoch 2787, training loss: 3366.22, average training loss: 3536.74, base loss: 4625.34
[INFO 2017-06-26 22:17:20,518 main.py:47] epoch 2788, training loss: 3368.32, average training loss: 3532.98, base loss: 4621.57
[INFO 2017-06-26 22:17:20,917 main.py:47] epoch 2789, training loss: 3348.39, average training loss: 3532.85, base loss: 4621.99
[INFO 2017-06-26 22:17:21,318 main.py:47] epoch 2790, training loss: 2784.20, average training loss: 3531.86, base loss: 4620.38
[INFO 2017-06-26 22:17:21,715 main.py:47] epoch 2791, training loss: 3260.60, average training loss: 3531.45, base loss: 4619.62
[INFO 2017-06-26 22:17:22,117 main.py:47] epoch 2792, training loss: 3130.11, average training loss: 3531.25, base loss: 4619.59
[INFO 2017-06-26 22:17:22,515 main.py:47] epoch 2793, training loss: 3464.59, average training loss: 3531.26, base loss: 4620.05
[INFO 2017-06-26 22:17:22,916 main.py:47] epoch 2794, training loss: 3367.39, average training loss: 3530.77, base loss: 4619.69
[INFO 2017-06-26 22:17:23,318 main.py:47] epoch 2795, training loss: 2741.68, average training loss: 3526.45, base loss: 4615.17
[INFO 2017-06-26 22:17:23,717 main.py:47] epoch 2796, training loss: 3017.17, average training loss: 3526.54, base loss: 4615.47
[INFO 2017-06-26 22:17:24,121 main.py:47] epoch 2797, training loss: 3123.66, average training loss: 3526.01, base loss: 4615.20
[INFO 2017-06-26 22:17:24,524 main.py:47] epoch 2798, training loss: 3261.99, average training loss: 3526.28, base loss: 4615.71
[INFO 2017-06-26 22:17:24,927 main.py:47] epoch 2799, training loss: 3227.81, average training loss: 3526.41, base loss: 4616.27
[INFO 2017-06-26 22:17:24,927 main.py:49] epoch 2799, testing
[INFO 2017-06-26 22:17:26,596 main.py:102] average testing loss: 3241.48, base loss: 4312.43
[INFO 2017-06-26 22:17:26,597 main.py:103] improve_loss: 1070.95, improve_percent: 0.25
[INFO 2017-06-26 22:17:26,597 main.py:73] current best improved percent: 0.25
[INFO 2017-06-26 22:17:26,999 main.py:47] epoch 2800, training loss: 2973.59, average training loss: 3525.28, base loss: 4614.62
[INFO 2017-06-26 22:17:27,399 main.py:47] epoch 2801, training loss: 3310.21, average training loss: 3524.83, base loss: 4614.23
[INFO 2017-06-26 22:17:27,801 main.py:47] epoch 2802, training loss: 3204.52, average training loss: 3524.34, base loss: 4613.65
[INFO 2017-06-26 22:17:28,203 main.py:47] epoch 2803, training loss: 3190.62, average training loss: 3524.29, base loss: 4614.03
[INFO 2017-06-26 22:17:28,605 main.py:47] epoch 2804, training loss: 3626.55, average training loss: 3524.25, base loss: 4614.35
[INFO 2017-06-26 22:17:29,007 main.py:47] epoch 2805, training loss: 3061.00, average training loss: 3523.85, base loss: 4614.33
[INFO 2017-06-26 22:17:29,409 main.py:47] epoch 2806, training loss: 3452.64, average training loss: 3524.00, base loss: 4615.00
[INFO 2017-06-26 22:17:29,813 main.py:47] epoch 2807, training loss: 2990.68, average training loss: 3523.68, base loss: 4614.53
[INFO 2017-06-26 22:17:30,210 main.py:47] epoch 2808, training loss: 3224.73, average training loss: 3523.50, base loss: 4614.73
[INFO 2017-06-26 22:17:30,607 main.py:47] epoch 2809, training loss: 3021.10, average training loss: 3523.58, base loss: 4614.90
[INFO 2017-06-26 22:17:31,008 main.py:47] epoch 2810, training loss: 3068.49, average training loss: 3523.39, base loss: 4614.65
[INFO 2017-06-26 22:17:31,410 main.py:47] epoch 2811, training loss: 3353.76, average training loss: 3522.98, base loss: 4614.12
[INFO 2017-06-26 22:17:31,813 main.py:47] epoch 2812, training loss: 3756.74, average training loss: 3523.30, base loss: 4615.51
[INFO 2017-06-26 22:17:32,214 main.py:47] epoch 2813, training loss: 3360.37, average training loss: 3522.63, base loss: 4615.02
[INFO 2017-06-26 22:17:32,613 main.py:47] epoch 2814, training loss: 3349.77, average training loss: 3522.43, base loss: 4614.65
[INFO 2017-06-26 22:17:33,015 main.py:47] epoch 2815, training loss: 3415.83, average training loss: 3522.07, base loss: 4614.44
[INFO 2017-06-26 22:17:33,417 main.py:47] epoch 2816, training loss: 3424.41, average training loss: 3521.97, base loss: 4614.75
[INFO 2017-06-26 22:17:33,819 main.py:47] epoch 2817, training loss: 3697.83, average training loss: 3522.05, base loss: 4615.24
[INFO 2017-06-26 22:17:34,219 main.py:47] epoch 2818, training loss: 3318.50, average training loss: 3518.46, base loss: 4611.91
[INFO 2017-06-26 22:17:34,619 main.py:47] epoch 2819, training loss: 3536.37, average training loss: 3518.42, base loss: 4612.42
[INFO 2017-06-26 22:17:35,021 main.py:47] epoch 2820, training loss: 3333.69, average training loss: 3518.56, base loss: 4612.87
[INFO 2017-06-26 22:17:35,424 main.py:47] epoch 2821, training loss: 3255.60, average training loss: 3518.59, base loss: 4613.30
[INFO 2017-06-26 22:17:35,826 main.py:47] epoch 2822, training loss: 3670.58, average training loss: 3518.91, base loss: 4613.99
[INFO 2017-06-26 22:17:36,225 main.py:47] epoch 2823, training loss: 3355.06, average training loss: 3518.65, base loss: 4614.17
[INFO 2017-06-26 22:17:36,624 main.py:47] epoch 2824, training loss: 3154.04, average training loss: 3518.44, base loss: 4614.01
[INFO 2017-06-26 22:17:37,022 main.py:47] epoch 2825, training loss: 3194.04, average training loss: 3518.32, base loss: 4614.33
[INFO 2017-06-26 22:17:37,422 main.py:47] epoch 2826, training loss: 6929.35, average training loss: 3521.28, base loss: 4617.75
[INFO 2017-06-26 22:17:37,824 main.py:47] epoch 2827, training loss: 3347.77, average training loss: 3521.42, base loss: 4618.42
[INFO 2017-06-26 22:17:38,221 main.py:47] epoch 2828, training loss: 3098.57, average training loss: 3520.75, base loss: 4617.73
[INFO 2017-06-26 22:17:38,618 main.py:47] epoch 2829, training loss: 2993.78, average training loss: 3519.70, base loss: 4616.46
[INFO 2017-06-26 22:17:39,021 main.py:47] epoch 2830, training loss: 3331.21, average training loss: 3519.25, base loss: 4615.74
[INFO 2017-06-26 22:17:39,424 main.py:47] epoch 2831, training loss: 3589.10, average training loss: 3519.62, base loss: 4616.74
[INFO 2017-06-26 22:17:39,830 main.py:47] epoch 2832, training loss: 3175.78, average training loss: 3519.33, base loss: 4616.50
[INFO 2017-06-26 22:17:40,232 main.py:47] epoch 2833, training loss: 3371.90, average training loss: 3519.44, base loss: 4616.57
[INFO 2017-06-26 22:17:40,629 main.py:47] epoch 2834, training loss: 3084.62, average training loss: 3519.57, base loss: 4616.98
[INFO 2017-06-26 22:17:41,028 main.py:47] epoch 2835, training loss: 3439.73, average training loss: 3519.81, base loss: 4617.85
[INFO 2017-06-26 22:17:41,427 main.py:47] epoch 2836, training loss: 3037.90, average training loss: 3518.91, base loss: 4616.63
[INFO 2017-06-26 22:17:41,834 main.py:47] epoch 2837, training loss: 3442.45, average training loss: 3519.41, base loss: 4617.82
[INFO 2017-06-26 22:17:42,231 main.py:47] epoch 2838, training loss: 3212.89, average training loss: 3519.27, base loss: 4617.97
[INFO 2017-06-26 22:17:42,629 main.py:47] epoch 2839, training loss: 3333.74, average training loss: 3515.49, base loss: 4614.56
[INFO 2017-06-26 22:17:43,027 main.py:47] epoch 2840, training loss: 3572.51, average training loss: 3515.36, base loss: 4614.99
[INFO 2017-06-26 22:17:43,430 main.py:47] epoch 2841, training loss: 3636.33, average training loss: 3515.42, base loss: 4615.72
[INFO 2017-06-26 22:17:43,832 main.py:47] epoch 2842, training loss: 3323.74, average training loss: 3515.32, base loss: 4615.83
[INFO 2017-06-26 22:17:44,236 main.py:47] epoch 2843, training loss: 3512.62, average training loss: 3515.79, base loss: 4617.34
[INFO 2017-06-26 22:17:44,641 main.py:47] epoch 2844, training loss: 3530.69, average training loss: 3516.38, base loss: 4618.80
[INFO 2017-06-26 22:17:45,044 main.py:47] epoch 2845, training loss: 2980.30, average training loss: 3515.09, base loss: 4617.01
[INFO 2017-06-26 22:17:45,453 main.py:47] epoch 2846, training loss: 3271.40, average training loss: 3515.14, base loss: 4617.80
[INFO 2017-06-26 22:17:45,856 main.py:47] epoch 2847, training loss: 3253.21, average training loss: 3514.80, base loss: 4617.70
[INFO 2017-06-26 22:17:46,255 main.py:47] epoch 2848, training loss: 3916.62, average training loss: 3515.70, base loss: 4619.40
[INFO 2017-06-26 22:17:46,658 main.py:47] epoch 2849, training loss: 3577.50, average training loss: 3515.45, base loss: 4619.61
[INFO 2017-06-26 22:17:47,060 main.py:47] epoch 2850, training loss: 3104.23, average training loss: 3515.41, base loss: 4619.81
[INFO 2017-06-26 22:17:47,463 main.py:47] epoch 2851, training loss: 3450.25, average training loss: 3515.87, base loss: 4621.11
[INFO 2017-06-26 22:17:47,869 main.py:47] epoch 2852, training loss: 6766.04, average training loss: 3519.57, base loss: 4625.27
[INFO 2017-06-26 22:17:48,270 main.py:47] epoch 2853, training loss: 3632.05, average training loss: 3519.57, base loss: 4625.91
[INFO 2017-06-26 22:17:48,667 main.py:47] epoch 2854, training loss: 3331.83, average training loss: 3519.52, base loss: 4626.50
[INFO 2017-06-26 22:17:49,064 main.py:47] epoch 2855, training loss: 3181.00, average training loss: 3518.87, base loss: 4626.23
[INFO 2017-06-26 22:17:49,464 main.py:47] epoch 2856, training loss: 3246.22, average training loss: 3518.64, base loss: 4625.92
[INFO 2017-06-26 22:17:49,871 main.py:47] epoch 2857, training loss: 6799.86, average training loss: 3521.66, base loss: 4629.24
[INFO 2017-06-26 22:17:50,272 main.py:47] epoch 2858, training loss: 3365.58, average training loss: 3521.32, base loss: 4628.84
[INFO 2017-06-26 22:17:50,673 main.py:47] epoch 2859, training loss: 3346.21, average training loss: 3521.37, base loss: 4629.29
[INFO 2017-06-26 22:17:51,072 main.py:47] epoch 2860, training loss: 3323.72, average training loss: 3521.35, base loss: 4629.54
[INFO 2017-06-26 22:17:51,477 main.py:47] epoch 2861, training loss: 2812.45, average training loss: 3520.39, base loss: 4628.19
[INFO 2017-06-26 22:17:51,883 main.py:47] epoch 2862, training loss: 3538.87, average training loss: 3519.89, base loss: 4627.63
[INFO 2017-06-26 22:17:52,293 main.py:47] epoch 2863, training loss: 3290.76, average training loss: 3519.94, base loss: 4628.09
[INFO 2017-06-26 22:17:52,702 main.py:47] epoch 2864, training loss: 3582.55, average training loss: 3519.75, base loss: 4628.14
[INFO 2017-06-26 22:17:53,112 main.py:47] epoch 2865, training loss: 6791.20, average training loss: 3522.68, base loss: 4631.56
[INFO 2017-06-26 22:17:53,516 main.py:47] epoch 2866, training loss: 3055.75, average training loss: 3522.55, base loss: 4631.31
[INFO 2017-06-26 22:17:54,027 main.py:47] epoch 2867, training loss: 3185.83, average training loss: 3522.23, base loss: 4631.03
[INFO 2017-06-26 22:17:54,437 main.py:47] epoch 2868, training loss: 2936.23, average training loss: 3521.22, base loss: 4629.51
[INFO 2017-06-26 22:17:54,845 main.py:47] epoch 2869, training loss: 3271.32, average training loss: 3521.10, base loss: 4629.36
[INFO 2017-06-26 22:17:55,248 main.py:47] epoch 2870, training loss: 3455.67, average training loss: 3521.10, base loss: 4629.51
[INFO 2017-06-26 22:17:55,665 main.py:47] epoch 2871, training loss: 3497.64, average training loss: 3520.83, base loss: 4629.23
[INFO 2017-06-26 22:17:56,069 main.py:47] epoch 2872, training loss: 3101.98, average training loss: 3519.85, base loss: 4627.85
[INFO 2017-06-26 22:17:56,478 main.py:47] epoch 2873, training loss: 3126.64, average training loss: 3519.90, base loss: 4628.37
[INFO 2017-06-26 22:17:56,881 main.py:47] epoch 2874, training loss: 3266.22, average training loss: 3519.86, base loss: 4628.88
[INFO 2017-06-26 22:17:57,283 main.py:47] epoch 2875, training loss: 3175.40, average training loss: 3519.39, base loss: 4628.63
[INFO 2017-06-26 22:17:57,689 main.py:47] epoch 2876, training loss: 6477.75, average training loss: 3522.71, base loss: 4632.68
[INFO 2017-06-26 22:17:58,087 main.py:47] epoch 2877, training loss: 3267.04, average training loss: 3522.55, base loss: 4632.63
[INFO 2017-06-26 22:17:58,490 main.py:47] epoch 2878, training loss: 3246.55, average training loss: 3521.86, base loss: 4632.04
[INFO 2017-06-26 22:17:58,888 main.py:47] epoch 2879, training loss: 3287.35, average training loss: 3521.59, base loss: 4632.14
[INFO 2017-06-26 22:17:59,289 main.py:47] epoch 2880, training loss: 3038.81, average training loss: 3521.60, base loss: 4632.30
[INFO 2017-06-26 22:17:59,693 main.py:47] epoch 2881, training loss: 2801.97, average training loss: 3520.80, base loss: 4631.16
[INFO 2017-06-26 22:18:00,101 main.py:47] epoch 2882, training loss: 3103.82, average training loss: 3520.41, base loss: 4630.76
[INFO 2017-06-26 22:18:00,506 main.py:47] epoch 2883, training loss: 2888.61, average training loss: 3520.11, base loss: 4630.70
[INFO 2017-06-26 22:18:00,908 main.py:47] epoch 2884, training loss: 6507.97, average training loss: 3523.36, base loss: 4634.86
[INFO 2017-06-26 22:18:01,314 main.py:47] epoch 2885, training loss: 3213.10, average training loss: 3523.11, base loss: 4634.97
[INFO 2017-06-26 22:18:01,716 main.py:47] epoch 2886, training loss: 3012.33, average training loss: 3522.95, base loss: 4634.98
[INFO 2017-06-26 22:18:02,115 main.py:47] epoch 2887, training loss: 3103.67, average training loss: 3522.24, base loss: 4633.77
[INFO 2017-06-26 22:18:02,518 main.py:47] epoch 2888, training loss: 3004.53, average training loss: 3522.20, base loss: 4633.76
[INFO 2017-06-26 22:18:02,922 main.py:47] epoch 2889, training loss: 6967.21, average training loss: 3525.39, base loss: 4636.71
[INFO 2017-06-26 22:18:03,322 main.py:47] epoch 2890, training loss: 3020.03, average training loss: 3525.02, base loss: 4636.47
[INFO 2017-06-26 22:18:03,729 main.py:47] epoch 2891, training loss: 3090.67, average training loss: 3524.78, base loss: 4636.20
[INFO 2017-06-26 22:18:04,130 main.py:47] epoch 2892, training loss: 3280.73, average training loss: 3524.33, base loss: 4635.45
[INFO 2017-06-26 22:18:04,529 main.py:47] epoch 2893, training loss: 3200.95, average training loss: 3524.04, base loss: 4634.96
[INFO 2017-06-26 22:18:04,931 main.py:47] epoch 2894, training loss: 3293.06, average training loss: 3524.02, base loss: 4635.01
[INFO 2017-06-26 22:18:05,335 main.py:47] epoch 2895, training loss: 3057.08, average training loss: 3523.34, base loss: 4634.14
[INFO 2017-06-26 22:18:05,742 main.py:47] epoch 2896, training loss: 3809.51, average training loss: 3523.63, base loss: 4635.30
[INFO 2017-06-26 22:18:06,144 main.py:47] epoch 2897, training loss: 2953.11, average training loss: 3523.34, base loss: 4635.15
[INFO 2017-06-26 22:18:06,547 main.py:47] epoch 2898, training loss: 3567.10, average training loss: 3523.76, base loss: 4635.78
[INFO 2017-06-26 22:18:06,951 main.py:47] epoch 2899, training loss: 3464.75, average training loss: 3524.15, base loss: 4637.49
[INFO 2017-06-26 22:18:06,952 main.py:49] epoch 2899, testing
[INFO 2017-06-26 22:18:08,612 main.py:102] average testing loss: 3242.60, base loss: 4428.48
[INFO 2017-06-26 22:18:08,612 main.py:103] improve_loss: 1185.87, improve_percent: 0.27
[INFO 2017-06-26 22:18:08,612 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 22:18:08,625 main.py:73] current best improved percent: 0.27
[INFO 2017-06-26 22:18:09,030 main.py:47] epoch 2900, training loss: 3366.45, average training loss: 3523.81, base loss: 4637.12
[INFO 2017-06-26 22:18:09,435 main.py:47] epoch 2901, training loss: 3215.97, average training loss: 3523.36, base loss: 4636.42
[INFO 2017-06-26 22:18:09,838 main.py:47] epoch 2902, training loss: 3633.30, average training loss: 3523.58, base loss: 4637.41
[INFO 2017-06-26 22:18:10,242 main.py:47] epoch 2903, training loss: 3019.41, average training loss: 3523.15, base loss: 4636.67
[INFO 2017-06-26 22:18:10,646 main.py:47] epoch 2904, training loss: 3673.47, average training loss: 3523.22, base loss: 4637.45
[INFO 2017-06-26 22:18:11,050 main.py:47] epoch 2905, training loss: 3411.73, average training loss: 3523.09, base loss: 4637.96
[INFO 2017-06-26 22:18:11,452 main.py:47] epoch 2906, training loss: 6435.86, average training loss: 3526.45, base loss: 4642.02
[INFO 2017-06-26 22:18:11,854 main.py:47] epoch 2907, training loss: 3149.85, average training loss: 3526.09, base loss: 4641.77
[INFO 2017-06-26 22:18:12,256 main.py:47] epoch 2908, training loss: 3043.76, average training loss: 3526.04, base loss: 4641.92
[INFO 2017-06-26 22:18:12,659 main.py:47] epoch 2909, training loss: 3487.01, average training loss: 3526.31, base loss: 4642.75
[INFO 2017-06-26 22:18:13,065 main.py:47] epoch 2910, training loss: 3663.62, average training loss: 3525.93, base loss: 4642.65
[INFO 2017-06-26 22:18:13,478 main.py:47] epoch 2911, training loss: 4040.08, average training loss: 3526.13, base loss: 4643.54
[INFO 2017-06-26 22:18:13,881 main.py:47] epoch 2912, training loss: 3395.15, average training loss: 3525.70, base loss: 4643.16
[INFO 2017-06-26 22:18:14,290 main.py:47] epoch 2913, training loss: 3465.94, average training loss: 3525.55, base loss: 4643.27
[INFO 2017-06-26 22:18:14,699 main.py:47] epoch 2914, training loss: 3320.99, average training loss: 3525.49, base loss: 4643.17
[INFO 2017-06-26 22:18:15,102 main.py:47] epoch 2915, training loss: 3261.78, average training loss: 3524.98, base loss: 4642.55
[INFO 2017-06-26 22:18:15,509 main.py:47] epoch 2916, training loss: 2711.07, average training loss: 3523.80, base loss: 4640.87
[INFO 2017-06-26 22:18:15,914 main.py:47] epoch 2917, training loss: 3446.33, average training loss: 3524.36, base loss: 4642.43
[INFO 2017-06-26 22:18:16,337 main.py:47] epoch 2918, training loss: 3288.65, average training loss: 3524.28, base loss: 4642.99
[INFO 2017-06-26 22:18:16,743 main.py:47] epoch 2919, training loss: 3764.08, average training loss: 3524.68, base loss: 4644.17
[INFO 2017-06-26 22:18:17,151 main.py:47] epoch 2920, training loss: 3039.36, average training loss: 3523.65, base loss: 4642.79
[INFO 2017-06-26 22:18:17,571 main.py:47] epoch 2921, training loss: 2778.30, average training loss: 3523.30, base loss: 4642.21
[INFO 2017-06-26 22:18:17,975 main.py:47] epoch 2922, training loss: 3461.53, average training loss: 3523.73, base loss: 4643.29
[INFO 2017-06-26 22:18:18,388 main.py:47] epoch 2923, training loss: 3073.38, average training loss: 3523.14, base loss: 4642.28
[INFO 2017-06-26 22:18:18,792 main.py:47] epoch 2924, training loss: 3491.18, average training loss: 3523.05, base loss: 4642.59
[INFO 2017-06-26 22:18:19,205 main.py:47] epoch 2925, training loss: 3046.37, average training loss: 3522.59, base loss: 4642.06
[INFO 2017-06-26 22:18:19,616 main.py:47] epoch 2926, training loss: 6613.79, average training loss: 3525.50, base loss: 4645.54
[INFO 2017-06-26 22:18:20,024 main.py:47] epoch 2927, training loss: 3375.09, average training loss: 3525.42, base loss: 4646.06
[INFO 2017-06-26 22:18:20,440 main.py:47] epoch 2928, training loss: 3308.80, average training loss: 3525.02, base loss: 4646.29
[INFO 2017-06-26 22:18:20,914 main.py:47] epoch 2929, training loss: 3024.47, average training loss: 3524.40, base loss: 4645.33
[INFO 2017-06-26 22:18:21,395 main.py:47] epoch 2930, training loss: 3097.09, average training loss: 3523.82, base loss: 4644.65
[INFO 2017-06-26 22:18:21,888 main.py:47] epoch 2931, training loss: 3835.73, average training loss: 3524.16, base loss: 4645.26
[INFO 2017-06-26 22:18:22,349 main.py:47] epoch 2932, training loss: 3189.70, average training loss: 3523.54, base loss: 4644.73
[INFO 2017-06-26 22:18:22,806 main.py:47] epoch 2933, training loss: 3432.47, average training loss: 3523.36, base loss: 4645.20
[INFO 2017-06-26 22:18:23,250 main.py:47] epoch 2934, training loss: 3356.68, average training loss: 3523.03, base loss: 4644.82
[INFO 2017-06-26 22:18:23,760 main.py:47] epoch 2935, training loss: 3168.76, average training loss: 3522.54, base loss: 4644.25
[INFO 2017-06-26 22:18:24,227 main.py:47] epoch 2936, training loss: 3488.65, average training loss: 3522.28, base loss: 4644.25
[INFO 2017-06-26 22:18:24,639 main.py:47] epoch 2937, training loss: 3307.88, average training loss: 3522.32, base loss: 4644.67
[INFO 2017-06-26 22:18:25,044 main.py:47] epoch 2938, training loss: 3125.82, average training loss: 3521.77, base loss: 4644.41
[INFO 2017-06-26 22:18:25,447 main.py:47] epoch 2939, training loss: 3407.51, average training loss: 3521.67, base loss: 4644.76
[INFO 2017-06-26 22:18:25,881 main.py:47] epoch 2940, training loss: 3601.12, average training loss: 3521.48, base loss: 4644.54
[INFO 2017-06-26 22:18:26,290 main.py:47] epoch 2941, training loss: 3118.83, average training loss: 3521.46, base loss: 4644.63
[INFO 2017-06-26 22:18:26,686 main.py:47] epoch 2942, training loss: 3404.15, average training loss: 3521.31, base loss: 4644.82
[INFO 2017-06-26 22:18:27,089 main.py:47] epoch 2943, training loss: 3094.15, average training loss: 3521.16, base loss: 4644.75
[INFO 2017-06-26 22:18:27,491 main.py:47] epoch 2944, training loss: 3371.44, average training loss: 3521.35, base loss: 4645.07
[INFO 2017-06-26 22:18:27,892 main.py:47] epoch 2945, training loss: 3197.89, average training loss: 3521.38, base loss: 4645.60
[INFO 2017-06-26 22:18:28,294 main.py:47] epoch 2946, training loss: 2692.07, average training loss: 3520.61, base loss: 4644.40
[INFO 2017-06-26 22:18:28,692 main.py:47] epoch 2947, training loss: 6794.58, average training loss: 3524.50, base loss: 4648.63
[INFO 2017-06-26 22:18:29,094 main.py:47] epoch 2948, training loss: 3296.22, average training loss: 3524.54, base loss: 4649.17
[INFO 2017-06-26 22:18:29,493 main.py:47] epoch 2949, training loss: 3118.16, average training loss: 3524.67, base loss: 4649.83
[INFO 2017-06-26 22:18:29,891 main.py:47] epoch 2950, training loss: 3666.67, average training loss: 3525.12, base loss: 4651.22
[INFO 2017-06-26 22:18:30,290 main.py:47] epoch 2951, training loss: 2977.11, average training loss: 3524.86, base loss: 4651.18
[INFO 2017-06-26 22:18:30,688 main.py:47] epoch 2952, training loss: 3112.47, average training loss: 3524.81, base loss: 4651.18
[INFO 2017-06-26 22:18:31,088 main.py:47] epoch 2953, training loss: 3262.98, average training loss: 3524.81, base loss: 4651.48
[INFO 2017-06-26 22:18:31,490 main.py:47] epoch 2954, training loss: 3781.80, average training loss: 3525.48, base loss: 4653.12
[INFO 2017-06-26 22:18:31,892 main.py:47] epoch 2955, training loss: 3278.05, average training loss: 3525.05, base loss: 4652.49
[INFO 2017-06-26 22:18:32,301 main.py:47] epoch 2956, training loss: 3572.55, average training loss: 3525.58, base loss: 4653.61
[INFO 2017-06-26 22:18:32,708 main.py:47] epoch 2957, training loss: 3267.02, average training loss: 3525.49, base loss: 4654.01
[INFO 2017-06-26 22:18:33,111 main.py:47] epoch 2958, training loss: 2836.11, average training loss: 3525.27, base loss: 4653.85
[INFO 2017-06-26 22:18:33,516 main.py:47] epoch 2959, training loss: 3559.56, average training loss: 3525.73, base loss: 4654.59
[INFO 2017-06-26 22:18:33,978 main.py:47] epoch 2960, training loss: 3414.13, average training loss: 3525.85, base loss: 4654.75
[INFO 2017-06-26 22:18:34,382 main.py:47] epoch 2961, training loss: 3476.62, average training loss: 3525.78, base loss: 4654.65
[INFO 2017-06-26 22:18:34,790 main.py:47] epoch 2962, training loss: 3379.80, average training loss: 3525.83, base loss: 4654.88
[INFO 2017-06-26 22:18:35,206 main.py:47] epoch 2963, training loss: 3248.12, average training loss: 3525.44, base loss: 4654.45
[INFO 2017-06-26 22:18:35,612 main.py:47] epoch 2964, training loss: 3411.44, average training loss: 3525.25, base loss: 4654.28
[INFO 2017-06-26 22:18:36,012 main.py:47] epoch 2965, training loss: 3114.20, average training loss: 3524.78, base loss: 4654.10
[INFO 2017-06-26 22:18:36,423 main.py:47] epoch 2966, training loss: 3164.03, average training loss: 3524.19, base loss: 4653.48
[INFO 2017-06-26 22:18:36,824 main.py:47] epoch 2967, training loss: 3263.70, average training loss: 3524.55, base loss: 4654.65
[INFO 2017-06-26 22:18:37,224 main.py:47] epoch 2968, training loss: 3132.75, average training loss: 3524.90, base loss: 4655.44
[INFO 2017-06-26 22:18:37,626 main.py:47] epoch 2969, training loss: 3761.95, average training loss: 3525.46, base loss: 4656.47
[INFO 2017-06-26 22:18:38,041 main.py:47] epoch 2970, training loss: 3841.26, average training loss: 3525.80, base loss: 4657.39
[INFO 2017-06-26 22:18:38,502 main.py:47] epoch 2971, training loss: 3280.55, average training loss: 3525.65, base loss: 4657.25
[INFO 2017-06-26 22:18:38,950 main.py:47] epoch 2972, training loss: 3286.74, average training loss: 3526.08, base loss: 4657.95
[INFO 2017-06-26 22:18:39,376 main.py:47] epoch 2973, training loss: 3041.30, average training loss: 3526.01, base loss: 4658.30
[INFO 2017-06-26 22:18:39,788 main.py:47] epoch 2974, training loss: 3329.03, average training loss: 3525.48, base loss: 4657.63
[INFO 2017-06-26 22:18:40,200 main.py:47] epoch 2975, training loss: 3269.79, average training loss: 3525.32, base loss: 4657.73
[INFO 2017-06-26 22:18:40,603 main.py:47] epoch 2976, training loss: 3123.69, average training loss: 3521.46, base loss: 4654.25
[INFO 2017-06-26 22:18:41,042 main.py:47] epoch 2977, training loss: 3457.66, average training loss: 3521.13, base loss: 4654.23
[INFO 2017-06-26 22:18:41,447 main.py:47] epoch 2978, training loss: 3701.39, average training loss: 3521.66, base loss: 4655.77
[INFO 2017-06-26 22:18:41,849 main.py:47] epoch 2979, training loss: 3461.69, average training loss: 3521.04, base loss: 4654.67
[INFO 2017-06-26 22:18:42,249 main.py:47] epoch 2980, training loss: 2970.51, average training loss: 3520.80, base loss: 4654.72
[INFO 2017-06-26 22:18:42,650 main.py:47] epoch 2981, training loss: 2923.09, average training loss: 3520.45, base loss: 4654.16
[INFO 2017-06-26 22:18:43,054 main.py:47] epoch 2982, training loss: 3355.49, average training loss: 3520.41, base loss: 4654.18
[INFO 2017-06-26 22:18:43,458 main.py:47] epoch 2983, training loss: 3405.79, average training loss: 3520.09, base loss: 4653.86
[INFO 2017-06-26 22:18:43,860 main.py:47] epoch 2984, training loss: 3606.16, average training loss: 3516.43, base loss: 4650.57
[INFO 2017-06-26 22:18:44,261 main.py:47] epoch 2985, training loss: 3358.35, average training loss: 3516.12, base loss: 4650.36
[INFO 2017-06-26 22:18:44,664 main.py:47] epoch 2986, training loss: 3647.61, average training loss: 3516.47, base loss: 4650.98
[INFO 2017-06-26 22:18:45,066 main.py:47] epoch 2987, training loss: 3363.34, average training loss: 3516.65, base loss: 4651.25
[INFO 2017-06-26 22:18:45,469 main.py:47] epoch 2988, training loss: 3402.15, average training loss: 3516.33, base loss: 4650.77
[INFO 2017-06-26 22:18:45,872 main.py:47] epoch 2989, training loss: 3178.63, average training loss: 3515.96, base loss: 4650.95
[INFO 2017-06-26 22:18:46,270 main.py:47] epoch 2990, training loss: 3247.39, average training loss: 3515.96, base loss: 4651.31
[INFO 2017-06-26 22:18:46,676 main.py:47] epoch 2991, training loss: 2994.42, average training loss: 3515.12, base loss: 4650.16
[INFO 2017-06-26 22:18:47,079 main.py:47] epoch 2992, training loss: 3354.16, average training loss: 3515.05, base loss: 4650.38
[INFO 2017-06-26 22:18:47,479 main.py:47] epoch 2993, training loss: 3195.27, average training loss: 3514.52, base loss: 4649.81
[INFO 2017-06-26 22:18:47,882 main.py:47] epoch 2994, training loss: 3060.66, average training loss: 3514.38, base loss: 4649.93
[INFO 2017-06-26 22:18:48,286 main.py:47] epoch 2995, training loss: 3619.91, average training loss: 3514.57, base loss: 4650.48
[INFO 2017-06-26 22:18:48,685 main.py:47] epoch 2996, training loss: 3131.68, average training loss: 3514.75, base loss: 4651.05
[INFO 2017-06-26 22:18:49,089 main.py:47] epoch 2997, training loss: 3267.99, average training loss: 3514.67, base loss: 4651.18
[INFO 2017-06-26 22:18:49,489 main.py:47] epoch 2998, training loss: 3449.93, average training loss: 3514.48, base loss: 4651.27
[INFO 2017-06-26 22:18:49,889 main.py:47] epoch 2999, training loss: 3034.64, average training loss: 3513.56, base loss: 4650.03
[INFO 2017-06-26 22:18:49,889 main.py:49] epoch 2999, testing
[INFO 2017-06-26 22:18:51,543 main.py:102] average testing loss: 3327.28, base loss: 4698.60
[INFO 2017-06-26 22:18:51,543 main.py:103] improve_loss: 1371.32, improve_percent: 0.29
[INFO 2017-06-26 22:18:51,544 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 22:18:51,556 main.py:73] current best improved percent: 0.29
[INFO 2017-06-26 22:18:51,956 main.py:47] epoch 3000, training loss: 3320.56, average training loss: 3513.23, base loss: 4649.96
[INFO 2017-06-26 22:18:52,363 main.py:47] epoch 3001, training loss: 3507.15, average training loss: 3513.44, base loss: 4650.61
[INFO 2017-06-26 22:18:52,766 main.py:47] epoch 3002, training loss: 3381.09, average training loss: 3513.43, base loss: 4650.87
[INFO 2017-06-26 22:18:53,166 main.py:47] epoch 3003, training loss: 3054.27, average training loss: 3512.80, base loss: 4649.98
[INFO 2017-06-26 22:18:53,563 main.py:47] epoch 3004, training loss: 3620.72, average training loss: 3512.60, base loss: 4649.87
[INFO 2017-06-26 22:18:53,964 main.py:47] epoch 3005, training loss: 3449.07, average training loss: 3512.81, base loss: 4650.39
[INFO 2017-06-26 22:18:54,368 main.py:47] epoch 3006, training loss: 3440.01, average training loss: 3513.11, base loss: 4651.13
[INFO 2017-06-26 22:18:54,771 main.py:47] epoch 3007, training loss: 3207.72, average training loss: 3512.93, base loss: 4651.29
[INFO 2017-06-26 22:18:55,176 main.py:47] epoch 3008, training loss: 3439.36, average training loss: 3512.86, base loss: 4651.65
[INFO 2017-06-26 22:18:55,577 main.py:47] epoch 3009, training loss: 3181.01, average training loss: 3512.95, base loss: 4652.37
[INFO 2017-06-26 22:18:55,981 main.py:47] epoch 3010, training loss: 3445.36, average training loss: 3513.08, base loss: 4652.56
[INFO 2017-06-26 22:18:56,386 main.py:47] epoch 3011, training loss: 3108.47, average training loss: 3512.90, base loss: 4652.55
[INFO 2017-06-26 22:18:56,787 main.py:47] epoch 3012, training loss: 2938.05, average training loss: 3512.39, base loss: 4651.81
[INFO 2017-06-26 22:18:57,193 main.py:47] epoch 3013, training loss: 2969.23, average training loss: 3511.65, base loss: 4650.66
[INFO 2017-06-26 22:18:57,596 main.py:47] epoch 3014, training loss: 3066.11, average training loss: 3511.48, base loss: 4650.78
[INFO 2017-06-26 22:18:58,002 main.py:47] epoch 3015, training loss: 2900.35, average training loss: 3510.67, base loss: 4649.57
[INFO 2017-06-26 22:18:58,407 main.py:47] epoch 3016, training loss: 3025.16, average training loss: 3510.52, base loss: 4649.43
[INFO 2017-06-26 22:18:58,812 main.py:47] epoch 3017, training loss: 7331.29, average training loss: 3514.77, base loss: 4654.67
[INFO 2017-06-26 22:18:59,217 main.py:47] epoch 3018, training loss: 3124.33, average training loss: 3514.49, base loss: 4654.63
[INFO 2017-06-26 22:18:59,618 main.py:47] epoch 3019, training loss: 3263.31, average training loss: 3514.67, base loss: 4655.30
[INFO 2017-06-26 22:19:00,025 main.py:47] epoch 3020, training loss: 3065.74, average training loss: 3514.19, base loss: 4654.97
[INFO 2017-06-26 22:19:00,430 main.py:47] epoch 3021, training loss: 2714.00, average training loss: 3509.62, base loss: 4649.87
[INFO 2017-06-26 22:19:00,843 main.py:47] epoch 3022, training loss: 3333.49, average training loss: 3509.03, base loss: 4648.86
[INFO 2017-06-26 22:19:01,250 main.py:47] epoch 3023, training loss: 3138.32, average training loss: 3509.20, base loss: 4649.57
[INFO 2017-06-26 22:19:01,658 main.py:47] epoch 3024, training loss: 3125.22, average training loss: 3508.67, base loss: 4649.17
[INFO 2017-06-26 22:19:02,065 main.py:47] epoch 3025, training loss: 3077.27, average training loss: 3508.19, base loss: 4648.63
[INFO 2017-06-26 22:19:02,469 main.py:47] epoch 3026, training loss: 3004.22, average training loss: 3508.18, base loss: 4649.08
[INFO 2017-06-26 22:19:02,876 main.py:47] epoch 3027, training loss: 3110.02, average training loss: 3507.80, base loss: 4648.83
[INFO 2017-06-26 22:19:03,295 main.py:47] epoch 3028, training loss: 3212.76, average training loss: 3507.20, base loss: 4648.29
[INFO 2017-06-26 22:19:03,696 main.py:47] epoch 3029, training loss: 2950.74, average training loss: 3507.20, base loss: 4648.21
[INFO 2017-06-26 22:19:04,099 main.py:47] epoch 3030, training loss: 2979.94, average training loss: 3505.83, base loss: 4646.17
[INFO 2017-06-26 22:19:04,502 main.py:47] epoch 3031, training loss: 3792.65, average training loss: 3506.20, base loss: 4647.32
[INFO 2017-06-26 22:19:04,904 main.py:47] epoch 3032, training loss: 3236.32, average training loss: 3505.70, base loss: 4646.54
[INFO 2017-06-26 22:19:05,304 main.py:47] epoch 3033, training loss: 3309.80, average training loss: 3505.31, base loss: 4645.91
[INFO 2017-06-26 22:19:05,702 main.py:47] epoch 3034, training loss: 3289.54, average training loss: 3505.18, base loss: 4645.69
[INFO 2017-06-26 22:19:06,103 main.py:47] epoch 3035, training loss: 6495.45, average training loss: 3508.54, base loss: 4649.80
[INFO 2017-06-26 22:19:06,504 main.py:47] epoch 3036, training loss: 3013.19, average training loss: 3508.08, base loss: 4649.09
[INFO 2017-06-26 22:19:06,910 main.py:47] epoch 3037, training loss: 3232.95, average training loss: 3508.18, base loss: 4649.57
[INFO 2017-06-26 22:19:07,314 main.py:47] epoch 3038, training loss: 3327.96, average training loss: 3508.17, base loss: 4650.10
[INFO 2017-06-26 22:19:07,713 main.py:47] epoch 3039, training loss: 3084.73, average training loss: 3507.61, base loss: 4649.53
[INFO 2017-06-26 22:19:08,118 main.py:47] epoch 3040, training loss: 3070.18, average training loss: 3507.26, base loss: 4649.17
[INFO 2017-06-26 22:19:08,526 main.py:47] epoch 3041, training loss: 3512.65, average training loss: 3507.51, base loss: 4650.05
[INFO 2017-06-26 22:19:08,932 main.py:47] epoch 3042, training loss: 3597.01, average training loss: 3507.28, base loss: 4649.85
[INFO 2017-06-26 22:19:09,335 main.py:47] epoch 3043, training loss: 3452.81, average training loss: 3507.23, base loss: 4650.38
[INFO 2017-06-26 22:19:09,734 main.py:47] epoch 3044, training loss: 2917.28, average training loss: 3506.68, base loss: 4649.67
[INFO 2017-06-26 22:19:10,136 main.py:47] epoch 3045, training loss: 3113.15, average training loss: 3506.50, base loss: 4649.47
[INFO 2017-06-26 22:19:10,537 main.py:47] epoch 3046, training loss: 3528.18, average training loss: 3506.53, base loss: 4650.13
[INFO 2017-06-26 22:19:10,937 main.py:47] epoch 3047, training loss: 3144.69, average training loss: 3505.77, base loss: 4649.06
[INFO 2017-06-26 22:19:11,345 main.py:47] epoch 3048, training loss: 2949.30, average training loss: 3505.48, base loss: 4648.59
[INFO 2017-06-26 22:19:11,752 main.py:47] epoch 3049, training loss: 3559.72, average training loss: 3505.75, base loss: 4649.39
[INFO 2017-06-26 22:19:12,151 main.py:47] epoch 3050, training loss: 3234.97, average training loss: 3505.75, base loss: 4649.55
[INFO 2017-06-26 22:19:12,554 main.py:47] epoch 3051, training loss: 3028.67, average training loss: 3505.25, base loss: 4648.84
[INFO 2017-06-26 22:19:12,954 main.py:47] epoch 3052, training loss: 3088.72, average training loss: 3504.82, base loss: 4647.94
[INFO 2017-06-26 22:19:13,354 main.py:47] epoch 3053, training loss: 2819.30, average training loss: 3504.45, base loss: 4647.55
[INFO 2017-06-26 22:19:13,753 main.py:47] epoch 3054, training loss: 3443.15, average training loss: 3504.42, base loss: 4647.91
[INFO 2017-06-26 22:19:14,156 main.py:47] epoch 3055, training loss: 3312.11, average training loss: 3504.33, base loss: 4647.81
[INFO 2017-06-26 22:19:14,559 main.py:47] epoch 3056, training loss: 3082.48, average training loss: 3504.29, base loss: 4648.01
[INFO 2017-06-26 22:19:14,961 main.py:47] epoch 3057, training loss: 3263.52, average training loss: 3503.92, base loss: 4647.71
[INFO 2017-06-26 22:19:15,362 main.py:47] epoch 3058, training loss: 3196.60, average training loss: 3503.50, base loss: 4647.43
[INFO 2017-06-26 22:19:15,766 main.py:47] epoch 3059, training loss: 3272.05, average training loss: 3503.63, base loss: 4648.42
[INFO 2017-06-26 22:19:16,169 main.py:47] epoch 3060, training loss: 3053.11, average training loss: 3503.48, base loss: 4648.48
[INFO 2017-06-26 22:19:16,577 main.py:47] epoch 3061, training loss: 2862.22, average training loss: 3502.60, base loss: 4646.94
[INFO 2017-06-26 22:19:16,977 main.py:47] epoch 3062, training loss: 3179.97, average training loss: 3502.94, base loss: 4647.68
[INFO 2017-06-26 22:19:17,384 main.py:47] epoch 3063, training loss: 3446.77, average training loss: 3502.73, base loss: 4647.33
[INFO 2017-06-26 22:19:17,789 main.py:47] epoch 3064, training loss: 2989.44, average training loss: 3502.11, base loss: 4646.43
[INFO 2017-06-26 22:19:18,192 main.py:47] epoch 3065, training loss: 3082.76, average training loss: 3501.88, base loss: 4646.28
[INFO 2017-06-26 22:19:18,597 main.py:47] epoch 3066, training loss: 3250.71, average training loss: 3501.88, base loss: 4646.30
[INFO 2017-06-26 22:19:18,999 main.py:47] epoch 3067, training loss: 3905.41, average training loss: 3502.39, base loss: 4647.79
[INFO 2017-06-26 22:19:19,404 main.py:47] epoch 3068, training loss: 3581.93, average training loss: 3502.73, base loss: 4648.54
[INFO 2017-06-26 22:19:19,808 main.py:47] epoch 3069, training loss: 2970.96, average training loss: 3502.25, base loss: 4648.07
[INFO 2017-06-26 22:19:20,211 main.py:47] epoch 3070, training loss: 3310.74, average training loss: 3502.52, base loss: 4648.39
[INFO 2017-06-26 22:19:20,616 main.py:47] epoch 3071, training loss: 3466.48, average training loss: 3502.57, base loss: 4648.96
[INFO 2017-06-26 22:19:21,019 main.py:47] epoch 3072, training loss: 6443.76, average training loss: 3505.83, base loss: 4652.90
[INFO 2017-06-26 22:19:21,423 main.py:47] epoch 3073, training loss: 2763.73, average training loss: 3505.23, base loss: 4652.18
[INFO 2017-06-26 22:19:21,825 main.py:47] epoch 3074, training loss: 3223.23, average training loss: 3505.09, base loss: 4652.02
[INFO 2017-06-26 22:19:22,223 main.py:47] epoch 3075, training loss: 2895.24, average training loss: 3504.56, base loss: 4651.62
[INFO 2017-06-26 22:19:22,626 main.py:47] epoch 3076, training loss: 2992.77, average training loss: 3503.85, base loss: 4650.84
[INFO 2017-06-26 22:19:23,030 main.py:47] epoch 3077, training loss: 3491.39, average training loss: 3504.04, base loss: 4651.68
[INFO 2017-06-26 22:19:23,434 main.py:47] epoch 3078, training loss: 2969.58, average training loss: 3503.79, base loss: 4651.47
[INFO 2017-06-26 22:19:23,838 main.py:47] epoch 3079, training loss: 3311.11, average training loss: 3503.65, base loss: 4651.60
[INFO 2017-06-26 22:19:24,242 main.py:47] epoch 3080, training loss: 3222.44, average training loss: 3503.87, base loss: 4652.06
[INFO 2017-06-26 22:19:24,642 main.py:47] epoch 3081, training loss: 3387.20, average training loss: 3503.56, base loss: 4651.63
[INFO 2017-06-26 22:19:25,043 main.py:47] epoch 3082, training loss: 3728.07, average training loss: 3503.49, base loss: 4651.82
[INFO 2017-06-26 22:19:25,447 main.py:47] epoch 3083, training loss: 3290.50, average training loss: 3503.61, base loss: 4652.64
[INFO 2017-06-26 22:19:25,851 main.py:47] epoch 3084, training loss: 3526.00, average training loss: 3503.88, base loss: 4653.04
[INFO 2017-06-26 22:19:26,251 main.py:47] epoch 3085, training loss: 3007.46, average training loss: 3499.94, base loss: 4648.61
[INFO 2017-06-26 22:19:26,656 main.py:47] epoch 3086, training loss: 3151.44, average training loss: 3499.83, base loss: 4648.14
[INFO 2017-06-26 22:19:27,063 main.py:47] epoch 3087, training loss: 3213.54, average training loss: 3499.77, base loss: 4648.79
[INFO 2017-06-26 22:19:27,467 main.py:47] epoch 3088, training loss: 3330.75, average training loss: 3500.28, base loss: 4649.70
[INFO 2017-06-26 22:19:27,870 main.py:47] epoch 3089, training loss: 3671.82, average training loss: 3500.88, base loss: 4651.04
[INFO 2017-06-26 22:19:28,272 main.py:47] epoch 3090, training loss: 3651.90, average training loss: 3497.65, base loss: 4648.16
[INFO 2017-06-26 22:19:28,671 main.py:47] epoch 3091, training loss: 3050.06, average training loss: 3497.35, base loss: 4647.99
[INFO 2017-06-26 22:19:29,074 main.py:47] epoch 3092, training loss: 3196.73, average training loss: 3497.59, base loss: 4648.61
[INFO 2017-06-26 22:19:29,479 main.py:47] epoch 3093, training loss: 3315.29, average training loss: 3497.39, base loss: 4648.54
[INFO 2017-06-26 22:19:29,883 main.py:47] epoch 3094, training loss: 2995.76, average training loss: 3496.70, base loss: 4647.83
[INFO 2017-06-26 22:19:30,286 main.py:47] epoch 3095, training loss: 3126.76, average training loss: 3496.35, base loss: 4647.55
[INFO 2017-06-26 22:19:30,684 main.py:47] epoch 3096, training loss: 3416.40, average training loss: 3496.35, base loss: 4648.19
[INFO 2017-06-26 22:19:31,086 main.py:47] epoch 3097, training loss: 3169.45, average training loss: 3496.45, base loss: 4648.36
[INFO 2017-06-26 22:19:31,491 main.py:47] epoch 3098, training loss: 6953.87, average training loss: 3499.91, base loss: 4651.81
[INFO 2017-06-26 22:19:31,893 main.py:47] epoch 3099, training loss: 3546.77, average training loss: 3500.32, base loss: 4652.72
[INFO 2017-06-26 22:19:31,893 main.py:49] epoch 3099, testing
[INFO 2017-06-26 22:19:33,556 main.py:102] average testing loss: 3230.86, base loss: 4413.73
[INFO 2017-06-26 22:19:33,556 main.py:103] improve_loss: 1182.87, improve_percent: 0.27
[INFO 2017-06-26 22:19:33,556 main.py:73] current best improved percent: 0.29
[INFO 2017-06-26 22:19:33,961 main.py:47] epoch 3100, training loss: 2707.53, average training loss: 3499.52, base loss: 4651.61
[INFO 2017-06-26 22:19:34,364 main.py:47] epoch 3101, training loss: 3340.14, average training loss: 3499.63, base loss: 4652.10
[INFO 2017-06-26 22:19:34,766 main.py:47] epoch 3102, training loss: 2996.18, average training loss: 3499.08, base loss: 4651.22
[INFO 2017-06-26 22:19:35,169 main.py:47] epoch 3103, training loss: 3398.42, average training loss: 3499.00, base loss: 4651.62
[INFO 2017-06-26 22:19:35,571 main.py:47] epoch 3104, training loss: 3657.56, average training loss: 3499.45, base loss: 4652.79
[INFO 2017-06-26 22:19:35,975 main.py:47] epoch 3105, training loss: 3177.14, average training loss: 3495.50, base loss: 4649.16
[INFO 2017-06-26 22:19:36,381 main.py:47] epoch 3106, training loss: 3039.43, average training loss: 3495.22, base loss: 4648.79
[INFO 2017-06-26 22:19:36,783 main.py:47] epoch 3107, training loss: 3261.39, average training loss: 3495.28, base loss: 4649.05
[INFO 2017-06-26 22:19:37,187 main.py:47] epoch 3108, training loss: 3708.56, average training loss: 3495.56, base loss: 4649.60
[INFO 2017-06-26 22:19:37,589 main.py:47] epoch 3109, training loss: 3273.23, average training loss: 3495.36, base loss: 4649.88
[INFO 2017-06-26 22:19:37,992 main.py:47] epoch 3110, training loss: 3406.13, average training loss: 3495.40, base loss: 4650.17
[INFO 2017-06-26 22:19:38,395 main.py:47] epoch 3111, training loss: 3366.72, average training loss: 3495.19, base loss: 4650.03
[INFO 2017-06-26 22:19:38,803 main.py:47] epoch 3112, training loss: 3037.69, average training loss: 3495.09, base loss: 4649.97
[INFO 2017-06-26 22:19:39,207 main.py:47] epoch 3113, training loss: 3031.81, average training loss: 3494.50, base loss: 4649.28
[INFO 2017-06-26 22:19:39,610 main.py:47] epoch 3114, training loss: 2930.20, average training loss: 3493.93, base loss: 4648.57
[INFO 2017-06-26 22:19:40,016 main.py:47] epoch 3115, training loss: 2957.43, average training loss: 3493.55, base loss: 4648.11
[INFO 2017-06-26 22:19:40,419 main.py:47] epoch 3116, training loss: 3598.91, average training loss: 3493.84, base loss: 4649.12
[INFO 2017-06-26 22:19:40,820 main.py:47] epoch 3117, training loss: 3545.20, average training loss: 3490.54, base loss: 4646.21
[INFO 2017-06-26 22:19:41,222 main.py:47] epoch 3118, training loss: 2996.74, average training loss: 3486.57, base loss: 4642.30
[INFO 2017-06-26 22:19:41,624 main.py:47] epoch 3119, training loss: 3446.94, average training loss: 3486.40, base loss: 4642.42
[INFO 2017-06-26 22:19:42,023 main.py:47] epoch 3120, training loss: 3760.33, average training loss: 3486.72, base loss: 4643.46
[INFO 2017-06-26 22:19:42,426 main.py:47] epoch 3121, training loss: 3783.22, average training loss: 3487.09, base loss: 4644.78
[INFO 2017-06-26 22:19:42,830 main.py:47] epoch 3122, training loss: 3012.40, average training loss: 3487.03, base loss: 4644.93
[INFO 2017-06-26 22:19:43,233 main.py:47] epoch 3123, training loss: 3048.52, average training loss: 3486.59, base loss: 4644.57
[INFO 2017-06-26 22:19:43,636 main.py:47] epoch 3124, training loss: 3358.08, average training loss: 3486.55, base loss: 4644.50
[INFO 2017-06-26 22:19:44,039 main.py:47] epoch 3125, training loss: 3279.44, average training loss: 3486.59, base loss: 4644.92
[INFO 2017-06-26 22:19:44,442 main.py:47] epoch 3126, training loss: 3353.93, average training loss: 3486.70, base loss: 4645.19
[INFO 2017-06-26 22:19:44,850 main.py:47] epoch 3127, training loss: 3399.12, average training loss: 3487.25, base loss: 4646.51
[INFO 2017-06-26 22:19:45,253 main.py:47] epoch 3128, training loss: 2801.55, average training loss: 3486.70, base loss: 4645.92
[INFO 2017-06-26 22:19:45,656 main.py:47] epoch 3129, training loss: 3317.89, average training loss: 3482.54, base loss: 4641.58
[INFO 2017-06-26 22:19:46,061 main.py:47] epoch 3130, training loss: 3295.83, average training loss: 3482.58, base loss: 4641.81
[INFO 2017-06-26 22:19:46,464 main.py:47] epoch 3131, training loss: 3252.72, average training loss: 3482.39, base loss: 4641.71
[INFO 2017-06-26 22:19:46,868 main.py:47] epoch 3132, training loss: 3089.92, average training loss: 3482.00, base loss: 4641.63
[INFO 2017-06-26 22:19:47,272 main.py:47] epoch 3133, training loss: 3289.83, average training loss: 3481.88, base loss: 4641.69
[INFO 2017-06-26 22:19:47,688 main.py:47] epoch 3134, training loss: 3221.92, average training loss: 3481.35, base loss: 4641.03
[INFO 2017-06-26 22:19:48,095 main.py:47] epoch 3135, training loss: 3433.90, average training loss: 3481.70, base loss: 4641.75
[INFO 2017-06-26 22:19:48,506 main.py:47] epoch 3136, training loss: 3743.00, average training loss: 3481.70, base loss: 4641.73
[INFO 2017-06-26 22:19:48,915 main.py:47] epoch 3137, training loss: 2987.16, average training loss: 3481.10, base loss: 4640.91
[INFO 2017-06-26 22:19:49,368 main.py:47] epoch 3138, training loss: 3236.52, average training loss: 3476.76, base loss: 4636.27
[INFO 2017-06-26 22:19:49,787 main.py:47] epoch 3139, training loss: 3023.16, average training loss: 3476.26, base loss: 4635.37
[INFO 2017-06-26 22:19:50,194 main.py:47] epoch 3140, training loss: 3677.88, average training loss: 3476.65, base loss: 4636.13
[INFO 2017-06-26 22:19:50,598 main.py:47] epoch 3141, training loss: 6982.67, average training loss: 3480.55, base loss: 4640.74
[INFO 2017-06-26 22:19:51,008 main.py:47] epoch 3142, training loss: 2923.98, average training loss: 3480.04, base loss: 4639.97
[INFO 2017-06-26 22:19:51,416 main.py:47] epoch 3143, training loss: 3072.56, average training loss: 3479.64, base loss: 4639.26
[INFO 2017-06-26 22:19:51,839 main.py:47] epoch 3144, training loss: 2865.64, average training loss: 3479.46, base loss: 4639.47
[INFO 2017-06-26 22:19:52,249 main.py:47] epoch 3145, training loss: 3228.17, average training loss: 3479.43, base loss: 4639.66
[INFO 2017-06-26 22:19:52,675 main.py:47] epoch 3146, training loss: 3221.36, average training loss: 3475.05, base loss: 4635.17
[INFO 2017-06-26 22:19:53,090 main.py:47] epoch 3147, training loss: 3430.76, average training loss: 3475.36, base loss: 4636.14
[INFO 2017-06-26 22:19:53,531 main.py:47] epoch 3148, training loss: 3376.63, average training loss: 3475.34, base loss: 4636.26
[INFO 2017-06-26 22:19:54,022 main.py:47] epoch 3149, training loss: 3437.79, average training loss: 3475.85, base loss: 4637.26
[INFO 2017-06-26 22:19:54,491 main.py:47] epoch 3150, training loss: 2941.03, average training loss: 3475.81, base loss: 4637.10
[INFO 2017-06-26 22:19:54,969 main.py:47] epoch 3151, training loss: 3435.96, average training loss: 3476.55, base loss: 4639.06
[INFO 2017-06-26 22:19:55,450 main.py:47] epoch 3152, training loss: 2895.42, average training loss: 3476.09, base loss: 4638.52
[INFO 2017-06-26 22:19:55,880 main.py:47] epoch 3153, training loss: 3671.84, average training loss: 3476.38, base loss: 4639.34
[INFO 2017-06-26 22:19:56,374 main.py:47] epoch 3154, training loss: 3217.73, average training loss: 3476.09, base loss: 4639.19
[INFO 2017-06-26 22:19:56,874 main.py:47] epoch 3155, training loss: 3111.30, average training loss: 3475.65, base loss: 4638.86
[INFO 2017-06-26 22:19:57,313 main.py:47] epoch 3156, training loss: 3238.72, average training loss: 3471.99, base loss: 4635.18
[INFO 2017-06-26 22:19:57,724 main.py:47] epoch 3157, training loss: 3172.34, average training loss: 3471.68, base loss: 4634.94
[INFO 2017-06-26 22:19:58,132 main.py:47] epoch 3158, training loss: 3097.33, average training loss: 3471.38, base loss: 4634.64
[INFO 2017-06-26 22:19:58,537 main.py:47] epoch 3159, training loss: 6674.70, average training loss: 3474.24, base loss: 4637.22
[INFO 2017-06-26 22:19:58,941 main.py:47] epoch 3160, training loss: 3452.30, average training loss: 3474.81, base loss: 4638.93
[INFO 2017-06-26 22:19:59,349 main.py:47] epoch 3161, training loss: 3792.35, average training loss: 3475.57, base loss: 4640.87
[INFO 2017-06-26 22:19:59,787 main.py:47] epoch 3162, training loss: 3438.41, average training loss: 3475.92, base loss: 4641.77
[INFO 2017-06-26 22:20:00,191 main.py:47] epoch 3163, training loss: 3255.42, average training loss: 3475.43, base loss: 4640.81
[INFO 2017-06-26 22:20:00,593 main.py:47] epoch 3164, training loss: 3470.30, average training loss: 3475.65, base loss: 4641.48
[INFO 2017-06-26 22:20:00,995 main.py:47] epoch 3165, training loss: 3130.98, average training loss: 3475.46, base loss: 4641.19
[INFO 2017-06-26 22:20:01,406 main.py:47] epoch 3166, training loss: 3168.64, average training loss: 3475.44, base loss: 4641.75
[INFO 2017-06-26 22:20:01,810 main.py:47] epoch 3167, training loss: 3295.06, average training loss: 3475.41, base loss: 4642.12
[INFO 2017-06-26 22:20:02,215 main.py:47] epoch 3168, training loss: 3521.09, average training loss: 3475.12, base loss: 4641.51
[INFO 2017-06-26 22:20:02,679 main.py:47] epoch 3169, training loss: 3228.41, average training loss: 3474.89, base loss: 4641.53
[INFO 2017-06-26 22:20:03,089 main.py:47] epoch 3170, training loss: 3134.16, average training loss: 3474.38, base loss: 4640.50
[INFO 2017-06-26 22:20:03,498 main.py:47] epoch 3171, training loss: 3247.94, average training loss: 3474.29, base loss: 4640.61
[INFO 2017-06-26 22:20:03,910 main.py:47] epoch 3172, training loss: 6164.58, average training loss: 3476.74, base loss: 4643.26
[INFO 2017-06-26 22:20:04,316 main.py:47] epoch 3173, training loss: 2778.60, average training loss: 3475.99, base loss: 4642.38
[INFO 2017-06-26 22:20:04,717 main.py:47] epoch 3174, training loss: 2991.14, average training loss: 3475.45, base loss: 4641.74
[INFO 2017-06-26 22:20:05,116 main.py:47] epoch 3175, training loss: 6651.53, average training loss: 3478.96, base loss: 4645.34
[INFO 2017-06-26 22:20:05,525 main.py:47] epoch 3176, training loss: 3152.32, average training loss: 3478.88, base loss: 4645.16
[INFO 2017-06-26 22:20:05,923 main.py:47] epoch 3177, training loss: 3536.13, average training loss: 3479.10, base loss: 4645.89
[INFO 2017-06-26 22:20:06,323 main.py:47] epoch 3178, training loss: 3169.89, average training loss: 3478.81, base loss: 4645.51
[INFO 2017-06-26 22:20:06,725 main.py:47] epoch 3179, training loss: 3124.91, average training loss: 3478.52, base loss: 4645.06
[INFO 2017-06-26 22:20:07,130 main.py:47] epoch 3180, training loss: 3430.19, average training loss: 3478.71, base loss: 4646.10
[INFO 2017-06-26 22:20:07,533 main.py:47] epoch 3181, training loss: 3057.37, average training loss: 3478.12, base loss: 4645.25
[INFO 2017-06-26 22:20:07,936 main.py:47] epoch 3182, training loss: 3211.01, average training loss: 3477.65, base loss: 4644.61
[INFO 2017-06-26 22:20:08,339 main.py:47] epoch 3183, training loss: 6082.71, average training loss: 3480.22, base loss: 4647.37
[INFO 2017-06-26 22:20:08,739 main.py:47] epoch 3184, training loss: 2958.43, average training loss: 3479.86, base loss: 4646.71
[INFO 2017-06-26 22:20:09,147 main.py:47] epoch 3185, training loss: 3234.24, average training loss: 3479.83, base loss: 4646.71
[INFO 2017-06-26 22:20:09,548 main.py:47] epoch 3186, training loss: 2875.84, average training loss: 3479.40, base loss: 4646.06
[INFO 2017-06-26 22:20:09,958 main.py:47] epoch 3187, training loss: 3275.35, average training loss: 3479.30, base loss: 4646.20
[INFO 2017-06-26 22:20:10,360 main.py:47] epoch 3188, training loss: 3732.69, average training loss: 3479.99, base loss: 4647.84
[INFO 2017-06-26 22:20:10,769 main.py:47] epoch 3189, training loss: 2996.42, average training loss: 3479.54, base loss: 4647.64
[INFO 2017-06-26 22:20:11,173 main.py:47] epoch 3190, training loss: 3247.33, average training loss: 3479.48, base loss: 4647.91
[INFO 2017-06-26 22:20:11,575 main.py:47] epoch 3191, training loss: 3421.93, average training loss: 3479.29, base loss: 4648.02
[INFO 2017-06-26 22:20:11,978 main.py:47] epoch 3192, training loss: 3095.87, average training loss: 3479.43, base loss: 4648.35
[INFO 2017-06-26 22:20:12,384 main.py:47] epoch 3193, training loss: 3328.40, average training loss: 3479.48, base loss: 4648.62
[INFO 2017-06-26 22:20:12,788 main.py:47] epoch 3194, training loss: 2706.67, average training loss: 3479.28, base loss: 4648.60
[INFO 2017-06-26 22:20:13,192 main.py:47] epoch 3195, training loss: 3180.86, average training loss: 3479.37, base loss: 4649.45
[INFO 2017-06-26 22:20:13,595 main.py:47] epoch 3196, training loss: 2864.81, average training loss: 3478.95, base loss: 4649.06
[INFO 2017-06-26 22:20:13,998 main.py:47] epoch 3197, training loss: 2915.89, average training loss: 3478.83, base loss: 4649.03
[INFO 2017-06-26 22:20:14,402 main.py:47] epoch 3198, training loss: 3589.02, average training loss: 3478.97, base loss: 4649.48
[INFO 2017-06-26 22:20:14,807 main.py:47] epoch 3199, training loss: 3375.13, average training loss: 3479.42, base loss: 4650.75
[INFO 2017-06-26 22:20:14,807 main.py:49] epoch 3199, testing
[INFO 2017-06-26 22:20:16,469 main.py:102] average testing loss: 3107.92, base loss: 4355.11
[INFO 2017-06-26 22:20:16,469 main.py:103] improve_loss: 1247.19, improve_percent: 0.29
[INFO 2017-06-26 22:20:16,470 main.py:73] current best improved percent: 0.29
[INFO 2017-06-26 22:20:16,867 main.py:47] epoch 3200, training loss: 3390.29, average training loss: 3479.19, base loss: 4650.64
[INFO 2017-06-26 22:20:17,264 main.py:47] epoch 3201, training loss: 3325.46, average training loss: 3479.30, base loss: 4651.44
[INFO 2017-06-26 22:20:17,661 main.py:47] epoch 3202, training loss: 3304.66, average training loss: 3479.31, base loss: 4651.56
[INFO 2017-06-26 22:20:18,067 main.py:47] epoch 3203, training loss: 3467.11, average training loss: 3479.54, base loss: 4652.42
[INFO 2017-06-26 22:20:18,471 main.py:47] epoch 3204, training loss: 2979.58, average training loss: 3478.96, base loss: 4651.92
[INFO 2017-06-26 22:20:18,872 main.py:47] epoch 3205, training loss: 3757.55, average training loss: 3479.01, base loss: 4652.16
[INFO 2017-06-26 22:20:19,277 main.py:47] epoch 3206, training loss: 3625.86, average training loss: 3479.46, base loss: 4653.03
[INFO 2017-06-26 22:20:19,680 main.py:47] epoch 3207, training loss: 3305.17, average training loss: 3479.33, base loss: 4653.46
[INFO 2017-06-26 22:20:20,078 main.py:47] epoch 3208, training loss: 3407.18, average training loss: 3479.59, base loss: 4653.76
[INFO 2017-06-26 22:20:20,480 main.py:47] epoch 3209, training loss: 3301.02, average training loss: 3479.59, base loss: 4654.00
[INFO 2017-06-26 22:20:20,890 main.py:47] epoch 3210, training loss: 3068.13, average training loss: 3479.39, base loss: 4653.98
[INFO 2017-06-26 22:20:21,290 main.py:47] epoch 3211, training loss: 3009.19, average training loss: 3479.32, base loss: 4654.11
[INFO 2017-06-26 22:20:21,693 main.py:47] epoch 3212, training loss: 3146.45, average training loss: 3478.81, base loss: 4653.79
[INFO 2017-06-26 22:20:22,096 main.py:47] epoch 3213, training loss: 3599.92, average training loss: 3479.20, base loss: 4654.80
[INFO 2017-06-26 22:20:22,498 main.py:47] epoch 3214, training loss: 2971.31, average training loss: 3479.25, base loss: 4655.52
[INFO 2017-06-26 22:20:22,898 main.py:47] epoch 3215, training loss: 3125.02, average training loss: 3479.22, base loss: 4655.99
[INFO 2017-06-26 22:20:23,296 main.py:47] epoch 3216, training loss: 3091.00, average training loss: 3479.02, base loss: 4656.00
[INFO 2017-06-26 22:20:23,700 main.py:47] epoch 3217, training loss: 3397.99, average training loss: 3479.34, base loss: 4656.47
[INFO 2017-06-26 22:20:24,102 main.py:47] epoch 3218, training loss: 3119.47, average training loss: 3478.38, base loss: 4655.28
[INFO 2017-06-26 22:20:24,500 main.py:47] epoch 3219, training loss: 3241.28, average training loss: 3478.05, base loss: 4655.07
[INFO 2017-06-26 22:20:24,899 main.py:47] epoch 3220, training loss: 3225.28, average training loss: 3477.53, base loss: 4654.44
[INFO 2017-06-26 22:20:25,299 main.py:47] epoch 3221, training loss: 3503.98, average training loss: 3477.87, base loss: 4655.24
[INFO 2017-06-26 22:20:25,703 main.py:47] epoch 3222, training loss: 3377.00, average training loss: 3477.61, base loss: 4655.38
[INFO 2017-06-26 22:20:26,107 main.py:47] epoch 3223, training loss: 3373.84, average training loss: 3477.72, base loss: 4655.75
[INFO 2017-06-26 22:20:26,511 main.py:47] epoch 3224, training loss: 2748.67, average training loss: 3476.84, base loss: 4654.56
[INFO 2017-06-26 22:20:26,911 main.py:47] epoch 3225, training loss: 3264.07, average training loss: 3476.57, base loss: 4654.45
[INFO 2017-06-26 22:20:27,312 main.py:47] epoch 3226, training loss: 3143.74, average training loss: 3476.05, base loss: 4654.08
[INFO 2017-06-26 22:20:27,713 main.py:47] epoch 3227, training loss: 2951.60, average training loss: 3475.27, base loss: 4652.90
[INFO 2017-06-26 22:20:28,112 main.py:47] epoch 3228, training loss: 3452.54, average training loss: 3475.16, base loss: 4653.23
[INFO 2017-06-26 22:20:28,511 main.py:47] epoch 3229, training loss: 3358.72, average training loss: 3475.27, base loss: 4653.80
[INFO 2017-06-26 22:20:28,913 main.py:47] epoch 3230, training loss: 3110.25, average training loss: 3475.01, base loss: 4653.34
[INFO 2017-06-26 22:20:29,314 main.py:47] epoch 3231, training loss: 3158.46, average training loss: 3474.65, base loss: 4653.24
[INFO 2017-06-26 22:20:29,717 main.py:47] epoch 3232, training loss: 3246.54, average training loss: 3474.28, base loss: 4653.06
[INFO 2017-06-26 22:20:30,116 main.py:47] epoch 3233, training loss: 3044.07, average training loss: 3474.20, base loss: 4653.27
[INFO 2017-06-26 22:20:30,515 main.py:47] epoch 3234, training loss: 3365.99, average training loss: 3473.92, base loss: 4653.35
[INFO 2017-06-26 22:20:30,916 main.py:47] epoch 3235, training loss: 2951.45, average training loss: 3473.60, base loss: 4653.11
[INFO 2017-06-26 22:20:31,316 main.py:47] epoch 3236, training loss: 3629.64, average training loss: 3474.04, base loss: 4654.50
[INFO 2017-06-26 22:20:31,716 main.py:47] epoch 3237, training loss: 6413.69, average training loss: 3476.96, base loss: 4657.36
[INFO 2017-06-26 22:20:32,117 main.py:47] epoch 3238, training loss: 3068.04, average training loss: 3476.92, base loss: 4657.67
[INFO 2017-06-26 22:20:32,516 main.py:47] epoch 3239, training loss: 3917.41, average training loss: 3477.66, base loss: 4659.45
[INFO 2017-06-26 22:20:32,919 main.py:47] epoch 3240, training loss: 3094.83, average training loss: 3477.51, base loss: 4659.30
[INFO 2017-06-26 22:20:33,321 main.py:47] epoch 3241, training loss: 2881.37, average training loss: 3476.91, base loss: 4658.42
[INFO 2017-06-26 22:20:33,719 main.py:47] epoch 3242, training loss: 3060.76, average training loss: 3476.68, base loss: 4658.15
[INFO 2017-06-26 22:20:34,116 main.py:47] epoch 3243, training loss: 6818.53, average training loss: 3480.21, base loss: 4662.11
[INFO 2017-06-26 22:20:34,517 main.py:47] epoch 3244, training loss: 3061.58, average training loss: 3479.75, base loss: 4661.62
[INFO 2017-06-26 22:20:34,915 main.py:47] epoch 3245, training loss: 3105.66, average training loss: 3479.72, base loss: 4661.67
[INFO 2017-06-26 22:20:35,314 main.py:47] epoch 3246, training loss: 2974.35, average training loss: 3479.30, base loss: 4661.26
[INFO 2017-06-26 22:20:35,715 main.py:47] epoch 3247, training loss: 3008.88, average training loss: 3478.76, base loss: 4660.70
[INFO 2017-06-26 22:20:36,118 main.py:47] epoch 3248, training loss: 3077.59, average training loss: 3478.40, base loss: 4660.55
[INFO 2017-06-26 22:20:36,518 main.py:47] epoch 3249, training loss: 3299.84, average training loss: 3478.27, base loss: 4660.47
[INFO 2017-06-26 22:20:36,917 main.py:47] epoch 3250, training loss: 6305.56, average training loss: 3481.04, base loss: 4663.64
[INFO 2017-06-26 22:20:37,316 main.py:47] epoch 3251, training loss: 3147.78, average training loss: 3481.35, base loss: 4664.29
[INFO 2017-06-26 22:20:37,715 main.py:47] epoch 3252, training loss: 3004.22, average training loss: 3477.71, base loss: 4660.68
[INFO 2017-06-26 22:20:38,114 main.py:47] epoch 3253, training loss: 3512.97, average training loss: 3477.43, base loss: 4660.37
[INFO 2017-06-26 22:20:38,516 main.py:47] epoch 3254, training loss: 3003.63, average training loss: 3476.73, base loss: 4659.37
[INFO 2017-06-26 22:20:38,925 main.py:47] epoch 3255, training loss: 3352.50, average training loss: 3476.74, base loss: 4659.40
[INFO 2017-06-26 22:20:39,330 main.py:47] epoch 3256, training loss: 3420.02, average training loss: 3476.38, base loss: 4658.79
[INFO 2017-06-26 22:20:39,734 main.py:47] epoch 3257, training loss: 3082.53, average training loss: 3476.07, base loss: 4658.43
[INFO 2017-06-26 22:20:40,133 main.py:47] epoch 3258, training loss: 2895.73, average training loss: 3475.87, base loss: 4658.29
[INFO 2017-06-26 22:20:40,617 main.py:47] epoch 3259, training loss: 5980.37, average training loss: 3478.59, base loss: 4661.33
[INFO 2017-06-26 22:20:41,029 main.py:47] epoch 3260, training loss: 3440.82, average training loss: 3479.07, base loss: 4662.66
[INFO 2017-06-26 22:20:41,434 main.py:47] epoch 3261, training loss: 3101.62, average training loss: 3479.06, base loss: 4662.75
[INFO 2017-06-26 22:20:41,842 main.py:47] epoch 3262, training loss: 3220.71, average training loss: 3478.49, base loss: 4662.02
[INFO 2017-06-26 22:20:42,341 main.py:47] epoch 3263, training loss: 3173.45, average training loss: 3478.48, base loss: 4662.23
[INFO 2017-06-26 22:20:42,771 main.py:47] epoch 3264, training loss: 3487.92, average training loss: 3478.84, base loss: 4662.96
[INFO 2017-06-26 22:20:43,175 main.py:47] epoch 3265, training loss: 3314.68, average training loss: 3478.73, base loss: 4662.81
[INFO 2017-06-26 22:20:43,608 main.py:47] epoch 3266, training loss: 3433.30, average training loss: 3478.73, base loss: 4663.22
[INFO 2017-06-26 22:20:44,014 main.py:47] epoch 3267, training loss: 3217.80, average training loss: 3478.89, base loss: 4663.70
[INFO 2017-06-26 22:20:44,415 main.py:47] epoch 3268, training loss: 3073.73, average training loss: 3474.89, base loss: 4659.62
[INFO 2017-06-26 22:20:44,817 main.py:47] epoch 3269, training loss: 3335.99, average training loss: 3474.96, base loss: 4660.15
[INFO 2017-06-26 22:20:45,218 main.py:47] epoch 3270, training loss: 3463.13, average training loss: 3475.18, base loss: 4660.98
[INFO 2017-06-26 22:20:45,616 main.py:47] epoch 3271, training loss: 3314.66, average training loss: 3475.48, base loss: 4661.93
[INFO 2017-06-26 22:20:46,013 main.py:47] epoch 3272, training loss: 2897.68, average training loss: 3475.21, base loss: 4661.73
[INFO 2017-06-26 22:20:46,414 main.py:47] epoch 3273, training loss: 3103.48, average training loss: 3471.27, base loss: 4657.56
[INFO 2017-06-26 22:20:46,813 main.py:47] epoch 3274, training loss: 3262.47, average training loss: 3470.92, base loss: 4657.12
[INFO 2017-06-26 22:20:47,212 main.py:47] epoch 3275, training loss: 3069.01, average training loss: 3470.39, base loss: 4656.59
[INFO 2017-06-26 22:20:47,612 main.py:47] epoch 3276, training loss: 3134.27, average training loss: 3470.04, base loss: 4656.26
[INFO 2017-06-26 22:20:48,016 main.py:47] epoch 3277, training loss: 2987.70, average training loss: 3469.02, base loss: 4654.90
[INFO 2017-06-26 22:20:48,418 main.py:47] epoch 3278, training loss: 3141.40, average training loss: 3469.03, base loss: 4655.18
[INFO 2017-06-26 22:20:48,821 main.py:47] epoch 3279, training loss: 3588.00, average training loss: 3469.63, base loss: 4656.54
[INFO 2017-06-26 22:20:49,223 main.py:47] epoch 3280, training loss: 3493.11, average training loss: 3469.40, base loss: 4656.64
[INFO 2017-06-26 22:20:49,630 main.py:47] epoch 3281, training loss: 2945.40, average training loss: 3469.12, base loss: 4656.06
[INFO 2017-06-26 22:20:50,039 main.py:47] epoch 3282, training loss: 3100.16, average training loss: 3469.11, base loss: 4656.65
[INFO 2017-06-26 22:20:50,442 main.py:47] epoch 3283, training loss: 2973.78, average training loss: 3469.00, base loss: 4656.77
[INFO 2017-06-26 22:20:50,844 main.py:47] epoch 3284, training loss: 3465.21, average training loss: 3469.08, base loss: 4657.18
[INFO 2017-06-26 22:20:51,244 main.py:47] epoch 3285, training loss: 3347.77, average training loss: 3469.14, base loss: 4657.45
[INFO 2017-06-26 22:20:51,644 main.py:47] epoch 3286, training loss: 2990.42, average training loss: 3468.77, base loss: 4656.81
[INFO 2017-06-26 22:20:52,048 main.py:47] epoch 3287, training loss: 3690.26, average training loss: 3469.39, base loss: 4658.75
[INFO 2017-06-26 22:20:52,452 main.py:47] epoch 3288, training loss: 3324.43, average training loss: 3469.00, base loss: 4658.45
[INFO 2017-06-26 22:20:52,851 main.py:47] epoch 3289, training loss: 3533.95, average training loss: 3468.98, base loss: 4658.40
[INFO 2017-06-26 22:20:53,256 main.py:47] epoch 3290, training loss: 3378.31, average training loss: 3469.06, base loss: 4658.87
[INFO 2017-06-26 22:20:53,658 main.py:47] epoch 3291, training loss: 3523.80, average training loss: 3469.08, base loss: 4659.65
[INFO 2017-06-26 22:20:54,060 main.py:47] epoch 3292, training loss: 3335.98, average training loss: 3468.61, base loss: 4659.16
[INFO 2017-06-26 22:20:54,459 main.py:47] epoch 3293, training loss: 2805.21, average training loss: 3467.81, base loss: 4658.14
[INFO 2017-06-26 22:20:54,860 main.py:47] epoch 3294, training loss: 2978.82, average training loss: 3467.61, base loss: 4657.70
[INFO 2017-06-26 22:20:55,258 main.py:47] epoch 3295, training loss: 3436.99, average training loss: 3467.62, base loss: 4657.75
[INFO 2017-06-26 22:20:55,660 main.py:47] epoch 3296, training loss: 3308.21, average training loss: 3467.33, base loss: 4657.62
[INFO 2017-06-26 22:20:56,059 main.py:47] epoch 3297, training loss: 3262.84, average training loss: 3467.55, base loss: 4658.54
[INFO 2017-06-26 22:20:56,456 main.py:47] epoch 3298, training loss: 3204.40, average training loss: 3467.40, base loss: 4658.65
[INFO 2017-06-26 22:20:56,854 main.py:47] epoch 3299, training loss: 3569.43, average training loss: 3466.94, base loss: 4658.01
[INFO 2017-06-26 22:20:56,854 main.py:49] epoch 3299, testing
[INFO 2017-06-26 22:20:58,513 main.py:102] average testing loss: 3259.53, base loss: 4607.02
[INFO 2017-06-26 22:20:58,514 main.py:103] improve_loss: 1347.49, improve_percent: 0.29
[INFO 2017-06-26 22:20:58,514 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 22:20:58,527 main.py:73] current best improved percent: 0.29
[INFO 2017-06-26 22:20:58,931 main.py:47] epoch 3300, training loss: 3200.94, average training loss: 3466.97, base loss: 4658.34
[INFO 2017-06-26 22:20:59,330 main.py:47] epoch 3301, training loss: 2762.05, average training loss: 3466.48, base loss: 4657.50
[INFO 2017-06-26 22:20:59,728 main.py:47] epoch 3302, training loss: 3292.63, average training loss: 3466.82, base loss: 4658.48
[INFO 2017-06-26 22:21:00,125 main.py:47] epoch 3303, training loss: 3552.37, average training loss: 3466.88, base loss: 4659.50
[INFO 2017-06-26 22:21:00,530 main.py:47] epoch 3304, training loss: 3229.62, average training loss: 3466.63, base loss: 4659.02
[INFO 2017-06-26 22:21:00,935 main.py:47] epoch 3305, training loss: 3212.00, average training loss: 3467.16, base loss: 4660.03
[INFO 2017-06-26 22:21:01,339 main.py:47] epoch 3306, training loss: 2822.52, average training loss: 3466.62, base loss: 4659.38
[INFO 2017-06-26 22:21:01,740 main.py:47] epoch 3307, training loss: 3254.65, average training loss: 3466.58, base loss: 4659.71
[INFO 2017-06-26 22:21:02,141 main.py:47] epoch 3308, training loss: 3174.84, average training loss: 3466.36, base loss: 4659.65
[INFO 2017-06-26 22:21:02,543 main.py:47] epoch 3309, training loss: 6307.30, average training loss: 3469.48, base loss: 4663.38
[INFO 2017-06-26 22:21:02,942 main.py:47] epoch 3310, training loss: 3336.89, average training loss: 3469.31, base loss: 4663.15
[INFO 2017-06-26 22:21:03,341 main.py:47] epoch 3311, training loss: 3501.09, average training loss: 3469.03, base loss: 4662.96
[INFO 2017-06-26 22:21:03,739 main.py:47] epoch 3312, training loss: 3546.10, average training loss: 3469.60, base loss: 4664.58
[INFO 2017-06-26 22:21:04,136 main.py:47] epoch 3313, training loss: 6152.34, average training loss: 3472.47, base loss: 4668.29
[INFO 2017-06-26 22:21:04,534 main.py:47] epoch 3314, training loss: 3450.92, average training loss: 3472.61, base loss: 4668.62
[INFO 2017-06-26 22:21:04,936 main.py:47] epoch 3315, training loss: 3301.13, average training loss: 3472.61, base loss: 4668.52
[INFO 2017-06-26 22:21:05,339 main.py:47] epoch 3316, training loss: 3408.35, average training loss: 3472.57, base loss: 4668.34
[INFO 2017-06-26 22:21:05,737 main.py:47] epoch 3317, training loss: 3301.75, average training loss: 3468.81, base loss: 4664.63
[INFO 2017-06-26 22:21:06,135 main.py:47] epoch 3318, training loss: 3354.43, average training loss: 3469.15, base loss: 4665.23
[INFO 2017-06-26 22:21:06,539 main.py:47] epoch 3319, training loss: 3158.09, average training loss: 3469.16, base loss: 4665.63
[INFO 2017-06-26 22:21:06,941 main.py:47] epoch 3320, training loss: 3041.28, average training loss: 3468.53, base loss: 4664.89
[INFO 2017-06-26 22:21:07,344 main.py:47] epoch 3321, training loss: 3621.27, average training loss: 3461.99, base loss: 4658.93
[INFO 2017-06-26 22:21:07,747 main.py:47] epoch 3322, training loss: 3067.28, average training loss: 3461.54, base loss: 4658.22
[INFO 2017-06-26 22:21:08,151 main.py:47] epoch 3323, training loss: 3211.44, average training loss: 3461.29, base loss: 4658.04
[INFO 2017-06-26 22:21:08,552 main.py:47] epoch 3324, training loss: 3007.26, average training loss: 3461.01, base loss: 4657.65
[INFO 2017-06-26 22:21:08,961 main.py:47] epoch 3325, training loss: 3148.45, average training loss: 3460.91, base loss: 4657.70
[INFO 2017-06-26 22:21:09,372 main.py:47] epoch 3326, training loss: 3170.28, average training loss: 3461.06, base loss: 4658.30
[INFO 2017-06-26 22:21:09,775 main.py:47] epoch 3327, training loss: 7176.78, average training loss: 3464.91, base loss: 4662.55
[INFO 2017-06-26 22:21:10,180 main.py:47] epoch 3328, training loss: 3373.59, average training loss: 3465.19, base loss: 4663.55
[INFO 2017-06-26 22:21:10,587 main.py:47] epoch 3329, training loss: 3352.11, average training loss: 3465.59, base loss: 4664.34
[INFO 2017-06-26 22:21:10,994 main.py:47] epoch 3330, training loss: 3503.76, average training loss: 3465.61, base loss: 4664.58
[INFO 2017-06-26 22:21:11,398 main.py:47] epoch 3331, training loss: 3488.12, average training loss: 3465.50, base loss: 4664.83
[INFO 2017-06-26 22:21:11,812 main.py:47] epoch 3332, training loss: 3210.88, average training loss: 3465.51, base loss: 4665.07
[INFO 2017-06-26 22:21:12,220 main.py:47] epoch 3333, training loss: 3286.97, average training loss: 3465.48, base loss: 4665.20
[INFO 2017-06-26 22:21:12,626 main.py:47] epoch 3334, training loss: 3041.05, average training loss: 3461.35, base loss: 4660.66
[INFO 2017-06-26 22:21:13,054 main.py:47] epoch 3335, training loss: 3134.50, average training loss: 3461.03, base loss: 4660.21
[INFO 2017-06-26 22:21:13,491 main.py:47] epoch 3336, training loss: 3739.53, average training loss: 3461.38, base loss: 4661.30
[INFO 2017-06-26 22:21:13,905 main.py:47] epoch 3337, training loss: 3110.50, average training loss: 3461.03, base loss: 4660.79
[INFO 2017-06-26 22:21:14,318 main.py:47] epoch 3338, training loss: 2642.57, average training loss: 3460.07, base loss: 4658.86
[INFO 2017-06-26 22:21:14,728 main.py:47] epoch 3339, training loss: 2852.86, average training loss: 3459.67, base loss: 4658.09
[INFO 2017-06-26 22:21:15,138 main.py:47] epoch 3340, training loss: 3276.46, average training loss: 3460.10, base loss: 4659.72
[INFO 2017-06-26 22:21:15,601 main.py:47] epoch 3341, training loss: 3342.72, average training loss: 3459.86, base loss: 4659.58
[INFO 2017-06-26 22:21:16,069 main.py:47] epoch 3342, training loss: 3072.87, average training loss: 3459.70, base loss: 4659.42
[INFO 2017-06-26 22:21:16,547 main.py:47] epoch 3343, training loss: 2567.29, average training loss: 3459.19, base loss: 4658.84
[INFO 2017-06-26 22:21:17,034 main.py:47] epoch 3344, training loss: 3491.43, average training loss: 3458.83, base loss: 4658.50
[INFO 2017-06-26 22:21:17,500 main.py:47] epoch 3345, training loss: 3330.59, average training loss: 3458.87, base loss: 4658.63
[INFO 2017-06-26 22:21:17,921 main.py:47] epoch 3346, training loss: 3258.45, average training loss: 3458.96, base loss: 4658.69
[INFO 2017-06-26 22:21:18,411 main.py:47] epoch 3347, training loss: 3250.33, average training loss: 3459.04, base loss: 4659.27
[INFO 2017-06-26 22:21:18,897 main.py:47] epoch 3348, training loss: 3257.49, average training loss: 3455.33, base loss: 4655.76
[INFO 2017-06-26 22:21:19,352 main.py:47] epoch 3349, training loss: 2987.79, average training loss: 3454.74, base loss: 4654.98
[INFO 2017-06-26 22:21:19,762 main.py:47] epoch 3350, training loss: 3149.30, average training loss: 3454.63, base loss: 4654.95
[INFO 2017-06-26 22:21:20,167 main.py:47] epoch 3351, training loss: 6530.30, average training loss: 3457.97, base loss: 4658.38
[INFO 2017-06-26 22:21:20,573 main.py:47] epoch 3352, training loss: 3366.01, average training loss: 3458.16, base loss: 4659.15
[INFO 2017-06-26 22:21:21,087 main.py:47] epoch 3353, training loss: 2953.48, average training loss: 3458.29, base loss: 4659.67
[INFO 2017-06-26 22:21:21,500 main.py:47] epoch 3354, training loss: 3054.87, average training loss: 3457.82, base loss: 4659.06
[INFO 2017-06-26 22:21:21,989 main.py:47] epoch 3355, training loss: 2875.24, average training loss: 3457.29, base loss: 4658.41
[INFO 2017-06-26 22:21:22,430 main.py:47] epoch 3356, training loss: 2777.11, average training loss: 3456.97, base loss: 4658.07
[INFO 2017-06-26 22:21:22,849 main.py:47] epoch 3357, training loss: 3012.53, average training loss: 3456.24, base loss: 4656.67
[INFO 2017-06-26 22:21:23,267 main.py:47] epoch 3358, training loss: 3055.25, average training loss: 3456.05, base loss: 4656.39
[INFO 2017-06-26 22:21:23,683 main.py:47] epoch 3359, training loss: 3369.99, average training loss: 3455.79, base loss: 4656.12
[INFO 2017-06-26 22:21:24,089 main.py:47] epoch 3360, training loss: 3262.44, average training loss: 3455.56, base loss: 4656.25
[INFO 2017-06-26 22:21:24,492 main.py:47] epoch 3361, training loss: 2778.90, average training loss: 3455.30, base loss: 4656.04
[INFO 2017-06-26 22:21:24,894 main.py:47] epoch 3362, training loss: 2808.45, average training loss: 3454.67, base loss: 4654.91
[INFO 2017-06-26 22:21:25,296 main.py:47] epoch 3363, training loss: 3043.45, average training loss: 3454.25, base loss: 4654.29
[INFO 2017-06-26 22:21:25,698 main.py:47] epoch 3364, training loss: 3196.65, average training loss: 3454.04, base loss: 4654.11
[INFO 2017-06-26 22:21:26,096 main.py:47] epoch 3365, training loss: 3437.65, average training loss: 3454.08, base loss: 4654.72
[INFO 2017-06-26 22:21:26,495 main.py:47] epoch 3366, training loss: 3535.20, average training loss: 3454.20, base loss: 4655.95
[INFO 2017-06-26 22:21:26,895 main.py:47] epoch 3367, training loss: 3093.31, average training loss: 3453.62, base loss: 4654.98
[INFO 2017-06-26 22:21:27,296 main.py:47] epoch 3368, training loss: 3301.64, average training loss: 3453.59, base loss: 4655.26
[INFO 2017-06-26 22:21:27,698 main.py:47] epoch 3369, training loss: 3483.73, average training loss: 3453.86, base loss: 4655.96
[INFO 2017-06-26 22:21:28,103 main.py:47] epoch 3370, training loss: 3233.28, average training loss: 3453.52, base loss: 4655.86
[INFO 2017-06-26 22:21:28,506 main.py:47] epoch 3371, training loss: 3104.13, average training loss: 3453.34, base loss: 4655.93
[INFO 2017-06-26 22:21:28,906 main.py:47] epoch 3372, training loss: 3030.89, average training loss: 3453.00, base loss: 4655.48
[INFO 2017-06-26 22:21:29,312 main.py:47] epoch 3373, training loss: 3012.98, average training loss: 3452.26, base loss: 4654.14
[INFO 2017-06-26 22:21:29,715 main.py:47] epoch 3374, training loss: 3138.03, average training loss: 3452.05, base loss: 4653.80
[INFO 2017-06-26 22:21:30,114 main.py:47] epoch 3375, training loss: 3142.63, average training loss: 3451.72, base loss: 4653.35
[INFO 2017-06-26 22:21:30,514 main.py:47] epoch 3376, training loss: 3371.89, average training loss: 3451.61, base loss: 4653.18
[INFO 2017-06-26 22:21:30,922 main.py:47] epoch 3377, training loss: 3529.08, average training loss: 3451.58, base loss: 4653.77
[INFO 2017-06-26 22:21:31,325 main.py:47] epoch 3378, training loss: 2783.03, average training loss: 3451.44, base loss: 4654.02
[INFO 2017-06-26 22:21:31,726 main.py:47] epoch 3379, training loss: 3234.43, average training loss: 3451.75, base loss: 4654.58
[INFO 2017-06-26 22:21:32,130 main.py:47] epoch 3380, training loss: 3566.98, average training loss: 3452.29, base loss: 4655.90
[INFO 2017-06-26 22:21:32,545 main.py:47] epoch 3381, training loss: 3081.29, average training loss: 3452.31, base loss: 4656.38
[INFO 2017-06-26 22:21:32,955 main.py:47] epoch 3382, training loss: 3353.80, average training loss: 3451.95, base loss: 4656.29
[INFO 2017-06-26 22:21:33,359 main.py:47] epoch 3383, training loss: 3005.72, average training loss: 3448.17, base loss: 4652.79
[INFO 2017-06-26 22:21:33,762 main.py:47] epoch 3384, training loss: 3148.80, average training loss: 3447.96, base loss: 4652.49
[INFO 2017-06-26 22:21:34,164 main.py:47] epoch 3385, training loss: 3186.71, average training loss: 3447.14, base loss: 4651.20
[INFO 2017-06-26 22:21:34,565 main.py:47] epoch 3386, training loss: 2995.79, average training loss: 3446.22, base loss: 4649.88
[INFO 2017-06-26 22:21:34,965 main.py:47] epoch 3387, training loss: 3728.16, average training loss: 3446.92, base loss: 4651.83
[INFO 2017-06-26 22:21:35,371 main.py:47] epoch 3388, training loss: 3087.89, average training loss: 3446.55, base loss: 4651.27
[INFO 2017-06-26 22:21:35,773 main.py:47] epoch 3389, training loss: 3138.80, average training loss: 3445.90, base loss: 4650.34
[INFO 2017-06-26 22:21:36,177 main.py:47] epoch 3390, training loss: 3401.27, average training loss: 3446.05, base loss: 4651.03
[INFO 2017-06-26 22:21:36,578 main.py:47] epoch 3391, training loss: 3421.23, average training loss: 3442.49, base loss: 4647.58
[INFO 2017-06-26 22:21:36,978 main.py:47] epoch 3392, training loss: 3228.48, average training loss: 3442.11, base loss: 4647.32
[INFO 2017-06-26 22:21:37,376 main.py:47] epoch 3393, training loss: 2987.74, average training loss: 3441.84, base loss: 4647.17
[INFO 2017-06-26 22:21:37,774 main.py:47] epoch 3394, training loss: 3058.90, average training loss: 3441.44, base loss: 4646.66
[INFO 2017-06-26 22:21:38,186 main.py:47] epoch 3395, training loss: 2955.61, average training loss: 3441.32, base loss: 4646.92
[INFO 2017-06-26 22:21:38,588 main.py:47] epoch 3396, training loss: 6588.83, average training loss: 3444.46, base loss: 4650.00
[INFO 2017-06-26 22:21:38,991 main.py:47] epoch 3397, training loss: 3195.83, average training loss: 3443.95, base loss: 4649.66
[INFO 2017-06-26 22:21:39,395 main.py:47] epoch 3398, training loss: 3140.98, average training loss: 3443.93, base loss: 4649.84
[INFO 2017-06-26 22:21:39,803 main.py:47] epoch 3399, training loss: 3199.89, average training loss: 3444.09, base loss: 4650.68
[INFO 2017-06-26 22:21:39,803 main.py:49] epoch 3399, testing
[INFO 2017-06-26 22:21:41,462 main.py:102] average testing loss: 3149.34, base loss: 4537.48
[INFO 2017-06-26 22:21:41,462 main.py:103] improve_loss: 1388.14, improve_percent: 0.31
[INFO 2017-06-26 22:21:41,463 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 22:21:41,475 main.py:73] current best improved percent: 0.31
[INFO 2017-06-26 22:21:41,875 main.py:47] epoch 3400, training loss: 3301.12, average training loss: 3444.25, base loss: 4651.82
[INFO 2017-06-26 22:21:42,276 main.py:47] epoch 3401, training loss: 2912.96, average training loss: 3443.99, base loss: 4651.57
[INFO 2017-06-26 22:21:42,678 main.py:47] epoch 3402, training loss: 3131.71, average training loss: 3443.82, base loss: 4651.28
[INFO 2017-06-26 22:21:43,083 main.py:47] epoch 3403, training loss: 2938.40, average training loss: 3443.37, base loss: 4650.58
[INFO 2017-06-26 22:21:43,487 main.py:47] epoch 3404, training loss: 3215.03, average training loss: 3443.33, base loss: 4650.93
[INFO 2017-06-26 22:21:43,890 main.py:47] epoch 3405, training loss: 3127.77, average training loss: 3443.12, base loss: 4650.88
[INFO 2017-06-26 22:21:44,292 main.py:47] epoch 3406, training loss: 3086.00, average training loss: 3443.07, base loss: 4650.71
[INFO 2017-06-26 22:21:44,694 main.py:47] epoch 3407, training loss: 6218.21, average training loss: 3446.05, base loss: 4653.72
[INFO 2017-06-26 22:21:45,101 main.py:47] epoch 3408, training loss: 3282.36, average training loss: 3446.12, base loss: 4654.44
[INFO 2017-06-26 22:21:45,502 main.py:47] epoch 3409, training loss: 3236.72, average training loss: 3446.27, base loss: 4654.85
[INFO 2017-06-26 22:21:45,904 main.py:47] epoch 3410, training loss: 3552.38, average training loss: 3446.65, base loss: 4656.08
[INFO 2017-06-26 22:21:46,309 main.py:47] epoch 3411, training loss: 2990.07, average training loss: 3446.50, base loss: 4656.44
[INFO 2017-06-26 22:21:46,724 main.py:47] epoch 3412, training loss: 3176.96, average training loss: 3446.67, base loss: 4657.05
[INFO 2017-06-26 22:21:47,128 main.py:47] epoch 3413, training loss: 3097.74, average training loss: 3446.59, base loss: 4657.17
[INFO 2017-06-26 22:21:47,541 main.py:47] epoch 3414, training loss: 3364.63, average training loss: 3447.22, base loss: 4658.54
[INFO 2017-06-26 22:21:47,948 main.py:47] epoch 3415, training loss: 3067.06, average training loss: 3447.21, base loss: 4659.09
[INFO 2017-06-26 22:21:48,349 main.py:47] epoch 3416, training loss: 3094.39, average training loss: 3447.17, base loss: 4659.04
[INFO 2017-06-26 22:21:48,760 main.py:47] epoch 3417, training loss: 2847.85, average training loss: 3446.54, base loss: 4657.98
[INFO 2017-06-26 22:21:49,168 main.py:47] epoch 3418, training loss: 3169.33, average training loss: 3446.28, base loss: 4657.33
[INFO 2017-06-26 22:21:49,577 main.py:47] epoch 3419, training loss: 3146.13, average training loss: 3446.38, base loss: 4657.43
[INFO 2017-06-26 22:21:49,988 main.py:47] epoch 3420, training loss: 3071.34, average training loss: 3446.03, base loss: 4657.35
[INFO 2017-06-26 22:21:50,400 main.py:47] epoch 3421, training loss: 3118.62, average training loss: 3445.56, base loss: 4656.54
[INFO 2017-06-26 22:21:50,813 main.py:47] epoch 3422, training loss: 3092.81, average training loss: 3445.63, base loss: 4657.13
[INFO 2017-06-26 22:21:51,280 main.py:47] epoch 3423, training loss: 2800.10, average training loss: 3445.24, base loss: 4656.45
[INFO 2017-06-26 22:21:51,754 main.py:47] epoch 3424, training loss: 2780.44, average training loss: 3444.88, base loss: 4656.30
[INFO 2017-06-26 22:21:52,234 main.py:47] epoch 3425, training loss: 3222.22, average training loss: 3444.92, base loss: 4656.62
[INFO 2017-06-26 22:21:52,708 main.py:47] epoch 3426, training loss: 3167.94, average training loss: 3444.95, base loss: 4656.94
[INFO 2017-06-26 22:21:53,203 main.py:47] epoch 3427, training loss: 3165.05, average training loss: 3445.24, base loss: 4657.83
[INFO 2017-06-26 22:21:53,629 main.py:47] epoch 3428, training loss: 3284.62, average training loss: 3441.71, base loss: 4654.97
[INFO 2017-06-26 22:21:54,121 main.py:47] epoch 3429, training loss: 3130.40, average training loss: 3441.25, base loss: 4654.33
[INFO 2017-06-26 22:21:54,630 main.py:47] epoch 3430, training loss: 2815.21, average training loss: 3440.27, base loss: 4652.91
[INFO 2017-06-26 22:21:55,063 main.py:47] epoch 3431, training loss: 3269.71, average training loss: 3440.30, base loss: 4653.30
[INFO 2017-06-26 22:21:55,486 main.py:47] epoch 3432, training loss: 3486.11, average training loss: 3440.32, base loss: 4653.46
[INFO 2017-06-26 22:21:55,891 main.py:47] epoch 3433, training loss: 3286.91, average training loss: 3440.46, base loss: 4654.10
[INFO 2017-06-26 22:21:56,292 main.py:47] epoch 3434, training loss: 3443.70, average training loss: 3440.57, base loss: 4654.90
[INFO 2017-06-26 22:21:56,797 main.py:47] epoch 3435, training loss: 2798.61, average training loss: 3440.22, base loss: 4654.44
[INFO 2017-06-26 22:21:57,219 main.py:47] epoch 3436, training loss: 3290.68, average training loss: 3440.13, base loss: 4654.68
[INFO 2017-06-26 22:21:57,630 main.py:47] epoch 3437, training loss: 3111.27, average training loss: 3439.57, base loss: 4653.68
[INFO 2017-06-26 22:21:58,045 main.py:47] epoch 3438, training loss: 3398.97, average training loss: 3439.44, base loss: 4654.19
[INFO 2017-06-26 22:21:58,459 main.py:47] epoch 3439, training loss: 3310.34, average training loss: 3439.73, base loss: 4655.28
[INFO 2017-06-26 22:21:58,863 main.py:47] epoch 3440, training loss: 3177.51, average training loss: 3439.51, base loss: 4655.32
[INFO 2017-06-26 22:21:59,267 main.py:47] epoch 3441, training loss: 2986.78, average training loss: 3439.07, base loss: 4654.94
[INFO 2017-06-26 22:21:59,670 main.py:47] epoch 3442, training loss: 3066.58, average training loss: 3438.79, base loss: 4654.83
[INFO 2017-06-26 22:22:00,073 main.py:47] epoch 3443, training loss: 3066.92, average training loss: 3438.74, base loss: 4654.81
[INFO 2017-06-26 22:22:00,475 main.py:47] epoch 3444, training loss: 3552.74, average training loss: 3439.36, base loss: 4656.12
[INFO 2017-06-26 22:22:00,877 main.py:47] epoch 3445, training loss: 3120.53, average training loss: 3439.35, base loss: 4656.53
[INFO 2017-06-26 22:22:01,282 main.py:47] epoch 3446, training loss: 3638.45, average training loss: 3436.17, base loss: 4654.43
[INFO 2017-06-26 22:22:01,684 main.py:47] epoch 3447, training loss: 3112.52, average training loss: 3436.00, base loss: 4654.23
[INFO 2017-06-26 22:22:02,088 main.py:47] epoch 3448, training loss: 2815.63, average training loss: 3434.91, base loss: 4652.38
[INFO 2017-06-26 22:22:02,493 main.py:47] epoch 3449, training loss: 3408.17, average training loss: 3435.02, base loss: 4652.52
[INFO 2017-06-26 22:22:02,896 main.py:47] epoch 3450, training loss: 2829.68, average training loss: 3434.76, base loss: 4652.25
[INFO 2017-06-26 22:22:03,299 main.py:47] epoch 3451, training loss: 3486.15, average training loss: 3434.65, base loss: 4652.23
[INFO 2017-06-26 22:22:03,702 main.py:47] epoch 3452, training loss: 3271.09, average training loss: 3434.40, base loss: 4651.80
[INFO 2017-06-26 22:22:04,104 main.py:47] epoch 3453, training loss: 3489.37, average training loss: 3434.84, base loss: 4652.64
[INFO 2017-06-26 22:22:04,507 main.py:47] epoch 3454, training loss: 3858.12, average training loss: 3435.17, base loss: 4653.48
[INFO 2017-06-26 22:22:04,912 main.py:47] epoch 3455, training loss: 3511.06, average training loss: 3435.58, base loss: 4654.75
[INFO 2017-06-26 22:22:05,316 main.py:47] epoch 3456, training loss: 3139.46, average training loss: 3435.33, base loss: 4654.97
[INFO 2017-06-26 22:22:05,719 main.py:47] epoch 3457, training loss: 3118.20, average training loss: 3435.01, base loss: 4654.74
[INFO 2017-06-26 22:22:06,122 main.py:47] epoch 3458, training loss: 3208.97, average training loss: 3435.43, base loss: 4656.01
[INFO 2017-06-26 22:22:06,531 main.py:47] epoch 3459, training loss: 3272.60, average training loss: 3435.52, base loss: 4656.01
[INFO 2017-06-26 22:22:06,937 main.py:47] epoch 3460, training loss: 3177.64, average training loss: 3431.83, base loss: 4652.21
[INFO 2017-06-26 22:22:07,383 main.py:47] epoch 3461, training loss: 3307.52, average training loss: 3431.66, base loss: 4651.97
[INFO 2017-06-26 22:22:07,820 main.py:47] epoch 3462, training loss: 3308.50, average training loss: 3431.57, base loss: 4652.24
[INFO 2017-06-26 22:22:08,232 main.py:47] epoch 3463, training loss: 3145.42, average training loss: 3431.46, base loss: 4652.04
[INFO 2017-06-26 22:22:08,736 main.py:47] epoch 3464, training loss: 5720.11, average training loss: 3433.86, base loss: 4654.85
[INFO 2017-06-26 22:22:09,147 main.py:47] epoch 3465, training loss: 2884.30, average training loss: 3433.54, base loss: 4654.52
[INFO 2017-06-26 22:22:09,638 main.py:47] epoch 3466, training loss: 3491.72, average training loss: 3434.16, base loss: 4656.17
[INFO 2017-06-26 22:22:10,048 main.py:47] epoch 3467, training loss: 3567.87, average training loss: 3433.96, base loss: 4656.17
[INFO 2017-06-26 22:22:10,462 main.py:47] epoch 3468, training loss: 3512.06, average training loss: 3434.19, base loss: 4656.79
[INFO 2017-06-26 22:22:10,876 main.py:47] epoch 3469, training loss: 2926.21, average training loss: 3433.97, base loss: 4656.73
[INFO 2017-06-26 22:22:11,287 main.py:47] epoch 3470, training loss: 3342.29, average training loss: 3434.09, base loss: 4657.27
[INFO 2017-06-26 22:22:11,685 main.py:47] epoch 3471, training loss: 3327.97, average training loss: 3433.77, base loss: 4656.84
[INFO 2017-06-26 22:22:12,084 main.py:47] epoch 3472, training loss: 3615.73, average training loss: 3434.09, base loss: 4657.91
[INFO 2017-06-26 22:22:12,488 main.py:47] epoch 3473, training loss: 3245.21, average training loss: 3434.05, base loss: 4657.80
[INFO 2017-06-26 22:22:12,894 main.py:47] epoch 3474, training loss: 3272.40, average training loss: 3434.43, base loss: 4659.00
[INFO 2017-06-26 22:22:13,296 main.py:47] epoch 3475, training loss: 3204.51, average training loss: 3434.52, base loss: 4659.40
[INFO 2017-06-26 22:22:13,701 main.py:47] epoch 3476, training loss: 3047.20, average training loss: 3434.28, base loss: 4659.47
[INFO 2017-06-26 22:22:14,105 main.py:47] epoch 3477, training loss: 3458.46, average training loss: 3434.27, base loss: 4659.91
[INFO 2017-06-26 22:22:14,502 main.py:47] epoch 3478, training loss: 3250.10, average training loss: 3434.01, base loss: 4659.73
[INFO 2017-06-26 22:22:14,901 main.py:47] epoch 3479, training loss: 6532.29, average training loss: 3429.94, base loss: 4656.01
[INFO 2017-06-26 22:22:15,304 main.py:47] epoch 3480, training loss: 3358.55, average training loss: 3430.01, base loss: 4656.10
[INFO 2017-06-26 22:22:15,707 main.py:47] epoch 3481, training loss: 3472.48, average training loss: 3429.95, base loss: 4655.99
[INFO 2017-06-26 22:22:16,109 main.py:47] epoch 3482, training loss: 3096.33, average training loss: 3430.01, base loss: 4656.15
[INFO 2017-06-26 22:22:16,506 main.py:47] epoch 3483, training loss: 3308.23, average training loss: 3430.30, base loss: 4657.00
[INFO 2017-06-26 22:22:16,908 main.py:47] epoch 3484, training loss: 3031.99, average training loss: 3429.80, base loss: 4656.47
[INFO 2017-06-26 22:22:17,311 main.py:47] epoch 3485, training loss: 2834.97, average training loss: 3429.38, base loss: 4655.98
[INFO 2017-06-26 22:22:17,714 main.py:47] epoch 3486, training loss: 3311.07, average training loss: 3429.22, base loss: 4655.97
[INFO 2017-06-26 22:22:18,215 main.py:47] epoch 3487, training loss: 3303.27, average training loss: 3428.82, base loss: 4655.40
[INFO 2017-06-26 22:22:18,621 main.py:47] epoch 3488, training loss: 3035.46, average training loss: 3428.85, base loss: 4655.87
[INFO 2017-06-26 22:22:19,065 main.py:47] epoch 3489, training loss: 3210.44, average training loss: 3428.62, base loss: 4655.82
[INFO 2017-06-26 22:22:19,514 main.py:47] epoch 3490, training loss: 3624.86, average training loss: 3429.20, base loss: 4657.37
[INFO 2017-06-26 22:22:19,923 main.py:47] epoch 3491, training loss: 3391.01, average training loss: 3429.33, base loss: 4657.78
[INFO 2017-06-26 22:22:20,335 main.py:47] epoch 3492, training loss: 2811.38, average training loss: 3429.09, base loss: 4657.66
[INFO 2017-06-26 22:22:20,734 main.py:47] epoch 3493, training loss: 2942.27, average training loss: 3428.46, base loss: 4656.88
[INFO 2017-06-26 22:22:21,133 main.py:47] epoch 3494, training loss: 2875.96, average training loss: 3428.03, base loss: 4656.34
[INFO 2017-06-26 22:22:21,553 main.py:47] epoch 3495, training loss: 5989.35, average training loss: 3430.83, base loss: 4660.07
[INFO 2017-06-26 22:22:21,952 main.py:47] epoch 3496, training loss: 3116.12, average training loss: 3430.50, base loss: 4659.58
[INFO 2017-06-26 22:22:22,357 main.py:47] epoch 3497, training loss: 2849.46, average training loss: 3429.83, base loss: 4658.75
[INFO 2017-06-26 22:22:22,771 main.py:47] epoch 3498, training loss: 3168.27, average training loss: 3429.70, base loss: 4659.18
[INFO 2017-06-26 22:22:23,174 main.py:47] epoch 3499, training loss: 3383.90, average training loss: 3429.64, base loss: 4659.33
[INFO 2017-06-26 22:22:23,174 main.py:49] epoch 3499, testing
[INFO 2017-06-26 22:22:24,830 main.py:102] average testing loss: 3067.10, base loss: 4288.00
[INFO 2017-06-26 22:22:24,831 main.py:103] improve_loss: 1220.91, improve_percent: 0.28
[INFO 2017-06-26 22:22:24,831 main.py:73] current best improved percent: 0.31
[INFO 2017-06-26 22:22:25,232 main.py:47] epoch 3500, training loss: 3610.66, average training loss: 3430.23, base loss: 4660.46
[INFO 2017-06-26 22:22:25,635 main.py:47] epoch 3501, training loss: 3552.06, average training loss: 3430.41, base loss: 4660.89
[INFO 2017-06-26 22:22:26,036 main.py:47] epoch 3502, training loss: 3327.36, average training loss: 3430.25, base loss: 4660.62
[INFO 2017-06-26 22:22:26,435 main.py:47] epoch 3503, training loss: 3765.69, average training loss: 3430.04, base loss: 4660.73
[INFO 2017-06-26 22:22:26,837 main.py:47] epoch 3504, training loss: 3551.00, average training loss: 3430.15, base loss: 4660.97
[INFO 2017-06-26 22:22:27,237 main.py:47] epoch 3505, training loss: 3430.25, average training loss: 3430.13, base loss: 4661.63
[INFO 2017-06-26 22:22:27,641 main.py:47] epoch 3506, training loss: 3338.28, average training loss: 3430.18, base loss: 4661.89
[INFO 2017-06-26 22:22:28,041 main.py:47] epoch 3507, training loss: 3238.39, average training loss: 3423.40, base loss: 4655.55
[INFO 2017-06-26 22:22:28,438 main.py:47] epoch 3508, training loss: 3096.14, average training loss: 3423.80, base loss: 4656.60
[INFO 2017-06-26 22:22:28,837 main.py:47] epoch 3509, training loss: 3150.90, average training loss: 3423.49, base loss: 4656.63
[INFO 2017-06-26 22:22:29,235 main.py:47] epoch 3510, training loss: 3326.54, average training loss: 3423.87, base loss: 4657.34
[INFO 2017-06-26 22:22:29,640 main.py:47] epoch 3511, training loss: 3281.88, average training loss: 3423.82, base loss: 4657.58
[INFO 2017-06-26 22:22:30,045 main.py:47] epoch 3512, training loss: 2885.15, average training loss: 3419.48, base loss: 4652.87
[INFO 2017-06-26 22:22:30,447 main.py:47] epoch 3513, training loss: 2890.73, average training loss: 3419.26, base loss: 4652.59
[INFO 2017-06-26 22:22:30,846 main.py:47] epoch 3514, training loss: 3449.30, average training loss: 3419.29, base loss: 4652.85
[INFO 2017-06-26 22:22:31,244 main.py:47] epoch 3515, training loss: 3239.24, average training loss: 3419.12, base loss: 4652.42
[INFO 2017-06-26 22:22:31,648 main.py:47] epoch 3516, training loss: 3438.99, average training loss: 3419.35, base loss: 4652.87
[INFO 2017-06-26 22:22:32,051 main.py:47] epoch 3517, training loss: 3192.82, average training loss: 3419.28, base loss: 4652.92
[INFO 2017-06-26 22:22:32,454 main.py:47] epoch 3518, training loss: 3092.47, average training loss: 3419.02, base loss: 4652.37
[INFO 2017-06-26 22:22:32,852 main.py:47] epoch 3519, training loss: 2907.90, average training loss: 3418.83, base loss: 4652.33
[INFO 2017-06-26 22:22:33,250 main.py:47] epoch 3520, training loss: 3229.17, average training loss: 3418.70, base loss: 4652.67
[INFO 2017-06-26 22:22:33,653 main.py:47] epoch 3521, training loss: 3230.99, average training loss: 3419.10, base loss: 4654.09
[INFO 2017-06-26 22:22:34,058 main.py:47] epoch 3522, training loss: 2981.11, average training loss: 3414.77, base loss: 4649.22
[INFO 2017-06-26 22:22:34,462 main.py:47] epoch 3523, training loss: 3036.02, average training loss: 3413.59, base loss: 4647.86
[INFO 2017-06-26 22:22:34,862 main.py:47] epoch 3524, training loss: 3215.15, average training loss: 3413.42, base loss: 4647.63
[INFO 2017-06-26 22:22:35,265 main.py:47] epoch 3525, training loss: 3033.21, average training loss: 3412.99, base loss: 4647.42
[INFO 2017-06-26 22:22:35,666 main.py:47] epoch 3526, training loss: 3217.76, average training loss: 3413.28, base loss: 4648.05
[INFO 2017-06-26 22:22:36,068 main.py:47] epoch 3527, training loss: 3168.15, average training loss: 3413.47, base loss: 4648.70
[INFO 2017-06-26 22:22:36,471 main.py:47] epoch 3528, training loss: 6517.92, average training loss: 3416.74, base loss: 4652.16
[INFO 2017-06-26 22:22:36,875 main.py:47] epoch 3529, training loss: 3087.94, average training loss: 3416.60, base loss: 4652.30
[INFO 2017-06-26 22:22:37,279 main.py:47] epoch 3530, training loss: 3597.82, average training loss: 3417.20, base loss: 4653.76
[INFO 2017-06-26 22:22:37,676 main.py:47] epoch 3531, training loss: 2953.64, average training loss: 3416.65, base loss: 4652.74
[INFO 2017-06-26 22:22:38,080 main.py:47] epoch 3532, training loss: 3300.17, average training loss: 3416.84, base loss: 4653.30
[INFO 2017-06-26 22:22:38,485 main.py:47] epoch 3533, training loss: 3489.04, average training loss: 3416.62, base loss: 4653.13
[INFO 2017-06-26 22:22:38,889 main.py:47] epoch 3534, training loss: 3356.26, average training loss: 3416.91, base loss: 4654.13
[INFO 2017-06-26 22:22:39,298 main.py:47] epoch 3535, training loss: 3248.88, average training loss: 3417.06, base loss: 4654.92
[INFO 2017-06-26 22:22:39,697 main.py:47] epoch 3536, training loss: 3095.03, average training loss: 3417.07, base loss: 4655.41
[INFO 2017-06-26 22:22:40,096 main.py:47] epoch 3537, training loss: 2971.31, average training loss: 3416.98, base loss: 4655.45
[INFO 2017-06-26 22:22:40,495 main.py:47] epoch 3538, training loss: 2915.73, average training loss: 3416.42, base loss: 4654.86
[INFO 2017-06-26 22:22:40,895 main.py:47] epoch 3539, training loss: 2896.09, average training loss: 3416.22, base loss: 4655.12
[INFO 2017-06-26 22:22:41,296 main.py:47] epoch 3540, training loss: 3327.58, average training loss: 3416.49, base loss: 4656.11
[INFO 2017-06-26 22:22:41,705 main.py:47] epoch 3541, training loss: 2883.95, average training loss: 3415.69, base loss: 4654.73
[INFO 2017-06-26 22:22:42,111 main.py:47] epoch 3542, training loss: 3114.56, average training loss: 3415.51, base loss: 4654.71
[INFO 2017-06-26 22:22:42,509 main.py:47] epoch 3543, training loss: 2832.01, average training loss: 3414.82, base loss: 4653.98
[INFO 2017-06-26 22:22:42,908 main.py:47] epoch 3544, training loss: 3296.76, average training loss: 3414.80, base loss: 4654.35
[INFO 2017-06-26 22:22:43,313 main.py:47] epoch 3545, training loss: 3165.00, average training loss: 3414.59, base loss: 4654.20
[INFO 2017-06-26 22:22:43,712 main.py:47] epoch 3546, training loss: 3121.27, average training loss: 3414.38, base loss: 4653.98
[INFO 2017-06-26 22:22:44,123 main.py:47] epoch 3547, training loss: 3006.38, average training loss: 3414.31, base loss: 4654.07
[INFO 2017-06-26 22:22:44,522 main.py:47] epoch 3548, training loss: 3230.04, average training loss: 3414.08, base loss: 4654.32
[INFO 2017-06-26 22:22:44,932 main.py:47] epoch 3549, training loss: 3038.39, average training loss: 3413.47, base loss: 4653.53
[INFO 2017-06-26 22:22:45,337 main.py:47] epoch 3550, training loss: 2956.01, average training loss: 3412.72, base loss: 4652.22
[INFO 2017-06-26 22:22:45,742 main.py:47] epoch 3551, training loss: 2884.78, average training loss: 3412.55, base loss: 4652.20
[INFO 2017-06-26 22:22:46,146 main.py:47] epoch 3552, training loss: 3361.14, average training loss: 3412.69, base loss: 4652.61
[INFO 2017-06-26 22:22:46,549 main.py:47] epoch 3553, training loss: 3014.50, average training loss: 3412.22, base loss: 4652.28
[INFO 2017-06-26 22:22:46,949 main.py:47] epoch 3554, training loss: 3303.55, average training loss: 3412.41, base loss: 4652.88
[INFO 2017-06-26 22:22:47,349 main.py:47] epoch 3555, training loss: 3592.95, average training loss: 3409.63, base loss: 4650.92
[INFO 2017-06-26 22:22:47,752 main.py:47] epoch 3556, training loss: 3558.46, average training loss: 3409.75, base loss: 4651.23
[INFO 2017-06-26 22:22:48,156 main.py:47] epoch 3557, training loss: 3105.42, average training loss: 3409.77, base loss: 4651.88
[INFO 2017-06-26 22:22:48,560 main.py:47] epoch 3558, training loss: 3355.48, average training loss: 3409.59, base loss: 4651.95
[INFO 2017-06-26 22:22:48,965 main.py:47] epoch 3559, training loss: 3297.23, average training loss: 3409.72, base loss: 4652.45
[INFO 2017-06-26 22:22:49,369 main.py:47] epoch 3560, training loss: 3223.26, average training loss: 3410.14, base loss: 4653.47
[INFO 2017-06-26 22:22:49,773 main.py:47] epoch 3561, training loss: 2870.92, average training loss: 3409.94, base loss: 4653.22
[INFO 2017-06-26 22:22:50,175 main.py:47] epoch 3562, training loss: 3566.24, average training loss: 3410.60, base loss: 4654.84
[INFO 2017-06-26 22:22:50,574 main.py:47] epoch 3563, training loss: 5942.94, average training loss: 3413.46, base loss: 4658.71
[INFO 2017-06-26 22:22:50,979 main.py:47] epoch 3564, training loss: 3079.79, average training loss: 3413.34, base loss: 4658.95
[INFO 2017-06-26 22:22:51,384 main.py:47] epoch 3565, training loss: 3026.41, average training loss: 3412.62, base loss: 4658.07
[INFO 2017-06-26 22:22:51,783 main.py:47] epoch 3566, training loss: 3426.74, average training loss: 3413.41, base loss: 4659.53
[INFO 2017-06-26 22:22:52,180 main.py:47] epoch 3567, training loss: 3584.25, average training loss: 3413.77, base loss: 4660.63
[INFO 2017-06-26 22:22:52,579 main.py:47] epoch 3568, training loss: 3441.07, average training loss: 3413.99, base loss: 4661.64
[INFO 2017-06-26 22:22:52,977 main.py:47] epoch 3569, training loss: 3393.73, average training loss: 3414.35, base loss: 4662.49
[INFO 2017-06-26 22:22:53,374 main.py:47] epoch 3570, training loss: 3063.83, average training loss: 3413.66, base loss: 4661.33
[INFO 2017-06-26 22:22:53,773 main.py:47] epoch 3571, training loss: 3262.97, average training loss: 3413.53, base loss: 4661.34
[INFO 2017-06-26 22:22:54,175 main.py:47] epoch 3572, training loss: 3356.40, average training loss: 3413.78, base loss: 4662.11
[INFO 2017-06-26 22:22:54,574 main.py:47] epoch 3573, training loss: 2802.26, average training loss: 3413.22, base loss: 4661.53
[INFO 2017-06-26 22:22:54,976 main.py:47] epoch 3574, training loss: 2853.45, average training loss: 3412.41, base loss: 4660.40
[INFO 2017-06-26 22:22:55,379 main.py:47] epoch 3575, training loss: 2844.14, average training loss: 3412.08, base loss: 4660.15
[INFO 2017-06-26 22:22:55,782 main.py:47] epoch 3576, training loss: 3558.18, average training loss: 3412.44, base loss: 4661.30
[INFO 2017-06-26 22:22:56,180 main.py:47] epoch 3577, training loss: 2958.49, average training loss: 3412.14, base loss: 4660.97
[INFO 2017-06-26 22:22:56,578 main.py:47] epoch 3578, training loss: 3417.75, average training loss: 3412.63, base loss: 4661.89
[INFO 2017-06-26 22:22:56,985 main.py:47] epoch 3579, training loss: 3022.27, average training loss: 3412.42, base loss: 4661.83
[INFO 2017-06-26 22:22:57,387 main.py:47] epoch 3580, training loss: 3021.44, average training loss: 3408.45, base loss: 4657.71
[INFO 2017-06-26 22:22:57,789 main.py:47] epoch 3581, training loss: 3401.15, average training loss: 3408.24, base loss: 4657.49
[INFO 2017-06-26 22:22:58,195 main.py:47] epoch 3582, training loss: 3412.68, average training loss: 3407.86, base loss: 4656.71
[INFO 2017-06-26 22:22:58,598 main.py:47] epoch 3583, training loss: 3308.80, average training loss: 3407.59, base loss: 4656.70
[INFO 2017-06-26 22:22:59,001 main.py:47] epoch 3584, training loss: 3395.54, average training loss: 3407.90, base loss: 4657.32
[INFO 2017-06-26 22:22:59,403 main.py:47] epoch 3585, training loss: 3166.87, average training loss: 3407.93, base loss: 4657.31
[INFO 2017-06-26 22:22:59,807 main.py:47] epoch 3586, training loss: 6545.09, average training loss: 3407.68, base loss: 4657.11
[INFO 2017-06-26 22:23:00,208 main.py:47] epoch 3587, training loss: 3249.60, average training loss: 3407.92, base loss: 4657.70
[INFO 2017-06-26 22:23:00,608 main.py:47] epoch 3588, training loss: 3280.23, average training loss: 3408.11, base loss: 4658.48
[INFO 2017-06-26 22:23:01,014 main.py:47] epoch 3589, training loss: 2925.67, average training loss: 3407.46, base loss: 4657.35
[INFO 2017-06-26 22:23:01,416 main.py:47] epoch 3590, training loss: 3034.78, average training loss: 3407.37, base loss: 4657.19
[INFO 2017-06-26 22:23:01,818 main.py:47] epoch 3591, training loss: 3016.45, average training loss: 3407.09, base loss: 4656.85
[INFO 2017-06-26 22:23:02,221 main.py:47] epoch 3592, training loss: 2989.00, average training loss: 3406.81, base loss: 4656.81
[INFO 2017-06-26 22:23:02,621 main.py:47] epoch 3593, training loss: 3121.29, average training loss: 3406.64, base loss: 4656.90
[INFO 2017-06-26 22:23:03,026 main.py:47] epoch 3594, training loss: 3042.81, average training loss: 3406.34, base loss: 4656.23
[INFO 2017-06-26 22:23:03,430 main.py:47] epoch 3595, training loss: 3317.17, average training loss: 3406.88, base loss: 4657.51
[INFO 2017-06-26 22:23:03,877 main.py:47] epoch 3596, training loss: 2975.46, average training loss: 3403.46, base loss: 4654.30
[INFO 2017-06-26 22:23:04,326 main.py:47] epoch 3597, training loss: 3203.48, average training loss: 3403.08, base loss: 4653.61
[INFO 2017-06-26 22:23:04,736 main.py:47] epoch 3598, training loss: 3100.84, average training loss: 3403.13, base loss: 4653.96
[INFO 2017-06-26 22:23:05,161 main.py:47] epoch 3599, training loss: 3099.21, average training loss: 3403.31, base loss: 4654.38
[INFO 2017-06-26 22:23:05,161 main.py:49] epoch 3599, testing
[INFO 2017-06-26 22:23:06,811 main.py:102] average testing loss: 3160.23, base loss: 4315.94
[INFO 2017-06-26 22:23:06,811 main.py:103] improve_loss: 1155.70, improve_percent: 0.27
[INFO 2017-06-26 22:23:06,812 main.py:73] current best improved percent: 0.31
[INFO 2017-06-26 22:23:07,223 main.py:47] epoch 3600, training loss: 3300.65, average training loss: 3399.90, base loss: 4650.99
[INFO 2017-06-26 22:23:07,627 main.py:47] epoch 3601, training loss: 3330.12, average training loss: 3399.78, base loss: 4651.35
[INFO 2017-06-26 22:23:08,039 main.py:47] epoch 3602, training loss: 2867.43, average training loss: 3399.54, base loss: 4651.09
[INFO 2017-06-26 22:23:08,441 main.py:47] epoch 3603, training loss: 3043.69, average training loss: 3398.68, base loss: 4649.48
[INFO 2017-06-26 22:23:08,843 main.py:47] epoch 3604, training loss: 2765.44, average training loss: 3398.47, base loss: 4648.99
[INFO 2017-06-26 22:23:09,248 main.py:47] epoch 3605, training loss: 3046.46, average training loss: 3398.03, base loss: 4648.38
[INFO 2017-06-26 22:23:09,654 main.py:47] epoch 3606, training loss: 3091.62, average training loss: 3397.48, base loss: 4647.45
[INFO 2017-06-26 22:23:10,055 main.py:47] epoch 3607, training loss: 3060.38, average training loss: 3397.13, base loss: 4646.99
[INFO 2017-06-26 22:23:10,457 main.py:47] epoch 3608, training loss: 3003.87, average training loss: 3396.85, base loss: 4646.53
[INFO 2017-06-26 22:23:10,854 main.py:47] epoch 3609, training loss: 3937.03, average training loss: 3397.70, base loss: 4648.23
[INFO 2017-06-26 22:23:11,259 main.py:47] epoch 3610, training loss: 3131.35, average training loss: 3397.41, base loss: 4647.82
[INFO 2017-06-26 22:23:11,669 main.py:47] epoch 3611, training loss: 3106.06, average training loss: 3393.48, base loss: 4643.59
[INFO 2017-06-26 22:23:12,069 main.py:47] epoch 3612, training loss: 3326.47, average training loss: 3394.00, base loss: 4644.87
[INFO 2017-06-26 22:23:12,472 main.py:47] epoch 3613, training loss: 3931.86, average training loss: 3394.74, base loss: 4646.58
[INFO 2017-06-26 22:23:12,971 main.py:47] epoch 3614, training loss: 3423.20, average training loss: 3394.83, base loss: 4647.36
[INFO 2017-06-26 22:23:13,378 main.py:47] epoch 3615, training loss: 3494.86, average training loss: 3395.31, base loss: 4648.78
[INFO 2017-06-26 22:23:13,790 main.py:47] epoch 3616, training loss: 3119.99, average training loss: 3395.04, base loss: 4648.36
[INFO 2017-06-26 22:23:14,224 main.py:47] epoch 3617, training loss: 2891.09, average training loss: 3394.92, base loss: 4648.34
[INFO 2017-06-26 22:23:14,626 main.py:47] epoch 3618, training loss: 3536.70, average training loss: 3395.01, base loss: 4648.82
[INFO 2017-06-26 22:23:15,028 main.py:47] epoch 3619, training loss: 2906.77, average training loss: 3395.15, base loss: 4649.05
[INFO 2017-06-26 22:23:15,431 main.py:47] epoch 3620, training loss: 3193.94, average training loss: 3388.18, base loss: 4642.38
[INFO 2017-06-26 22:23:15,835 main.py:47] epoch 3621, training loss: 2914.17, average training loss: 3387.89, base loss: 4641.89
[INFO 2017-06-26 22:23:16,342 main.py:47] epoch 3622, training loss: 2882.30, average training loss: 3387.48, base loss: 4641.18
[INFO 2017-06-26 22:23:16,750 main.py:47] epoch 3623, training loss: 3164.60, average training loss: 3387.20, base loss: 4640.53
[INFO 2017-06-26 22:23:17,156 main.py:47] epoch 3624, training loss: 3289.70, average training loss: 3386.99, base loss: 4640.67
[INFO 2017-06-26 22:23:17,576 main.py:47] epoch 3625, training loss: 3020.38, average training loss: 3383.00, base loss: 4636.23
[INFO 2017-06-26 22:23:17,980 main.py:47] epoch 3626, training loss: 6332.10, average training loss: 3385.95, base loss: 4639.87
[INFO 2017-06-26 22:23:18,383 main.py:47] epoch 3627, training loss: 6437.57, average training loss: 3388.57, base loss: 4642.25
[INFO 2017-06-26 22:23:18,787 main.py:47] epoch 3628, training loss: 6311.32, average training loss: 3391.39, base loss: 4646.10
[INFO 2017-06-26 22:23:19,190 main.py:47] epoch 3629, training loss: 2545.78, average training loss: 3390.50, base loss: 4644.47
[INFO 2017-06-26 22:23:19,593 main.py:47] epoch 3630, training loss: 3258.68, average training loss: 3386.98, base loss: 4640.84
[INFO 2017-06-26 22:23:19,997 main.py:47] epoch 3631, training loss: 3298.50, average training loss: 3387.02, base loss: 4640.93
[INFO 2017-06-26 22:23:20,400 main.py:47] epoch 3632, training loss: 3189.94, average training loss: 3387.35, base loss: 4641.87
[INFO 2017-06-26 22:23:20,802 main.py:47] epoch 3633, training loss: 3000.94, average training loss: 3387.05, base loss: 4641.43
[INFO 2017-06-26 22:23:21,206 main.py:47] epoch 3634, training loss: 3012.11, average training loss: 3387.24, base loss: 4641.86
[INFO 2017-06-26 22:23:21,609 main.py:47] epoch 3635, training loss: 3234.67, average training loss: 3387.07, base loss: 4642.03
[INFO 2017-06-26 22:23:22,011 main.py:47] epoch 3636, training loss: 2910.89, average training loss: 3386.57, base loss: 4641.66
[INFO 2017-06-26 22:23:22,415 main.py:47] epoch 3637, training loss: 3322.23, average training loss: 3386.51, base loss: 4641.99
[INFO 2017-06-26 22:23:22,820 main.py:47] epoch 3638, training loss: 3304.66, average training loss: 3386.16, base loss: 4641.80
[INFO 2017-06-26 22:23:23,225 main.py:47] epoch 3639, training loss: 3165.61, average training loss: 3386.00, base loss: 4641.74
[INFO 2017-06-26 22:23:23,629 main.py:47] epoch 3640, training loss: 2915.17, average training loss: 3385.29, base loss: 4640.91
[INFO 2017-06-26 22:23:24,033 main.py:47] epoch 3641, training loss: 3277.64, average training loss: 3385.03, base loss: 4640.98
[INFO 2017-06-26 22:23:24,435 main.py:47] epoch 3642, training loss: 2966.68, average training loss: 3384.52, base loss: 4640.23
[INFO 2017-06-26 22:23:24,843 main.py:47] epoch 3643, training loss: 2905.70, average training loss: 3384.03, base loss: 4639.40
[INFO 2017-06-26 22:23:25,246 main.py:47] epoch 3644, training loss: 3197.92, average training loss: 3383.87, base loss: 4640.01
[INFO 2017-06-26 22:23:25,655 main.py:47] epoch 3645, training loss: 2958.21, average training loss: 3383.83, base loss: 4640.19
[INFO 2017-06-26 22:23:26,060 main.py:47] epoch 3646, training loss: 3251.44, average training loss: 3383.57, base loss: 4640.58
[INFO 2017-06-26 22:23:26,466 main.py:47] epoch 3647, training loss: 3335.48, average training loss: 3383.21, base loss: 4640.56
[INFO 2017-06-26 22:23:26,871 main.py:47] epoch 3648, training loss: 6020.48, average training loss: 3385.96, base loss: 4644.34
[INFO 2017-06-26 22:23:27,278 main.py:47] epoch 3649, training loss: 6140.58, average training loss: 3388.65, base loss: 4647.94
[INFO 2017-06-26 22:23:27,756 main.py:47] epoch 3650, training loss: 3312.13, average training loss: 3388.80, base loss: 4648.45
[INFO 2017-06-26 22:23:28,180 main.py:47] epoch 3651, training loss: 3130.31, average training loss: 3388.74, base loss: 4648.76
[INFO 2017-06-26 22:23:28,584 main.py:47] epoch 3652, training loss: 3082.52, average training loss: 3389.02, base loss: 4649.33
[INFO 2017-06-26 22:23:29,020 main.py:47] epoch 3653, training loss: 2960.55, average training loss: 3388.94, base loss: 4649.34
[INFO 2017-06-26 22:23:29,424 main.py:47] epoch 3654, training loss: 3324.80, average training loss: 3389.12, base loss: 4649.80
[INFO 2017-06-26 22:23:29,829 main.py:47] epoch 3655, training loss: 3297.22, average training loss: 3389.03, base loss: 4650.01
[INFO 2017-06-26 22:23:30,227 main.py:47] epoch 3656, training loss: 2910.89, average training loss: 3388.63, base loss: 4649.63
[INFO 2017-06-26 22:23:30,628 main.py:47] epoch 3657, training loss: 2789.82, average training loss: 3388.45, base loss: 4649.71
[INFO 2017-06-26 22:23:31,027 main.py:47] epoch 3658, training loss: 3379.97, average training loss: 3388.31, base loss: 4650.05
[INFO 2017-06-26 22:23:31,429 main.py:47] epoch 3659, training loss: 2984.54, average training loss: 3388.50, base loss: 4650.67
[INFO 2017-06-26 22:23:31,832 main.py:47] epoch 3660, training loss: 3172.84, average training loss: 3388.13, base loss: 4650.19
[INFO 2017-06-26 22:23:32,234 main.py:47] epoch 3661, training loss: 3182.27, average training loss: 3387.86, base loss: 4649.86
[INFO 2017-06-26 22:23:32,633 main.py:47] epoch 3662, training loss: 3450.69, average training loss: 3388.30, base loss: 4650.96
[INFO 2017-06-26 22:23:33,031 main.py:47] epoch 3663, training loss: 2851.05, average training loss: 3387.78, base loss: 4650.11
[INFO 2017-06-26 22:23:33,432 main.py:47] epoch 3664, training loss: 3225.48, average training loss: 3387.43, base loss: 4650.22
[INFO 2017-06-26 22:23:33,831 main.py:47] epoch 3665, training loss: 3314.15, average training loss: 3387.26, base loss: 4649.86
[INFO 2017-06-26 22:23:34,233 main.py:47] epoch 3666, training loss: 3333.44, average training loss: 3386.91, base loss: 4649.65
[INFO 2017-06-26 22:23:34,633 main.py:47] epoch 3667, training loss: 3293.54, average training loss: 3383.90, base loss: 4647.29
[INFO 2017-06-26 22:23:35,032 main.py:47] epoch 3668, training loss: 3142.38, average training loss: 3383.27, base loss: 4646.51
[INFO 2017-06-26 22:23:35,435 main.py:47] epoch 3669, training loss: 3458.93, average training loss: 3383.40, base loss: 4646.98
[INFO 2017-06-26 22:23:35,838 main.py:47] epoch 3670, training loss: 3052.32, average training loss: 3383.43, base loss: 4647.42
[INFO 2017-06-26 22:23:36,241 main.py:47] epoch 3671, training loss: 3104.80, average training loss: 3383.15, base loss: 4647.19
[INFO 2017-06-26 22:23:36,644 main.py:47] epoch 3672, training loss: 3368.34, average training loss: 3382.75, base loss: 4646.99
[INFO 2017-06-26 22:23:37,043 main.py:47] epoch 3673, training loss: 3500.95, average training loss: 3382.68, base loss: 4647.27
[INFO 2017-06-26 22:23:37,448 main.py:47] epoch 3674, training loss: 2987.35, average training loss: 3382.26, base loss: 4646.96
[INFO 2017-06-26 22:23:37,849 main.py:47] epoch 3675, training loss: 3285.03, average training loss: 3382.32, base loss: 4647.32
[INFO 2017-06-26 22:23:38,256 main.py:47] epoch 3676, training loss: 3157.99, average training loss: 3382.52, base loss: 4647.78
[INFO 2017-06-26 22:23:38,658 main.py:47] epoch 3677, training loss: 3060.64, average training loss: 3382.08, base loss: 4647.18
[INFO 2017-06-26 22:23:39,063 main.py:47] epoch 3678, training loss: 3123.95, average training loss: 3381.99, base loss: 4647.27
[INFO 2017-06-26 22:23:39,469 main.py:47] epoch 3679, training loss: 3082.65, average training loss: 3382.21, base loss: 4647.96
[INFO 2017-06-26 22:23:39,868 main.py:47] epoch 3680, training loss: 3064.41, average training loss: 3382.01, base loss: 4647.54
[INFO 2017-06-26 22:23:40,270 main.py:47] epoch 3681, training loss: 3059.60, average training loss: 3381.73, base loss: 4647.42
[INFO 2017-06-26 22:23:40,675 main.py:47] epoch 3682, training loss: 3047.44, average training loss: 3381.16, base loss: 4646.58
[INFO 2017-06-26 22:23:41,081 main.py:47] epoch 3683, training loss: 6063.57, average training loss: 3384.22, base loss: 4650.42
[INFO 2017-06-26 22:23:41,488 main.py:47] epoch 3684, training loss: 3151.18, average training loss: 3384.02, base loss: 4650.50
[INFO 2017-06-26 22:23:41,892 main.py:47] epoch 3685, training loss: 6440.33, average training loss: 3387.07, base loss: 4653.81
[INFO 2017-06-26 22:23:42,305 main.py:47] epoch 3686, training loss: 2994.65, average training loss: 3386.61, base loss: 4653.16
[INFO 2017-06-26 22:23:42,708 main.py:47] epoch 3687, training loss: 2876.32, average training loss: 3386.40, base loss: 4652.86
[INFO 2017-06-26 22:23:43,118 main.py:47] epoch 3688, training loss: 3126.30, average training loss: 3386.26, base loss: 4652.91
[INFO 2017-06-26 22:23:43,521 main.py:47] epoch 3689, training loss: 3347.05, average training loss: 3386.32, base loss: 4653.30
[INFO 2017-06-26 22:23:43,927 main.py:47] epoch 3690, training loss: 3250.43, average training loss: 3386.19, base loss: 4653.60
[INFO 2017-06-26 22:23:44,332 main.py:47] epoch 3691, training loss: 3193.19, average training loss: 3386.16, base loss: 4654.14
[INFO 2017-06-26 22:23:44,779 main.py:47] epoch 3692, training loss: 3170.08, average training loss: 3385.90, base loss: 4654.16
[INFO 2017-06-26 22:23:45,220 main.py:47] epoch 3693, training loss: 3052.77, average training loss: 3385.85, base loss: 4654.16
[INFO 2017-06-26 22:23:45,634 main.py:47] epoch 3694, training loss: 3064.35, average training loss: 3385.23, base loss: 4653.45
[INFO 2017-06-26 22:23:46,057 main.py:47] epoch 3695, training loss: 2909.08, average training loss: 3384.83, base loss: 4652.89
[INFO 2017-06-26 22:23:46,466 main.py:47] epoch 3696, training loss: 3277.96, average training loss: 3384.58, base loss: 4652.70
[INFO 2017-06-26 22:23:46,865 main.py:47] epoch 3697, training loss: 6345.25, average training loss: 3387.69, base loss: 4656.22
[INFO 2017-06-26 22:23:47,271 main.py:47] epoch 3698, training loss: 3161.69, average training loss: 3384.34, base loss: 4653.26
[INFO 2017-06-26 22:23:47,674 main.py:47] epoch 3699, training loss: 2832.19, average training loss: 3384.17, base loss: 4652.80
[INFO 2017-06-26 22:23:47,674 main.py:49] epoch 3699, testing
[INFO 2017-06-26 22:23:49,438 main.py:102] average testing loss: 3243.54, base loss: 4649.38
[INFO 2017-06-26 22:23:49,438 main.py:103] improve_loss: 1405.83, improve_percent: 0.30
[INFO 2017-06-26 22:23:49,439 main.py:73] current best improved percent: 0.31
[INFO 2017-06-26 22:23:49,838 main.py:47] epoch 3700, training loss: 3203.67, average training loss: 3384.00, base loss: 4652.54
[INFO 2017-06-26 22:23:50,243 main.py:47] epoch 3701, training loss: 3341.56, average training loss: 3384.30, base loss: 4653.56
[INFO 2017-06-26 22:23:50,645 main.py:47] epoch 3702, training loss: 3072.90, average training loss: 3384.39, base loss: 4653.69
[INFO 2017-06-26 22:23:51,045 main.py:47] epoch 3703, training loss: 2969.10, average training loss: 3384.10, base loss: 4653.27
[INFO 2017-06-26 22:23:51,448 main.py:47] epoch 3704, training loss: 3446.79, average training loss: 3384.05, base loss: 4653.31
[INFO 2017-06-26 22:23:51,846 main.py:47] epoch 3705, training loss: 2901.92, average training loss: 3383.29, base loss: 4652.08
[INFO 2017-06-26 22:23:52,243 main.py:47] epoch 3706, training loss: 2987.62, average training loss: 3382.48, base loss: 4650.68
[INFO 2017-06-26 22:23:52,656 main.py:47] epoch 3707, training loss: 3370.78, average training loss: 3382.49, base loss: 4651.06
[INFO 2017-06-26 22:23:53,060 main.py:47] epoch 3708, training loss: 3185.45, average training loss: 3382.17, base loss: 4650.47
[INFO 2017-06-26 22:23:53,463 main.py:47] epoch 3709, training loss: 2773.91, average training loss: 3381.58, base loss: 4649.41
[INFO 2017-06-26 22:23:53,865 main.py:47] epoch 3710, training loss: 3187.69, average training loss: 3381.10, base loss: 4648.52
[INFO 2017-06-26 22:23:54,274 main.py:47] epoch 3711, training loss: 3349.71, average training loss: 3381.09, base loss: 4648.85
[INFO 2017-06-26 22:23:54,679 main.py:47] epoch 3712, training loss: 3043.18, average training loss: 3380.79, base loss: 4648.20
[INFO 2017-06-26 22:23:55,078 main.py:47] epoch 3713, training loss: 2880.44, average training loss: 3377.19, base loss: 4644.32
[INFO 2017-06-26 22:23:55,481 main.py:47] epoch 3714, training loss: 3231.68, average training loss: 3377.42, base loss: 4644.79
[INFO 2017-06-26 22:23:55,883 main.py:47] epoch 3715, training loss: 2960.99, average training loss: 3377.19, base loss: 4644.32
[INFO 2017-06-26 22:23:56,283 main.py:47] epoch 3716, training loss: 3271.02, average training loss: 3376.88, base loss: 4643.90
[INFO 2017-06-26 22:23:56,682 main.py:47] epoch 3717, training loss: 2995.59, average training loss: 3377.05, base loss: 4644.28
[INFO 2017-06-26 22:23:57,081 main.py:47] epoch 3718, training loss: 3501.87, average training loss: 3377.07, base loss: 4644.59
[INFO 2017-06-26 22:23:57,483 main.py:47] epoch 3719, training loss: 2938.96, average training loss: 3376.79, base loss: 4644.05
[INFO 2017-06-26 22:23:57,894 main.py:47] epoch 3720, training loss: 3355.28, average training loss: 3376.67, base loss: 4644.08
[INFO 2017-06-26 22:23:58,295 main.py:47] epoch 3721, training loss: 2916.61, average training loss: 3371.91, base loss: 4638.36
[INFO 2017-06-26 22:23:58,696 main.py:47] epoch 3722, training loss: 2942.98, average training loss: 3371.45, base loss: 4637.74
[INFO 2017-06-26 22:23:59,096 main.py:47] epoch 3723, training loss: 3161.67, average training loss: 3371.08, base loss: 4637.64
[INFO 2017-06-26 22:23:59,505 main.py:47] epoch 3724, training loss: 3225.59, average training loss: 3370.60, base loss: 4636.87
[INFO 2017-06-26 22:23:59,907 main.py:47] epoch 3725, training loss: 2798.44, average training loss: 3370.07, base loss: 4635.99
[INFO 2017-06-26 22:24:00,309 main.py:47] epoch 3726, training loss: 2976.35, average training loss: 3369.85, base loss: 4635.59
[INFO 2017-06-26 22:24:00,717 main.py:47] epoch 3727, training loss: 3389.29, average training loss: 3370.13, base loss: 4636.63
[INFO 2017-06-26 22:24:01,119 main.py:47] epoch 3728, training loss: 3124.79, average training loss: 3369.74, base loss: 4635.98
[INFO 2017-06-26 22:24:01,522 main.py:47] epoch 3729, training loss: 2636.92, average training loss: 3365.88, base loss: 4631.51
[INFO 2017-06-26 22:24:01,924 main.py:47] epoch 3730, training loss: 3137.47, average training loss: 3365.67, base loss: 4631.49
[INFO 2017-06-26 22:24:02,328 main.py:47] epoch 3731, training loss: 3188.30, average training loss: 3365.61, base loss: 4631.66
[INFO 2017-06-26 22:24:02,725 main.py:47] epoch 3732, training loss: 3515.88, average training loss: 3365.55, base loss: 4631.82
[INFO 2017-06-26 22:24:03,120 main.py:47] epoch 3733, training loss: 2885.57, average training loss: 3365.24, base loss: 4631.58
[INFO 2017-06-26 22:24:03,524 main.py:47] epoch 3734, training loss: 3420.98, average training loss: 3364.95, base loss: 4631.32
[INFO 2017-06-26 22:24:03,927 main.py:47] epoch 3735, training loss: 3041.24, average training loss: 3364.91, base loss: 4631.27
[INFO 2017-06-26 22:24:04,335 main.py:47] epoch 3736, training loss: 3007.25, average training loss: 3364.91, base loss: 4631.35
[INFO 2017-06-26 22:24:04,734 main.py:47] epoch 3737, training loss: 2869.61, average training loss: 3364.92, base loss: 4631.25
[INFO 2017-06-26 22:24:05,133 main.py:47] epoch 3738, training loss: 3159.30, average training loss: 3364.37, base loss: 4630.70
[INFO 2017-06-26 22:24:05,534 main.py:47] epoch 3739, training loss: 2934.57, average training loss: 3364.26, base loss: 4630.66
[INFO 2017-06-26 22:24:05,938 main.py:47] epoch 3740, training loss: 3015.84, average training loss: 3363.83, base loss: 4630.22
[INFO 2017-06-26 22:24:06,341 main.py:47] epoch 3741, training loss: 2899.30, average training loss: 3363.44, base loss: 4629.89
[INFO 2017-06-26 22:24:06,736 main.py:47] epoch 3742, training loss: 3085.03, average training loss: 3363.52, base loss: 4630.17
[INFO 2017-06-26 22:24:07,139 main.py:47] epoch 3743, training loss: 6709.09, average training loss: 3367.23, base loss: 4634.50
[INFO 2017-06-26 22:24:07,540 main.py:47] epoch 3744, training loss: 2960.24, average training loss: 3367.29, base loss: 4635.08
[INFO 2017-06-26 22:24:07,942 main.py:47] epoch 3745, training loss: 2939.02, average training loss: 3366.90, base loss: 4634.19
[INFO 2017-06-26 22:24:08,343 main.py:47] epoch 3746, training loss: 3434.71, average training loss: 3367.00, base loss: 4634.69
[INFO 2017-06-26 22:24:08,744 main.py:47] epoch 3747, training loss: 6360.02, average training loss: 3370.40, base loss: 4638.42
[INFO 2017-06-26 22:24:09,141 main.py:47] epoch 3748, training loss: 3047.18, average training loss: 3370.07, base loss: 4638.21
[INFO 2017-06-26 22:24:09,537 main.py:47] epoch 3749, training loss: 2929.76, average training loss: 3369.24, base loss: 4637.33
[INFO 2017-06-26 22:24:09,937 main.py:47] epoch 3750, training loss: 3566.24, average training loss: 3369.64, base loss: 4638.45
[INFO 2017-06-26 22:24:10,339 main.py:47] epoch 3751, training loss: 3220.82, average training loss: 3369.88, base loss: 4639.15
[INFO 2017-06-26 22:24:10,736 main.py:47] epoch 3752, training loss: 3197.61, average training loss: 3369.79, base loss: 4638.90
[INFO 2017-06-26 22:24:11,132 main.py:47] epoch 3753, training loss: 3229.28, average training loss: 3370.07, base loss: 4639.49
[INFO 2017-06-26 22:24:11,541 main.py:47] epoch 3754, training loss: 3750.49, average training loss: 3370.59, base loss: 4640.43
[INFO 2017-06-26 22:24:11,942 main.py:47] epoch 3755, training loss: 3317.13, average training loss: 3369.90, base loss: 4639.28
[INFO 2017-06-26 22:24:12,345 main.py:47] epoch 3756, training loss: 3022.69, average training loss: 3369.41, base loss: 4638.47
[INFO 2017-06-26 22:24:12,743 main.py:47] epoch 3757, training loss: 2960.12, average training loss: 3368.67, base loss: 4637.34
[INFO 2017-06-26 22:24:13,140 main.py:47] epoch 3758, training loss: 3417.47, average training loss: 3368.94, base loss: 4638.32
[INFO 2017-06-26 22:24:13,542 main.py:47] epoch 3759, training loss: 2974.77, average training loss: 3368.81, base loss: 4638.09
[INFO 2017-06-26 22:24:13,938 main.py:47] epoch 3760, training loss: 3077.53, average training loss: 3368.57, base loss: 4638.37
[INFO 2017-06-26 22:24:14,340 main.py:47] epoch 3761, training loss: 3358.85, average training loss: 3368.58, base loss: 4639.12
[INFO 2017-06-26 22:24:14,735 main.py:47] epoch 3762, training loss: 2951.42, average training loss: 3367.97, base loss: 4638.43
[INFO 2017-06-26 22:24:15,141 main.py:47] epoch 3763, training loss: 3038.65, average training loss: 3367.81, base loss: 4638.55
[INFO 2017-06-26 22:24:15,549 main.py:47] epoch 3764, training loss: 3071.63, average training loss: 3367.53, base loss: 4638.48
[INFO 2017-06-26 22:24:15,951 main.py:47] epoch 3765, training loss: 3463.36, average training loss: 3367.67, base loss: 4639.07
[INFO 2017-06-26 22:24:16,347 main.py:47] epoch 3766, training loss: 2910.30, average training loss: 3367.63, base loss: 4639.25
[INFO 2017-06-26 22:24:16,743 main.py:47] epoch 3767, training loss: 3186.97, average training loss: 3367.71, base loss: 4639.29
[INFO 2017-06-26 22:24:17,151 main.py:47] epoch 3768, training loss: 3601.61, average training loss: 3368.34, base loss: 4640.58
[INFO 2017-06-26 22:24:17,560 main.py:47] epoch 3769, training loss: 3100.43, average training loss: 3367.83, base loss: 4639.68
[INFO 2017-06-26 22:24:17,970 main.py:47] epoch 3770, training loss: 3099.64, average training loss: 3368.02, base loss: 4640.14
[INFO 2017-06-26 22:24:18,391 main.py:47] epoch 3771, training loss: 3117.51, average training loss: 3367.67, base loss: 4639.55
[INFO 2017-06-26 22:24:18,822 main.py:47] epoch 3772, training loss: 2668.14, average training loss: 3367.15, base loss: 4638.79
[INFO 2017-06-26 22:24:19,233 main.py:47] epoch 3773, training loss: 3001.35, average training loss: 3367.09, base loss: 4638.85
[INFO 2017-06-26 22:24:19,643 main.py:47] epoch 3774, training loss: 2873.53, average training loss: 3366.47, base loss: 4637.73
[INFO 2017-06-26 22:24:20,047 main.py:47] epoch 3775, training loss: 2893.54, average training loss: 3365.97, base loss: 4637.02
[INFO 2017-06-26 22:24:20,451 main.py:47] epoch 3776, training loss: 3444.53, average training loss: 3366.12, base loss: 4637.99
[INFO 2017-06-26 22:24:20,859 main.py:47] epoch 3777, training loss: 3376.26, average training loss: 3366.45, base loss: 4638.85
[INFO 2017-06-26 22:24:21,262 main.py:47] epoch 3778, training loss: 3137.29, average training loss: 3366.89, base loss: 4639.67
[INFO 2017-06-26 22:24:21,660 main.py:47] epoch 3779, training loss: 2963.02, average training loss: 3366.41, base loss: 4639.11
[INFO 2017-06-26 22:24:22,115 main.py:47] epoch 3780, training loss: 3186.17, average training loss: 3366.14, base loss: 4638.69
[INFO 2017-06-26 22:24:22,563 main.py:47] epoch 3781, training loss: 2783.47, average training loss: 3365.57, base loss: 4637.91
[INFO 2017-06-26 22:24:22,970 main.py:47] epoch 3782, training loss: 3157.44, average training loss: 3365.64, base loss: 4638.21
[INFO 2017-06-26 22:24:23,374 main.py:47] epoch 3783, training loss: 3107.28, average training loss: 3365.13, base loss: 4637.64
[INFO 2017-06-26 22:24:23,777 main.py:47] epoch 3784, training loss: 3031.65, average training loss: 3364.88, base loss: 4637.14
[INFO 2017-06-26 22:24:24,191 main.py:47] epoch 3785, training loss: 3345.27, average training loss: 3365.01, base loss: 4637.79
[INFO 2017-06-26 22:24:24,593 main.py:47] epoch 3786, training loss: 3171.34, average training loss: 3364.54, base loss: 4637.31
[INFO 2017-06-26 22:24:24,992 main.py:47] epoch 3787, training loss: 3420.68, average training loss: 3364.60, base loss: 4638.13
[INFO 2017-06-26 22:24:25,397 main.py:47] epoch 3788, training loss: 3408.47, average training loss: 3364.64, base loss: 4638.86
[INFO 2017-06-26 22:24:25,800 main.py:47] epoch 3789, training loss: 3036.65, average training loss: 3364.32, base loss: 4638.31
[INFO 2017-06-26 22:24:26,203 main.py:47] epoch 3790, training loss: 3510.11, average training loss: 3365.05, base loss: 4640.61
[INFO 2017-06-26 22:24:26,698 main.py:47] epoch 3791, training loss: 2949.38, average training loss: 3364.74, base loss: 4640.29
[INFO 2017-06-26 22:24:27,121 main.py:47] epoch 3792, training loss: 3124.52, average training loss: 3364.73, base loss: 4640.65
[INFO 2017-06-26 22:24:27,527 main.py:47] epoch 3793, training loss: 3078.05, average training loss: 3364.35, base loss: 4640.03
[INFO 2017-06-26 22:24:28,034 main.py:47] epoch 3794, training loss: 3027.75, average training loss: 3364.01, base loss: 4639.78
[INFO 2017-06-26 22:24:28,457 main.py:47] epoch 3795, training loss: 3284.64, average training loss: 3364.55, base loss: 4641.69
[INFO 2017-06-26 22:24:28,864 main.py:47] epoch 3796, training loss: 2939.57, average training loss: 3364.47, base loss: 4642.09
[INFO 2017-06-26 22:24:29,281 main.py:47] epoch 3797, training loss: 3113.54, average training loss: 3364.46, base loss: 4641.96
[INFO 2017-06-26 22:24:29,685 main.py:47] epoch 3798, training loss: 3102.39, average training loss: 3364.30, base loss: 4642.32
[INFO 2017-06-26 22:24:30,088 main.py:47] epoch 3799, training loss: 3223.02, average training loss: 3364.30, base loss: 4642.77
[INFO 2017-06-26 22:24:30,089 main.py:49] epoch 3799, testing
[INFO 2017-06-26 22:24:31,743 main.py:102] average testing loss: 3013.00, base loss: 4204.28
[INFO 2017-06-26 22:24:31,743 main.py:103] improve_loss: 1191.29, improve_percent: 0.28
[INFO 2017-06-26 22:24:31,743 main.py:73] current best improved percent: 0.31
[INFO 2017-06-26 22:24:32,155 main.py:47] epoch 3800, training loss: 3067.73, average training loss: 3364.39, base loss: 4643.36
[INFO 2017-06-26 22:24:32,579 main.py:47] epoch 3801, training loss: 3038.25, average training loss: 3364.12, base loss: 4643.03
[INFO 2017-06-26 22:24:32,986 main.py:47] epoch 3802, training loss: 2837.72, average training loss: 3363.75, base loss: 4642.61
[INFO 2017-06-26 22:24:33,390 main.py:47] epoch 3803, training loss: 3378.14, average training loss: 3363.94, base loss: 4643.45
[INFO 2017-06-26 22:24:33,795 main.py:47] epoch 3804, training loss: 2907.17, average training loss: 3363.22, base loss: 4642.26
[INFO 2017-06-26 22:24:34,200 main.py:47] epoch 3805, training loss: 3538.41, average training loss: 3363.70, base loss: 4643.01
[INFO 2017-06-26 22:24:34,605 main.py:47] epoch 3806, training loss: 3131.09, average training loss: 3363.38, base loss: 4642.42
[INFO 2017-06-26 22:24:35,010 main.py:47] epoch 3807, training loss: 3484.92, average training loss: 3363.87, base loss: 4643.86
[INFO 2017-06-26 22:24:35,414 main.py:47] epoch 3808, training loss: 3320.65, average training loss: 3363.97, base loss: 4644.30
[INFO 2017-06-26 22:24:35,819 main.py:47] epoch 3809, training loss: 3216.04, average training loss: 3364.16, base loss: 4645.27
[INFO 2017-06-26 22:24:36,224 main.py:47] epoch 3810, training loss: 3195.74, average training loss: 3364.29, base loss: 4645.97
[INFO 2017-06-26 22:24:36,630 main.py:47] epoch 3811, training loss: 3090.69, average training loss: 3364.03, base loss: 4645.69
[INFO 2017-06-26 22:24:37,040 main.py:47] epoch 3812, training loss: 2952.33, average training loss: 3363.22, base loss: 4644.22
[INFO 2017-06-26 22:24:37,445 main.py:47] epoch 3813, training loss: 3088.78, average training loss: 3362.95, base loss: 4644.00
[INFO 2017-06-26 22:24:37,854 main.py:47] epoch 3814, training loss: 3290.41, average training loss: 3362.89, base loss: 4644.11
[INFO 2017-06-26 22:24:38,261 main.py:47] epoch 3815, training loss: 3113.73, average training loss: 3362.59, base loss: 4644.25
[INFO 2017-06-26 22:24:38,669 main.py:47] epoch 3816, training loss: 3345.34, average training loss: 3362.51, base loss: 4643.99
[INFO 2017-06-26 22:24:39,086 main.py:47] epoch 3817, training loss: 2897.29, average training loss: 3361.71, base loss: 4642.73
[INFO 2017-06-26 22:24:39,493 main.py:47] epoch 3818, training loss: 3185.93, average training loss: 3361.58, base loss: 4642.70
[INFO 2017-06-26 22:24:39,898 main.py:47] epoch 3819, training loss: 2984.70, average training loss: 3361.03, base loss: 4641.66
[INFO 2017-06-26 22:24:40,303 main.py:47] epoch 3820, training loss: 3227.10, average training loss: 3360.92, base loss: 4642.24
[INFO 2017-06-26 22:24:40,708 main.py:47] epoch 3821, training loss: 2711.42, average training loss: 3360.38, base loss: 4641.22
[INFO 2017-06-26 22:24:41,116 main.py:47] epoch 3822, training loss: 3237.00, average training loss: 3359.94, base loss: 4640.46
[INFO 2017-06-26 22:24:41,521 main.py:47] epoch 3823, training loss: 3293.39, average training loss: 3359.88, base loss: 4640.75
[INFO 2017-06-26 22:24:41,935 main.py:47] epoch 3824, training loss: 6517.45, average training loss: 3363.24, base loss: 4644.88
[INFO 2017-06-26 22:24:42,345 main.py:47] epoch 3825, training loss: 2906.85, average training loss: 3362.96, base loss: 4644.58
[INFO 2017-06-26 22:24:42,836 main.py:47] epoch 3826, training loss: 3046.40, average training loss: 3359.07, base loss: 4640.44
[INFO 2017-06-26 22:24:43,267 main.py:47] epoch 3827, training loss: 3074.67, average training loss: 3358.80, base loss: 4640.25
[INFO 2017-06-26 22:24:43,674 main.py:47] epoch 3828, training loss: 3383.44, average training loss: 3359.09, base loss: 4641.28
[INFO 2017-06-26 22:24:44,081 main.py:47] epoch 3829, training loss: 3165.18, average training loss: 3359.26, base loss: 4641.68
[INFO 2017-06-26 22:24:44,485 main.py:47] epoch 3830, training loss: 3425.67, average training loss: 3359.35, base loss: 4641.98
[INFO 2017-06-26 22:24:44,892 main.py:47] epoch 3831, training loss: 3032.14, average training loss: 3358.79, base loss: 4641.05
[INFO 2017-06-26 22:24:45,299 main.py:47] epoch 3832, training loss: 3527.75, average training loss: 3359.15, base loss: 4642.12
[INFO 2017-06-26 22:24:45,704 main.py:47] epoch 3833, training loss: 2970.88, average training loss: 3358.74, base loss: 4641.80
[INFO 2017-06-26 22:24:46,114 main.py:47] epoch 3834, training loss: 3006.74, average training loss: 3358.67, base loss: 4641.98
[INFO 2017-06-26 22:24:46,525 main.py:47] epoch 3835, training loss: 3092.29, average training loss: 3358.32, base loss: 4641.76
[INFO 2017-06-26 22:24:46,930 main.py:47] epoch 3836, training loss: 3366.90, average training loss: 3358.65, base loss: 4642.37
[INFO 2017-06-26 22:24:47,335 main.py:47] epoch 3837, training loss: 3142.51, average training loss: 3358.35, base loss: 4641.65
[INFO 2017-06-26 22:24:47,753 main.py:47] epoch 3838, training loss: 3070.72, average training loss: 3358.21, base loss: 4641.61
[INFO 2017-06-26 22:24:48,229 main.py:47] epoch 3839, training loss: 2969.68, average training loss: 3357.84, base loss: 4641.56
[INFO 2017-06-26 22:24:48,658 main.py:47] epoch 3840, training loss: 3364.89, average training loss: 3357.63, base loss: 4641.42
[INFO 2017-06-26 22:24:49,115 main.py:47] epoch 3841, training loss: 3509.68, average training loss: 3357.51, base loss: 4641.24
[INFO 2017-06-26 22:24:49,568 main.py:47] epoch 3842, training loss: 3102.21, average training loss: 3357.29, base loss: 4641.00
[INFO 2017-06-26 22:24:49,993 main.py:47] epoch 3843, training loss: 3585.22, average training loss: 3357.36, base loss: 4641.16
[INFO 2017-06-26 22:24:50,490 main.py:47] epoch 3844, training loss: 2997.68, average training loss: 3356.83, base loss: 4640.61
[INFO 2017-06-26 22:24:50,910 main.py:47] epoch 3845, training loss: 5681.96, average training loss: 3359.53, base loss: 4644.21
[INFO 2017-06-26 22:24:51,315 main.py:47] epoch 3846, training loss: 2903.66, average training loss: 3359.16, base loss: 4643.62
[INFO 2017-06-26 22:24:51,717 main.py:47] epoch 3847, training loss: 3209.06, average training loss: 3359.12, base loss: 4643.45
[INFO 2017-06-26 22:24:52,132 main.py:47] epoch 3848, training loss: 6472.40, average training loss: 3361.67, base loss: 4646.15
[INFO 2017-06-26 22:24:52,537 main.py:47] epoch 3849, training loss: 3112.43, average training loss: 3361.21, base loss: 4645.60
[INFO 2017-06-26 22:24:52,932 main.py:47] epoch 3850, training loss: 3616.48, average training loss: 3361.72, base loss: 4646.91
[INFO 2017-06-26 22:24:53,337 main.py:47] epoch 3851, training loss: 3331.10, average training loss: 3361.60, base loss: 4646.76
[INFO 2017-06-26 22:24:53,731 main.py:47] epoch 3852, training loss: 3278.16, average training loss: 3358.11, base loss: 4643.62
[INFO 2017-06-26 22:24:54,136 main.py:47] epoch 3853, training loss: 2993.83, average training loss: 3357.47, base loss: 4642.53
[INFO 2017-06-26 22:24:54,543 main.py:47] epoch 3854, training loss: 3074.97, average training loss: 3357.22, base loss: 4642.28
[INFO 2017-06-26 22:24:54,944 main.py:47] epoch 3855, training loss: 2694.84, average training loss: 3356.73, base loss: 4641.43
[INFO 2017-06-26 22:24:55,351 main.py:47] epoch 3856, training loss: 2840.69, average training loss: 3356.33, base loss: 4641.41
[INFO 2017-06-26 22:24:55,751 main.py:47] epoch 3857, training loss: 3031.75, average training loss: 3352.56, base loss: 4637.07
[INFO 2017-06-26 22:24:56,151 main.py:47] epoch 3858, training loss: 2936.17, average training loss: 3352.13, base loss: 4636.67
[INFO 2017-06-26 22:24:56,554 main.py:47] epoch 3859, training loss: 2824.11, average training loss: 3351.61, base loss: 4635.64
[INFO 2017-06-26 22:24:56,958 main.py:47] epoch 3860, training loss: 3436.82, average training loss: 3351.72, base loss: 4636.23
[INFO 2017-06-26 22:24:57,358 main.py:47] epoch 3861, training loss: 3110.95, average training loss: 3352.02, base loss: 4637.29
[INFO 2017-06-26 22:24:57,776 main.py:47] epoch 3862, training loss: 3262.95, average training loss: 3351.74, base loss: 4637.01
[INFO 2017-06-26 22:24:58,187 main.py:47] epoch 3863, training loss: 2762.53, average training loss: 3351.21, base loss: 4636.46
[INFO 2017-06-26 22:24:58,618 main.py:47] epoch 3864, training loss: 3108.72, average training loss: 3350.74, base loss: 4635.76
[INFO 2017-06-26 22:24:59,047 main.py:47] epoch 3865, training loss: 3309.42, average training loss: 3347.26, base loss: 4631.92
[INFO 2017-06-26 22:24:59,452 main.py:47] epoch 3866, training loss: 6281.04, average training loss: 3350.48, base loss: 4636.06
[INFO 2017-06-26 22:24:59,934 main.py:47] epoch 3867, training loss: 3142.14, average training loss: 3350.44, base loss: 4636.15
[INFO 2017-06-26 22:25:00,369 main.py:47] epoch 3868, training loss: 3154.87, average training loss: 3350.66, base loss: 4636.68
[INFO 2017-06-26 22:25:00,774 main.py:47] epoch 3869, training loss: 3217.35, average training loss: 3350.60, base loss: 4637.61
[INFO 2017-06-26 22:25:01,187 main.py:47] epoch 3870, training loss: 3135.89, average training loss: 3350.28, base loss: 4637.13
[INFO 2017-06-26 22:25:01,589 main.py:47] epoch 3871, training loss: 3798.37, average training loss: 3350.58, base loss: 4637.65
[INFO 2017-06-26 22:25:01,991 main.py:47] epoch 3872, training loss: 2670.74, average training loss: 3350.15, base loss: 4636.83
[INFO 2017-06-26 22:25:02,391 main.py:47] epoch 3873, training loss: 3137.27, average training loss: 3350.16, base loss: 4637.03
[INFO 2017-06-26 22:25:02,790 main.py:47] epoch 3874, training loss: 3425.48, average training loss: 3350.32, base loss: 4637.63
[INFO 2017-06-26 22:25:03,190 main.py:47] epoch 3875, training loss: 3203.37, average training loss: 3350.35, base loss: 4637.90
[INFO 2017-06-26 22:25:03,587 main.py:47] epoch 3876, training loss: 3489.47, average training loss: 3347.36, base loss: 4634.93
[INFO 2017-06-26 22:25:03,993 main.py:47] epoch 3877, training loss: 2840.49, average training loss: 3346.94, base loss: 4634.22
[INFO 2017-06-26 22:25:04,395 main.py:47] epoch 3878, training loss: 2727.25, average training loss: 3346.42, base loss: 4633.57
[INFO 2017-06-26 22:25:04,799 main.py:47] epoch 3879, training loss: 3159.60, average training loss: 3346.29, base loss: 4633.58
[INFO 2017-06-26 22:25:05,202 main.py:47] epoch 3880, training loss: 3327.17, average training loss: 3346.58, base loss: 4634.39
[INFO 2017-06-26 22:25:05,632 main.py:47] epoch 3881, training loss: 3114.18, average training loss: 3346.89, base loss: 4635.77
[INFO 2017-06-26 22:25:06,061 main.py:47] epoch 3882, training loss: 3167.10, average training loss: 3346.95, base loss: 4636.24
[INFO 2017-06-26 22:25:06,465 main.py:47] epoch 3883, training loss: 2982.55, average training loss: 3347.05, base loss: 4636.70
[INFO 2017-06-26 22:25:06,869 main.py:47] epoch 3884, training loss: 3521.58, average training loss: 3344.06, base loss: 4633.57
[INFO 2017-06-26 22:25:07,271 main.py:47] epoch 3885, training loss: 2808.29, average training loss: 3343.66, base loss: 4632.81
[INFO 2017-06-26 22:25:07,672 main.py:47] epoch 3886, training loss: 3167.36, average training loss: 3343.81, base loss: 4633.47
[INFO 2017-06-26 22:25:08,072 main.py:47] epoch 3887, training loss: 3095.34, average training loss: 3343.80, base loss: 4633.83
[INFO 2017-06-26 22:25:08,473 main.py:47] epoch 3888, training loss: 2821.75, average training loss: 3343.62, base loss: 4634.03
[INFO 2017-06-26 22:25:08,876 main.py:47] epoch 3889, training loss: 3403.26, average training loss: 3340.06, base loss: 4630.95
[INFO 2017-06-26 22:25:09,289 main.py:47] epoch 3890, training loss: 5944.72, average training loss: 3342.98, base loss: 4635.28
[INFO 2017-06-26 22:25:09,689 main.py:47] epoch 3891, training loss: 2998.67, average training loss: 3342.89, base loss: 4635.52
[INFO 2017-06-26 22:25:10,092 main.py:47] epoch 3892, training loss: 3167.57, average training loss: 3342.78, base loss: 4635.71
[INFO 2017-06-26 22:25:10,494 main.py:47] epoch 3893, training loss: 2877.62, average training loss: 3342.45, base loss: 4635.36
[INFO 2017-06-26 22:25:10,891 main.py:47] epoch 3894, training loss: 2944.27, average training loss: 3342.10, base loss: 4635.13
[INFO 2017-06-26 22:25:11,289 main.py:47] epoch 3895, training loss: 3095.01, average training loss: 3342.14, base loss: 4635.18
[INFO 2017-06-26 22:25:11,684 main.py:47] epoch 3896, training loss: 3500.46, average training loss: 3341.83, base loss: 4634.68
[INFO 2017-06-26 22:25:12,082 main.py:47] epoch 3897, training loss: 3156.18, average training loss: 3342.04, base loss: 4635.37
[INFO 2017-06-26 22:25:12,481 main.py:47] epoch 3898, training loss: 2815.61, average training loss: 3341.28, base loss: 4634.35
[INFO 2017-06-26 22:25:12,879 main.py:47] epoch 3899, training loss: 2802.87, average training loss: 3340.62, base loss: 4633.10
[INFO 2017-06-26 22:25:12,879 main.py:49] epoch 3899, testing
[INFO 2017-06-26 22:25:14,524 main.py:102] average testing loss: 3681.23, base loss: 5251.57
[INFO 2017-06-26 22:25:14,524 main.py:103] improve_loss: 1570.34, improve_percent: 0.30
[INFO 2017-06-26 22:25:14,525 main.py:73] current best improved percent: 0.31
[INFO 2017-06-26 22:25:14,926 main.py:47] epoch 3900, training loss: 3104.53, average training loss: 3340.36, base loss: 4632.75
[INFO 2017-06-26 22:25:15,327 main.py:47] epoch 3901, training loss: 3004.46, average training loss: 3340.15, base loss: 4632.69
[INFO 2017-06-26 22:25:15,729 main.py:47] epoch 3902, training loss: 3534.48, average training loss: 3340.05, base loss: 4632.52
[INFO 2017-06-26 22:25:16,132 main.py:47] epoch 3903, training loss: 2994.55, average training loss: 3340.02, base loss: 4632.68
[INFO 2017-06-26 22:25:16,537 main.py:47] epoch 3904, training loss: 3226.89, average training loss: 3339.58, base loss: 4631.82
[INFO 2017-06-26 22:25:16,939 main.py:47] epoch 3905, training loss: 2987.28, average training loss: 3339.15, base loss: 4631.08
[INFO 2017-06-26 22:25:17,340 main.py:47] epoch 3906, training loss: 3387.03, average training loss: 3336.10, base loss: 4628.16
[INFO 2017-06-26 22:25:17,791 main.py:47] epoch 3907, training loss: 2917.91, average training loss: 3335.87, base loss: 4627.85
[INFO 2017-06-26 22:25:18,218 main.py:47] epoch 3908, training loss: 3132.20, average training loss: 3335.96, base loss: 4628.40
[INFO 2017-06-26 22:25:18,643 main.py:47] epoch 3909, training loss: 3146.56, average training loss: 3335.62, base loss: 4628.16
[INFO 2017-06-26 22:25:19,070 main.py:47] epoch 3910, training loss: 2886.82, average training loss: 3334.84, base loss: 4626.96
[INFO 2017-06-26 22:25:19,474 main.py:47] epoch 3911, training loss: 2714.76, average training loss: 3333.52, base loss: 4624.45
[INFO 2017-06-26 22:25:19,877 main.py:47] epoch 3912, training loss: 2779.25, average training loss: 3332.90, base loss: 4623.32
[INFO 2017-06-26 22:25:20,280 main.py:47] epoch 3913, training loss: 2972.47, average training loss: 3332.41, base loss: 4622.69
[INFO 2017-06-26 22:25:20,685 main.py:47] epoch 3914, training loss: 3242.99, average training loss: 3332.33, base loss: 4623.23
[INFO 2017-06-26 22:25:21,089 main.py:47] epoch 3915, training loss: 2888.39, average training loss: 3331.96, base loss: 4622.70
[INFO 2017-06-26 22:25:21,486 main.py:47] epoch 3916, training loss: 3073.87, average training loss: 3332.32, base loss: 4623.22
[INFO 2017-06-26 22:25:21,887 main.py:47] epoch 3917, training loss: 3553.86, average training loss: 3332.43, base loss: 4623.25
[INFO 2017-06-26 22:25:22,376 main.py:47] epoch 3918, training loss: 3502.41, average training loss: 3332.64, base loss: 4623.55
[INFO 2017-06-26 22:25:22,804 main.py:47] epoch 3919, training loss: 2912.78, average training loss: 3331.79, base loss: 4622.16
[INFO 2017-06-26 22:25:23,228 main.py:47] epoch 3920, training loss: 3070.19, average training loss: 3331.82, base loss: 4622.92
[INFO 2017-06-26 22:25:23,683 main.py:47] epoch 3921, training loss: 2914.42, average training loss: 3331.96, base loss: 4623.49
[INFO 2017-06-26 22:25:24,093 main.py:47] epoch 3922, training loss: 3459.26, average training loss: 3331.96, base loss: 4623.49
[INFO 2017-06-26 22:25:24,508 main.py:47] epoch 3923, training loss: 3064.95, average training loss: 3331.95, base loss: 4624.04
[INFO 2017-06-26 22:25:25,005 main.py:47] epoch 3924, training loss: 3121.26, average training loss: 3331.58, base loss: 4623.63
[INFO 2017-06-26 22:25:25,415 main.py:47] epoch 3925, training loss: 6163.01, average training loss: 3334.69, base loss: 4627.62
[INFO 2017-06-26 22:25:25,820 main.py:47] epoch 3926, training loss: 3680.40, average training loss: 3331.76, base loss: 4624.75
[INFO 2017-06-26 22:25:26,304 main.py:47] epoch 3927, training loss: 3126.39, average training loss: 3331.51, base loss: 4624.39
[INFO 2017-06-26 22:25:26,712 main.py:47] epoch 3928, training loss: 3283.00, average training loss: 3331.49, base loss: 4624.60
[INFO 2017-06-26 22:25:27,123 main.py:47] epoch 3929, training loss: 3144.40, average training loss: 3331.61, base loss: 4625.10
[INFO 2017-06-26 22:25:27,525 main.py:47] epoch 3930, training loss: 3096.22, average training loss: 3331.61, base loss: 4625.59
[INFO 2017-06-26 22:25:27,927 main.py:47] epoch 3931, training loss: 3220.40, average training loss: 3330.99, base loss: 4624.95
[INFO 2017-06-26 22:25:28,332 main.py:47] epoch 3932, training loss: 3191.32, average training loss: 3330.99, base loss: 4624.84
[INFO 2017-06-26 22:25:28,738 main.py:47] epoch 3933, training loss: 3003.59, average training loss: 3330.56, base loss: 4623.98
[INFO 2017-06-26 22:25:29,230 main.py:47] epoch 3934, training loss: 3031.30, average training loss: 3330.24, base loss: 4623.90
[INFO 2017-06-26 22:25:29,659 main.py:47] epoch 3935, training loss: 3503.72, average training loss: 3330.57, base loss: 4624.77
[INFO 2017-06-26 22:25:30,068 main.py:47] epoch 3936, training loss: 3280.50, average training loss: 3330.36, base loss: 4625.04
[INFO 2017-06-26 22:25:30,478 main.py:47] epoch 3937, training loss: 3205.76, average training loss: 3330.26, base loss: 4625.37
[INFO 2017-06-26 22:25:30,887 main.py:47] epoch 3938, training loss: 3308.13, average training loss: 3330.44, base loss: 4625.88
[INFO 2017-06-26 22:25:31,291 main.py:47] epoch 3939, training loss: 3041.59, average training loss: 3330.08, base loss: 4624.98
[INFO 2017-06-26 22:25:31,695 main.py:47] epoch 3940, training loss: 3405.53, average training loss: 3329.88, base loss: 4625.09
[INFO 2017-06-26 22:25:32,096 main.py:47] epoch 3941, training loss: 3163.76, average training loss: 3329.93, base loss: 4625.41
[INFO 2017-06-26 22:25:32,501 main.py:47] epoch 3942, training loss: 3173.77, average training loss: 3329.70, base loss: 4625.19
[INFO 2017-06-26 22:25:32,904 main.py:47] epoch 3943, training loss: 2907.19, average training loss: 3329.51, base loss: 4625.24
[INFO 2017-06-26 22:25:33,307 main.py:47] epoch 3944, training loss: 5334.52, average training loss: 3331.47, base loss: 4627.71
[INFO 2017-06-26 22:25:33,704 main.py:47] epoch 3945, training loss: 2901.68, average training loss: 3331.18, base loss: 4627.44
[INFO 2017-06-26 22:25:34,108 main.py:47] epoch 3946, training loss: 3266.99, average training loss: 3331.75, base loss: 4629.25
[INFO 2017-06-26 22:25:34,505 main.py:47] epoch 3947, training loss: 3390.46, average training loss: 3328.35, base loss: 4626.26
[INFO 2017-06-26 22:25:34,903 main.py:47] epoch 3948, training loss: 3038.16, average training loss: 3328.09, base loss: 4625.68
[INFO 2017-06-26 22:25:35,304 main.py:47] epoch 3949, training loss: 3207.86, average training loss: 3328.18, base loss: 4625.64
[INFO 2017-06-26 22:25:35,701 main.py:47] epoch 3950, training loss: 2647.37, average training loss: 3327.16, base loss: 4623.69
[INFO 2017-06-26 22:25:36,104 main.py:47] epoch 3951, training loss: 5629.53, average training loss: 3329.81, base loss: 4626.91
[INFO 2017-06-26 22:25:36,504 main.py:47] epoch 3952, training loss: 3157.69, average training loss: 3329.86, base loss: 4626.93
[INFO 2017-06-26 22:25:36,905 main.py:47] epoch 3953, training loss: 2898.84, average training loss: 3329.49, base loss: 4626.70
[INFO 2017-06-26 22:25:37,310 main.py:47] epoch 3954, training loss: 3121.42, average training loss: 3328.83, base loss: 4625.37
[INFO 2017-06-26 22:25:37,707 main.py:47] epoch 3955, training loss: 2693.10, average training loss: 3328.25, base loss: 4624.77
[INFO 2017-06-26 22:25:38,116 main.py:47] epoch 3956, training loss: 3075.85, average training loss: 3327.75, base loss: 4624.25
[INFO 2017-06-26 22:25:38,522 main.py:47] epoch 3957, training loss: 3139.10, average training loss: 3327.62, base loss: 4624.06
[INFO 2017-06-26 22:25:38,921 main.py:47] epoch 3958, training loss: 3452.65, average training loss: 3328.24, base loss: 4625.07
[INFO 2017-06-26 22:25:39,323 main.py:47] epoch 3959, training loss: 2934.98, average training loss: 3327.62, base loss: 4624.14
[INFO 2017-06-26 22:25:39,720 main.py:47] epoch 3960, training loss: 3298.77, average training loss: 3327.50, base loss: 4624.60
[INFO 2017-06-26 22:25:40,125 main.py:47] epoch 3961, training loss: 3165.85, average training loss: 3327.19, base loss: 4624.46
[INFO 2017-06-26 22:25:40,527 main.py:47] epoch 3962, training loss: 2905.04, average training loss: 3326.71, base loss: 4623.89
[INFO 2017-06-26 22:25:40,934 main.py:47] epoch 3963, training loss: 2946.18, average training loss: 3326.41, base loss: 4623.27
[INFO 2017-06-26 22:25:41,332 main.py:47] epoch 3964, training loss: 3258.18, average training loss: 3326.26, base loss: 4623.31
[INFO 2017-06-26 22:25:41,739 main.py:47] epoch 3965, training loss: 6138.75, average training loss: 3329.28, base loss: 4627.10
[INFO 2017-06-26 22:25:42,142 main.py:47] epoch 3966, training loss: 3043.71, average training loss: 3329.16, base loss: 4627.16
[INFO 2017-06-26 22:25:42,546 main.py:47] epoch 3967, training loss: 2995.54, average training loss: 3328.90, base loss: 4626.61
[INFO 2017-06-26 22:25:43,051 main.py:47] epoch 3968, training loss: 2780.79, average training loss: 3328.54, base loss: 4626.24
[INFO 2017-06-26 22:25:43,479 main.py:47] epoch 3969, training loss: 2858.43, average training loss: 3327.64, base loss: 4625.20
[INFO 2017-06-26 22:25:43,894 main.py:47] epoch 3970, training loss: 3190.31, average training loss: 3326.99, base loss: 4624.08
[INFO 2017-06-26 22:25:44,427 main.py:47] epoch 3971, training loss: 3179.67, average training loss: 3326.89, base loss: 4624.53
[INFO 2017-06-26 22:25:44,834 main.py:47] epoch 3972, training loss: 3008.45, average training loss: 3326.61, base loss: 4624.64
[INFO 2017-06-26 22:25:45,338 main.py:47] epoch 3973, training loss: 2799.84, average training loss: 3326.37, base loss: 4624.01
[INFO 2017-06-26 22:25:45,743 main.py:47] epoch 3974, training loss: 3134.53, average training loss: 3326.17, base loss: 4624.21
[INFO 2017-06-26 22:25:46,227 main.py:47] epoch 3975, training loss: 3529.25, average training loss: 3326.43, base loss: 4625.25
[INFO 2017-06-26 22:25:46,652 main.py:47] epoch 3976, training loss: 2804.46, average training loss: 3326.11, base loss: 4624.69
[INFO 2017-06-26 22:25:47,150 main.py:47] epoch 3977, training loss: 2987.36, average training loss: 3325.64, base loss: 4623.94
[INFO 2017-06-26 22:25:47,560 main.py:47] epoch 3978, training loss: 3346.52, average training loss: 3325.29, base loss: 4623.38
[INFO 2017-06-26 22:25:48,009 main.py:47] epoch 3979, training loss: 6355.78, average training loss: 3328.18, base loss: 4626.98
[INFO 2017-06-26 22:25:48,436 main.py:47] epoch 3980, training loss: 3096.54, average training loss: 3328.31, base loss: 4627.13
[INFO 2017-06-26 22:25:48,842 main.py:47] epoch 3981, training loss: 3521.17, average training loss: 3328.91, base loss: 4628.30
[INFO 2017-06-26 22:25:49,249 main.py:47] epoch 3982, training loss: 3243.99, average training loss: 3328.80, base loss: 4627.59
[INFO 2017-06-26 22:25:49,688 main.py:47] epoch 3983, training loss: 3079.33, average training loss: 3328.47, base loss: 4627.15
[INFO 2017-06-26 22:25:50,091 main.py:47] epoch 3984, training loss: 2965.89, average training loss: 3327.83, base loss: 4625.58
[INFO 2017-06-26 22:25:50,497 main.py:47] epoch 3985, training loss: 3312.81, average training loss: 3327.78, base loss: 4625.39
[INFO 2017-06-26 22:25:50,905 main.py:47] epoch 3986, training loss: 3307.66, average training loss: 3327.44, base loss: 4624.55
[INFO 2017-06-26 22:25:51,310 main.py:47] epoch 3987, training loss: 3821.29, average training loss: 3327.90, base loss: 4625.07
[INFO 2017-06-26 22:25:51,714 main.py:47] epoch 3988, training loss: 3515.97, average training loss: 3328.02, base loss: 4625.13
[INFO 2017-06-26 22:25:52,117 main.py:47] epoch 3989, training loss: 3146.36, average training loss: 3327.98, base loss: 4624.83
[INFO 2017-06-26 22:25:52,526 main.py:47] epoch 3990, training loss: 3225.85, average training loss: 3327.96, base loss: 4624.29
[INFO 2017-06-26 22:25:52,933 main.py:47] epoch 3991, training loss: 3438.77, average training loss: 3328.41, base loss: 4624.91
[INFO 2017-06-26 22:25:53,338 main.py:47] epoch 3992, training loss: 3171.98, average training loss: 3328.22, base loss: 4624.23
[INFO 2017-06-26 22:25:53,823 main.py:47] epoch 3993, training loss: 3197.10, average training loss: 3328.23, base loss: 4623.97
[INFO 2017-06-26 22:25:54,245 main.py:47] epoch 3994, training loss: 3787.87, average training loss: 3328.95, base loss: 4625.19
[INFO 2017-06-26 22:25:54,653 main.py:47] epoch 3995, training loss: 3697.87, average training loss: 3329.03, base loss: 4625.27
[INFO 2017-06-26 22:25:55,058 main.py:47] epoch 3996, training loss: 3298.44, average training loss: 3329.20, base loss: 4625.23
[INFO 2017-06-26 22:25:55,458 main.py:47] epoch 3997, training loss: 3364.70, average training loss: 3329.29, base loss: 4625.11
[INFO 2017-06-26 22:25:55,862 main.py:47] epoch 3998, training loss: 3210.78, average training loss: 3329.06, base loss: 4624.21
[INFO 2017-06-26 22:25:56,263 main.py:47] epoch 3999, training loss: 3629.75, average training loss: 3329.65, base loss: 4625.27
[INFO 2017-06-26 22:25:56,264 main.py:49] epoch 3999, testing
[INFO 2017-06-26 22:25:57,932 main.py:102] average testing loss: 3416.86, base loss: 4530.33
[INFO 2017-06-26 22:25:57,932 main.py:103] improve_loss: 1113.47, improve_percent: 0.25
[INFO 2017-06-26 22:25:57,933 main.py:73] current best improved percent: 0.31
[INFO 2017-06-26 22:25:58,335 main.py:47] epoch 4000, training loss: 3572.87, average training loss: 3329.90, base loss: 4625.10
[INFO 2017-06-26 22:25:58,735 main.py:47] epoch 4001, training loss: 3002.33, average training loss: 3329.40, base loss: 4624.00
[INFO 2017-06-26 22:25:59,134 main.py:47] epoch 4002, training loss: 3276.92, average training loss: 3329.29, base loss: 4623.71
[INFO 2017-06-26 22:25:59,536 main.py:47] epoch 4003, training loss: 3434.24, average training loss: 3329.67, base loss: 4624.16
[INFO 2017-06-26 22:25:59,939 main.py:47] epoch 4004, training loss: 3463.58, average training loss: 3329.52, base loss: 4623.66
[INFO 2017-06-26 22:26:00,359 main.py:47] epoch 4005, training loss: 3193.95, average training loss: 3329.26, base loss: 4622.81
[INFO 2017-06-26 22:26:00,761 main.py:47] epoch 4006, training loss: 3402.71, average training loss: 3329.22, base loss: 4622.46
[INFO 2017-06-26 22:26:01,163 main.py:47] epoch 4007, training loss: 3041.54, average training loss: 3329.06, base loss: 4622.09
[INFO 2017-06-26 22:26:01,566 main.py:47] epoch 4008, training loss: 6857.14, average training loss: 3332.48, base loss: 4624.85
[INFO 2017-06-26 22:26:01,964 main.py:47] epoch 4009, training loss: 2811.61, average training loss: 3332.11, base loss: 4623.83
[INFO 2017-06-26 22:26:02,368 main.py:47] epoch 4010, training loss: 3491.51, average training loss: 3332.15, base loss: 4623.85
[INFO 2017-06-26 22:26:02,773 main.py:47] epoch 4011, training loss: 3313.31, average training loss: 3332.36, base loss: 4624.37
[INFO 2017-06-26 22:26:03,176 main.py:47] epoch 4012, training loss: 3463.12, average training loss: 3332.88, base loss: 4625.32
[INFO 2017-06-26 22:26:03,579 main.py:47] epoch 4013, training loss: 3403.99, average training loss: 3333.32, base loss: 4626.06
[INFO 2017-06-26 22:26:03,980 main.py:47] epoch 4014, training loss: 3426.27, average training loss: 3333.68, base loss: 4626.50
[INFO 2017-06-26 22:26:04,383 main.py:47] epoch 4015, training loss: 3735.26, average training loss: 3334.51, base loss: 4628.14
[INFO 2017-06-26 22:26:04,783 main.py:47] epoch 4016, training loss: 3738.75, average training loss: 3335.23, base loss: 4629.53
[INFO 2017-06-26 22:26:05,185 main.py:47] epoch 4017, training loss: 3549.77, average training loss: 3331.44, base loss: 4625.66
[INFO 2017-06-26 22:26:05,585 main.py:47] epoch 4018, training loss: 3138.80, average training loss: 3331.46, base loss: 4625.58
[INFO 2017-06-26 22:26:05,984 main.py:47] epoch 4019, training loss: 3148.74, average training loss: 3331.34, base loss: 4624.82
[INFO 2017-06-26 22:26:06,386 main.py:47] epoch 4020, training loss: 3594.43, average training loss: 3331.87, base loss: 4625.64
[INFO 2017-06-26 22:26:06,785 main.py:47] epoch 4021, training loss: 2811.94, average training loss: 3331.97, base loss: 4625.64
[INFO 2017-06-26 22:26:07,187 main.py:47] epoch 4022, training loss: 3190.18, average training loss: 3331.83, base loss: 4625.32
[INFO 2017-06-26 22:26:07,590 main.py:47] epoch 4023, training loss: 3102.93, average training loss: 3331.79, base loss: 4625.00
[INFO 2017-06-26 22:26:07,989 main.py:47] epoch 4024, training loss: 3186.26, average training loss: 3331.85, base loss: 4624.86
[INFO 2017-06-26 22:26:08,387 main.py:47] epoch 4025, training loss: 3615.62, average training loss: 3332.39, base loss: 4625.67
[INFO 2017-06-26 22:26:08,790 main.py:47] epoch 4026, training loss: 3144.12, average training loss: 3332.53, base loss: 4625.27
[INFO 2017-06-26 22:26:09,193 main.py:47] epoch 4027, training loss: 3141.30, average training loss: 3332.56, base loss: 4625.16
[INFO 2017-06-26 22:26:09,595 main.py:47] epoch 4028, training loss: 2889.54, average training loss: 3332.24, base loss: 4624.60
[INFO 2017-06-26 22:26:09,996 main.py:47] epoch 4029, training loss: 3196.18, average training loss: 3332.48, base loss: 4625.29
[INFO 2017-06-26 22:26:10,397 main.py:47] epoch 4030, training loss: 3082.42, average training loss: 3332.59, base loss: 4625.53
[INFO 2017-06-26 22:26:10,800 main.py:47] epoch 4031, training loss: 2910.46, average training loss: 3331.70, base loss: 4623.79
[INFO 2017-06-26 22:26:11,203 main.py:47] epoch 4032, training loss: 3083.86, average training loss: 3331.55, base loss: 4623.25
[INFO 2017-06-26 22:26:11,605 main.py:47] epoch 4033, training loss: 3316.47, average training loss: 3331.56, base loss: 4623.42
[INFO 2017-06-26 22:26:12,006 main.py:47] epoch 4034, training loss: 3144.44, average training loss: 3331.41, base loss: 4623.58
[INFO 2017-06-26 22:26:12,410 main.py:47] epoch 4035, training loss: 3679.57, average training loss: 3328.60, base loss: 4620.96
[INFO 2017-06-26 22:26:12,817 main.py:47] epoch 4036, training loss: 3320.62, average training loss: 3328.91, base loss: 4621.49
[INFO 2017-06-26 22:26:13,221 main.py:47] epoch 4037, training loss: 3242.43, average training loss: 3328.92, base loss: 4621.68
[INFO 2017-06-26 22:26:13,625 main.py:47] epoch 4038, training loss: 3651.35, average training loss: 3329.24, base loss: 4622.66
[INFO 2017-06-26 22:26:14,029 main.py:47] epoch 4039, training loss: 3473.81, average training loss: 3329.63, base loss: 4623.34
[INFO 2017-06-26 22:26:14,433 main.py:47] epoch 4040, training loss: 2995.78, average training loss: 3329.55, base loss: 4623.39
[INFO 2017-06-26 22:26:14,834 main.py:47] epoch 4041, training loss: 3098.61, average training loss: 3329.14, base loss: 4622.81
[INFO 2017-06-26 22:26:15,237 main.py:47] epoch 4042, training loss: 2950.99, average training loss: 3328.49, base loss: 4621.74
[INFO 2017-06-26 22:26:15,638 main.py:47] epoch 4043, training loss: 3051.14, average training loss: 3328.09, base loss: 4620.67
[INFO 2017-06-26 22:26:16,042 main.py:47] epoch 4044, training loss: 3535.07, average training loss: 3328.71, base loss: 4622.05
[INFO 2017-06-26 22:26:16,450 main.py:47] epoch 4045, training loss: 3536.00, average training loss: 3329.13, base loss: 4622.68
[INFO 2017-06-26 22:26:16,854 main.py:47] epoch 4046, training loss: 3234.91, average training loss: 3328.84, base loss: 4621.71
[INFO 2017-06-26 22:26:17,260 main.py:47] epoch 4047, training loss: 3117.70, average training loss: 3328.81, base loss: 4621.67
[INFO 2017-06-26 22:26:17,665 main.py:47] epoch 4048, training loss: 3381.17, average training loss: 3329.24, base loss: 4622.92
[INFO 2017-06-26 22:26:18,068 main.py:47] epoch 4049, training loss: 3531.00, average training loss: 3329.21, base loss: 4622.97
[INFO 2017-06-26 22:26:18,474 main.py:47] epoch 4050, training loss: 3209.74, average training loss: 3329.19, base loss: 4623.05
[INFO 2017-06-26 22:26:18,883 main.py:47] epoch 4051, training loss: 3054.37, average training loss: 3329.22, base loss: 4623.14
[INFO 2017-06-26 22:26:19,286 main.py:47] epoch 4052, training loss: 3284.61, average training loss: 3329.41, base loss: 4623.91
[INFO 2017-06-26 22:26:19,684 main.py:47] epoch 4053, training loss: 3630.57, average training loss: 3330.22, base loss: 4625.76
[INFO 2017-06-26 22:26:20,089 main.py:47] epoch 4054, training loss: 3046.13, average training loss: 3329.83, base loss: 4624.99
[INFO 2017-06-26 22:26:20,492 main.py:47] epoch 4055, training loss: 3393.03, average training loss: 3329.91, base loss: 4625.72
[INFO 2017-06-26 22:26:20,893 main.py:47] epoch 4056, training loss: 3297.06, average training loss: 3330.12, base loss: 4625.91
[INFO 2017-06-26 22:26:21,299 main.py:47] epoch 4057, training loss: 2927.06, average training loss: 3329.78, base loss: 4625.16
[INFO 2017-06-26 22:26:21,702 main.py:47] epoch 4058, training loss: 3420.85, average training loss: 3330.01, base loss: 4625.24
[INFO 2017-06-26 22:26:22,102 main.py:47] epoch 4059, training loss: 2871.42, average training loss: 3329.61, base loss: 4624.43
[INFO 2017-06-26 22:26:22,507 main.py:47] epoch 4060, training loss: 2933.60, average training loss: 3329.49, base loss: 4624.16
[INFO 2017-06-26 22:26:22,908 main.py:47] epoch 4061, training loss: 3090.47, average training loss: 3329.72, base loss: 4624.53
[INFO 2017-06-26 22:26:23,312 main.py:47] epoch 4062, training loss: 3200.46, average training loss: 3329.74, base loss: 4624.75
[INFO 2017-06-26 22:26:23,707 main.py:47] epoch 4063, training loss: 2894.42, average training loss: 3329.19, base loss: 4624.09
[INFO 2017-06-26 22:26:24,110 main.py:47] epoch 4064, training loss: 2886.97, average training loss: 3329.08, base loss: 4623.83
[INFO 2017-06-26 22:26:24,512 main.py:47] epoch 4065, training loss: 3098.97, average training loss: 3329.10, base loss: 4623.89
[INFO 2017-06-26 22:26:24,915 main.py:47] epoch 4066, training loss: 3615.34, average training loss: 3329.46, base loss: 4624.95
[INFO 2017-06-26 22:26:25,318 main.py:47] epoch 4067, training loss: 3402.90, average training loss: 3328.96, base loss: 4624.09
[INFO 2017-06-26 22:26:25,720 main.py:47] epoch 4068, training loss: 3069.05, average training loss: 3328.45, base loss: 4622.93
[INFO 2017-06-26 22:26:26,122 main.py:47] epoch 4069, training loss: 3376.31, average training loss: 3328.85, base loss: 4623.68
[INFO 2017-06-26 22:26:26,523 main.py:47] epoch 4070, training loss: 3085.21, average training loss: 3328.63, base loss: 4623.50
[INFO 2017-06-26 22:26:26,930 main.py:47] epoch 4071, training loss: 3360.43, average training loss: 3328.52, base loss: 4623.29
[INFO 2017-06-26 22:26:27,333 main.py:47] epoch 4072, training loss: 3177.32, average training loss: 3325.26, base loss: 4619.44
[INFO 2017-06-26 22:26:27,732 main.py:47] epoch 4073, training loss: 2856.32, average training loss: 3325.35, base loss: 4619.61
[INFO 2017-06-26 22:26:28,147 main.py:47] epoch 4074, training loss: 7002.63, average training loss: 3329.13, base loss: 4623.69
[INFO 2017-06-26 22:26:28,546 main.py:47] epoch 4075, training loss: 3096.81, average training loss: 3329.33, base loss: 4624.51
[INFO 2017-06-26 22:26:28,955 main.py:47] epoch 4076, training loss: 3267.63, average training loss: 3329.60, base loss: 4625.11
[INFO 2017-06-26 22:26:29,358 main.py:47] epoch 4077, training loss: 6455.49, average training loss: 3332.57, base loss: 4627.65
[INFO 2017-06-26 22:26:29,770 main.py:47] epoch 4078, training loss: 3171.42, average training loss: 3332.77, base loss: 4628.54
[INFO 2017-06-26 22:26:30,173 main.py:47] epoch 4079, training loss: 3454.05, average training loss: 3332.91, base loss: 4628.80
[INFO 2017-06-26 22:26:30,578 main.py:47] epoch 4080, training loss: 2994.03, average training loss: 3332.68, base loss: 4628.51
[INFO 2017-06-26 22:26:30,979 main.py:47] epoch 4081, training loss: 3252.10, average training loss: 3332.55, base loss: 4628.50
[INFO 2017-06-26 22:26:31,385 main.py:47] epoch 4082, training loss: 3082.43, average training loss: 3331.90, base loss: 4627.57
[INFO 2017-06-26 22:26:31,794 main.py:47] epoch 4083, training loss: 2741.22, average training loss: 3331.35, base loss: 4626.23
[INFO 2017-06-26 22:26:32,203 main.py:47] epoch 4084, training loss: 3019.66, average training loss: 3330.85, base loss: 4625.28
[INFO 2017-06-26 22:26:32,617 main.py:47] epoch 4085, training loss: 2884.78, average training loss: 3330.73, base loss: 4625.30
[INFO 2017-06-26 22:26:33,026 main.py:47] epoch 4086, training loss: 3025.55, average training loss: 3330.60, base loss: 4625.73
[INFO 2017-06-26 22:26:33,436 main.py:47] epoch 4087, training loss: 6372.80, average training loss: 3333.76, base loss: 4629.24
[INFO 2017-06-26 22:26:33,848 main.py:47] epoch 4088, training loss: 3134.22, average training loss: 3333.56, base loss: 4629.24
[INFO 2017-06-26 22:26:34,258 main.py:47] epoch 4089, training loss: 3086.63, average training loss: 3332.98, base loss: 4627.87
[INFO 2017-06-26 22:26:34,662 main.py:47] epoch 4090, training loss: 3602.27, average training loss: 3332.93, base loss: 4627.59
[INFO 2017-06-26 22:26:35,067 main.py:47] epoch 4091, training loss: 3306.16, average training loss: 3333.18, base loss: 4628.58
[INFO 2017-06-26 22:26:35,469 main.py:47] epoch 4092, training loss: 3398.15, average training loss: 3333.38, base loss: 4629.46
[INFO 2017-06-26 22:26:35,873 main.py:47] epoch 4093, training loss: 6209.11, average training loss: 3336.28, base loss: 4632.50
[INFO 2017-06-26 22:26:36,280 main.py:47] epoch 4094, training loss: 2791.48, average training loss: 3336.07, base loss: 4632.02
[INFO 2017-06-26 22:26:36,682 main.py:47] epoch 4095, training loss: 2971.53, average training loss: 3335.92, base loss: 4631.69
[INFO 2017-06-26 22:26:37,089 main.py:47] epoch 4096, training loss: 3160.73, average training loss: 3335.66, base loss: 4631.06
[INFO 2017-06-26 22:26:37,493 main.py:47] epoch 4097, training loss: 3160.47, average training loss: 3335.65, base loss: 4631.46
[INFO 2017-06-26 22:26:37,898 main.py:47] epoch 4098, training loss: 3115.50, average training loss: 3331.82, base loss: 4627.82
[INFO 2017-06-26 22:26:38,309 main.py:47] epoch 4099, training loss: 3203.38, average training loss: 3331.47, base loss: 4627.29
[INFO 2017-06-26 22:26:38,309 main.py:49] epoch 4099, testing
[INFO 2017-06-26 22:26:39,971 main.py:102] average testing loss: 3481.59, base loss: 4681.30
[INFO 2017-06-26 22:26:39,971 main.py:103] improve_loss: 1199.72, improve_percent: 0.26
[INFO 2017-06-26 22:26:39,971 main.py:73] current best improved percent: 0.31
[INFO 2017-06-26 22:26:40,382 main.py:47] epoch 4100, training loss: 3415.82, average training loss: 3332.18, base loss: 4628.94
[INFO 2017-06-26 22:26:40,789 main.py:47] epoch 4101, training loss: 3640.72, average training loss: 3332.48, base loss: 4629.17
[INFO 2017-06-26 22:26:41,193 main.py:47] epoch 4102, training loss: 6431.23, average training loss: 3335.92, base loss: 4633.38
[INFO 2017-06-26 22:26:41,596 main.py:47] epoch 4103, training loss: 3483.51, average training loss: 3336.00, base loss: 4633.61
[INFO 2017-06-26 22:26:41,999 main.py:47] epoch 4104, training loss: 3130.54, average training loss: 3335.47, base loss: 4632.29
[INFO 2017-06-26 22:26:42,402 main.py:47] epoch 4105, training loss: 2868.38, average training loss: 3335.17, base loss: 4631.60
[INFO 2017-06-26 22:26:42,808 main.py:47] epoch 4106, training loss: 2965.53, average training loss: 3335.09, base loss: 4631.27
[INFO 2017-06-26 22:26:43,212 main.py:47] epoch 4107, training loss: 3210.71, average training loss: 3335.04, base loss: 4631.29
[INFO 2017-06-26 22:26:43,618 main.py:47] epoch 4108, training loss: 3147.91, average training loss: 3334.48, base loss: 4630.80
[INFO 2017-06-26 22:26:44,025 main.py:47] epoch 4109, training loss: 2935.73, average training loss: 3334.14, base loss: 4630.18
[INFO 2017-06-26 22:26:44,433 main.py:47] epoch 4110, training loss: 2999.71, average training loss: 3333.74, base loss: 4629.59
[INFO 2017-06-26 22:26:44,841 main.py:47] epoch 4111, training loss: 3327.48, average training loss: 3333.70, base loss: 4629.38
[INFO 2017-06-26 22:26:45,244 main.py:47] epoch 4112, training loss: 3352.48, average training loss: 3334.01, base loss: 4630.67
[INFO 2017-06-26 22:26:45,654 main.py:47] epoch 4113, training loss: 3250.78, average training loss: 3334.23, base loss: 4631.16
[INFO 2017-06-26 22:26:46,056 main.py:47] epoch 4114, training loss: 2900.18, average training loss: 3334.20, base loss: 4631.22
[INFO 2017-06-26 22:26:46,454 main.py:47] epoch 4115, training loss: 6288.94, average training loss: 3337.53, base loss: 4635.60
[INFO 2017-06-26 22:26:46,861 main.py:47] epoch 4116, training loss: 2844.52, average training loss: 3336.78, base loss: 4633.90
[INFO 2017-06-26 22:26:47,263 main.py:47] epoch 4117, training loss: 3108.01, average training loss: 3336.34, base loss: 4633.11
[INFO 2017-06-26 22:26:47,664 main.py:47] epoch 4118, training loss: 3162.49, average training loss: 3336.51, base loss: 4633.76
[INFO 2017-06-26 22:26:48,063 main.py:47] epoch 4119, training loss: 3077.87, average training loss: 3336.14, base loss: 4633.07
[INFO 2017-06-26 22:26:48,471 main.py:47] epoch 4120, training loss: 2975.79, average training loss: 3335.35, base loss: 4631.43
[INFO 2017-06-26 22:26:48,871 main.py:47] epoch 4121, training loss: 3090.74, average training loss: 3334.66, base loss: 4629.99
[INFO 2017-06-26 22:26:49,271 main.py:47] epoch 4122, training loss: 3156.72, average training loss: 3334.81, base loss: 4630.40
[INFO 2017-06-26 22:26:49,674 main.py:47] epoch 4123, training loss: 3107.42, average training loss: 3334.86, base loss: 4630.45
[INFO 2017-06-26 22:26:50,077 main.py:47] epoch 4124, training loss: 3361.82, average training loss: 3334.87, base loss: 4630.76
[INFO 2017-06-26 22:26:50,484 main.py:47] epoch 4125, training loss: 3084.21, average training loss: 3334.67, base loss: 4630.87
[INFO 2017-06-26 22:26:50,889 main.py:47] epoch 4126, training loss: 3204.41, average training loss: 3334.52, base loss: 4631.04
[INFO 2017-06-26 22:26:51,291 main.py:47] epoch 4127, training loss: 3297.40, average training loss: 3334.42, base loss: 4630.84
[INFO 2017-06-26 22:26:51,688 main.py:47] epoch 4128, training loss: 3229.61, average training loss: 3334.85, base loss: 4631.66
[INFO 2017-06-26 22:26:52,091 main.py:47] epoch 4129, training loss: 3078.94, average training loss: 3334.61, base loss: 4631.39
[INFO 2017-06-26 22:26:52,495 main.py:47] epoch 4130, training loss: 2939.63, average training loss: 3334.25, base loss: 4630.66
[INFO 2017-06-26 22:26:52,896 main.py:47] epoch 4131, training loss: 2818.81, average training loss: 3333.82, base loss: 4630.01
[INFO 2017-06-26 22:26:53,301 main.py:47] epoch 4132, training loss: 3084.91, average training loss: 3333.82, base loss: 4629.73
[INFO 2017-06-26 22:26:53,698 main.py:47] epoch 4133, training loss: 3498.01, average training loss: 3334.02, base loss: 4630.20
[INFO 2017-06-26 22:26:54,097 main.py:47] epoch 4134, training loss: 3369.62, average training loss: 3334.17, base loss: 4630.46
[INFO 2017-06-26 22:26:54,503 main.py:47] epoch 4135, training loss: 6194.53, average training loss: 3336.93, base loss: 4633.88
[INFO 2017-06-26 22:26:54,911 main.py:47] epoch 4136, training loss: 2916.22, average training loss: 3336.11, base loss: 4632.35
[INFO 2017-06-26 22:26:55,322 main.py:47] epoch 4137, training loss: 3306.69, average training loss: 3336.42, base loss: 4632.94
[INFO 2017-06-26 22:26:55,729 main.py:47] epoch 4138, training loss: 3214.54, average training loss: 3336.40, base loss: 4632.94
[INFO 2017-06-26 22:26:56,135 main.py:47] epoch 4139, training loss: 3003.26, average training loss: 3336.38, base loss: 4633.13
[INFO 2017-06-26 22:26:56,541 main.py:47] epoch 4140, training loss: 5934.07, average training loss: 3338.64, base loss: 4635.59
[INFO 2017-06-26 22:26:56,947 main.py:47] epoch 4141, training loss: 2969.78, average training loss: 3334.63, base loss: 4631.54
[INFO 2017-06-26 22:26:57,352 main.py:47] epoch 4142, training loss: 3013.41, average training loss: 3334.72, base loss: 4631.70
[INFO 2017-06-26 22:26:57,761 main.py:47] epoch 4143, training loss: 3282.67, average training loss: 3334.93, base loss: 4632.70
[INFO 2017-06-26 22:26:58,169 main.py:47] epoch 4144, training loss: 5829.49, average training loss: 3337.89, base loss: 4636.63
[INFO 2017-06-26 22:26:58,575 main.py:47] epoch 4145, training loss: 3155.55, average training loss: 3337.82, base loss: 4636.32
[INFO 2017-06-26 22:26:58,981 main.py:47] epoch 4146, training loss: 2977.33, average training loss: 3337.57, base loss: 4635.89
[INFO 2017-06-26 22:26:59,386 main.py:47] epoch 4147, training loss: 3051.85, average training loss: 3337.19, base loss: 4635.14
[INFO 2017-06-26 22:26:59,786 main.py:47] epoch 4148, training loss: 3470.90, average training loss: 3337.29, base loss: 4635.63
[INFO 2017-06-26 22:27:00,194 main.py:47] epoch 4149, training loss: 2984.16, average training loss: 3336.83, base loss: 4635.08
[INFO 2017-06-26 22:27:00,597 main.py:47] epoch 4150, training loss: 2980.67, average training loss: 3336.87, base loss: 4635.50
[INFO 2017-06-26 22:27:00,999 main.py:47] epoch 4151, training loss: 3022.96, average training loss: 3336.46, base loss: 4634.20
[INFO 2017-06-26 22:27:01,403 main.py:47] epoch 4152, training loss: 3057.04, average training loss: 3336.62, base loss: 4635.02
[INFO 2017-06-26 22:27:01,808 main.py:47] epoch 4153, training loss: 3126.61, average training loss: 3336.08, base loss: 4634.15
[INFO 2017-06-26 22:27:02,212 main.py:47] epoch 4154, training loss: 3236.17, average training loss: 3336.10, base loss: 4634.34
[INFO 2017-06-26 22:27:02,616 main.py:47] epoch 4155, training loss: 2919.03, average training loss: 3335.90, base loss: 4633.88
[INFO 2017-06-26 22:27:03,021 main.py:47] epoch 4156, training loss: 3185.84, average training loss: 3335.85, base loss: 4634.13
[INFO 2017-06-26 22:27:03,431 main.py:47] epoch 4157, training loss: 3180.80, average training loss: 3335.86, base loss: 4634.60
[INFO 2017-06-26 22:27:03,835 main.py:47] epoch 4158, training loss: 3042.72, average training loss: 3335.80, base loss: 4634.53
[INFO 2017-06-26 22:27:04,241 main.py:47] epoch 4159, training loss: 3221.81, average training loss: 3332.35, base loss: 4631.56
[INFO 2017-06-26 22:27:04,642 main.py:47] epoch 4160, training loss: 3245.29, average training loss: 3332.14, base loss: 4630.94
[INFO 2017-06-26 22:27:05,050 main.py:47] epoch 4161, training loss: 3409.05, average training loss: 3331.76, base loss: 4629.95
[INFO 2017-06-26 22:27:05,458 main.py:47] epoch 4162, training loss: 3563.83, average training loss: 3331.89, base loss: 4630.36
[INFO 2017-06-26 22:27:05,866 main.py:47] epoch 4163, training loss: 2980.17, average training loss: 3331.61, base loss: 4630.33
[INFO 2017-06-26 22:27:06,275 main.py:47] epoch 4164, training loss: 3091.31, average training loss: 3331.23, base loss: 4630.10
[INFO 2017-06-26 22:27:06,684 main.py:47] epoch 4165, training loss: 3047.71, average training loss: 3331.15, base loss: 4630.02
[INFO 2017-06-26 22:27:07,093 main.py:47] epoch 4166, training loss: 2794.36, average training loss: 3330.78, base loss: 4629.34
[INFO 2017-06-26 22:27:07,498 main.py:47] epoch 4167, training loss: 3427.37, average training loss: 3330.91, base loss: 4629.97
[INFO 2017-06-26 22:27:07,903 main.py:47] epoch 4168, training loss: 3057.81, average training loss: 3330.44, base loss: 4629.28
[INFO 2017-06-26 22:27:08,305 main.py:47] epoch 4169, training loss: 3550.47, average training loss: 3330.77, base loss: 4630.12
[INFO 2017-06-26 22:27:08,707 main.py:47] epoch 4170, training loss: 3585.33, average training loss: 3331.22, base loss: 4631.47
[INFO 2017-06-26 22:27:09,115 main.py:47] epoch 4171, training loss: 2983.46, average training loss: 3330.95, base loss: 4631.10
[INFO 2017-06-26 22:27:09,525 main.py:47] epoch 4172, training loss: 3282.98, average training loss: 3328.07, base loss: 4628.21
[INFO 2017-06-26 22:27:09,934 main.py:47] epoch 4173, training loss: 3227.76, average training loss: 3328.52, base loss: 4629.38
[INFO 2017-06-26 22:27:10,338 main.py:47] epoch 4174, training loss: 3479.06, average training loss: 3329.01, base loss: 4630.39
[INFO 2017-06-26 22:27:10,748 main.py:47] epoch 4175, training loss: 2965.95, average training loss: 3325.32, base loss: 4626.75
[INFO 2017-06-26 22:27:11,152 main.py:47] epoch 4176, training loss: 3207.83, average training loss: 3325.38, base loss: 4627.13
[INFO 2017-06-26 22:27:11,558 main.py:47] epoch 4177, training loss: 3277.36, average training loss: 3325.12, base loss: 4626.75
[INFO 2017-06-26 22:27:11,969 main.py:47] epoch 4178, training loss: 3489.78, average training loss: 3325.44, base loss: 4627.86
[INFO 2017-06-26 22:27:12,377 main.py:47] epoch 4179, training loss: 3189.97, average training loss: 3325.50, base loss: 4628.39
[INFO 2017-06-26 22:27:12,786 main.py:47] epoch 4180, training loss: 2901.95, average training loss: 3324.98, base loss: 4627.12
[INFO 2017-06-26 22:27:13,190 main.py:47] epoch 4181, training loss: 3110.30, average training loss: 3325.03, base loss: 4627.49
[INFO 2017-06-26 22:27:13,597 main.py:47] epoch 4182, training loss: 3312.26, average training loss: 3325.13, base loss: 4628.16
[INFO 2017-06-26 22:27:14,006 main.py:47] epoch 4183, training loss: 3040.82, average training loss: 3322.09, base loss: 4625.11
[INFO 2017-06-26 22:27:14,420 main.py:47] epoch 4184, training loss: 2895.72, average training loss: 3322.03, base loss: 4624.93
[INFO 2017-06-26 22:27:14,830 main.py:47] epoch 4185, training loss: 3122.23, average training loss: 3321.91, base loss: 4624.79
[INFO 2017-06-26 22:27:15,238 main.py:47] epoch 4186, training loss: 3072.52, average training loss: 3322.11, base loss: 4625.43
[INFO 2017-06-26 22:27:15,641 main.py:47] epoch 4187, training loss: 3073.96, average training loss: 3321.91, base loss: 4625.40
[INFO 2017-06-26 22:27:16,046 main.py:47] epoch 4188, training loss: 2637.09, average training loss: 3320.81, base loss: 4623.16
[INFO 2017-06-26 22:27:16,456 main.py:47] epoch 4189, training loss: 2970.46, average training loss: 3320.79, base loss: 4623.01
[INFO 2017-06-26 22:27:16,863 main.py:47] epoch 4190, training loss: 3443.63, average training loss: 3320.98, base loss: 4623.78
[INFO 2017-06-26 22:27:17,267 main.py:47] epoch 4191, training loss: 3251.88, average training loss: 3320.81, base loss: 4623.65
[INFO 2017-06-26 22:27:17,683 main.py:47] epoch 4192, training loss: 2952.06, average training loss: 3320.67, base loss: 4623.55
[INFO 2017-06-26 22:27:18,088 main.py:47] epoch 4193, training loss: 2779.87, average training loss: 3320.12, base loss: 4622.72
[INFO 2017-06-26 22:27:18,497 main.py:47] epoch 4194, training loss: 3065.50, average training loss: 3320.48, base loss: 4623.47
[INFO 2017-06-26 22:27:18,907 main.py:47] epoch 4195, training loss: 3043.83, average training loss: 3320.34, base loss: 4623.24
[INFO 2017-06-26 22:27:19,312 main.py:47] epoch 4196, training loss: 3010.27, average training loss: 3320.49, base loss: 4623.36
[INFO 2017-06-26 22:27:19,716 main.py:47] epoch 4197, training loss: 3514.70, average training loss: 3321.09, base loss: 4624.79
[INFO 2017-06-26 22:27:20,121 main.py:47] epoch 4198, training loss: 3175.48, average training loss: 3320.67, base loss: 4624.21
[INFO 2017-06-26 22:27:20,526 main.py:47] epoch 4199, training loss: 2985.71, average training loss: 3320.28, base loss: 4623.26
[INFO 2017-06-26 22:27:20,526 main.py:49] epoch 4199, testing
[INFO 2017-06-26 22:27:22,193 main.py:102] average testing loss: 3470.30, base loss: 4843.21
[INFO 2017-06-26 22:27:22,193 main.py:103] improve_loss: 1372.90, improve_percent: 0.28
[INFO 2017-06-26 22:27:22,193 main.py:73] current best improved percent: 0.31
[INFO 2017-06-26 22:27:22,597 main.py:47] epoch 4200, training loss: 2815.50, average training loss: 3319.71, base loss: 4622.15
[INFO 2017-06-26 22:27:23,007 main.py:47] epoch 4201, training loss: 3348.71, average training loss: 3319.73, base loss: 4622.31
[INFO 2017-06-26 22:27:23,412 main.py:47] epoch 4202, training loss: 3079.45, average training loss: 3319.51, base loss: 4621.90
[INFO 2017-06-26 22:27:23,817 main.py:47] epoch 4203, training loss: 2935.41, average training loss: 3318.98, base loss: 4621.04
[INFO 2017-06-26 22:27:24,221 main.py:47] epoch 4204, training loss: 3038.71, average training loss: 3319.04, base loss: 4621.33
[INFO 2017-06-26 22:27:24,624 main.py:47] epoch 4205, training loss: 2953.86, average training loss: 3318.23, base loss: 4620.27
[INFO 2017-06-26 22:27:25,028 main.py:47] epoch 4206, training loss: 3027.75, average training loss: 3317.63, base loss: 4619.61
[INFO 2017-06-26 22:27:25,517 main.py:47] epoch 4207, training loss: 3597.13, average training loss: 3317.93, base loss: 4620.05
[INFO 2017-06-26 22:27:25,920 main.py:47] epoch 4208, training loss: 3308.20, average training loss: 3317.83, base loss: 4620.46
[INFO 2017-06-26 22:27:26,318 main.py:47] epoch 4209, training loss: 3184.23, average training loss: 3317.71, base loss: 4620.45
[INFO 2017-06-26 22:27:26,717 main.py:47] epoch 4210, training loss: 3257.15, average training loss: 3317.90, base loss: 4621.01
[INFO 2017-06-26 22:27:27,116 main.py:47] epoch 4211, training loss: 3197.15, average training loss: 3318.09, base loss: 4621.44
[INFO 2017-06-26 22:27:27,515 main.py:47] epoch 4212, training loss: 3127.67, average training loss: 3318.07, base loss: 4621.54
[INFO 2017-06-26 22:27:28,025 main.py:47] epoch 4213, training loss: 3249.58, average training loss: 3317.72, base loss: 4620.88
[INFO 2017-06-26 22:27:28,441 main.py:47] epoch 4214, training loss: 3326.65, average training loss: 3318.07, base loss: 4621.24
[INFO 2017-06-26 22:27:28,847 main.py:47] epoch 4215, training loss: 3010.90, average training loss: 3317.96, base loss: 4621.07
[INFO 2017-06-26 22:27:29,255 main.py:47] epoch 4216, training loss: 3283.28, average training loss: 3318.15, base loss: 4621.79
[INFO 2017-06-26 22:27:29,670 main.py:47] epoch 4217, training loss: 3302.37, average training loss: 3318.06, base loss: 4622.33
[INFO 2017-06-26 22:27:30,077 main.py:47] epoch 4218, training loss: 3155.45, average training loss: 3318.09, base loss: 4622.34
[INFO 2017-06-26 22:27:30,479 main.py:47] epoch 4219, training loss: 3414.54, average training loss: 3318.26, base loss: 4623.04
[INFO 2017-06-26 22:27:30,879 main.py:47] epoch 4220, training loss: 3230.96, average training loss: 3318.27, base loss: 4623.54
[INFO 2017-06-26 22:27:31,291 main.py:47] epoch 4221, training loss: 3104.66, average training loss: 3317.87, base loss: 4622.87
[INFO 2017-06-26 22:27:31,699 main.py:47] epoch 4222, training loss: 3366.90, average training loss: 3317.86, base loss: 4623.34
[INFO 2017-06-26 22:27:32,176 main.py:47] epoch 4223, training loss: 2855.53, average training loss: 3317.34, base loss: 4622.53
[INFO 2017-06-26 22:27:32,598 main.py:47] epoch 4224, training loss: 3421.54, average training loss: 3318.02, base loss: 4624.06
[INFO 2017-06-26 22:27:33,001 main.py:47] epoch 4225, training loss: 2714.74, average training loss: 3317.47, base loss: 4623.16
[INFO 2017-06-26 22:27:33,403 main.py:47] epoch 4226, training loss: 3019.39, average training loss: 3317.34, base loss: 4622.97
[INFO 2017-06-26 22:27:33,803 main.py:47] epoch 4227, training loss: 3264.32, average training loss: 3317.65, base loss: 4623.88
[INFO 2017-06-26 22:27:34,212 main.py:47] epoch 4228, training loss: 3297.91, average training loss: 3317.50, base loss: 4623.68
[INFO 2017-06-26 22:27:34,614 main.py:47] epoch 4229, training loss: 2870.71, average training loss: 3317.01, base loss: 4622.78
[INFO 2017-06-26 22:27:35,019 main.py:47] epoch 4230, training loss: 2878.98, average training loss: 3316.78, base loss: 4622.51
[INFO 2017-06-26 22:27:35,424 main.py:47] epoch 4231, training loss: 3242.87, average training loss: 3316.86, base loss: 4622.90
[INFO 2017-06-26 22:27:35,827 main.py:47] epoch 4232, training loss: 3297.12, average training loss: 3316.92, base loss: 4623.42
[INFO 2017-06-26 22:27:36,239 main.py:47] epoch 4233, training loss: 2756.37, average training loss: 3316.63, base loss: 4623.02
[INFO 2017-06-26 22:27:36,641 main.py:47] epoch 4234, training loss: 2973.66, average training loss: 3316.24, base loss: 4622.15
[INFO 2017-06-26 22:27:37,154 main.py:47] epoch 4235, training loss: 2993.89, average training loss: 3316.28, base loss: 4622.11
[INFO 2017-06-26 22:27:37,559 main.py:47] epoch 4236, training loss: 3172.47, average training loss: 3315.82, base loss: 4621.56
[INFO 2017-06-26 22:27:38,039 main.py:47] epoch 4237, training loss: 3343.95, average training loss: 3312.75, base loss: 4619.20
[INFO 2017-06-26 22:27:38,467 main.py:47] epoch 4238, training loss: 5778.39, average training loss: 3315.46, base loss: 4622.75
[INFO 2017-06-26 22:27:38,871 main.py:47] epoch 4239, training loss: 2580.50, average training loss: 3314.12, base loss: 4620.07
[INFO 2017-06-26 22:27:39,274 main.py:47] epoch 4240, training loss: 3190.76, average training loss: 3314.22, base loss: 4620.51
[INFO 2017-06-26 22:27:39,680 main.py:47] epoch 4241, training loss: 3423.24, average training loss: 3314.76, base loss: 4622.05
[INFO 2017-06-26 22:27:40,083 main.py:47] epoch 4242, training loss: 2875.11, average training loss: 3314.58, base loss: 4622.01
[INFO 2017-06-26 22:27:40,486 main.py:47] epoch 4243, training loss: 6266.74, average training loss: 3314.02, base loss: 4621.72
[INFO 2017-06-26 22:27:40,895 main.py:47] epoch 4244, training loss: 2631.59, average training loss: 3313.59, base loss: 4620.72
[INFO 2017-06-26 22:27:41,296 main.py:47] epoch 4245, training loss: 3301.07, average training loss: 3313.79, base loss: 4621.55
[INFO 2017-06-26 22:27:41,697 main.py:47] epoch 4246, training loss: 3213.48, average training loss: 3314.03, base loss: 4621.97
[INFO 2017-06-26 22:27:42,095 main.py:47] epoch 4247, training loss: 3276.58, average training loss: 3314.30, base loss: 4622.29
[INFO 2017-06-26 22:27:42,498 main.py:47] epoch 4248, training loss: 2658.67, average training loss: 3313.88, base loss: 4621.54
[INFO 2017-06-26 22:27:42,896 main.py:47] epoch 4249, training loss: 2645.67, average training loss: 3313.22, base loss: 4620.54
[INFO 2017-06-26 22:27:43,298 main.py:47] epoch 4250, training loss: 2974.22, average training loss: 3309.89, base loss: 4616.66
[INFO 2017-06-26 22:27:43,696 main.py:47] epoch 4251, training loss: 3271.84, average training loss: 3310.02, base loss: 4617.34
[INFO 2017-06-26 22:27:44,099 main.py:47] epoch 4252, training loss: 3147.16, average training loss: 3310.16, base loss: 4617.72
[INFO 2017-06-26 22:27:44,498 main.py:47] epoch 4253, training loss: 5578.30, average training loss: 3312.22, base loss: 4620.36
[INFO 2017-06-26 22:27:44,902 main.py:47] epoch 4254, training loss: 3250.36, average training loss: 3312.47, base loss: 4620.88
[INFO 2017-06-26 22:27:45,307 main.py:47] epoch 4255, training loss: 3193.68, average training loss: 3312.31, base loss: 4620.84
[INFO 2017-06-26 22:27:45,714 main.py:47] epoch 4256, training loss: 6439.96, average training loss: 3315.33, base loss: 4624.21
[INFO 2017-06-26 22:27:46,117 main.py:47] epoch 4257, training loss: 3213.01, average training loss: 3315.46, base loss: 4624.33
[INFO 2017-06-26 22:27:46,524 main.py:47] epoch 4258, training loss: 3314.93, average training loss: 3315.88, base loss: 4625.52
[INFO 2017-06-26 22:27:46,929 main.py:47] epoch 4259, training loss: 2974.06, average training loss: 3312.88, base loss: 4622.24
[INFO 2017-06-26 22:27:47,331 main.py:47] epoch 4260, training loss: 2926.58, average training loss: 3312.36, base loss: 4621.27
[INFO 2017-06-26 22:27:47,732 main.py:47] epoch 4261, training loss: 3158.64, average training loss: 3312.42, base loss: 4621.42
[INFO 2017-06-26 22:27:48,141 main.py:47] epoch 4262, training loss: 3340.11, average training loss: 3312.54, base loss: 4621.81
[INFO 2017-06-26 22:27:48,543 main.py:47] epoch 4263, training loss: 3029.32, average training loss: 3312.39, base loss: 4621.76
[INFO 2017-06-26 22:27:48,948 main.py:47] epoch 4264, training loss: 3204.58, average training loss: 3312.11, base loss: 4621.63
[INFO 2017-06-26 22:27:49,354 main.py:47] epoch 4265, training loss: 2959.11, average training loss: 3311.76, base loss: 4620.91
[INFO 2017-06-26 22:27:49,755 main.py:47] epoch 4266, training loss: 3165.32, average training loss: 3311.49, base loss: 4620.46
[INFO 2017-06-26 22:27:50,200 main.py:47] epoch 4267, training loss: 2894.41, average training loss: 3311.16, base loss: 4620.13
[INFO 2017-06-26 22:27:50,620 main.py:47] epoch 4268, training loss: 2999.22, average training loss: 3311.09, base loss: 4620.09
[INFO 2017-06-26 22:27:51,025 main.py:47] epoch 4269, training loss: 3321.50, average training loss: 3311.07, base loss: 4619.87
[INFO 2017-06-26 22:27:51,434 main.py:47] epoch 4270, training loss: 6431.39, average training loss: 3314.04, base loss: 4623.82
[INFO 2017-06-26 22:27:51,837 main.py:47] epoch 4271, training loss: 2980.80, average training loss: 3313.71, base loss: 4623.49
[INFO 2017-06-26 22:27:52,257 main.py:47] epoch 4272, training loss: 3150.63, average training loss: 3313.96, base loss: 4623.82
[INFO 2017-06-26 22:27:52,689 main.py:47] epoch 4273, training loss: 3060.55, average training loss: 3313.92, base loss: 4623.90
[INFO 2017-06-26 22:27:53,096 main.py:47] epoch 4274, training loss: 2916.70, average training loss: 3313.57, base loss: 4623.37
[INFO 2017-06-26 22:27:53,593 main.py:47] epoch 4275, training loss: 3184.75, average training loss: 3313.69, base loss: 4623.60
[INFO 2017-06-26 22:27:53,996 main.py:47] epoch 4276, training loss: 3388.63, average training loss: 3313.94, base loss: 4624.11
[INFO 2017-06-26 22:27:54,398 main.py:47] epoch 4277, training loss: 2738.46, average training loss: 3313.69, base loss: 4623.44
[INFO 2017-06-26 22:27:54,807 main.py:47] epoch 4278, training loss: 3328.31, average training loss: 3313.88, base loss: 4624.17
[INFO 2017-06-26 22:27:55,211 main.py:47] epoch 4279, training loss: 2892.97, average training loss: 3313.19, base loss: 4622.99
[INFO 2017-06-26 22:27:55,611 main.py:47] epoch 4280, training loss: 3291.64, average training loss: 3312.98, base loss: 4622.41
[INFO 2017-06-26 22:27:56,013 main.py:47] epoch 4281, training loss: 2778.58, average training loss: 3312.82, base loss: 4622.41
[INFO 2017-06-26 22:27:56,422 main.py:47] epoch 4282, training loss: 2578.61, average training loss: 3312.30, base loss: 4621.27
[INFO 2017-06-26 22:27:56,825 main.py:47] epoch 4283, training loss: 2990.26, average training loss: 3312.31, base loss: 4621.47
[INFO 2017-06-26 22:27:57,228 main.py:47] epoch 4284, training loss: 3007.92, average training loss: 3311.86, base loss: 4620.78
[INFO 2017-06-26 22:27:57,631 main.py:47] epoch 4285, training loss: 5737.27, average training loss: 3314.25, base loss: 4624.23
[INFO 2017-06-26 22:27:58,035 main.py:47] epoch 4286, training loss: 3447.03, average training loss: 3314.70, base loss: 4625.31
[INFO 2017-06-26 22:27:58,444 main.py:47] epoch 4287, training loss: 3134.07, average training loss: 3314.15, base loss: 4624.31
[INFO 2017-06-26 22:27:58,848 main.py:47] epoch 4288, training loss: 3231.64, average training loss: 3314.05, base loss: 4624.35
[INFO 2017-06-26 22:27:59,258 main.py:47] epoch 4289, training loss: 3428.57, average training loss: 3313.95, base loss: 4624.55
[INFO 2017-06-26 22:27:59,657 main.py:47] epoch 4290, training loss: 3504.21, average training loss: 3314.07, base loss: 4625.10
[INFO 2017-06-26 22:28:00,059 main.py:47] epoch 4291, training loss: 2924.75, average training loss: 3313.47, base loss: 4623.67
[INFO 2017-06-26 22:28:00,463 main.py:47] epoch 4292, training loss: 3164.07, average training loss: 3313.30, base loss: 4623.44
[INFO 2017-06-26 22:28:00,870 main.py:47] epoch 4293, training loss: 3196.20, average training loss: 3313.69, base loss: 4624.22
[INFO 2017-06-26 22:28:01,274 main.py:47] epoch 4294, training loss: 3000.83, average training loss: 3313.72, base loss: 4624.79
[INFO 2017-06-26 22:28:01,679 main.py:47] epoch 4295, training loss: 2693.07, average training loss: 3312.97, base loss: 4623.46
[INFO 2017-06-26 22:28:02,085 main.py:47] epoch 4296, training loss: 3082.40, average training loss: 3312.75, base loss: 4623.04
[INFO 2017-06-26 22:28:02,484 main.py:47] epoch 4297, training loss: 3435.28, average training loss: 3312.92, base loss: 4623.63
[INFO 2017-06-26 22:28:02,887 main.py:47] epoch 4298, training loss: 2732.44, average training loss: 3312.45, base loss: 4622.69
[INFO 2017-06-26 22:28:03,290 main.py:47] epoch 4299, training loss: 2983.30, average training loss: 3311.86, base loss: 4622.02
[INFO 2017-06-26 22:28:03,290 main.py:49] epoch 4299, testing
[INFO 2017-06-26 22:28:04,956 main.py:102] average testing loss: 3058.98, base loss: 4363.63
[INFO 2017-06-26 22:28:04,956 main.py:103] improve_loss: 1304.65, improve_percent: 0.30
[INFO 2017-06-26 22:28:04,957 main.py:73] current best improved percent: 0.31
[INFO 2017-06-26 22:28:05,357 main.py:47] epoch 4300, training loss: 2992.87, average training loss: 3311.65, base loss: 4621.48
[INFO 2017-06-26 22:28:05,756 main.py:47] epoch 4301, training loss: 2967.58, average training loss: 3311.86, base loss: 4622.20
[INFO 2017-06-26 22:28:06,157 main.py:47] epoch 4302, training loss: 2890.85, average training loss: 3311.46, base loss: 4621.65
[INFO 2017-06-26 22:28:06,557 main.py:47] epoch 4303, training loss: 3281.67, average training loss: 3311.18, base loss: 4620.68
[INFO 2017-06-26 22:28:06,962 main.py:47] epoch 4304, training loss: 2750.01, average training loss: 3310.71, base loss: 4620.06
[INFO 2017-06-26 22:28:07,363 main.py:47] epoch 4305, training loss: 3288.72, average training loss: 3310.78, base loss: 4620.40
[INFO 2017-06-26 22:28:07,766 main.py:47] epoch 4306, training loss: 3750.90, average training loss: 3311.71, base loss: 4622.52
[INFO 2017-06-26 22:28:08,170 main.py:47] epoch 4307, training loss: 2923.53, average training loss: 3311.38, base loss: 4621.53
[INFO 2017-06-26 22:28:08,581 main.py:47] epoch 4308, training loss: 3146.64, average training loss: 3311.35, base loss: 4621.55
[INFO 2017-06-26 22:28:08,981 main.py:47] epoch 4309, training loss: 3488.00, average training loss: 3308.53, base loss: 4618.96
[INFO 2017-06-26 22:28:09,389 main.py:47] epoch 4310, training loss: 3273.01, average training loss: 3308.47, base loss: 4618.90
[INFO 2017-06-26 22:28:09,795 main.py:47] epoch 4311, training loss: 3403.52, average training loss: 3308.37, base loss: 4619.15
[INFO 2017-06-26 22:28:10,199 main.py:47] epoch 4312, training loss: 3293.63, average training loss: 3308.12, base loss: 4618.75
[INFO 2017-06-26 22:28:10,605 main.py:47] epoch 4313, training loss: 3066.94, average training loss: 3305.03, base loss: 4614.94
[INFO 2017-06-26 22:28:11,009 main.py:47] epoch 4314, training loss: 3623.14, average training loss: 3305.20, base loss: 4615.88
[INFO 2017-06-26 22:28:11,412 main.py:47] epoch 4315, training loss: 3173.45, average training loss: 3305.08, base loss: 4615.97
[INFO 2017-06-26 22:28:11,817 main.py:47] epoch 4316, training loss: 3203.33, average training loss: 3304.87, base loss: 4615.94
[INFO 2017-06-26 22:28:12,223 main.py:47] epoch 4317, training loss: 3015.37, average training loss: 3304.59, base loss: 4615.60
[INFO 2017-06-26 22:28:12,627 main.py:47] epoch 4318, training loss: 3210.25, average training loss: 3304.44, base loss: 4615.91
[INFO 2017-06-26 22:28:13,032 main.py:47] epoch 4319, training loss: 3105.69, average training loss: 3304.39, base loss: 4615.77
[INFO 2017-06-26 22:28:13,475 main.py:47] epoch 4320, training loss: 2853.26, average training loss: 3304.20, base loss: 4615.17
[INFO 2017-06-26 22:28:13,880 main.py:47] epoch 4321, training loss: 3430.84, average training loss: 3304.01, base loss: 4615.46
[INFO 2017-06-26 22:28:14,286 main.py:47] epoch 4322, training loss: 3108.58, average training loss: 3304.05, base loss: 4615.55
[INFO 2017-06-26 22:28:14,686 main.py:47] epoch 4323, training loss: 3533.87, average training loss: 3304.37, base loss: 4616.42
[INFO 2017-06-26 22:28:15,089 main.py:47] epoch 4324, training loss: 3154.72, average training loss: 3304.52, base loss: 4616.95
[INFO 2017-06-26 22:28:15,494 main.py:47] epoch 4325, training loss: 3447.46, average training loss: 3304.82, base loss: 4617.65
[INFO 2017-06-26 22:28:15,912 main.py:47] epoch 4326, training loss: 2941.91, average training loss: 3304.59, base loss: 4617.23
[INFO 2017-06-26 22:28:16,312 main.py:47] epoch 4327, training loss: 3546.12, average training loss: 3300.96, base loss: 4614.10
[INFO 2017-06-26 22:28:16,717 main.py:47] epoch 4328, training loss: 2886.66, average training loss: 3300.47, base loss: 4613.19
[INFO 2017-06-26 22:28:17,120 main.py:47] epoch 4329, training loss: 3652.53, average training loss: 3300.78, base loss: 4614.09
[INFO 2017-06-26 22:28:17,524 main.py:47] epoch 4330, training loss: 3387.12, average training loss: 3300.66, base loss: 4613.96
[INFO 2017-06-26 22:28:17,929 main.py:47] epoch 4331, training loss: 3570.85, average training loss: 3300.74, base loss: 4614.81
[INFO 2017-06-26 22:28:18,336 main.py:47] epoch 4332, training loss: 3120.89, average training loss: 3300.65, base loss: 4614.37
[INFO 2017-06-26 22:28:18,740 main.py:47] epoch 4333, training loss: 3249.08, average training loss: 3300.61, base loss: 4614.36
[INFO 2017-06-26 22:28:19,144 main.py:47] epoch 4334, training loss: 3092.82, average training loss: 3300.67, base loss: 4614.72
[INFO 2017-06-26 22:28:19,555 main.py:47] epoch 4335, training loss: 3380.31, average training loss: 3300.91, base loss: 4615.66
[INFO 2017-06-26 22:28:19,958 main.py:47] epoch 4336, training loss: 5997.56, average training loss: 3303.17, base loss: 4617.74
[INFO 2017-06-26 22:28:20,362 main.py:47] epoch 4337, training loss: 2981.04, average training loss: 3303.04, base loss: 4617.57
[INFO 2017-06-26 22:28:20,771 main.py:47] epoch 4338, training loss: 3295.28, average training loss: 3303.69, base loss: 4619.11
[INFO 2017-06-26 22:28:21,179 main.py:47] epoch 4339, training loss: 3109.50, average training loss: 3303.95, base loss: 4619.78
[INFO 2017-06-26 22:28:21,580 main.py:47] epoch 4340, training loss: 3534.20, average training loss: 3304.21, base loss: 4620.62
[INFO 2017-06-26 22:28:21,978 main.py:47] epoch 4341, training loss: 3073.82, average training loss: 3303.94, base loss: 4620.04
[INFO 2017-06-26 22:28:22,378 main.py:47] epoch 4342, training loss: 2827.09, average training loss: 3303.69, base loss: 4619.66
[INFO 2017-06-26 22:28:22,785 main.py:47] epoch 4343, training loss: 2976.93, average training loss: 3304.10, base loss: 4620.53
[INFO 2017-06-26 22:28:23,189 main.py:47] epoch 4344, training loss: 2775.88, average training loss: 3303.39, base loss: 4619.16
[INFO 2017-06-26 22:28:23,590 main.py:47] epoch 4345, training loss: 3247.28, average training loss: 3303.30, base loss: 4619.48
[INFO 2017-06-26 22:28:23,993 main.py:47] epoch 4346, training loss: 3271.78, average training loss: 3303.32, base loss: 4619.84
[INFO 2017-06-26 22:28:24,399 main.py:47] epoch 4347, training loss: 3057.19, average training loss: 3303.12, base loss: 4619.57
[INFO 2017-06-26 22:28:24,803 main.py:47] epoch 4348, training loss: 2953.33, average training loss: 3302.82, base loss: 4619.14
[INFO 2017-06-26 22:28:25,210 main.py:47] epoch 4349, training loss: 2809.38, average training loss: 3302.64, base loss: 4618.84
[INFO 2017-06-26 22:28:25,655 main.py:47] epoch 4350, training loss: 3253.89, average training loss: 3302.74, base loss: 4618.88
[INFO 2017-06-26 22:28:26,063 main.py:47] epoch 4351, training loss: 2978.65, average training loss: 3299.19, base loss: 4615.30
[INFO 2017-06-26 22:28:26,467 main.py:47] epoch 4352, training loss: 3004.55, average training loss: 3298.83, base loss: 4614.51
[INFO 2017-06-26 22:28:26,871 main.py:47] epoch 4353, training loss: 3204.96, average training loss: 3299.08, base loss: 4614.80
[INFO 2017-06-26 22:28:27,366 main.py:47] epoch 4354, training loss: 3545.66, average training loss: 3299.57, base loss: 4616.10
[INFO 2017-06-26 22:28:27,772 main.py:47] epoch 4355, training loss: 2845.47, average training loss: 3299.54, base loss: 4616.17
[INFO 2017-06-26 22:28:28,177 main.py:47] epoch 4356, training loss: 5712.55, average training loss: 3302.48, base loss: 4620.09
[INFO 2017-06-26 22:28:28,583 main.py:47] epoch 4357, training loss: 3405.76, average training loss: 3302.87, base loss: 4621.08
[INFO 2017-06-26 22:28:28,988 main.py:47] epoch 4358, training loss: 2844.01, average training loss: 3302.66, base loss: 4620.78
[INFO 2017-06-26 22:28:29,395 main.py:47] epoch 4359, training loss: 3069.32, average training loss: 3302.36, base loss: 4620.52
[INFO 2017-06-26 22:28:29,800 main.py:47] epoch 4360, training loss: 3214.34, average training loss: 3302.31, base loss: 4620.40
[INFO 2017-06-26 22:28:30,213 main.py:47] epoch 4361, training loss: 3211.77, average training loss: 3302.75, base loss: 4621.15
[INFO 2017-06-26 22:28:30,615 main.py:47] epoch 4362, training loss: 2999.21, average training loss: 3302.94, base loss: 4621.86
[INFO 2017-06-26 22:28:31,024 main.py:47] epoch 4363, training loss: 3738.70, average training loss: 3303.63, base loss: 4623.26
[INFO 2017-06-26 22:28:31,429 main.py:47] epoch 4364, training loss: 3044.65, average training loss: 3303.48, base loss: 4623.16
[INFO 2017-06-26 22:28:31,840 main.py:47] epoch 4365, training loss: 3285.84, average training loss: 3303.33, base loss: 4622.96
[INFO 2017-06-26 22:28:32,246 main.py:47] epoch 4366, training loss: 3133.55, average training loss: 3302.93, base loss: 4622.33
[INFO 2017-06-26 22:28:32,649 main.py:47] epoch 4367, training loss: 3089.60, average training loss: 3302.92, base loss: 4622.87
[INFO 2017-06-26 22:28:33,052 main.py:47] epoch 4368, training loss: 3371.45, average training loss: 3302.99, base loss: 4623.39
[INFO 2017-06-26 22:28:33,535 main.py:47] epoch 4369, training loss: 3233.46, average training loss: 3302.74, base loss: 4623.26
[INFO 2017-06-26 22:28:33,968 main.py:47] epoch 4370, training loss: 3351.84, average training loss: 3302.86, base loss: 4623.86
[INFO 2017-06-26 22:28:34,379 main.py:47] epoch 4371, training loss: 2962.98, average training loss: 3302.72, base loss: 4623.65
[INFO 2017-06-26 22:28:34,781 main.py:47] epoch 4372, training loss: 3540.79, average training loss: 3303.23, base loss: 4624.88
[INFO 2017-06-26 22:28:35,187 main.py:47] epoch 4373, training loss: 2885.90, average training loss: 3303.10, base loss: 4624.55
[INFO 2017-06-26 22:28:35,596 main.py:47] epoch 4374, training loss: 2769.29, average training loss: 3302.73, base loss: 4624.03
[INFO 2017-06-26 22:28:36,002 main.py:47] epoch 4375, training loss: 2775.33, average training loss: 3302.37, base loss: 4623.36
[INFO 2017-06-26 22:28:36,406 main.py:47] epoch 4376, training loss: 2928.44, average training loss: 3301.92, base loss: 4622.89
[INFO 2017-06-26 22:28:36,812 main.py:47] epoch 4377, training loss: 3307.85, average training loss: 3301.70, base loss: 4622.28
[INFO 2017-06-26 22:28:37,216 main.py:47] epoch 4378, training loss: 2869.63, average training loss: 3301.79, base loss: 4622.66
[INFO 2017-06-26 22:28:37,664 main.py:47] epoch 4379, training loss: 3037.19, average training loss: 3301.59, base loss: 4622.34
[INFO 2017-06-26 22:28:38,115 main.py:47] epoch 4380, training loss: 3345.03, average training loss: 3301.37, base loss: 4622.51
[INFO 2017-06-26 22:28:38,516 main.py:47] epoch 4381, training loss: 3104.32, average training loss: 3301.39, base loss: 4622.78
[INFO 2017-06-26 22:28:38,921 main.py:47] epoch 4382, training loss: 3228.58, average training loss: 3301.27, base loss: 4622.67
[INFO 2017-06-26 22:28:39,335 main.py:47] epoch 4383, training loss: 3109.76, average training loss: 3301.37, base loss: 4622.67
[INFO 2017-06-26 22:28:39,743 main.py:47] epoch 4384, training loss: 2746.41, average training loss: 3300.97, base loss: 4622.13
[INFO 2017-06-26 22:28:40,148 main.py:47] epoch 4385, training loss: 3363.79, average training loss: 3301.15, base loss: 4622.77
[INFO 2017-06-26 22:28:40,557 main.py:47] epoch 4386, training loss: 2953.75, average training loss: 3301.10, base loss: 4622.48
[INFO 2017-06-26 22:28:40,963 main.py:47] epoch 4387, training loss: 2997.84, average training loss: 3300.37, base loss: 4620.92
[INFO 2017-06-26 22:28:41,370 main.py:47] epoch 4388, training loss: 3018.84, average training loss: 3300.30, base loss: 4620.91
[INFO 2017-06-26 22:28:41,775 main.py:47] epoch 4389, training loss: 3287.04, average training loss: 3300.45, base loss: 4621.40
[INFO 2017-06-26 22:28:42,193 main.py:47] epoch 4390, training loss: 2831.63, average training loss: 3299.88, base loss: 4620.45
[INFO 2017-06-26 22:28:42,602 main.py:47] epoch 4391, training loss: 3116.54, average training loss: 3299.58, base loss: 4620.19
[INFO 2017-06-26 22:28:43,007 main.py:47] epoch 4392, training loss: 3092.98, average training loss: 3299.44, base loss: 4620.09
[INFO 2017-06-26 22:28:43,415 main.py:47] epoch 4393, training loss: 3293.18, average training loss: 3299.75, base loss: 4620.67
[INFO 2017-06-26 22:28:43,826 main.py:47] epoch 4394, training loss: 3133.43, average training loss: 3299.82, base loss: 4621.15
[INFO 2017-06-26 22:28:44,231 main.py:47] epoch 4395, training loss: 2960.15, average training loss: 3299.83, base loss: 4621.23
[INFO 2017-06-26 22:28:44,635 main.py:47] epoch 4396, training loss: 3038.33, average training loss: 3296.28, base loss: 4617.76
[INFO 2017-06-26 22:28:45,040 main.py:47] epoch 4397, training loss: 2956.22, average training loss: 3296.04, base loss: 4617.22
[INFO 2017-06-26 22:28:45,443 main.py:47] epoch 4398, training loss: 3566.25, average training loss: 3296.46, base loss: 4618.18
[INFO 2017-06-26 22:28:45,853 main.py:47] epoch 4399, training loss: 3115.03, average training loss: 3296.38, base loss: 4618.10
[INFO 2017-06-26 22:28:45,853 main.py:49] epoch 4399, testing
[INFO 2017-06-26 22:28:47,507 main.py:102] average testing loss: 3214.32, base loss: 4778.89
[INFO 2017-06-26 22:28:47,507 main.py:103] improve_loss: 1564.57, improve_percent: 0.33
[INFO 2017-06-26 22:28:47,508 main.py:69] model save to ./model/final.pth
[INFO 2017-06-26 22:28:47,520 main.py:73] current best improved percent: 0.33
[INFO 2017-06-26 22:28:47,931 main.py:47] epoch 4400, training loss: 3081.88, average training loss: 3296.16, base loss: 4617.53
[INFO 2017-06-26 22:28:48,335 main.py:47] epoch 4401, training loss: 2946.84, average training loss: 3296.19, base loss: 4617.71
[INFO 2017-06-26 22:28:48,739 main.py:47] epoch 4402, training loss: 3370.31, average training loss: 3296.43, base loss: 4618.58
[INFO 2017-06-26 22:28:49,145 main.py:47] epoch 4403, training loss: 3098.30, average training loss: 3296.59, base loss: 4619.08
[INFO 2017-06-26 22:28:49,548 main.py:47] epoch 4404, training loss: 3289.46, average training loss: 3296.67, base loss: 4619.02
[INFO 2017-06-26 22:28:49,953 main.py:47] epoch 4405, training loss: 3408.41, average training loss: 3296.95, base loss: 4620.11
[INFO 2017-06-26 22:28:50,355 main.py:47] epoch 4406, training loss: 3118.83, average training loss: 3296.98, base loss: 4620.90
[INFO 2017-06-26 22:28:50,855 main.py:47] epoch 4407, training loss: 3149.30, average training loss: 3293.91, base loss: 4618.10
[INFO 2017-06-26 22:28:51,259 main.py:47] epoch 4408, training loss: 3165.57, average training loss: 3293.79, base loss: 4618.00
[INFO 2017-06-26 22:28:51,734 main.py:47] epoch 4409, training loss: 5088.28, average training loss: 3295.64, base loss: 4620.47
[INFO 2017-06-26 22:28:52,165 main.py:47] epoch 4410, training loss: 3288.47, average training loss: 3295.38, base loss: 4619.89
[INFO 2017-06-26 22:28:52,610 main.py:47] epoch 4411, training loss: 3252.57, average training loss: 3295.64, base loss: 4620.55
[INFO 2017-06-26 22:28:53,069 main.py:47] epoch 4412, training loss: 2920.97, average training loss: 3295.39, base loss: 4619.94
[INFO 2017-06-26 22:28:53,470 main.py:47] epoch 4413, training loss: 2710.08, average training loss: 3295.00, base loss: 4619.22
[INFO 2017-06-26 22:28:53,912 main.py:47] epoch 4414, training loss: 2440.33, average training loss: 3294.08, base loss: 4617.54
[INFO 2017-06-26 22:28:54,345 main.py:47] epoch 4415, training loss: 3086.86, average training loss: 3294.09, base loss: 4617.61
[INFO 2017-06-26 22:28:54,747 main.py:47] epoch 4416, training loss: 2766.30, average training loss: 3293.77, base loss: 4617.48
[INFO 2017-06-26 22:28:55,150 main.py:47] epoch 4417, training loss: 3274.67, average training loss: 3294.19, base loss: 4618.04
[INFO 2017-06-26 22:28:55,579 main.py:47] epoch 4418, training loss: 3154.18, average training loss: 3294.18, base loss: 4618.59
[INFO 2017-06-26 22:28:55,985 main.py:47] epoch 4419, training loss: 3024.62, average training loss: 3294.06, base loss: 4618.76
[INFO 2017-06-26 22:28:56,391 main.py:47] epoch 4420, training loss: 3144.71, average training loss: 3294.13, base loss: 4618.75
[INFO 2017-06-26 22:28:56,799 main.py:47] epoch 4421, training loss: 3326.65, average training loss: 3294.34, base loss: 4619.45
[INFO 2017-06-26 22:28:57,203 main.py:47] epoch 4422, training loss: 2880.22, average training loss: 3294.13, base loss: 4619.04
[INFO 2017-06-26 22:28:57,606 main.py:47] epoch 4423, training loss: 3489.93, average training loss: 3294.82, base loss: 4620.59
[INFO 2017-06-26 22:28:58,015 main.py:47] epoch 4424, training loss: 2900.42, average training loss: 3294.94, base loss: 4620.84
[INFO 2017-06-26 22:28:58,423 main.py:47] epoch 4425, training loss: 2736.73, average training loss: 3294.45, base loss: 4619.97
[INFO 2017-06-26 22:28:58,825 main.py:47] epoch 4426, training loss: 3503.53, average training loss: 3294.79, base loss: 4621.22
[INFO 2017-06-26 22:28:59,229 main.py:47] epoch 4427, training loss: 3211.02, average training loss: 3294.83, base loss: 4621.30
[INFO 2017-06-26 22:28:59,639 main.py:47] epoch 4428, training loss: 3040.16, average training loss: 3294.59, base loss: 4620.87
[INFO 2017-06-26 22:29:00,045 main.py:47] epoch 4429, training loss: 3058.40, average training loss: 3294.52, base loss: 4620.74
[INFO 2017-06-26 22:29:00,455 main.py:47] epoch 4430, training loss: 3022.59, average training loss: 3294.72, base loss: 4621.28
[INFO 2017-06-26 22:29:00,862 main.py:47] epoch 4431, training loss: 5990.69, average training loss: 3297.44, base loss: 4625.33
[INFO 2017-06-26 22:29:01,267 main.py:47] epoch 4432, training loss: 3379.71, average training loss: 3297.34, base loss: 4625.35
[INFO 2017-06-26 22:29:01,665 main.py:47] epoch 4433, training loss: 2983.01, average training loss: 3297.03, base loss: 4624.93
[INFO 2017-06-26 22:29:02,115 main.py:47] epoch 4434, training loss: 3405.59, average training loss: 3297.00, base loss: 4624.86
[INFO 2017-06-26 22:29:02,526 main.py:47] epoch 4435, training loss: 3320.04, average training loss: 3297.52, base loss: 4626.45
[INFO 2017-06-26 22:29:03,023 main.py:47] epoch 4436, training loss: 3155.88, average training loss: 3297.38, base loss: 4626.26
[INFO 2017-06-26 22:29:03,431 main.py:47] epoch 4437, training loss: 2719.21, average training loss: 3296.99, base loss: 4625.73
[INFO 2017-06-26 22:29:03,836 main.py:47] epoch 4438, training loss: 3110.19, average training loss: 3296.70, base loss: 4624.97
[INFO 2017-06-26 22:29:04,241 main.py:47] epoch 4439, training loss: 2618.07, average training loss: 3296.01, base loss: 4623.47
[INFO 2017-06-26 22:29:04,655 main.py:47] epoch 4440, training loss: 3216.88, average training loss: 3296.05, base loss: 4623.61
[INFO 2017-06-26 22:29:05,063 main.py:47] epoch 4441, training loss: 3136.45, average training loss: 3296.20, base loss: 4624.17
[INFO 2017-06-26 22:29:05,466 main.py:47] epoch 4442, training loss: 2850.38, average training loss: 3295.98, base loss: 4623.75
[INFO 2017-06-26 22:29:05,868 main.py:47] epoch 4443, training loss: 3151.18, average training loss: 3296.07, base loss: 4624.26
[INFO 2017-06-26 22:29:06,269 main.py:47] epoch 4444, training loss: 3081.37, average training loss: 3295.59, base loss: 4623.74
[INFO 2017-06-26 22:29:06,666 main.py:47] epoch 4445, training loss: 2993.76, average training loss: 3295.47, base loss: 4623.49
[INFO 2017-06-26 22:29:07,070 main.py:47] epoch 4446, training loss: 3293.72, average training loss: 3295.12, base loss: 4622.80
[INFO 2017-06-26 22:29:07,475 main.py:47] epoch 4447, training loss: 3071.71, average training loss: 3295.08, base loss: 4623.39
[INFO 2017-06-26 22:29:07,877 main.py:47] epoch 4448, training loss: 2867.67, average training loss: 3295.13, base loss: 4623.59
[INFO 2017-06-26 22:29:08,279 main.py:47] epoch 4449, training loss: 3115.17, average training loss: 3294.84, base loss: 4623.40
[INFO 2017-06-26 22:29:08,683 main.py:47] epoch 4450, training loss: 3010.57, average training loss: 3295.02, base loss: 4623.95
[INFO 2017-06-26 22:29:09,086 main.py:47] epoch 4451, training loss: 3143.65, average training loss: 3294.68, base loss: 4623.77
[INFO 2017-06-26 22:29:09,488 main.py:47] epoch 4452, training loss: 2766.19, average training loss: 3294.17, base loss: 4622.91
[INFO 2017-06-26 22:29:09,891 main.py:47] epoch 4453, training loss: 3111.60, average training loss: 3293.80, base loss: 4622.26
[INFO 2017-06-26 22:29:10,293 main.py:47] epoch 4454, training loss: 3121.04, average training loss: 3293.06, base loss: 4621.29
[INFO 2017-06-26 22:29:10,690 main.py:47] epoch 4455, training loss: 3017.01, average training loss: 3292.57, base loss: 4620.29
[INFO 2017-06-26 22:29:11,094 main.py:47] epoch 4456, training loss: 2892.05, average training loss: 3292.32, base loss: 4620.13
[INFO 2017-06-26 22:29:11,500 main.py:47] epoch 4457, training loss: 2773.51, average training loss: 3291.97, base loss: 4619.72
[INFO 2017-06-26 22:29:11,903 main.py:47] epoch 4458, training loss: 2908.13, average training loss: 3291.67, base loss: 4619.11
[INFO 2017-06-26 22:29:12,309 main.py:47] epoch 4459, training loss: 2970.31, average training loss: 3291.37, base loss: 4618.96
[INFO 2017-06-26 22:29:12,713 main.py:47] epoch 4460, training loss: 5935.52, average training loss: 3294.13, base loss: 4623.05
[INFO 2017-06-26 22:29:13,123 main.py:47] epoch 4461, training loss: 3161.39, average training loss: 3293.98, base loss: 4622.97
[INFO 2017-06-26 22:29:13,527 main.py:47] epoch 4462, training loss: 2856.31, average training loss: 3293.53, base loss: 4622.43
[INFO 2017-06-26 22:29:13,929 main.py:47] epoch 4463, training loss: 2910.93, average training loss: 3293.30, base loss: 4621.90
[INFO 2017-06-26 22:29:14,420 main.py:47] epoch 4464, training loss: 3016.28, average training loss: 3290.59, base loss: 4618.76
[INFO 2017-06-26 22:29:14,848 main.py:47] epoch 4465, training loss: 2941.54, average training loss: 3290.65, base loss: 4618.94
[INFO 2017-06-26 22:29:15,253 main.py:47] epoch 4466, training loss: 3364.41, average training loss: 3290.52, base loss: 4618.78
[INFO 2017-06-26 22:29:15,657 main.py:47] epoch 4467, training loss: 3130.18, average training loss: 3290.08, base loss: 4617.91
[INFO 2017-06-26 22:29:16,060 main.py:47] epoch 4468, training loss: 3671.30, average training loss: 3290.24, base loss: 4618.55
[INFO 2017-06-26 22:29:16,468 main.py:47] epoch 4469, training loss: 3112.26, average training loss: 3290.43, base loss: 4619.47
[INFO 2017-06-26 22:29:16,876 main.py:47] epoch 4470, training loss: 2685.23, average training loss: 3289.77, base loss: 4618.28
[INFO 2017-06-26 22:29:17,284 main.py:47] epoch 4471, training loss: 3130.18, average training loss: 3289.57, base loss: 4618.11
[INFO 2017-06-26 22:29:17,713 main.py:47] epoch 4472, training loss: 3039.47, average training loss: 3289.00, base loss: 4617.26
[INFO 2017-06-26 22:29:18,128 main.py:47] epoch 4473, training loss: 3073.18, average training loss: 3288.83, base loss: 4616.88
[INFO 2017-06-26 22:29:18,534 main.py:47] epoch 4474, training loss: 3302.09, average training loss: 3288.86, base loss: 4617.04
[INFO 2017-06-26 22:29:18,947 main.py:47] epoch 4475, training loss: 2918.60, average training loss: 3288.57, base loss: 4616.60
[INFO 2017-06-26 22:29:19,358 main.py:47] epoch 4476, training loss: 3447.38, average training loss: 3288.97, base loss: 4617.10
[INFO 2017-06-26 22:29:19,796 main.py:47] epoch 4477, training loss: 2887.95, average training loss: 3288.40, base loss: 4616.02
[INFO 2017-06-26 22:29:20,203 main.py:47] epoch 4478, training loss: 3320.92, average training loss: 3288.47, base loss: 4616.29
[INFO 2017-06-26 22:29:20,613 main.py:47] epoch 4479, training loss: 2910.81, average training loss: 3284.85, base loss: 4611.79
[INFO 2017-06-26 22:29:21,022 main.py:47] epoch 4480, training loss: 2737.38, average training loss: 3284.23, base loss: 4610.72
[INFO 2017-06-26 22:29:21,447 main.py:47] epoch 4481, training loss: 5630.01, average training loss: 3286.39, base loss: 4613.83
[INFO 2017-06-26 22:29:21,940 main.py:47] epoch 4482, training loss: 2971.60, average training loss: 3286.26, base loss: 4613.54
[INFO 2017-06-26 22:29:22,423 main.py:47] epoch 4483, training loss: 2942.02, average training loss: 3285.89, base loss: 4613.06
[INFO 2017-06-26 22:29:22,929 main.py:47] epoch 4484, training loss: 3325.92, average training loss: 3286.19, base loss: 4613.82
[INFO 2017-06-26 22:29:23,408 main.py:47] epoch 4485, training loss: 3016.02, average training loss: 3286.37, base loss: 4614.25
[INFO 2017-06-26 22:29:23,839 main.py:47] epoch 4486, training loss: 2980.41, average training loss: 3286.04, base loss: 4613.81
[INFO 2017-06-26 22:29:24,293 main.py:47] epoch 4487, training loss: 2854.00, average training loss: 3285.59, base loss: 4613.03
[INFO 2017-06-26 22:29:24,809 main.py:47] epoch 4488, training loss: 2795.23, average training loss: 3285.35, base loss: 4612.67
[INFO 2017-06-26 22:29:25,234 main.py:47] epoch 4489, training loss: 3175.18, average training loss: 3285.31, base loss: 4612.50
[INFO 2017-06-26 22:29:25,640 main.py:47] epoch 4490, training loss: 2996.98, average training loss: 3284.69, base loss: 4611.29
[INFO 2017-06-26 22:29:26,041 main.py:47] epoch 4491, training loss: 2722.54, average training loss: 3284.02, base loss: 4610.16
[INFO 2017-06-26 22:29:26,448 main.py:47] epoch 4492, training loss: 2758.71, average training loss: 3283.96, base loss: 4609.99
[INFO 2017-06-26 22:29:26,852 main.py:47] epoch 4493, training loss: 2873.56, average training loss: 3283.90, base loss: 4609.84
[INFO 2017-06-26 22:29:27,360 main.py:47] epoch 4494, training loss: 2502.62, average training loss: 3283.52, base loss: 4609.38
[INFO 2017-06-26 22:29:27,767 main.py:47] epoch 4495, training loss: 2675.99, average training loss: 3280.21, base loss: 4605.15
[INFO 2017-06-26 22:29:28,172 main.py:47] epoch 4496, training loss: 3181.32, average training loss: 3280.27, base loss: 4605.49
[INFO 2017-06-26 22:29:28,578 main.py:47] epoch 4497, training loss: 3191.02, average training loss: 3280.62, base loss: 4606.15
[INFO 2017-06-26 22:29:28,992 main.py:47] epoch 4498, training loss: 3195.89, average training loss: 3280.64, base loss: 4606.26
[INFO 2017-06-26 22:29:29,395 main.py:47] epoch 4499, training loss: 2955.43, average training loss: 3280.22, base loss: 4605.56
[INFO 2017-06-26 22:29:29,395 main.py:49] epoch 4499, testing
[INFO 2017-06-26 22:29:31,040 main.py:102] average testing loss: 3004.82, base loss: 4340.25
[INFO 2017-06-26 22:29:31,041 main.py:103] improve_loss: 1335.42, improve_percent: 0.31
[INFO 2017-06-26 22:29:31,041 main.py:73] current best improved percent: 0.33
[INFO 2017-06-26 22:29:31,443 main.py:47] epoch 4500, training loss: 2720.58, average training loss: 3279.33, base loss: 4604.01
[INFO 2017-06-26 22:29:31,853 main.py:47] epoch 4501, training loss: 2708.13, average training loss: 3278.48, base loss: 4602.57
[INFO 2017-06-26 22:29:32,260 main.py:47] epoch 4502, training loss: 2775.96, average training loss: 3277.93, base loss: 4601.65
[INFO 2017-06-26 22:29:32,662 main.py:47] epoch 4503, training loss: 3183.95, average training loss: 3277.35, base loss: 4600.29
[INFO 2017-06-26 22:29:33,060 main.py:47] epoch 4504, training loss: 2696.12, average training loss: 3276.49, base loss: 4598.96
[INFO 2017-06-26 22:29:33,463 main.py:47] epoch 4505, training loss: 3323.79, average training loss: 3276.39, base loss: 4598.68
[INFO 2017-06-26 22:29:33,867 main.py:47] epoch 4506, training loss: 2705.24, average training loss: 3275.75, base loss: 4597.75
[INFO 2017-06-26 22:29:34,271 main.py:47] epoch 4507, training loss: 3037.65, average training loss: 3275.55, base loss: 4597.46
[INFO 2017-06-26 22:29:34,693 main.py:47] epoch 4508, training loss: 2835.72, average training loss: 3275.29, base loss: 4597.03
[INFO 2017-06-26 22:29:35,107 main.py:47] epoch 4509, training loss: 3642.78, average training loss: 3275.78, base loss: 4598.03
[INFO 2017-06-26 22:29:35,507 main.py:47] epoch 4510, training loss: 3328.08, average training loss: 3275.79, base loss: 4598.19
[INFO 2017-06-26 22:29:35,912 main.py:47] epoch 4511, training loss: 3198.50, average training loss: 3275.70, base loss: 4598.39
[INFO 2017-06-26 22:29:36,316 main.py:47] epoch 4512, training loss: 3102.56, average training loss: 3275.92, base loss: 4599.01
[INFO 2017-06-26 22:29:36,720 main.py:47] epoch 4513, training loss: 2974.68, average training loss: 3276.00, base loss: 4599.24
[INFO 2017-06-26 22:29:37,124 main.py:47] epoch 4514, training loss: 3052.76, average training loss: 3275.61, base loss: 4598.81
[INFO 2017-06-26 22:29:37,528 main.py:47] epoch 4515, training loss: 5786.56, average training loss: 3278.15, base loss: 4602.53
[INFO 2017-06-26 22:29:37,932 main.py:47] epoch 4516, training loss: 3334.77, average training loss: 3278.05, base loss: 4602.90
[INFO 2017-06-26 22:29:38,335 main.py:47] epoch 4517, training loss: 3183.60, average training loss: 3278.04, base loss: 4602.90
[INFO 2017-06-26 22:29:38,740 main.py:47] epoch 4518, training loss: 2668.16, average training loss: 3277.62, base loss: 4601.93
[INFO 2017-06-26 22:29:39,148 main.py:47] epoch 4519, training loss: 3120.59, average training loss: 3277.83, base loss: 4602.19
[INFO 2017-06-26 22:29:39,553 main.py:47] epoch 4520, training loss: 3316.91, average training loss: 3277.92, base loss: 4602.58
[INFO 2017-06-26 22:29:39,959 main.py:47] epoch 4521, training loss: 3204.79, average training loss: 3277.89, base loss: 4602.42
[INFO 2017-06-26 22:29:40,429 main.py:47] epoch 4522, training loss: 2893.46, average training loss: 3277.80, base loss: 4602.56
[INFO 2017-06-26 22:29:40,858 main.py:47] epoch 4523, training loss: 3249.32, average training loss: 3278.02, base loss: 4603.05
[INFO 2017-06-26 22:29:41,273 main.py:47] epoch 4524, training loss: 3406.56, average training loss: 3278.21, base loss: 4603.69
[INFO 2017-06-26 22:29:41,679 main.py:47] epoch 4525, training loss: 2830.87, average training loss: 3278.01, base loss: 4603.16
[INFO 2017-06-26 22:29:42,083 main.py:47] epoch 4526, training loss: 3320.56, average training loss: 3278.11, base loss: 4603.66
[INFO 2017-06-26 22:29:42,532 main.py:47] epoch 4527, training loss: 2871.96, average training loss: 3277.81, base loss: 4603.37
[INFO 2017-06-26 22:29:42,964 main.py:47] epoch 4528, training loss: 3380.43, average training loss: 3274.68, base loss: 4600.66
[INFO 2017-06-26 22:29:43,416 main.py:47] epoch 4529, training loss: 3258.87, average training loss: 3274.85, base loss: 4601.33
[INFO 2017-06-26 22:29:43,885 main.py:47] epoch 4530, training loss: 3162.28, average training loss: 3274.41, base loss: 4600.85
[INFO 2017-06-26 22:29:44,291 main.py:47] epoch 4531, training loss: 3132.50, average training loss: 3274.59, base loss: 4601.48
[INFO 2017-06-26 22:29:44,700 main.py:47] epoch 4532, training loss: 3304.40, average training loss: 3274.59, base loss: 4601.42
[INFO 2017-06-26 22:29:45,105 main.py:47] epoch 4533, training loss: 3027.93, average training loss: 3274.13, base loss: 4601.10
[INFO 2017-06-26 22:29:45,509 main.py:47] epoch 4534, training loss: 2742.46, average training loss: 3273.52, base loss: 4600.23
[INFO 2017-06-26 22:29:45,916 main.py:47] epoch 4535, training loss: 3282.51, average training loss: 3273.55, base loss: 4600.66
[INFO 2017-06-26 22:29:46,325 main.py:47] epoch 4536, training loss: 3214.25, average training loss: 3273.67, base loss: 4600.85
[INFO 2017-06-26 22:29:46,731 main.py:47] epoch 4537, training loss: 3017.74, average training loss: 3273.72, base loss: 4601.31
[INFO 2017-06-26 22:29:47,134 main.py:47] epoch 4538, training loss: 2857.44, average training loss: 3273.66, base loss: 4601.13
[INFO 2017-06-26 22:29:47,539 main.py:47] epoch 4539, training loss: 6033.18, average training loss: 3276.80, base loss: 4605.09
[INFO 2017-06-26 22:29:47,940 main.py:47] epoch 4540, training loss: 3241.25, average training loss: 3276.71, base loss: 4605.25
[INFO 2017-06-26 22:29:48,356 main.py:47] epoch 4541, training loss: 2991.68, average training loss: 3276.82, base loss: 4605.63
[INFO 2017-06-26 22:29:48,760 main.py:47] epoch 4542, training loss: 3562.32, average training loss: 3277.27, base loss: 4606.99
[INFO 2017-06-26 22:29:49,160 main.py:47] epoch 4543, training loss: 3071.51, average training loss: 3277.51, base loss: 4607.34
[INFO 2017-06-26 22:29:49,645 main.py:47] epoch 4544, training loss: 3507.36, average training loss: 3277.72, base loss: 4607.85
[INFO 2017-06-26 22:29:50,069 main.py:47] epoch 4545, training loss: 3209.57, average training loss: 3277.76, base loss: 4608.20
[INFO 2017-06-26 22:29:50,486 main.py:47] epoch 4546, training loss: 3256.40, average training loss: 3277.90, base loss: 4608.61
[INFO 2017-06-26 22:29:50,991 main.py:47] epoch 4547, training loss: 2742.91, average training loss: 3277.63, base loss: 4607.99
[INFO 2017-06-26 22:29:51,400 main.py:47] epoch 4548, training loss: 2933.58, average training loss: 3277.34, base loss: 4607.50
[INFO 2017-06-26 22:29:51,811 main.py:47] epoch 4549, training loss: 3202.12, average training loss: 3277.50, base loss: 4607.82
[INFO 2017-06-26 22:29:52,214 main.py:47] epoch 4550, training loss: 2688.76, average training loss: 3277.23, base loss: 4607.76
[INFO 2017-06-26 22:29:52,618 main.py:47] epoch 4551, training loss: 5593.76, average training loss: 3279.94, base loss: 4611.79
[INFO 2017-06-26 22:29:53,028 main.py:47] epoch 4552, training loss: 2733.38, average training loss: 3279.31, base loss: 4610.73
[INFO 2017-06-26 22:29:53,432 main.py:47] epoch 4553, training loss: 3043.77, average training loss: 3279.34, base loss: 4611.22
[INFO 2017-06-26 22:29:53,840 main.py:47] epoch 4554, training loss: 2953.60, average training loss: 3278.99, base loss: 4610.64
[INFO 2017-06-26 22:29:54,245 main.py:47] epoch 4555, training loss: 2808.34, average training loss: 3278.21, base loss: 4609.10
[INFO 2017-06-26 22:29:54,652 main.py:47] epoch 4556, training loss: 5908.42, average training loss: 3280.56, base loss: 4612.34
[INFO 2017-06-26 22:29:55,056 main.py:47] epoch 4557, training loss: 2755.54, average training loss: 3280.21, base loss: 4611.43
[INFO 2017-06-26 22:29:55,462 main.py:47] epoch 4558, training loss: 3239.91, average training loss: 3280.09, base loss: 4611.36
[INFO 2017-06-26 22:29:55,867 main.py:47] epoch 4559, training loss: 2970.99, average training loss: 3279.77, base loss: 4610.62
[INFO 2017-06-26 22:29:56,313 main.py:47] epoch 4560, training loss: 2887.58, average training loss: 3279.43, base loss: 4610.05
[INFO 2017-06-26 22:29:56,750 main.py:47] epoch 4561, training loss: 2820.46, average training loss: 3279.38, base loss: 4609.95
[INFO 2017-06-26 22:29:57,155 main.py:47] epoch 4562, training loss: 3141.53, average training loss: 3278.96, base loss: 4609.76
[INFO 2017-06-26 22:29:57,575 main.py:47] epoch 4563, training loss: 3061.79, average training loss: 3276.07, base loss: 4606.26
[INFO 2017-06-26 22:29:57,975 main.py:47] epoch 4564, training loss: 3166.45, average training loss: 3276.16, base loss: 4606.03
[INFO 2017-06-26 22:29:58,385 main.py:47] epoch 4565, training loss: 2895.12, average training loss: 3276.03, base loss: 4605.92
[INFO 2017-06-26 22:29:58,788 main.py:47] epoch 4566, training loss: 3042.90, average training loss: 3275.65, base loss: 4605.31
[INFO 2017-06-26 22:29:59,193 main.py:47] epoch 4567, training loss: 2959.52, average training loss: 3275.02, base loss: 4603.99
[INFO 2017-06-26 22:29:59,594 main.py:47] epoch 4568, training loss: 3310.01, average training loss: 3274.89, base loss: 4603.66
[INFO 2017-06-26 22:29:59,999 main.py:47] epoch 4569, training loss: 3356.09, average training loss: 3274.85, base loss: 4604.14
[INFO 2017-06-26 22:30:00,405 main.py:47] epoch 4570, training loss: 2876.78, average training loss: 3274.67, base loss: 4603.67
[INFO 2017-06-26 22:30:00,809 main.py:47] epoch 4571, training loss: 3067.63, average training loss: 3274.47, base loss: 4603.54
[INFO 2017-06-26 22:30:01,215 main.py:47] epoch 4572, training loss: 2966.62, average training loss: 3274.08, base loss: 4602.83
[INFO 2017-06-26 22:30:01,624 main.py:47] epoch 4573, training loss: 2765.02, average training loss: 3274.04, base loss: 4602.74
[INFO 2017-06-26 22:30:02,073 main.py:47] epoch 4574, training loss: 3231.22, average training loss: 3274.42, base loss: 4603.62
[INFO 2017-06-26 22:30:02,509 main.py:47] epoch 4575, training loss: 3187.80, average training loss: 3274.76, base loss: 4604.27
[INFO 2017-06-26 22:30:02,954 main.py:47] epoch 4576, training loss: 2846.78, average training loss: 3274.05, base loss: 4602.94
[INFO 2017-06-26 22:30:03,394 main.py:47] epoch 4577, training loss: 3310.67, average training loss: 3274.41, base loss: 4603.90
[INFO 2017-06-26 22:30:03,803 main.py:47] epoch 4578, training loss: 2742.03, average training loss: 3273.73, base loss: 4603.04
[INFO 2017-06-26 22:30:04,208 main.py:47] epoch 4579, training loss: 3219.66, average training loss: 3273.93, base loss: 4603.96
[INFO 2017-06-26 22:30:04,621 main.py:47] epoch 4580, training loss: 2996.33, average training loss: 3273.90, base loss: 4604.09
[INFO 2017-06-26 22:30:05,027 main.py:47] epoch 4581, training loss: 3698.27, average training loss: 3274.20, base loss: 4603.92
[INFO 2017-06-26 22:30:05,432 main.py:47] epoch 4582, training loss: 3100.44, average training loss: 3273.89, base loss: 4603.33
[INFO 2017-06-26 22:30:05,836 main.py:47] epoch 4583, training loss: 6054.20, average training loss: 3276.63, base loss: 4605.98
[INFO 2017-06-26 22:30:06,243 main.py:47] epoch 4584, training loss: 3538.42, average training loss: 3276.78, base loss: 4605.84
[INFO 2017-06-26 22:30:06,647 main.py:47] epoch 4585, training loss: 3788.46, average training loss: 3277.40, base loss: 4606.58
[INFO 2017-06-26 22:30:07,052 main.py:47] epoch 4586, training loss: 3747.27, average training loss: 3274.60, base loss: 4603.55
[INFO 2017-06-26 22:30:07,458 main.py:47] epoch 4587, training loss: 3490.29, average training loss: 3274.84, base loss: 4603.11
[INFO 2017-06-26 22:30:07,875 main.py:47] epoch 4588, training loss: 4280.68, average training loss: 3275.84, base loss: 4604.43
[INFO 2017-06-26 22:30:08,280 main.py:47] epoch 4589, training loss: 3229.93, average training loss: 3276.14, base loss: 4604.51
[INFO 2017-06-26 22:30:08,689 main.py:47] epoch 4590, training loss: 3031.65, average training loss: 3276.14, base loss: 4604.26
[INFO 2017-06-26 22:30:09,095 main.py:47] epoch 4591, training loss: 3156.74, average training loss: 3276.28, base loss: 4603.96
[INFO 2017-06-26 22:30:09,594 main.py:47] epoch 4592, training loss: 3945.31, average training loss: 3277.24, base loss: 4605.16
[INFO 2017-06-26 22:30:09,999 main.py:47] epoch 4593, training loss: 3157.32, average training loss: 3277.27, base loss: 4604.63
[INFO 2017-06-26 22:30:10,404 main.py:47] epoch 4594, training loss: 3724.28, average training loss: 3277.96, base loss: 4605.28
[INFO 2017-06-26 22:30:10,938 main.py:47] epoch 4595, training loss: 3609.21, average training loss: 3278.25, base loss: 4605.49
[INFO 2017-06-26 22:30:11,354 main.py:47] epoch 4596, training loss: 3615.44, average training loss: 3278.89, base loss: 4606.38
[INFO 2017-06-26 22:30:11,788 main.py:47] epoch 4597, training loss: 3604.77, average training loss: 3279.29, base loss: 4606.70
[INFO 2017-06-26 22:30:12,195 main.py:47] epoch 4598, training loss: 6897.52, average training loss: 3283.09, base loss: 4610.30
[INFO 2017-06-26 22:30:12,728 main.py:47] epoch 4599, training loss: 3482.98, average training loss: 3283.47, base loss: 4610.78
[INFO 2017-06-26 22:30:12,729 main.py:49] epoch 4599, testing
[INFO 2017-06-26 22:30:14,515 main.py:102] average testing loss: 3421.90, base loss: 4426.31
[INFO 2017-06-26 22:30:14,515 main.py:103] improve_loss: 1004.41, improve_percent: 0.23
[INFO 2017-06-26 22:30:14,516 main.py:73] current best improved percent: 0.33
[INFO 2017-06-26 22:30:14,919 main.py:47] epoch 4600, training loss: 3679.62, average training loss: 3283.85, base loss: 4611.33
[INFO 2017-06-26 22:30:15,326 main.py:47] epoch 4601, training loss: 3380.87, average training loss: 3283.90, base loss: 4610.81
[INFO 2017-06-26 22:30:15,724 main.py:47] epoch 4602, training loss: 3701.33, average training loss: 3284.73, base loss: 4612.13
[INFO 2017-06-26 22:30:16,123 main.py:47] epoch 4603, training loss: 3619.03, average training loss: 3285.31, base loss: 4612.65
[INFO 2017-06-26 22:30:16,525 main.py:47] epoch 4604, training loss: 2965.83, average training loss: 3285.51, base loss: 4612.78
[INFO 2017-06-26 22:30:16,931 main.py:47] epoch 4605, training loss: 6722.98, average training loss: 3289.18, base loss: 4615.96
[INFO 2017-06-26 22:30:17,335 main.py:47] epoch 4606, training loss: 2924.65, average training loss: 3289.02, base loss: 4615.57
[INFO 2017-06-26 22:30:17,735 main.py:47] epoch 4607, training loss: 2938.63, average training loss: 3288.90, base loss: 4615.03
[INFO 2017-06-26 22:30:18,136 main.py:47] epoch 4608, training loss: 3106.50, average training loss: 3289.00, base loss: 4615.16
[INFO 2017-06-26 22:30:18,541 main.py:47] epoch 4609, training loss: 2832.25, average training loss: 3287.89, base loss: 4613.07
[INFO 2017-06-26 22:30:18,942 main.py:47] epoch 4610, training loss: 3169.62, average training loss: 3287.93, base loss: 4612.68
[INFO 2017-06-26 22:30:19,345 main.py:47] epoch 4611, training loss: 3053.66, average training loss: 3287.88, base loss: 4612.47
[INFO 2017-06-26 22:30:19,750 main.py:47] epoch 4612, training loss: 2874.82, average training loss: 3287.43, base loss: 4611.26
[INFO 2017-06-26 22:30:20,154 main.py:47] epoch 4613, training loss: 3106.07, average training loss: 3286.60, base loss: 4609.42
[INFO 2017-06-26 22:30:20,562 main.py:47] epoch 4614, training loss: 3464.65, average training loss: 3286.64, base loss: 4609.00
[INFO 2017-06-26 22:30:20,966 main.py:47] epoch 4615, training loss: 3140.03, average training loss: 3286.29, base loss: 4608.09
[INFO 2017-06-26 22:30:21,406 main.py:47] epoch 4616, training loss: 3262.36, average training loss: 3286.43, base loss: 4608.35
[INFO 2017-06-26 22:30:21,836 main.py:47] epoch 4617, training loss: 2837.82, average training loss: 3286.38, base loss: 4608.20
[INFO 2017-06-26 22:30:22,250 main.py:47] epoch 4618, training loss: 3530.90, average training loss: 3286.37, base loss: 4608.26
[INFO 2017-06-26 22:30:22,661 main.py:47] epoch 4619, training loss: 3239.26, average training loss: 3286.70, base loss: 4609.06
[INFO 2017-06-26 22:30:23,069 main.py:47] epoch 4620, training loss: 3154.21, average training loss: 3286.67, base loss: 4608.42
[INFO 2017-06-26 22:30:23,472 main.py:47] epoch 4621, training loss: 3077.07, average training loss: 3286.83, base loss: 4608.75
[INFO 2017-06-26 22:30:23,874 main.py:47] epoch 4622, training loss: 3205.86, average training loss: 3287.15, base loss: 4609.11
[INFO 2017-06-26 22:30:24,279 main.py:47] epoch 4623, training loss: 3342.49, average training loss: 3287.33, base loss: 4609.56
[INFO 2017-06-26 22:30:24,684 main.py:47] epoch 4624, training loss: 2990.92, average training loss: 3287.03, base loss: 4608.68
[INFO 2017-06-26 22:30:25,087 main.py:47] epoch 4625, training loss: 2694.08, average training loss: 3286.70, base loss: 4608.18
[INFO 2017-06-26 22:30:25,491 main.py:47] epoch 4626, training loss: 3282.63, average training loss: 3283.65, base loss: 4604.67
[INFO 2017-06-26 22:30:25,899 main.py:47] epoch 4627, training loss: 2699.34, average training loss: 3279.92, base loss: 4600.19
[INFO 2017-06-26 22:30:26,303 main.py:47] epoch 4628, training loss: 3296.72, average training loss: 3276.90, base loss: 4596.53
[INFO 2017-06-26 22:30:26,722 main.py:47] epoch 4629, training loss: 6467.49, average training loss: 3280.82, base loss: 4600.94
[INFO 2017-06-26 22:30:27,155 main.py:47] epoch 4630, training loss: 2968.34, average training loss: 3280.53, base loss: 4600.41
[INFO 2017-06-26 22:30:27,562 main.py:47] epoch 4631, training loss: 2922.23, average training loss: 3280.16, base loss: 4599.60
[INFO 2017-06-26 22:30:28,086 main.py:47] epoch 4632, training loss: 3781.79, average training loss: 3280.75, base loss: 4600.91
[INFO 2017-06-26 22:30:28,490 main.py:47] epoch 4633, training loss: 2944.36, average training loss: 3280.69, base loss: 4600.88
[INFO 2017-06-26 22:30:28,975 main.py:47] epoch 4634, training loss: 2911.32, average training loss: 3280.59, base loss: 4601.10
[INFO 2017-06-26 22:30:29,386 main.py:47] epoch 4635, training loss: 2989.62, average training loss: 3280.35, base loss: 4600.66
[INFO 2017-06-26 22:30:29,799 main.py:47] epoch 4636, training loss: 3214.39, average training loss: 3280.65, base loss: 4600.97
[INFO 2017-06-26 22:30:30,298 main.py:47] epoch 4637, training loss: 2924.22, average training loss: 3280.25, base loss: 4599.99
[INFO 2017-06-26 22:30:30,709 main.py:47] epoch 4638, training loss: 3243.36, average training loss: 3280.19, base loss: 4599.65
[INFO 2017-06-26 22:30:31,113 main.py:47] epoch 4639, training loss: 3748.13, average training loss: 3280.77, base loss: 4600.89
[INFO 2017-06-26 22:30:31,515 main.py:47] epoch 4640, training loss: 3114.58, average training loss: 3280.97, base loss: 4601.15
[INFO 2017-06-26 22:30:31,920 main.py:47] epoch 4641, training loss: 3223.16, average training loss: 3280.92, base loss: 4601.03
[INFO 2017-06-26 22:30:32,324 main.py:47] epoch 4642, training loss: 3514.95, average training loss: 3281.47, base loss: 4602.03
[INFO 2017-06-26 22:30:32,727 main.py:47] epoch 4643, training loss: 3594.42, average training loss: 3282.16, base loss: 4603.16
[INFO 2017-06-26 22:30:33,130 main.py:47] epoch 4644, training loss: 3244.24, average training loss: 3282.20, base loss: 4602.42
[INFO 2017-06-26 22:30:33,535 main.py:47] epoch 4645, training loss: 3217.22, average training loss: 3282.46, base loss: 4603.07
[INFO 2017-06-26 22:30:33,944 main.py:47] epoch 4646, training loss: 3207.00, average training loss: 3282.42, base loss: 4602.80
[INFO 2017-06-26 22:30:34,348 main.py:47] epoch 4647, training loss: 3361.44, average training loss: 3282.44, base loss: 4602.56
[INFO 2017-06-26 22:30:34,772 main.py:47] epoch 4648, training loss: 3118.16, average training loss: 3279.54, base loss: 4598.75
[INFO 2017-06-26 22:30:35,183 main.py:47] epoch 4649, training loss: 2624.06, average training loss: 3276.02, base loss: 4593.80
[INFO 2017-06-26 22:30:35,705 main.py:47] epoch 4650, training loss: 3155.98, average training loss: 3275.87, base loss: 4593.73
[INFO 2017-06-26 22:30:36,116 main.py:47] epoch 4651, training loss: 2897.19, average training loss: 3275.63, base loss: 4593.32
[INFO 2017-06-26 22:30:36,622 main.py:47] epoch 4652, training loss: 3241.38, average training loss: 3275.79, base loss: 4593.72
[INFO 2017-06-26 22:30:37,029 main.py:47] epoch 4653, training loss: 3335.18, average training loss: 3276.17, base loss: 4594.66
[INFO 2017-06-26 22:30:37,433 main.py:47] epoch 4654, training loss: 2767.86, average training loss: 3275.61, base loss: 4593.72
[INFO 2017-06-26 22:30:37,839 main.py:47] epoch 4655, training loss: 3496.98, average training loss: 3275.81, base loss: 4594.01
[INFO 2017-06-26 22:30:38,242 main.py:47] epoch 4656, training loss: 2989.65, average training loss: 3275.89, base loss: 4594.02
[INFO 2017-06-26 22:30:38,641 main.py:47] epoch 4657, training loss: 2985.76, average training loss: 3276.08, base loss: 4594.58
[INFO 2017-06-26 22:30:39,050 main.py:47] epoch 4658, training loss: 2772.86, average training loss: 3275.48, base loss: 4593.75
[INFO 2017-06-26 22:30:39,454 main.py:47] epoch 4659, training loss: 3149.11, average training loss: 3275.64, base loss: 4593.57
[INFO 2017-06-26 22:30:39,860 main.py:47] epoch 4660, training loss: 3047.65, average training loss: 3275.52, base loss: 4593.39
[INFO 2017-06-26 22:30:40,261 main.py:47] epoch 4661, training loss: 3028.54, average training loss: 3275.36, base loss: 4593.10
[INFO 2017-06-26 22:30:40,659 main.py:47] epoch 4662, training loss: 3277.08, average training loss: 3275.19, base loss: 4592.90
[INFO 2017-06-26 22:30:41,063 main.py:47] epoch 4663, training loss: 2933.32, average training loss: 3275.27, base loss: 4593.11
[INFO 2017-06-26 22:30:41,472 main.py:47] epoch 4664, training loss: 3099.41, average training loss: 3275.15, base loss: 4592.81
[INFO 2017-06-26 22:30:41,881 main.py:47] epoch 4665, training loss: 3466.75, average training loss: 3275.30, base loss: 4593.36
[INFO 2017-06-26 22:30:42,285 main.py:47] epoch 4666, training loss: 2976.18, average training loss: 3274.94, base loss: 4592.48
[INFO 2017-06-26 22:30:42,690 main.py:47] epoch 4667, training loss: 6876.83, average training loss: 3278.52, base loss: 4596.15
[INFO 2017-06-26 22:30:43,093 main.py:47] epoch 4668, training loss: 3213.94, average training loss: 3278.60, base loss: 4596.31
[INFO 2017-06-26 22:30:43,493 main.py:47] epoch 4669, training loss: 2687.15, average training loss: 3277.82, base loss: 4594.84
[INFO 2017-06-26 22:30:43,900 main.py:47] epoch 4670, training loss: 3387.30, average training loss: 3278.16, base loss: 4595.39
[INFO 2017-06-26 22:30:44,307 main.py:47] epoch 4671, training loss: 2954.55, average training loss: 3278.01, base loss: 4595.23
[INFO 2017-06-26 22:30:44,710 main.py:47] epoch 4672, training loss: 3119.83, average training loss: 3277.76, base loss: 4594.34
[INFO 2017-06-26 22:30:45,185 main.py:47] epoch 4673, training loss: 3177.94, average training loss: 3277.44, base loss: 4593.96
[INFO 2017-06-26 22:30:45,619 main.py:47] epoch 4674, training loss: 3090.98, average training loss: 3277.54, base loss: 4594.30
[INFO 2017-06-26 22:30:46,040 main.py:47] epoch 4675, training loss: 2771.90, average training loss: 3277.03, base loss: 4593.34
[INFO 2017-06-26 22:30:46,449 main.py:47] epoch 4676, training loss: 3097.68, average training loss: 3276.97, base loss: 4593.44
[INFO 2017-06-26 22:30:46,855 main.py:47] epoch 4677, training loss: 3328.12, average training loss: 3277.24, base loss: 4594.15
[INFO 2017-06-26 22:30:47,321 main.py:47] epoch 4678, training loss: 6866.20, average training loss: 3280.98, base loss: 4597.98
[INFO 2017-06-26 22:30:47,730 main.py:47] epoch 4679, training loss: 3249.46, average training loss: 3281.14, base loss: 4598.14
[INFO 2017-06-26 22:30:48,224 main.py:47] epoch 4680, training loss: 2987.06, average training loss: 3281.07, base loss: 4598.09
[INFO 2017-06-26 22:30:48,644 main.py:47] epoch 4681, training loss: 3252.19, average training loss: 3281.26, base loss: 4598.90
[INFO 2017-06-26 22:30:49,140 main.py:47] epoch 4682, training loss: 3118.42, average training loss: 3281.33, base loss: 4599.33
[INFO 2017-06-26 22:30:49,582 main.py:47] epoch 4683, training loss: 3554.55, average training loss: 3278.82, base loss: 4596.69
[INFO 2017-06-26 22:30:49,992 main.py:47] epoch 4684, training loss: 2914.06, average training loss: 3278.58, base loss: 4596.68
[INFO 2017-06-26 22:30:50,397 main.py:47] epoch 4685, training loss: 3059.06, average training loss: 3275.20, base loss: 4593.24
[INFO 2017-06-26 22:30:50,797 main.py:47] epoch 4686, training loss: 2880.41, average training loss: 3275.09, base loss: 4593.24
[INFO 2017-06-26 22:30:51,197 main.py:47] epoch 4687, training loss: 3127.33, average training loss: 3275.34, base loss: 4593.67
[INFO 2017-06-26 22:30:51,698 main.py:47] epoch 4688, training loss: 3437.91, average training loss: 3275.65, base loss: 4594.52
[INFO 2017-06-26 22:30:52,103 main.py:47] epoch 4689, training loss: 3034.69, average training loss: 3275.34, base loss: 4593.87
[INFO 2017-06-26 22:30:52,586 main.py:47] epoch 4690, training loss: 3195.07, average training loss: 3275.28, base loss: 4593.94
[INFO 2017-06-26 22:30:53,023 main.py:47] epoch 4691, training loss: 2930.44, average training loss: 3275.02, base loss: 4593.38
[INFO 2017-06-26 22:30:53,527 main.py:47] epoch 4692, training loss: 3017.07, average training loss: 3274.87, base loss: 4593.16
[INFO 2017-06-26 22:30:53,955 main.py:47] epoch 4693, training loss: 3287.20, average training loss: 3275.10, base loss: 4593.71
[INFO 2017-06-26 22:30:54,456 main.py:47] epoch 4694, training loss: 2899.97, average training loss: 3274.94, base loss: 4593.09
[INFO 2017-06-26 22:30:54,910 main.py:47] epoch 4695, training loss: 3106.99, average training loss: 3275.14, base loss: 4593.48
[INFO 2017-06-26 22:30:55,335 main.py:47] epoch 4696, training loss: 3150.91, average training loss: 3275.01, base loss: 4593.61
[INFO 2017-06-26 22:30:55,828 main.py:47] epoch 4697, training loss: 3162.09, average training loss: 3271.83, base loss: 4589.95
[INFO 2017-06-26 22:30:56,244 main.py:47] epoch 4698, training loss: 3062.54, average training loss: 3271.73, base loss: 4589.68
[INFO 2017-06-26 22:30:56,787 main.py:47] epoch 4699, training loss: 2859.85, average training loss: 3271.75, base loss: 4589.86
[INFO 2017-06-26 22:30:56,787 main.py:49] epoch 4699, testing
[INFO 2017-06-26 22:30:58,572 main.py:102] average testing loss: 3229.24, base loss: 4749.20
[INFO 2017-06-26 22:30:58,572 main.py:103] improve_loss: 1519.96, improve_percent: 0.32
[INFO 2017-06-26 22:30:58,573 main.py:73] current best improved percent: 0.33
[INFO 2017-06-26 22:30:58,996 main.py:47] epoch 4700, training loss: 2715.19, average training loss: 3271.27, base loss: 4588.89
[INFO 2017-06-26 22:30:59,415 main.py:47] epoch 4701, training loss: 2811.17, average training loss: 3270.74, base loss: 4587.79
[INFO 2017-06-26 22:30:59,898 main.py:47] epoch 4702, training loss: 3049.87, average training loss: 3270.71, base loss: 4587.87
[INFO 2017-06-26 22:31:00,335 main.py:47] epoch 4703, training loss: 3089.25, average training loss: 3270.83, base loss: 4588.43
[INFO 2017-06-26 22:31:00,820 main.py:47] epoch 4704, training loss: 3246.01, average training loss: 3270.63, base loss: 4587.96
[INFO 2017-06-26 22:31:01,258 main.py:47] epoch 4705, training loss: 3637.24, average training loss: 3271.37, base loss: 4589.58
[INFO 2017-06-26 22:31:01,721 main.py:47] epoch 4706, training loss: 3013.40, average training loss: 3271.39, base loss: 4589.95
[INFO 2017-06-26 22:31:02,171 main.py:47] epoch 4707, training loss: 3027.82, average training loss: 3271.05, base loss: 4589.36
[INFO 2017-06-26 22:31:02,579 main.py:47] epoch 4708, training loss: 3287.91, average training loss: 3271.15, base loss: 4589.75
[INFO 2017-06-26 22:31:03,070 main.py:47] epoch 4709, training loss: 3470.66, average training loss: 3271.85, base loss: 4591.52
[INFO 2017-06-26 22:31:03,500 main.py:47] epoch 4710, training loss: 6545.05, average training loss: 3275.21, base loss: 4594.78
[INFO 2017-06-26 22:31:03,995 main.py:47] epoch 4711, training loss: 2655.94, average training loss: 3274.51, base loss: 4593.49
[INFO 2017-06-26 22:31:04,456 main.py:47] epoch 4712, training loss: 3168.80, average training loss: 3274.64, base loss: 4594.25
[INFO 2017-06-26 22:31:04,969 main.py:47] epoch 4713, training loss: 3450.48, average training loss: 3275.21, base loss: 4595.67
[INFO 2017-06-26 22:31:05,381 main.py:47] epoch 4714, training loss: 3704.89, average training loss: 3275.68, base loss: 4597.23
[INFO 2017-06-26 22:31:05,789 main.py:47] epoch 4715, training loss: 2920.75, average training loss: 3275.64, base loss: 4597.28
[INFO 2017-06-26 22:31:06,190 main.py:47] epoch 4716, training loss: 3084.87, average training loss: 3275.46, base loss: 4597.36
[INFO 2017-06-26 22:31:06,699 main.py:47] epoch 4717, training loss: 2968.26, average training loss: 3275.43, base loss: 4597.38
[INFO 2017-06-26 22:31:07,146 main.py:47] epoch 4718, training loss: 5692.59, average training loss: 3277.62, base loss: 4599.97
[INFO 2017-06-26 22:31:07,655 main.py:47] epoch 4719, training loss: 2873.38, average training loss: 3277.55, base loss: 4600.07
[INFO 2017-06-26 22:31:08,078 main.py:47] epoch 4720, training loss: 2943.74, average training loss: 3277.14, base loss: 4599.49
[INFO 2017-06-26 22:31:08,512 main.py:47] epoch 4721, training loss: 3133.53, average training loss: 3277.36, base loss: 4600.00
[INFO 2017-06-26 22:31:08,926 main.py:47] epoch 4722, training loss: 2879.37, average training loss: 3277.29, base loss: 4599.94
[INFO 2017-06-26 22:31:09,376 main.py:47] epoch 4723, training loss: 2973.50, average training loss: 3277.11, base loss: 4599.73
[INFO 2017-06-26 22:31:09,815 main.py:47] epoch 4724, training loss: 2930.78, average training loss: 3276.81, base loss: 4599.10
[INFO 2017-06-26 22:31:10,221 main.py:47] epoch 4725, training loss: 2932.85, average training loss: 3276.95, base loss: 4599.58
[INFO 2017-06-26 22:31:10,631 main.py:47] epoch 4726, training loss: 3038.53, average training loss: 3277.01, base loss: 4599.89
[INFO 2017-06-26 22:31:11,040 main.py:47] epoch 4727, training loss: 3259.59, average training loss: 3276.88, base loss: 4599.93
[INFO 2017-06-26 22:31:11,443 main.py:47] epoch 4728, training loss: 2822.16, average training loss: 3276.58, base loss: 4599.43
[INFO 2017-06-26 22:31:11,846 main.py:47] epoch 4729, training loss: 3426.89, average training loss: 3277.37, base loss: 4601.19
[INFO 2017-06-26 22:31:12,250 main.py:47] epoch 4730, training loss: 3264.49, average training loss: 3277.49, base loss: 4601.58
[INFO 2017-06-26 22:31:12,653 main.py:47] epoch 4731, training loss: 3078.06, average training loss: 3277.38, base loss: 4601.37
[INFO 2017-06-26 22:31:13,056 main.py:47] epoch 4732, training loss: 2859.69, average training loss: 3276.73, base loss: 4600.53
[INFO 2017-06-26 22:31:13,458 main.py:47] epoch 4733, training loss: 3242.81, average training loss: 3277.08, base loss: 4601.46
[INFO 2017-06-26 22:31:13,867 main.py:47] epoch 4734, training loss: 2778.71, average training loss: 3276.44, base loss: 4600.32
[INFO 2017-06-26 22:31:14,277 main.py:47] epoch 4735, training loss: 3545.50, average training loss: 3276.95, base loss: 4602.02
[INFO 2017-06-26 22:31:14,680 main.py:47] epoch 4736, training loss: 2903.10, average training loss: 3276.84, base loss: 4601.56
[INFO 2017-06-26 22:31:15,088 main.py:47] epoch 4737, training loss: 2861.07, average training loss: 3276.83, base loss: 4601.88
[INFO 2017-06-26 22:31:15,491 main.py:47] epoch 4738, training loss: 3127.38, average training loss: 3276.80, base loss: 4602.05
[INFO 2017-06-26 22:31:15,897 main.py:47] epoch 4739, training loss: 2996.33, average training loss: 3276.86, base loss: 4602.30
[INFO 2017-06-26 22:31:16,299 main.py:47] epoch 4740, training loss: 2613.08, average training loss: 3276.46, base loss: 4601.53
[INFO 2017-06-26 22:31:16,701 main.py:47] epoch 4741, training loss: 3068.10, average training loss: 3276.63, base loss: 4601.97
[INFO 2017-06-26 22:31:17,109 main.py:47] epoch 4742, training loss: 2784.29, average training loss: 3276.33, base loss: 4601.23
[INFO 2017-06-26 22:31:17,512 main.py:47] epoch 4743, training loss: 3098.76, average training loss: 3272.72, base loss: 4597.48
[INFO 2017-06-26 22:31:17,916 main.py:47] epoch 4744, training loss: 3179.41, average training loss: 3272.94, base loss: 4597.99
[INFO 2017-06-26 22:31:18,319 main.py:47] epoch 4745, training loss: 2906.82, average training loss: 3272.91, base loss: 4598.37
[INFO 2017-06-26 22:31:18,721 main.py:47] epoch 4746, training loss: 3464.79, average training loss: 3272.94, base loss: 4598.64
[INFO 2017-06-26 22:31:19,129 main.py:47] epoch 4747, training loss: 3128.94, average training loss: 3269.70, base loss: 4595.21
[INFO 2017-06-26 22:31:19,532 main.py:47] epoch 4748, training loss: 3127.96, average training loss: 3269.78, base loss: 4595.45
[INFO 2017-06-26 22:31:19,940 main.py:47] epoch 4749, training loss: 3216.07, average training loss: 3270.07, base loss: 4596.28
[INFO 2017-06-26 22:31:20,344 main.py:47] epoch 4750, training loss: 2953.75, average training loss: 3269.46, base loss: 4595.36
[INFO 2017-06-26 22:31:20,746 main.py:47] epoch 4751, training loss: 3024.95, average training loss: 3269.26, base loss: 4595.30
[INFO 2017-06-26 22:31:21,154 main.py:47] epoch 4752, training loss: 2704.19, average training loss: 3268.77, base loss: 4594.77
[INFO 2017-06-26 22:31:21,562 main.py:47] epoch 4753, training loss: 2928.35, average training loss: 3268.47, base loss: 4594.55
[INFO 2017-06-26 22:31:22,011 main.py:47] epoch 4754, training loss: 2969.57, average training loss: 3267.69, base loss: 4593.46
[INFO 2017-06-26 22:31:22,450 main.py:47] epoch 4755, training loss: 3219.45, average training loss: 3267.59, base loss: 4593.42
[INFO 2017-06-26 22:31:22,857 main.py:47] epoch 4756, training loss: 3464.01, average training loss: 3268.03, base loss: 4594.57
[INFO 2017-06-26 22:31:23,270 main.py:47] epoch 4757, training loss: 5297.40, average training loss: 3270.37, base loss: 4598.07
[INFO 2017-06-26 22:31:23,672 main.py:47] epoch 4758, training loss: 2813.73, average training loss: 3269.76, base loss: 4596.69
[INFO 2017-06-26 22:31:24,085 main.py:47] epoch 4759, training loss: 3146.37, average training loss: 3269.94, base loss: 4597.42
[INFO 2017-06-26 22:31:24,490 main.py:47] epoch 4760, training loss: 2720.59, average training loss: 3269.58, base loss: 4596.56
[INFO 2017-06-26 22:31:24,893 main.py:47] epoch 4761, training loss: 2895.33, average training loss: 3269.12, base loss: 4595.64
[INFO 2017-06-26 22:31:25,297 main.py:47] epoch 4762, training loss: 3115.75, average training loss: 3269.28, base loss: 4596.06
[INFO 2017-06-26 22:31:25,704 main.py:47] epoch 4763, training loss: 2883.03, average training loss: 3269.12, base loss: 4595.92
[INFO 2017-06-26 22:31:26,112 main.py:47] epoch 4764, training loss: 2842.84, average training loss: 3268.90, base loss: 4595.52
[INFO 2017-06-26 22:31:26,516 main.py:47] epoch 4765, training loss: 3086.37, average training loss: 3268.52, base loss: 4594.99
[INFO 2017-06-26 22:31:26,934 main.py:47] epoch 4766, training loss: 2943.68, average training loss: 3268.55, base loss: 4595.05
[INFO 2017-06-26 22:31:27,339 main.py:47] epoch 4767, training loss: 2835.67, average training loss: 3268.20, base loss: 4594.81
[INFO 2017-06-26 22:31:27,745 main.py:47] epoch 4768, training loss: 3273.27, average training loss: 3267.87, base loss: 4594.53
[INFO 2017-06-26 22:31:28,149 main.py:47] epoch 4769, training loss: 3034.05, average training loss: 3267.81, base loss: 4594.70
[INFO 2017-06-26 22:31:28,557 main.py:47] epoch 4770, training loss: 3068.30, average training loss: 3267.77, base loss: 4594.99
[INFO 2017-06-26 22:31:28,960 main.py:47] epoch 4771, training loss: 3229.60, average training loss: 3267.89, base loss: 4595.30
[INFO 2017-06-26 22:31:29,364 main.py:47] epoch 4772, training loss: 2689.49, average training loss: 3267.91, base loss: 4595.61
[INFO 2017-06-26 22:31:29,767 main.py:47] epoch 4773, training loss: 3112.40, average training loss: 3268.02, base loss: 4595.82
[INFO 2017-06-26 22:31:30,168 main.py:47] epoch 4774, training loss: 2915.56, average training loss: 3268.06, base loss: 4595.84
[INFO 2017-06-26 22:31:30,574 main.py:47] epoch 4775, training loss: 3654.80, average training loss: 3268.82, base loss: 4597.62
[INFO 2017-06-26 22:31:30,984 main.py:47] epoch 4776, training loss: 2835.96, average training loss: 3268.21, base loss: 4596.50
[INFO 2017-06-26 22:31:31,388 main.py:47] epoch 4777, training loss: 3120.06, average training loss: 3267.96, base loss: 4596.27
[INFO 2017-06-26 22:31:31,791 main.py:47] epoch 4778, training loss: 3192.40, average training loss: 3268.01, base loss: 4596.57
[INFO 2017-06-26 22:31:32,202 main.py:47] epoch 4779, training loss: 3163.65, average training loss: 3268.21, base loss: 4597.39
[INFO 2017-06-26 22:31:32,606 main.py:47] epoch 4780, training loss: 2955.91, average training loss: 3267.98, base loss: 4597.53
[INFO 2017-06-26 22:31:33,011 main.py:47] epoch 4781, training loss: 3083.76, average training loss: 3268.28, base loss: 4598.04
[INFO 2017-06-26 22:31:33,416 main.py:47] epoch 4782, training loss: 3511.76, average training loss: 3268.64, base loss: 4599.12
[INFO 2017-06-26 22:31:33,828 main.py:47] epoch 4783, training loss: 6842.62, average training loss: 3272.37, base loss: 4603.01
[INFO 2017-06-26 22:31:34,230 main.py:47] epoch 4784, training loss: 2728.01, average training loss: 3272.07, base loss: 4602.73
[INFO 2017-06-26 22:31:34,632 main.py:47] epoch 4785, training loss: 3132.94, average training loss: 3271.86, base loss: 4602.25
[INFO 2017-06-26 22:31:35,037 main.py:47] epoch 4786, training loss: 5485.95, average training loss: 3274.17, base loss: 4605.65
[INFO 2017-06-26 22:31:35,445 main.py:47] epoch 4787, training loss: 3142.51, average training loss: 3273.89, base loss: 4605.09
[INFO 2017-06-26 22:31:35,849 main.py:47] epoch 4788, training loss: 2907.65, average training loss: 3273.39, base loss: 4604.27
[INFO 2017-06-26 22:31:36,257 main.py:47] epoch 4789, training loss: 2809.76, average training loss: 3273.17, base loss: 4604.20
[INFO 2017-06-26 22:31:36,663 main.py:47] epoch 4790, training loss: 3115.38, average training loss: 3272.77, base loss: 4603.30
[INFO 2017-06-26 22:31:37,070 main.py:47] epoch 4791, training loss: 3367.55, average training loss: 3273.19, base loss: 4604.70
[INFO 2017-06-26 22:31:37,469 main.py:47] epoch 4792, training loss: 3620.41, average training loss: 3273.69, base loss: 4605.96
[INFO 2017-06-26 22:31:37,873 main.py:47] epoch 4793, training loss: 3603.57, average training loss: 3274.21, base loss: 4607.16
[INFO 2017-06-26 22:31:38,279 main.py:47] epoch 4794, training loss: 3159.34, average training loss: 3274.34, base loss: 4607.23
[INFO 2017-06-26 22:31:38,686 main.py:47] epoch 4795, training loss: 2785.47, average training loss: 3273.84, base loss: 4606.08
[INFO 2017-06-26 22:31:39,089 main.py:47] epoch 4796, training loss: 3413.54, average training loss: 3274.32, base loss: 4607.24
[INFO 2017-06-26 22:31:39,496 main.py:47] epoch 4797, training loss: 2986.09, average training loss: 3274.19, base loss: 4606.68
[INFO 2017-06-26 22:31:39,906 main.py:47] epoch 4798, training loss: 2701.21, average training loss: 3273.79, base loss: 4605.71
[INFO 2017-06-26 22:31:40,309 main.py:47] epoch 4799, training loss: 2958.56, average training loss: 3273.52, base loss: 4605.12
[INFO 2017-06-26 22:31:40,309 main.py:49] epoch 4799, testing
[INFO 2017-06-26 22:31:41,965 main.py:102] average testing loss: 2989.07, base loss: 4265.44
[INFO 2017-06-26 22:31:41,965 main.py:103] improve_loss: 1276.37, improve_percent: 0.30
[INFO 2017-06-26 22:31:41,965 main.py:73] current best improved percent: 0.33
[INFO 2017-06-26 22:31:42,377 main.py:47] epoch 4800, training loss: 3002.53, average training loss: 3273.46, base loss: 4604.95
[INFO 2017-06-26 22:31:42,784 main.py:47] epoch 4801, training loss: 3085.38, average training loss: 3273.51, base loss: 4605.30
[INFO 2017-06-26 22:31:43,188 main.py:47] epoch 4802, training loss: 2815.44, average training loss: 3273.48, base loss: 4605.23
[INFO 2017-06-26 22:31:43,599 main.py:47] epoch 4803, training loss: 3381.50, average training loss: 3273.49, base loss: 4605.20
[INFO 2017-06-26 22:31:44,005 main.py:47] epoch 4804, training loss: 2870.15, average training loss: 3273.45, base loss: 4605.16
[INFO 2017-06-26 22:31:44,416 main.py:47] epoch 4805, training loss: 2990.66, average training loss: 3272.90, base loss: 4604.41
[INFO 2017-06-26 22:31:44,824 main.py:47] epoch 4806, training loss: 3342.73, average training loss: 3273.11, base loss: 4605.14
[INFO 2017-06-26 22:31:45,234 main.py:47] epoch 4807, training loss: 3578.49, average training loss: 3273.21, base loss: 4605.47
[INFO 2017-06-26 22:31:45,644 main.py:47] epoch 4808, training loss: 3026.61, average training loss: 3272.91, base loss: 4604.87
[INFO 2017-06-26 22:31:46,053 main.py:47] epoch 4809, training loss: 3006.23, average training loss: 3272.70, base loss: 4604.56
[INFO 2017-06-26 22:31:46,459 main.py:47] epoch 4810, training loss: 2986.44, average training loss: 3272.49, base loss: 4603.99
[INFO 2017-06-26 22:31:46,864 main.py:47] epoch 4811, training loss: 3360.60, average training loss: 3272.76, base loss: 4605.07
[INFO 2017-06-26 22:31:47,262 main.py:47] epoch 4812, training loss: 3114.37, average training loss: 3272.93, base loss: 4605.67
[INFO 2017-06-26 22:31:47,671 main.py:47] epoch 4813, training loss: 6746.57, average training loss: 3276.58, base loss: 4609.61
[INFO 2017-06-26 22:31:48,078 main.py:47] epoch 4814, training loss: 3388.66, average training loss: 3276.68, base loss: 4610.31
[INFO 2017-06-26 22:31:48,485 main.py:47] epoch 4815, training loss: 2741.54, average training loss: 3276.31, base loss: 4609.23
[INFO 2017-06-26 22:31:48,890 main.py:47] epoch 4816, training loss: 3041.01, average training loss: 3276.01, base loss: 4609.23
[INFO 2017-06-26 22:31:49,291 main.py:47] epoch 4817, training loss: 3082.54, average training loss: 3276.19, base loss: 4609.78
[INFO 2017-06-26 22:31:49,698 main.py:47] epoch 4818, training loss: 3569.00, average training loss: 3276.57, base loss: 4610.54
[INFO 2017-06-26 22:31:50,103 main.py:47] epoch 4819, training loss: 3092.62, average training loss: 3276.68, base loss: 4611.16
[INFO 2017-06-26 22:31:50,513 main.py:47] epoch 4820, training loss: 2737.70, average training loss: 3276.19, base loss: 4610.20
[INFO 2017-06-26 22:31:50,921 main.py:47] epoch 4821, training loss: 3083.77, average training loss: 3276.57, base loss: 4610.96
[INFO 2017-06-26 22:31:51,329 main.py:47] epoch 4822, training loss: 2942.73, average training loss: 3276.27, base loss: 4610.57
[INFO 2017-06-26 22:31:51,733 main.py:47] epoch 4823, training loss: 3178.02, average training loss: 3276.16, base loss: 4610.85
[INFO 2017-06-26 22:31:52,138 main.py:47] epoch 4824, training loss: 3260.16, average training loss: 3272.90, base loss: 4607.07
[INFO 2017-06-26 22:31:52,541 main.py:47] epoch 4825, training loss: 3354.52, average training loss: 3273.35, base loss: 4607.84
[INFO 2017-06-26 22:31:52,941 main.py:47] epoch 4826, training loss: 2595.30, average training loss: 3272.89, base loss: 4606.96
[INFO 2017-06-26 22:31:53,344 main.py:47] epoch 4827, training loss: 3344.50, average training loss: 3273.16, base loss: 4607.67
[INFO 2017-06-26 22:31:53,751 main.py:47] epoch 4828, training loss: 3394.64, average training loss: 3273.18, base loss: 4607.48
[INFO 2017-06-26 22:31:54,153 main.py:47] epoch 4829, training loss: 2787.53, average training loss: 3272.80, base loss: 4607.12
[INFO 2017-06-26 22:31:54,557 main.py:47] epoch 4830, training loss: 2732.37, average training loss: 3272.10, base loss: 4606.31
[INFO 2017-06-26 22:31:54,962 main.py:47] epoch 4831, training loss: 2979.57, average training loss: 3272.05, base loss: 4606.56
[INFO 2017-06-26 22:31:55,370 main.py:47] epoch 4832, training loss: 3039.25, average training loss: 3271.56, base loss: 4605.53
[INFO 2017-06-26 22:31:55,781 main.py:47] epoch 4833, training loss: 3270.01, average training loss: 3271.86, base loss: 4606.20
[INFO 2017-06-26 22:31:56,183 main.py:47] epoch 4834, training loss: 2810.06, average training loss: 3271.67, base loss: 4606.17
[INFO 2017-06-26 22:31:56,585 main.py:47] epoch 4835, training loss: 3268.23, average training loss: 3271.84, base loss: 4606.82
[INFO 2017-06-26 22:31:56,989 main.py:47] epoch 4836, training loss: 6774.00, average training loss: 3275.25, base loss: 4610.25
[INFO 2017-06-26 22:31:57,391 main.py:47] epoch 4837, training loss: 2854.07, average training loss: 3274.96, base loss: 4610.24
[INFO 2017-06-26 22:31:57,795 main.py:47] epoch 4838, training loss: 2808.09, average training loss: 3274.70, base loss: 4609.99
[INFO 2017-06-26 22:31:58,206 main.py:47] epoch 4839, training loss: 2883.37, average training loss: 3274.61, base loss: 4609.62
[INFO 2017-06-26 22:31:58,610 main.py:47] epoch 4840, training loss: 3034.13, average training loss: 3274.28, base loss: 4608.56
[INFO 2017-06-26 22:31:59,012 main.py:47] epoch 4841, training loss: 3266.89, average training loss: 3274.04, base loss: 4608.23
[INFO 2017-06-26 22:31:59,421 main.py:47] epoch 4842, training loss: 2893.96, average training loss: 3273.83, base loss: 4608.01
[INFO 2017-06-26 22:31:59,823 main.py:47] epoch 4843, training loss: 2973.44, average training loss: 3273.22, base loss: 4606.64
[INFO 2017-06-26 22:32:00,230 main.py:47] epoch 4844, training loss: 3098.61, average training loss: 3273.32, base loss: 4607.12
[INFO 2017-06-26 22:32:00,639 main.py:47] epoch 4845, training loss: 3364.89, average training loss: 3271.00, base loss: 4605.05
[INFO 2017-06-26 22:32:01,043 main.py:47] epoch 4846, training loss: 2604.60, average training loss: 3270.70, base loss: 4604.33
[INFO 2017-06-26 22:32:01,442 main.py:47] epoch 4847, training loss: 2929.20, average training loss: 3270.42, base loss: 4604.15
[INFO 2017-06-26 22:32:01,847 main.py:47] epoch 4848, training loss: 3085.24, average training loss: 3267.04, base loss: 4600.27
[INFO 2017-06-26 22:32:02,248 main.py:47] epoch 4849, training loss: 2665.08, average training loss: 3266.59, base loss: 4599.22
[INFO 2017-06-26 22:32:02,653 main.py:47] epoch 4850, training loss: 3011.02, average training loss: 3265.98, base loss: 4598.21
[INFO 2017-06-26 22:32:03,059 main.py:47] epoch 4851, training loss: 2986.19, average training loss: 3265.64, base loss: 4597.70
[INFO 2017-06-26 22:32:03,465 main.py:47] epoch 4852, training loss: 3047.86, average training loss: 3265.41, base loss: 4597.62
[INFO 2017-06-26 22:32:03,871 main.py:47] epoch 4853, training loss: 2876.97, average training loss: 3265.29, base loss: 4597.75
[INFO 2017-06-26 22:32:04,274 main.py:47] epoch 4854, training loss: 3255.44, average training loss: 3265.47, base loss: 4598.10
[INFO 2017-06-26 22:32:04,681 main.py:47] epoch 4855, training loss: 2823.28, average training loss: 3265.60, base loss: 4598.27
[INFO 2017-06-26 22:32:05,083 main.py:47] epoch 4856, training loss: 2996.79, average training loss: 3265.76, base loss: 4598.26
[INFO 2017-06-26 22:32:05,487 main.py:47] epoch 4857, training loss: 3183.13, average training loss: 3265.91, base loss: 4599.11
[INFO 2017-06-26 22:32:05,891 main.py:47] epoch 4858, training loss: 3488.69, average training loss: 3266.46, base loss: 4600.76
[INFO 2017-06-26 22:32:06,293 main.py:47] epoch 4859, training loss: 2757.71, average training loss: 3266.39, base loss: 4600.83
[INFO 2017-06-26 22:32:06,699 main.py:47] epoch 4860, training loss: 3013.55, average training loss: 3265.97, base loss: 4600.72
[INFO 2017-06-26 22:32:07,100 main.py:47] epoch 4861, training loss: 3520.53, average training loss: 3266.38, base loss: 4601.79
[INFO 2017-06-26 22:32:07,506 main.py:47] epoch 4862, training loss: 2947.67, average training loss: 3266.06, base loss: 4601.51
[INFO 2017-06-26 22:32:07,902 main.py:47] epoch 4863, training loss: 2795.76, average training loss: 3266.10, base loss: 4601.40
[INFO 2017-06-26 22:32:08,306 main.py:47] epoch 4864, training loss: 3202.62, average training loss: 3266.19, base loss: 4601.95
[INFO 2017-06-26 22:32:08,706 main.py:47] epoch 4865, training loss: 2908.72, average training loss: 3265.79, base loss: 4601.32
[INFO 2017-06-26 22:32:09,110 main.py:47] epoch 4866, training loss: 3008.47, average training loss: 3262.52, base loss: 4597.69
[INFO 2017-06-26 22:32:09,513 main.py:47] epoch 4867, training loss: 3111.30, average training loss: 3262.49, base loss: 4597.58
[INFO 2017-06-26 22:32:09,921 main.py:47] epoch 4868, training loss: 2985.51, average training loss: 3262.32, base loss: 4597.47
[INFO 2017-06-26 22:32:10,329 main.py:47] epoch 4869, training loss: 3026.28, average training loss: 3262.13, base loss: 4596.77
[INFO 2017-06-26 22:32:10,733 main.py:47] epoch 4870, training loss: 3106.44, average training loss: 3262.10, base loss: 4597.14
[INFO 2017-06-26 22:32:11,134 main.py:47] epoch 4871, training loss: 2836.92, average training loss: 3261.14, base loss: 4595.70
[INFO 2017-06-26 22:32:11,533 main.py:47] epoch 4872, training loss: 3325.83, average training loss: 3261.79, base loss: 4597.43
[INFO 2017-06-26 22:32:11,936 main.py:47] epoch 4873, training loss: 5647.56, average training loss: 3264.30, base loss: 4601.36
[INFO 2017-06-26 22:32:12,341 main.py:47] epoch 4874, training loss: 2929.54, average training loss: 3263.81, base loss: 4599.94
[INFO 2017-06-26 22:32:12,742 main.py:47] epoch 4875, training loss: 3114.08, average training loss: 3263.72, base loss: 4599.78
[INFO 2017-06-26 22:32:13,147 main.py:47] epoch 4876, training loss: 3044.44, average training loss: 3263.27, base loss: 4599.13
[INFO 2017-06-26 22:32:13,546 main.py:47] epoch 4877, training loss: 2651.99, average training loss: 3263.08, base loss: 4598.75
[INFO 2017-06-26 22:32:13,949 main.py:47] epoch 4878, training loss: 3083.08, average training loss: 3263.44, base loss: 4599.54
[INFO 2017-06-26 22:32:14,356 main.py:47] epoch 4879, training loss: 2991.41, average training loss: 3263.27, base loss: 4599.11
[INFO 2017-06-26 22:32:14,760 main.py:47] epoch 4880, training loss: 2755.47, average training loss: 3262.70, base loss: 4598.08
[INFO 2017-06-26 22:32:15,164 main.py:47] epoch 4881, training loss: 5277.19, average training loss: 3264.86, base loss: 4600.61
[INFO 2017-06-26 22:32:15,568 main.py:47] epoch 4882, training loss: 3133.22, average training loss: 3264.83, base loss: 4600.52
[INFO 2017-06-26 22:32:15,972 main.py:47] epoch 4883, training loss: 3088.77, average training loss: 3264.93, base loss: 4601.12
[INFO 2017-06-26 22:32:16,380 main.py:47] epoch 4884, training loss: 3196.66, average training loss: 3264.61, base loss: 4600.63
[INFO 2017-06-26 22:32:16,779 main.py:47] epoch 4885, training loss: 3003.59, average training loss: 3264.80, base loss: 4601.23
[INFO 2017-06-26 22:32:17,181 main.py:47] epoch 4886, training loss: 2935.30, average training loss: 3264.57, base loss: 4601.11
[INFO 2017-06-26 22:32:17,584 main.py:47] epoch 4887, training loss: 3217.33, average training loss: 3264.69, base loss: 4601.51
[INFO 2017-06-26 22:32:17,990 main.py:47] epoch 4888, training loss: 3365.01, average training loss: 3265.24, base loss: 4602.67
[INFO 2017-06-26 22:32:18,389 main.py:47] epoch 4889, training loss: 3167.14, average training loss: 3265.00, base loss: 4601.95
[INFO 2017-06-26 22:32:18,797 main.py:47] epoch 4890, training loss: 3060.41, average training loss: 3262.12, base loss: 4598.33
[INFO 2017-06-26 22:32:19,199 main.py:47] epoch 4891, training loss: 3004.11, average training loss: 3262.12, base loss: 4598.59
[INFO 2017-06-26 22:32:19,610 main.py:47] epoch 4892, training loss: 3029.89, average training loss: 3261.99, base loss: 4598.40
[INFO 2017-06-26 22:32:20,012 main.py:47] epoch 4893, training loss: 3165.60, average training loss: 3262.27, base loss: 4599.03
[INFO 2017-06-26 22:32:20,416 main.py:47] epoch 4894, training loss: 6387.47, average training loss: 3265.72, base loss: 4602.55
[INFO 2017-06-26 22:32:20,823 main.py:47] epoch 4895, training loss: 2857.34, average training loss: 3265.48, base loss: 4601.99
[INFO 2017-06-26 22:32:21,228 main.py:47] epoch 4896, training loss: 3016.76, average training loss: 3265.00, base loss: 4601.15
[INFO 2017-06-26 22:32:21,637 main.py:47] epoch 4897, training loss: 2812.38, average training loss: 3264.65, base loss: 4600.48
[INFO 2017-06-26 22:32:22,047 main.py:47] epoch 4898, training loss: 2840.98, average training loss: 3264.68, base loss: 4600.55
[INFO 2017-06-26 22:32:22,451 main.py:47] epoch 4899, training loss: 3152.80, average training loss: 3265.03, base loss: 4601.20
[INFO 2017-06-26 22:32:22,451 main.py:49] epoch 4899, testing
[INFO 2017-06-26 22:32:24,112 main.py:102] average testing loss: 3391.30, base loss: 4740.12
[INFO 2017-06-26 22:32:24,112 main.py:103] improve_loss: 1348.82, improve_percent: 0.28
[INFO 2017-06-26 22:32:24,113 main.py:73] current best improved percent: 0.33
[INFO 2017-06-26 22:32:24,516 main.py:47] epoch 4900, training loss: 5324.87, average training loss: 3267.25, base loss: 4604.55
[INFO 2017-06-26 22:32:24,920 main.py:47] epoch 4901, training loss: 3010.90, average training loss: 3267.25, base loss: 4604.77
[INFO 2017-06-26 22:32:25,323 main.py:47] epoch 4902, training loss: 3148.21, average training loss: 3266.87, base loss: 4604.01
[INFO 2017-06-26 22:32:25,727 main.py:47] epoch 4903, training loss: 3119.43, average training loss: 3266.99, base loss: 4604.85
[INFO 2017-06-26 22:32:26,131 main.py:47] epoch 4904, training loss: 6611.08, average training loss: 3270.38, base loss: 4608.25
[INFO 2017-06-26 22:32:26,535 main.py:47] epoch 4905, training loss: 3199.35, average training loss: 3270.59, base loss: 4608.75
[INFO 2017-06-26 22:32:26,939 main.py:47] epoch 4906, training loss: 3296.91, average training loss: 3270.50, base loss: 4608.70
[INFO 2017-06-26 22:32:27,343 main.py:47] epoch 4907, training loss: 2919.82, average training loss: 3270.50, base loss: 4608.75
[INFO 2017-06-26 22:32:27,746 main.py:47] epoch 4908, training loss: 3197.51, average training loss: 3270.57, base loss: 4608.97
[INFO 2017-06-26 22:32:28,150 main.py:47] epoch 4909, training loss: 2756.53, average training loss: 3270.18, base loss: 4608.19
[INFO 2017-06-26 22:32:28,554 main.py:47] epoch 4910, training loss: 3140.33, average training loss: 3270.43, base loss: 4609.01
[INFO 2017-06-26 22:32:28,958 main.py:47] epoch 4911, training loss: 3178.00, average training loss: 3270.89, base loss: 4609.96
[INFO 2017-06-26 22:32:29,362 main.py:47] epoch 4912, training loss: 2862.77, average training loss: 3270.98, base loss: 4610.15
[INFO 2017-06-26 22:32:29,765 main.py:47] epoch 4913, training loss: 3089.15, average training loss: 3271.09, base loss: 4610.69
[INFO 2017-06-26 22:32:30,169 main.py:47] epoch 4914, training loss: 3038.12, average training loss: 3270.89, base loss: 4610.46
[INFO 2017-06-26 22:32:30,578 main.py:47] epoch 4915, training loss: 3263.42, average training loss: 3271.26, base loss: 4611.52
[INFO 2017-06-26 22:32:30,980 main.py:47] epoch 4916, training loss: 2767.38, average training loss: 3270.96, base loss: 4611.26
[INFO 2017-06-26 22:32:31,384 main.py:47] epoch 4917, training loss: 2714.49, average training loss: 3270.12, base loss: 4609.77
[INFO 2017-06-26 22:32:31,790 main.py:47] epoch 4918, training loss: 2836.65, average training loss: 3269.45, base loss: 4608.60
[INFO 2017-06-26 22:32:32,194 main.py:47] epoch 4919, training loss: 3077.58, average training loss: 3269.62, base loss: 4609.06
[INFO 2017-06-26 22:32:32,602 main.py:47] epoch 4920, training loss: 3075.16, average training loss: 3269.62, base loss: 4608.77
[INFO 2017-06-26 22:32:33,009 main.py:47] epoch 4921, training loss: 2779.06, average training loss: 3269.49, base loss: 4608.58
[INFO 2017-06-26 22:32:33,413 main.py:47] epoch 4922, training loss: 2941.01, average training loss: 3268.97, base loss: 4607.85
[INFO 2017-06-26 22:32:33,817 main.py:47] epoch 4923, training loss: 3463.88, average training loss: 3269.37, base loss: 4608.97
[INFO 2017-06-26 22:32:34,220 main.py:47] epoch 4924, training loss: 2625.56, average training loss: 3268.87, base loss: 4608.03
[INFO 2017-06-26 22:32:34,624 main.py:47] epoch 4925, training loss: 2997.71, average training loss: 3265.70, base loss: 4604.24
[INFO 2017-06-26 22:32:35,028 main.py:47] epoch 4926, training loss: 3201.58, average training loss: 3265.23, base loss: 4603.63
[INFO 2017-06-26 22:32:35,433 main.py:47] epoch 4927, training loss: 3412.31, average training loss: 3265.51, base loss: 4604.55
[INFO 2017-06-26 22:32:35,843 main.py:47] epoch 4928, training loss: 3414.97, average training loss: 3265.64, base loss: 4604.71
[INFO 2017-06-26 22:32:36,251 main.py:47] epoch 4929, training loss: 3010.23, average training loss: 3265.51, base loss: 4604.96
[INFO 2017-06-26 22:32:36,655 main.py:47] epoch 4930, training loss: 2887.90, average training loss: 3265.30, base loss: 4604.42
[INFO 2017-06-26 22:32:37,059 main.py:47] epoch 4931, training loss: 3227.38, average training loss: 3265.31, base loss: 4604.70
[INFO 2017-06-26 22:32:37,463 main.py:47] epoch 4932, training loss: 2666.86, average training loss: 3264.78, base loss: 4603.96
[INFO 2017-06-26 22:32:37,870 main.py:47] epoch 4933, training loss: 3029.77, average training loss: 3264.81, base loss: 4604.41
[INFO 2017-06-26 22:32:38,278 main.py:47] epoch 4934, training loss: 2942.90, average training loss: 3264.72, base loss: 4604.20
[INFO 2017-06-26 22:32:38,682 main.py:47] epoch 4935, training loss: 3026.37, average training loss: 3264.24, base loss: 4603.50
[INFO 2017-06-26 22:32:39,092 main.py:47] epoch 4936, training loss: 3095.36, average training loss: 3264.06, base loss: 4603.29
[INFO 2017-06-26 22:32:39,496 main.py:47] epoch 4937, training loss: 2837.20, average training loss: 3263.69, base loss: 4602.68
[INFO 2017-06-26 22:32:39,905 main.py:47] epoch 4938, training loss: 3216.10, average training loss: 3263.60, base loss: 4602.94
[INFO 2017-06-26 22:32:40,315 main.py:47] epoch 4939, training loss: 2868.32, average training loss: 3263.43, base loss: 4602.98
[INFO 2017-06-26 22:32:40,720 main.py:47] epoch 4940, training loss: 2966.71, average training loss: 3262.99, base loss: 4602.41
[INFO 2017-06-26 22:32:41,129 main.py:47] epoch 4941, training loss: 2789.05, average training loss: 3262.61, base loss: 4601.81
[INFO 2017-06-26 22:32:41,538 main.py:47] epoch 4942, training loss: 2818.21, average training loss: 3262.26, base loss: 4601.30
[INFO 2017-06-26 22:32:41,949 main.py:47] epoch 4943, training loss: 3121.00, average training loss: 3262.47, base loss: 4601.81
[INFO 2017-06-26 22:32:42,353 main.py:47] epoch 4944, training loss: 6620.21, average training loss: 3263.76, base loss: 4602.65
[INFO 2017-06-26 22:32:42,758 main.py:47] epoch 4945, training loss: 3338.20, average training loss: 3264.19, base loss: 4603.68
[INFO 2017-06-26 22:32:43,161 main.py:47] epoch 4946, training loss: 2805.25, average training loss: 3263.73, base loss: 4602.59
[INFO 2017-06-26 22:32:43,564 main.py:47] epoch 4947, training loss: 5676.33, average training loss: 3266.02, base loss: 4605.75
[INFO 2017-06-26 22:32:43,974 main.py:47] epoch 4948, training loss: 3348.00, average training loss: 3266.33, base loss: 4607.29
[INFO 2017-06-26 22:32:44,378 main.py:47] epoch 4949, training loss: 2941.73, average training loss: 3266.06, base loss: 4607.34
[INFO 2017-06-26 22:32:44,783 main.py:47] epoch 4950, training loss: 3218.78, average training loss: 3266.63, base loss: 4608.90
[INFO 2017-06-26 22:32:45,187 main.py:47] epoch 4951, training loss: 2954.86, average training loss: 3263.96, base loss: 4606.32
[INFO 2017-06-26 22:32:45,591 main.py:47] epoch 4952, training loss: 3331.63, average training loss: 3264.13, base loss: 4607.06
[INFO 2017-06-26 22:32:45,995 main.py:47] epoch 4953, training loss: 3062.41, average training loss: 3264.29, base loss: 4607.50
[INFO 2017-06-26 22:32:46,398 main.py:47] epoch 4954, training loss: 3164.04, average training loss: 3264.34, base loss: 4607.99
[INFO 2017-06-26 22:32:46,805 main.py:47] epoch 4955, training loss: 2894.74, average training loss: 3264.54, base loss: 4608.23
[INFO 2017-06-26 22:32:47,209 main.py:47] epoch 4956, training loss: 3154.54, average training loss: 3264.62, base loss: 4608.35
[INFO 2017-06-26 22:32:47,611 main.py:47] epoch 4957, training loss: 3089.55, average training loss: 3264.57, base loss: 4608.54
[INFO 2017-06-26 22:32:48,022 main.py:47] epoch 4958, training loss: 3088.53, average training loss: 3264.20, base loss: 4608.51
[INFO 2017-06-26 22:32:48,426 main.py:47] epoch 4959, training loss: 3130.98, average training loss: 3264.40, base loss: 4609.44
[INFO 2017-06-26 22:32:48,834 main.py:47] epoch 4960, training loss: 3033.89, average training loss: 3264.13, base loss: 4608.81
[INFO 2017-06-26 22:32:49,241 main.py:47] epoch 4961, training loss: 2956.89, average training loss: 3263.93, base loss: 4608.63
[INFO 2017-06-26 22:32:49,646 main.py:47] epoch 4962, training loss: 2968.82, average training loss: 3263.99, base loss: 4608.82
[INFO 2017-06-26 22:32:50,050 main.py:47] epoch 4963, training loss: 3405.96, average training loss: 3264.45, base loss: 4610.11
[INFO 2017-06-26 22:32:50,453 main.py:47] epoch 4964, training loss: 2906.60, average training loss: 3264.10, base loss: 4609.41
[INFO 2017-06-26 22:32:50,857 main.py:47] epoch 4965, training loss: 3084.22, average training loss: 3261.04, base loss: 4605.93
[INFO 2017-06-26 22:32:51,261 main.py:47] epoch 4966, training loss: 2850.43, average training loss: 3260.85, base loss: 4605.73
[INFO 2017-06-26 22:32:51,663 main.py:47] epoch 4967, training loss: 2658.38, average training loss: 3260.51, base loss: 4605.30
[INFO 2017-06-26 22:32:52,067 main.py:47] epoch 4968, training loss: 2938.22, average training loss: 3260.67, base loss: 4605.76
[INFO 2017-06-26 22:32:52,476 main.py:47] epoch 4969, training loss: 2706.09, average training loss: 3260.52, base loss: 4605.49
[INFO 2017-06-26 22:32:52,887 main.py:47] epoch 4970, training loss: 2909.38, average training loss: 3260.24, base loss: 4605.52
[INFO 2017-06-26 22:32:53,297 main.py:47] epoch 4971, training loss: 3100.84, average training loss: 3260.16, base loss: 4605.30
[INFO 2017-06-26 22:32:53,701 main.py:47] epoch 4972, training loss: 3191.77, average training loss: 3260.34, base loss: 4605.73
[INFO 2017-06-26 22:32:54,111 main.py:47] epoch 4973, training loss: 2995.07, average training loss: 3260.54, base loss: 4606.46
[INFO 2017-06-26 22:32:54,515 main.py:47] epoch 4974, training loss: 3340.72, average training loss: 3260.74, base loss: 4606.72
[INFO 2017-06-26 22:32:54,919 main.py:47] epoch 4975, training loss: 2913.14, average training loss: 3260.13, base loss: 4605.30
[INFO 2017-06-26 22:32:55,322 main.py:47] epoch 4976, training loss: 3143.79, average training loss: 3260.47, base loss: 4606.24
[INFO 2017-06-26 22:32:55,725 main.py:47] epoch 4977, training loss: 3079.73, average training loss: 3260.56, base loss: 4606.70
[INFO 2017-06-26 22:32:56,128 main.py:47] epoch 4978, training loss: 2822.97, average training loss: 3260.03, base loss: 4605.95
[INFO 2017-06-26 22:32:56,533 main.py:47] epoch 4979, training loss: 2978.07, average training loss: 3256.66, base loss: 4602.23
[INFO 2017-06-26 22:32:56,937 main.py:47] epoch 4980, training loss: 3166.77, average training loss: 3256.73, base loss: 4602.58
[INFO 2017-06-26 22:32:57,346 main.py:47] epoch 4981, training loss: 3074.09, average training loss: 3256.28, base loss: 4601.99
[INFO 2017-06-26 22:32:57,750 main.py:47] epoch 4982, training loss: 2783.99, average training loss: 3255.82, base loss: 4601.74
[INFO 2017-06-26 22:32:58,153 main.py:47] epoch 4983, training loss: 3292.55, average training loss: 3256.03, base loss: 4602.65
[INFO 2017-06-26 22:32:58,557 main.py:47] epoch 4984, training loss: 3109.83, average training loss: 3256.18, base loss: 4603.66
[INFO 2017-06-26 22:32:58,961 main.py:47] epoch 4985, training loss: 2703.65, average training loss: 3255.57, base loss: 4602.88
[INFO 2017-06-26 22:32:59,370 main.py:47] epoch 4986, training loss: 3009.22, average training loss: 3255.27, base loss: 4602.94
[INFO 2017-06-26 22:32:59,780 main.py:47] epoch 4987, training loss: 2974.05, average training loss: 3254.42, base loss: 4602.21
[INFO 2017-06-26 22:33:00,184 main.py:47] epoch 4988, training loss: 5511.87, average training loss: 3256.42, base loss: 4605.32
[INFO 2017-06-26 22:33:00,587 main.py:47] epoch 4989, training loss: 2868.05, average training loss: 3256.14, base loss: 4605.40
[INFO 2017-06-26 22:33:00,991 main.py:47] epoch 4990, training loss: 3116.57, average training loss: 3256.03, base loss: 4606.40
[INFO 2017-06-26 22:33:01,395 main.py:47] epoch 4991, training loss: 3055.04, average training loss: 3255.65, base loss: 4606.20
[INFO 2017-06-26 22:33:01,798 main.py:47] epoch 4992, training loss: 2805.60, average training loss: 3255.28, base loss: 4606.16
[INFO 2017-06-26 22:33:02,202 main.py:47] epoch 4993, training loss: 3142.36, average training loss: 3255.23, base loss: 4606.50
[INFO 2017-06-26 22:33:02,605 main.py:47] epoch 4994, training loss: 2926.69, average training loss: 3254.36, base loss: 4605.57
[INFO 2017-06-26 22:33:03,012 main.py:47] epoch 4995, training loss: 2840.53, average training loss: 3253.51, base loss: 4604.24
[INFO 2017-06-26 22:33:03,415 main.py:47] epoch 4996, training loss: 3021.96, average training loss: 3253.23, base loss: 4604.18
[INFO 2017-06-26 22:33:03,826 main.py:47] epoch 4997, training loss: 3084.65, average training loss: 3252.95, base loss: 4604.14
[INFO 2017-06-26 22:33:04,229 main.py:47] epoch 4998, training loss: 2945.56, average training loss: 3252.69, base loss: 4604.51
[INFO 2017-06-26 22:33:04,633 main.py:47] epoch 4999, training loss: 6498.86, average training loss: 3255.56, base loss: 4606.91
[INFO 2017-06-26 22:33:04,633 main.py:49] epoch 4999, testing
[INFO 2017-06-26 22:33:06,287 main.py:102] average testing loss: 3011.97, base loss: 4329.96
[INFO 2017-06-26 22:33:06,287 main.py:103] improve_loss: 1317.99, improve_percent: 0.30
[INFO 2017-06-26 22:33:06,288 main.py:73] current best improved percent: 0.33
[INFO 2017-06-26 22:33:06,690 main.py:47] epoch 5000, training loss: 2825.13, average training loss: 3254.81, base loss: 4606.20
[INFO 2017-06-26 22:33:07,094 main.py:47] epoch 5001, training loss: 2913.37, average training loss: 3254.72, base loss: 4606.73
[INFO 2017-06-26 22:33:07,498 main.py:47] epoch 5002, training loss: 3077.10, average training loss: 3254.52, base loss: 4607.08
[INFO 2017-06-26 22:33:07,901 main.py:47] epoch 5003, training loss: 3126.92, average training loss: 3254.21, base loss: 4607.21
[INFO 2017-06-26 22:33:08,305 main.py:47] epoch 5004, training loss: 2821.83, average training loss: 3253.57, base loss: 4606.42
[INFO 2017-06-26 22:33:08,709 main.py:47] epoch 5005, training loss: 2774.24, average training loss: 3253.15, base loss: 4606.26
[INFO 2017-06-26 22:33:09,112 main.py:47] epoch 5006, training loss: 3120.80, average training loss: 3252.87, base loss: 4606.48
[INFO 2017-06-26 22:33:09,515 main.py:47] epoch 5007, training loss: 3167.03, average training loss: 3252.99, base loss: 4607.26
[INFO 2017-06-26 22:33:09,925 main.py:47] epoch 5008, training loss: 3121.82, average training loss: 3249.26, base loss: 4603.76
[INFO 2017-06-26 22:33:10,328 main.py:47] epoch 5009, training loss: 3307.88, average training loss: 3249.75, base loss: 4605.11
[INFO 2017-06-26 22:33:10,737 main.py:47] epoch 5010, training loss: 3113.28, average training loss: 3249.38, base loss: 4604.94
[INFO 2017-06-26 22:33:11,142 main.py:47] epoch 5011, training loss: 2989.15, average training loss: 3249.05, base loss: 4604.38
[INFO 2017-06-26 22:33:11,550 main.py:47] epoch 5012, training loss: 2943.67, average training loss: 3248.53, base loss: 4604.04
[INFO 2017-06-26 22:33:11,956 main.py:47] epoch 5013, training loss: 3151.79, average training loss: 3248.28, base loss: 4604.19
