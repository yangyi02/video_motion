[INFO 2017-06-30 00:54:03,985 main.py:168] Namespace(batch_size=32, display=False, flow_dir='flow', flow_video_dir='flow-video', flow_video_fps=1, image_size=64, init_model_path='', learning_rate=0.001, method='unsupervised', motion_range=4, num_channel=3, num_inputs=2, save_dir='./model', test=False, test_dir='/home/yi/Downloads/mpii-test-64', test_epoch=10, test_interval=100, test_video=False, train=True, train_dir='/home/yi/Downloads/mpii-test-64', train_epoch=100000)
[INFO 2017-06-30 00:54:08,726 main.py:52] epoch 0, training loss: 39637.40, average training loss: 39637.40, base loss: 14910.85
[INFO 2017-06-30 00:54:11,202 main.py:52] epoch 1, training loss: 33234.39, average training loss: 36435.89, base loss: 14735.83
[INFO 2017-06-30 00:54:13,695 main.py:52] epoch 2, training loss: 30359.38, average training loss: 34410.39, base loss: 15000.92
[INFO 2017-06-30 00:54:16,197 main.py:52] epoch 3, training loss: 26305.19, average training loss: 32384.09, base loss: 15668.44
[INFO 2017-06-30 00:54:18,681 main.py:52] epoch 4, training loss: 23287.04, average training loss: 30564.68, base loss: 15507.59
[INFO 2017-06-30 00:54:21,177 main.py:52] epoch 5, training loss: 21184.70, average training loss: 29001.35, base loss: 15630.97
[INFO 2017-06-30 00:54:23,622 main.py:52] epoch 6, training loss: 19892.39, average training loss: 27700.07, base loss: 15689.69
[INFO 2017-06-30 00:54:26,097 main.py:52] epoch 7, training loss: 17614.56, average training loss: 26439.38, base loss: 15561.16
[INFO 2017-06-30 00:54:28,622 main.py:52] epoch 8, training loss: 14276.68, average training loss: 25087.97, base loss: 15429.81
[INFO 2017-06-30 00:54:31,129 main.py:52] epoch 9, training loss: 16447.13, average training loss: 24223.89, base loss: 15539.13
[INFO 2017-06-30 00:54:33,644 main.py:52] epoch 10, training loss: 14741.52, average training loss: 23361.85, base loss: 15645.29
[INFO 2017-06-30 00:54:36,137 main.py:52] epoch 11, training loss: 13835.44, average training loss: 22567.99, base loss: 15550.98
[INFO 2017-06-30 00:54:38,884 main.py:52] epoch 12, training loss: 13236.10, average training loss: 21850.15, base loss: 15611.34
[INFO 2017-06-30 00:54:41,887 main.py:52] epoch 13, training loss: 13412.91, average training loss: 21247.49, base loss: 15691.00
[INFO 2017-06-30 00:54:44,875 main.py:52] epoch 14, training loss: 12950.49, average training loss: 20694.35, base loss: 15707.70
[INFO 2017-06-30 00:54:47,887 main.py:52] epoch 15, training loss: 12430.87, average training loss: 20177.89, base loss: 15757.24
[INFO 2017-06-30 00:54:50,919 main.py:52] epoch 16, training loss: 12499.48, average training loss: 19726.22, base loss: 15815.64
[INFO 2017-06-30 00:54:54,069 main.py:52] epoch 17, training loss: 12185.81, average training loss: 19307.30, base loss: 15849.18
[INFO 2017-06-30 00:54:57,234 main.py:52] epoch 18, training loss: 10947.84, average training loss: 18867.33, base loss: 15827.24
[INFO 2017-06-30 00:55:00,442 main.py:52] epoch 19, training loss: 11881.74, average training loss: 18518.05, base loss: 15867.66
[INFO 2017-06-30 00:55:03,613 main.py:52] epoch 20, training loss: 12449.54, average training loss: 18229.08, base loss: 15942.65
[INFO 2017-06-30 00:55:06,743 main.py:52] epoch 21, training loss: 11888.46, average training loss: 17940.87, base loss: 15979.26
[INFO 2017-06-30 00:55:09,902 main.py:52] epoch 22, training loss: 11300.37, average training loss: 17652.15, base loss: 15982.28
[INFO 2017-06-30 00:55:13,039 main.py:52] epoch 23, training loss: 11581.26, average training loss: 17399.20, base loss: 16025.83
[INFO 2017-06-30 00:55:16,159 main.py:52] epoch 24, training loss: 10770.08, average training loss: 17134.03, base loss: 16044.82
[INFO 2017-06-30 00:55:19,314 main.py:52] epoch 25, training loss: 11372.92, average training loss: 16912.45, base loss: 16075.82
[INFO 2017-06-30 00:55:22,477 main.py:52] epoch 26, training loss: 10702.42, average training loss: 16682.45, base loss: 16066.00
[INFO 2017-06-30 00:55:25,622 main.py:52] epoch 27, training loss: 10452.58, average training loss: 16459.95, base loss: 16027.18
[INFO 2017-06-30 00:55:28,767 main.py:52] epoch 28, training loss: 11086.34, average training loss: 16274.66, base loss: 16025.85
[INFO 2017-06-30 00:55:31,897 main.py:52] epoch 29, training loss: 10439.16, average training loss: 16080.14, base loss: 16036.05
[INFO 2017-06-30 00:55:35,040 main.py:52] epoch 30, training loss: 11724.38, average training loss: 15939.63, base loss: 16070.89
[INFO 2017-06-30 00:55:38,178 main.py:52] epoch 31, training loss: 10043.65, average training loss: 15755.38, base loss: 16054.47
[INFO 2017-06-30 00:55:41,318 main.py:52] epoch 32, training loss: 10453.04, average training loss: 15594.71, base loss: 16030.38
[INFO 2017-06-30 00:55:44,491 main.py:52] epoch 33, training loss: 10887.65, average training loss: 15456.26, base loss: 16026.59
[INFO 2017-06-30 00:55:47,626 main.py:52] epoch 34, training loss: 9860.34, average training loss: 15296.38, base loss: 15991.41
[INFO 2017-06-30 00:55:50,809 main.py:52] epoch 35, training loss: 9583.44, average training loss: 15137.69, base loss: 15950.46
[INFO 2017-06-30 00:55:53,915 main.py:52] epoch 36, training loss: 12309.96, average training loss: 15061.26, base loss: 16001.74
[INFO 2017-06-30 00:55:57,079 main.py:52] epoch 37, training loss: 11317.36, average training loss: 14962.74, base loss: 16015.06
[INFO 2017-06-30 00:56:00,215 main.py:52] epoch 38, training loss: 11443.71, average training loss: 14872.51, base loss: 16036.42
[INFO 2017-06-30 00:56:03,382 main.py:52] epoch 39, training loss: 11303.01, average training loss: 14783.27, base loss: 16062.90
[INFO 2017-06-30 00:56:06,578 main.py:52] epoch 40, training loss: 10557.55, average training loss: 14680.20, base loss: 16066.18
[INFO 2017-06-30 00:56:09,787 main.py:52] epoch 41, training loss: 11347.28, average training loss: 14600.85, base loss: 16092.95
[INFO 2017-06-30 00:56:12,986 main.py:52] epoch 42, training loss: 10548.80, average training loss: 14506.61, base loss: 16091.79
[INFO 2017-06-30 00:56:16,157 main.py:52] epoch 43, training loss: 10119.29, average training loss: 14406.90, base loss: 16072.59
[INFO 2017-06-30 00:56:19,334 main.py:52] epoch 44, training loss: 10777.08, average training loss: 14326.24, base loss: 16077.21
[INFO 2017-06-30 00:56:22,501 main.py:52] epoch 45, training loss: 10417.22, average training loss: 14241.26, base loss: 16067.90
[INFO 2017-06-30 00:56:25,612 main.py:52] epoch 46, training loss: 10230.18, average training loss: 14155.92, base loss: 16063.39
[INFO 2017-06-30 00:56:28,791 main.py:52] epoch 47, training loss: 10528.03, average training loss: 14080.34, base loss: 16073.38
[INFO 2017-06-30 00:56:31,909 main.py:52] epoch 48, training loss: 10169.03, average training loss: 14000.51, base loss: 16067.81
[INFO 2017-06-30 00:56:35,044 main.py:52] epoch 49, training loss: 11617.21, average training loss: 13952.85, base loss: 16101.35
[INFO 2017-06-30 00:56:38,208 main.py:52] epoch 50, training loss: 10342.04, average training loss: 13882.05, base loss: 16097.36
[INFO 2017-06-30 00:56:41,377 main.py:52] epoch 51, training loss: 11728.05, average training loss: 13840.62, base loss: 16129.76
[INFO 2017-06-30 00:56:44,521 main.py:52] epoch 52, training loss: 9257.82, average training loss: 13754.16, base loss: 16085.67
[INFO 2017-06-30 00:56:47,694 main.py:52] epoch 53, training loss: 11055.26, average training loss: 13704.18, base loss: 16101.73
[INFO 2017-06-30 00:56:50,838 main.py:52] epoch 54, training loss: 9662.51, average training loss: 13630.69, base loss: 16080.54
[INFO 2017-06-30 00:56:54,007 main.py:52] epoch 55, training loss: 9420.48, average training loss: 13555.51, base loss: 16057.57
[INFO 2017-06-30 00:56:57,175 main.py:52] epoch 56, training loss: 10559.35, average training loss: 13502.95, base loss: 16070.57
[INFO 2017-06-30 00:57:00,352 main.py:52] epoch 57, training loss: 10627.07, average training loss: 13453.36, base loss: 16075.07
[INFO 2017-06-30 00:57:03,524 main.py:52] epoch 58, training loss: 11875.31, average training loss: 13426.62, base loss: 16113.91
[INFO 2017-06-30 00:57:06,675 main.py:52] epoch 59, training loss: 10032.79, average training loss: 13370.05, base loss: 16110.73
[INFO 2017-06-30 00:57:09,843 main.py:52] epoch 60, training loss: 10535.51, average training loss: 13323.58, base loss: 16121.85
[INFO 2017-06-30 00:57:13,024 main.py:52] epoch 61, training loss: 10010.96, average training loss: 13270.15, base loss: 16117.27
[INFO 2017-06-30 00:57:16,133 main.py:52] epoch 62, training loss: 10345.44, average training loss: 13223.73, base loss: 16118.26
[INFO 2017-06-30 00:57:19,314 main.py:52] epoch 63, training loss: 9436.28, average training loss: 13164.55, base loss: 16101.83
[INFO 2017-06-30 00:57:22,476 main.py:52] epoch 64, training loss: 10048.09, average training loss: 13116.61, base loss: 16095.66
[INFO 2017-06-30 00:57:25,680 main.py:52] epoch 65, training loss: 10128.88, average training loss: 13071.34, base loss: 16099.50
[INFO 2017-06-30 00:57:28,892 main.py:52] epoch 66, training loss: 9118.52, average training loss: 13012.34, base loss: 16071.16
[INFO 2017-06-30 00:57:32,076 main.py:52] epoch 67, training loss: 10374.10, average training loss: 12973.54, base loss: 16080.11
[INFO 2017-06-30 00:57:35,235 main.py:52] epoch 68, training loss: 9389.31, average training loss: 12921.60, base loss: 16073.56
[INFO 2017-06-30 00:57:38,419 main.py:52] epoch 69, training loss: 10835.77, average training loss: 12891.80, base loss: 16079.14
[INFO 2017-06-30 00:57:41,582 main.py:52] epoch 70, training loss: 9834.91, average training loss: 12848.74, base loss: 16078.19
[INFO 2017-06-30 00:57:44,728 main.py:52] epoch 71, training loss: 11676.21, average training loss: 12832.46, base loss: 16111.94
[INFO 2017-06-30 00:57:47,873 main.py:52] epoch 72, training loss: 10227.48, average training loss: 12796.77, base loss: 16118.58
[INFO 2017-06-30 00:57:51,021 main.py:52] epoch 73, training loss: 8633.62, average training loss: 12740.52, base loss: 16090.49
[INFO 2017-06-30 00:57:54,197 main.py:52] epoch 74, training loss: 9584.53, average training loss: 12698.44, base loss: 16083.24
[INFO 2017-06-30 00:57:57,381 main.py:52] epoch 75, training loss: 9539.85, average training loss: 12656.88, base loss: 16083.68
[INFO 2017-06-30 00:58:00,568 main.py:52] epoch 76, training loss: 9839.79, average training loss: 12620.29, base loss: 16092.87
[INFO 2017-06-30 00:58:03,737 main.py:52] epoch 77, training loss: 10407.09, average training loss: 12591.92, base loss: 16102.32
[INFO 2017-06-30 00:58:06,912 main.py:52] epoch 78, training loss: 10950.09, average training loss: 12571.13, base loss: 16124.12
[INFO 2017-06-30 00:58:10,080 main.py:52] epoch 79, training loss: 9908.05, average training loss: 12537.84, base loss: 16115.32
[INFO 2017-06-30 00:58:13,283 main.py:52] epoch 80, training loss: 9503.13, average training loss: 12500.38, base loss: 16111.57
[INFO 2017-06-30 00:58:16,431 main.py:52] epoch 81, training loss: 11171.70, average training loss: 12484.18, base loss: 16136.73
[INFO 2017-06-30 00:58:19,601 main.py:52] epoch 82, training loss: 10087.60, average training loss: 12455.30, base loss: 16135.85
[INFO 2017-06-30 00:58:22,778 main.py:52] epoch 83, training loss: 9239.45, average training loss: 12417.02, base loss: 16126.58
[INFO 2017-06-30 00:58:25,929 main.py:52] epoch 84, training loss: 9024.53, average training loss: 12377.11, base loss: 16114.90
[INFO 2017-06-30 00:58:29,102 main.py:52] epoch 85, training loss: 10050.83, average training loss: 12350.06, base loss: 16122.79
[INFO 2017-06-30 00:58:32,269 main.py:52] epoch 86, training loss: 9198.09, average training loss: 12313.83, base loss: 16109.20
[INFO 2017-06-30 00:58:35,407 main.py:52] epoch 87, training loss: 10530.69, average training loss: 12293.56, base loss: 16123.50
[INFO 2017-06-30 00:58:38,585 main.py:52] epoch 88, training loss: 8353.62, average training loss: 12249.29, base loss: 16102.24
[INFO 2017-06-30 00:58:41,722 main.py:52] epoch 89, training loss: 10233.29, average training loss: 12226.89, base loss: 16111.54
[INFO 2017-06-30 00:58:44,868 main.py:52] epoch 90, training loss: 10339.57, average training loss: 12206.15, base loss: 16136.81
[INFO 2017-06-30 00:58:48,005 main.py:52] epoch 91, training loss: 8547.34, average training loss: 12166.38, base loss: 16126.42
[INFO 2017-06-30 00:58:51,151 main.py:52] epoch 92, training loss: 9767.26, average training loss: 12140.59, base loss: 16119.82
[INFO 2017-06-30 00:58:54,308 main.py:52] epoch 93, training loss: 9879.08, average training loss: 12116.53, base loss: 16132.09
[INFO 2017-06-30 00:58:57,493 main.py:52] epoch 94, training loss: 9016.55, average training loss: 12083.90, base loss: 16117.41
[INFO 2017-06-30 00:59:00,659 main.py:52] epoch 95, training loss: 9574.11, average training loss: 12057.75, base loss: 16121.54
[INFO 2017-06-30 00:59:03,871 main.py:52] epoch 96, training loss: 8584.62, average training loss: 12021.95, base loss: 16105.38
[INFO 2017-06-30 00:59:07,012 main.py:52] epoch 97, training loss: 9494.54, average training loss: 11996.16, base loss: 16102.20
[INFO 2017-06-30 00:59:10,210 main.py:52] epoch 98, training loss: 9306.38, average training loss: 11968.99, base loss: 16107.14
[INFO 2017-06-30 00:59:13,369 main.py:52] epoch 99, training loss: 9440.62, average training loss: 11943.71, base loss: 16105.52
[INFO 2017-06-30 00:59:13,369 main.py:54] epoch 99, testing
[INFO 2017-06-30 00:59:26,655 main.py:97] average testing loss: 9377.26, base loss: 16404.31
[INFO 2017-06-30 00:59:26,655 main.py:98] improve_loss: 7027.05, improve_percent: 0.43
[INFO 2017-06-30 00:59:26,657 main.py:62] model save to ./model/final.pth
[INFO 2017-06-30 00:59:26,690 main.py:66] current best improved percent: 0.43
[INFO 2017-06-30 00:59:29,803 main.py:52] epoch 100, training loss: 8269.53, average training loss: 11907.33, base loss: 16090.28
[INFO 2017-06-30 00:59:32,984 main.py:52] epoch 101, training loss: 8649.80, average training loss: 11875.39, base loss: 16083.59
[INFO 2017-06-30 00:59:36,131 main.py:52] epoch 102, training loss: 8604.79, average training loss: 11843.64, base loss: 16069.75
[INFO 2017-06-30 00:59:39,293 main.py:52] epoch 103, training loss: 9160.85, average training loss: 11817.84, base loss: 16070.74
[INFO 2017-06-30 00:59:42,457 main.py:52] epoch 104, training loss: 9123.15, average training loss: 11792.18, base loss: 16072.95
[INFO 2017-06-30 00:59:45,644 main.py:52] epoch 105, training loss: 9081.46, average training loss: 11766.61, base loss: 16073.41
[INFO 2017-06-30 00:59:48,805 main.py:52] epoch 106, training loss: 9348.17, average training loss: 11744.00, base loss: 16078.80
[INFO 2017-06-30 00:59:51,971 main.py:52] epoch 107, training loss: 9066.22, average training loss: 11719.21, base loss: 16080.23
[INFO 2017-06-30 00:59:55,126 main.py:52] epoch 108, training loss: 8322.43, average training loss: 11688.05, base loss: 16071.03
[INFO 2017-06-30 00:59:58,290 main.py:52] epoch 109, training loss: 9496.01, average training loss: 11668.12, base loss: 16082.78
[INFO 2017-06-30 01:00:01,430 main.py:52] epoch 110, training loss: 7550.80, average training loss: 11631.03, base loss: 16056.55
[INFO 2017-06-30 01:00:04,585 main.py:52] epoch 111, training loss: 8976.80, average training loss: 11607.33, base loss: 16058.15
[INFO 2017-06-30 01:00:07,765 main.py:52] epoch 112, training loss: 9506.68, average training loss: 11588.74, base loss: 16053.31
[INFO 2017-06-30 01:00:10,979 main.py:52] epoch 113, training loss: 9943.22, average training loss: 11574.30, base loss: 16059.92
[INFO 2017-06-30 01:00:14,162 main.py:52] epoch 114, training loss: 9065.62, average training loss: 11552.49, base loss: 16063.13
[INFO 2017-06-30 01:00:17,327 main.py:52] epoch 115, training loss: 8478.81, average training loss: 11525.99, base loss: 16053.90
[INFO 2017-06-30 01:00:20,499 main.py:52] epoch 116, training loss: 9179.98, average training loss: 11505.94, base loss: 16049.97
[INFO 2017-06-30 01:00:23,680 main.py:52] epoch 117, training loss: 9093.01, average training loss: 11485.49, base loss: 16053.83
[INFO 2017-06-30 01:00:26,826 main.py:52] epoch 118, training loss: 9234.25, average training loss: 11466.57, base loss: 16055.37
[INFO 2017-06-30 01:00:29,966 main.py:52] epoch 119, training loss: 8337.35, average training loss: 11440.50, base loss: 16042.53
[INFO 2017-06-30 01:00:33,146 main.py:52] epoch 120, training loss: 9068.46, average training loss: 11420.89, base loss: 16042.01
[INFO 2017-06-30 01:00:36,289 main.py:52] epoch 121, training loss: 9126.11, average training loss: 11402.08, base loss: 16046.94
[INFO 2017-06-30 01:00:39,421 main.py:52] epoch 122, training loss: 8778.68, average training loss: 11380.75, base loss: 16043.07
[INFO 2017-06-30 01:00:42,651 main.py:52] epoch 123, training loss: 8956.34, average training loss: 11361.20, base loss: 16045.90
[INFO 2017-06-30 01:00:45,840 main.py:52] epoch 124, training loss: 10182.71, average training loss: 11351.77, base loss: 16064.87
[INFO 2017-06-30 01:00:49,015 main.py:52] epoch 125, training loss: 8217.67, average training loss: 11326.90, base loss: 16050.86
[INFO 2017-06-30 01:00:52,204 main.py:52] epoch 126, training loss: 8527.87, average training loss: 11304.86, base loss: 16046.54
[INFO 2017-06-30 01:00:55,406 main.py:52] epoch 127, training loss: 8845.00, average training loss: 11285.64, base loss: 16044.73
[INFO 2017-06-30 01:00:58,548 main.py:52] epoch 128, training loss: 7950.40, average training loss: 11259.79, base loss: 16027.81
[INFO 2017-06-30 01:01:01,668 main.py:52] epoch 129, training loss: 9229.99, average training loss: 11244.17, base loss: 16032.31
[INFO 2017-06-30 01:01:04,810 main.py:52] epoch 130, training loss: 8753.48, average training loss: 11225.16, base loss: 16032.50
[INFO 2017-06-30 01:01:07,987 main.py:52] epoch 131, training loss: 9396.90, average training loss: 11211.31, base loss: 16034.54
[INFO 2017-06-30 01:01:11,140 main.py:52] epoch 132, training loss: 9739.23, average training loss: 11200.24, base loss: 16038.63
[INFO 2017-06-30 01:01:14,312 main.py:52] epoch 133, training loss: 8738.10, average training loss: 11181.87, base loss: 16035.65
[INFO 2017-06-30 01:01:17,456 main.py:52] epoch 134, training loss: 8625.59, average training loss: 11162.93, base loss: 16024.98
[INFO 2017-06-30 01:01:20,587 main.py:52] epoch 135, training loss: 8215.63, average training loss: 11141.26, base loss: 16012.43
[INFO 2017-06-30 01:01:23,745 main.py:52] epoch 136, training loss: 8665.09, average training loss: 11123.19, base loss: 16004.22
[INFO 2017-06-30 01:01:26,894 main.py:52] epoch 137, training loss: 9054.32, average training loss: 11108.20, base loss: 16010.60
[INFO 2017-06-30 01:01:30,059 main.py:52] epoch 138, training loss: 8173.19, average training loss: 11087.08, base loss: 15998.81
[INFO 2017-06-30 01:01:33,250 main.py:52] epoch 139, training loss: 9078.07, average training loss: 11072.73, base loss: 16003.53
[INFO 2017-06-30 01:01:36,385 main.py:52] epoch 140, training loss: 9239.62, average training loss: 11059.73, base loss: 16015.06
[INFO 2017-06-30 01:01:39,541 main.py:52] epoch 141, training loss: 8599.58, average training loss: 11042.41, base loss: 16000.99
[INFO 2017-06-30 01:01:42,691 main.py:52] epoch 142, training loss: 9228.33, average training loss: 11029.72, base loss: 15999.59
[INFO 2017-06-30 01:01:45,875 main.py:52] epoch 143, training loss: 9199.15, average training loss: 11017.01, base loss: 16000.26
[INFO 2017-06-30 01:01:49,028 main.py:52] epoch 144, training loss: 9251.03, average training loss: 11004.83, base loss: 16005.20
[INFO 2017-06-30 01:01:52,219 main.py:52] epoch 145, training loss: 8688.23, average training loss: 10988.96, base loss: 16005.42
[INFO 2017-06-30 01:01:55,418 main.py:52] epoch 146, training loss: 8534.00, average training loss: 10972.26, base loss: 16000.02
[INFO 2017-06-30 01:01:58,634 main.py:52] epoch 147, training loss: 8393.26, average training loss: 10954.83, base loss: 15992.67
[INFO 2017-06-30 01:02:01,828 main.py:52] epoch 148, training loss: 8679.83, average training loss: 10939.57, base loss: 15991.84
[INFO 2017-06-30 01:02:04,991 main.py:52] epoch 149, training loss: 10005.46, average training loss: 10933.34, base loss: 16006.27
[INFO 2017-06-30 01:02:08,177 main.py:52] epoch 150, training loss: 9004.50, average training loss: 10920.57, base loss: 16010.96
[INFO 2017-06-30 01:02:11,329 main.py:52] epoch 151, training loss: 8426.19, average training loss: 10904.15, base loss: 16008.36
[INFO 2017-06-30 01:02:14,463 main.py:52] epoch 152, training loss: 8321.13, average training loss: 10887.27, base loss: 16007.35
[INFO 2017-06-30 01:02:17,670 main.py:52] epoch 153, training loss: 9103.53, average training loss: 10875.69, base loss: 16004.50
[INFO 2017-06-30 01:02:20,837 main.py:52] epoch 154, training loss: 8982.29, average training loss: 10863.47, base loss: 16010.89
[INFO 2017-06-30 01:02:24,023 main.py:52] epoch 155, training loss: 9062.02, average training loss: 10851.93, base loss: 16014.13
[INFO 2017-06-30 01:02:27,168 main.py:52] epoch 156, training loss: 8249.99, average training loss: 10835.35, base loss: 16005.85
[INFO 2017-06-30 01:02:30,321 main.py:52] epoch 157, training loss: 9184.52, average training loss: 10824.91, base loss: 16007.66
[INFO 2017-06-30 01:02:33,508 main.py:52] epoch 158, training loss: 9060.97, average training loss: 10813.81, base loss: 16008.79
[INFO 2017-06-30 01:02:36,666 main.py:52] epoch 159, training loss: 9470.69, average training loss: 10805.42, base loss: 16013.46
[INFO 2017-06-30 01:02:39,838 main.py:52] epoch 160, training loss: 10479.26, average training loss: 10803.39, base loss: 16037.91
[INFO 2017-06-30 01:02:43,018 main.py:52] epoch 161, training loss: 9131.12, average training loss: 10793.07, base loss: 16036.21
[INFO 2017-06-30 01:02:46,202 main.py:52] epoch 162, training loss: 8850.72, average training loss: 10781.15, base loss: 16031.62
[INFO 2017-06-30 01:02:49,371 main.py:52] epoch 163, training loss: 9312.48, average training loss: 10772.20, base loss: 16039.11
[INFO 2017-06-30 01:02:52,603 main.py:52] epoch 164, training loss: 8234.98, average training loss: 10756.82, base loss: 16036.16
[INFO 2017-06-30 01:02:55,769 main.py:52] epoch 165, training loss: 8654.61, average training loss: 10744.16, base loss: 16033.48
[INFO 2017-06-30 01:02:58,976 main.py:52] epoch 166, training loss: 9256.67, average training loss: 10735.25, base loss: 16043.52
[INFO 2017-06-30 01:03:02,205 main.py:52] epoch 167, training loss: 8118.27, average training loss: 10719.67, base loss: 16035.53
[INFO 2017-06-30 01:03:05,370 main.py:52] epoch 168, training loss: 9262.94, average training loss: 10711.05, base loss: 16042.72
[INFO 2017-06-30 01:03:08,549 main.py:52] epoch 169, training loss: 8384.42, average training loss: 10697.37, base loss: 16040.64
[INFO 2017-06-30 01:03:11,691 main.py:52] epoch 170, training loss: 8710.06, average training loss: 10685.74, base loss: 16036.87
[INFO 2017-06-30 01:03:14,858 main.py:52] epoch 171, training loss: 8322.89, average training loss: 10672.01, base loss: 16032.69
[INFO 2017-06-30 01:03:18,006 main.py:52] epoch 172, training loss: 8042.58, average training loss: 10656.81, base loss: 16031.32
[INFO 2017-06-30 01:03:21,150 main.py:52] epoch 173, training loss: 8056.33, average training loss: 10641.86, base loss: 16026.47
[INFO 2017-06-30 01:03:24,312 main.py:52] epoch 174, training loss: 8630.56, average training loss: 10630.37, base loss: 16033.18
[INFO 2017-06-30 01:03:27,469 main.py:52] epoch 175, training loss: 8163.07, average training loss: 10616.35, base loss: 16028.60
[INFO 2017-06-30 01:03:30,650 main.py:52] epoch 176, training loss: 9093.87, average training loss: 10607.75, base loss: 16036.21
[INFO 2017-06-30 01:03:33,805 main.py:52] epoch 177, training loss: 8308.38, average training loss: 10594.83, base loss: 16034.48
[INFO 2017-06-30 01:03:36,967 main.py:52] epoch 178, training loss: 7943.09, average training loss: 10580.02, base loss: 16030.32
[INFO 2017-06-30 01:03:40,115 main.py:52] epoch 179, training loss: 8149.40, average training loss: 10566.51, base loss: 16028.17
[INFO 2017-06-30 01:03:43,262 main.py:52] epoch 180, training loss: 7853.30, average training loss: 10551.52, base loss: 16020.63
[INFO 2017-06-30 01:03:46,460 main.py:52] epoch 181, training loss: 8813.85, average training loss: 10541.98, base loss: 16024.85
[INFO 2017-06-30 01:03:49,619 main.py:52] epoch 182, training loss: 7972.98, average training loss: 10527.94, base loss: 16018.94
[INFO 2017-06-30 01:03:52,783 main.py:52] epoch 183, training loss: 7991.52, average training loss: 10514.15, base loss: 16014.99
[INFO 2017-06-30 01:03:55,947 main.py:52] epoch 184, training loss: 8100.37, average training loss: 10501.10, base loss: 16011.08
[INFO 2017-06-30 01:03:59,068 main.py:52] epoch 185, training loss: 8773.43, average training loss: 10491.82, base loss: 16016.41
[INFO 2017-06-30 01:04:02,252 main.py:52] epoch 186, training loss: 8091.53, average training loss: 10478.98, base loss: 16014.40
[INFO 2017-06-30 01:04:05,419 main.py:52] epoch 187, training loss: 8359.95, average training loss: 10467.71, base loss: 16014.60
[INFO 2017-06-30 01:04:08,608 main.py:52] epoch 188, training loss: 8697.07, average training loss: 10458.34, base loss: 16015.51
[INFO 2017-06-30 01:04:11,794 main.py:52] epoch 189, training loss: 8238.32, average training loss: 10446.66, base loss: 16010.64
[INFO 2017-06-30 01:04:14,982 main.py:52] epoch 190, training loss: 9041.17, average training loss: 10439.30, base loss: 16019.26
[INFO 2017-06-30 01:04:18,164 main.py:52] epoch 191, training loss: 9819.54, average training loss: 10436.07, base loss: 16030.58
[INFO 2017-06-30 01:04:21,334 main.py:52] epoch 192, training loss: 8303.79, average training loss: 10425.02, base loss: 16032.28
[INFO 2017-06-30 01:04:24,547 main.py:52] epoch 193, training loss: 8284.32, average training loss: 10413.99, base loss: 16034.43
[INFO 2017-06-30 01:04:27,713 main.py:52] epoch 194, training loss: 8876.50, average training loss: 10406.10, base loss: 16038.37
[INFO 2017-06-30 01:04:30,886 main.py:52] epoch 195, training loss: 7823.34, average training loss: 10392.93, base loss: 16033.13
[INFO 2017-06-30 01:04:34,076 main.py:52] epoch 196, training loss: 8617.08, average training loss: 10383.91, base loss: 16038.33
[INFO 2017-06-30 01:04:37,261 main.py:52] epoch 197, training loss: 8214.89, average training loss: 10372.96, base loss: 16037.38
[INFO 2017-06-30 01:04:40,455 main.py:52] epoch 198, training loss: 8435.14, average training loss: 10363.22, base loss: 16038.10
[INFO 2017-06-30 01:04:43,669 main.py:52] epoch 199, training loss: 7876.03, average training loss: 10350.78, base loss: 16033.22
[INFO 2017-06-30 01:04:43,669 main.py:54] epoch 199, testing
[INFO 2017-06-30 01:04:56,924 main.py:97] average testing loss: 8409.37, base loss: 16206.44
[INFO 2017-06-30 01:04:56,925 main.py:98] improve_loss: 7797.07, improve_percent: 0.48
[INFO 2017-06-30 01:04:56,926 main.py:62] model save to ./model/final.pth
[INFO 2017-06-30 01:04:56,961 main.py:66] current best improved percent: 0.48
[INFO 2017-06-30 01:05:00,111 main.py:52] epoch 200, training loss: 7589.46, average training loss: 10337.04, base loss: 16024.99
[INFO 2017-06-30 01:05:03,268 main.py:52] epoch 201, training loss: 7372.80, average training loss: 10322.37, base loss: 16015.14
[INFO 2017-06-30 01:05:06,431 main.py:52] epoch 202, training loss: 8208.35, average training loss: 10311.96, base loss: 16008.89
[INFO 2017-06-30 01:05:09,627 main.py:52] epoch 203, training loss: 9195.32, average training loss: 10306.48, base loss: 16011.65
[INFO 2017-06-30 01:05:12,810 main.py:52] epoch 204, training loss: 8500.54, average training loss: 10297.67, base loss: 16010.08
[INFO 2017-06-30 01:05:15,928 main.py:52] epoch 205, training loss: 8687.43, average training loss: 10289.86, base loss: 16016.40
[INFO 2017-06-30 01:05:19,113 main.py:52] epoch 206, training loss: 8664.69, average training loss: 10282.01, base loss: 16018.79
[INFO 2017-06-30 01:05:22,308 main.py:52] epoch 207, training loss: 8416.59, average training loss: 10273.04, base loss: 16018.67
[INFO 2017-06-30 01:05:25,461 main.py:52] epoch 208, training loss: 8601.16, average training loss: 10265.04, base loss: 16018.38
[INFO 2017-06-30 01:05:28,674 main.py:52] epoch 209, training loss: 7902.89, average training loss: 10253.79, base loss: 16012.30
[INFO 2017-06-30 01:05:31,865 main.py:52] epoch 210, training loss: 7995.97, average training loss: 10243.09, base loss: 16003.81
[INFO 2017-06-30 01:05:35,020 main.py:52] epoch 211, training loss: 7761.85, average training loss: 10231.38, base loss: 15999.26
[INFO 2017-06-30 01:05:38,194 main.py:52] epoch 212, training loss: 8920.15, average training loss: 10225.23, base loss: 16002.90
[INFO 2017-06-30 01:05:41,373 main.py:52] epoch 213, training loss: 8719.48, average training loss: 10218.19, base loss: 16005.48
[INFO 2017-06-30 01:05:44,587 main.py:52] epoch 214, training loss: 8982.90, average training loss: 10212.45, base loss: 16012.12
[INFO 2017-06-30 01:05:47,735 main.py:52] epoch 215, training loss: 8372.73, average training loss: 10203.93, base loss: 16014.99
[INFO 2017-06-30 01:05:50,899 main.py:52] epoch 216, training loss: 9166.26, average training loss: 10199.15, base loss: 16025.32
[INFO 2017-06-30 01:05:54,028 main.py:52] epoch 217, training loss: 8260.05, average training loss: 10190.25, base loss: 16021.70
[INFO 2017-06-30 01:05:57,232 main.py:52] epoch 218, training loss: 9365.93, average training loss: 10186.49, base loss: 16030.66
[INFO 2017-06-30 01:06:00,474 main.py:52] epoch 219, training loss: 8000.21, average training loss: 10176.55, base loss: 16026.27
[INFO 2017-06-30 01:06:03,626 main.py:52] epoch 220, training loss: 7746.79, average training loss: 10165.56, base loss: 16023.47
[INFO 2017-06-30 01:06:06,817 main.py:52] epoch 221, training loss: 8999.93, average training loss: 10160.31, base loss: 16025.27
[INFO 2017-06-30 01:06:09,975 main.py:52] epoch 222, training loss: 8664.65, average training loss: 10153.60, base loss: 16025.63
[INFO 2017-06-30 01:06:13,200 main.py:52] epoch 223, training loss: 8206.10, average training loss: 10144.90, base loss: 16022.13
[INFO 2017-06-30 01:06:16,438 main.py:52] epoch 224, training loss: 8383.30, average training loss: 10137.08, base loss: 16023.44
[INFO 2017-06-30 01:06:19,590 main.py:52] epoch 225, training loss: 7651.47, average training loss: 10126.08, base loss: 16016.55
[INFO 2017-06-30 01:06:22,749 main.py:52] epoch 226, training loss: 7994.44, average training loss: 10116.69, base loss: 16008.29
[INFO 2017-06-30 01:06:25,908 main.py:52] epoch 227, training loss: 8027.59, average training loss: 10107.52, base loss: 16004.71
[INFO 2017-06-30 01:06:29,071 main.py:52] epoch 228, training loss: 8547.00, average training loss: 10100.71, base loss: 16007.22
[INFO 2017-06-30 01:06:32,273 main.py:52] epoch 229, training loss: 9357.81, average training loss: 10097.48, base loss: 16015.83
[INFO 2017-06-30 01:06:35,418 main.py:52] epoch 230, training loss: 8012.90, average training loss: 10088.46, base loss: 16016.56
[INFO 2017-06-30 01:06:38,568 main.py:52] epoch 231, training loss: 8455.60, average training loss: 10081.42, base loss: 16020.65
[INFO 2017-06-30 01:06:41,725 main.py:52] epoch 232, training loss: 8114.57, average training loss: 10072.98, base loss: 16021.43
[INFO 2017-06-30 01:06:44,959 main.py:52] epoch 233, training loss: 7645.99, average training loss: 10062.60, base loss: 16015.88
[INFO 2017-06-30 01:06:48,155 main.py:52] epoch 234, training loss: 7715.70, average training loss: 10052.62, base loss: 16009.22
[INFO 2017-06-30 01:06:51,311 main.py:52] epoch 235, training loss: 9635.55, average training loss: 10050.85, base loss: 16018.35
[INFO 2017-06-30 01:06:54,489 main.py:52] epoch 236, training loss: 8326.93, average training loss: 10043.58, base loss: 16021.18
[INFO 2017-06-30 01:06:57,716 main.py:52] epoch 237, training loss: 8042.42, average training loss: 10035.17, base loss: 16016.64
[INFO 2017-06-30 01:07:00,901 main.py:52] epoch 238, training loss: 8083.65, average training loss: 10027.00, base loss: 16009.13
[INFO 2017-06-30 01:07:04,070 main.py:52] epoch 239, training loss: 8051.14, average training loss: 10018.77, base loss: 16007.64
[INFO 2017-06-30 01:07:07,321 main.py:52] epoch 240, training loss: 8136.17, average training loss: 10010.96, base loss: 16005.44
[INFO 2017-06-30 01:07:10,470 main.py:52] epoch 241, training loss: 8475.92, average training loss: 10004.62, base loss: 16002.72
[INFO 2017-06-30 01:07:13,609 main.py:52] epoch 242, training loss: 8157.34, average training loss: 9997.01, base loss: 16003.35
[INFO 2017-06-30 01:07:16,833 main.py:52] epoch 243, training loss: 8906.45, average training loss: 9992.54, base loss: 16005.90
[INFO 2017-06-30 01:07:19,982 main.py:52] epoch 244, training loss: 8200.46, average training loss: 9985.23, base loss: 16004.40
[INFO 2017-06-30 01:07:23,177 main.py:52] epoch 245, training loss: 8192.37, average training loss: 9977.94, base loss: 15999.38
[INFO 2017-06-30 01:07:26,348 main.py:52] epoch 246, training loss: 8435.27, average training loss: 9971.70, base loss: 16002.49
[INFO 2017-06-30 01:07:29,528 main.py:52] epoch 247, training loss: 7802.86, average training loss: 9962.95, base loss: 16001.32
[INFO 2017-06-30 01:07:32,685 main.py:52] epoch 248, training loss: 9203.63, average training loss: 9959.90, base loss: 16005.64
[INFO 2017-06-30 01:07:35,841 main.py:52] epoch 249, training loss: 8781.55, average training loss: 9955.19, base loss: 16012.15
[INFO 2017-06-30 01:07:38,991 main.py:52] epoch 250, training loss: 8575.21, average training loss: 9949.69, base loss: 16016.73
[INFO 2017-06-30 01:07:42,174 main.py:52] epoch 251, training loss: 8191.03, average training loss: 9942.71, base loss: 16014.39
[INFO 2017-06-30 01:07:45,358 main.py:52] epoch 252, training loss: 7856.40, average training loss: 9934.46, base loss: 16012.69
[INFO 2017-06-30 01:07:48,509 main.py:52] epoch 253, training loss: 8627.20, average training loss: 9929.32, base loss: 16014.31
[INFO 2017-06-30 01:07:51,672 main.py:52] epoch 254, training loss: 7668.30, average training loss: 9920.45, base loss: 16009.49
[INFO 2017-06-30 01:07:54,842 main.py:52] epoch 255, training loss: 8719.58, average training loss: 9915.76, base loss: 16017.26
[INFO 2017-06-30 01:07:57,987 main.py:52] epoch 256, training loss: 7612.22, average training loss: 9906.80, base loss: 16011.24
[INFO 2017-06-30 01:08:01,107 main.py:52] epoch 257, training loss: 8286.56, average training loss: 9900.52, base loss: 16014.41
[INFO 2017-06-30 01:08:04,229 main.py:52] epoch 258, training loss: 7635.11, average training loss: 9891.77, base loss: 16016.80
[INFO 2017-06-30 01:08:07,378 main.py:52] epoch 259, training loss: 7947.21, average training loss: 9884.29, base loss: 16012.10
[INFO 2017-06-30 01:08:10,567 main.py:52] epoch 260, training loss: 9381.26, average training loss: 9882.36, base loss: 16018.65
[INFO 2017-06-30 01:08:13,758 main.py:52] epoch 261, training loss: 8170.25, average training loss: 9875.83, base loss: 16021.23
[INFO 2017-06-30 01:08:16,933 main.py:52] epoch 262, training loss: 8387.01, average training loss: 9870.17, base loss: 16022.76
[INFO 2017-06-30 01:08:20,102 main.py:52] epoch 263, training loss: 8057.85, average training loss: 9863.30, base loss: 16023.19
[INFO 2017-06-30 01:08:23,266 main.py:52] epoch 264, training loss: 7897.40, average training loss: 9855.88, base loss: 16021.20
[INFO 2017-06-30 01:08:26,457 main.py:52] epoch 265, training loss: 7937.19, average training loss: 9848.67, base loss: 16019.26
[INFO 2017-06-30 01:08:29,629 main.py:52] epoch 266, training loss: 7714.56, average training loss: 9840.68, base loss: 16015.05
[INFO 2017-06-30 01:08:32,780 main.py:52] epoch 267, training loss: 8311.12, average training loss: 9834.97, base loss: 16015.92
[INFO 2017-06-30 01:08:35,966 main.py:52] epoch 268, training loss: 8280.80, average training loss: 9829.19, base loss: 16018.63
[INFO 2017-06-30 01:08:39,097 main.py:52] epoch 269, training loss: 7600.69, average training loss: 9820.94, base loss: 16015.28
[INFO 2017-06-30 01:08:42,277 main.py:52] epoch 270, training loss: 7653.69, average training loss: 9812.94, base loss: 16010.32
[INFO 2017-06-30 01:08:45,419 main.py:52] epoch 271, training loss: 7543.99, average training loss: 9804.60, base loss: 16005.56
[INFO 2017-06-30 01:08:48,652 main.py:52] epoch 272, training loss: 7650.38, average training loss: 9796.71, base loss: 16002.58
[INFO 2017-06-30 01:08:51,817 main.py:52] epoch 273, training loss: 8654.37, average training loss: 9792.54, base loss: 16005.36
[INFO 2017-06-30 01:08:55,001 main.py:52] epoch 274, training loss: 7906.92, average training loss: 9785.68, base loss: 16006.16
[INFO 2017-06-30 01:08:58,113 main.py:52] epoch 275, training loss: 8448.71, average training loss: 9780.84, base loss: 16008.11
[INFO 2017-06-30 01:09:01,297 main.py:52] epoch 276, training loss: 8020.30, average training loss: 9774.48, base loss: 16008.09
[INFO 2017-06-30 01:09:04,477 main.py:52] epoch 277, training loss: 7487.14, average training loss: 9766.26, base loss: 16002.30
[INFO 2017-06-30 01:09:07,658 main.py:52] epoch 278, training loss: 8029.28, average training loss: 9760.03, base loss: 16004.98
[INFO 2017-06-30 01:09:10,835 main.py:52] epoch 279, training loss: 7133.73, average training loss: 9750.65, base loss: 16001.72
[INFO 2017-06-30 01:09:13,986 main.py:52] epoch 280, training loss: 8363.95, average training loss: 9745.72, base loss: 16003.47
[INFO 2017-06-30 01:09:17,159 main.py:52] epoch 281, training loss: 8598.53, average training loss: 9741.65, base loss: 16005.94
[INFO 2017-06-30 01:09:20,329 main.py:52] epoch 282, training loss: 8078.01, average training loss: 9735.77, base loss: 16003.49
[INFO 2017-06-30 01:09:23,537 main.py:52] epoch 283, training loss: 8094.67, average training loss: 9729.99, base loss: 16002.85
[INFO 2017-06-30 01:09:26,689 main.py:52] epoch 284, training loss: 7487.25, average training loss: 9722.12, base loss: 15997.44
[INFO 2017-06-30 01:09:29,851 main.py:52] epoch 285, training loss: 8381.69, average training loss: 9717.43, base loss: 15997.59
[INFO 2017-06-30 01:09:33,030 main.py:52] epoch 286, training loss: 7234.34, average training loss: 9708.78, base loss: 15992.63
[INFO 2017-06-30 01:09:36,218 main.py:52] epoch 287, training loss: 7887.18, average training loss: 9702.46, base loss: 15993.35
[INFO 2017-06-30 01:09:39,367 main.py:52] epoch 288, training loss: 8265.94, average training loss: 9697.49, base loss: 15995.93
[INFO 2017-06-30 01:09:42,522 main.py:52] epoch 289, training loss: 7635.28, average training loss: 9690.38, base loss: 15992.91
[INFO 2017-06-30 01:09:45,730 main.py:52] epoch 290, training loss: 8349.48, average training loss: 9685.77, base loss: 15995.53
[INFO 2017-06-30 01:09:48,889 main.py:52] epoch 291, training loss: 8494.57, average training loss: 9681.69, base loss: 15993.39
[INFO 2017-06-30 01:09:52,011 main.py:52] epoch 292, training loss: 8523.00, average training loss: 9677.73, base loss: 15995.41
[INFO 2017-06-30 01:09:55,185 main.py:52] epoch 293, training loss: 8806.21, average training loss: 9674.77, base loss: 16000.68
[INFO 2017-06-30 01:09:58,343 main.py:52] epoch 294, training loss: 7186.61, average training loss: 9666.34, base loss: 15991.76
[INFO 2017-06-30 01:10:01,575 main.py:52] epoch 295, training loss: 8916.73, average training loss: 9663.80, base loss: 15997.45
[INFO 2017-06-30 01:10:04,728 main.py:52] epoch 296, training loss: 7293.48, average training loss: 9655.82, base loss: 15992.87
[INFO 2017-06-30 01:10:07,951 main.py:52] epoch 297, training loss: 7211.76, average training loss: 9647.62, base loss: 15990.60
[INFO 2017-06-30 01:10:11,055 main.py:52] epoch 298, training loss: 8041.86, average training loss: 9642.25, base loss: 15992.38
[INFO 2017-06-30 01:10:14,218 main.py:52] epoch 299, training loss: 7952.76, average training loss: 9636.62, base loss: 15989.75
[INFO 2017-06-30 01:10:14,218 main.py:54] epoch 299, testing
[INFO 2017-06-30 01:10:27,435 main.py:97] average testing loss: 7626.34, base loss: 15128.52
[INFO 2017-06-30 01:10:27,435 main.py:98] improve_loss: 7502.18, improve_percent: 0.50
[INFO 2017-06-30 01:10:27,438 main.py:62] model save to ./model/final.pth
[INFO 2017-06-30 01:10:27,472 main.py:66] current best improved percent: 0.50
[INFO 2017-06-30 01:10:30,638 main.py:52] epoch 300, training loss: 7704.48, average training loss: 9630.20, base loss: 15989.29
[INFO 2017-06-30 01:10:33,810 main.py:52] epoch 301, training loss: 7764.35, average training loss: 9624.02, base loss: 15990.62
[INFO 2017-06-30 01:10:36,973 main.py:52] epoch 302, training loss: 7446.56, average training loss: 9616.83, base loss: 15986.23
[INFO 2017-06-30 01:10:40,156 main.py:52] epoch 303, training loss: 7481.22, average training loss: 9609.81, base loss: 15983.03
[INFO 2017-06-30 01:10:43,340 main.py:52] epoch 304, training loss: 8060.41, average training loss: 9604.73, base loss: 15980.99
[INFO 2017-06-30 01:10:46,536 main.py:52] epoch 305, training loss: 7657.26, average training loss: 9598.37, base loss: 15978.30
[INFO 2017-06-30 01:10:49,675 main.py:52] epoch 306, training loss: 7536.66, average training loss: 9591.65, base loss: 15974.57
[INFO 2017-06-30 01:10:52,840 main.py:52] epoch 307, training loss: 8340.17, average training loss: 9587.59, base loss: 15975.48
[INFO 2017-06-30 01:10:56,039 main.py:52] epoch 308, training loss: 8183.10, average training loss: 9583.04, base loss: 15978.11
[INFO 2017-06-30 01:10:59,262 main.py:52] epoch 309, training loss: 7698.10, average training loss: 9576.96, base loss: 15977.76
[INFO 2017-06-30 01:11:02,446 main.py:52] epoch 310, training loss: 8521.83, average training loss: 9573.57, base loss: 15981.31
[INFO 2017-06-30 01:11:05,621 main.py:52] epoch 311, training loss: 7479.42, average training loss: 9566.86, base loss: 15978.66
[INFO 2017-06-30 01:11:08,785 main.py:52] epoch 312, training loss: 8560.18, average training loss: 9563.64, base loss: 15979.05
[INFO 2017-06-30 01:11:11,970 main.py:52] epoch 313, training loss: 8070.44, average training loss: 9558.88, base loss: 15977.96
[INFO 2017-06-30 01:11:15,134 main.py:52] epoch 314, training loss: 7926.66, average training loss: 9553.70, base loss: 15980.18
[INFO 2017-06-30 01:11:18,330 main.py:52] epoch 315, training loss: 7394.14, average training loss: 9546.87, base loss: 15976.27
[INFO 2017-06-30 01:11:21,505 main.py:52] epoch 316, training loss: 8293.55, average training loss: 9542.91, base loss: 15981.04
[INFO 2017-06-30 01:11:24,672 main.py:52] epoch 317, training loss: 7323.67, average training loss: 9535.94, base loss: 15975.78
[INFO 2017-06-30 01:11:27,808 main.py:52] epoch 318, training loss: 8080.20, average training loss: 9531.37, base loss: 15973.69
[INFO 2017-06-30 01:11:31,004 main.py:52] epoch 319, training loss: 8493.37, average training loss: 9528.13, base loss: 15978.74
[INFO 2017-06-30 01:11:34,169 main.py:52] epoch 320, training loss: 7560.28, average training loss: 9522.00, base loss: 15975.84
[INFO 2017-06-30 01:11:37,331 main.py:52] epoch 321, training loss: 7789.38, average training loss: 9516.62, base loss: 15971.98
[INFO 2017-06-30 01:11:40,476 main.py:52] epoch 322, training loss: 8804.77, average training loss: 9514.41, base loss: 15974.98
[INFO 2017-06-30 01:11:43,615 main.py:52] epoch 323, training loss: 8867.42, average training loss: 9512.42, base loss: 15981.32
[INFO 2017-06-30 01:11:46,795 main.py:52] epoch 324, training loss: 7988.42, average training loss: 9507.73, base loss: 15980.90
[INFO 2017-06-30 01:11:49,971 main.py:52] epoch 325, training loss: 6910.47, average training loss: 9499.76, base loss: 15974.85
[INFO 2017-06-30 01:11:53,118 main.py:52] epoch 326, training loss: 7897.82, average training loss: 9494.86, base loss: 15974.95
[INFO 2017-06-30 01:11:56,277 main.py:52] epoch 327, training loss: 7628.88, average training loss: 9489.17, base loss: 15972.76
[INFO 2017-06-30 01:11:59,464 main.py:52] epoch 328, training loss: 7455.75, average training loss: 9482.99, base loss: 15968.08
[INFO 2017-06-30 01:12:02,708 main.py:52] epoch 329, training loss: 8078.24, average training loss: 9478.74, base loss: 15969.18
[INFO 2017-06-30 01:12:05,867 main.py:52] epoch 330, training loss: 7599.58, average training loss: 9473.06, base loss: 15965.09
[INFO 2017-06-30 01:12:08,994 main.py:52] epoch 331, training loss: 7414.65, average training loss: 9466.86, base loss: 15962.70
[INFO 2017-06-30 01:12:12,165 main.py:52] epoch 332, training loss: 7499.66, average training loss: 9460.95, base loss: 15960.50
[INFO 2017-06-30 01:12:15,365 main.py:52] epoch 333, training loss: 7239.87, average training loss: 9454.30, base loss: 15954.11
[INFO 2017-06-30 01:12:18,560 main.py:52] epoch 334, training loss: 7593.86, average training loss: 9448.75, base loss: 15952.59
[INFO 2017-06-30 01:12:21,799 main.py:52] epoch 335, training loss: 7458.27, average training loss: 9442.82, base loss: 15951.47
[INFO 2017-06-30 01:12:24,975 main.py:52] epoch 336, training loss: 7620.15, average training loss: 9437.41, base loss: 15947.18
[INFO 2017-06-30 01:12:28,113 main.py:52] epoch 337, training loss: 7807.82, average training loss: 9432.59, base loss: 15946.14
[INFO 2017-06-30 01:12:31,317 main.py:52] epoch 338, training loss: 7803.63, average training loss: 9427.79, base loss: 15946.05
[INFO 2017-06-30 01:12:34,451 main.py:52] epoch 339, training loss: 6956.51, average training loss: 9420.52, base loss: 15940.86
[INFO 2017-06-30 01:12:37,622 main.py:52] epoch 340, training loss: 8177.92, average training loss: 9416.88, base loss: 15941.21
[INFO 2017-06-30 01:12:40,842 main.py:52] epoch 341, training loss: 7730.79, average training loss: 9411.95, base loss: 15940.50
[INFO 2017-06-30 01:12:44,025 main.py:52] epoch 342, training loss: 7956.10, average training loss: 9407.70, base loss: 15939.29
[INFO 2017-06-30 01:12:47,189 main.py:52] epoch 343, training loss: 7928.25, average training loss: 9403.40, base loss: 15939.29
[INFO 2017-06-30 01:12:50,384 main.py:52] epoch 344, training loss: 7254.14, average training loss: 9397.17, base loss: 15936.46
[INFO 2017-06-30 01:12:53,571 main.py:52] epoch 345, training loss: 8837.48, average training loss: 9395.55, base loss: 15939.26
[INFO 2017-06-30 01:12:56,725 main.py:52] epoch 346, training loss: 6995.72, average training loss: 9388.64, base loss: 15934.18
[INFO 2017-06-30 01:12:59,895 main.py:52] epoch 347, training loss: 7426.02, average training loss: 9383.00, base loss: 15933.18
[INFO 2017-06-30 01:13:03,036 main.py:52] epoch 348, training loss: 7696.73, average training loss: 9378.17, base loss: 15932.55
[INFO 2017-06-30 01:13:06,216 main.py:52] epoch 349, training loss: 8015.63, average training loss: 9374.27, base loss: 15934.16
[INFO 2017-06-30 01:13:09,387 main.py:52] epoch 350, training loss: 6992.02, average training loss: 9367.49, base loss: 15929.45
[INFO 2017-06-30 01:13:12,547 main.py:52] epoch 351, training loss: 8533.33, average training loss: 9365.12, base loss: 15935.24
[INFO 2017-06-30 01:13:15,721 main.py:52] epoch 352, training loss: 7955.64, average training loss: 9361.12, base loss: 15935.32
[INFO 2017-06-30 01:13:18,892 main.py:52] epoch 353, training loss: 7300.24, average training loss: 9355.30, base loss: 15929.49
[INFO 2017-06-30 01:13:22,100 main.py:52] epoch 354, training loss: 7667.72, average training loss: 9350.55, base loss: 15928.92
[INFO 2017-06-30 01:13:25,273 main.py:52] epoch 355, training loss: 7548.50, average training loss: 9345.49, base loss: 15927.09
[INFO 2017-06-30 01:13:28,452 main.py:52] epoch 356, training loss: 7449.46, average training loss: 9340.17, base loss: 15924.61
[INFO 2017-06-30 01:13:31,644 main.py:52] epoch 357, training loss: 7854.87, average training loss: 9336.03, base loss: 15927.71
[INFO 2017-06-30 01:13:34,803 main.py:52] epoch 358, training loss: 8341.70, average training loss: 9333.26, base loss: 15932.56
[INFO 2017-06-30 01:13:37,939 main.py:52] epoch 359, training loss: 7782.42, average training loss: 9328.95, base loss: 15934.59
[INFO 2017-06-30 01:13:41,153 main.py:52] epoch 360, training loss: 7646.40, average training loss: 9324.29, base loss: 15936.73
[INFO 2017-06-30 01:13:44,316 main.py:52] epoch 361, training loss: 7298.53, average training loss: 9318.69, base loss: 15932.88
[INFO 2017-06-30 01:13:47,504 main.py:52] epoch 362, training loss: 8023.21, average training loss: 9315.12, base loss: 15934.88
[INFO 2017-06-30 01:13:50,685 main.py:52] epoch 363, training loss: 8459.02, average training loss: 9312.77, base loss: 15940.74
[INFO 2017-06-30 01:13:53,866 main.py:52] epoch 364, training loss: 7501.58, average training loss: 9307.81, base loss: 15939.12
[INFO 2017-06-30 01:13:57,064 main.py:52] epoch 365, training loss: 7564.42, average training loss: 9303.05, base loss: 15939.38
[INFO 2017-06-30 01:14:00,236 main.py:52] epoch 366, training loss: 8309.37, average training loss: 9300.34, base loss: 15941.43
[INFO 2017-06-30 01:14:03,466 main.py:52] epoch 367, training loss: 8631.10, average training loss: 9298.52, base loss: 15948.43
[INFO 2017-06-30 01:14:06,653 main.py:52] epoch 368, training loss: 7572.97, average training loss: 9293.84, base loss: 15949.12
[INFO 2017-06-30 01:14:09,795 main.py:52] epoch 369, training loss: 7208.87, average training loss: 9288.21, base loss: 15946.85
[INFO 2017-06-30 01:14:12,993 main.py:52] epoch 370, training loss: 7533.56, average training loss: 9283.48, base loss: 15948.66
[INFO 2017-06-30 01:14:16,193 main.py:52] epoch 371, training loss: 8398.53, average training loss: 9281.10, base loss: 15953.36
[INFO 2017-06-30 01:14:19,347 main.py:52] epoch 372, training loss: 7350.30, average training loss: 9275.92, base loss: 15953.08
[INFO 2017-06-30 01:14:22,516 main.py:52] epoch 373, training loss: 7522.68, average training loss: 9271.24, base loss: 15949.68
[INFO 2017-06-30 01:14:25,681 main.py:52] epoch 374, training loss: 8186.48, average training loss: 9268.34, base loss: 15951.30
[INFO 2017-06-30 01:14:28,911 main.py:52] epoch 375, training loss: 8103.90, average training loss: 9265.25, base loss: 15953.03
[INFO 2017-06-30 01:14:32,067 main.py:52] epoch 376, training loss: 7819.90, average training loss: 9261.41, base loss: 15955.16
[INFO 2017-06-30 01:14:35,245 main.py:52] epoch 377, training loss: 7764.50, average training loss: 9257.45, base loss: 15957.35
[INFO 2017-06-30 01:14:38,439 main.py:52] epoch 378, training loss: 7531.04, average training loss: 9252.90, base loss: 15957.03
[INFO 2017-06-30 01:14:41,625 main.py:52] epoch 379, training loss: 7688.52, average training loss: 9248.78, base loss: 15955.93
[INFO 2017-06-30 01:14:44,774 main.py:52] epoch 380, training loss: 8104.67, average training loss: 9245.78, base loss: 15958.03
[INFO 2017-06-30 01:14:47,974 main.py:52] epoch 381, training loss: 7604.07, average training loss: 9241.48, base loss: 15956.75
[INFO 2017-06-30 01:14:51,165 main.py:52] epoch 382, training loss: 7660.00, average training loss: 9237.35, base loss: 15957.18
[INFO 2017-06-30 01:14:54,353 main.py:52] epoch 383, training loss: 7795.02, average training loss: 9233.59, base loss: 15955.91
[INFO 2017-06-30 01:14:57,541 main.py:52] epoch 384, training loss: 7697.13, average training loss: 9229.60, base loss: 15956.30
[INFO 2017-06-30 01:15:00,701 main.py:52] epoch 385, training loss: 7059.71, average training loss: 9223.98, base loss: 15954.78
[INFO 2017-06-30 01:15:03,950 main.py:52] epoch 386, training loss: 7895.42, average training loss: 9220.55, base loss: 15955.67
[INFO 2017-06-30 01:15:07,119 main.py:52] epoch 387, training loss: 7694.71, average training loss: 9216.62, base loss: 15956.00
[INFO 2017-06-30 01:15:10,296 main.py:52] epoch 388, training loss: 7764.31, average training loss: 9212.88, base loss: 15955.88
[INFO 2017-06-30 01:15:13,451 main.py:52] epoch 389, training loss: 7559.02, average training loss: 9208.64, base loss: 15956.46
[INFO 2017-06-30 01:15:16,642 main.py:52] epoch 390, training loss: 8435.67, average training loss: 9206.66, base loss: 15957.69
[INFO 2017-06-30 01:15:19,837 main.py:52] epoch 391, training loss: 7513.96, average training loss: 9202.35, base loss: 15956.98
[INFO 2017-06-30 01:15:22,985 main.py:52] epoch 392, training loss: 7904.19, average training loss: 9199.04, base loss: 15957.02
[INFO 2017-06-30 01:15:26,194 main.py:52] epoch 393, training loss: 7882.18, average training loss: 9195.70, base loss: 15958.45
[INFO 2017-06-30 01:15:29,354 main.py:52] epoch 394, training loss: 7027.97, average training loss: 9190.21, base loss: 15956.62
[INFO 2017-06-30 01:15:32,493 main.py:52] epoch 395, training loss: 8247.67, average training loss: 9187.83, base loss: 15963.69
[INFO 2017-06-30 01:15:35,666 main.py:52] epoch 396, training loss: 7721.64, average training loss: 9184.14, base loss: 15964.99
[INFO 2017-06-30 01:15:38,846 main.py:52] epoch 397, training loss: 8134.15, average training loss: 9181.50, base loss: 15967.63
[INFO 2017-06-30 01:15:42,043 main.py:52] epoch 398, training loss: 7016.72, average training loss: 9176.08, base loss: 15965.11
[INFO 2017-06-30 01:15:45,217 main.py:52] epoch 399, training loss: 7947.26, average training loss: 9173.00, base loss: 15967.37
[INFO 2017-06-30 01:15:45,218 main.py:54] epoch 399, testing
[INFO 2017-06-30 01:15:58,482 main.py:97] average testing loss: 7461.28, base loss: 15485.40
[INFO 2017-06-30 01:15:58,482 main.py:98] improve_loss: 8024.12, improve_percent: 0.52
[INFO 2017-06-30 01:15:58,483 main.py:62] model save to ./model/final.pth
[INFO 2017-06-30 01:15:58,518 main.py:66] current best improved percent: 0.52
[INFO 2017-06-30 01:16:01,691 main.py:52] epoch 400, training loss: 8106.68, average training loss: 9170.35, base loss: 15970.07
[INFO 2017-06-30 01:16:04,891 main.py:52] epoch 401, training loss: 7501.59, average training loss: 9166.19, base loss: 15968.43
[INFO 2017-06-30 01:16:08,086 main.py:52] epoch 402, training loss: 7424.78, average training loss: 9161.87, base loss: 15967.86
[INFO 2017-06-30 01:16:11,258 main.py:52] epoch 403, training loss: 7729.81, average training loss: 9158.33, base loss: 15968.17
[INFO 2017-06-30 01:16:14,416 main.py:52] epoch 404, training loss: 8378.94, average training loss: 9156.40, base loss: 15973.63
[INFO 2017-06-30 01:16:17,556 main.py:52] epoch 405, training loss: 7542.44, average training loss: 9152.43, base loss: 15972.39
[INFO 2017-06-30 01:16:20,669 main.py:52] epoch 406, training loss: 7870.18, average training loss: 9149.28, base loss: 15974.38
[INFO 2017-06-30 01:16:23,818 main.py:52] epoch 407, training loss: 8392.89, average training loss: 9147.42, base loss: 15978.34
[INFO 2017-06-30 01:16:26,987 main.py:52] epoch 408, training loss: 7153.34, average training loss: 9142.55, base loss: 15975.77
[INFO 2017-06-30 01:16:30,147 main.py:52] epoch 409, training loss: 8266.69, average training loss: 9140.41, base loss: 15975.23
[INFO 2017-06-30 01:16:33,316 main.py:52] epoch 410, training loss: 7927.80, average training loss: 9137.46, base loss: 15978.13
[INFO 2017-06-30 01:16:36,471 main.py:52] epoch 411, training loss: 6964.02, average training loss: 9132.19, base loss: 15976.02
[INFO 2017-06-30 01:16:39,656 main.py:52] epoch 412, training loss: 7540.00, average training loss: 9128.33, base loss: 15975.47
[INFO 2017-06-30 01:16:42,811 main.py:52] epoch 413, training loss: 7638.45, average training loss: 9124.73, base loss: 15973.09
[INFO 2017-06-30 01:16:45,995 main.py:52] epoch 414, training loss: 7956.73, average training loss: 9121.92, base loss: 15973.07
[INFO 2017-06-30 01:16:49,142 main.py:52] epoch 415, training loss: 7580.82, average training loss: 9118.21, base loss: 15972.82
[INFO 2017-06-30 01:16:52,342 main.py:52] epoch 416, training loss: 7939.34, average training loss: 9115.39, base loss: 15974.56
[INFO 2017-06-30 01:16:55,489 main.py:52] epoch 417, training loss: 7898.76, average training loss: 9112.48, base loss: 15974.39
[INFO 2017-06-30 01:16:58,663 main.py:52] epoch 418, training loss: 8051.65, average training loss: 9109.94, base loss: 15977.39
[INFO 2017-06-30 01:17:01,816 main.py:52] epoch 419, training loss: 7546.62, average training loss: 9106.22, base loss: 15977.90
[INFO 2017-06-30 01:17:05,019 main.py:52] epoch 420, training loss: 7762.06, average training loss: 9103.03, base loss: 15979.26
[INFO 2017-06-30 01:17:08,194 main.py:52] epoch 421, training loss: 7464.32, average training loss: 9099.15, base loss: 15980.49
[INFO 2017-06-30 01:17:11,403 main.py:52] epoch 422, training loss: 8325.71, average training loss: 9097.32, base loss: 15984.70
[INFO 2017-06-30 01:17:14,551 main.py:52] epoch 423, training loss: 7792.22, average training loss: 9094.24, base loss: 15984.65
[INFO 2017-06-30 01:17:17,707 main.py:52] epoch 424, training loss: 7150.84, average training loss: 9089.67, base loss: 15980.85
[INFO 2017-06-30 01:17:20,905 main.py:52] epoch 425, training loss: 8404.77, average training loss: 9088.06, base loss: 15982.81
[INFO 2017-06-30 01:17:24,068 main.py:52] epoch 426, training loss: 7600.29, average training loss: 9084.57, base loss: 15981.19
[INFO 2017-06-30 01:17:27,265 main.py:52] epoch 427, training loss: 8064.06, average training loss: 9082.19, base loss: 15981.29
[INFO 2017-06-30 01:17:30,422 main.py:52] epoch 428, training loss: 8427.64, average training loss: 9080.66, base loss: 15984.94
[INFO 2017-06-30 01:17:33,590 main.py:52] epoch 429, training loss: 8092.61, average training loss: 9078.37, base loss: 15989.04
[INFO 2017-06-30 01:17:36,769 main.py:52] epoch 430, training loss: 8686.20, average training loss: 9077.46, base loss: 15993.00
[INFO 2017-06-30 01:17:39,938 main.py:52] epoch 431, training loss: 7654.47, average training loss: 9074.16, base loss: 15991.00
[INFO 2017-06-30 01:17:43,121 main.py:52] epoch 432, training loss: 8001.59, average training loss: 9071.69, base loss: 15992.06
[INFO 2017-06-30 01:17:46,306 main.py:52] epoch 433, training loss: 8146.01, average training loss: 9069.55, base loss: 15992.23
[INFO 2017-06-30 01:17:49,493 main.py:52] epoch 434, training loss: 7996.60, average training loss: 9067.09, base loss: 15992.29
[INFO 2017-06-30 01:17:52,724 main.py:52] epoch 435, training loss: 6965.06, average training loss: 9062.27, base loss: 15988.55
[INFO 2017-06-30 01:17:55,950 main.py:52] epoch 436, training loss: 8087.96, average training loss: 9060.04, base loss: 15987.50
[INFO 2017-06-30 01:17:59,108 main.py:52] epoch 437, training loss: 7333.79, average training loss: 9056.09, base loss: 15983.57
[INFO 2017-06-30 01:18:02,323 main.py:52] epoch 438, training loss: 8053.29, average training loss: 9053.81, base loss: 15984.17
[INFO 2017-06-30 01:18:05,483 main.py:52] epoch 439, training loss: 7329.15, average training loss: 9049.89, base loss: 15981.33
[INFO 2017-06-30 01:18:08,659 main.py:52] epoch 440, training loss: 8573.71, average training loss: 9048.81, base loss: 15985.09
[INFO 2017-06-30 01:18:11,794 main.py:52] epoch 441, training loss: 7998.19, average training loss: 9046.43, base loss: 15987.21
[INFO 2017-06-30 01:18:14,983 main.py:52] epoch 442, training loss: 7239.87, average training loss: 9042.36, base loss: 15983.20
[INFO 2017-06-30 01:18:18,165 main.py:52] epoch 443, training loss: 7405.57, average training loss: 9038.67, base loss: 15981.07
[INFO 2017-06-30 01:18:21,306 main.py:52] epoch 444, training loss: 7793.79, average training loss: 9035.87, base loss: 15982.09
[INFO 2017-06-30 01:18:24,453 main.py:52] epoch 445, training loss: 7732.57, average training loss: 9032.95, base loss: 15979.49
[INFO 2017-06-30 01:18:27,657 main.py:52] epoch 446, training loss: 7285.74, average training loss: 9029.04, base loss: 15975.74
[INFO 2017-06-30 01:18:30,808 main.py:52] epoch 447, training loss: 7180.90, average training loss: 9024.92, base loss: 15975.09
[INFO 2017-06-30 01:18:33,983 main.py:52] epoch 448, training loss: 6719.59, average training loss: 9019.78, base loss: 15971.83
[INFO 2017-06-30 01:18:37,188 main.py:52] epoch 449, training loss: 6621.08, average training loss: 9014.45, base loss: 15966.44
[INFO 2017-06-30 01:18:40,392 main.py:52] epoch 450, training loss: 7415.33, average training loss: 9010.91, base loss: 15963.65
[INFO 2017-06-30 01:18:43,541 main.py:52] epoch 451, training loss: 8042.73, average training loss: 9008.76, base loss: 15965.53
[INFO 2017-06-30 01:18:46,730 main.py:52] epoch 452, training loss: 8225.67, average training loss: 9007.03, base loss: 15967.98
[INFO 2017-06-30 01:18:49,906 main.py:52] epoch 453, training loss: 7065.15, average training loss: 9002.76, base loss: 15965.48
[INFO 2017-06-30 01:18:53,070 main.py:52] epoch 454, training loss: 6811.32, average training loss: 8997.94, base loss: 15961.92
[INFO 2017-06-30 01:18:56,251 main.py:52] epoch 455, training loss: 7244.13, average training loss: 8994.09, base loss: 15960.72
[INFO 2017-06-30 01:18:59,378 main.py:52] epoch 456, training loss: 7480.43, average training loss: 8990.78, base loss: 15960.29
[INFO 2017-06-30 01:19:02,579 main.py:52] epoch 457, training loss: 7435.68, average training loss: 8987.39, base loss: 15959.93
[INFO 2017-06-30 01:19:05,781 main.py:52] epoch 458, training loss: 7440.28, average training loss: 8984.02, base loss: 15959.95
[INFO 2017-06-30 01:19:08,961 main.py:52] epoch 459, training loss: 7964.62, average training loss: 8981.80, base loss: 15962.59
[INFO 2017-06-30 01:19:12,079 main.py:52] epoch 460, training loss: 7593.94, average training loss: 8978.79, base loss: 15965.15
[INFO 2017-06-30 01:19:15,285 main.py:52] epoch 461, training loss: 6905.34, average training loss: 8974.30, base loss: 15960.63
[INFO 2017-06-30 01:19:18,499 main.py:52] epoch 462, training loss: 7740.97, average training loss: 8971.64, base loss: 15961.61
[INFO 2017-06-30 01:19:21,649 main.py:52] epoch 463, training loss: 7323.86, average training loss: 8968.09, base loss: 15961.86
[INFO 2017-06-30 01:19:24,825 main.py:52] epoch 464, training loss: 8503.56, average training loss: 8967.09, base loss: 15964.49
[INFO 2017-06-30 01:19:27,979 main.py:52] epoch 465, training loss: 7360.93, average training loss: 8963.64, base loss: 15964.98
[INFO 2017-06-30 01:19:31,161 main.py:52] epoch 466, training loss: 7984.00, average training loss: 8961.54, base loss: 15968.17
[INFO 2017-06-30 01:19:34,276 main.py:52] epoch 467, training loss: 7700.61, average training loss: 8958.85, base loss: 15969.74
[INFO 2017-06-30 01:19:37,424 main.py:52] epoch 468, training loss: 7307.20, average training loss: 8955.33, base loss: 15969.31
[INFO 2017-06-30 01:19:40,618 main.py:52] epoch 469, training loss: 8409.57, average training loss: 8954.17, base loss: 15973.49
[INFO 2017-06-30 01:19:43,839 main.py:52] epoch 470, training loss: 7642.88, average training loss: 8951.38, base loss: 15972.80
[INFO 2017-06-30 01:19:47,021 main.py:52] epoch 471, training loss: 7159.83, average training loss: 8947.59, base loss: 15971.57
[INFO 2017-06-30 01:19:50,173 main.py:52] epoch 472, training loss: 7742.01, average training loss: 8945.04, base loss: 15971.89
[INFO 2017-06-30 01:19:53,326 main.py:52] epoch 473, training loss: 7161.88, average training loss: 8941.28, base loss: 15969.06
[INFO 2017-06-30 01:19:56,503 main.py:52] epoch 474, training loss: 7310.47, average training loss: 8937.84, base loss: 15968.51
[INFO 2017-06-30 01:19:59,704 main.py:52] epoch 475, training loss: 8783.38, average training loss: 8937.52, base loss: 15972.56
[INFO 2017-06-30 01:20:02,852 main.py:52] epoch 476, training loss: 8355.50, average training loss: 8936.30, base loss: 15974.83
[INFO 2017-06-30 01:20:06,044 main.py:52] epoch 477, training loss: 7047.54, average training loss: 8932.35, base loss: 15974.02
[INFO 2017-06-30 01:20:09,192 main.py:52] epoch 478, training loss: 7580.62, average training loss: 8929.52, base loss: 15975.52
[INFO 2017-06-30 01:20:12,357 main.py:52] epoch 479, training loss: 8059.86, average training loss: 8927.71, base loss: 15979.52
[INFO 2017-06-30 01:20:15,562 main.py:52] epoch 480, training loss: 7992.22, average training loss: 8925.77, base loss: 15981.23
[INFO 2017-06-30 01:20:18,761 main.py:52] epoch 481, training loss: 7385.40, average training loss: 8922.57, base loss: 15980.00
[INFO 2017-06-30 01:20:21,992 main.py:52] epoch 482, training loss: 7971.63, average training loss: 8920.60, base loss: 15981.88
[INFO 2017-06-30 01:20:25,148 main.py:52] epoch 483, training loss: 6829.65, average training loss: 8916.28, base loss: 15979.24
[INFO 2017-06-30 01:20:28,341 main.py:52] epoch 484, training loss: 7165.04, average training loss: 8912.67, base loss: 15977.33
[INFO 2017-06-30 01:20:31,542 main.py:52] epoch 485, training loss: 7682.85, average training loss: 8910.14, base loss: 15976.34
[INFO 2017-06-30 01:20:34,726 main.py:52] epoch 486, training loss: 7569.43, average training loss: 8907.39, base loss: 15977.48
[INFO 2017-06-30 01:20:37,858 main.py:52] epoch 487, training loss: 7582.78, average training loss: 8904.67, base loss: 15978.65
[INFO 2017-06-30 01:20:41,057 main.py:52] epoch 488, training loss: 7912.53, average training loss: 8902.65, base loss: 15978.81
[INFO 2017-06-30 01:20:44,285 main.py:52] epoch 489, training loss: 7629.67, average training loss: 8900.05, base loss: 15978.89
[INFO 2017-06-30 01:20:47,471 main.py:52] epoch 490, training loss: 7433.45, average training loss: 8897.06, base loss: 15978.35
[INFO 2017-06-30 01:20:50,642 main.py:52] epoch 491, training loss: 6978.86, average training loss: 8893.16, base loss: 15974.93
[INFO 2017-06-30 01:20:53,823 main.py:52] epoch 492, training loss: 7146.68, average training loss: 8889.62, base loss: 15972.98
[INFO 2017-06-30 01:20:57,002 main.py:52] epoch 493, training loss: 7074.81, average training loss: 8885.95, base loss: 15972.20
[INFO 2017-06-30 01:21:00,216 main.py:52] epoch 494, training loss: 7461.00, average training loss: 8883.07, base loss: 15972.84
[INFO 2017-06-30 01:21:03,357 main.py:52] epoch 495, training loss: 7530.47, average training loss: 8880.34, base loss: 15972.69
[INFO 2017-06-30 01:21:06,472 main.py:52] epoch 496, training loss: 7469.47, average training loss: 8877.50, base loss: 15974.02
[INFO 2017-06-30 01:21:09,663 main.py:52] epoch 497, training loss: 7336.27, average training loss: 8874.41, base loss: 15973.88
[INFO 2017-06-30 01:21:12,835 main.py:52] epoch 498, training loss: 7657.04, average training loss: 8871.97, base loss: 15970.22
[INFO 2017-06-30 01:21:16,006 main.py:52] epoch 499, training loss: 7600.38, average training loss: 8869.42, base loss: 15969.29
[INFO 2017-06-30 01:21:16,006 main.py:54] epoch 499, testing
[INFO 2017-06-30 01:21:29,334 main.py:97] average testing loss: 7475.80, base loss: 15908.39
[INFO 2017-06-30 01:21:29,334 main.py:98] improve_loss: 8432.58, improve_percent: 0.53
[INFO 2017-06-30 01:21:29,336 main.py:62] model save to ./model/final.pth
[INFO 2017-06-30 01:21:29,371 main.py:66] current best improved percent: 0.53
[INFO 2017-06-30 01:21:32,560 main.py:52] epoch 500, training loss: 7577.01, average training loss: 8866.84, base loss: 15969.84
[INFO 2017-06-30 01:21:35,761 main.py:52] epoch 501, training loss: 7767.09, average training loss: 8864.65, base loss: 15970.05
[INFO 2017-06-30 01:21:38,946 main.py:52] epoch 502, training loss: 7492.35, average training loss: 8861.92, base loss: 15971.84
[INFO 2017-06-30 01:21:42,134 main.py:52] epoch 503, training loss: 7375.46, average training loss: 8858.98, base loss: 15971.62
[INFO 2017-06-30 01:21:45,301 main.py:52] epoch 504, training loss: 7900.07, average training loss: 8857.08, base loss: 15974.11
[INFO 2017-06-30 01:21:48,425 main.py:52] epoch 505, training loss: 7725.80, average training loss: 8854.84, base loss: 15976.81
[INFO 2017-06-30 01:21:51,603 main.py:52] epoch 506, training loss: 8076.82, average training loss: 8853.31, base loss: 15977.60
[INFO 2017-06-30 01:21:54,785 main.py:52] epoch 507, training loss: 7520.67, average training loss: 8850.68, base loss: 15977.20
[INFO 2017-06-30 01:21:57,972 main.py:52] epoch 508, training loss: 8036.37, average training loss: 8849.08, base loss: 15978.57
[INFO 2017-06-30 01:22:01,151 main.py:52] epoch 509, training loss: 8091.62, average training loss: 8847.60, base loss: 15980.12
[INFO 2017-06-30 01:22:04,384 main.py:52] epoch 510, training loss: 7492.74, average training loss: 8844.95, base loss: 15978.52
[INFO 2017-06-30 01:22:07,529 main.py:52] epoch 511, training loss: 7713.35, average training loss: 8842.74, base loss: 15978.84
[INFO 2017-06-30 01:22:10,670 main.py:52] epoch 512, training loss: 7572.20, average training loss: 8840.26, base loss: 15976.50
[INFO 2017-06-30 01:22:13,897 main.py:52] epoch 513, training loss: 7631.66, average training loss: 8837.91, base loss: 15974.97
[INFO 2017-06-30 01:22:17,040 main.py:52] epoch 514, training loss: 8051.30, average training loss: 8836.38, base loss: 15978.47
[INFO 2017-06-30 01:22:20,234 main.py:52] epoch 515, training loss: 7689.58, average training loss: 8834.16, base loss: 15980.36
[INFO 2017-06-30 01:22:23,424 main.py:52] epoch 516, training loss: 7099.81, average training loss: 8830.80, base loss: 15978.71
[INFO 2017-06-30 01:22:26,579 main.py:52] epoch 517, training loss: 8225.86, average training loss: 8829.64, base loss: 15982.68
[INFO 2017-06-30 01:22:29,747 main.py:52] epoch 518, training loss: 7401.47, average training loss: 8826.88, base loss: 15982.79
[INFO 2017-06-30 01:22:32,943 main.py:52] epoch 519, training loss: 7094.82, average training loss: 8823.55, base loss: 15979.44
[INFO 2017-06-30 01:22:36,179 main.py:52] epoch 520, training loss: 7092.39, average training loss: 8820.23, base loss: 15977.06
[INFO 2017-06-30 01:22:39,398 main.py:52] epoch 521, training loss: 7491.21, average training loss: 8817.68, base loss: 15978.23
[INFO 2017-06-30 01:22:42,560 main.py:52] epoch 522, training loss: 7064.03, average training loss: 8814.33, base loss: 15978.86
[INFO 2017-06-30 01:22:45,729 main.py:52] epoch 523, training loss: 7060.36, average training loss: 8810.98, base loss: 15978.09
[INFO 2017-06-30 01:22:48,914 main.py:52] epoch 524, training loss: 7702.65, average training loss: 8808.87, base loss: 15977.90
[INFO 2017-06-30 01:22:52,115 main.py:52] epoch 525, training loss: 7850.44, average training loss: 8807.05, base loss: 15979.44
[INFO 2017-06-30 01:22:55,301 main.py:52] epoch 526, training loss: 7759.22, average training loss: 8805.06, base loss: 15978.63
[INFO 2017-06-30 01:22:58,545 main.py:52] epoch 527, training loss: 7876.09, average training loss: 8803.30, base loss: 15977.71
[INFO 2017-06-30 01:23:01,709 main.py:52] epoch 528, training loss: 7797.23, average training loss: 8801.40, base loss: 15978.39
[INFO 2017-06-30 01:23:04,857 main.py:52] epoch 529, training loss: 6997.36, average training loss: 8798.00, base loss: 15975.25
[INFO 2017-06-30 01:23:08,088 main.py:52] epoch 530, training loss: 7600.22, average training loss: 8795.74, base loss: 15975.71
[INFO 2017-06-30 01:23:11,264 main.py:52] epoch 531, training loss: 7437.22, average training loss: 8793.19, base loss: 15974.50
[INFO 2017-06-30 01:23:14,433 main.py:52] epoch 532, training loss: 7018.71, average training loss: 8789.86, base loss: 15973.56
[INFO 2017-06-30 01:23:17,611 main.py:52] epoch 533, training loss: 7018.53, average training loss: 8786.54, base loss: 15970.06
[INFO 2017-06-30 01:23:20,771 main.py:52] epoch 534, training loss: 6972.02, average training loss: 8783.15, base loss: 15969.42
[INFO 2017-06-30 01:23:23,932 main.py:52] epoch 535, training loss: 7040.06, average training loss: 8779.90, base loss: 15968.98
[INFO 2017-06-30 01:23:27,109 main.py:52] epoch 536, training loss: 7435.99, average training loss: 8777.40, base loss: 15969.98
[INFO 2017-06-30 01:23:30,348 main.py:52] epoch 537, training loss: 7138.13, average training loss: 8774.35, base loss: 15970.61
[INFO 2017-06-30 01:23:33,544 main.py:52] epoch 538, training loss: 7125.94, average training loss: 8771.29, base loss: 15967.94
[INFO 2017-06-30 01:23:36,773 main.py:52] epoch 539, training loss: 7381.58, average training loss: 8768.72, base loss: 15967.49
[INFO 2017-06-30 01:23:39,989 main.py:52] epoch 540, training loss: 7110.39, average training loss: 8765.65, base loss: 15965.72
[INFO 2017-06-30 01:23:43,186 main.py:52] epoch 541, training loss: 7125.06, average training loss: 8762.62, base loss: 15964.02
[INFO 2017-06-30 01:23:46,363 main.py:52] epoch 542, training loss: 7086.56, average training loss: 8759.54, base loss: 15963.57
[INFO 2017-06-30 01:23:49,511 main.py:52] epoch 543, training loss: 7111.18, average training loss: 8756.51, base loss: 15960.90
[INFO 2017-06-30 01:23:52,683 main.py:52] epoch 544, training loss: 8119.00, average training loss: 8755.34, base loss: 15963.03
[INFO 2017-06-30 01:23:55,831 main.py:52] epoch 545, training loss: 7422.34, average training loss: 8752.90, base loss: 15962.76
[INFO 2017-06-30 01:23:58,970 main.py:52] epoch 546, training loss: 7304.52, average training loss: 8750.25, base loss: 15959.98
[INFO 2017-06-30 01:24:02,124 main.py:52] epoch 547, training loss: 7185.10, average training loss: 8747.39, base loss: 15956.75
[INFO 2017-06-30 01:24:05,272 main.py:52] epoch 548, training loss: 7587.45, average training loss: 8745.28, base loss: 15956.90
[INFO 2017-06-30 01:24:08,425 main.py:52] epoch 549, training loss: 7017.09, average training loss: 8742.14, base loss: 15957.06
[INFO 2017-06-30 01:24:11,602 main.py:52] epoch 550, training loss: 7027.54, average training loss: 8739.03, base loss: 15953.13
[INFO 2017-06-30 01:24:14,781 main.py:52] epoch 551, training loss: 7267.87, average training loss: 8736.36, base loss: 15952.64
[INFO 2017-06-30 01:24:17,923 main.py:52] epoch 552, training loss: 7402.38, average training loss: 8733.95, base loss: 15951.19
[INFO 2017-06-30 01:24:21,112 main.py:52] epoch 553, training loss: 7702.43, average training loss: 8732.09, base loss: 15952.14
[INFO 2017-06-30 01:24:24,265 main.py:52] epoch 554, training loss: 7469.81, average training loss: 8729.81, base loss: 15955.06
[INFO 2017-06-30 01:24:27,495 main.py:52] epoch 555, training loss: 7804.39, average training loss: 8728.15, base loss: 15956.41
[INFO 2017-06-30 01:24:30,634 main.py:52] epoch 556, training loss: 7522.02, average training loss: 8725.98, base loss: 15956.28
[INFO 2017-06-30 01:24:33,792 main.py:52] epoch 557, training loss: 6794.59, average training loss: 8722.52, base loss: 15952.26
[INFO 2017-06-30 01:24:36,972 main.py:52] epoch 558, training loss: 7433.49, average training loss: 8720.22, base loss: 15951.29
[INFO 2017-06-30 01:24:40,141 main.py:52] epoch 559, training loss: 7106.36, average training loss: 8717.33, base loss: 15948.77
[INFO 2017-06-30 01:24:43,308 main.py:52] epoch 560, training loss: 6951.02, average training loss: 8714.19, base loss: 15945.87
[INFO 2017-06-30 01:24:46,510 main.py:52] epoch 561, training loss: 7452.32, average training loss: 8711.94, base loss: 15945.50
[INFO 2017-06-30 01:24:49,701 main.py:52] epoch 562, training loss: 6659.66, average training loss: 8708.29, base loss: 15942.41
[INFO 2017-06-30 01:24:52,905 main.py:52] epoch 563, training loss: 6632.43, average training loss: 8704.61, base loss: 15937.52
[INFO 2017-06-30 01:24:56,078 main.py:52] epoch 564, training loss: 6644.50, average training loss: 8700.97, base loss: 15935.25
[INFO 2017-06-30 01:24:59,222 main.py:52] epoch 565, training loss: 6605.99, average training loss: 8697.27, base loss: 15934.82
[INFO 2017-06-30 01:25:02,387 main.py:52] epoch 566, training loss: 7606.53, average training loss: 8695.34, base loss: 15936.03
[INFO 2017-06-30 01:25:05,571 main.py:52] epoch 567, training loss: 7136.57, average training loss: 8692.60, base loss: 15935.17
[INFO 2017-06-30 01:25:08,744 main.py:52] epoch 568, training loss: 7505.84, average training loss: 8690.51, base loss: 15937.79
[INFO 2017-06-30 01:25:11,891 main.py:52] epoch 569, training loss: 7631.60, average training loss: 8688.65, base loss: 15938.87
[INFO 2017-06-30 01:25:15,044 main.py:52] epoch 570, training loss: 8026.15, average training loss: 8687.49, base loss: 15943.32
[INFO 2017-06-30 01:25:18,236 main.py:52] epoch 571, training loss: 7617.00, average training loss: 8685.62, base loss: 15945.20
[INFO 2017-06-30 01:25:21,406 main.py:52] epoch 572, training loss: 7275.29, average training loss: 8683.16, base loss: 15940.43
[INFO 2017-06-30 01:25:24,566 main.py:52] epoch 573, training loss: 7951.48, average training loss: 8681.89, base loss: 15941.08
[INFO 2017-06-30 01:25:27,721 main.py:52] epoch 574, training loss: 7118.72, average training loss: 8679.17, base loss: 15938.33
[INFO 2017-06-30 01:25:30,873 main.py:52] epoch 575, training loss: 7322.51, average training loss: 8676.81, base loss: 15939.18
[INFO 2017-06-30 01:25:34,057 main.py:52] epoch 576, training loss: 7381.92, average training loss: 8674.57, base loss: 15937.96
[INFO 2017-06-30 01:25:37,266 main.py:52] epoch 577, training loss: 7736.69, average training loss: 8672.95, base loss: 15939.38
[INFO 2017-06-30 01:25:40,426 main.py:52] epoch 578, training loss: 7512.91, average training loss: 8670.94, base loss: 15938.01
[INFO 2017-06-30 01:25:43,585 main.py:52] epoch 579, training loss: 7254.47, average training loss: 8668.50, base loss: 15936.10
[INFO 2017-06-30 01:25:46,702 main.py:52] epoch 580, training loss: 7209.25, average training loss: 8665.99, base loss: 15934.25
[INFO 2017-06-30 01:25:49,913 main.py:52] epoch 581, training loss: 7343.45, average training loss: 8663.72, base loss: 15934.09
[INFO 2017-06-30 01:25:53,054 main.py:52] epoch 582, training loss: 7428.01, average training loss: 8661.60, base loss: 15931.28
[INFO 2017-06-30 01:25:56,200 main.py:52] epoch 583, training loss: 7078.36, average training loss: 8658.89, base loss: 15928.85
[INFO 2017-06-30 01:25:59,344 main.py:52] epoch 584, training loss: 7019.13, average training loss: 8656.08, base loss: 15926.65
[INFO 2017-06-30 01:26:02,465 main.py:52] epoch 585, training loss: 7185.05, average training loss: 8653.57, base loss: 15925.71
[INFO 2017-06-30 01:26:05,664 main.py:52] epoch 586, training loss: 6904.74, average training loss: 8650.59, base loss: 15924.29
[INFO 2017-06-30 01:26:08,811 main.py:52] epoch 587, training loss: 7882.17, average training loss: 8649.29, base loss: 15927.74
[INFO 2017-06-30 01:26:12,021 main.py:52] epoch 588, training loss: 6808.37, average training loss: 8646.16, base loss: 15925.71
[INFO 2017-06-30 01:26:15,265 main.py:52] epoch 589, training loss: 8072.48, average training loss: 8645.19, base loss: 15925.87
[INFO 2017-06-30 01:26:18,437 main.py:52] epoch 590, training loss: 7288.74, average training loss: 8642.89, base loss: 15925.95
[INFO 2017-06-30 01:26:21,567 main.py:52] epoch 591, training loss: 7652.20, average training loss: 8641.22, base loss: 15927.68
[INFO 2017-06-30 01:26:24,774 main.py:52] epoch 592, training loss: 7594.31, average training loss: 8639.45, base loss: 15928.79
[INFO 2017-06-30 01:26:27,943 main.py:52] epoch 593, training loss: 7568.53, average training loss: 8637.65, base loss: 15930.25
[INFO 2017-06-30 01:26:31,165 main.py:52] epoch 594, training loss: 6981.60, average training loss: 8634.87, base loss: 15928.54
[INFO 2017-06-30 01:26:34,327 main.py:52] epoch 595, training loss: 7015.91, average training loss: 8632.15, base loss: 15928.37
[INFO 2017-06-30 01:26:37,489 main.py:52] epoch 596, training loss: 7943.79, average training loss: 8631.00, base loss: 15930.28
[INFO 2017-06-30 01:26:40,687 main.py:52] epoch 597, training loss: 7339.10, average training loss: 8628.84, base loss: 15930.39
[INFO 2017-06-30 01:26:43,858 main.py:52] epoch 598, training loss: 7334.55, average training loss: 8626.68, base loss: 15929.60
[INFO 2017-06-30 01:26:47,056 main.py:52] epoch 599, training loss: 7616.05, average training loss: 8624.99, base loss: 15927.96
[INFO 2017-06-30 01:26:47,057 main.py:54] epoch 599, testing
[INFO 2017-06-30 01:27:00,336 main.py:97] average testing loss: 7435.70, base loss: 16144.37
[INFO 2017-06-30 01:27:00,336 main.py:98] improve_loss: 8708.67, improve_percent: 0.54
[INFO 2017-06-30 01:27:00,338 main.py:62] model save to ./model/final.pth
[INFO 2017-06-30 01:27:00,372 main.py:66] current best improved percent: 0.54
[INFO 2017-06-30 01:27:03,543 main.py:52] epoch 600, training loss: 7289.27, average training loss: 8622.77, base loss: 15928.18
[INFO 2017-06-30 01:27:06,687 main.py:52] epoch 601, training loss: 7239.62, average training loss: 8620.47, base loss: 15929.02
[INFO 2017-06-30 01:27:09,856 main.py:52] epoch 602, training loss: 7882.65, average training loss: 8619.25, base loss: 15929.44
[INFO 2017-06-30 01:27:13,038 main.py:52] epoch 603, training loss: 7723.26, average training loss: 8617.77, base loss: 15929.00
[INFO 2017-06-30 01:27:16,250 main.py:52] epoch 604, training loss: 7643.18, average training loss: 8616.16, base loss: 15932.06
[INFO 2017-06-30 01:27:19,432 main.py:52] epoch 605, training loss: 7193.04, average training loss: 8613.81, base loss: 15934.23
[INFO 2017-06-30 01:27:22,616 main.py:52] epoch 606, training loss: 6699.14, average training loss: 8610.65, base loss: 15931.19
[INFO 2017-06-30 01:27:25,795 main.py:52] epoch 607, training loss: 7104.45, average training loss: 8608.18, base loss: 15929.60
[INFO 2017-06-30 01:27:29,000 main.py:52] epoch 608, training loss: 7260.77, average training loss: 8605.96, base loss: 15929.80
[INFO 2017-06-30 01:27:32,175 main.py:52] epoch 609, training loss: 7419.33, average training loss: 8604.02, base loss: 15930.44
[INFO 2017-06-30 01:27:35,353 main.py:52] epoch 610, training loss: 7325.38, average training loss: 8601.93, base loss: 15929.60
[INFO 2017-06-30 01:27:38,513 main.py:52] epoch 611, training loss: 7091.15, average training loss: 8599.46, base loss: 15930.68
[INFO 2017-06-30 01:27:41,724 main.py:52] epoch 612, training loss: 8437.46, average training loss: 8599.19, base loss: 15934.02
[INFO 2017-06-30 01:27:44,904 main.py:52] epoch 613, training loss: 7110.22, average training loss: 8596.77, base loss: 15933.10
[INFO 2017-06-30 01:27:48,047 main.py:52] epoch 614, training loss: 7170.89, average training loss: 8594.45, base loss: 15932.11
[INFO 2017-06-30 01:27:51,227 main.py:52] epoch 615, training loss: 7697.06, average training loss: 8592.99, base loss: 15933.36
[INFO 2017-06-30 01:27:54,386 main.py:52] epoch 616, training loss: 7041.26, average training loss: 8590.48, base loss: 15928.35
[INFO 2017-06-30 01:27:57,565 main.py:52] epoch 617, training loss: 6629.65, average training loss: 8587.30, base loss: 15921.94
[INFO 2017-06-30 01:28:00,760 main.py:52] epoch 618, training loss: 7154.94, average training loss: 8584.99, base loss: 15921.77
[INFO 2017-06-30 01:28:03,941 main.py:52] epoch 619, training loss: 8200.87, average training loss: 8584.37, base loss: 15925.87
[INFO 2017-06-30 01:28:07,118 main.py:52] epoch 620, training loss: 8353.35, average training loss: 8584.00, base loss: 15927.13
[INFO 2017-06-30 01:28:10,312 main.py:52] epoch 621, training loss: 7376.32, average training loss: 8582.06, base loss: 15926.45
[INFO 2017-06-30 01:28:13,488 main.py:52] epoch 622, training loss: 7130.79, average training loss: 8579.73, base loss: 15924.34
[INFO 2017-06-30 01:28:16,659 main.py:52] epoch 623, training loss: 7195.31, average training loss: 8577.51, base loss: 15922.28
[INFO 2017-06-30 01:28:19,849 main.py:52] epoch 624, training loss: 7646.88, average training loss: 8576.02, base loss: 15922.92
[INFO 2017-06-30 01:28:23,088 main.py:52] epoch 625, training loss: 7545.67, average training loss: 8574.37, base loss: 15926.34
[INFO 2017-06-30 01:28:26,232 main.py:52] epoch 626, training loss: 7096.22, average training loss: 8572.02, base loss: 15926.92
[INFO 2017-06-30 01:28:29,434 main.py:52] epoch 627, training loss: 6716.95, average training loss: 8569.06, base loss: 15924.97
[INFO 2017-06-30 01:28:32,586 main.py:52] epoch 628, training loss: 7779.91, average training loss: 8567.81, base loss: 15927.48
[INFO 2017-06-30 01:28:35,762 main.py:52] epoch 629, training loss: 6547.10, average training loss: 8564.60, base loss: 15924.91
[INFO 2017-06-30 01:28:38,909 main.py:52] epoch 630, training loss: 7533.78, average training loss: 8562.97, base loss: 15925.72
[INFO 2017-06-30 01:28:42,082 main.py:52] epoch 631, training loss: 7219.49, average training loss: 8560.84, base loss: 15924.53
[INFO 2017-06-30 01:28:45,236 main.py:52] epoch 632, training loss: 7431.24, average training loss: 8559.06, base loss: 15924.68
[INFO 2017-06-30 01:28:48,396 main.py:52] epoch 633, training loss: 7192.96, average training loss: 8556.90, base loss: 15926.81
[INFO 2017-06-30 01:28:51,578 main.py:52] epoch 634, training loss: 6910.36, average training loss: 8554.31, base loss: 15925.43
[INFO 2017-06-30 01:28:54,773 main.py:52] epoch 635, training loss: 6631.42, average training loss: 8551.29, base loss: 15922.10
[INFO 2017-06-30 01:28:57,937 main.py:52] epoch 636, training loss: 6928.37, average training loss: 8548.74, base loss: 15922.02
[INFO 2017-06-30 01:29:01,061 main.py:52] epoch 637, training loss: 7349.33, average training loss: 8546.86, base loss: 15922.48
[INFO 2017-06-30 01:29:04,224 main.py:52] epoch 638, training loss: 6983.29, average training loss: 8544.41, base loss: 15920.42
[INFO 2017-06-30 01:29:07,409 main.py:52] epoch 639, training loss: 7188.75, average training loss: 8542.29, base loss: 15918.61
[INFO 2017-06-30 01:29:10,538 main.py:52] epoch 640, training loss: 7085.36, average training loss: 8540.02, base loss: 15917.80
[INFO 2017-06-30 01:29:13,691 main.py:52] epoch 641, training loss: 7278.77, average training loss: 8538.06, base loss: 15918.56
[INFO 2017-06-30 01:29:16,877 main.py:52] epoch 642, training loss: 7225.95, average training loss: 8536.01, base loss: 15916.20
[INFO 2017-06-30 01:29:20,026 main.py:52] epoch 643, training loss: 7895.39, average training loss: 8535.02, base loss: 15916.54
[INFO 2017-06-30 01:29:23,154 main.py:52] epoch 644, training loss: 7374.56, average training loss: 8533.22, base loss: 15916.72
[INFO 2017-06-30 01:29:26,351 main.py:52] epoch 645, training loss: 7511.72, average training loss: 8531.64, base loss: 15916.80
[INFO 2017-06-30 01:29:29,503 main.py:52] epoch 646, training loss: 7584.88, average training loss: 8530.18, base loss: 15918.40
[INFO 2017-06-30 01:29:32,658 main.py:52] epoch 647, training loss: 6654.12, average training loss: 8527.28, base loss: 15918.28
[INFO 2017-06-30 01:29:35,871 main.py:52] epoch 648, training loss: 7627.45, average training loss: 8525.89, base loss: 15917.23
[INFO 2017-06-30 01:29:39,030 main.py:52] epoch 649, training loss: 7319.82, average training loss: 8524.04, base loss: 15916.97
[INFO 2017-06-30 01:29:42,175 main.py:52] epoch 650, training loss: 7209.68, average training loss: 8522.02, base loss: 15916.20
[INFO 2017-06-30 01:29:45,347 main.py:52] epoch 651, training loss: 7015.79, average training loss: 8519.71, base loss: 15914.86
[INFO 2017-06-30 01:29:48,467 main.py:52] epoch 652, training loss: 7282.46, average training loss: 8517.82, base loss: 15914.20
[INFO 2017-06-30 01:29:51,638 main.py:52] epoch 653, training loss: 6803.70, average training loss: 8515.19, base loss: 15909.60
[INFO 2017-06-30 01:29:54,829 main.py:52] epoch 654, training loss: 7611.90, average training loss: 8513.81, base loss: 15909.26
[INFO 2017-06-30 01:29:58,010 main.py:52] epoch 655, training loss: 8048.03, average training loss: 8513.10, base loss: 15910.94
[INFO 2017-06-30 01:30:01,159 main.py:52] epoch 656, training loss: 8094.77, average training loss: 8512.47, base loss: 15912.13
[INFO 2017-06-30 01:30:04,311 main.py:52] epoch 657, training loss: 7043.32, average training loss: 8510.24, base loss: 15908.00
[INFO 2017-06-30 01:30:07,435 main.py:52] epoch 658, training loss: 7776.03, average training loss: 8509.12, base loss: 15909.08
[INFO 2017-06-30 01:30:10,580 main.py:52] epoch 659, training loss: 7582.22, average training loss: 8507.72, base loss: 15908.63
[INFO 2017-06-30 01:30:13,777 main.py:52] epoch 660, training loss: 7693.28, average training loss: 8506.48, base loss: 15909.65
[INFO 2017-06-30 01:30:16,990 main.py:52] epoch 661, training loss: 7566.67, average training loss: 8505.07, base loss: 15910.05
[INFO 2017-06-30 01:30:20,199 main.py:52] epoch 662, training loss: 7986.62, average training loss: 8504.28, base loss: 15909.12
[INFO 2017-06-30 01:30:23,353 main.py:52] epoch 663, training loss: 7416.60, average training loss: 8502.65, base loss: 15906.02
[INFO 2017-06-30 01:30:26,513 main.py:52] epoch 664, training loss: 8095.83, average training loss: 8502.03, base loss: 15907.54
[INFO 2017-06-30 01:30:29,658 main.py:52] epoch 665, training loss: 7336.40, average training loss: 8500.28, base loss: 15906.68
[INFO 2017-06-30 01:30:32,844 main.py:52] epoch 666, training loss: 7566.71, average training loss: 8498.88, base loss: 15905.21
[INFO 2017-06-30 01:30:36,041 main.py:52] epoch 667, training loss: 7363.87, average training loss: 8497.18, base loss: 15904.36
[INFO 2017-06-30 01:30:39,227 main.py:52] epoch 668, training loss: 7939.04, average training loss: 8496.35, base loss: 15905.16
[INFO 2017-06-30 01:30:42,463 main.py:52] epoch 669, training loss: 7889.72, average training loss: 8495.44, base loss: 15907.30
[INFO 2017-06-30 01:30:45,682 main.py:52] epoch 670, training loss: 7233.43, average training loss: 8493.56, base loss: 15904.63
[INFO 2017-06-30 01:30:48,859 main.py:52] epoch 671, training loss: 7272.71, average training loss: 8491.75, base loss: 15901.56
[INFO 2017-06-30 01:30:52,028 main.py:52] epoch 672, training loss: 7078.92, average training loss: 8489.65, base loss: 15901.22
[INFO 2017-06-30 01:30:55,191 main.py:52] epoch 673, training loss: 7888.52, average training loss: 8488.76, base loss: 15903.69
[INFO 2017-06-30 01:30:58,380 main.py:52] epoch 674, training loss: 7541.57, average training loss: 8487.35, base loss: 15904.02
[INFO 2017-06-30 01:31:01,522 main.py:52] epoch 675, training loss: 8031.02, average training loss: 8486.68, base loss: 15905.17
[INFO 2017-06-30 01:31:04,685 main.py:52] epoch 676, training loss: 7252.12, average training loss: 8484.85, base loss: 15904.64
[INFO 2017-06-30 01:31:07,886 main.py:52] epoch 677, training loss: 7173.84, average training loss: 8482.92, base loss: 15904.27
[INFO 2017-06-30 01:31:11,043 main.py:52] epoch 678, training loss: 7855.36, average training loss: 8482.00, base loss: 15905.26
[INFO 2017-06-30 01:31:14,218 main.py:52] epoch 679, training loss: 7795.61, average training loss: 8480.99, base loss: 15905.45
[INFO 2017-06-30 01:31:17,403 main.py:52] epoch 680, training loss: 7743.59, average training loss: 8479.90, base loss: 15906.05
[INFO 2017-06-30 01:31:20,593 main.py:52] epoch 681, training loss: 7525.44, average training loss: 8478.50, base loss: 15905.28
[INFO 2017-06-30 01:31:23,764 main.py:52] epoch 682, training loss: 6874.07, average training loss: 8476.16, base loss: 15904.00
[INFO 2017-06-30 01:31:26,975 main.py:52] epoch 683, training loss: 6809.50, average training loss: 8473.72, base loss: 15902.15
[INFO 2017-06-30 01:31:30,132 main.py:52] epoch 684, training loss: 7489.36, average training loss: 8472.28, base loss: 15902.55
[INFO 2017-06-30 01:31:33,324 main.py:52] epoch 685, training loss: 7180.02, average training loss: 8470.40, base loss: 15901.97
[INFO 2017-06-30 01:31:36,523 main.py:52] epoch 686, training loss: 7068.22, average training loss: 8468.36, base loss: 15900.83
[INFO 2017-06-30 01:31:39,716 main.py:52] epoch 687, training loss: 7257.50, average training loss: 8466.60, base loss: 15898.76
[INFO 2017-06-30 01:31:42,886 main.py:52] epoch 688, training loss: 7497.63, average training loss: 8465.19, base loss: 15901.07
[INFO 2017-06-30 01:31:46,014 main.py:52] epoch 689, training loss: 7216.93, average training loss: 8463.38, base loss: 15901.19
[INFO 2017-06-30 01:31:49,194 main.py:52] epoch 690, training loss: 7538.36, average training loss: 8462.04, base loss: 15900.96
[INFO 2017-06-30 01:31:52,375 main.py:52] epoch 691, training loss: 7523.42, average training loss: 8460.69, base loss: 15900.14
[INFO 2017-06-30 01:31:55,579 main.py:52] epoch 692, training loss: 7211.28, average training loss: 8458.88, base loss: 15901.20
[INFO 2017-06-30 01:31:58,711 main.py:52] epoch 693, training loss: 6746.92, average training loss: 8456.42, base loss: 15900.64
[INFO 2017-06-30 01:32:01,919 main.py:52] epoch 694, training loss: 7177.70, average training loss: 8454.58, base loss: 15898.82
[INFO 2017-06-30 01:32:05,096 main.py:52] epoch 695, training loss: 7540.87, average training loss: 8453.26, base loss: 15899.55
[INFO 2017-06-30 01:32:08,247 main.py:52] epoch 696, training loss: 8188.81, average training loss: 8452.88, base loss: 15902.72
[INFO 2017-06-30 01:32:11,424 main.py:52] epoch 697, training loss: 7077.69, average training loss: 8450.91, base loss: 15901.46
[INFO 2017-06-30 01:32:14,578 main.py:52] epoch 698, training loss: 7338.83, average training loss: 8449.32, base loss: 15902.43
[INFO 2017-06-30 01:32:17,757 main.py:52] epoch 699, training loss: 7835.19, average training loss: 8448.45, base loss: 15903.24
[INFO 2017-06-30 01:32:17,757 main.py:54] epoch 699, testing
[INFO 2017-06-30 01:32:31,049 main.py:97] average testing loss: 7394.22, base loss: 15998.20
[INFO 2017-06-30 01:32:31,050 main.py:98] improve_loss: 8603.98, improve_percent: 0.54
[INFO 2017-06-30 01:32:31,051 main.py:66] current best improved percent: 0.54
[INFO 2017-06-30 01:32:34,213 main.py:52] epoch 700, training loss: 6951.70, average training loss: 8446.31, base loss: 15901.44
[INFO 2017-06-30 01:32:37,373 main.py:52] epoch 701, training loss: 7340.57, average training loss: 8444.74, base loss: 15903.67
[INFO 2017-06-30 01:32:40,522 main.py:52] epoch 702, training loss: 6770.18, average training loss: 8442.35, base loss: 15901.85
[INFO 2017-06-30 01:32:43,692 main.py:52] epoch 703, training loss: 7293.40, average training loss: 8440.72, base loss: 15899.66
[INFO 2017-06-30 01:32:46,887 main.py:52] epoch 704, training loss: 6494.39, average training loss: 8437.96, base loss: 15897.66
[INFO 2017-06-30 01:32:50,028 main.py:52] epoch 705, training loss: 6866.63, average training loss: 8435.74, base loss: 15898.18
[INFO 2017-06-30 01:32:53,164 main.py:52] epoch 706, training loss: 7962.28, average training loss: 8435.07, base loss: 15901.04
[INFO 2017-06-30 01:32:56,313 main.py:52] epoch 707, training loss: 7109.01, average training loss: 8433.19, base loss: 15900.79
[INFO 2017-06-30 01:32:59,463 main.py:52] epoch 708, training loss: 7457.80, average training loss: 8431.82, base loss: 15901.32
[INFO 2017-06-30 01:33:02,617 main.py:52] epoch 709, training loss: 8409.13, average training loss: 8431.79, base loss: 15907.09
[INFO 2017-06-30 01:33:05,833 main.py:52] epoch 710, training loss: 7189.60, average training loss: 8430.04, base loss: 15905.50
[INFO 2017-06-30 01:33:09,010 main.py:52] epoch 711, training loss: 7048.09, average training loss: 8428.10, base loss: 15903.14
[INFO 2017-06-30 01:33:12,194 main.py:52] epoch 712, training loss: 7536.47, average training loss: 8426.85, base loss: 15904.61
[INFO 2017-06-30 01:33:15,330 main.py:52] epoch 713, training loss: 6620.67, average training loss: 8424.32, base loss: 15902.23
[INFO 2017-06-30 01:33:18,467 main.py:52] epoch 714, training loss: 6896.53, average training loss: 8422.18, base loss: 15901.54
[INFO 2017-06-30 01:33:21,608 main.py:52] epoch 715, training loss: 7689.46, average training loss: 8421.16, base loss: 15902.52
[INFO 2017-06-30 01:33:24,796 main.py:52] epoch 716, training loss: 6738.69, average training loss: 8418.81, base loss: 15900.69
[INFO 2017-06-30 01:33:27,975 main.py:52] epoch 717, training loss: 7530.80, average training loss: 8417.57, base loss: 15901.61
[INFO 2017-06-30 01:33:31,133 main.py:52] epoch 718, training loss: 6439.29, average training loss: 8414.82, base loss: 15897.81
[INFO 2017-06-30 01:33:34,307 main.py:52] epoch 719, training loss: 6928.53, average training loss: 8412.76, base loss: 15895.07
[INFO 2017-06-30 01:33:37,463 main.py:52] epoch 720, training loss: 7483.35, average training loss: 8411.47, base loss: 15897.10
[INFO 2017-06-30 01:33:40,599 main.py:52] epoch 721, training loss: 6625.53, average training loss: 8409.00, base loss: 15896.14
[INFO 2017-06-30 01:33:43,774 main.py:52] epoch 722, training loss: 7013.94, average training loss: 8407.07, base loss: 15896.68
[INFO 2017-06-30 01:33:46,937 main.py:52] epoch 723, training loss: 7636.12, average training loss: 8406.00, base loss: 15898.09
[INFO 2017-06-30 01:33:50,105 main.py:52] epoch 724, training loss: 7682.61, average training loss: 8405.00, base loss: 15899.67
[INFO 2017-06-30 01:33:53,282 main.py:52] epoch 725, training loss: 7163.34, average training loss: 8403.29, base loss: 15899.22
[INFO 2017-06-30 01:33:56,478 main.py:52] epoch 726, training loss: 7526.11, average training loss: 8402.09, base loss: 15899.27
[INFO 2017-06-30 01:33:59,721 main.py:52] epoch 727, training loss: 7696.31, average training loss: 8401.12, base loss: 15900.16
[INFO 2017-06-30 01:34:02,898 main.py:52] epoch 728, training loss: 6661.26, average training loss: 8398.73, base loss: 15897.92
[INFO 2017-06-30 01:34:06,101 main.py:52] epoch 729, training loss: 6262.41, average training loss: 8395.80, base loss: 15894.12
[INFO 2017-06-30 01:34:09,242 main.py:52] epoch 730, training loss: 7549.77, average training loss: 8394.65, base loss: 15896.50
[INFO 2017-06-30 01:34:12,474 main.py:52] epoch 731, training loss: 7954.33, average training loss: 8394.04, base loss: 15900.05
[INFO 2017-06-30 01:34:15,655 main.py:52] epoch 732, training loss: 6914.34, average training loss: 8392.03, base loss: 15900.56
[INFO 2017-06-30 01:34:18,864 main.py:52] epoch 733, training loss: 7361.78, average training loss: 8390.62, base loss: 15901.38
[INFO 2017-06-30 01:34:22,063 main.py:52] epoch 734, training loss: 8324.23, average training loss: 8390.53, base loss: 15906.19
[INFO 2017-06-30 01:34:25,188 main.py:52] epoch 735, training loss: 7526.74, average training loss: 8389.36, base loss: 15907.25
[INFO 2017-06-30 01:34:28,392 main.py:52] epoch 736, training loss: 6817.91, average training loss: 8387.23, base loss: 15906.13
[INFO 2017-06-30 01:34:31,562 main.py:52] epoch 737, training loss: 7384.44, average training loss: 8385.87, base loss: 15906.34
[INFO 2017-06-30 01:34:34,781 main.py:52] epoch 738, training loss: 7052.34, average training loss: 8384.06, base loss: 15906.75
[INFO 2017-06-30 01:34:37,981 main.py:52] epoch 739, training loss: 7148.67, average training loss: 8382.39, base loss: 15906.63
[INFO 2017-06-30 01:34:41,149 main.py:52] epoch 740, training loss: 7483.21, average training loss: 8381.18, base loss: 15906.86
[INFO 2017-06-30 01:34:44,330 main.py:52] epoch 741, training loss: 7044.12, average training loss: 8379.38, base loss: 15906.15
[INFO 2017-06-30 01:34:47,534 main.py:52] epoch 742, training loss: 6879.51, average training loss: 8377.36, base loss: 15903.91
[INFO 2017-06-30 01:34:50,672 main.py:52] epoch 743, training loss: 7590.52, average training loss: 8376.30, base loss: 15903.93
[INFO 2017-06-30 01:34:53,873 main.py:52] epoch 744, training loss: 7433.30, average training loss: 8375.04, base loss: 15906.37
[INFO 2017-06-30 01:34:57,040 main.py:52] epoch 745, training loss: 6757.50, average training loss: 8372.87, base loss: 15906.95
[INFO 2017-06-30 01:35:00,197 main.py:52] epoch 746, training loss: 6960.50, average training loss: 8370.98, base loss: 15904.30
[INFO 2017-06-30 01:35:03,323 main.py:52] epoch 747, training loss: 7655.06, average training loss: 8370.02, base loss: 15904.45
[INFO 2017-06-30 01:35:06,543 main.py:52] epoch 748, training loss: 8147.48, average training loss: 8369.72, base loss: 15905.38
[INFO 2017-06-30 01:35:09,702 main.py:52] epoch 749, training loss: 8012.74, average training loss: 8369.25, base loss: 15903.61
[INFO 2017-06-30 01:35:12,868 main.py:52] epoch 750, training loss: 6960.46, average training loss: 8367.37, base loss: 15905.55
[INFO 2017-06-30 01:35:16,061 main.py:52] epoch 751, training loss: 7623.09, average training loss: 8366.38, base loss: 15907.89
[INFO 2017-06-30 01:35:19,183 main.py:52] epoch 752, training loss: 7255.09, average training loss: 8364.91, base loss: 15909.03
[INFO 2017-06-30 01:35:22,374 main.py:52] epoch 753, training loss: 7401.60, average training loss: 8363.63, base loss: 15910.42
[INFO 2017-06-30 01:35:25,562 main.py:52] epoch 754, training loss: 7199.60, average training loss: 8362.09, base loss: 15911.60
[INFO 2017-06-30 01:35:28,738 main.py:52] epoch 755, training loss: 7423.51, average training loss: 8360.84, base loss: 15913.76
[INFO 2017-06-30 01:35:31,914 main.py:52] epoch 756, training loss: 7206.30, average training loss: 8359.32, base loss: 15913.24
[INFO 2017-06-30 01:35:35,175 main.py:52] epoch 757, training loss: 7299.78, average training loss: 8357.92, base loss: 15915.51
[INFO 2017-06-30 01:35:38,367 main.py:52] epoch 758, training loss: 8179.22, average training loss: 8357.69, base loss: 15918.95
[INFO 2017-06-30 01:35:41,539 main.py:52] epoch 759, training loss: 7027.74, average training loss: 8355.94, base loss: 15918.66
[INFO 2017-06-30 01:35:44,692 main.py:52] epoch 760, training loss: 6805.24, average training loss: 8353.90, base loss: 15915.83
[INFO 2017-06-30 01:35:47,835 main.py:52] epoch 761, training loss: 6997.64, average training loss: 8352.12, base loss: 15914.06
[INFO 2017-06-30 01:35:51,023 main.py:52] epoch 762, training loss: 7580.38, average training loss: 8351.11, base loss: 15914.55
[INFO 2017-06-30 01:35:54,166 main.py:52] epoch 763, training loss: 6549.46, average training loss: 8348.75, base loss: 15911.15
[INFO 2017-06-30 01:35:57,352 main.py:52] epoch 764, training loss: 7135.61, average training loss: 8347.16, base loss: 15910.04
[INFO 2017-06-30 01:36:00,522 main.py:52] epoch 765, training loss: 7265.49, average training loss: 8345.75, base loss: 15909.56
[INFO 2017-06-30 01:36:03,708 main.py:52] epoch 766, training loss: 7178.35, average training loss: 8344.23, base loss: 15909.80
[INFO 2017-06-30 01:36:06,857 main.py:52] epoch 767, training loss: 7063.18, average training loss: 8342.56, base loss: 15909.91
[INFO 2017-06-30 01:36:10,010 main.py:52] epoch 768, training loss: 6961.65, average training loss: 8340.77, base loss: 15911.95
[INFO 2017-06-30 01:36:13,146 main.py:52] epoch 769, training loss: 7078.28, average training loss: 8339.13, base loss: 15911.89
[INFO 2017-06-30 01:36:16,358 main.py:52] epoch 770, training loss: 7056.87, average training loss: 8337.46, base loss: 15910.72
[INFO 2017-06-30 01:36:19,583 main.py:52] epoch 771, training loss: 7530.84, average training loss: 8336.42, base loss: 15912.02
[INFO 2017-06-30 01:36:22,788 main.py:52] epoch 772, training loss: 7503.41, average training loss: 8335.34, base loss: 15914.18
[INFO 2017-06-30 01:36:25,968 main.py:52] epoch 773, training loss: 7157.99, average training loss: 8333.82, base loss: 15915.06
[INFO 2017-06-30 01:36:29,134 main.py:52] epoch 774, training loss: 7349.73, average training loss: 8332.55, base loss: 15915.12
[INFO 2017-06-30 01:36:32,293 main.py:52] epoch 775, training loss: 7050.07, average training loss: 8330.90, base loss: 15912.74
[INFO 2017-06-30 01:36:35,470 main.py:52] epoch 776, training loss: 7180.32, average training loss: 8329.42, base loss: 15912.63
[INFO 2017-06-30 01:36:38,654 main.py:52] epoch 777, training loss: 6710.90, average training loss: 8327.34, base loss: 15914.15
[INFO 2017-06-30 01:36:41,803 main.py:52] epoch 778, training loss: 6923.34, average training loss: 8325.53, base loss: 15912.63
[INFO 2017-06-30 01:36:44,951 main.py:52] epoch 779, training loss: 7247.80, average training loss: 8324.15, base loss: 15912.45
[INFO 2017-06-30 01:36:48,141 main.py:52] epoch 780, training loss: 7113.44, average training loss: 8322.60, base loss: 15909.81
[INFO 2017-06-30 01:36:51,312 main.py:52] epoch 781, training loss: 7697.21, average training loss: 8321.80, base loss: 15910.51
[INFO 2017-06-30 01:36:54,493 main.py:52] epoch 782, training loss: 6713.15, average training loss: 8319.75, base loss: 15909.15
[INFO 2017-06-30 01:36:57,668 main.py:52] epoch 783, training loss: 7265.43, average training loss: 8318.40, base loss: 15910.91
[INFO 2017-06-30 01:37:00,866 main.py:52] epoch 784, training loss: 7435.35, average training loss: 8317.28, base loss: 15912.68
[INFO 2017-06-30 01:37:04,053 main.py:52] epoch 785, training loss: 7832.56, average training loss: 8316.66, base loss: 15915.27
[INFO 2017-06-30 01:37:07,243 main.py:52] epoch 786, training loss: 6973.57, average training loss: 8314.95, base loss: 15913.74
[INFO 2017-06-30 01:37:10,413 main.py:52] epoch 787, training loss: 6803.31, average training loss: 8313.04, base loss: 15913.67
[INFO 2017-06-30 01:37:13,590 main.py:52] epoch 788, training loss: 7301.25, average training loss: 8311.75, base loss: 15913.03
[INFO 2017-06-30 01:37:16,794 main.py:52] epoch 789, training loss: 6927.84, average training loss: 8310.00, base loss: 15911.43
[INFO 2017-06-30 01:37:19,947 main.py:52] epoch 790, training loss: 7178.86, average training loss: 8308.57, base loss: 15912.15
[INFO 2017-06-30 01:37:23,131 main.py:52] epoch 791, training loss: 6736.81, average training loss: 8306.59, base loss: 15912.61
[INFO 2017-06-30 01:37:26,307 main.py:52] epoch 792, training loss: 7603.13, average training loss: 8305.70, base loss: 15914.69
[INFO 2017-06-30 01:37:29,467 main.py:52] epoch 793, training loss: 8079.45, average training loss: 8305.41, base loss: 15918.65
[INFO 2017-06-30 01:37:32,647 main.py:52] epoch 794, training loss: 7114.40, average training loss: 8303.92, base loss: 15920.51
[INFO 2017-06-30 01:37:35,780 main.py:52] epoch 795, training loss: 7652.43, average training loss: 8303.10, base loss: 15925.12
[INFO 2017-06-30 01:37:39,007 main.py:52] epoch 796, training loss: 7094.19, average training loss: 8301.58, base loss: 15925.42
[INFO 2017-06-30 01:37:42,157 main.py:52] epoch 797, training loss: 7082.81, average training loss: 8300.05, base loss: 15926.03
[INFO 2017-06-30 01:37:45,318 main.py:52] epoch 798, training loss: 7046.48, average training loss: 8298.49, base loss: 15924.77
[INFO 2017-06-30 01:37:48,510 main.py:52] epoch 799, training loss: 7742.60, average training loss: 8297.79, base loss: 15925.95
[INFO 2017-06-30 01:37:48,510 main.py:54] epoch 799, testing
[INFO 2017-06-30 01:38:01,765 main.py:97] average testing loss: 7131.54, base loss: 16016.67
[INFO 2017-06-30 01:38:01,766 main.py:98] improve_loss: 8885.13, improve_percent: 0.55
[INFO 2017-06-30 01:38:01,767 main.py:62] model save to ./model/final.pth
[INFO 2017-06-30 01:38:01,802 main.py:66] current best improved percent: 0.55
[INFO 2017-06-30 01:38:04,974 main.py:52] epoch 800, training loss: 6849.02, average training loss: 8295.98, base loss: 15923.81
[INFO 2017-06-30 01:38:08,171 main.py:52] epoch 801, training loss: 6533.18, average training loss: 8293.78, base loss: 15919.65
[INFO 2017-06-30 01:38:11,333 main.py:52] epoch 802, training loss: 7023.60, average training loss: 8292.20, base loss: 15919.82
[INFO 2017-06-30 01:38:14,467 main.py:52] epoch 803, training loss: 7024.83, average training loss: 8290.63, base loss: 15918.18
[INFO 2017-06-30 01:38:17,697 main.py:52] epoch 804, training loss: 6831.43, average training loss: 8288.81, base loss: 15916.39
[INFO 2017-06-30 01:38:20,895 main.py:52] epoch 805, training loss: 6417.34, average training loss: 8286.49, base loss: 15914.30
[INFO 2017-06-30 01:38:24,062 main.py:52] epoch 806, training loss: 6598.88, average training loss: 8284.40, base loss: 15913.48
[INFO 2017-06-30 01:38:27,214 main.py:52] epoch 807, training loss: 6641.34, average training loss: 8282.37, base loss: 15913.05
[INFO 2017-06-30 01:38:30,421 main.py:52] epoch 808, training loss: 7180.89, average training loss: 8281.00, base loss: 15912.39
[INFO 2017-06-30 01:38:33,594 main.py:52] epoch 809, training loss: 6950.99, average training loss: 8279.36, base loss: 15910.85
[INFO 2017-06-30 01:38:36,787 main.py:52] epoch 810, training loss: 7039.89, average training loss: 8277.83, base loss: 15909.57
[INFO 2017-06-30 01:38:39,959 main.py:52] epoch 811, training loss: 6980.80, average training loss: 8276.24, base loss: 15909.31
[INFO 2017-06-30 01:38:43,159 main.py:52] epoch 812, training loss: 6822.98, average training loss: 8274.45, base loss: 15909.15
[INFO 2017-06-30 01:38:46,319 main.py:52] epoch 813, training loss: 7143.37, average training loss: 8273.06, base loss: 15909.07
[INFO 2017-06-30 01:38:49,552 main.py:52] epoch 814, training loss: 6626.94, average training loss: 8271.04, base loss: 15908.11
[INFO 2017-06-30 01:38:52,734 main.py:52] epoch 815, training loss: 6614.76, average training loss: 8269.01, base loss: 15907.15
[INFO 2017-06-30 01:38:55,883 main.py:52] epoch 816, training loss: 6945.72, average training loss: 8267.39, base loss: 15906.52
[INFO 2017-06-30 01:38:59,089 main.py:52] epoch 817, training loss: 6820.41, average training loss: 8265.62, base loss: 15905.39
[INFO 2017-06-30 01:39:02,305 main.py:52] epoch 818, training loss: 6893.68, average training loss: 8263.95, base loss: 15903.65
[INFO 2017-06-30 01:39:05,457 main.py:52] epoch 819, training loss: 7142.28, average training loss: 8262.58, base loss: 15902.79
[INFO 2017-06-30 01:39:08,680 main.py:52] epoch 820, training loss: 7136.04, average training loss: 8261.21, base loss: 15899.93
[INFO 2017-06-30 01:39:11,911 main.py:52] epoch 821, training loss: 7079.36, average training loss: 8259.77, base loss: 15897.61
[INFO 2017-06-30 01:39:15,089 main.py:52] epoch 822, training loss: 7017.03, average training loss: 8258.26, base loss: 15897.59
[INFO 2017-06-30 01:39:18,284 main.py:52] epoch 823, training loss: 7692.55, average training loss: 8257.57, base loss: 15899.26
[INFO 2017-06-30 01:39:21,429 main.py:52] epoch 824, training loss: 7565.96, average training loss: 8256.73, base loss: 15901.21
[INFO 2017-06-30 01:39:24,604 main.py:52] epoch 825, training loss: 6807.33, average training loss: 8254.98, base loss: 15901.41
[INFO 2017-06-30 01:39:27,811 main.py:52] epoch 826, training loss: 6508.92, average training loss: 8252.87, base loss: 15897.27
[INFO 2017-06-30 01:39:30,978 main.py:52] epoch 827, training loss: 7385.25, average training loss: 8251.82, base loss: 15895.02
[INFO 2017-06-30 01:39:34,142 main.py:52] epoch 828, training loss: 7412.82, average training loss: 8250.81, base loss: 15895.56
[INFO 2017-06-30 01:39:37,301 main.py:52] epoch 829, training loss: 6631.14, average training loss: 8248.86, base loss: 15894.01
[INFO 2017-06-30 01:39:40,521 main.py:52] epoch 830, training loss: 6932.51, average training loss: 8247.27, base loss: 15892.84
[INFO 2017-06-30 01:39:43,694 main.py:52] epoch 831, training loss: 7280.89, average training loss: 8246.11, base loss: 15892.05
[INFO 2017-06-30 01:39:46,900 main.py:52] epoch 832, training loss: 7097.96, average training loss: 8244.73, base loss: 15890.30
[INFO 2017-06-30 01:39:50,052 main.py:52] epoch 833, training loss: 6916.00, average training loss: 8243.14, base loss: 15886.96
[INFO 2017-06-30 01:39:53,179 main.py:52] epoch 834, training loss: 6299.06, average training loss: 8240.81, base loss: 15885.41
[INFO 2017-06-30 01:39:56,348 main.py:52] epoch 835, training loss: 6683.72, average training loss: 8238.95, base loss: 15884.63
[INFO 2017-06-30 01:39:59,526 main.py:52] epoch 836, training loss: 7234.25, average training loss: 8237.75, base loss: 15883.99
[INFO 2017-06-30 01:40:02,672 main.py:52] epoch 837, training loss: 6748.64, average training loss: 8235.97, base loss: 15881.39
[INFO 2017-06-30 01:40:05,859 main.py:52] epoch 838, training loss: 7568.16, average training loss: 8235.18, base loss: 15881.97
[INFO 2017-06-30 01:40:09,068 main.py:52] epoch 839, training loss: 6906.42, average training loss: 8233.59, base loss: 15882.42
[INFO 2017-06-30 01:40:12,233 main.py:52] epoch 840, training loss: 7583.06, average training loss: 8232.82, base loss: 15884.65
[INFO 2017-06-30 01:40:15,405 main.py:52] epoch 841, training loss: 6879.63, average training loss: 8231.21, base loss: 15885.07
[INFO 2017-06-30 01:40:18,571 main.py:52] epoch 842, training loss: 6947.92, average training loss: 8229.69, base loss: 15884.19
[INFO 2017-06-30 01:40:21,784 main.py:52] epoch 843, training loss: 7181.13, average training loss: 8228.45, base loss: 15884.85
[INFO 2017-06-30 01:40:24,959 main.py:52] epoch 844, training loss: 6560.73, average training loss: 8226.47, base loss: 15883.03
[INFO 2017-06-30 01:40:28,154 main.py:52] epoch 845, training loss: 7473.84, average training loss: 8225.59, base loss: 15884.99
[INFO 2017-06-30 01:40:31,365 main.py:52] epoch 846, training loss: 6940.04, average training loss: 8224.07, base loss: 15883.50
[INFO 2017-06-30 01:40:34,528 main.py:52] epoch 847, training loss: 7694.95, average training loss: 8223.44, base loss: 15886.27
[INFO 2017-06-30 01:40:37,709 main.py:52] epoch 848, training loss: 6974.56, average training loss: 8221.97, base loss: 15885.65
[INFO 2017-06-30 01:40:40,940 main.py:52] epoch 849, training loss: 7311.69, average training loss: 8220.90, base loss: 15886.62
[INFO 2017-06-30 01:40:44,162 main.py:52] epoch 850, training loss: 7557.06, average training loss: 8220.12, base loss: 15887.49
[INFO 2017-06-30 01:40:47,390 main.py:52] epoch 851, training loss: 6979.16, average training loss: 8218.66, base loss: 15887.85
[INFO 2017-06-30 01:40:50,523 main.py:52] epoch 852, training loss: 7342.28, average training loss: 8217.64, base loss: 15887.34
[INFO 2017-06-30 01:40:53,709 main.py:52] epoch 853, training loss: 7341.39, average training loss: 8216.61, base loss: 15884.85
[INFO 2017-06-30 01:40:56,894 main.py:52] epoch 854, training loss: 6265.84, average training loss: 8214.33, base loss: 15880.29
[INFO 2017-06-30 01:41:00,139 main.py:52] epoch 855, training loss: 6824.92, average training loss: 8212.71, base loss: 15877.77
[INFO 2017-06-30 01:41:03,308 main.py:52] epoch 856, training loss: 7388.68, average training loss: 8211.75, base loss: 15878.50
[INFO 2017-06-30 01:41:06,465 main.py:52] epoch 857, training loss: 7056.55, average training loss: 8210.40, base loss: 15878.16
[INFO 2017-06-30 01:41:09,701 main.py:52] epoch 858, training loss: 7272.74, average training loss: 8209.31, base loss: 15878.36
[INFO 2017-06-30 01:41:12,889 main.py:52] epoch 859, training loss: 7353.61, average training loss: 8208.31, base loss: 15877.86
[INFO 2017-06-30 01:41:16,082 main.py:52] epoch 860, training loss: 6643.49, average training loss: 8206.49, base loss: 15876.10
[INFO 2017-06-30 01:41:19,279 main.py:52] epoch 861, training loss: 7166.50, average training loss: 8205.29, base loss: 15878.63
[INFO 2017-06-30 01:41:22,435 main.py:52] epoch 862, training loss: 7026.52, average training loss: 8203.92, base loss: 15878.14
[INFO 2017-06-30 01:41:25,623 main.py:52] epoch 863, training loss: 7004.10, average training loss: 8202.53, base loss: 15876.16
[INFO 2017-06-30 01:41:28,814 main.py:52] epoch 864, training loss: 7326.45, average training loss: 8201.52, base loss: 15877.24
[INFO 2017-06-30 01:41:32,045 main.py:52] epoch 865, training loss: 7408.73, average training loss: 8200.61, base loss: 15879.02
[INFO 2017-06-30 01:41:35,202 main.py:52] epoch 866, training loss: 6901.26, average training loss: 8199.11, base loss: 15879.51
[INFO 2017-06-30 01:41:38,355 main.py:52] epoch 867, training loss: 6819.80, average training loss: 8197.52, base loss: 15878.38
[INFO 2017-06-30 01:41:41,494 main.py:52] epoch 868, training loss: 6876.19, average training loss: 8196.00, base loss: 15878.26
[INFO 2017-06-30 01:41:44,664 main.py:52] epoch 869, training loss: 6280.74, average training loss: 8193.80, base loss: 15876.19
[INFO 2017-06-30 01:41:47,804 main.py:52] epoch 870, training loss: 6874.91, average training loss: 8192.28, base loss: 15874.35
[INFO 2017-06-30 01:41:50,959 main.py:52] epoch 871, training loss: 7273.64, average training loss: 8191.23, base loss: 15874.41
[INFO 2017-06-30 01:41:54,157 main.py:52] epoch 872, training loss: 6967.63, average training loss: 8189.83, base loss: 15872.67
[INFO 2017-06-30 01:41:57,342 main.py:52] epoch 873, training loss: 7177.03, average training loss: 8188.67, base loss: 15874.18
[INFO 2017-06-30 01:42:00,471 main.py:52] epoch 874, training loss: 7863.95, average training loss: 8188.30, base loss: 15878.19
[INFO 2017-06-30 01:42:03,692 main.py:52] epoch 875, training loss: 6960.02, average training loss: 8186.89, base loss: 15877.41
[INFO 2017-06-30 01:42:06,851 main.py:52] epoch 876, training loss: 6776.09, average training loss: 8185.29, base loss: 15876.39
[INFO 2017-06-30 01:42:10,044 main.py:52] epoch 877, training loss: 6502.62, average training loss: 8183.37, base loss: 15875.67
[INFO 2017-06-30 01:42:13,210 main.py:52] epoch 878, training loss: 6735.51, average training loss: 8181.72, base loss: 15875.56
[INFO 2017-06-30 01:42:16,387 main.py:52] epoch 879, training loss: 6947.77, average training loss: 8180.32, base loss: 15874.85
[INFO 2017-06-30 01:42:19,596 main.py:52] epoch 880, training loss: 6492.77, average training loss: 8178.40, base loss: 15871.73
[INFO 2017-06-30 01:42:22,757 main.py:52] epoch 881, training loss: 7553.94, average training loss: 8177.70, base loss: 15871.10
[INFO 2017-06-30 01:42:25,934 main.py:52] epoch 882, training loss: 6683.57, average training loss: 8176.00, base loss: 15867.93
[INFO 2017-06-30 01:42:29,110 main.py:52] epoch 883, training loss: 7011.62, average training loss: 8174.69, base loss: 15866.17
[INFO 2017-06-30 01:42:32,269 main.py:52] epoch 884, training loss: 6661.11, average training loss: 8172.98, base loss: 15864.01
[INFO 2017-06-30 01:42:35,410 main.py:52] epoch 885, training loss: 7518.42, average training loss: 8172.24, base loss: 15865.54
[INFO 2017-06-30 01:42:38,634 main.py:52] epoch 886, training loss: 6599.43, average training loss: 8170.46, base loss: 15864.29
[INFO 2017-06-30 01:42:41,838 main.py:52] epoch 887, training loss: 6938.50, average training loss: 8169.08, base loss: 15863.88
[INFO 2017-06-30 01:42:45,022 main.py:52] epoch 888, training loss: 6696.16, average training loss: 8167.42, base loss: 15861.16
[INFO 2017-06-30 01:42:48,215 main.py:52] epoch 889, training loss: 7555.68, average training loss: 8166.73, base loss: 15863.52
[INFO 2017-06-30 01:42:51,420 main.py:52] epoch 890, training loss: 7360.72, average training loss: 8165.83, base loss: 15865.59
[INFO 2017-06-30 01:42:54,573 main.py:52] epoch 891, training loss: 6628.92, average training loss: 8164.11, base loss: 15864.88
[INFO 2017-06-30 01:42:57,761 main.py:52] epoch 892, training loss: 6577.70, average training loss: 8162.33, base loss: 15862.65
[INFO 2017-06-30 01:43:00,921 main.py:52] epoch 893, training loss: 6623.19, average training loss: 8160.61, base loss: 15861.02
[INFO 2017-06-30 01:43:04,067 main.py:52] epoch 894, training loss: 6719.70, average training loss: 8159.00, base loss: 15859.18
[INFO 2017-06-30 01:43:07,255 main.py:52] epoch 895, training loss: 7518.05, average training loss: 8158.28, base loss: 15861.24
[INFO 2017-06-30 01:43:10,432 main.py:52] epoch 896, training loss: 6806.15, average training loss: 8156.77, base loss: 15859.98
[INFO 2017-06-30 01:43:13,618 main.py:52] epoch 897, training loss: 7076.15, average training loss: 8155.57, base loss: 15859.91
[INFO 2017-06-30 01:43:16,797 main.py:52] epoch 898, training loss: 7210.36, average training loss: 8154.52, base loss: 15861.38
[INFO 2017-06-30 01:43:20,024 main.py:52] epoch 899, training loss: 7098.91, average training loss: 8153.35, base loss: 15863.95
[INFO 2017-06-30 01:43:20,024 main.py:54] epoch 899, testing
[INFO 2017-06-30 01:43:33,385 main.py:97] average testing loss: 7034.40, base loss: 15386.57
[INFO 2017-06-30 01:43:33,385 main.py:98] improve_loss: 8352.18, improve_percent: 0.54
[INFO 2017-06-30 01:43:33,388 main.py:66] current best improved percent: 0.55
[INFO 2017-06-30 01:43:36,553 main.py:52] epoch 900, training loss: 7422.75, average training loss: 8152.54, base loss: 15865.86
[INFO 2017-06-30 01:43:39,756 main.py:52] epoch 901, training loss: 6985.50, average training loss: 8151.24, base loss: 15867.94
[INFO 2017-06-30 01:43:42,917 main.py:52] epoch 902, training loss: 7101.47, average training loss: 8150.08, base loss: 15869.34
[INFO 2017-06-30 01:43:46,057 main.py:52] epoch 903, training loss: 6770.08, average training loss: 8148.55, base loss: 15869.62
[INFO 2017-06-30 01:43:49,241 main.py:52] epoch 904, training loss: 7013.03, average training loss: 8147.30, base loss: 15869.91
[INFO 2017-06-30 01:43:52,434 main.py:52] epoch 905, training loss: 6398.50, average training loss: 8145.37, base loss: 15868.88
[INFO 2017-06-30 01:43:55,630 main.py:52] epoch 906, training loss: 7292.83, average training loss: 8144.43, base loss: 15869.35
[INFO 2017-06-30 01:43:58,805 main.py:52] epoch 907, training loss: 7172.42, average training loss: 8143.36, base loss: 15869.58
[INFO 2017-06-30 01:44:01,921 main.py:52] epoch 908, training loss: 6807.49, average training loss: 8141.89, base loss: 15868.57
[INFO 2017-06-30 01:44:05,082 main.py:52] epoch 909, training loss: 6965.92, average training loss: 8140.60, base loss: 15868.37
[INFO 2017-06-30 01:44:08,230 main.py:52] epoch 910, training loss: 6713.85, average training loss: 8139.03, base loss: 15867.42
[INFO 2017-06-30 01:44:11,412 main.py:52] epoch 911, training loss: 6744.85, average training loss: 8137.50, base loss: 15866.85
[INFO 2017-06-30 01:44:14,578 main.py:52] epoch 912, training loss: 6406.41, average training loss: 8135.61, base loss: 15864.40
[INFO 2017-06-30 01:44:17,788 main.py:52] epoch 913, training loss: 7234.63, average training loss: 8134.62, base loss: 15864.49
[INFO 2017-06-30 01:44:20,957 main.py:52] epoch 914, training loss: 6691.12, average training loss: 8133.04, base loss: 15864.31
[INFO 2017-06-30 01:44:24,092 main.py:52] epoch 915, training loss: 7348.49, average training loss: 8132.19, base loss: 15864.96
[INFO 2017-06-30 01:44:27,234 main.py:52] epoch 916, training loss: 6807.98, average training loss: 8130.74, base loss: 15864.25
[INFO 2017-06-30 01:44:30,389 main.py:52] epoch 917, training loss: 7241.61, average training loss: 8129.77, base loss: 15866.07
[INFO 2017-06-30 01:44:33,550 main.py:52] epoch 918, training loss: 6917.24, average training loss: 8128.45, base loss: 15866.29
[INFO 2017-06-30 01:44:36,715 main.py:52] epoch 919, training loss: 6996.80, average training loss: 8127.22, base loss: 15866.89
[INFO 2017-06-30 01:44:39,875 main.py:52] epoch 920, training loss: 7146.70, average training loss: 8126.16, base loss: 15869.70
[INFO 2017-06-30 01:44:43,061 main.py:52] epoch 921, training loss: 6797.83, average training loss: 8124.72, base loss: 15870.63
[INFO 2017-06-30 01:44:46,179 main.py:52] epoch 922, training loss: 6756.32, average training loss: 8123.24, base loss: 15870.97
[INFO 2017-06-30 01:44:49,363 main.py:52] epoch 923, training loss: 6801.13, average training loss: 8121.80, base loss: 15871.13
[INFO 2017-06-30 01:44:52,523 main.py:52] epoch 924, training loss: 6542.83, average training loss: 8120.10, base loss: 15868.85
[INFO 2017-06-30 01:44:55,723 main.py:52] epoch 925, training loss: 7469.74, average training loss: 8119.40, base loss: 15870.97
[INFO 2017-06-30 01:44:58,853 main.py:52] epoch 926, training loss: 6824.17, average training loss: 8118.00, base loss: 15868.61
[INFO 2017-06-30 01:45:02,008 main.py:52] epoch 927, training loss: 7700.45, average training loss: 8117.55, base loss: 15870.30
[INFO 2017-06-30 01:45:05,186 main.py:52] epoch 928, training loss: 6743.65, average training loss: 8116.07, base loss: 15867.82
[INFO 2017-06-30 01:45:08,340 main.py:52] epoch 929, training loss: 6974.33, average training loss: 8114.84, base loss: 15866.88
[INFO 2017-06-30 01:45:11,503 main.py:52] epoch 930, training loss: 6647.53, average training loss: 8113.27, base loss: 15866.75
[INFO 2017-06-30 01:45:14,623 main.py:52] epoch 931, training loss: 5989.97, average training loss: 8110.99, base loss: 15862.56
[INFO 2017-06-30 01:45:17,780 main.py:52] epoch 932, training loss: 6908.25, average training loss: 8109.70, base loss: 15862.09
[INFO 2017-06-30 01:45:20,926 main.py:52] epoch 933, training loss: 7902.76, average training loss: 8109.48, base loss: 15863.73
[INFO 2017-06-30 01:45:24,102 main.py:52] epoch 934, training loss: 6883.85, average training loss: 8108.17, base loss: 15863.85
[INFO 2017-06-30 01:45:27,258 main.py:52] epoch 935, training loss: 6576.50, average training loss: 8106.53, base loss: 15862.36
[INFO 2017-06-30 01:45:30,473 main.py:52] epoch 936, training loss: 7586.78, average training loss: 8105.97, base loss: 15863.82
[INFO 2017-06-30 01:45:33,640 main.py:52] epoch 937, training loss: 6782.06, average training loss: 8104.56, base loss: 15860.35
[INFO 2017-06-30 01:45:36,825 main.py:52] epoch 938, training loss: 7668.90, average training loss: 8104.10, base loss: 15864.23
[INFO 2017-06-30 01:45:39,974 main.py:52] epoch 939, training loss: 6950.90, average training loss: 8102.87, base loss: 15864.47
[INFO 2017-06-30 01:45:43,157 main.py:52] epoch 940, training loss: 7697.23, average training loss: 8102.44, base loss: 15868.55
[INFO 2017-06-30 01:45:46,323 main.py:52] epoch 941, training loss: 7027.35, average training loss: 8101.30, base loss: 15869.05
[INFO 2017-06-30 01:45:49,503 main.py:52] epoch 942, training loss: 7438.59, average training loss: 8100.60, base loss: 15871.65
[INFO 2017-06-30 01:45:52,646 main.py:52] epoch 943, training loss: 6964.04, average training loss: 8099.39, base loss: 15871.00
[INFO 2017-06-30 01:45:55,878 main.py:52] epoch 944, training loss: 7073.84, average training loss: 8098.31, base loss: 15870.40
[INFO 2017-06-30 01:45:59,049 main.py:52] epoch 945, training loss: 6457.54, average training loss: 8096.57, base loss: 15867.16
[INFO 2017-06-30 01:46:02,222 main.py:52] epoch 946, training loss: 6906.60, average training loss: 8095.32, base loss: 15868.10
[INFO 2017-06-30 01:46:05,407 main.py:52] epoch 947, training loss: 7264.52, average training loss: 8094.44, base loss: 15869.99
[INFO 2017-06-30 01:46:08,576 main.py:52] epoch 948, training loss: 7665.29, average training loss: 8093.99, base loss: 15872.10
[INFO 2017-06-30 01:46:11,754 main.py:52] epoch 949, training loss: 6772.95, average training loss: 8092.60, base loss: 15869.70
[INFO 2017-06-30 01:46:14,920 main.py:52] epoch 950, training loss: 6942.11, average training loss: 8091.39, base loss: 15870.89
[INFO 2017-06-30 01:46:18,080 main.py:52] epoch 951, training loss: 6674.90, average training loss: 8089.90, base loss: 15872.72
[INFO 2017-06-30 01:46:21,241 main.py:52] epoch 952, training loss: 6955.58, average training loss: 8088.71, base loss: 15872.87
[INFO 2017-06-30 01:46:24,440 main.py:52] epoch 953, training loss: 6672.57, average training loss: 8087.23, base loss: 15873.21
[INFO 2017-06-30 01:46:27,639 main.py:52] epoch 954, training loss: 7149.26, average training loss: 8086.24, base loss: 15875.32
[INFO 2017-06-30 01:46:30,829 main.py:52] epoch 955, training loss: 7563.85, average training loss: 8085.70, base loss: 15877.80
[INFO 2017-06-30 01:46:34,038 main.py:52] epoch 956, training loss: 6611.73, average training loss: 8084.16, base loss: 15875.68
[INFO 2017-06-30 01:46:37,225 main.py:52] epoch 957, training loss: 7645.59, average training loss: 8083.70, base loss: 15876.36
[INFO 2017-06-30 01:46:40,400 main.py:52] epoch 958, training loss: 6932.43, average training loss: 8082.50, base loss: 15874.00
[INFO 2017-06-30 01:46:43,536 main.py:52] epoch 959, training loss: 7018.05, average training loss: 8081.39, base loss: 15871.54
[INFO 2017-06-30 01:46:46,690 main.py:52] epoch 960, training loss: 7531.25, average training loss: 8080.82, base loss: 15873.92
[INFO 2017-06-30 01:46:49,837 main.py:52] epoch 961, training loss: 6434.50, average training loss: 8079.11, base loss: 15873.03
[INFO 2017-06-30 01:46:52,973 main.py:52] epoch 962, training loss: 6990.86, average training loss: 8077.98, base loss: 15871.43
[INFO 2017-06-30 01:46:56,215 main.py:52] epoch 963, training loss: 7218.29, average training loss: 8077.08, base loss: 15871.96
[INFO 2017-06-30 01:46:59,370 main.py:52] epoch 964, training loss: 6814.90, average training loss: 8075.78, base loss: 15870.75
[INFO 2017-06-30 01:47:02,600 main.py:52] epoch 965, training loss: 7651.54, average training loss: 8075.34, base loss: 15872.15
[INFO 2017-06-30 01:47:05,755 main.py:52] epoch 966, training loss: 7112.28, average training loss: 8074.34, base loss: 15874.75
[INFO 2017-06-30 01:47:08,918 main.py:52] epoch 967, training loss: 7263.84, average training loss: 8073.50, base loss: 15875.77
[INFO 2017-06-30 01:47:12,092 main.py:52] epoch 968, training loss: 6582.24, average training loss: 8071.96, base loss: 15874.96
[INFO 2017-06-30 01:47:15,275 main.py:52] epoch 969, training loss: 6358.08, average training loss: 8070.20, base loss: 15873.77
[INFO 2017-06-30 01:47:18,453 main.py:52] epoch 970, training loss: 7710.81, average training loss: 8069.83, base loss: 15874.86
[INFO 2017-06-30 01:47:21,618 main.py:52] epoch 971, training loss: 6752.30, average training loss: 8068.47, base loss: 15873.26
[INFO 2017-06-30 01:47:24,827 main.py:52] epoch 972, training loss: 7056.93, average training loss: 8067.43, base loss: 15873.69
[INFO 2017-06-30 01:47:28,034 main.py:52] epoch 973, training loss: 6699.60, average training loss: 8066.03, base loss: 15873.26
[INFO 2017-06-30 01:47:31,232 main.py:52] epoch 974, training loss: 7043.19, average training loss: 8064.98, base loss: 15874.40
[INFO 2017-06-30 01:47:34,391 main.py:52] epoch 975, training loss: 7881.60, average training loss: 8064.79, base loss: 15878.75
[INFO 2017-06-30 01:47:37,558 main.py:52] epoch 976, training loss: 6753.11, average training loss: 8063.45, base loss: 15879.01
[INFO 2017-06-30 01:47:40,700 main.py:52] epoch 977, training loss: 6807.79, average training loss: 8062.16, base loss: 15879.27
[INFO 2017-06-30 01:47:43,864 main.py:52] epoch 978, training loss: 6918.65, average training loss: 8061.00, base loss: 15879.72
[INFO 2017-06-30 01:47:47,020 main.py:52] epoch 979, training loss: 6599.53, average training loss: 8059.51, base loss: 15877.40
[INFO 2017-06-30 01:47:50,259 main.py:52] epoch 980, training loss: 6925.00, average training loss: 8058.35, base loss: 15877.99
[INFO 2017-06-30 01:47:53,441 main.py:52] epoch 981, training loss: 6810.70, average training loss: 8057.08, base loss: 15878.01
[INFO 2017-06-30 01:47:56,599 main.py:52] epoch 982, training loss: 7941.70, average training loss: 8056.96, base loss: 15880.63
[INFO 2017-06-30 01:47:59,773 main.py:52] epoch 983, training loss: 7174.72, average training loss: 8056.06, base loss: 15883.01
[INFO 2017-06-30 01:48:02,948 main.py:52] epoch 984, training loss: 7417.44, average training loss: 8055.42, base loss: 15883.94
[INFO 2017-06-30 01:48:06,113 main.py:52] epoch 985, training loss: 7074.01, average training loss: 8054.42, base loss: 15883.89
[INFO 2017-06-30 01:48:09,266 main.py:52] epoch 986, training loss: 6944.58, average training loss: 8053.30, base loss: 15883.72
[INFO 2017-06-30 01:48:12,448 main.py:52] epoch 987, training loss: 6488.23, average training loss: 8051.71, base loss: 15882.52
[INFO 2017-06-30 01:48:15,605 main.py:52] epoch 988, training loss: 7268.43, average training loss: 8050.92, base loss: 15883.34
[INFO 2017-06-30 01:48:18,758 main.py:52] epoch 989, training loss: 6985.59, average training loss: 8049.84, base loss: 15883.32
[INFO 2017-06-30 01:48:21,921 main.py:52] epoch 990, training loss: 7505.41, average training loss: 8049.29, base loss: 15886.78
[INFO 2017-06-30 01:48:25,114 main.py:52] epoch 991, training loss: 6167.01, average training loss: 8047.40, base loss: 15885.43
[INFO 2017-06-30 01:48:28,288 main.py:52] epoch 992, training loss: 6520.34, average training loss: 8045.86, base loss: 15884.14
[INFO 2017-06-30 01:48:31,492 main.py:52] epoch 993, training loss: 7185.43, average training loss: 8044.99, base loss: 15883.87
[INFO 2017-06-30 01:48:34,669 main.py:52] epoch 994, training loss: 7754.98, average training loss: 8044.70, base loss: 15886.24
[INFO 2017-06-30 01:48:37,842 main.py:52] epoch 995, training loss: 6572.57, average training loss: 8043.22, base loss: 15884.98
[INFO 2017-06-30 01:48:40,988 main.py:52] epoch 996, training loss: 7179.99, average training loss: 8042.36, base loss: 15884.61
[INFO 2017-06-30 01:48:44,150 main.py:52] epoch 997, training loss: 6373.05, average training loss: 8040.69, base loss: 15882.68
[INFO 2017-06-30 01:48:47,335 main.py:52] epoch 998, training loss: 7110.56, average training loss: 8039.75, base loss: 15881.73
[INFO 2017-06-30 01:48:50,481 main.py:52] epoch 999, training loss: 6692.87, average training loss: 8038.41, base loss: 15878.66
[INFO 2017-06-30 01:48:50,481 main.py:54] epoch 999, testing
[INFO 2017-06-30 01:49:03,881 main.py:97] average testing loss: 7110.47, base loss: 16092.75
[INFO 2017-06-30 01:49:03,882 main.py:98] improve_loss: 8982.28, improve_percent: 0.56
[INFO 2017-06-30 01:49:03,883 main.py:62] model save to ./model/final.pth
[INFO 2017-06-30 01:49:03,918 main.py:66] current best improved percent: 0.56
[INFO 2017-06-30 01:49:07,037 main.py:52] epoch 1000, training loss: 6605.89, average training loss: 8005.38, base loss: 15877.43
[INFO 2017-06-30 01:49:10,184 main.py:52] epoch 1001, training loss: 7820.70, average training loss: 7979.96, base loss: 15878.89
[INFO 2017-06-30 01:49:13,343 main.py:52] epoch 1002, training loss: 6995.84, average training loss: 7956.60, base loss: 15879.47
[INFO 2017-06-30 01:49:16,524 main.py:52] epoch 1003, training loss: 6842.95, average training loss: 7937.14, base loss: 15879.65
[INFO 2017-06-30 01:49:19,685 main.py:52] epoch 1004, training loss: 7237.42, average training loss: 7921.09, base loss: 15880.33
[INFO 2017-06-30 01:49:22,840 main.py:52] epoch 1005, training loss: 6491.08, average training loss: 7906.39, base loss: 15878.65
[INFO 2017-06-30 01:49:26,002 main.py:52] epoch 1006, training loss: 6923.51, average training loss: 7893.42, base loss: 15877.18
[INFO 2017-06-30 01:49:29,189 main.py:52] epoch 1007, training loss: 6862.54, average training loss: 7882.67, base loss: 15876.42
[INFO 2017-06-30 01:49:32,364 main.py:52] epoch 1008, training loss: 6673.85, average training loss: 7875.07, base loss: 15876.18
[INFO 2017-06-30 01:49:35,527 main.py:52] epoch 1009, training loss: 7437.36, average training loss: 7866.06, base loss: 15879.06
[INFO 2017-06-30 01:49:38,691 main.py:52] epoch 1010, training loss: 7281.00, average training loss: 7858.60, base loss: 15878.06
[INFO 2017-06-30 01:49:41,842 main.py:52] epoch 1011, training loss: 7050.12, average training loss: 7851.81, base loss: 15876.08
[INFO 2017-06-30 01:49:45,026 main.py:52] epoch 1012, training loss: 6790.45, average training loss: 7845.37, base loss: 15875.52
[INFO 2017-06-30 01:49:48,205 main.py:52] epoch 1013, training loss: 6906.79, average training loss: 7838.86, base loss: 15874.97
[INFO 2017-06-30 01:49:51,353 main.py:52] epoch 1014, training loss: 7197.85, average training loss: 7833.11, base loss: 15877.16
[INFO 2017-06-30 01:49:54,509 main.py:52] epoch 1015, training loss: 7155.92, average training loss: 7827.83, base loss: 15877.52
[INFO 2017-06-30 01:49:57,662 main.py:52] epoch 1016, training loss: 7667.54, average training loss: 7823.00, base loss: 15880.91
[INFO 2017-06-30 01:50:00,826 main.py:52] epoch 1017, training loss: 6712.38, average training loss: 7817.53, base loss: 15880.98
[INFO 2017-06-30 01:50:03,990 main.py:52] epoch 1018, training loss: 6815.92, average training loss: 7813.40, base loss: 15881.86
[INFO 2017-06-30 01:50:07,149 main.py:52] epoch 1019, training loss: 6738.89, average training loss: 7808.25, base loss: 15880.08
[INFO 2017-06-30 01:50:10,305 main.py:52] epoch 1020, training loss: 6758.21, average training loss: 7802.56, base loss: 15877.55
[INFO 2017-06-30 01:50:13,460 main.py:52] epoch 1021, training loss: 6962.49, average training loss: 7797.64, base loss: 15876.86
[INFO 2017-06-30 01:50:16,657 main.py:52] epoch 1022, training loss: 6943.65, average training loss: 7793.28, base loss: 15876.65
[INFO 2017-06-30 01:50:19,836 main.py:52] epoch 1023, training loss: 6954.47, average training loss: 7788.65, base loss: 15875.24
[INFO 2017-06-30 01:50:23,016 main.py:52] epoch 1024, training loss: 7114.28, average training loss: 7785.00, base loss: 15875.98
[INFO 2017-06-30 01:50:26,207 main.py:52] epoch 1025, training loss: 7239.45, average training loss: 7780.86, base loss: 15877.11
[INFO 2017-06-30 01:50:29,427 main.py:52] epoch 1026, training loss: 6643.31, average training loss: 7776.81, base loss: 15875.05
[INFO 2017-06-30 01:50:32,593 main.py:52] epoch 1027, training loss: 7040.35, average training loss: 7773.39, base loss: 15876.63
[INFO 2017-06-30 01:50:35,777 main.py:52] epoch 1028, training loss: 6978.26, average training loss: 7769.29, base loss: 15877.69
[INFO 2017-06-30 01:50:38,930 main.py:52] epoch 1029, training loss: 6761.02, average training loss: 7765.61, base loss: 15877.32
[INFO 2017-06-30 01:50:42,123 main.py:52] epoch 1030, training loss: 6691.69, average training loss: 7760.57, base loss: 15875.00
[INFO 2017-06-30 01:50:45,304 main.py:52] epoch 1031, training loss: 7120.54, average training loss: 7757.65, base loss: 15877.33
[INFO 2017-06-30 01:50:48,497 main.py:52] epoch 1032, training loss: 6575.38, average training loss: 7753.77, base loss: 15877.79
[INFO 2017-06-30 01:50:51,652 main.py:52] epoch 1033, training loss: 7454.11, average training loss: 7750.34, base loss: 15879.91
[INFO 2017-06-30 01:50:54,809 main.py:52] epoch 1034, training loss: 7078.84, average training loss: 7747.56, base loss: 15880.29
[INFO 2017-06-30 01:50:57,994 main.py:52] epoch 1035, training loss: 6800.31, average training loss: 7744.78, base loss: 15879.77
[INFO 2017-06-30 01:51:01,132 main.py:52] epoch 1036, training loss: 6400.21, average training loss: 7738.87, base loss: 15878.12
[INFO 2017-06-30 01:51:04,295 main.py:52] epoch 1037, training loss: 7534.98, average training loss: 7735.08, base loss: 15878.05
[INFO 2017-06-30 01:51:07,480 main.py:52] epoch 1038, training loss: 7583.46, average training loss: 7731.22, base loss: 15881.43
[INFO 2017-06-30 01:51:10,629 main.py:52] epoch 1039, training loss: 6936.06, average training loss: 7726.86, base loss: 15882.21
[INFO 2017-06-30 01:51:13,820 main.py:52] epoch 1040, training loss: 6862.54, average training loss: 7723.16, base loss: 15884.09
[INFO 2017-06-30 01:51:17,000 main.py:52] epoch 1041, training loss: 6859.17, average training loss: 7718.67, base loss: 15884.91
[INFO 2017-06-30 01:51:20,179 main.py:52] epoch 1042, training loss: 7340.39, average training loss: 7715.46, base loss: 15886.58
[INFO 2017-06-30 01:51:23,385 main.py:52] epoch 1043, training loss: 7052.53, average training loss: 7712.40, base loss: 15887.30
[INFO 2017-06-30 01:51:26,601 main.py:52] epoch 1044, training loss: 6579.00, average training loss: 7708.20, base loss: 15887.27
[INFO 2017-06-30 01:51:29,792 main.py:52] epoch 1045, training loss: 6813.96, average training loss: 7704.60, base loss: 15886.93
[INFO 2017-06-30 01:51:32,962 main.py:52] epoch 1046, training loss: 6563.93, average training loss: 7700.93, base loss: 15885.13
[INFO 2017-06-30 01:51:36,143 main.py:52] epoch 1047, training loss: 6904.78, average training loss: 7697.31, base loss: 15884.60
[INFO 2017-06-30 01:51:39,314 main.py:52] epoch 1048, training loss: 7174.25, average training loss: 7694.31, base loss: 15885.53
[INFO 2017-06-30 01:51:42,500 main.py:52] epoch 1049, training loss: 7217.85, average training loss: 7689.91, base loss: 15886.33
[INFO 2017-06-30 01:51:45,646 main.py:52] epoch 1050, training loss: 7076.24, average training loss: 7686.65, base loss: 15886.66
[INFO 2017-06-30 01:51:48,794 main.py:52] epoch 1051, training loss: 6812.04, average training loss: 7681.73, base loss: 15885.29
[INFO 2017-06-30 01:51:51,984 main.py:52] epoch 1052, training loss: 6956.32, average training loss: 7679.43, base loss: 15885.35
[INFO 2017-06-30 01:51:55,118 main.py:52] epoch 1053, training loss: 7345.04, average training loss: 7675.72, base loss: 15888.40
[INFO 2017-06-30 01:51:58,357 main.py:52] epoch 1054, training loss: 7501.67, average training loss: 7673.56, base loss: 15891.91
[INFO 2017-06-30 01:52:01,549 main.py:52] epoch 1055, training loss: 6833.13, average training loss: 7670.97, base loss: 15892.71
[INFO 2017-06-30 01:52:04,730 main.py:52] epoch 1056, training loss: 7063.53, average training loss: 7667.48, base loss: 15891.84
[INFO 2017-06-30 01:52:07,882 main.py:52] epoch 1057, training loss: 7018.86, average training loss: 7663.87, base loss: 15890.35
[INFO 2017-06-30 01:52:11,050 main.py:52] epoch 1058, training loss: 6874.76, average training loss: 7658.87, base loss: 15891.26
[INFO 2017-06-30 01:52:14,217 main.py:52] epoch 1059, training loss: 7080.72, average training loss: 7655.91, base loss: 15890.83
[INFO 2017-06-30 01:52:17,380 main.py:52] epoch 1060, training loss: 6688.25, average training loss: 7652.07, base loss: 15889.57
[INFO 2017-06-30 01:52:20,529 main.py:52] epoch 1061, training loss: 7232.87, average training loss: 7649.29, base loss: 15889.64
[INFO 2017-06-30 01:52:23,754 main.py:52] epoch 1062, training loss: 6463.86, average training loss: 7645.41, base loss: 15886.96
[INFO 2017-06-30 01:52:26,932 main.py:52] epoch 1063, training loss: 7061.99, average training loss: 7643.03, base loss: 15886.44
[INFO 2017-06-30 01:52:30,066 main.py:52] epoch 1064, training loss: 6550.06, average training loss: 7639.54, base loss: 15885.94
[INFO 2017-06-30 01:52:33,229 main.py:52] epoch 1065, training loss: 6791.37, average training loss: 7636.20, base loss: 15886.80
[INFO 2017-06-30 01:52:36,385 main.py:52] epoch 1066, training loss: 6894.64, average training loss: 7633.97, base loss: 15887.41
[INFO 2017-06-30 01:52:39,539 main.py:52] epoch 1067, training loss: 7086.42, average training loss: 7630.69, base loss: 15887.20
[INFO 2017-06-30 01:52:42,743 main.py:52] epoch 1068, training loss: 6775.30, average training loss: 7628.07, base loss: 15886.50
[INFO 2017-06-30 01:52:45,924 main.py:52] epoch 1069, training loss: 6664.05, average training loss: 7623.90, base loss: 15887.26
[INFO 2017-06-30 01:52:49,093 main.py:52] epoch 1070, training loss: 7731.50, average training loss: 7621.80, base loss: 15888.93
[INFO 2017-06-30 01:52:52,275 main.py:52] epoch 1071, training loss: 7128.90, average training loss: 7617.25, base loss: 15891.48
[INFO 2017-06-30 01:52:55,420 main.py:52] epoch 1072, training loss: 7153.26, average training loss: 7614.18, base loss: 15893.58
[INFO 2017-06-30 01:52:58,596 main.py:52] epoch 1073, training loss: 6700.81, average training loss: 7612.24, base loss: 15893.99
[INFO 2017-06-30 01:53:01,780 main.py:52] epoch 1074, training loss: 6986.54, average training loss: 7609.64, base loss: 15893.83
[INFO 2017-06-30 01:53:04,991 main.py:52] epoch 1075, training loss: 7023.25, average training loss: 7607.13, base loss: 15893.60
[INFO 2017-06-30 01:53:08,179 main.py:52] epoch 1076, training loss: 6660.98, average training loss: 7603.95, base loss: 15891.87
[INFO 2017-06-30 01:53:11,358 main.py:52] epoch 1077, training loss: 7397.05, average training loss: 7600.94, base loss: 15891.56
[INFO 2017-06-30 01:53:14,493 main.py:52] epoch 1078, training loss: 6370.19, average training loss: 7596.36, base loss: 15890.65
[INFO 2017-06-30 01:53:17,660 main.py:52] epoch 1079, training loss: 7380.61, average training loss: 7593.83, base loss: 15893.07
[INFO 2017-06-30 01:53:20,823 main.py:52] epoch 1080, training loss: 6991.35, average training loss: 7591.32, base loss: 15894.21
[INFO 2017-06-30 01:53:23,974 main.py:52] epoch 1081, training loss: 6785.40, average training loss: 7586.93, base loss: 15893.23
[INFO 2017-06-30 01:53:27,120 main.py:52] epoch 1082, training loss: 7237.13, average training loss: 7584.08, base loss: 15895.46
[INFO 2017-06-30 01:53:30,279 main.py:52] epoch 1083, training loss: 6511.64, average training loss: 7581.36, base loss: 15894.01
[INFO 2017-06-30 01:53:33,416 main.py:52] epoch 1084, training loss: 6745.52, average training loss: 7579.08, base loss: 15893.83
[INFO 2017-06-30 01:53:36,607 main.py:52] epoch 1085, training loss: 6985.72, average training loss: 7576.01, base loss: 15895.78
[INFO 2017-06-30 01:53:39,780 main.py:52] epoch 1086, training loss: 6344.08, average training loss: 7573.16, base loss: 15893.15
[INFO 2017-06-30 01:53:42,928 main.py:52] epoch 1087, training loss: 6375.89, average training loss: 7569.00, base loss: 15891.61
[INFO 2017-06-30 01:53:46,077 main.py:52] epoch 1088, training loss: 6369.64, average training loss: 7567.02, base loss: 15891.75
[INFO 2017-06-30 01:53:49,281 main.py:52] epoch 1089, training loss: 7278.32, average training loss: 7564.06, base loss: 15895.20
[INFO 2017-06-30 01:53:52,436 main.py:52] epoch 1090, training loss: 6847.21, average training loss: 7560.57, base loss: 15894.81
[INFO 2017-06-30 01:53:55,600 main.py:52] epoch 1091, training loss: 6641.54, average training loss: 7558.67, base loss: 15894.03
[INFO 2017-06-30 01:53:58,741 main.py:52] epoch 1092, training loss: 6907.47, average training loss: 7555.81, base loss: 15896.19
[INFO 2017-06-30 01:54:01,907 main.py:52] epoch 1093, training loss: 7171.08, average training loss: 7553.10, base loss: 15898.12
[INFO 2017-06-30 01:54:05,095 main.py:52] epoch 1094, training loss: 7072.03, average training loss: 7551.15, base loss: 15900.13
[INFO 2017-06-30 01:54:08,265 main.py:52] epoch 1095, training loss: 6730.05, average training loss: 7548.31, base loss: 15900.18
[INFO 2017-06-30 01:54:11,448 main.py:52] epoch 1096, training loss: 6424.53, average training loss: 7546.15, base loss: 15898.19
[INFO 2017-06-30 01:54:14,666 main.py:52] epoch 1097, training loss: 7813.83, average training loss: 7544.47, base loss: 15900.95
[INFO 2017-06-30 01:54:17,849 main.py:52] epoch 1098, training loss: 6785.25, average training loss: 7541.95, base loss: 15900.23
[INFO 2017-06-30 01:54:21,012 main.py:52] epoch 1099, training loss: 6494.61, average training loss: 7539.00, base loss: 15901.02
[INFO 2017-06-30 01:54:21,013 main.py:54] epoch 1099, testing
[INFO 2017-06-30 01:54:34,313 main.py:97] average testing loss: 7032.61, base loss: 16413.80
[INFO 2017-06-30 01:54:34,313 main.py:98] improve_loss: 9381.18, improve_percent: 0.57
[INFO 2017-06-30 01:54:34,315 main.py:62] model save to ./model/final.pth
[INFO 2017-06-30 01:54:34,350 main.py:66] current best improved percent: 0.57
[INFO 2017-06-30 01:54:37,495 main.py:52] epoch 1100, training loss: 7147.44, average training loss: 7537.88, base loss: 15902.17
[INFO 2017-06-30 01:54:40,666 main.py:52] epoch 1101, training loss: 6679.77, average training loss: 7535.91, base loss: 15901.77
[INFO 2017-06-30 01:54:43,832 main.py:52] epoch 1102, training loss: 6879.29, average training loss: 7534.18, base loss: 15901.71
[INFO 2017-06-30 01:54:47,012 main.py:52] epoch 1103, training loss: 6841.66, average training loss: 7531.86, base loss: 15902.63
[INFO 2017-06-30 01:54:50,142 main.py:52] epoch 1104, training loss: 6907.77, average training loss: 7529.65, base loss: 15903.51
[INFO 2017-06-30 01:54:53,329 main.py:52] epoch 1105, training loss: 6596.07, average training loss: 7527.16, base loss: 15903.63
[INFO 2017-06-30 01:54:56,496 main.py:52] epoch 1106, training loss: 7588.80, average training loss: 7525.40, base loss: 15905.68
[INFO 2017-06-30 01:54:59,642 main.py:52] epoch 1107, training loss: 6698.61, average training loss: 7523.04, base loss: 15905.24
[INFO 2017-06-30 01:55:02,800 main.py:52] epoch 1108, training loss: 6545.31, average training loss: 7521.26, base loss: 15905.62
[INFO 2017-06-30 01:55:05,981 main.py:52] epoch 1109, training loss: 6944.13, average training loss: 7518.71, base loss: 15906.96
[INFO 2017-06-30 01:55:09,158 main.py:52] epoch 1110, training loss: 6511.27, average training loss: 7517.67, base loss: 15905.25
[INFO 2017-06-30 01:55:12,310 main.py:52] epoch 1111, training loss: 6727.68, average training loss: 7515.42, base loss: 15903.79
[INFO 2017-06-30 01:55:15,498 main.py:52] epoch 1112, training loss: 7462.82, average training loss: 7513.38, base loss: 15906.24
[INFO 2017-06-30 01:55:18,634 main.py:52] epoch 1113, training loss: 6580.65, average training loss: 7510.01, base loss: 15904.73
[INFO 2017-06-30 01:55:21,799 main.py:52] epoch 1114, training loss: 7116.46, average training loss: 7508.06, base loss: 15904.11
[INFO 2017-06-30 01:55:25,008 main.py:52] epoch 1115, training loss: 7556.37, average training loss: 7507.14, base loss: 15905.70
[INFO 2017-06-30 01:55:28,191 main.py:52] epoch 1116, training loss: 7017.02, average training loss: 7504.98, base loss: 15906.12
[INFO 2017-06-30 01:55:31,352 main.py:52] epoch 1117, training loss: 7373.56, average training loss: 7503.26, base loss: 15906.95
[INFO 2017-06-30 01:55:34,502 main.py:52] epoch 1118, training loss: 6555.06, average training loss: 7500.58, base loss: 15905.92
[INFO 2017-06-30 01:55:37,664 main.py:52] epoch 1119, training loss: 7708.16, average training loss: 7499.95, base loss: 15907.99
[INFO 2017-06-30 01:55:40,853 main.py:52] epoch 1120, training loss: 6621.53, average training loss: 7497.50, base loss: 15906.96
[INFO 2017-06-30 01:55:44,042 main.py:52] epoch 1121, training loss: 6832.50, average training loss: 7495.21, base loss: 15906.26
[INFO 2017-06-30 01:55:47,221 main.py:52] epoch 1122, training loss: 6629.08, average training loss: 7493.06, base loss: 15906.63
[INFO 2017-06-30 01:55:50,435 main.py:52] epoch 1123, training loss: 6606.88, average training loss: 7490.71, base loss: 15906.33
[INFO 2017-06-30 01:55:53,610 main.py:52] epoch 1124, training loss: 7233.81, average training loss: 7487.76, base loss: 15908.00
[INFO 2017-06-30 01:55:56,790 main.py:52] epoch 1125, training loss: 6977.48, average training loss: 7486.52, base loss: 15907.54
[INFO 2017-06-30 01:55:59,975 main.py:52] epoch 1126, training loss: 6402.77, average training loss: 7484.40, base loss: 15906.87
[INFO 2017-06-30 01:56:03,110 main.py:52] epoch 1127, training loss: 6686.07, average training loss: 7482.24, base loss: 15906.32
[INFO 2017-06-30 01:56:06,281 main.py:52] epoch 1128, training loss: 6982.90, average training loss: 7481.27, base loss: 15906.24
[INFO 2017-06-30 01:56:09,442 main.py:52] epoch 1129, training loss: 6637.25, average training loss: 7478.68, base loss: 15906.01
[INFO 2017-06-30 01:56:12,635 main.py:52] epoch 1130, training loss: 6742.97, average training loss: 7476.67, base loss: 15905.79
[INFO 2017-06-30 01:56:15,820 main.py:52] epoch 1131, training loss: 6660.05, average training loss: 7473.93, base loss: 15906.04
[INFO 2017-06-30 01:56:19,049 main.py:52] epoch 1132, training loss: 7065.87, average training loss: 7471.26, base loss: 15906.12
[INFO 2017-06-30 01:56:22,229 main.py:52] epoch 1133, training loss: 6583.47, average training loss: 7469.10, base loss: 15905.85
[INFO 2017-06-30 01:56:25,391 main.py:52] epoch 1134, training loss: 7151.39, average training loss: 7467.63, base loss: 15908.69
[INFO 2017-06-30 01:56:28,539 main.py:52] epoch 1135, training loss: 6778.80, average training loss: 7466.19, base loss: 15909.81
[INFO 2017-06-30 01:56:31,751 main.py:52] epoch 1136, training loss: 6875.47, average training loss: 7464.40, base loss: 15910.94
[INFO 2017-06-30 01:56:34,907 main.py:52] epoch 1137, training loss: 6670.99, average training loss: 7462.02, base loss: 15910.36
[INFO 2017-06-30 01:56:38,086 main.py:52] epoch 1138, training loss: 6392.32, average training loss: 7460.24, base loss: 15907.89
[INFO 2017-06-30 01:56:41,250 main.py:52] epoch 1139, training loss: 6816.08, average training loss: 7457.98, base loss: 15905.60
[INFO 2017-06-30 01:56:44,426 main.py:52] epoch 1140, training loss: 7166.87, average training loss: 7455.90, base loss: 15907.45
[INFO 2017-06-30 01:56:47,586 main.py:52] epoch 1141, training loss: 6740.41, average training loss: 7454.04, base loss: 15908.54
[INFO 2017-06-30 01:56:50,819 main.py:52] epoch 1142, training loss: 6415.30, average training loss: 7451.23, base loss: 15907.38
[INFO 2017-06-30 01:56:53,942 main.py:52] epoch 1143, training loss: 6729.91, average training loss: 7448.76, base loss: 15906.98
[INFO 2017-06-30 01:56:57,118 main.py:52] epoch 1144, training loss: 6824.18, average training loss: 7446.33, base loss: 15905.64
[INFO 2017-06-30 01:57:00,319 main.py:52] epoch 1145, training loss: 6763.85, average training loss: 7444.41, base loss: 15905.77
[INFO 2017-06-30 01:57:03,483 main.py:52] epoch 1146, training loss: 6836.76, average training loss: 7442.71, base loss: 15904.62
[INFO 2017-06-30 01:57:06,658 main.py:52] epoch 1147, training loss: 6339.57, average training loss: 7440.66, base loss: 15900.58
[INFO 2017-06-30 01:57:09,855 main.py:52] epoch 1148, training loss: 7004.53, average training loss: 7438.98, base loss: 15902.32
[INFO 2017-06-30 01:57:12,997 main.py:52] epoch 1149, training loss: 6930.14, average training loss: 7435.91, base loss: 15902.46
[INFO 2017-06-30 01:57:16,178 main.py:52] epoch 1150, training loss: 7000.43, average training loss: 7433.90, base loss: 15903.77
[INFO 2017-06-30 01:57:19,397 main.py:52] epoch 1151, training loss: 6794.48, average training loss: 7432.27, base loss: 15904.15
[INFO 2017-06-30 01:57:22,571 main.py:52] epoch 1152, training loss: 7066.41, average training loss: 7431.02, base loss: 15906.13
[INFO 2017-06-30 01:57:25,778 main.py:52] epoch 1153, training loss: 7035.46, average training loss: 7428.95, base loss: 15906.76
[INFO 2017-06-30 01:57:28,952 main.py:52] epoch 1154, training loss: 6738.65, average training loss: 7426.71, base loss: 15907.25
[INFO 2017-06-30 01:57:32,150 main.py:52] epoch 1155, training loss: 7002.27, average training loss: 7424.65, base loss: 15907.48
[INFO 2017-06-30 01:57:35,271 main.py:52] epoch 1156, training loss: 6174.84, average training loss: 7422.57, base loss: 15907.13
[INFO 2017-06-30 01:57:38,395 main.py:52] epoch 1157, training loss: 6832.84, average training loss: 7420.22, base loss: 15908.70
[INFO 2017-06-30 01:57:41,590 main.py:52] epoch 1158, training loss: 6779.63, average training loss: 7417.94, base loss: 15908.85
[INFO 2017-06-30 01:57:44,749 main.py:52] epoch 1159, training loss: 6989.29, average training loss: 7415.46, base loss: 15909.28
[INFO 2017-06-30 01:57:47,888 main.py:52] epoch 1160, training loss: 6702.13, average training loss: 7411.68, base loss: 15908.74
[INFO 2017-06-30 01:57:51,038 main.py:52] epoch 1161, training loss: 7010.42, average training loss: 7409.56, base loss: 15909.89
[INFO 2017-06-30 01:57:54,212 main.py:52] epoch 1162, training loss: 6967.71, average training loss: 7407.68, base loss: 15910.56
[INFO 2017-06-30 01:57:57,404 main.py:52] epoch 1163, training loss: 6719.93, average training loss: 7405.08, base loss: 15911.50
[INFO 2017-06-30 01:58:00,575 main.py:52] epoch 1164, training loss: 7285.45, average training loss: 7404.13, base loss: 15913.89
[INFO 2017-06-30 01:58:03,750 main.py:52] epoch 1165, training loss: 6578.02, average training loss: 7402.06, base loss: 15913.51
[INFO 2017-06-30 01:58:06,926 main.py:52] epoch 1166, training loss: 6939.77, average training loss: 7399.74, base loss: 15917.58
[INFO 2017-06-30 01:58:10,103 main.py:52] epoch 1167, training loss: 6389.12, average training loss: 7398.01, base loss: 15918.20
[INFO 2017-06-30 01:58:13,258 main.py:52] epoch 1168, training loss: 6384.00, average training loss: 7395.13, base loss: 15915.30
[INFO 2017-06-30 01:58:16,450 main.py:52] epoch 1169, training loss: 6154.68, average training loss: 7392.90, base loss: 15913.97
[INFO 2017-06-30 01:58:19,640 main.py:52] epoch 1170, training loss: 8388.62, average training loss: 7392.58, base loss: 15920.00
[INFO 2017-06-30 01:58:22,810 main.py:52] epoch 1171, training loss: 6656.56, average training loss: 7390.91, base loss: 15918.59
[INFO 2017-06-30 01:58:25,990 main.py:52] epoch 1172, training loss: 7552.04, average training loss: 7390.42, base loss: 15922.67
[INFO 2017-06-30 01:58:29,211 main.py:52] epoch 1173, training loss: 6409.26, average training loss: 7388.78, base loss: 15922.76
[INFO 2017-06-30 01:58:32,392 main.py:52] epoch 1174, training loss: 7046.33, average training loss: 7387.19, base loss: 15923.21
[INFO 2017-06-30 01:58:35,563 main.py:52] epoch 1175, training loss: 6766.82, average training loss: 7385.80, base loss: 15922.42
[INFO 2017-06-30 01:58:38,769 main.py:52] epoch 1176, training loss: 6584.54, average training loss: 7383.29, base loss: 15921.70
[INFO 2017-06-30 01:58:41,953 main.py:52] epoch 1177, training loss: 7303.59, average training loss: 7382.28, base loss: 15924.46
[INFO 2017-06-30 01:58:45,135 main.py:52] epoch 1178, training loss: 6605.09, average training loss: 7380.94, base loss: 15925.14
[INFO 2017-06-30 01:58:48,291 main.py:52] epoch 1179, training loss: 6924.80, average training loss: 7379.72, base loss: 15926.37
[INFO 2017-06-30 01:58:51,430 main.py:52] epoch 1180, training loss: 7230.08, average training loss: 7379.10, base loss: 15928.08
[INFO 2017-06-30 01:58:54,598 main.py:52] epoch 1181, training loss: 6618.44, average training loss: 7376.90, base loss: 15926.04
[INFO 2017-06-30 01:58:57,765 main.py:52] epoch 1182, training loss: 7308.66, average training loss: 7376.24, base loss: 15926.80
[INFO 2017-06-30 01:59:00,949 main.py:52] epoch 1183, training loss: 6791.68, average training loss: 7375.04, base loss: 15926.16
[INFO 2017-06-30 01:59:04,102 main.py:52] epoch 1184, training loss: 6707.78, average training loss: 7373.64, base loss: 15925.40
[INFO 2017-06-30 01:59:07,283 main.py:52] epoch 1185, training loss: 7516.81, average training loss: 7372.39, base loss: 15928.14
[INFO 2017-06-30 01:59:10,450 main.py:52] epoch 1186, training loss: 6660.59, average training loss: 7370.96, base loss: 15927.89
[INFO 2017-06-30 01:59:13,585 main.py:52] epoch 1187, training loss: 6784.52, average training loss: 7369.38, base loss: 15927.08
[INFO 2017-06-30 01:59:16,760 main.py:52] epoch 1188, training loss: 6927.76, average training loss: 7367.61, base loss: 15928.45
[INFO 2017-06-30 01:59:19,938 main.py:52] epoch 1189, training loss: 6447.50, average training loss: 7365.82, base loss: 15927.68
[INFO 2017-06-30 01:59:23,113 main.py:52] epoch 1190, training loss: 7044.75, average training loss: 7363.82, base loss: 15928.44
[INFO 2017-06-30 01:59:26,307 main.py:52] epoch 1191, training loss: 6426.06, average training loss: 7360.43, base loss: 15926.41
[INFO 2017-06-30 01:59:29,466 main.py:52] epoch 1192, training loss: 6892.38, average training loss: 7359.02, base loss: 15928.63
[INFO 2017-06-30 01:59:32,621 main.py:52] epoch 1193, training loss: 7152.50, average training loss: 7357.89, base loss: 15929.61
[INFO 2017-06-30 01:59:35,767 main.py:52] epoch 1194, training loss: 7154.94, average training loss: 7356.17, base loss: 15931.34
[INFO 2017-06-30 01:59:38,938 main.py:52] epoch 1195, training loss: 6474.49, average training loss: 7354.82, base loss: 15929.85
[INFO 2017-06-30 01:59:42,110 main.py:52] epoch 1196, training loss: 6725.83, average training loss: 7352.93, base loss: 15930.16
[INFO 2017-06-30 01:59:45,316 main.py:52] epoch 1197, training loss: 6520.36, average training loss: 7351.23, base loss: 15928.81
[INFO 2017-06-30 01:59:48,471 main.py:52] epoch 1198, training loss: 7163.61, average training loss: 7349.96, base loss: 15929.96
[INFO 2017-06-30 01:59:51,612 main.py:52] epoch 1199, training loss: 7343.44, average training loss: 7349.43, base loss: 15932.60
[INFO 2017-06-30 01:59:51,612 main.py:54] epoch 1199, testing
[INFO 2017-06-30 02:00:05,030 main.py:97] average testing loss: 6783.47, base loss: 16261.34
[INFO 2017-06-30 02:00:05,031 main.py:98] improve_loss: 9477.88, improve_percent: 0.58
[INFO 2017-06-30 02:00:05,032 main.py:62] model save to ./model/final.pth
[INFO 2017-06-30 02:00:05,067 main.py:66] current best improved percent: 0.58
[INFO 2017-06-30 02:00:08,231 main.py:52] epoch 1200, training loss: 6886.60, average training loss: 7348.72, base loss: 15934.90
[INFO 2017-06-30 02:00:11,363 main.py:52] epoch 1201, training loss: 7258.20, average training loss: 7348.61, base loss: 15938.17
[INFO 2017-06-30 02:00:14,565 main.py:52] epoch 1202, training loss: 7401.66, average training loss: 7347.80, base loss: 15939.62
[INFO 2017-06-30 02:00:17,722 main.py:52] epoch 1203, training loss: 6863.62, average training loss: 7345.47, base loss: 15938.85
[INFO 2017-06-30 02:00:20,907 main.py:52] epoch 1204, training loss: 6453.96, average training loss: 7343.43, base loss: 15938.59
[INFO 2017-06-30 02:00:24,060 main.py:52] epoch 1205, training loss: 6274.93, average training loss: 7341.01, base loss: 15937.21
[INFO 2017-06-30 02:00:27,255 main.py:52] epoch 1206, training loss: 7117.29, average training loss: 7339.47, base loss: 15938.82
[INFO 2017-06-30 02:00:30,427 main.py:52] epoch 1207, training loss: 7237.74, average training loss: 7338.29, base loss: 15939.80
[INFO 2017-06-30 02:00:33,602 main.py:52] epoch 1208, training loss: 6760.61, average training loss: 7336.45, base loss: 15940.37
[INFO 2017-06-30 02:00:36,780 main.py:52] epoch 1209, training loss: 7304.63, average training loss: 7335.85, base loss: 15941.27
[INFO 2017-06-30 02:00:39,926 main.py:52] epoch 1210, training loss: 7295.65, average training loss: 7335.15, base loss: 15942.14
[INFO 2017-06-30 02:00:43,053 main.py:52] epoch 1211, training loss: 6680.19, average training loss: 7334.07, base loss: 15941.93
[INFO 2017-06-30 02:00:46,182 main.py:52] epoch 1212, training loss: 7351.56, average training loss: 7332.50, base loss: 15944.33
[INFO 2017-06-30 02:00:49,331 main.py:52] epoch 1213, training loss: 7362.06, average training loss: 7331.14, base loss: 15947.15
[INFO 2017-06-30 02:00:52,483 main.py:52] epoch 1214, training loss: 6554.28, average training loss: 7328.71, base loss: 15944.58
[INFO 2017-06-30 02:00:55,670 main.py:52] epoch 1215, training loss: 6902.40, average training loss: 7327.24, base loss: 15943.84
[INFO 2017-06-30 02:00:58,791 main.py:52] epoch 1216, training loss: 6832.07, average training loss: 7324.91, base loss: 15943.97
[INFO 2017-06-30 02:01:01,938 main.py:52] epoch 1217, training loss: 7393.98, average training loss: 7324.04, base loss: 15945.23
[INFO 2017-06-30 02:01:05,129 main.py:52] epoch 1218, training loss: 7287.76, average training loss: 7321.96, base loss: 15945.62
[INFO 2017-06-30 02:01:08,305 main.py:52] epoch 1219, training loss: 6388.13, average training loss: 7320.35, base loss: 15943.33
[INFO 2017-06-30 02:01:11,493 main.py:52] epoch 1220, training loss: 6562.13, average training loss: 7319.17, base loss: 15941.57
[INFO 2017-06-30 02:01:14,687 main.py:52] epoch 1221, training loss: 6222.30, average training loss: 7316.39, base loss: 15938.48
[INFO 2017-06-30 02:01:17,856 main.py:52] epoch 1222, training loss: 6791.33, average training loss: 7314.51, base loss: 15938.28
[INFO 2017-06-30 02:01:21,013 main.py:52] epoch 1223, training loss: 7158.78, average training loss: 7313.47, base loss: 15940.20
[INFO 2017-06-30 02:01:24,164 main.py:52] epoch 1224, training loss: 6685.17, average training loss: 7311.77, base loss: 15938.96
[INFO 2017-06-30 02:01:27,329 main.py:52] epoch 1225, training loss: 7474.19, average training loss: 7311.59, base loss: 15940.46
[INFO 2017-06-30 02:01:30,548 main.py:52] epoch 1226, training loss: 6590.44, average training loss: 7310.19, base loss: 15938.85
[INFO 2017-06-30 02:01:33,733 main.py:52] epoch 1227, training loss: 6528.32, average training loss: 7308.69, base loss: 15936.34
[INFO 2017-06-30 02:01:36,881 main.py:52] epoch 1228, training loss: 6829.12, average training loss: 7306.97, base loss: 15936.78
[INFO 2017-06-30 02:01:40,060 main.py:52] epoch 1229, training loss: 6529.81, average training loss: 7304.14, base loss: 15938.29
[INFO 2017-06-30 02:01:43,235 main.py:52] epoch 1230, training loss: 7245.34, average training loss: 7303.38, base loss: 15939.93
[INFO 2017-06-30 02:01:46,415 main.py:52] epoch 1231, training loss: 6685.16, average training loss: 7301.60, base loss: 15939.60
[INFO 2017-06-30 02:01:49,543 main.py:52] epoch 1232, training loss: 6880.69, average training loss: 7300.37, base loss: 15939.22
[INFO 2017-06-30 02:01:52,719 main.py:52] epoch 1233, training loss: 7496.22, average training loss: 7300.22, base loss: 15941.04
[INFO 2017-06-30 02:01:55,888 main.py:52] epoch 1234, training loss: 6221.01, average training loss: 7298.73, base loss: 15938.34
[INFO 2017-06-30 02:01:59,027 main.py:52] epoch 1235, training loss: 6428.09, average training loss: 7295.52, base loss: 15935.11
[INFO 2017-06-30 02:02:02,165 main.py:52] epoch 1236, training loss: 6492.49, average training loss: 7293.68, base loss: 15933.78
[INFO 2017-06-30 02:02:05,384 main.py:52] epoch 1237, training loss: 6833.99, average training loss: 7292.48, base loss: 15932.31
[INFO 2017-06-30 02:02:08,597 main.py:52] epoch 1238, training loss: 7259.20, average training loss: 7291.65, base loss: 15934.71
[INFO 2017-06-30 02:02:11,759 main.py:52] epoch 1239, training loss: 6769.05, average training loss: 7290.37, base loss: 15934.30
[INFO 2017-06-30 02:02:14,922 main.py:52] epoch 1240, training loss: 7453.99, average training loss: 7289.69, base loss: 15935.39
[INFO 2017-06-30 02:02:18,102 main.py:52] epoch 1241, training loss: 7108.26, average training loss: 7288.32, base loss: 15935.56
[INFO 2017-06-30 02:02:21,290 main.py:52] epoch 1242, training loss: 6591.69, average training loss: 7286.75, base loss: 15934.56
[INFO 2017-06-30 02:02:24,489 main.py:52] epoch 1243, training loss: 6573.35, average training loss: 7284.42, base loss: 15934.24
[INFO 2017-06-30 02:02:27,636 main.py:52] epoch 1244, training loss: 6057.15, average training loss: 7282.28, base loss: 15931.94
[INFO 2017-06-30 02:02:30,776 main.py:52] epoch 1245, training loss: 6828.45, average training loss: 7280.91, base loss: 15931.22
[INFO 2017-06-30 02:02:33,905 main.py:52] epoch 1246, training loss: 6130.08, average training loss: 7278.61, base loss: 15929.76
[INFO 2017-06-30 02:02:37,113 main.py:52] epoch 1247, training loss: 6689.56, average training loss: 7277.50, base loss: 15931.26
[INFO 2017-06-30 02:02:40,308 main.py:52] epoch 1248, training loss: 7295.79, average training loss: 7275.59, base loss: 15932.52
[INFO 2017-06-30 02:02:43,493 main.py:52] epoch 1249, training loss: 6851.13, average training loss: 7273.66, base loss: 15933.51
[INFO 2017-06-30 02:02:46,657 main.py:52] epoch 1250, training loss: 6966.30, average training loss: 7272.05, base loss: 15934.95
[INFO 2017-06-30 02:02:49,812 main.py:52] epoch 1251, training loss: 6952.39, average training loss: 7270.81, base loss: 15935.01
[INFO 2017-06-30 02:02:53,003 main.py:52] epoch 1252, training loss: 6616.58, average training loss: 7269.57, base loss: 15934.65
[INFO 2017-06-30 02:02:56,170 main.py:52] epoch 1253, training loss: 6988.66, average training loss: 7267.93, base loss: 15933.48
[INFO 2017-06-30 02:02:59,359 main.py:52] epoch 1254, training loss: 7062.92, average training loss: 7267.33, base loss: 15934.59
[INFO 2017-06-30 02:03:02,544 main.py:52] epoch 1255, training loss: 6178.30, average training loss: 7264.78, base loss: 15932.96
[INFO 2017-06-30 02:03:05,734 main.py:52] epoch 1256, training loss: 7075.37, average training loss: 7264.25, base loss: 15933.87
[INFO 2017-06-30 02:03:08,907 main.py:52] epoch 1257, training loss: 7054.61, average training loss: 7263.02, base loss: 15935.64
[INFO 2017-06-30 02:03:12,106 main.py:52] epoch 1258, training loss: 6996.11, average training loss: 7262.38, base loss: 15936.75
[INFO 2017-06-30 02:03:15,278 main.py:52] epoch 1259, training loss: 6363.25, average training loss: 7260.79, base loss: 15935.75
[INFO 2017-06-30 02:03:18,420 main.py:52] epoch 1260, training loss: 6883.85, average training loss: 7258.30, base loss: 15935.85
[INFO 2017-06-30 02:03:21,602 main.py:52] epoch 1261, training loss: 6814.78, average training loss: 7256.94, base loss: 15936.52
[INFO 2017-06-30 02:03:24,787 main.py:52] epoch 1262, training loss: 6754.05, average training loss: 7255.31, base loss: 15937.35
[INFO 2017-06-30 02:03:27,983 main.py:52] epoch 1263, training loss: 6725.62, average training loss: 7253.97, base loss: 15937.28
[INFO 2017-06-30 02:03:31,150 main.py:52] epoch 1264, training loss: 6237.47, average training loss: 7252.31, base loss: 15935.13
[INFO 2017-06-30 02:03:34,336 main.py:52] epoch 1265, training loss: 6669.01, average training loss: 7251.05, base loss: 15935.43
[INFO 2017-06-30 02:03:37,530 main.py:52] epoch 1266, training loss: 7567.62, average training loss: 7250.90, base loss: 15936.12
[INFO 2017-06-30 02:03:40,707 main.py:52] epoch 1267, training loss: 6514.64, average training loss: 7249.10, base loss: 15935.76
[INFO 2017-06-30 02:03:43,894 main.py:52] epoch 1268, training loss: 6402.81, average training loss: 7247.23, base loss: 15934.90
[INFO 2017-06-30 02:03:47,041 main.py:52] epoch 1269, training loss: 6432.47, average training loss: 7246.06, base loss: 15934.07
[INFO 2017-06-30 02:03:50,194 main.py:52] epoch 1270, training loss: 6468.65, average training loss: 7244.87, base loss: 15934.35
[INFO 2017-06-30 02:03:53,371 main.py:52] epoch 1271, training loss: 7471.04, average training loss: 7244.80, base loss: 15935.58
[INFO 2017-06-30 02:03:56,578 main.py:52] epoch 1272, training loss: 7067.10, average training loss: 7244.22, base loss: 15935.72
[INFO 2017-06-30 02:03:59,765 main.py:52] epoch 1273, training loss: 6320.65, average training loss: 7241.88, base loss: 15935.13
[INFO 2017-06-30 02:04:02,913 main.py:52] epoch 1274, training loss: 7026.11, average training loss: 7241.00, base loss: 15935.32
[INFO 2017-06-30 02:04:06,106 main.py:52] epoch 1275, training loss: 6426.30, average training loss: 7238.98, base loss: 15935.70
[INFO 2017-06-30 02:04:09,279 main.py:52] epoch 1276, training loss: 6710.80, average training loss: 7237.67, base loss: 15935.63
[INFO 2017-06-30 02:04:12,468 main.py:52] epoch 1277, training loss: 6435.47, average training loss: 7236.62, base loss: 15933.73
[INFO 2017-06-30 02:04:15,690 main.py:52] epoch 1278, training loss: 6550.24, average training loss: 7235.14, base loss: 15932.00
[INFO 2017-06-30 02:04:18,870 main.py:52] epoch 1279, training loss: 6709.44, average training loss: 7234.71, base loss: 15933.38
[INFO 2017-06-30 02:04:22,090 main.py:52] epoch 1280, training loss: 6685.09, average training loss: 7233.04, base loss: 15933.56
[INFO 2017-06-30 02:04:25,236 main.py:52] epoch 1281, training loss: 7055.58, average training loss: 7231.49, base loss: 15934.45
[INFO 2017-06-30 02:04:28,405 main.py:52] epoch 1282, training loss: 6521.23, average training loss: 7229.94, base loss: 15933.63
[INFO 2017-06-30 02:04:31,557 main.py:52] epoch 1283, training loss: 6264.72, average training loss: 7228.11, base loss: 15931.38
[INFO 2017-06-30 02:04:34,747 main.py:52] epoch 1284, training loss: 6941.03, average training loss: 7227.56, base loss: 15932.57
[INFO 2017-06-30 02:04:37,881 main.py:52] epoch 1285, training loss: 6438.40, average training loss: 7225.62, base loss: 15931.83
[INFO 2017-06-30 02:04:41,054 main.py:52] epoch 1286, training loss: 6505.45, average training loss: 7224.89, base loss: 15931.22
[INFO 2017-06-30 02:04:44,175 main.py:52] epoch 1287, training loss: 6522.15, average training loss: 7223.52, base loss: 15930.20
[INFO 2017-06-30 02:04:47,315 main.py:52] epoch 1288, training loss: 7800.08, average training loss: 7223.06, base loss: 15934.83
[INFO 2017-06-30 02:04:50,517 main.py:52] epoch 1289, training loss: 6726.71, average training loss: 7222.15, base loss: 15934.77
[INFO 2017-06-30 02:04:53,727 main.py:52] epoch 1290, training loss: 6696.99, average training loss: 7220.50, base loss: 15933.48
[INFO 2017-06-30 02:04:56,897 main.py:52] epoch 1291, training loss: 6957.94, average training loss: 7218.96, base loss: 15933.37
[INFO 2017-06-30 02:05:00,087 main.py:52] epoch 1292, training loss: 6234.02, average training loss: 7216.67, base loss: 15931.89
[INFO 2017-06-30 02:05:03,292 main.py:52] epoch 1293, training loss: 6725.69, average training loss: 7214.59, base loss: 15931.49
[INFO 2017-06-30 02:05:06,459 main.py:52] epoch 1294, training loss: 6799.77, average training loss: 7214.20, base loss: 15931.34
[INFO 2017-06-30 02:05:09,614 main.py:52] epoch 1295, training loss: 6453.44, average training loss: 7211.74, base loss: 15930.45
[INFO 2017-06-30 02:05:12,745 main.py:52] epoch 1296, training loss: 6376.96, average training loss: 7210.82, base loss: 15929.33
[INFO 2017-06-30 02:05:15,966 main.py:52] epoch 1297, training loss: 6576.35, average training loss: 7210.19, base loss: 15930.63
[INFO 2017-06-30 02:05:19,125 main.py:52] epoch 1298, training loss: 6729.49, average training loss: 7208.87, base loss: 15930.77
[INFO 2017-06-30 02:05:22,314 main.py:52] epoch 1299, training loss: 6839.92, average training loss: 7207.76, base loss: 15930.95
[INFO 2017-06-30 02:05:22,314 main.py:54] epoch 1299, testing
[INFO 2017-06-30 02:05:35,619 main.py:97] average testing loss: 6893.32, base loss: 16693.16
[INFO 2017-06-30 02:05:35,619 main.py:98] improve_loss: 9799.84, improve_percent: 0.59
[INFO 2017-06-30 02:05:35,621 main.py:62] model save to ./model/final.pth
[INFO 2017-06-30 02:05:35,655 main.py:66] current best improved percent: 0.59
[INFO 2017-06-30 02:05:38,811 main.py:52] epoch 1300, training loss: 6079.80, average training loss: 7206.14, base loss: 15927.77
[INFO 2017-06-30 02:05:41,956 main.py:52] epoch 1301, training loss: 6836.78, average training loss: 7205.21, base loss: 15927.39
[INFO 2017-06-30 02:05:45,087 main.py:52] epoch 1302, training loss: 6577.29, average training loss: 7204.34, base loss: 15928.36
[INFO 2017-06-30 02:05:48,233 main.py:52] epoch 1303, training loss: 6804.89, average training loss: 7203.66, base loss: 15928.82
[INFO 2017-06-30 02:05:51,367 main.py:52] epoch 1304, training loss: 7036.53, average training loss: 7202.64, base loss: 15929.70
[INFO 2017-06-30 02:05:54,548 main.py:52] epoch 1305, training loss: 6823.26, average training loss: 7201.81, base loss: 15930.56
[INFO 2017-06-30 02:05:57,727 main.py:52] epoch 1306, training loss: 7079.32, average training loss: 7201.35, base loss: 15931.85
[INFO 2017-06-30 02:06:00,930 main.py:52] epoch 1307, training loss: 6760.54, average training loss: 7199.77, base loss: 15931.62
[INFO 2017-06-30 02:06:04,091 main.py:52] epoch 1308, training loss: 7707.73, average training loss: 7199.29, base loss: 15932.97
[INFO 2017-06-30 02:06:07,276 main.py:52] epoch 1309, training loss: 6647.96, average training loss: 7198.24, base loss: 15932.30
[INFO 2017-06-30 02:06:10,463 main.py:52] epoch 1310, training loss: 6340.70, average training loss: 7196.06, base loss: 15930.76
[INFO 2017-06-30 02:06:13,674 main.py:52] epoch 1311, training loss: 6795.52, average training loss: 7195.38, base loss: 15932.38
[INFO 2017-06-30 02:06:16,849 main.py:52] epoch 1312, training loss: 6569.77, average training loss: 7193.39, base loss: 15931.33
[INFO 2017-06-30 02:06:20,074 main.py:52] epoch 1313, training loss: 6931.51, average training loss: 7192.25, base loss: 15931.11
[INFO 2017-06-30 02:06:23,228 main.py:52] epoch 1314, training loss: 6689.67, average training loss: 7191.01, base loss: 15929.75
[INFO 2017-06-30 02:06:26,404 main.py:52] epoch 1315, training loss: 7156.07, average training loss: 7190.77, base loss: 15930.20
[INFO 2017-06-30 02:06:29,536 main.py:52] epoch 1316, training loss: 6930.61, average training loss: 7189.41, base loss: 15931.93
[INFO 2017-06-30 02:06:32,703 main.py:52] epoch 1317, training loss: 7048.56, average training loss: 7189.14, base loss: 15933.07
[INFO 2017-06-30 02:06:35,868 main.py:52] epoch 1318, training loss: 6570.81, average training loss: 7187.63, base loss: 15931.98
[INFO 2017-06-30 02:06:39,049 main.py:52] epoch 1319, training loss: 6548.80, average training loss: 7185.68, base loss: 15931.85
[INFO 2017-06-30 02:06:42,236 main.py:52] epoch 1320, training loss: 6467.89, average training loss: 7184.59, base loss: 15929.94
[INFO 2017-06-30 02:06:45,429 main.py:52] epoch 1321, training loss: 6884.88, average training loss: 7183.69, base loss: 15928.68
[INFO 2017-06-30 02:06:48,604 main.py:52] epoch 1322, training loss: 6438.23, average training loss: 7181.32, base loss: 15927.69
[INFO 2017-06-30 02:06:51,765 main.py:52] epoch 1323, training loss: 6396.70, average training loss: 7178.85, base loss: 15927.07
[INFO 2017-06-30 02:06:54,925 main.py:52] epoch 1324, training loss: 6613.63, average training loss: 7177.47, base loss: 15927.91
[INFO 2017-06-30 02:06:58,080 main.py:52] epoch 1325, training loss: 6326.72, average training loss: 7176.89, base loss: 15926.21
[INFO 2017-06-30 02:07:01,243 main.py:52] epoch 1326, training loss: 6827.58, average training loss: 7175.82, base loss: 15926.93
[INFO 2017-06-30 02:07:04,413 main.py:52] epoch 1327, training loss: 6629.38, average training loss: 7174.82, base loss: 15927.28
[INFO 2017-06-30 02:07:07,591 main.py:52] epoch 1328, training loss: 6400.23, average training loss: 7173.76, base loss: 15925.70
[INFO 2017-06-30 02:07:10,739 main.py:52] epoch 1329, training loss: 7042.56, average training loss: 7172.73, base loss: 15926.86
[INFO 2017-06-30 02:07:13,915 main.py:52] epoch 1330, training loss: 6383.54, average training loss: 7171.51, base loss: 15927.13
[INFO 2017-06-30 02:07:17,110 main.py:52] epoch 1331, training loss: 6627.44, average training loss: 7170.73, base loss: 15926.76
[INFO 2017-06-30 02:07:20,264 main.py:52] epoch 1332, training loss: 6767.64, average training loss: 7169.99, base loss: 15927.25
[INFO 2017-06-30 02:07:23,447 main.py:52] epoch 1333, training loss: 6240.04, average training loss: 7168.99, base loss: 15924.79
[INFO 2017-06-30 02:07:26,619 main.py:52] epoch 1334, training loss: 6998.82, average training loss: 7168.40, base loss: 15925.57
[INFO 2017-06-30 02:07:29,824 main.py:52] epoch 1335, training loss: 6977.42, average training loss: 7167.92, base loss: 15926.73
[INFO 2017-06-30 02:07:32,980 main.py:52] epoch 1336, training loss: 7050.58, average training loss: 7167.35, base loss: 15927.74
[INFO 2017-06-30 02:07:36,109 main.py:52] epoch 1337, training loss: 7006.47, average training loss: 7166.55, base loss: 15928.35
[INFO 2017-06-30 02:07:39,288 main.py:52] epoch 1338, training loss: 6630.58, average training loss: 7165.37, base loss: 15927.95
[INFO 2017-06-30 02:07:42,467 main.py:52] epoch 1339, training loss: 6361.90, average training loss: 7164.78, base loss: 15927.68
[INFO 2017-06-30 02:07:45,621 main.py:52] epoch 1340, training loss: 6231.75, average training loss: 7162.83, base loss: 15925.44
[INFO 2017-06-30 02:07:48,771 main.py:52] epoch 1341, training loss: 6978.38, average training loss: 7162.08, base loss: 15925.64
[INFO 2017-06-30 02:07:51,985 main.py:52] epoch 1342, training loss: 6726.26, average training loss: 7160.85, base loss: 15925.37
[INFO 2017-06-30 02:07:55,173 main.py:52] epoch 1343, training loss: 6668.51, average training loss: 7159.59, base loss: 15925.32
[INFO 2017-06-30 02:07:58,378 main.py:52] epoch 1344, training loss: 7096.02, average training loss: 7159.43, base loss: 15925.96
[INFO 2017-06-30 02:08:01,558 main.py:52] epoch 1345, training loss: 6753.55, average training loss: 7157.35, base loss: 15925.14
[INFO 2017-06-30 02:08:04,713 main.py:52] epoch 1346, training loss: 6676.75, average training loss: 7157.03, base loss: 15925.11
[INFO 2017-06-30 02:08:07,894 main.py:52] epoch 1347, training loss: 6224.61, average training loss: 7155.83, base loss: 15922.63
[INFO 2017-06-30 02:08:11,058 main.py:52] epoch 1348, training loss: 6927.30, average training loss: 7155.06, base loss: 15925.36
[INFO 2017-06-30 02:08:14,250 main.py:52] epoch 1349, training loss: 7201.79, average training loss: 7154.25, base loss: 15928.30
[INFO 2017-06-30 02:08:17,459 main.py:52] epoch 1350, training loss: 7434.85, average training loss: 7154.69, base loss: 15930.18
[INFO 2017-06-30 02:08:20,593 main.py:52] epoch 1351, training loss: 7162.21, average training loss: 7153.32, base loss: 15931.21
[INFO 2017-06-30 02:08:23,743 main.py:52] epoch 1352, training loss: 6392.20, average training loss: 7151.75, base loss: 15930.81
[INFO 2017-06-30 02:08:26,895 main.py:52] epoch 1353, training loss: 6469.04, average training loss: 7150.92, base loss: 15930.45
[INFO 2017-06-30 02:08:30,085 main.py:52] epoch 1354, training loss: 6657.28, average training loss: 7149.91, base loss: 15930.09
[INFO 2017-06-30 02:08:33,231 main.py:52] epoch 1355, training loss: 6386.46, average training loss: 7148.75, base loss: 15927.99
[INFO 2017-06-30 02:08:36,397 main.py:52] epoch 1356, training loss: 6791.15, average training loss: 7148.09, base loss: 15927.53
[INFO 2017-06-30 02:08:39,539 main.py:52] epoch 1357, training loss: 6686.51, average training loss: 7146.92, base loss: 15926.86
[INFO 2017-06-30 02:08:42,707 main.py:52] epoch 1358, training loss: 6545.12, average training loss: 7145.13, base loss: 15926.52
[INFO 2017-06-30 02:08:45,913 main.py:52] epoch 1359, training loss: 6550.18, average training loss: 7143.89, base loss: 15924.98
[INFO 2017-06-30 02:08:49,091 main.py:52] epoch 1360, training loss: 6945.48, average training loss: 7143.19, base loss: 15925.26
[INFO 2017-06-30 02:08:52,323 main.py:52] epoch 1361, training loss: 7194.64, average training loss: 7143.09, base loss: 15926.30
[INFO 2017-06-30 02:08:55,461 main.py:52] epoch 1362, training loss: 7259.00, average training loss: 7142.33, base loss: 15928.93
[INFO 2017-06-30 02:08:58,632 main.py:52] epoch 1363, training loss: 6206.21, average training loss: 7140.07, base loss: 15927.53
[INFO 2017-06-30 02:09:01,801 main.py:52] epoch 1364, training loss: 6090.12, average training loss: 7138.66, base loss: 15925.52
[INFO 2017-06-30 02:09:04,942 main.py:52] epoch 1365, training loss: 6182.93, average training loss: 7137.28, base loss: 15924.23
[INFO 2017-06-30 02:09:08,111 main.py:52] epoch 1366, training loss: 6408.68, average training loss: 7135.38, base loss: 15923.45
[INFO 2017-06-30 02:09:11,277 main.py:52] epoch 1367, training loss: 6407.33, average training loss: 7133.16, base loss: 15922.85
[INFO 2017-06-30 02:09:14,497 main.py:52] epoch 1368, training loss: 6460.64, average training loss: 7132.04, base loss: 15922.60
[INFO 2017-06-30 02:09:17,683 main.py:52] epoch 1369, training loss: 6243.25, average training loss: 7131.08, base loss: 15920.45
[INFO 2017-06-30 02:09:20,841 main.py:52] epoch 1370, training loss: 6958.57, average training loss: 7130.50, base loss: 15923.19
[INFO 2017-06-30 02:09:24,023 main.py:52] epoch 1371, training loss: 6669.79, average training loss: 7128.77, base loss: 15923.82
[INFO 2017-06-30 02:09:27,175 main.py:52] epoch 1372, training loss: 6521.29, average training loss: 7127.94, base loss: 15923.94
[INFO 2017-06-30 02:09:30,368 main.py:52] epoch 1373, training loss: 6720.41, average training loss: 7127.14, base loss: 15925.10
[INFO 2017-06-30 02:09:33,539 main.py:52] epoch 1374, training loss: 7005.64, average training loss: 7125.96, base loss: 15926.85
[INFO 2017-06-30 02:09:36,687 main.py:52] epoch 1375, training loss: 6688.11, average training loss: 7124.55, base loss: 15927.32
[INFO 2017-06-30 02:09:39,858 main.py:52] epoch 1376, training loss: 6092.53, average training loss: 7122.82, base loss: 15925.93
[INFO 2017-06-30 02:09:43,028 main.py:52] epoch 1377, training loss: 7018.45, average training loss: 7122.07, base loss: 15927.33
[INFO 2017-06-30 02:09:46,168 main.py:52] epoch 1378, training loss: 6358.95, average training loss: 7120.90, base loss: 15926.83
[INFO 2017-06-30 02:09:49,400 main.py:52] epoch 1379, training loss: 6494.89, average training loss: 7119.71, base loss: 15927.59
[INFO 2017-06-30 02:09:52,599 main.py:52] epoch 1380, training loss: 6257.20, average training loss: 7117.86, base loss: 15925.45
[INFO 2017-06-30 02:09:55,759 main.py:52] epoch 1381, training loss: 6515.10, average training loss: 7116.77, base loss: 15923.75
[INFO 2017-06-30 02:09:58,958 main.py:52] epoch 1382, training loss: 6465.95, average training loss: 7115.58, base loss: 15922.51
[INFO 2017-06-30 02:10:02,184 main.py:52] epoch 1383, training loss: 7521.60, average training loss: 7115.30, base loss: 15924.87
[INFO 2017-06-30 02:10:05,338 main.py:52] epoch 1384, training loss: 6418.47, average training loss: 7114.02, base loss: 15925.63
[INFO 2017-06-30 02:10:08,510 main.py:52] epoch 1385, training loss: 5996.00, average training loss: 7112.96, base loss: 15924.69
[INFO 2017-06-30 02:10:11,688 main.py:52] epoch 1386, training loss: 6863.75, average training loss: 7111.93, base loss: 15925.63
[INFO 2017-06-30 02:10:14,867 main.py:52] epoch 1387, training loss: 6626.62, average training loss: 7110.86, base loss: 15924.58
[INFO 2017-06-30 02:10:18,037 main.py:52] epoch 1388, training loss: 5993.09, average training loss: 7109.09, base loss: 15922.52
[INFO 2017-06-30 02:10:21,216 main.py:52] epoch 1389, training loss: 6742.23, average training loss: 7108.27, base loss: 15921.94
[INFO 2017-06-30 02:10:24,388 main.py:52] epoch 1390, training loss: 6971.73, average training loss: 7106.81, base loss: 15923.61
[INFO 2017-06-30 02:10:27,551 main.py:52] epoch 1391, training loss: 6605.27, average training loss: 7105.90, base loss: 15924.15
[INFO 2017-06-30 02:10:30,722 main.py:52] epoch 1392, training loss: 6828.51, average training loss: 7104.82, base loss: 15925.34
[INFO 2017-06-30 02:10:33,845 main.py:52] epoch 1393, training loss: 6423.99, average training loss: 7103.37, base loss: 15926.69
[INFO 2017-06-30 02:10:37,014 main.py:52] epoch 1394, training loss: 6656.14, average training loss: 7102.99, base loss: 15927.28
[INFO 2017-06-30 02:10:40,177 main.py:52] epoch 1395, training loss: 7107.86, average training loss: 7101.85, base loss: 15928.57
[INFO 2017-06-30 02:10:43,343 main.py:52] epoch 1396, training loss: 6131.46, average training loss: 7100.26, base loss: 15928.25
[INFO 2017-06-30 02:10:46,569 main.py:52] epoch 1397, training loss: 6066.21, average training loss: 7098.20, base loss: 15927.05
[INFO 2017-06-30 02:10:49,746 main.py:52] epoch 1398, training loss: 6731.20, average training loss: 7097.91, base loss: 15928.88
[INFO 2017-06-30 02:10:52,905 main.py:52] epoch 1399, training loss: 6420.19, average training loss: 7096.38, base loss: 15929.24
[INFO 2017-06-30 02:10:52,906 main.py:54] epoch 1399, testing
[INFO 2017-06-30 02:11:06,146 main.py:97] average testing loss: 6683.44, base loss: 15909.53
[INFO 2017-06-30 02:11:06,146 main.py:98] improve_loss: 9226.09, improve_percent: 0.58
[INFO 2017-06-30 02:11:06,148 main.py:66] current best improved percent: 0.59
[INFO 2017-06-30 02:11:09,279 main.py:52] epoch 1400, training loss: 6606.60, average training loss: 7094.88, base loss: 15930.28
[INFO 2017-06-30 02:11:12,451 main.py:52] epoch 1401, training loss: 6809.30, average training loss: 7094.19, base loss: 15930.07
[INFO 2017-06-30 02:11:15,643 main.py:52] epoch 1402, training loss: 7027.43, average training loss: 7093.79, base loss: 15931.94
[INFO 2017-06-30 02:11:18,777 main.py:52] epoch 1403, training loss: 6909.04, average training loss: 7092.97, base loss: 15931.70
[INFO 2017-06-30 02:11:21,960 main.py:52] epoch 1404, training loss: 6902.31, average training loss: 7091.50, base loss: 15929.69
[INFO 2017-06-30 02:11:25,177 main.py:52] epoch 1405, training loss: 6213.91, average training loss: 7090.17, base loss: 15928.09
[INFO 2017-06-30 02:11:28,357 main.py:52] epoch 1406, training loss: 6703.23, average training loss: 7089.00, base loss: 15927.70
[INFO 2017-06-30 02:11:31,534 main.py:52] epoch 1407, training loss: 6717.81, average training loss: 7087.33, base loss: 15928.49
[INFO 2017-06-30 02:11:34,706 main.py:52] epoch 1408, training loss: 6660.69, average training loss: 7086.83, base loss: 15929.38
[INFO 2017-06-30 02:11:37,908 main.py:52] epoch 1409, training loss: 7409.77, average training loss: 7085.98, base loss: 15932.19
[INFO 2017-06-30 02:11:41,047 main.py:52] epoch 1410, training loss: 6817.57, average training loss: 7084.87, base loss: 15933.82
[INFO 2017-06-30 02:11:44,200 main.py:52] epoch 1411, training loss: 6859.94, average training loss: 7084.76, base loss: 15934.51
[INFO 2017-06-30 02:11:47,452 main.py:52] epoch 1412, training loss: 7195.73, average training loss: 7084.42, base loss: 15936.14
[INFO 2017-06-30 02:11:50,599 main.py:52] epoch 1413, training loss: 6731.32, average training loss: 7083.51, base loss: 15937.13
[INFO 2017-06-30 02:11:53,762 main.py:52] epoch 1414, training loss: 6647.94, average training loss: 7082.20, base loss: 15937.47
[INFO 2017-06-30 02:11:56,902 main.py:52] epoch 1415, training loss: 6745.67, average training loss: 7081.37, base loss: 15937.29
[INFO 2017-06-30 02:12:00,066 main.py:52] epoch 1416, training loss: 6558.99, average training loss: 7079.99, base loss: 15936.63
[INFO 2017-06-30 02:12:03,224 main.py:52] epoch 1417, training loss: 6073.94, average training loss: 7078.16, base loss: 15933.97
[INFO 2017-06-30 02:12:06,444 main.py:52] epoch 1418, training loss: 6989.05, average training loss: 7077.10, base loss: 15933.98
[INFO 2017-06-30 02:12:09,606 main.py:52] epoch 1419, training loss: 6622.42, average training loss: 7076.17, base loss: 15933.10
[INFO 2017-06-30 02:12:12,743 main.py:52] epoch 1420, training loss: 7167.55, average training loss: 7075.58, base loss: 15933.91
[INFO 2017-06-30 02:12:15,932 main.py:52] epoch 1421, training loss: 7130.65, average training loss: 7075.25, base loss: 15935.17
[INFO 2017-06-30 02:12:19,090 main.py:52] epoch 1422, training loss: 6300.80, average training loss: 7073.22, base loss: 15935.33
[INFO 2017-06-30 02:12:22,282 main.py:52] epoch 1423, training loss: 6985.23, average training loss: 7072.41, base loss: 15936.90
[INFO 2017-06-30 02:12:25,434 main.py:52] epoch 1424, training loss: 6458.50, average training loss: 7071.72, base loss: 15936.46
[INFO 2017-06-30 02:12:28,594 main.py:52] epoch 1425, training loss: 7101.15, average training loss: 7070.42, base loss: 15937.15
[INFO 2017-06-30 02:12:31,765 main.py:52] epoch 1426, training loss: 6473.33, average training loss: 7069.29, base loss: 15935.71
[INFO 2017-06-30 02:12:34,983 main.py:52] epoch 1427, training loss: 6571.16, average training loss: 7067.80, base loss: 15936.29
[INFO 2017-06-30 02:12:38,142 main.py:52] epoch 1428, training loss: 6424.94, average training loss: 7065.80, base loss: 15934.77
[INFO 2017-06-30 02:12:41,316 main.py:52] epoch 1429, training loss: 6741.78, average training loss: 7064.45, base loss: 15935.20
[INFO 2017-06-30 02:12:44,513 main.py:52] epoch 1430, training loss: 6751.49, average training loss: 7062.51, base loss: 15935.24
[INFO 2017-06-30 02:12:47,715 main.py:52] epoch 1431, training loss: 6190.26, average training loss: 7061.05, base loss: 15933.66
[INFO 2017-06-30 02:12:50,884 main.py:52] epoch 1432, training loss: 6807.83, average training loss: 7059.85, base loss: 15933.40
[INFO 2017-06-30 02:12:54,035 main.py:52] epoch 1433, training loss: 6708.06, average training loss: 7058.41, base loss: 15933.66
[INFO 2017-06-30 02:12:57,241 main.py:52] epoch 1434, training loss: 6915.65, average training loss: 7057.33, base loss: 15934.55
[INFO 2017-06-30 02:13:00,428 main.py:52] epoch 1435, training loss: 6767.40, average training loss: 7057.14, base loss: 15934.12
[INFO 2017-06-30 02:13:03,634 main.py:52] epoch 1436, training loss: 6393.85, average training loss: 7055.44, base loss: 15934.40
[INFO 2017-06-30 02:13:06,835 main.py:52] epoch 1437, training loss: 6724.15, average training loss: 7054.83, base loss: 15935.76
[INFO 2017-06-30 02:13:09,996 main.py:52] epoch 1438, training loss: 7208.13, average training loss: 7053.99, base loss: 15936.40
[INFO 2017-06-30 02:13:13,170 main.py:52] epoch 1439, training loss: 6288.73, average training loss: 7052.95, base loss: 15935.31
[INFO 2017-06-30 02:13:16,335 main.py:52] epoch 1440, training loss: 7091.21, average training loss: 7051.46, base loss: 15937.45
[INFO 2017-06-30 02:13:19,540 main.py:52] epoch 1441, training loss: 6768.12, average training loss: 7050.23, base loss: 15938.66
[INFO 2017-06-30 02:13:22,679 main.py:52] epoch 1442, training loss: 6993.93, average training loss: 7049.99, base loss: 15939.89
[INFO 2017-06-30 02:13:25,871 main.py:52] epoch 1443, training loss: 6581.82, average training loss: 7049.16, base loss: 15938.43
[INFO 2017-06-30 02:13:29,022 main.py:52] epoch 1444, training loss: 7070.40, average training loss: 7048.44, base loss: 15940.13
[INFO 2017-06-30 02:13:32,168 main.py:52] epoch 1445, training loss: 7019.78, average training loss: 7047.73, base loss: 15940.28
[INFO 2017-06-30 02:13:35,360 main.py:52] epoch 1446, training loss: 7323.67, average training loss: 7047.77, base loss: 15943.69
[INFO 2017-06-30 02:13:38,551 main.py:52] epoch 1447, training loss: 6702.33, average training loss: 7047.29, base loss: 15944.97
[INFO 2017-06-30 02:13:41,738 main.py:52] epoch 1448, training loss: 6305.77, average training loss: 7046.87, base loss: 15945.06
[INFO 2017-06-30 02:13:44,851 main.py:52] epoch 1449, training loss: 6314.74, average training loss: 7046.57, base loss: 15944.65
[INFO 2017-06-30 02:13:48,028 main.py:52] epoch 1450, training loss: 6909.58, average training loss: 7046.06, base loss: 15943.11
[INFO 2017-06-30 02:13:51,180 main.py:52] epoch 1451, training loss: 6351.47, average training loss: 7044.37, base loss: 15941.44
[INFO 2017-06-30 02:13:54,342 main.py:52] epoch 1452, training loss: 6380.00, average training loss: 7042.52, base loss: 15940.87
[INFO 2017-06-30 02:13:57,513 main.py:52] epoch 1453, training loss: 5920.70, average training loss: 7041.38, base loss: 15938.58
[INFO 2017-06-30 02:14:00,690 main.py:52] epoch 1454, training loss: 6121.39, average training loss: 7040.69, base loss: 15935.35
[INFO 2017-06-30 02:14:03,867 main.py:52] epoch 1455, training loss: 6314.24, average training loss: 7039.76, base loss: 15933.02
[INFO 2017-06-30 02:14:07,031 main.py:52] epoch 1456, training loss: 6929.84, average training loss: 7039.21, base loss: 15934.02
[INFO 2017-06-30 02:14:10,213 main.py:52] epoch 1457, training loss: 6258.63, average training loss: 7038.03, base loss: 15932.54
[INFO 2017-06-30 02:14:13,372 main.py:52] epoch 1458, training loss: 6859.00, average training loss: 7037.45, base loss: 15933.10
[INFO 2017-06-30 02:14:16,526 main.py:52] epoch 1459, training loss: 6158.59, average training loss: 7035.65, base loss: 15931.46
[INFO 2017-06-30 02:14:19,696 main.py:52] epoch 1460, training loss: 6160.51, average training loss: 7034.21, base loss: 15929.68
[INFO 2017-06-30 02:14:22,926 main.py:52] epoch 1461, training loss: 6155.60, average training loss: 7033.46, base loss: 15927.94
[INFO 2017-06-30 02:14:26,107 main.py:52] epoch 1462, training loss: 6885.75, average training loss: 7032.61, base loss: 15928.75
[INFO 2017-06-30 02:14:29,283 main.py:52] epoch 1463, training loss: 6150.75, average training loss: 7031.43, base loss: 15927.45
[INFO 2017-06-30 02:14:32,462 main.py:52] epoch 1464, training loss: 6917.30, average training loss: 7029.85, base loss: 15928.74
[INFO 2017-06-30 02:14:35,645 main.py:52] epoch 1465, training loss: 6616.45, average training loss: 7029.10, base loss: 15928.12
[INFO 2017-06-30 02:14:38,805 main.py:52] epoch 1466, training loss: 6671.33, average training loss: 7027.79, base loss: 15928.14
[INFO 2017-06-30 02:14:41,948 main.py:52] epoch 1467, training loss: 6297.41, average training loss: 7026.39, base loss: 15927.00
[INFO 2017-06-30 02:14:45,101 main.py:52] epoch 1468, training loss: 6593.60, average training loss: 7025.67, base loss: 15927.36
[INFO 2017-06-30 02:14:48,332 main.py:52] epoch 1469, training loss: 6365.33, average training loss: 7023.63, base loss: 15926.90
[INFO 2017-06-30 02:14:51,501 main.py:52] epoch 1470, training loss: 6501.09, average training loss: 7022.49, base loss: 15926.33
[INFO 2017-06-30 02:14:54,618 main.py:52] epoch 1471, training loss: 6705.83, average training loss: 7022.03, base loss: 15926.50
[INFO 2017-06-30 02:14:57,773 main.py:52] epoch 1472, training loss: 6106.82, average training loss: 7020.40, base loss: 15925.54
[INFO 2017-06-30 02:15:00,944 main.py:52] epoch 1473, training loss: 7053.79, average training loss: 7020.29, base loss: 15925.19
[INFO 2017-06-30 02:15:04,110 main.py:52] epoch 1474, training loss: 7006.93, average training loss: 7019.99, base loss: 15925.59
[INFO 2017-06-30 02:15:07,235 main.py:52] epoch 1475, training loss: 6736.72, average training loss: 7017.94, base loss: 15925.90
[INFO 2017-06-30 02:15:10,437 main.py:52] epoch 1476, training loss: 6755.24, average training loss: 7016.34, base loss: 15927.21
[INFO 2017-06-30 02:15:13,593 main.py:52] epoch 1477, training loss: 6401.04, average training loss: 7015.69, base loss: 15928.39
[INFO 2017-06-30 02:15:16,790 main.py:52] epoch 1478, training loss: 7193.57, average training loss: 7015.31, base loss: 15930.05
[INFO 2017-06-30 02:15:20,005 main.py:52] epoch 1479, training loss: 6692.00, average training loss: 7013.94, base loss: 15930.07
[INFO 2017-06-30 02:15:23,172 main.py:52] epoch 1480, training loss: 6958.92, average training loss: 7012.91, base loss: 15930.35
[INFO 2017-06-30 02:15:26,419 main.py:52] epoch 1481, training loss: 6766.83, average training loss: 7012.29, base loss: 15930.43
[INFO 2017-06-30 02:15:29,578 main.py:52] epoch 1482, training loss: 7023.24, average training loss: 7011.34, base loss: 15931.41
[INFO 2017-06-30 02:15:32,752 main.py:52] epoch 1483, training loss: 6207.49, average training loss: 7010.72, base loss: 15931.66
[INFO 2017-06-30 02:15:35,893 main.py:52] epoch 1484, training loss: 6183.90, average training loss: 7009.74, base loss: 15930.29
[INFO 2017-06-30 02:15:39,048 main.py:52] epoch 1485, training loss: 6684.30, average training loss: 7008.74, base loss: 15932.14
[INFO 2017-06-30 02:15:42,213 main.py:52] epoch 1486, training loss: 7113.08, average training loss: 7008.28, base loss: 15934.71
[INFO 2017-06-30 02:15:45,352 main.py:52] epoch 1487, training loss: 6519.82, average training loss: 7007.22, base loss: 15935.35
[INFO 2017-06-30 02:15:48,500 main.py:52] epoch 1488, training loss: 6380.25, average training loss: 7005.68, base loss: 15935.39
[INFO 2017-06-30 02:15:51,677 main.py:52] epoch 1489, training loss: 6637.18, average training loss: 7004.69, base loss: 15935.61
[INFO 2017-06-30 02:15:54,822 main.py:52] epoch 1490, training loss: 6866.81, average training loss: 7004.13, base loss: 15935.44
[INFO 2017-06-30 02:15:57,968 main.py:52] epoch 1491, training loss: 6902.54, average training loss: 7004.05, base loss: 15935.94
[INFO 2017-06-30 02:16:01,125 main.py:52] epoch 1492, training loss: 6094.06, average training loss: 7003.00, base loss: 15934.63
[INFO 2017-06-30 02:16:04,335 main.py:52] epoch 1493, training loss: 6613.36, average training loss: 7002.54, base loss: 15934.28
[INFO 2017-06-30 02:16:07,522 main.py:52] epoch 1494, training loss: 6061.91, average training loss: 7001.14, base loss: 15932.70
[INFO 2017-06-30 02:16:10,719 main.py:52] epoch 1495, training loss: 6978.08, average training loss: 7000.58, base loss: 15934.66
[INFO 2017-06-30 02:16:13,874 main.py:52] epoch 1496, training loss: 6640.55, average training loss: 6999.76, base loss: 15935.68
[INFO 2017-06-30 02:16:17,051 main.py:52] epoch 1497, training loss: 6759.61, average training loss: 6999.18, base loss: 15937.17
[INFO 2017-06-30 02:16:20,245 main.py:52] epoch 1498, training loss: 6578.02, average training loss: 6998.10, base loss: 15938.10
[INFO 2017-06-30 02:16:23,431 main.py:52] epoch 1499, training loss: 6473.17, average training loss: 6996.97, base loss: 15938.35
[INFO 2017-06-30 02:16:23,431 main.py:54] epoch 1499, testing
[INFO 2017-06-30 02:16:36,830 main.py:97] average testing loss: 6643.86, base loss: 16243.91
[INFO 2017-06-30 02:16:36,830 main.py:98] improve_loss: 9600.04, improve_percent: 0.59
[INFO 2017-06-30 02:16:36,831 main.py:62] model save to ./model/final.pth
[INFO 2017-06-30 02:16:36,866 main.py:66] current best improved percent: 0.59
[INFO 2017-06-30 02:16:40,041 main.py:52] epoch 1500, training loss: 6738.79, average training loss: 6996.13, base loss: 15937.66
[INFO 2017-06-30 02:16:43,194 main.py:52] epoch 1501, training loss: 6610.23, average training loss: 6994.98, base loss: 15937.92
[INFO 2017-06-30 02:16:46,437 main.py:52] epoch 1502, training loss: 7039.46, average training loss: 6994.52, base loss: 15939.18
[INFO 2017-06-30 02:16:49,614 main.py:52] epoch 1503, training loss: 7175.85, average training loss: 6994.32, base loss: 15941.90
[INFO 2017-06-30 02:16:52,770 main.py:52] epoch 1504, training loss: 6081.35, average training loss: 6992.51, base loss: 15941.24
[INFO 2017-06-30 02:16:55,945 main.py:52] epoch 1505, training loss: 6976.06, average training loss: 6991.76, base loss: 15942.70
[INFO 2017-06-30 02:16:59,118 main.py:52] epoch 1506, training loss: 6291.61, average training loss: 6989.97, base loss: 15942.44
[INFO 2017-06-30 02:17:02,261 main.py:52] epoch 1507, training loss: 6430.79, average training loss: 6988.88, base loss: 15941.33
[INFO 2017-06-30 02:17:05,425 main.py:52] epoch 1508, training loss: 6648.76, average training loss: 6987.49, base loss: 15940.46
[INFO 2017-06-30 02:17:08,627 main.py:52] epoch 1509, training loss: 6240.65, average training loss: 6985.64, base loss: 15939.58
[INFO 2017-06-30 02:17:11,781 main.py:52] epoch 1510, training loss: 7082.55, average training loss: 6985.23, base loss: 15940.07
[INFO 2017-06-30 02:17:14,967 main.py:52] epoch 1511, training loss: 6517.70, average training loss: 6984.04, base loss: 15939.81
[INFO 2017-06-30 02:17:18,126 main.py:52] epoch 1512, training loss: 6366.88, average training loss: 6982.83, base loss: 15938.22
[INFO 2017-06-30 02:17:21,254 main.py:52] epoch 1513, training loss: 6534.32, average training loss: 6981.73, base loss: 15936.93
[INFO 2017-06-30 02:17:24,443 main.py:52] epoch 1514, training loss: 6919.64, average training loss: 6980.60, base loss: 15938.49
[INFO 2017-06-30 02:17:27,629 main.py:52] epoch 1515, training loss: 6552.71, average training loss: 6979.47, base loss: 15937.57
[INFO 2017-06-30 02:17:30,816 main.py:52] epoch 1516, training loss: 6477.94, average training loss: 6978.84, base loss: 15937.12
[INFO 2017-06-30 02:17:33,996 main.py:52] epoch 1517, training loss: 6488.11, average training loss: 6977.11, base loss: 15935.55
[INFO 2017-06-30 02:17:37,186 main.py:52] epoch 1518, training loss: 6509.80, average training loss: 6976.21, base loss: 15935.72
[INFO 2017-06-30 02:17:40,396 main.py:52] epoch 1519, training loss: 6072.79, average training loss: 6975.19, base loss: 15934.66
[INFO 2017-06-30 02:17:43,578 main.py:52] epoch 1520, training loss: 6759.83, average training loss: 6974.86, base loss: 15934.85
[INFO 2017-06-30 02:17:46,765 main.py:52] epoch 1521, training loss: 6478.33, average training loss: 6973.85, base loss: 15934.67
[INFO 2017-06-30 02:17:49,926 main.py:52] epoch 1522, training loss: 6651.66, average training loss: 6973.43, base loss: 15933.76
[INFO 2017-06-30 02:17:53,117 main.py:52] epoch 1523, training loss: 6240.90, average training loss: 6972.61, base loss: 15933.12
[INFO 2017-06-30 02:17:56,317 main.py:52] epoch 1524, training loss: 6317.09, average training loss: 6971.23, base loss: 15933.24
[INFO 2017-06-30 02:17:59,519 main.py:52] epoch 1525, training loss: 6121.60, average training loss: 6969.50, base loss: 15932.69
[INFO 2017-06-30 02:18:02,747 main.py:52] epoch 1526, training loss: 6558.18, average training loss: 6968.30, base loss: 15933.98
[INFO 2017-06-30 02:18:05,885 main.py:52] epoch 1527, training loss: 6914.24, average training loss: 6967.34, base loss: 15934.64
[INFO 2017-06-30 02:18:09,071 main.py:52] epoch 1528, training loss: 6455.09, average training loss: 6966.00, base loss: 15933.45
[INFO 2017-06-30 02:18:12,276 main.py:52] epoch 1529, training loss: 6631.36, average training loss: 6965.63, base loss: 15930.93
[INFO 2017-06-30 02:18:15,439 main.py:52] epoch 1530, training loss: 6465.40, average training loss: 6964.49, base loss: 15930.36
[INFO 2017-06-30 02:18:18,598 main.py:52] epoch 1531, training loss: 6496.90, average training loss: 6963.55, base loss: 15929.95
[INFO 2017-06-30 02:18:21,801 main.py:52] epoch 1532, training loss: 6328.36, average training loss: 6962.86, base loss: 15929.24
[INFO 2017-06-30 02:18:24,991 main.py:52] epoch 1533, training loss: 6775.44, average training loss: 6962.62, base loss: 15929.90
[INFO 2017-06-30 02:18:28,223 main.py:52] epoch 1534, training loss: 5995.43, average training loss: 6961.64, base loss: 15928.13
[INFO 2017-06-30 02:18:31,487 main.py:52] epoch 1535, training loss: 7285.01, average training loss: 6961.89, base loss: 15928.37
[INFO 2017-06-30 02:18:34,690 main.py:52] epoch 1536, training loss: 6666.27, average training loss: 6961.12, base loss: 15929.21
[INFO 2017-06-30 02:18:37,894 main.py:52] epoch 1537, training loss: 6560.38, average training loss: 6960.54, base loss: 15929.32
[INFO 2017-06-30 02:18:41,117 main.py:52] epoch 1538, training loss: 6654.89, average training loss: 6960.07, base loss: 15929.87
[INFO 2017-06-30 02:18:44,280 main.py:52] epoch 1539, training loss: 6291.29, average training loss: 6958.98, base loss: 15931.21
[INFO 2017-06-30 02:18:47,475 main.py:52] epoch 1540, training loss: 6411.33, average training loss: 6958.28, base loss: 15931.12
[INFO 2017-06-30 02:18:50,620 main.py:52] epoch 1541, training loss: 6931.52, average training loss: 6958.09, base loss: 15932.88
[INFO 2017-06-30 02:18:53,816 main.py:52] epoch 1542, training loss: 6398.10, average training loss: 6957.40, base loss: 15932.41
[INFO 2017-06-30 02:18:57,004 main.py:52] epoch 1543, training loss: 7097.29, average training loss: 6957.39, base loss: 15933.78
[INFO 2017-06-30 02:19:00,176 main.py:52] epoch 1544, training loss: 6903.33, average training loss: 6956.17, base loss: 15934.26
[INFO 2017-06-30 02:19:03,336 main.py:52] epoch 1545, training loss: 6978.14, average training loss: 6955.73, base loss: 15936.02
[INFO 2017-06-30 02:19:06,498 main.py:52] epoch 1546, training loss: 6543.97, average training loss: 6954.96, base loss: 15937.09
[INFO 2017-06-30 02:19:09,643 main.py:52] epoch 1547, training loss: 6911.33, average training loss: 6954.69, base loss: 15938.63
[INFO 2017-06-30 02:19:12,809 main.py:52] epoch 1548, training loss: 6561.40, average training loss: 6953.67, base loss: 15938.15
[INFO 2017-06-30 02:19:15,975 main.py:52] epoch 1549, training loss: 6278.74, average training loss: 6952.93, base loss: 15935.83
[INFO 2017-06-30 02:19:19,132 main.py:52] epoch 1550, training loss: 6723.36, average training loss: 6952.62, base loss: 15935.92
[INFO 2017-06-30 02:19:22,326 main.py:52] epoch 1551, training loss: 6539.11, average training loss: 6951.89, base loss: 15935.78
[INFO 2017-06-30 02:19:25,499 main.py:52] epoch 1552, training loss: 6083.25, average training loss: 6950.57, base loss: 15935.79
[INFO 2017-06-30 02:19:28,658 main.py:52] epoch 1553, training loss: 6123.19, average training loss: 6949.00, base loss: 15935.67
[INFO 2017-06-30 02:19:31,863 main.py:52] epoch 1554, training loss: 6225.53, average training loss: 6947.75, base loss: 15934.87
[INFO 2017-06-30 02:19:35,011 main.py:52] epoch 1555, training loss: 6934.51, average training loss: 6946.88, base loss: 15936.29
[INFO 2017-06-30 02:19:38,167 main.py:52] epoch 1556, training loss: 6756.82, average training loss: 6946.12, base loss: 15936.45
[INFO 2017-06-30 02:19:41,345 main.py:52] epoch 1557, training loss: 6387.17, average training loss: 6945.71, base loss: 15935.21
[INFO 2017-06-30 02:19:44,535 main.py:52] epoch 1558, training loss: 6468.67, average training loss: 6944.74, base loss: 15935.78
[INFO 2017-06-30 02:19:47,703 main.py:52] epoch 1559, training loss: 6562.54, average training loss: 6944.20, base loss: 15935.50
[INFO 2017-06-30 02:19:50,876 main.py:52] epoch 1560, training loss: 6756.10, average training loss: 6944.01, base loss: 15936.34
[INFO 2017-06-30 02:19:54,041 main.py:52] epoch 1561, training loss: 6305.84, average training loss: 6942.86, base loss: 15935.93
[INFO 2017-06-30 02:19:57,240 main.py:52] epoch 1562, training loss: 6632.90, average training loss: 6942.83, base loss: 15934.79
[INFO 2017-06-30 02:20:00,379 main.py:52] epoch 1563, training loss: 6558.11, average training loss: 6942.76, base loss: 15934.12
[INFO 2017-06-30 02:20:03,569 main.py:52] epoch 1564, training loss: 6352.84, average training loss: 6942.47, base loss: 15935.10
[INFO 2017-06-30 02:20:06,736 main.py:52] epoch 1565, training loss: 7165.84, average training loss: 6943.03, base loss: 15935.76
[INFO 2017-06-30 02:20:09,871 main.py:52] epoch 1566, training loss: 7048.58, average training loss: 6942.47, base loss: 15937.11
[INFO 2017-06-30 02:20:13,052 main.py:52] epoch 1567, training loss: 6427.04, average training loss: 6941.76, base loss: 15937.55
[INFO 2017-06-30 02:20:16,247 main.py:52] epoch 1568, training loss: 5978.81, average training loss: 6940.23, base loss: 15937.22
[INFO 2017-06-30 02:20:19,459 main.py:52] epoch 1569, training loss: 5912.81, average training loss: 6938.51, base loss: 15935.27
[INFO 2017-06-30 02:20:22,643 main.py:52] epoch 1570, training loss: 6826.73, average training loss: 6937.31, base loss: 15935.77
[INFO 2017-06-30 02:20:25,783 main.py:52] epoch 1571, training loss: 6513.94, average training loss: 6936.21, base loss: 15935.25
[INFO 2017-06-30 02:20:28,977 main.py:52] epoch 1572, training loss: 6904.36, average training loss: 6935.84, base loss: 15937.27
[INFO 2017-06-30 02:20:32,126 main.py:52] epoch 1573, training loss: 6492.96, average training loss: 6934.38, base loss: 15936.83
[INFO 2017-06-30 02:20:35,296 main.py:52] epoch 1574, training loss: 6442.20, average training loss: 6933.70, base loss: 15935.69
[INFO 2017-06-30 02:20:38,511 main.py:52] epoch 1575, training loss: 6669.46, average training loss: 6933.05, base loss: 15936.28
[INFO 2017-06-30 02:20:41,733 main.py:52] epoch 1576, training loss: 6309.03, average training loss: 6931.98, base loss: 15935.49
[INFO 2017-06-30 02:20:44,979 main.py:52] epoch 1577, training loss: 7121.37, average training loss: 6931.36, base loss: 15938.71
[INFO 2017-06-30 02:20:48,163 main.py:52] epoch 1578, training loss: 6275.30, average training loss: 6930.13, base loss: 15938.50
[INFO 2017-06-30 02:20:51,318 main.py:52] epoch 1579, training loss: 6729.97, average training loss: 6929.60, base loss: 15938.98
[INFO 2017-06-30 02:20:54,482 main.py:52] epoch 1580, training loss: 6925.15, average training loss: 6929.32, base loss: 15939.19
[INFO 2017-06-30 02:20:57,688 main.py:52] epoch 1581, training loss: 6741.09, average training loss: 6928.71, base loss: 15938.88
[INFO 2017-06-30 02:21:00,928 main.py:52] epoch 1582, training loss: 6441.10, average training loss: 6927.73, base loss: 15938.78
[INFO 2017-06-30 02:21:04,107 main.py:52] epoch 1583, training loss: 6717.62, average training loss: 6927.37, base loss: 15939.26
[INFO 2017-06-30 02:21:07,298 main.py:52] epoch 1584, training loss: 6495.37, average training loss: 6926.84, base loss: 15939.80
[INFO 2017-06-30 02:21:10,509 main.py:52] epoch 1585, training loss: 6291.73, average training loss: 6925.95, base loss: 15938.90
[INFO 2017-06-30 02:21:13,609 main.py:52] epoch 1586, training loss: 7073.77, average training loss: 6926.12, base loss: 15941.37
[INFO 2017-06-30 02:21:16,756 main.py:52] epoch 1587, training loss: 6063.88, average training loss: 6924.30, base loss: 15940.92
[INFO 2017-06-30 02:21:19,914 main.py:52] epoch 1588, training loss: 6858.79, average training loss: 6924.35, base loss: 15940.87
[INFO 2017-06-30 02:21:23,090 main.py:52] epoch 1589, training loss: 6982.56, average training loss: 6923.26, base loss: 15942.52
[INFO 2017-06-30 02:21:26,247 main.py:52] epoch 1590, training loss: 7286.29, average training loss: 6923.26, base loss: 15944.99
[INFO 2017-06-30 02:21:29,385 main.py:52] epoch 1591, training loss: 6402.93, average training loss: 6922.01, base loss: 15944.31
[INFO 2017-06-30 02:21:32,559 main.py:52] epoch 1592, training loss: 7410.53, average training loss: 6921.83, base loss: 15945.98
[INFO 2017-06-30 02:21:35,678 main.py:52] epoch 1593, training loss: 6097.71, average training loss: 6920.35, base loss: 15944.39
[INFO 2017-06-30 02:21:38,857 main.py:52] epoch 1594, training loss: 6665.70, average training loss: 6920.04, base loss: 15945.53
[INFO 2017-06-30 02:21:42,040 main.py:52] epoch 1595, training loss: 6018.95, average training loss: 6919.04, base loss: 15944.43
[INFO 2017-06-30 02:21:45,218 main.py:52] epoch 1596, training loss: 6861.34, average training loss: 6917.96, base loss: 15944.75
[INFO 2017-06-30 02:21:48,416 main.py:52] epoch 1597, training loss: 6619.48, average training loss: 6917.24, base loss: 15944.74
[INFO 2017-06-30 02:21:51,596 main.py:52] epoch 1598, training loss: 6751.62, average training loss: 6916.66, base loss: 15944.69
[INFO 2017-06-30 02:21:54,777 main.py:52] epoch 1599, training loss: 6513.40, average training loss: 6915.55, base loss: 15943.94
[INFO 2017-06-30 02:21:54,777 main.py:54] epoch 1599, testing
[INFO 2017-06-30 02:22:08,098 main.py:97] average testing loss: 6476.28, base loss: 15821.73
[INFO 2017-06-30 02:22:08,099 main.py:98] improve_loss: 9345.45, improve_percent: 0.59
[INFO 2017-06-30 02:22:08,100 main.py:66] current best improved percent: 0.59
[INFO 2017-06-30 02:22:11,239 main.py:52] epoch 1600, training loss: 6469.30, average training loss: 6914.73, base loss: 15943.81
[INFO 2017-06-30 02:22:14,445 main.py:52] epoch 1601, training loss: 6787.65, average training loss: 6914.28, base loss: 15945.46
[INFO 2017-06-30 02:22:17,639 main.py:52] epoch 1602, training loss: 5979.14, average training loss: 6912.38, base loss: 15943.68
[INFO 2017-06-30 02:22:20,801 main.py:52] epoch 1603, training loss: 7041.00, average training loss: 6911.70, base loss: 15945.85
[INFO 2017-06-30 02:22:23,972 main.py:52] epoch 1604, training loss: 6549.40, average training loss: 6910.60, base loss: 15946.83
[INFO 2017-06-30 02:22:27,117 main.py:52] epoch 1605, training loss: 6980.68, average training loss: 6910.39, base loss: 15948.20
[INFO 2017-06-30 02:22:30,259 main.py:52] epoch 1606, training loss: 7080.76, average training loss: 6910.77, base loss: 15949.91
[INFO 2017-06-30 02:22:33,434 main.py:52] epoch 1607, training loss: 6703.73, average training loss: 6910.37, base loss: 15949.45
[INFO 2017-06-30 02:22:36,594 main.py:52] epoch 1608, training loss: 6542.13, average training loss: 6909.65, base loss: 15949.14
[INFO 2017-06-30 02:22:39,765 main.py:52] epoch 1609, training loss: 7350.70, average training loss: 6909.58, base loss: 15949.83
[INFO 2017-06-30 02:22:42,936 main.py:52] epoch 1610, training loss: 6610.55, average training loss: 6908.87, base loss: 15949.78
[INFO 2017-06-30 02:22:46,110 main.py:52] epoch 1611, training loss: 6754.66, average training loss: 6908.53, base loss: 15950.44
[INFO 2017-06-30 02:22:49,255 main.py:52] epoch 1612, training loss: 6192.54, average training loss: 6906.29, base loss: 15948.87
[INFO 2017-06-30 02:22:52,447 main.py:52] epoch 1613, training loss: 6528.59, average training loss: 6905.71, base loss: 15949.00
[INFO 2017-06-30 02:22:55,622 main.py:52] epoch 1614, training loss: 6483.66, average training loss: 6905.02, base loss: 15949.48
[INFO 2017-06-30 02:22:58,796 main.py:52] epoch 1615, training loss: 6571.17, average training loss: 6903.89, base loss: 15950.10
[INFO 2017-06-30 02:23:01,930 main.py:52] epoch 1616, training loss: 6559.93, average training loss: 6903.41, base loss: 15950.55
[INFO 2017-06-30 02:23:05,095 main.py:52] epoch 1617, training loss: 6869.92, average training loss: 6903.65, base loss: 15951.52
[INFO 2017-06-30 02:23:08,257 main.py:52] epoch 1618, training loss: 6500.38, average training loss: 6903.00, base loss: 15953.06
[INFO 2017-06-30 02:23:11,402 main.py:52] epoch 1619, training loss: 7022.81, average training loss: 6901.82, base loss: 15955.62
[INFO 2017-06-30 02:23:14,598 main.py:52] epoch 1620, training loss: 6767.17, average training loss: 6900.23, base loss: 15957.52
[INFO 2017-06-30 02:23:17,751 main.py:52] epoch 1621, training loss: 6712.52, average training loss: 6899.57, base loss: 15957.98
[INFO 2017-06-30 02:23:20,903 main.py:52] epoch 1622, training loss: 7107.30, average training loss: 6899.55, base loss: 15959.41
[INFO 2017-06-30 02:23:24,053 main.py:52] epoch 1623, training loss: 6921.34, average training loss: 6899.27, base loss: 15959.67
[INFO 2017-06-30 02:23:27,176 main.py:52] epoch 1624, training loss: 6778.33, average training loss: 6898.40, base loss: 15960.99
[INFO 2017-06-30 02:23:30,371 main.py:52] epoch 1625, training loss: 6228.99, average training loss: 6897.09, base loss: 15961.24
[INFO 2017-06-30 02:23:33,530 main.py:52] epoch 1626, training loss: 5962.69, average training loss: 6895.95, base loss: 15960.61
[INFO 2017-06-30 02:23:36,701 main.py:52] epoch 1627, training loss: 7284.94, average training loss: 6896.52, base loss: 15963.78
[INFO 2017-06-30 02:23:39,886 main.py:52] epoch 1628, training loss: 5977.19, average training loss: 6894.72, base loss: 15963.01
[INFO 2017-06-30 02:23:43,035 main.py:52] epoch 1629, training loss: 6957.03, average training loss: 6895.13, base loss: 15964.65
[INFO 2017-06-30 02:23:46,198 main.py:52] epoch 1630, training loss: 7419.31, average training loss: 6895.01, base loss: 15967.18
[INFO 2017-06-30 02:23:49,405 main.py:52] epoch 1631, training loss: 6403.79, average training loss: 6894.20, base loss: 15966.83
[INFO 2017-06-30 02:23:52,558 main.py:52] epoch 1632, training loss: 6513.58, average training loss: 6893.28, base loss: 15967.06
[INFO 2017-06-30 02:23:55,720 main.py:52] epoch 1633, training loss: 6744.69, average training loss: 6892.83, base loss: 15968.07
[INFO 2017-06-30 02:23:58,891 main.py:52] epoch 1634, training loss: 6581.75, average training loss: 6892.50, base loss: 15968.15
[INFO 2017-06-30 02:24:02,102 main.py:52] epoch 1635, training loss: 6241.88, average training loss: 6892.11, base loss: 15966.96
[INFO 2017-06-30 02:24:05,295 main.py:52] epoch 1636, training loss: 6866.90, average training loss: 6892.05, base loss: 15967.42
[INFO 2017-06-30 02:24:08,478 main.py:52] epoch 1637, training loss: 6310.65, average training loss: 6891.01, base loss: 15965.47
[INFO 2017-06-30 02:24:11,613 main.py:52] epoch 1638, training loss: 6208.25, average training loss: 6890.24, base loss: 15965.35
[INFO 2017-06-30 02:24:14,806 main.py:52] epoch 1639, training loss: 6754.52, average training loss: 6889.80, base loss: 15966.23
[INFO 2017-06-30 02:24:17,966 main.py:52] epoch 1640, training loss: 6125.32, average training loss: 6888.84, base loss: 15964.38
[INFO 2017-06-30 02:24:21,175 main.py:52] epoch 1641, training loss: 6220.07, average training loss: 6887.79, base loss: 15962.85
[INFO 2017-06-30 02:24:24,379 main.py:52] epoch 1642, training loss: 6386.73, average training loss: 6886.95, base loss: 15962.92
[INFO 2017-06-30 02:24:27,524 main.py:52] epoch 1643, training loss: 6123.12, average training loss: 6885.17, base loss: 15961.62
[INFO 2017-06-30 02:24:30,659 main.py:52] epoch 1644, training loss: 6565.70, average training loss: 6884.37, base loss: 15962.05
[INFO 2017-06-30 02:24:33,827 main.py:52] epoch 1645, training loss: 5848.30, average training loss: 6882.70, base loss: 15961.73
[INFO 2017-06-30 02:24:36,970 main.py:52] epoch 1646, training loss: 5908.63, average training loss: 6881.03, base loss: 15959.88
[INFO 2017-06-30 02:24:40,135 main.py:52] epoch 1647, training loss: 6299.81, average training loss: 6880.67, base loss: 15959.15
[INFO 2017-06-30 02:24:43,275 main.py:52] epoch 1648, training loss: 6552.87, average training loss: 6879.60, base loss: 15959.80
[INFO 2017-06-30 02:24:46,454 main.py:52] epoch 1649, training loss: 6549.22, average training loss: 6878.83, base loss: 15960.53
[INFO 2017-06-30 02:24:49,617 main.py:52] epoch 1650, training loss: 6518.78, average training loss: 6878.14, base loss: 15960.32
[INFO 2017-06-30 02:24:52,802 main.py:52] epoch 1651, training loss: 6335.69, average training loss: 6877.46, base loss: 15959.73
[INFO 2017-06-30 02:24:55,944 main.py:52] epoch 1652, training loss: 6320.82, average training loss: 6876.49, base loss: 15960.29
[INFO 2017-06-30 02:24:59,094 main.py:52] epoch 1653, training loss: 6208.88, average training loss: 6875.90, base loss: 15959.55
[INFO 2017-06-30 02:25:02,318 main.py:52] epoch 1654, training loss: 6211.21, average training loss: 6874.50, base loss: 15957.44
[INFO 2017-06-30 02:25:05,504 main.py:52] epoch 1655, training loss: 6627.80, average training loss: 6873.08, base loss: 15957.14
[INFO 2017-06-30 02:25:08,692 main.py:52] epoch 1656, training loss: 6395.68, average training loss: 6871.38, base loss: 15956.48
[INFO 2017-06-30 02:25:11,874 main.py:52] epoch 1657, training loss: 6259.38, average training loss: 6870.59, base loss: 15955.61
[INFO 2017-06-30 02:25:15,002 main.py:52] epoch 1658, training loss: 6316.91, average training loss: 6869.14, base loss: 15954.12
[INFO 2017-06-30 02:25:18,221 main.py:52] epoch 1659, training loss: 5885.25, average training loss: 6867.44, base loss: 15952.67
[INFO 2017-06-30 02:25:21,353 main.py:52] epoch 1660, training loss: 7048.06, average training loss: 6866.79, base loss: 15954.18
[INFO 2017-06-30 02:25:24,554 main.py:52] epoch 1661, training loss: 6390.17, average training loss: 6865.62, base loss: 15954.62
[INFO 2017-06-30 02:25:27,749 main.py:52] epoch 1662, training loss: 6913.04, average training loss: 6864.54, base loss: 15955.06
[INFO 2017-06-30 02:25:30,924 main.py:52] epoch 1663, training loss: 6730.90, average training loss: 6863.86, base loss: 15955.26
[INFO 2017-06-30 02:25:34,099 main.py:52] epoch 1664, training loss: 6959.96, average training loss: 6862.72, base loss: 15957.27
[INFO 2017-06-30 02:25:37,255 main.py:52] epoch 1665, training loss: 7043.62, average training loss: 6862.43, base loss: 15959.17
[INFO 2017-06-30 02:25:40,401 main.py:52] epoch 1666, training loss: 6692.06, average training loss: 6861.55, base loss: 15959.88
[INFO 2017-06-30 02:25:43,593 main.py:52] epoch 1667, training loss: 6569.07, average training loss: 6860.76, base loss: 15960.36
[INFO 2017-06-30 02:25:46,742 main.py:52] epoch 1668, training loss: 6461.22, average training loss: 6859.28, base loss: 15960.14
[INFO 2017-06-30 02:25:49,901 main.py:52] epoch 1669, training loss: 6250.49, average training loss: 6857.64, base loss: 15960.22
[INFO 2017-06-30 02:25:53,086 main.py:52] epoch 1670, training loss: 6778.28, average training loss: 6857.19, base loss: 15960.63
[INFO 2017-06-30 02:25:56,235 main.py:52] epoch 1671, training loss: 6886.11, average training loss: 6856.80, base loss: 15962.10
[INFO 2017-06-30 02:25:59,417 main.py:52] epoch 1672, training loss: 6247.67, average training loss: 6855.97, base loss: 15961.48
[INFO 2017-06-30 02:26:02,542 main.py:52] epoch 1673, training loss: 6410.33, average training loss: 6854.49, base loss: 15961.85
[INFO 2017-06-30 02:26:05,716 main.py:52] epoch 1674, training loss: 6355.21, average training loss: 6853.30, base loss: 15961.78
[INFO 2017-06-30 02:26:08,863 main.py:52] epoch 1675, training loss: 6967.67, average training loss: 6852.24, base loss: 15962.63
[INFO 2017-06-30 02:26:12,063 main.py:52] epoch 1676, training loss: 6460.71, average training loss: 6851.45, base loss: 15962.94
[INFO 2017-06-30 02:26:15,233 main.py:52] epoch 1677, training loss: 5818.83, average training loss: 6850.10, base loss: 15961.36
[INFO 2017-06-30 02:26:18,385 main.py:52] epoch 1678, training loss: 7157.66, average training loss: 6849.40, base loss: 15962.75
[INFO 2017-06-30 02:26:21,524 main.py:52] epoch 1679, training loss: 6698.75, average training loss: 6848.30, base loss: 15962.83
[INFO 2017-06-30 02:26:24,676 main.py:52] epoch 1680, training loss: 6633.58, average training loss: 6847.19, base loss: 15962.38
[INFO 2017-06-30 02:26:27,883 main.py:52] epoch 1681, training loss: 6633.11, average training loss: 6846.30, base loss: 15962.05
[INFO 2017-06-30 02:26:31,048 main.py:52] epoch 1682, training loss: 6896.67, average training loss: 6846.32, base loss: 15962.70
[INFO 2017-06-30 02:26:34,248 main.py:52] epoch 1683, training loss: 6529.03, average training loss: 6846.04, base loss: 15962.20
[INFO 2017-06-30 02:26:37,433 main.py:52] epoch 1684, training loss: 6813.85, average training loss: 6845.36, base loss: 15962.28
[INFO 2017-06-30 02:26:40,612 main.py:52] epoch 1685, training loss: 6846.78, average training loss: 6845.03, base loss: 15962.72
[INFO 2017-06-30 02:26:43,773 main.py:52] epoch 1686, training loss: 6081.89, average training loss: 6844.05, base loss: 15960.89
[INFO 2017-06-30 02:26:46,930 main.py:52] epoch 1687, training loss: 6410.74, average training loss: 6843.20, base loss: 15959.85
[INFO 2017-06-30 02:26:50,067 main.py:52] epoch 1688, training loss: 6724.80, average training loss: 6842.43, base loss: 15961.09
[INFO 2017-06-30 02:26:53,210 main.py:52] epoch 1689, training loss: 6660.21, average training loss: 6841.87, base loss: 15961.76
[INFO 2017-06-30 02:26:56,329 main.py:52] epoch 1690, training loss: 6751.09, average training loss: 6841.08, base loss: 15961.90
[INFO 2017-06-30 02:26:59,484 main.py:52] epoch 1691, training loss: 6304.83, average training loss: 6839.86, base loss: 15960.80
[INFO 2017-06-30 02:27:02,663 main.py:52] epoch 1692, training loss: 7049.69, average training loss: 6839.70, base loss: 15961.56
[INFO 2017-06-30 02:27:05,883 main.py:52] epoch 1693, training loss: 6239.08, average training loss: 6839.19, base loss: 15960.28
[INFO 2017-06-30 02:27:09,058 main.py:52] epoch 1694, training loss: 6620.71, average training loss: 6838.64, base loss: 15961.06
[INFO 2017-06-30 02:27:12,219 main.py:52] epoch 1695, training loss: 6428.03, average training loss: 6837.52, base loss: 15961.43
[INFO 2017-06-30 02:27:15,401 main.py:52] epoch 1696, training loss: 6095.76, average training loss: 6835.43, base loss: 15960.76
[INFO 2017-06-30 02:27:18,585 main.py:52] epoch 1697, training loss: 6249.90, average training loss: 6834.60, base loss: 15960.22
[INFO 2017-06-30 02:27:21,780 main.py:52] epoch 1698, training loss: 6571.20, average training loss: 6833.84, base loss: 15960.50
[INFO 2017-06-30 02:27:24,989 main.py:52] epoch 1699, training loss: 6021.37, average training loss: 6832.02, base loss: 15959.29
[INFO 2017-06-30 02:27:24,989 main.py:54] epoch 1699, testing
[INFO 2017-06-30 02:27:38,437 main.py:97] average testing loss: 6523.51, base loss: 16134.94
[INFO 2017-06-30 02:27:38,438 main.py:98] improve_loss: 9611.43, improve_percent: 0.60
[INFO 2017-06-30 02:27:38,439 main.py:62] model save to ./model/final.pth
[INFO 2017-06-30 02:27:38,474 main.py:66] current best improved percent: 0.60
[INFO 2017-06-30 02:27:41,690 main.py:52] epoch 1700, training loss: 6814.71, average training loss: 6831.88, base loss: 15959.49
[INFO 2017-06-30 02:27:44,884 main.py:52] epoch 1701, training loss: 6307.15, average training loss: 6830.85, base loss: 15958.78
[INFO 2017-06-30 02:27:48,069 main.py:52] epoch 1702, training loss: 6585.84, average training loss: 6830.67, base loss: 15959.34
[INFO 2017-06-30 02:27:51,280 main.py:52] epoch 1703, training loss: 6750.63, average training loss: 6830.12, base loss: 15959.38
[INFO 2017-06-30 02:27:54,434 main.py:52] epoch 1704, training loss: 6455.25, average training loss: 6830.08, base loss: 15957.87
[INFO 2017-06-30 02:27:57,588 main.py:52] epoch 1705, training loss: 5877.06, average training loss: 6829.10, base loss: 15955.18
[INFO 2017-06-30 02:28:00,702 main.py:52] epoch 1706, training loss: 6456.69, average training loss: 6827.59, base loss: 15954.39
[INFO 2017-06-30 02:28:03,860 main.py:52] epoch 1707, training loss: 6114.39, average training loss: 6826.60, base loss: 15953.07
[INFO 2017-06-30 02:28:07,033 main.py:52] epoch 1708, training loss: 7219.96, average training loss: 6826.36, base loss: 15954.59
[INFO 2017-06-30 02:28:10,182 main.py:52] epoch 1709, training loss: 7250.28, average training loss: 6825.20, base loss: 15955.65
[INFO 2017-06-30 02:28:13,351 main.py:52] epoch 1710, training loss: 6298.52, average training loss: 6824.31, base loss: 15955.46
[INFO 2017-06-30 02:28:16,501 main.py:52] epoch 1711, training loss: 6713.23, average training loss: 6823.97, base loss: 15955.94
[INFO 2017-06-30 02:28:19,636 main.py:52] epoch 1712, training loss: 6065.97, average training loss: 6822.50, base loss: 15954.62
[INFO 2017-06-30 02:28:22,818 main.py:52] epoch 1713, training loss: 6749.41, average training loss: 6822.63, base loss: 15956.54
[INFO 2017-06-30 02:28:26,014 main.py:52] epoch 1714, training loss: 6996.62, average training loss: 6822.73, base loss: 15957.56
[INFO 2017-06-30 02:28:29,160 main.py:52] epoch 1715, training loss: 6647.57, average training loss: 6821.69, base loss: 15958.84
[INFO 2017-06-30 02:28:32,327 main.py:52] epoch 1716, training loss: 6786.03, average training loss: 6821.74, base loss: 15960.31
[INFO 2017-06-30 02:28:35,467 main.py:52] epoch 1717, training loss: 6737.54, average training loss: 6820.94, base loss: 15961.70
[INFO 2017-06-30 02:28:38,649 main.py:52] epoch 1718, training loss: 6525.08, average training loss: 6821.03, base loss: 15960.65
[INFO 2017-06-30 02:28:41,814 main.py:52] epoch 1719, training loss: 6332.78, average training loss: 6820.43, base loss: 15960.08
[INFO 2017-06-30 02:28:44,988 main.py:52] epoch 1720, training loss: 5862.82, average training loss: 6818.81, base loss: 15959.35
[INFO 2017-06-30 02:28:48,163 main.py:52] epoch 1721, training loss: 6094.68, average training loss: 6818.28, base loss: 15958.81
[INFO 2017-06-30 02:28:51,307 main.py:52] epoch 1722, training loss: 6223.07, average training loss: 6817.49, base loss: 15957.87
[INFO 2017-06-30 02:28:54,453 main.py:52] epoch 1723, training loss: 6341.12, average training loss: 6816.20, base loss: 15956.34
[INFO 2017-06-30 02:28:57,596 main.py:52] epoch 1724, training loss: 7049.43, average training loss: 6815.56, base loss: 15956.46
[INFO 2017-06-30 02:29:00,764 main.py:52] epoch 1725, training loss: 6469.79, average training loss: 6814.87, base loss: 15954.53
[INFO 2017-06-30 02:29:03,942 main.py:52] epoch 1726, training loss: 6505.16, average training loss: 6813.85, base loss: 15953.79
[INFO 2017-06-30 02:29:07,168 main.py:52] epoch 1727, training loss: 6785.41, average training loss: 6812.94, base loss: 15954.13
[INFO 2017-06-30 02:29:10,326 main.py:52] epoch 1728, training loss: 6697.78, average training loss: 6812.97, base loss: 15954.37
[INFO 2017-06-30 02:29:13,475 main.py:52] epoch 1729, training loss: 6244.61, average training loss: 6812.96, base loss: 15954.29
[INFO 2017-06-30 02:29:16,597 main.py:52] epoch 1730, training loss: 6781.77, average training loss: 6812.19, base loss: 15954.58
[INFO 2017-06-30 02:29:19,764 main.py:52] epoch 1731, training loss: 6322.46, average training loss: 6810.56, base loss: 15953.56
[INFO 2017-06-30 02:29:22,924 main.py:52] epoch 1732, training loss: 6152.21, average training loss: 6809.79, base loss: 15953.07
[INFO 2017-06-30 02:29:26,116 main.py:52] epoch 1733, training loss: 6498.23, average training loss: 6808.93, base loss: 15953.39
[INFO 2017-06-30 02:29:29,246 main.py:52] epoch 1734, training loss: 6881.81, average training loss: 6807.49, base loss: 15954.60
[INFO 2017-06-30 02:29:32,414 main.py:52] epoch 1735, training loss: 6636.09, average training loss: 6806.60, base loss: 15956.06
[INFO 2017-06-30 02:29:35,609 main.py:52] epoch 1736, training loss: 7085.67, average training loss: 6806.87, base loss: 15957.52
[INFO 2017-06-30 02:29:38,758 main.py:52] epoch 1737, training loss: 6671.98, average training loss: 6806.15, base loss: 15957.04
[INFO 2017-06-30 02:29:41,919 main.py:52] epoch 1738, training loss: 6032.46, average training loss: 6805.13, base loss: 15954.23
[INFO 2017-06-30 02:29:45,119 main.py:52] epoch 1739, training loss: 6104.54, average training loss: 6804.09, base loss: 15952.20
[INFO 2017-06-30 02:29:48,266 main.py:52] epoch 1740, training loss: 6282.50, average training loss: 6802.89, base loss: 15952.03
[INFO 2017-06-30 02:29:51,418 main.py:52] epoch 1741, training loss: 6147.15, average training loss: 6801.99, base loss: 15950.22
[INFO 2017-06-30 02:29:54,599 main.py:52] epoch 1742, training loss: 6252.47, average training loss: 6801.36, base loss: 15949.67
[INFO 2017-06-30 02:29:57,754 main.py:52] epoch 1743, training loss: 6328.80, average training loss: 6800.10, base loss: 15949.24
[INFO 2017-06-30 02:30:00,920 main.py:52] epoch 1744, training loss: 6623.87, average training loss: 6799.29, base loss: 15950.83
[INFO 2017-06-30 02:30:04,115 main.py:52] epoch 1745, training loss: 5830.13, average training loss: 6798.37, base loss: 15949.39
[INFO 2017-06-30 02:30:07,315 main.py:52] epoch 1746, training loss: 6703.66, average training loss: 6798.11, base loss: 15950.94
[INFO 2017-06-30 02:30:10,449 main.py:52] epoch 1747, training loss: 6370.44, average training loss: 6796.82, base loss: 15950.50
[INFO 2017-06-30 02:30:13,637 main.py:52] epoch 1748, training loss: 5963.78, average training loss: 6794.64, base loss: 15948.68
[INFO 2017-06-30 02:30:16,840 main.py:52] epoch 1749, training loss: 6576.46, average training loss: 6793.20, base loss: 15949.20
[INFO 2017-06-30 02:30:19,993 main.py:52] epoch 1750, training loss: 6777.46, average training loss: 6793.02, base loss: 15949.59
[INFO 2017-06-30 02:30:23,197 main.py:52] epoch 1751, training loss: 6619.02, average training loss: 6792.02, base loss: 15948.52
[INFO 2017-06-30 02:30:26,390 main.py:52] epoch 1752, training loss: 6693.25, average training loss: 6791.46, base loss: 15949.09
[INFO 2017-06-30 02:30:29,563 main.py:52] epoch 1753, training loss: 6696.49, average training loss: 6790.75, base loss: 15949.95
[INFO 2017-06-30 02:30:32,694 main.py:52] epoch 1754, training loss: 6407.33, average training loss: 6789.96, base loss: 15949.39
[INFO 2017-06-30 02:30:35,886 main.py:52] epoch 1755, training loss: 6020.99, average training loss: 6788.56, base loss: 15948.64
[INFO 2017-06-30 02:30:39,103 main.py:52] epoch 1756, training loss: 6321.61, average training loss: 6787.67, base loss: 15948.00
[INFO 2017-06-30 02:30:42,311 main.py:52] epoch 1757, training loss: 6261.00, average training loss: 6786.63, base loss: 15946.71
[INFO 2017-06-30 02:30:45,460 main.py:52] epoch 1758, training loss: 6694.73, average training loss: 6785.15, base loss: 15948.88
[INFO 2017-06-30 02:30:48,636 main.py:52] epoch 1759, training loss: 6081.51, average training loss: 6784.20, base loss: 15948.30
[INFO 2017-06-30 02:30:51,777 main.py:52] epoch 1760, training loss: 6289.50, average training loss: 6783.69, base loss: 15947.31
[INFO 2017-06-30 02:30:54,945 main.py:52] epoch 1761, training loss: 6378.04, average training loss: 6783.07, base loss: 15946.86
[INFO 2017-06-30 02:30:58,105 main.py:52] epoch 1762, training loss: 6247.77, average training loss: 6781.73, base loss: 15946.31
[INFO 2017-06-30 02:31:01,241 main.py:52] epoch 1763, training loss: 7138.72, average training loss: 6782.32, base loss: 15948.18
[INFO 2017-06-30 02:31:04,433 main.py:52] epoch 1764, training loss: 7000.25, average training loss: 6782.19, base loss: 15950.24
[INFO 2017-06-30 02:31:07,610 main.py:52] epoch 1765, training loss: 6471.36, average training loss: 6781.39, base loss: 15951.06
[INFO 2017-06-30 02:31:10,808 main.py:52] epoch 1766, training loss: 6426.62, average training loss: 6780.64, base loss: 15951.74
[INFO 2017-06-30 02:31:13,948 main.py:52] epoch 1767, training loss: 6549.28, average training loss: 6780.13, base loss: 15950.90
[INFO 2017-06-30 02:31:17,103 main.py:52] epoch 1768, training loss: 6431.77, average training loss: 6779.60, base loss: 15950.66
[INFO 2017-06-30 02:31:20,299 main.py:52] epoch 1769, training loss: 5815.44, average training loss: 6778.33, base loss: 15948.51
[INFO 2017-06-30 02:31:23,494 main.py:52] epoch 1770, training loss: 6728.19, average training loss: 6778.01, base loss: 15950.34
[INFO 2017-06-30 02:31:26,662 main.py:52] epoch 1771, training loss: 6626.73, average training loss: 6777.10, base loss: 15951.91
[INFO 2017-06-30 02:31:29,827 main.py:52] epoch 1772, training loss: 6676.41, average training loss: 6776.27, base loss: 15951.94
[INFO 2017-06-30 02:31:33,005 main.py:52] epoch 1773, training loss: 6050.77, average training loss: 6775.17, base loss: 15951.05
[INFO 2017-06-30 02:31:36,146 main.py:52] epoch 1774, training loss: 6458.81, average training loss: 6774.28, base loss: 15951.14
[INFO 2017-06-30 02:31:39,326 main.py:52] epoch 1775, training loss: 6262.51, average training loss: 6773.49, base loss: 15951.16
[INFO 2017-06-30 02:31:42,458 main.py:52] epoch 1776, training loss: 6605.84, average training loss: 6772.91, base loss: 15951.24
[INFO 2017-06-30 02:31:45,621 main.py:52] epoch 1777, training loss: 5788.81, average training loss: 6771.99, base loss: 15949.96
[INFO 2017-06-30 02:31:48,816 main.py:52] epoch 1778, training loss: 6534.51, average training loss: 6771.60, base loss: 15951.46
[INFO 2017-06-30 02:31:51,955 main.py:52] epoch 1779, training loss: 6909.14, average training loss: 6771.26, base loss: 15952.29
[INFO 2017-06-30 02:31:55,105 main.py:52] epoch 1780, training loss: 6958.75, average training loss: 6771.11, base loss: 15953.66
[INFO 2017-06-30 02:31:58,261 main.py:52] epoch 1781, training loss: 6410.52, average training loss: 6769.82, base loss: 15954.03
[INFO 2017-06-30 02:32:01,401 main.py:52] epoch 1782, training loss: 6143.30, average training loss: 6769.25, base loss: 15952.08
[INFO 2017-06-30 02:32:04,563 main.py:52] epoch 1783, training loss: 6694.84, average training loss: 6768.68, base loss: 15952.93
[INFO 2017-06-30 02:32:07,709 main.py:52] epoch 1784, training loss: 6688.84, average training loss: 6767.94, base loss: 15954.98
[INFO 2017-06-30 02:32:10,872 main.py:52] epoch 1785, training loss: 6658.55, average training loss: 6766.76, base loss: 15956.45
[INFO 2017-06-30 02:32:14,039 main.py:52] epoch 1786, training loss: 6692.41, average training loss: 6766.48, base loss: 15957.21
[INFO 2017-06-30 02:32:17,208 main.py:52] epoch 1787, training loss: 6714.83, average training loss: 6766.39, base loss: 15955.62
[INFO 2017-06-30 02:32:20,394 main.py:52] epoch 1788, training loss: 6460.09, average training loss: 6765.55, base loss: 15954.66
[INFO 2017-06-30 02:32:23,563 main.py:52] epoch 1789, training loss: 5958.06, average training loss: 6764.58, base loss: 15953.91
[INFO 2017-06-30 02:32:26,789 main.py:52] epoch 1790, training loss: 6420.71, average training loss: 6763.82, base loss: 15954.08
[INFO 2017-06-30 02:32:29,951 main.py:52] epoch 1791, training loss: 6485.08, average training loss: 6763.57, base loss: 15954.33
[INFO 2017-06-30 02:32:33,096 main.py:52] epoch 1792, training loss: 6466.58, average training loss: 6762.44, base loss: 15955.42
[INFO 2017-06-30 02:32:36,241 main.py:52] epoch 1793, training loss: 6552.46, average training loss: 6760.91, base loss: 15955.20
[INFO 2017-06-30 02:32:39,383 main.py:52] epoch 1794, training loss: 6424.72, average training loss: 6760.22, base loss: 15955.62
[INFO 2017-06-30 02:32:42,564 main.py:52] epoch 1795, training loss: 6678.17, average training loss: 6759.24, base loss: 15957.91
[INFO 2017-06-30 02:32:45,722 main.py:52] epoch 1796, training loss: 5879.60, average training loss: 6758.03, base loss: 15957.11
[INFO 2017-06-30 02:32:48,911 main.py:52] epoch 1797, training loss: 6289.20, average training loss: 6757.24, base loss: 15956.31
[INFO 2017-06-30 02:32:52,095 main.py:52] epoch 1798, training loss: 6952.13, average training loss: 6757.14, base loss: 15958.13
[INFO 2017-06-30 02:32:55,267 main.py:52] epoch 1799, training loss: 6211.56, average training loss: 6755.61, base loss: 15957.35
[INFO 2017-06-30 02:32:55,267 main.py:54] epoch 1799, testing
[INFO 2017-06-30 02:33:08,567 main.py:97] average testing loss: 6492.30, base loss: 15892.79
[INFO 2017-06-30 02:33:08,568 main.py:98] improve_loss: 9400.49, improve_percent: 0.59
[INFO 2017-06-30 02:33:08,570 main.py:66] current best improved percent: 0.60
[INFO 2017-06-30 02:33:11,764 main.py:52] epoch 1800, training loss: 6953.50, average training loss: 6755.72, base loss: 15958.89
[INFO 2017-06-30 02:33:14,913 main.py:52] epoch 1801, training loss: 6417.01, average training loss: 6755.60, base loss: 15959.23
[INFO 2017-06-30 02:33:18,132 main.py:52] epoch 1802, training loss: 6559.76, average training loss: 6755.14, base loss: 15959.60
[INFO 2017-06-30 02:33:21,326 main.py:52] epoch 1803, training loss: 6244.22, average training loss: 6754.35, base loss: 15959.31
[INFO 2017-06-30 02:33:24,495 main.py:52] epoch 1804, training loss: 6427.47, average training loss: 6753.95, base loss: 15958.43
[INFO 2017-06-30 02:33:27,711 main.py:52] epoch 1805, training loss: 6110.29, average training loss: 6753.64, base loss: 15957.92
[INFO 2017-06-30 02:33:30,868 main.py:52] epoch 1806, training loss: 6813.78, average training loss: 6753.86, base loss: 15959.94
[INFO 2017-06-30 02:33:34,007 main.py:52] epoch 1807, training loss: 6764.04, average training loss: 6753.98, base loss: 15961.82
[INFO 2017-06-30 02:33:37,193 main.py:52] epoch 1808, training loss: 6938.38, average training loss: 6753.74, base loss: 15963.82
[INFO 2017-06-30 02:33:40,364 main.py:52] epoch 1809, training loss: 6944.24, average training loss: 6753.73, base loss: 15963.87
[INFO 2017-06-30 02:33:43,477 main.py:52] epoch 1810, training loss: 6157.89, average training loss: 6752.85, base loss: 15963.12
[INFO 2017-06-30 02:33:46,658 main.py:52] epoch 1811, training loss: 6509.93, average training loss: 6752.38, base loss: 15963.20
[INFO 2017-06-30 02:33:49,834 main.py:52] epoch 1812, training loss: 6409.57, average training loss: 6751.97, base loss: 15962.69
[INFO 2017-06-30 02:33:52,997 main.py:52] epoch 1813, training loss: 6472.00, average training loss: 6751.29, base loss: 15962.34
[INFO 2017-06-30 02:33:56,201 main.py:52] epoch 1814, training loss: 6387.51, average training loss: 6751.06, base loss: 15962.37
[INFO 2017-06-30 02:33:59,339 main.py:52] epoch 1815, training loss: 6229.55, average training loss: 6750.67, base loss: 15962.28
[INFO 2017-06-30 02:34:02,483 main.py:52] epoch 1816, training loss: 6122.74, average training loss: 6749.85, base loss: 15961.04
[INFO 2017-06-30 02:34:05,655 main.py:52] epoch 1817, training loss: 6984.46, average training loss: 6750.01, base loss: 15962.67
[INFO 2017-06-30 02:34:08,864 main.py:52] epoch 1818, training loss: 6443.10, average training loss: 6749.56, base loss: 15962.54
[INFO 2017-06-30 02:34:12,079 main.py:52] epoch 1819, training loss: 6482.92, average training loss: 6748.90, base loss: 15963.10
[INFO 2017-06-30 02:34:15,287 main.py:52] epoch 1820, training loss: 6291.79, average training loss: 6748.06, base loss: 15962.32
[INFO 2017-06-30 02:34:18,418 main.py:52] epoch 1821, training loss: 7122.22, average training loss: 6748.10, base loss: 15964.75
[INFO 2017-06-30 02:34:21,588 main.py:52] epoch 1822, training loss: 6509.30, average training loss: 6747.59, base loss: 15965.59
[INFO 2017-06-30 02:34:24,759 main.py:52] epoch 1823, training loss: 6384.71, average training loss: 6746.28, base loss: 15965.44
[INFO 2017-06-30 02:34:27,926 main.py:52] epoch 1824, training loss: 6259.80, average training loss: 6744.98, base loss: 15965.05
[INFO 2017-06-30 02:34:31,067 main.py:52] epoch 1825, training loss: 5836.69, average training loss: 6744.01, base loss: 15963.65
[INFO 2017-06-30 02:34:34,222 main.py:52] epoch 1826, training loss: 6788.10, average training loss: 6744.29, base loss: 15965.18
[INFO 2017-06-30 02:34:37,387 main.py:52] epoch 1827, training loss: 7272.58, average training loss: 6744.17, base loss: 15967.65
[INFO 2017-06-30 02:34:40,565 main.py:52] epoch 1828, training loss: 6386.55, average training loss: 6743.15, base loss: 15968.37
[INFO 2017-06-30 02:34:43,724 main.py:52] epoch 1829, training loss: 6252.11, average training loss: 6742.77, base loss: 15965.71
[INFO 2017-06-30 02:34:46,919 main.py:52] epoch 1830, training loss: 7014.84, average training loss: 6742.85, base loss: 15967.49
[INFO 2017-06-30 02:34:50,157 main.py:52] epoch 1831, training loss: 6265.30, average training loss: 6741.84, base loss: 15966.67
[INFO 2017-06-30 02:34:53,358 main.py:52] epoch 1832, training loss: 6125.74, average training loss: 6740.86, base loss: 15966.64
[INFO 2017-06-30 02:34:56,553 main.py:52] epoch 1833, training loss: 6398.88, average training loss: 6740.35, base loss: 15966.51
[INFO 2017-06-30 02:34:59,760 main.py:52] epoch 1834, training loss: 6540.24, average training loss: 6740.59, base loss: 15967.39
[INFO 2017-06-30 02:35:02,936 main.py:52] epoch 1835, training loss: 6589.36, average training loss: 6740.49, base loss: 15968.55
[INFO 2017-06-30 02:35:06,076 main.py:52] epoch 1836, training loss: 6410.10, average training loss: 6739.67, base loss: 15968.92
[INFO 2017-06-30 02:35:09,234 main.py:52] epoch 1837, training loss: 6533.75, average training loss: 6739.45, base loss: 15969.45
[INFO 2017-06-30 02:35:12,427 main.py:52] epoch 1838, training loss: 6467.07, average training loss: 6738.35, base loss: 15969.41
[INFO 2017-06-30 02:35:15,596 main.py:52] epoch 1839, training loss: 6175.02, average training loss: 6737.62, base loss: 15969.81
[INFO 2017-06-30 02:35:18,778 main.py:52] epoch 1840, training loss: 6272.68, average training loss: 6736.31, base loss: 15970.32
[INFO 2017-06-30 02:35:21,921 main.py:52] epoch 1841, training loss: 6550.09, average training loss: 6735.98, base loss: 15971.12
[INFO 2017-06-30 02:35:25,098 main.py:52] epoch 1842, training loss: 6228.60, average training loss: 6735.26, base loss: 15970.77
[INFO 2017-06-30 02:35:28,220 main.py:52] epoch 1843, training loss: 6648.74, average training loss: 6734.73, base loss: 15972.22
[INFO 2017-06-30 02:35:31,418 main.py:52] epoch 1844, training loss: 6132.93, average training loss: 6734.30, base loss: 15970.30
[INFO 2017-06-30 02:35:34,618 main.py:52] epoch 1845, training loss: 6425.72, average training loss: 6733.25, base loss: 15971.27
[INFO 2017-06-30 02:35:37,829 main.py:52] epoch 1846, training loss: 6678.59, average training loss: 6732.99, base loss: 15972.02
[INFO 2017-06-30 02:35:41,036 main.py:52] epoch 1847, training loss: 6974.05, average training loss: 6732.27, base loss: 15973.18
[INFO 2017-06-30 02:35:44,153 main.py:52] epoch 1848, training loss: 6037.11, average training loss: 6731.33, base loss: 15972.34
[INFO 2017-06-30 02:35:47,384 main.py:52] epoch 1849, training loss: 6732.43, average training loss: 6730.75, base loss: 15972.21
[INFO 2017-06-30 02:35:50,620 main.py:52] epoch 1850, training loss: 6051.37, average training loss: 6729.25, base loss: 15970.94
[INFO 2017-06-30 02:35:53,767 main.py:52] epoch 1851, training loss: 6463.22, average training loss: 6728.73, base loss: 15970.12
[INFO 2017-06-30 02:35:56,962 main.py:52] epoch 1852, training loss: 6303.51, average training loss: 6727.69, base loss: 15970.70
[INFO 2017-06-30 02:36:00,089 main.py:52] epoch 1853, training loss: 5891.86, average training loss: 6726.24, base loss: 15969.91
[INFO 2017-06-30 02:36:03,289 main.py:52] epoch 1854, training loss: 6507.10, average training loss: 6726.49, base loss: 15970.73
[INFO 2017-06-30 02:36:06,480 main.py:52] epoch 1855, training loss: 7054.46, average training loss: 6726.72, base loss: 15972.93
[INFO 2017-06-30 02:36:09,651 main.py:52] epoch 1856, training loss: 6624.84, average training loss: 6725.95, base loss: 15974.16
[INFO 2017-06-30 02:36:12,851 main.py:52] epoch 1857, training loss: 5778.40, average training loss: 6724.67, base loss: 15972.07
[INFO 2017-06-30 02:36:16,002 main.py:52] epoch 1858, training loss: 6368.37, average training loss: 6723.77, base loss: 15972.08
[INFO 2017-06-30 02:36:19,191 main.py:52] epoch 1859, training loss: 6341.16, average training loss: 6722.76, base loss: 15971.81
[INFO 2017-06-30 02:36:22,324 main.py:52] epoch 1860, training loss: 6419.17, average training loss: 6722.53, base loss: 15970.67
[INFO 2017-06-30 02:36:25,451 main.py:52] epoch 1861, training loss: 7039.88, average training loss: 6722.41, base loss: 15971.42
[INFO 2017-06-30 02:36:28,619 main.py:52] epoch 1862, training loss: 6683.71, average training loss: 6722.06, base loss: 15972.95
[INFO 2017-06-30 02:36:31,789 main.py:52] epoch 1863, training loss: 6104.29, average training loss: 6721.16, base loss: 15971.56
[INFO 2017-06-30 02:36:34,957 main.py:52] epoch 1864, training loss: 6175.27, average training loss: 6720.01, base loss: 15971.17
[INFO 2017-06-30 02:36:38,139 main.py:52] epoch 1865, training loss: 7028.21, average training loss: 6719.63, base loss: 15974.25
[INFO 2017-06-30 02:36:41,333 main.py:52] epoch 1866, training loss: 6142.89, average training loss: 6718.87, base loss: 15974.69
[INFO 2017-06-30 02:36:44,551 main.py:52] epoch 1867, training loss: 6930.81, average training loss: 6718.98, base loss: 15976.33
[INFO 2017-06-30 02:36:47,748 main.py:52] epoch 1868, training loss: 7142.76, average training loss: 6719.25, base loss: 15978.04
[INFO 2017-06-30 02:36:50,918 main.py:52] epoch 1869, training loss: 5979.93, average training loss: 6718.95, base loss: 15977.18
[INFO 2017-06-30 02:36:54,093 main.py:52] epoch 1870, training loss: 6335.64, average training loss: 6718.41, base loss: 15976.85
[INFO 2017-06-30 02:36:57,229 main.py:52] epoch 1871, training loss: 6395.16, average training loss: 6717.53, base loss: 15976.71
[INFO 2017-06-30 02:37:00,378 main.py:52] epoch 1872, training loss: 5938.01, average training loss: 6716.50, base loss: 15975.68
[INFO 2017-06-30 02:37:03,551 main.py:52] epoch 1873, training loss: 6262.36, average training loss: 6715.59, base loss: 15975.32
[INFO 2017-06-30 02:37:06,744 main.py:52] epoch 1874, training loss: 6361.28, average training loss: 6714.09, base loss: 15974.89
[INFO 2017-06-30 02:37:09,920 main.py:52] epoch 1875, training loss: 6043.22, average training loss: 6713.17, base loss: 15973.53
[INFO 2017-06-30 02:37:13,064 main.py:52] epoch 1876, training loss: 6430.13, average training loss: 6712.82, base loss: 15973.76
[INFO 2017-06-30 02:37:16,276 main.py:52] epoch 1877, training loss: 6846.64, average training loss: 6713.17, base loss: 15976.05
[INFO 2017-06-30 02:37:19,445 main.py:52] epoch 1878, training loss: 6140.75, average training loss: 6712.57, base loss: 15975.38
[INFO 2017-06-30 02:37:22,604 main.py:52] epoch 1879, training loss: 6146.12, average training loss: 6711.77, base loss: 15975.47
[INFO 2017-06-30 02:37:25,730 main.py:52] epoch 1880, training loss: 6468.91, average training loss: 6711.75, base loss: 15975.10
[INFO 2017-06-30 02:37:28,922 main.py:52] epoch 1881, training loss: 6551.45, average training loss: 6710.74, base loss: 15975.24
[INFO 2017-06-30 02:37:32,091 main.py:52] epoch 1882, training loss: 6386.06, average training loss: 6710.45, base loss: 15975.39
[INFO 2017-06-30 02:37:35,242 main.py:52] epoch 1883, training loss: 6634.26, average training loss: 6710.07, base loss: 15975.72
[INFO 2017-06-30 02:37:38,419 main.py:52] epoch 1884, training loss: 6575.16, average training loss: 6709.98, base loss: 15976.49
[INFO 2017-06-30 02:37:41,590 main.py:52] epoch 1885, training loss: 6445.61, average training loss: 6708.91, base loss: 15977.71
[INFO 2017-06-30 02:37:44,787 main.py:52] epoch 1886, training loss: 6693.94, average training loss: 6709.00, base loss: 15978.92
[INFO 2017-06-30 02:37:47,976 main.py:52] epoch 1887, training loss: 6534.90, average training loss: 6708.60, base loss: 15979.69
[INFO 2017-06-30 02:37:51,106 main.py:52] epoch 1888, training loss: 6736.06, average training loss: 6708.64, base loss: 15981.12
[INFO 2017-06-30 02:37:54,261 main.py:52] epoch 1889, training loss: 6517.22, average training loss: 6707.60, base loss: 15982.38
[INFO 2017-06-30 02:37:57,437 main.py:52] epoch 1890, training loss: 6724.87, average training loss: 6706.97, base loss: 15984.26
[INFO 2017-06-30 02:38:00,630 main.py:52] epoch 1891, training loss: 6500.14, average training loss: 6706.84, base loss: 15984.01
[INFO 2017-06-30 02:38:03,768 main.py:52] epoch 1892, training loss: 6026.40, average training loss: 6706.29, base loss: 15981.91
[INFO 2017-06-30 02:38:06,908 main.py:52] epoch 1893, training loss: 6154.91, average training loss: 6705.82, base loss: 15980.87
[INFO 2017-06-30 02:38:10,065 main.py:52] epoch 1894, training loss: 6642.94, average training loss: 6705.74, base loss: 15981.20
[INFO 2017-06-30 02:38:13,243 main.py:52] epoch 1895, training loss: 5829.50, average training loss: 6704.05, base loss: 15979.95
[INFO 2017-06-30 02:38:16,400 main.py:52] epoch 1896, training loss: 5894.08, average training loss: 6703.14, base loss: 15977.62
[INFO 2017-06-30 02:38:19,528 main.py:52] epoch 1897, training loss: 6427.91, average training loss: 6702.49, base loss: 15977.63
[INFO 2017-06-30 02:38:22,680 main.py:52] epoch 1898, training loss: 6449.94, average training loss: 6701.73, base loss: 15977.46
[INFO 2017-06-30 02:38:25,857 main.py:52] epoch 1899, training loss: 6417.38, average training loss: 6701.05, base loss: 15976.77
[INFO 2017-06-30 02:38:25,857 main.py:54] epoch 1899, testing
[INFO 2017-06-30 02:38:39,139 main.py:97] average testing loss: 6390.15, base loss: 16361.52
[INFO 2017-06-30 02:38:39,139 main.py:98] improve_loss: 9971.37, improve_percent: 0.61
[INFO 2017-06-30 02:38:39,140 main.py:62] model save to ./model/final.pth
[INFO 2017-06-30 02:38:39,175 main.py:66] current best improved percent: 0.61
[INFO 2017-06-30 02:38:42,364 main.py:52] epoch 1900, training loss: 6023.40, average training loss: 6699.65, base loss: 15976.70
[INFO 2017-06-30 02:38:45,555 main.py:52] epoch 1901, training loss: 6669.55, average training loss: 6699.34, base loss: 15978.72
[INFO 2017-06-30 02:38:48,756 main.py:52] epoch 1902, training loss: 6549.28, average training loss: 6698.78, base loss: 15978.87
[INFO 2017-06-30 02:38:51,910 main.py:52] epoch 1903, training loss: 6889.76, average training loss: 6698.90, base loss: 15980.59
[INFO 2017-06-30 02:38:55,096 main.py:52] epoch 1904, training loss: 6568.07, average training loss: 6698.46, base loss: 15980.41
[INFO 2017-06-30 02:38:58,288 main.py:52] epoch 1905, training loss: 6411.44, average training loss: 6698.47, base loss: 15980.63
[INFO 2017-06-30 02:39:01,516 main.py:52] epoch 1906, training loss: 6701.34, average training loss: 6697.88, base loss: 15983.25
[INFO 2017-06-30 02:39:04,709 main.py:52] epoch 1907, training loss: 6928.17, average training loss: 6697.63, base loss: 15983.67
[INFO 2017-06-30 02:39:07,845 main.py:52] epoch 1908, training loss: 6417.70, average training loss: 6697.25, base loss: 15985.23
[INFO 2017-06-30 02:39:11,041 main.py:52] epoch 1909, training loss: 6681.73, average training loss: 6696.96, base loss: 15987.15
[INFO 2017-06-30 02:39:14,213 main.py:52] epoch 1910, training loss: 6854.55, average training loss: 6697.10, base loss: 15987.68
[INFO 2017-06-30 02:39:17,382 main.py:52] epoch 1911, training loss: 6876.20, average training loss: 6697.23, base loss: 15988.11
[INFO 2017-06-30 02:39:20,555 main.py:52] epoch 1912, training loss: 6014.91, average training loss: 6696.84, base loss: 15986.15
[INFO 2017-06-30 02:39:23,715 main.py:52] epoch 1913, training loss: 6499.35, average training loss: 6696.11, base loss: 15986.22
[INFO 2017-06-30 02:39:26,881 main.py:52] epoch 1914, training loss: 6008.56, average training loss: 6695.42, base loss: 15985.36
[INFO 2017-06-30 02:39:30,135 main.py:52] epoch 1915, training loss: 6403.97, average training loss: 6694.48, base loss: 15985.37
[INFO 2017-06-30 02:39:33,304 main.py:52] epoch 1916, training loss: 6559.01, average training loss: 6694.23, base loss: 15985.55
[INFO 2017-06-30 02:39:36,511 main.py:52] epoch 1917, training loss: 6567.29, average training loss: 6693.56, base loss: 15986.56
[INFO 2017-06-30 02:39:39,658 main.py:52] epoch 1918, training loss: 6526.91, average training loss: 6693.17, base loss: 15986.27
[INFO 2017-06-30 02:39:42,854 main.py:52] epoch 1919, training loss: 6516.74, average training loss: 6692.69, base loss: 15984.78
[INFO 2017-06-30 02:39:46,009 main.py:52] epoch 1920, training loss: 6204.20, average training loss: 6691.74, base loss: 15985.19
[INFO 2017-06-30 02:39:49,146 main.py:52] epoch 1921, training loss: 5804.48, average training loss: 6690.75, base loss: 15984.38
[INFO 2017-06-30 02:39:52,294 main.py:52] epoch 1922, training loss: 6847.27, average training loss: 6690.84, base loss: 15985.47
[INFO 2017-06-30 02:39:55,444 main.py:52] epoch 1923, training loss: 6425.84, average training loss: 6690.47, base loss: 15985.80
[INFO 2017-06-30 02:39:58,592 main.py:52] epoch 1924, training loss: 6179.67, average training loss: 6690.10, base loss: 15985.59
[INFO 2017-06-30 02:40:01,741 main.py:52] epoch 1925, training loss: 6376.12, average training loss: 6689.01, base loss: 15984.08
[INFO 2017-06-30 02:40:04,954 main.py:52] epoch 1926, training loss: 6094.38, average training loss: 6688.28, base loss: 15984.00
[INFO 2017-06-30 02:40:08,183 main.py:52] epoch 1927, training loss: 6173.63, average training loss: 6686.75, base loss: 15983.80
[INFO 2017-06-30 02:40:11,375 main.py:52] epoch 1928, training loss: 6209.67, average training loss: 6686.22, base loss: 15983.17
[INFO 2017-06-30 02:40:14,530 main.py:52] epoch 1929, training loss: 6343.98, average training loss: 6685.59, base loss: 15982.71
[INFO 2017-06-30 02:40:17,684 main.py:52] epoch 1930, training loss: 6557.40, average training loss: 6685.50, base loss: 15983.27
[INFO 2017-06-30 02:40:20,859 main.py:52] epoch 1931, training loss: 6031.10, average training loss: 6685.54, base loss: 15981.60
[INFO 2017-06-30 02:40:24,039 main.py:52] epoch 1932, training loss: 6024.04, average training loss: 6684.65, base loss: 15981.01
[INFO 2017-06-30 02:40:27,242 main.py:52] epoch 1933, training loss: 6326.32, average training loss: 6683.08, base loss: 15980.85
[INFO 2017-06-30 02:40:30,461 main.py:52] epoch 1934, training loss: 6177.63, average training loss: 6682.37, base loss: 15981.31
[INFO 2017-06-30 02:40:33,666 main.py:52] epoch 1935, training loss: 6605.80, average training loss: 6682.40, base loss: 15981.74
[INFO 2017-06-30 02:40:36,869 main.py:52] epoch 1936, training loss: 7038.94, average training loss: 6681.85, base loss: 15983.84
[INFO 2017-06-30 02:40:40,072 main.py:52] epoch 1937, training loss: 5723.56, average training loss: 6680.79, base loss: 15983.49
[INFO 2017-06-30 02:40:43,262 main.py:52] epoch 1938, training loss: 6636.96, average training loss: 6679.76, base loss: 15983.60
[INFO 2017-06-30 02:40:46,435 main.py:52] epoch 1939, training loss: 5975.54, average training loss: 6678.79, base loss: 15983.62
[INFO 2017-06-30 02:40:49,662 main.py:52] epoch 1940, training loss: 6204.49, average training loss: 6677.29, base loss: 15983.09
[INFO 2017-06-30 02:40:52,842 main.py:52] epoch 1941, training loss: 6406.49, average training loss: 6676.67, base loss: 15983.26
[INFO 2017-06-30 02:40:56,003 main.py:52] epoch 1942, training loss: 6846.79, average training loss: 6676.08, base loss: 15985.21
[INFO 2017-06-30 02:40:59,145 main.py:52] epoch 1943, training loss: 6257.70, average training loss: 6675.38, base loss: 15984.98
[INFO 2017-06-30 02:41:02,328 main.py:52] epoch 1944, training loss: 5968.91, average training loss: 6674.27, base loss: 15984.07
[INFO 2017-06-30 02:41:05,478 main.py:52] epoch 1945, training loss: 6726.08, average training loss: 6674.54, base loss: 15984.65
[INFO 2017-06-30 02:41:08,634 main.py:52] epoch 1946, training loss: 6591.39, average training loss: 6674.22, base loss: 15984.59
[INFO 2017-06-30 02:41:11,837 main.py:52] epoch 1947, training loss: 6269.19, average training loss: 6673.23, base loss: 15983.60
[INFO 2017-06-30 02:41:14,986 main.py:52] epoch 1948, training loss: 6271.89, average training loss: 6671.84, base loss: 15983.85
[INFO 2017-06-30 02:41:18,160 main.py:52] epoch 1949, training loss: 6195.51, average training loss: 6671.26, base loss: 15983.02
[INFO 2017-06-30 02:41:21,345 main.py:52] epoch 1950, training loss: 6559.75, average training loss: 6670.88, base loss: 15984.81
[INFO 2017-06-30 02:41:24,559 main.py:52] epoch 1951, training loss: 5769.17, average training loss: 6669.97, base loss: 15983.04
[INFO 2017-06-30 02:41:27,716 main.py:52] epoch 1952, training loss: 6155.93, average training loss: 6669.17, base loss: 15982.71
[INFO 2017-06-30 02:41:30,864 main.py:52] epoch 1953, training loss: 6185.98, average training loss: 6668.68, base loss: 15982.24
[INFO 2017-06-30 02:41:34,037 main.py:52] epoch 1954, training loss: 6588.50, average training loss: 6668.12, base loss: 15982.15
[INFO 2017-06-30 02:41:37,194 main.py:52] epoch 1955, training loss: 6610.96, average training loss: 6667.17, base loss: 15982.30
[INFO 2017-06-30 02:41:40,330 main.py:52] epoch 1956, training loss: 6256.92, average training loss: 6666.82, base loss: 15982.65
[INFO 2017-06-30 02:41:43,497 main.py:52] epoch 1957, training loss: 6508.05, average training loss: 6665.68, base loss: 15983.00
[INFO 2017-06-30 02:41:46,667 main.py:52] epoch 1958, training loss: 7103.36, average training loss: 6665.85, base loss: 15985.46
[INFO 2017-06-30 02:41:49,792 main.py:52] epoch 1959, training loss: 6287.23, average training loss: 6665.12, base loss: 15985.95
[INFO 2017-06-30 02:41:52,977 main.py:52] epoch 1960, training loss: 6400.10, average training loss: 6663.99, base loss: 15985.66
[INFO 2017-06-30 02:41:56,145 main.py:52] epoch 1961, training loss: 6440.12, average training loss: 6663.99, base loss: 15983.50
[INFO 2017-06-30 02:41:59,307 main.py:52] epoch 1962, training loss: 6673.81, average training loss: 6663.68, base loss: 15983.66
[INFO 2017-06-30 02:42:02,481 main.py:52] epoch 1963, training loss: 6036.88, average training loss: 6662.49, base loss: 15982.07
[INFO 2017-06-30 02:42:05,659 main.py:52] epoch 1964, training loss: 6322.73, average training loss: 6662.00, base loss: 15981.35
[INFO 2017-06-30 02:42:08,830 main.py:52] epoch 1965, training loss: 6669.59, average training loss: 6661.02, base loss: 15981.23
[INFO 2017-06-30 02:42:11,986 main.py:52] epoch 1966, training loss: 5839.76, average training loss: 6659.75, base loss: 15980.11
[INFO 2017-06-30 02:42:15,170 main.py:52] epoch 1967, training loss: 6861.71, average training loss: 6659.34, base loss: 15980.41
[INFO 2017-06-30 02:42:18,351 main.py:52] epoch 1968, training loss: 6342.48, average training loss: 6659.11, base loss: 15978.35
[INFO 2017-06-30 02:42:21,492 main.py:52] epoch 1969, training loss: 6411.61, average training loss: 6659.16, base loss: 15976.67
[INFO 2017-06-30 02:42:24,667 main.py:52] epoch 1970, training loss: 6208.96, average training loss: 6657.66, base loss: 15975.97
[INFO 2017-06-30 02:42:27,832 main.py:52] epoch 1971, training loss: 6356.98, average training loss: 6657.26, base loss: 15975.56
[INFO 2017-06-30 02:42:31,010 main.py:52] epoch 1972, training loss: 6367.37, average training loss: 6656.57, base loss: 15976.08
[INFO 2017-06-30 02:42:34,158 main.py:52] epoch 1973, training loss: 6462.82, average training loss: 6656.34, base loss: 15977.32
[INFO 2017-06-30 02:42:37,295 main.py:52] epoch 1974, training loss: 6363.56, average training loss: 6655.66, base loss: 15977.82
[INFO 2017-06-30 02:42:40,453 main.py:52] epoch 1975, training loss: 6091.78, average training loss: 6653.87, base loss: 15977.07
[INFO 2017-06-30 02:42:43,621 main.py:52] epoch 1976, training loss: 6830.56, average training loss: 6653.94, base loss: 15977.28
[INFO 2017-06-30 02:42:46,788 main.py:52] epoch 1977, training loss: 6118.89, average training loss: 6653.25, base loss: 15977.31
[INFO 2017-06-30 02:42:49,973 main.py:52] epoch 1978, training loss: 6479.75, average training loss: 6652.82, base loss: 15978.49
[INFO 2017-06-30 02:42:53,136 main.py:52] epoch 1979, training loss: 6794.55, average training loss: 6653.01, base loss: 15979.21
[INFO 2017-06-30 02:42:56,338 main.py:52] epoch 1980, training loss: 6236.83, average training loss: 6652.32, base loss: 15978.74
[INFO 2017-06-30 02:42:59,484 main.py:52] epoch 1981, training loss: 6545.00, average training loss: 6652.06, base loss: 15978.96
[INFO 2017-06-30 02:43:02,656 main.py:52] epoch 1982, training loss: 6745.21, average training loss: 6650.86, base loss: 15979.20
[INFO 2017-06-30 02:43:05,847 main.py:52] epoch 1983, training loss: 6113.40, average training loss: 6649.80, base loss: 15979.30
[INFO 2017-06-30 02:43:09,031 main.py:52] epoch 1984, training loss: 6023.12, average training loss: 6648.40, base loss: 15979.22
[INFO 2017-06-30 02:43:12,249 main.py:52] epoch 1985, training loss: 6518.01, average training loss: 6647.85, base loss: 15980.24
[INFO 2017-06-30 02:43:15,426 main.py:52] epoch 1986, training loss: 6037.15, average training loss: 6646.94, base loss: 15978.79
[INFO 2017-06-30 02:43:18,626 main.py:52] epoch 1987, training loss: 6586.69, average training loss: 6647.04, base loss: 15978.87
[INFO 2017-06-30 02:43:21,797 main.py:52] epoch 1988, training loss: 6120.76, average training loss: 6645.89, base loss: 15977.98
[INFO 2017-06-30 02:43:24,943 main.py:52] epoch 1989, training loss: 6085.09, average training loss: 6644.99, base loss: 15977.03
[INFO 2017-06-30 02:43:28,083 main.py:52] epoch 1990, training loss: 5970.85, average training loss: 6643.46, base loss: 15976.48
[INFO 2017-06-30 02:43:31,229 main.py:52] epoch 1991, training loss: 6846.03, average training loss: 6644.14, base loss: 15976.64
[INFO 2017-06-30 02:43:34,420 main.py:52] epoch 1992, training loss: 6277.13, average training loss: 6643.89, base loss: 15976.82
[INFO 2017-06-30 02:43:37,579 main.py:52] epoch 1993, training loss: 7193.26, average training loss: 6643.90, base loss: 15979.06
[INFO 2017-06-30 02:43:40,737 main.py:52] epoch 1994, training loss: 5890.78, average training loss: 6642.04, base loss: 15978.41
[INFO 2017-06-30 02:43:43,902 main.py:52] epoch 1995, training loss: 6143.78, average training loss: 6641.61, base loss: 15978.08
[INFO 2017-06-30 02:43:47,108 main.py:52] epoch 1996, training loss: 5959.56, average training loss: 6640.39, base loss: 15976.06
[INFO 2017-06-30 02:43:50,303 main.py:52] epoch 1997, training loss: 6195.50, average training loss: 6640.21, base loss: 15975.45
[INFO 2017-06-30 02:43:53,472 main.py:52] epoch 1998, training loss: 6997.66, average training loss: 6640.10, base loss: 15976.99
[INFO 2017-06-30 02:43:56,646 main.py:52] epoch 1999, training loss: 6449.45, average training loss: 6639.85, base loss: 15977.28
[INFO 2017-06-30 02:43:56,646 main.py:54] epoch 1999, testing
[INFO 2017-06-30 02:44:09,945 main.py:97] average testing loss: 6567.07, base loss: 16707.03
[INFO 2017-06-30 02:44:09,945 main.py:98] improve_loss: 10139.96, improve_percent: 0.61
[INFO 2017-06-30 02:44:09,948 main.py:66] current best improved percent: 0.61
[INFO 2017-06-30 02:44:13,116 main.py:52] epoch 2000, training loss: 5856.59, average training loss: 6639.10, base loss: 15975.97
[INFO 2017-06-30 02:44:16,304 main.py:52] epoch 2001, training loss: 6384.37, average training loss: 6637.67, base loss: 15976.53
[INFO 2017-06-30 02:44:19,430 main.py:52] epoch 2002, training loss: 6181.69, average training loss: 6636.85, base loss: 15975.99
[INFO 2017-06-30 02:44:22,576 main.py:52] epoch 2003, training loss: 6297.83, average training loss: 6636.31, base loss: 15975.77
[INFO 2017-06-30 02:44:25,748 main.py:52] epoch 2004, training loss: 6027.93, average training loss: 6635.10, base loss: 15975.47
[INFO 2017-06-30 02:44:28,994 main.py:52] epoch 2005, training loss: 6962.16, average training loss: 6635.57, base loss: 15977.17
[INFO 2017-06-30 02:44:32,189 main.py:52] epoch 2006, training loss: 6226.20, average training loss: 6634.87, base loss: 15977.81
[INFO 2017-06-30 02:44:35,395 main.py:52] epoch 2007, training loss: 6445.91, average training loss: 6634.46, base loss: 15977.50
[INFO 2017-06-30 02:44:38,542 main.py:52] epoch 2008, training loss: 6202.44, average training loss: 6633.98, base loss: 15977.05
[INFO 2017-06-30 02:44:41,723 main.py:52] epoch 2009, training loss: 6502.55, average training loss: 6633.05, base loss: 15976.94
[INFO 2017-06-30 02:44:44,906 main.py:52] epoch 2010, training loss: 6479.42, average training loss: 6632.25, base loss: 15976.46
[INFO 2017-06-30 02:44:48,107 main.py:52] epoch 2011, training loss: 6243.73, average training loss: 6631.44, base loss: 15976.03
[INFO 2017-06-30 02:44:51,281 main.py:52] epoch 2012, training loss: 6251.11, average training loss: 6630.90, base loss: 15976.35
[INFO 2017-06-30 02:44:54,439 main.py:52] epoch 2013, training loss: 6214.02, average training loss: 6630.21, base loss: 15976.36
[INFO 2017-06-30 02:44:57,599 main.py:52] epoch 2014, training loss: 6390.89, average training loss: 6629.40, base loss: 15976.38
[INFO 2017-06-30 02:45:00,780 main.py:52] epoch 2015, training loss: 6522.22, average training loss: 6628.77, base loss: 15977.14
[INFO 2017-06-30 02:45:04,001 main.py:52] epoch 2016, training loss: 6296.69, average training loss: 6627.40, base loss: 15975.77
[INFO 2017-06-30 02:45:07,166 main.py:52] epoch 2017, training loss: 6238.64, average training loss: 6626.92, base loss: 15974.80
[INFO 2017-06-30 02:45:10,314 main.py:52] epoch 2018, training loss: 6183.46, average training loss: 6626.29, base loss: 15974.78
[INFO 2017-06-30 02:45:13,522 main.py:52] epoch 2019, training loss: 6674.11, average training loss: 6626.23, base loss: 15974.66
[INFO 2017-06-30 02:45:16,728 main.py:52] epoch 2020, training loss: 6344.81, average training loss: 6625.81, base loss: 15975.21
[INFO 2017-06-30 02:45:19,904 main.py:52] epoch 2021, training loss: 6189.04, average training loss: 6625.04, base loss: 15975.49
[INFO 2017-06-30 02:45:23,104 main.py:52] epoch 2022, training loss: 6747.70, average training loss: 6624.84, base loss: 15977.76
[INFO 2017-06-30 02:45:26,269 main.py:52] epoch 2023, training loss: 6992.93, average training loss: 6624.88, base loss: 15978.57
[INFO 2017-06-30 02:45:29,442 main.py:52] epoch 2024, training loss: 6047.54, average training loss: 6623.82, base loss: 15977.18
[INFO 2017-06-30 02:45:32,628 main.py:52] epoch 2025, training loss: 6584.39, average training loss: 6623.16, base loss: 15977.13
[INFO 2017-06-30 02:45:35,776 main.py:52] epoch 2026, training loss: 6372.42, average training loss: 6622.89, base loss: 15977.76
[INFO 2017-06-30 02:45:38,951 main.py:52] epoch 2027, training loss: 6557.00, average training loss: 6622.41, base loss: 15979.00
[INFO 2017-06-30 02:45:42,163 main.py:52] epoch 2028, training loss: 6322.78, average training loss: 6621.75, base loss: 15978.80
[INFO 2017-06-30 02:45:45,368 main.py:52] epoch 2029, training loss: 6318.02, average training loss: 6621.31, base loss: 15979.14
[INFO 2017-06-30 02:45:48,518 main.py:52] epoch 2030, training loss: 6674.07, average training loss: 6621.29, base loss: 15979.56
[INFO 2017-06-30 02:45:51,705 main.py:52] epoch 2031, training loss: 6904.61, average training loss: 6621.07, base loss: 15981.78
[INFO 2017-06-30 02:45:54,855 main.py:52] epoch 2032, training loss: 7091.61, average training loss: 6621.59, base loss: 15983.64
[INFO 2017-06-30 02:45:58,032 main.py:52] epoch 2033, training loss: 6153.17, average training loss: 6620.29, base loss: 15983.10
[INFO 2017-06-30 02:46:01,234 main.py:52] epoch 2034, training loss: 6327.56, average training loss: 6619.54, base loss: 15982.43
[INFO 2017-06-30 02:46:04,416 main.py:52] epoch 2035, training loss: 6728.51, average training loss: 6619.47, base loss: 15983.26
[INFO 2017-06-30 02:46:07,558 main.py:52] epoch 2036, training loss: 6066.31, average training loss: 6619.13, base loss: 15982.15
[INFO 2017-06-30 02:46:10,760 main.py:52] epoch 2037, training loss: 6957.25, average training loss: 6618.56, base loss: 15983.75
[INFO 2017-06-30 02:46:13,960 main.py:52] epoch 2038, training loss: 6116.68, average training loss: 6617.09, base loss: 15983.65
[INFO 2017-06-30 02:46:17,110 main.py:52] epoch 2039, training loss: 6416.28, average training loss: 6616.57, base loss: 15984.01
[INFO 2017-06-30 02:46:20,275 main.py:52] epoch 2040, training loss: 6387.58, average training loss: 6616.09, base loss: 15984.79
[INFO 2017-06-30 02:46:23,462 main.py:52] epoch 2041, training loss: 7081.83, average training loss: 6616.32, base loss: 15986.25
[INFO 2017-06-30 02:46:26,633 main.py:52] epoch 2042, training loss: 6362.99, average training loss: 6615.34, base loss: 15985.57
[INFO 2017-06-30 02:46:29,781 main.py:52] epoch 2043, training loss: 6121.02, average training loss: 6614.41, base loss: 15984.19
[INFO 2017-06-30 02:46:32,951 main.py:52] epoch 2044, training loss: 7219.70, average training loss: 6615.05, base loss: 15986.39
[INFO 2017-06-30 02:46:36,153 main.py:52] epoch 2045, training loss: 6737.86, average training loss: 6614.97, base loss: 15987.66
[INFO 2017-06-30 02:46:39,326 main.py:52] epoch 2046, training loss: 6519.14, average training loss: 6614.93, base loss: 15988.42
[INFO 2017-06-30 02:46:42,564 main.py:52] epoch 2047, training loss: 6564.29, average training loss: 6614.59, base loss: 15989.39
[INFO 2017-06-30 02:46:45,740 main.py:52] epoch 2048, training loss: 6139.29, average training loss: 6613.55, base loss: 15988.26
[INFO 2017-06-30 02:46:48,895 main.py:52] epoch 2049, training loss: 6075.91, average training loss: 6612.41, base loss: 15987.99
[INFO 2017-06-30 02:46:52,091 main.py:52] epoch 2050, training loss: 6275.84, average training loss: 6611.61, base loss: 15987.84
[INFO 2017-06-30 02:46:55,263 main.py:52] epoch 2051, training loss: 6407.61, average training loss: 6611.20, base loss: 15987.64
[INFO 2017-06-30 02:46:58,438 main.py:52] epoch 2052, training loss: 6353.46, average training loss: 6610.60, base loss: 15986.85
[INFO 2017-06-30 02:47:01,612 main.py:52] epoch 2053, training loss: 6799.51, average training loss: 6610.06, base loss: 15988.07
[INFO 2017-06-30 02:47:04,750 main.py:52] epoch 2054, training loss: 5786.33, average training loss: 6608.34, base loss: 15986.08
[INFO 2017-06-30 02:47:07,963 main.py:52] epoch 2055, training loss: 6117.50, average training loss: 6607.63, base loss: 15986.06
[INFO 2017-06-30 02:47:11,139 main.py:52] epoch 2056, training loss: 6401.02, average training loss: 6606.96, base loss: 15985.78
[INFO 2017-06-30 02:47:14,265 main.py:52] epoch 2057, training loss: 6256.67, average training loss: 6606.20, base loss: 15985.06
[INFO 2017-06-30 02:47:17,431 main.py:52] epoch 2058, training loss: 6227.69, average training loss: 6605.55, base loss: 15984.91
[INFO 2017-06-30 02:47:20,543 main.py:52] epoch 2059, training loss: 6233.22, average training loss: 6604.71, base loss: 15984.90
[INFO 2017-06-30 02:47:23,729 main.py:52] epoch 2060, training loss: 6338.26, average training loss: 6604.36, base loss: 15985.60
[INFO 2017-06-30 02:47:26,904 main.py:52] epoch 2061, training loss: 6451.70, average training loss: 6603.58, base loss: 15986.15
[INFO 2017-06-30 02:47:30,070 main.py:52] epoch 2062, training loss: 6553.97, average training loss: 6603.67, base loss: 15984.77
[INFO 2017-06-30 02:47:33,187 main.py:52] epoch 2063, training loss: 6695.69, average training loss: 6603.30, base loss: 15985.18
[INFO 2017-06-30 02:47:36,395 main.py:52] epoch 2064, training loss: 6355.90, average training loss: 6603.10, base loss: 15985.88
[INFO 2017-06-30 02:47:39,513 main.py:52] epoch 2065, training loss: 6196.06, average training loss: 6602.51, base loss: 15984.88
[INFO 2017-06-30 02:47:42,681 main.py:52] epoch 2066, training loss: 6188.20, average training loss: 6601.80, base loss: 15984.03
[INFO 2017-06-30 02:47:45,847 main.py:52] epoch 2067, training loss: 6225.68, average training loss: 6600.94, base loss: 15983.37
[INFO 2017-06-30 02:47:49,026 main.py:52] epoch 2068, training loss: 6338.78, average training loss: 6600.51, base loss: 15982.75
[INFO 2017-06-30 02:47:52,216 main.py:52] epoch 2069, training loss: 6645.17, average training loss: 6600.49, base loss: 15982.89
[INFO 2017-06-30 02:47:55,414 main.py:52] epoch 2070, training loss: 5973.34, average training loss: 6598.73, base loss: 15981.79
[INFO 2017-06-30 02:47:58,531 main.py:52] epoch 2071, training loss: 6718.97, average training loss: 6598.32, base loss: 15982.26
[INFO 2017-06-30 02:48:01,681 main.py:52] epoch 2072, training loss: 6478.20, average training loss: 6597.64, base loss: 15983.48
[INFO 2017-06-30 02:48:04,901 main.py:52] epoch 2073, training loss: 6502.45, average training loss: 6597.45, base loss: 15983.91
[INFO 2017-06-30 02:48:08,037 main.py:52] epoch 2074, training loss: 6629.74, average training loss: 6597.09, base loss: 15984.84
[INFO 2017-06-30 02:48:11,227 main.py:52] epoch 2075, training loss: 6223.91, average training loss: 6596.29, base loss: 15983.68
[INFO 2017-06-30 02:48:14,404 main.py:52] epoch 2076, training loss: 6992.22, average training loss: 6596.62, base loss: 15986.07
[INFO 2017-06-30 02:48:17,568 main.py:52] epoch 2077, training loss: 6541.56, average training loss: 6595.77, base loss: 15986.81
[INFO 2017-06-30 02:48:20,731 main.py:52] epoch 2078, training loss: 6491.77, average training loss: 6595.89, base loss: 15986.04
[INFO 2017-06-30 02:48:23,906 main.py:52] epoch 2079, training loss: 6026.09, average training loss: 6594.53, base loss: 15984.89
[INFO 2017-06-30 02:48:27,050 main.py:52] epoch 2080, training loss: 5712.28, average training loss: 6593.25, base loss: 15982.72
[INFO 2017-06-30 02:48:30,235 main.py:52] epoch 2081, training loss: 7701.40, average training loss: 6594.17, base loss: 15985.11
[INFO 2017-06-30 02:48:33,386 main.py:52] epoch 2082, training loss: 6374.01, average training loss: 6593.31, base loss: 15984.92
[INFO 2017-06-30 02:48:36,601 main.py:52] epoch 2083, training loss: 6739.83, average training loss: 6593.53, base loss: 15985.21
[INFO 2017-06-30 02:48:39,728 main.py:52] epoch 2084, training loss: 6341.86, average training loss: 6593.13, base loss: 15985.31
[INFO 2017-06-30 02:48:42,898 main.py:52] epoch 2085, training loss: 6792.74, average training loss: 6592.94, base loss: 15985.12
[INFO 2017-06-30 02:48:46,053 main.py:52] epoch 2086, training loss: 6200.61, average training loss: 6592.79, base loss: 15983.16
[INFO 2017-06-30 02:48:49,190 main.py:52] epoch 2087, training loss: 6455.77, average training loss: 6592.87, base loss: 15982.55
[INFO 2017-06-30 02:48:52,410 main.py:52] epoch 2088, training loss: 6007.48, average training loss: 6592.51, base loss: 15981.12
[INFO 2017-06-30 02:48:55,576 main.py:52] epoch 2089, training loss: 6493.33, average training loss: 6591.73, base loss: 15981.02
[INFO 2017-06-30 02:48:58,690 main.py:52] epoch 2090, training loss: 6147.69, average training loss: 6591.03, base loss: 15978.96
[INFO 2017-06-30 02:49:01,862 main.py:52] epoch 2091, training loss: 6763.54, average training loss: 6591.15, base loss: 15978.42
[INFO 2017-06-30 02:49:05,011 main.py:52] epoch 2092, training loss: 6290.85, average training loss: 6590.53, base loss: 15978.22
[INFO 2017-06-30 02:49:08,134 main.py:52] epoch 2093, training loss: 6750.16, average training loss: 6590.11, base loss: 15978.69
[INFO 2017-06-30 02:49:11,272 main.py:52] epoch 2094, training loss: 6480.73, average training loss: 6589.52, base loss: 15978.26
[INFO 2017-06-30 02:49:14,452 main.py:52] epoch 2095, training loss: 6392.83, average training loss: 6589.18, base loss: 15978.49
[INFO 2017-06-30 02:49:17,636 main.py:52] epoch 2096, training loss: 6453.86, average training loss: 6589.21, base loss: 15979.30
[INFO 2017-06-30 02:49:20,794 main.py:52] epoch 2097, training loss: 6227.27, average training loss: 6587.63, base loss: 15979.48
[INFO 2017-06-30 02:49:23,965 main.py:52] epoch 2098, training loss: 6027.94, average training loss: 6586.87, base loss: 15978.58
[INFO 2017-06-30 02:49:27,117 main.py:52] epoch 2099, training loss: 6354.89, average training loss: 6586.73, base loss: 15977.80
[INFO 2017-06-30 02:49:27,117 main.py:54] epoch 2099, testing
[INFO 2017-06-30 02:49:40,540 main.py:97] average testing loss: 6445.49, base loss: 16597.41
[INFO 2017-06-30 02:49:40,541 main.py:98] improve_loss: 10151.92, improve_percent: 0.61
[INFO 2017-06-30 02:49:40,542 main.py:62] model save to ./model/final.pth
[INFO 2017-06-30 02:49:40,577 main.py:66] current best improved percent: 0.61
[INFO 2017-06-30 02:49:43,731 main.py:52] epoch 2100, training loss: 6591.36, average training loss: 6586.17, base loss: 15979.20
[INFO 2017-06-30 02:49:46,896 main.py:52] epoch 2101, training loss: 6151.17, average training loss: 6585.64, base loss: 15979.18
[INFO 2017-06-30 02:49:50,036 main.py:52] epoch 2102, training loss: 6117.92, average training loss: 6584.88, base loss: 15979.78
[INFO 2017-06-30 02:49:53,212 main.py:52] epoch 2103, training loss: 7172.41, average training loss: 6585.21, base loss: 15982.32
[INFO 2017-06-30 02:49:56,483 main.py:52] epoch 2104, training loss: 6800.02, average training loss: 6585.11, base loss: 15982.91
[INFO 2017-06-30 02:49:59,637 main.py:52] epoch 2105, training loss: 5719.24, average training loss: 6584.23, base loss: 15981.07
[INFO 2017-06-30 02:50:02,852 main.py:52] epoch 2106, training loss: 6642.51, average training loss: 6583.28, base loss: 15982.86
[INFO 2017-06-30 02:50:06,024 main.py:52] epoch 2107, training loss: 6390.02, average training loss: 6582.97, base loss: 15983.52
[INFO 2017-06-30 02:50:09,215 main.py:52] epoch 2108, training loss: 6711.62, average training loss: 6583.14, base loss: 15985.03
[INFO 2017-06-30 02:50:12,402 main.py:52] epoch 2109, training loss: 6230.47, average training loss: 6582.43, base loss: 15984.57
[INFO 2017-06-30 02:50:15,564 main.py:52] epoch 2110, training loss: 6518.20, average training loss: 6582.43, base loss: 15985.65
[INFO 2017-06-30 02:50:18,736 main.py:52] epoch 2111, training loss: 6371.16, average training loss: 6582.08, base loss: 15986.48
[INFO 2017-06-30 02:50:21,917 main.py:52] epoch 2112, training loss: 6407.49, average training loss: 6581.02, base loss: 15986.18
[INFO 2017-06-30 02:50:25,076 main.py:52] epoch 2113, training loss: 6116.47, average training loss: 6580.56, base loss: 15986.00
[INFO 2017-06-30 02:50:28,234 main.py:52] epoch 2114, training loss: 6674.53, average training loss: 6580.12, base loss: 15987.42
[INFO 2017-06-30 02:50:31,398 main.py:52] epoch 2115, training loss: 6102.72, average training loss: 6578.66, base loss: 15987.35
[INFO 2017-06-30 02:50:34,642 main.py:52] epoch 2116, training loss: 6145.34, average training loss: 6577.79, base loss: 15986.28
[INFO 2017-06-30 02:50:37,849 main.py:52] epoch 2117, training loss: 6022.16, average training loss: 6576.44, base loss: 15985.79
[INFO 2017-06-30 02:50:41,066 main.py:52] epoch 2118, training loss: 6387.19, average training loss: 6576.27, base loss: 15986.95
[INFO 2017-06-30 02:50:44,213 main.py:52] epoch 2119, training loss: 6766.44, average training loss: 6575.33, base loss: 15988.78
[INFO 2017-06-30 02:50:47,450 main.py:52] epoch 2120, training loss: 6304.72, average training loss: 6575.01, base loss: 15989.85
[INFO 2017-06-30 02:50:50,608 main.py:52] epoch 2121, training loss: 6200.19, average training loss: 6574.38, base loss: 15990.05
[INFO 2017-06-30 02:50:53,799 main.py:52] epoch 2122, training loss: 6149.92, average training loss: 6573.90, base loss: 15989.49
[INFO 2017-06-30 02:50:56,980 main.py:52] epoch 2123, training loss: 6833.23, average training loss: 6574.13, base loss: 15990.66
[INFO 2017-06-30 02:51:00,138 main.py:52] epoch 2124, training loss: 6225.58, average training loss: 6573.12, base loss: 15990.86
[INFO 2017-06-30 02:51:03,361 main.py:52] epoch 2125, training loss: 6151.20, average training loss: 6572.29, base loss: 15990.80
[INFO 2017-06-30 02:51:06,508 main.py:52] epoch 2126, training loss: 5976.87, average training loss: 6571.87, base loss: 15989.93
[INFO 2017-06-30 02:51:09,678 main.py:52] epoch 2127, training loss: 6375.47, average training loss: 6571.56, base loss: 15990.58
[INFO 2017-06-30 02:51:12,872 main.py:52] epoch 2128, training loss: 6276.42, average training loss: 6570.85, base loss: 15990.36
[INFO 2017-06-30 02:51:16,085 main.py:52] epoch 2129, training loss: 6537.13, average training loss: 6570.75, base loss: 15991.24
[INFO 2017-06-30 02:51:19,265 main.py:52] epoch 2130, training loss: 6163.29, average training loss: 6570.17, base loss: 15990.47
[INFO 2017-06-30 02:51:22,410 main.py:52] epoch 2131, training loss: 6164.77, average training loss: 6569.68, base loss: 15989.96
[INFO 2017-06-30 02:51:25,577 main.py:52] epoch 2132, training loss: 6327.59, average training loss: 6568.94, base loss: 15989.69
[INFO 2017-06-30 02:51:28,761 main.py:52] epoch 2133, training loss: 6106.74, average training loss: 6568.46, base loss: 15989.41
[INFO 2017-06-30 02:51:31,897 main.py:52] epoch 2134, training loss: 6525.41, average training loss: 6567.83, base loss: 15990.51
[INFO 2017-06-30 02:51:35,136 main.py:52] epoch 2135, training loss: 6476.97, average training loss: 6567.53, base loss: 15991.79
[INFO 2017-06-30 02:51:38,286 main.py:52] epoch 2136, training loss: 6509.63, average training loss: 6567.17, base loss: 15993.09
[INFO 2017-06-30 02:51:41,488 main.py:52] epoch 2137, training loss: 6535.96, average training loss: 6567.03, base loss: 15994.15
[INFO 2017-06-30 02:51:44,705 main.py:52] epoch 2138, training loss: 6733.23, average training loss: 6567.37, base loss: 15994.89
[INFO 2017-06-30 02:51:47,874 main.py:52] epoch 2139, training loss: 6149.97, average training loss: 6566.71, base loss: 15994.40
[INFO 2017-06-30 02:51:51,038 main.py:52] epoch 2140, training loss: 5622.61, average training loss: 6565.16, base loss: 15993.94
[INFO 2017-06-30 02:51:54,145 main.py:52] epoch 2141, training loss: 6164.33, average training loss: 6564.59, base loss: 15993.33
[INFO 2017-06-30 02:51:57,332 main.py:52] epoch 2142, training loss: 6505.37, average training loss: 6564.68, base loss: 15992.88
[INFO 2017-06-30 02:52:00,521 main.py:52] epoch 2143, training loss: 5986.08, average training loss: 6563.93, base loss: 15991.92
[INFO 2017-06-30 02:52:03,701 main.py:52] epoch 2144, training loss: 6133.17, average training loss: 6563.24, base loss: 15991.46
[INFO 2017-06-30 02:52:06,875 main.py:52] epoch 2145, training loss: 6276.68, average training loss: 6562.75, base loss: 15990.85
[INFO 2017-06-30 02:52:10,044 main.py:52] epoch 2146, training loss: 6201.01, average training loss: 6562.12, base loss: 15990.41
[INFO 2017-06-30 02:52:13,208 main.py:52] epoch 2147, training loss: 6334.33, average training loss: 6562.11, base loss: 15990.12
[INFO 2017-06-30 02:52:16,426 main.py:52] epoch 2148, training loss: 6884.81, average training loss: 6561.99, base loss: 15991.83
[INFO 2017-06-30 02:52:19,595 main.py:52] epoch 2149, training loss: 6209.81, average training loss: 6561.27, base loss: 15991.58
[INFO 2017-06-30 02:52:22,757 main.py:52] epoch 2150, training loss: 6180.39, average training loss: 6560.45, base loss: 15991.58
[INFO 2017-06-30 02:52:25,936 main.py:52] epoch 2151, training loss: 6270.56, average training loss: 6559.93, base loss: 15992.82
[INFO 2017-06-30 02:52:29,097 main.py:52] epoch 2152, training loss: 6078.45, average training loss: 6558.94, base loss: 15991.41
[INFO 2017-06-30 02:52:32,255 main.py:52] epoch 2153, training loss: 6406.40, average training loss: 6558.31, base loss: 15991.44
[INFO 2017-06-30 02:52:35,443 main.py:52] epoch 2154, training loss: 6488.95, average training loss: 6558.06, base loss: 15992.57
[INFO 2017-06-30 02:52:38,564 main.py:52] epoch 2155, training loss: 6140.63, average training loss: 6557.20, base loss: 15992.78
[INFO 2017-06-30 02:52:41,792 main.py:52] epoch 2156, training loss: 6421.59, average training loss: 6557.45, base loss: 15993.50
[INFO 2017-06-30 02:52:44,983 main.py:52] epoch 2157, training loss: 6439.49, average training loss: 6557.05, base loss: 15993.92
[INFO 2017-06-30 02:52:48,169 main.py:52] epoch 2158, training loss: 6310.85, average training loss: 6556.59, base loss: 15994.38
[INFO 2017-06-30 02:52:51,293 main.py:52] epoch 2159, training loss: 6311.81, average training loss: 6555.91, base loss: 15995.27
[INFO 2017-06-30 02:52:54,455 main.py:52] epoch 2160, training loss: 6385.61, average training loss: 6555.59, base loss: 15995.58
[INFO 2017-06-30 02:52:57,625 main.py:52] epoch 2161, training loss: 5981.17, average training loss: 6554.56, base loss: 15995.20
[INFO 2017-06-30 02:53:00,794 main.py:52] epoch 2162, training loss: 6785.91, average training loss: 6554.38, base loss: 15996.81
[INFO 2017-06-30 02:53:03,988 main.py:52] epoch 2163, training loss: 6007.37, average training loss: 6553.67, base loss: 15996.12
[INFO 2017-06-30 02:53:07,162 main.py:52] epoch 2164, training loss: 6260.30, average training loss: 6552.64, base loss: 15996.66
[INFO 2017-06-30 02:53:10,345 main.py:52] epoch 2165, training loss: 6374.66, average training loss: 6552.44, base loss: 15996.71
[INFO 2017-06-30 02:53:13,505 main.py:52] epoch 2166, training loss: 5914.87, average training loss: 6551.41, base loss: 15996.40
[INFO 2017-06-30 02:53:16,701 main.py:52] epoch 2167, training loss: 5969.00, average training loss: 6550.99, base loss: 15995.67
[INFO 2017-06-30 02:53:19,864 main.py:52] epoch 2168, training loss: 6205.11, average training loss: 6550.82, base loss: 15995.37
[INFO 2017-06-30 02:53:23,137 main.py:52] epoch 2169, training loss: 6362.65, average training loss: 6551.02, base loss: 15996.32
[INFO 2017-06-30 02:53:26,322 main.py:52] epoch 2170, training loss: 6475.79, average training loss: 6549.11, base loss: 15997.87
[INFO 2017-06-30 02:53:29,492 main.py:52] epoch 2171, training loss: 6429.01, average training loss: 6548.88, base loss: 15998.44
[INFO 2017-06-30 02:53:32,636 main.py:52] epoch 2172, training loss: 6665.91, average training loss: 6548.00, base loss: 15999.62
[INFO 2017-06-30 02:53:35,864 main.py:52] epoch 2173, training loss: 6440.38, average training loss: 6548.03, base loss: 15999.93
[INFO 2017-06-30 02:53:39,071 main.py:52] epoch 2174, training loss: 6694.29, average training loss: 6547.68, base loss: 16001.50
[INFO 2017-06-30 02:53:42,246 main.py:52] epoch 2175, training loss: 6788.01, average training loss: 6547.70, base loss: 16002.01
[INFO 2017-06-30 02:53:45,423 main.py:52] epoch 2176, training loss: 6030.80, average training loss: 6547.14, base loss: 16001.03
[INFO 2017-06-30 02:53:48,685 main.py:52] epoch 2177, training loss: 6425.78, average training loss: 6546.27, base loss: 16000.67
[INFO 2017-06-30 02:53:51,896 main.py:52] epoch 2178, training loss: 6238.95, average training loss: 6545.90, base loss: 16000.32
[INFO 2017-06-30 02:53:55,060 main.py:52] epoch 2179, training loss: 6131.06, average training loss: 6545.11, base loss: 15999.62
[INFO 2017-06-30 02:53:58,193 main.py:52] epoch 2180, training loss: 6238.00, average training loss: 6544.11, base loss: 15999.60
[INFO 2017-06-30 02:54:01,377 main.py:52] epoch 2181, training loss: 6150.62, average training loss: 6543.65, base loss: 15999.11
[INFO 2017-06-30 02:54:04,564 main.py:52] epoch 2182, training loss: 6744.88, average training loss: 6543.08, base loss: 16000.31
[INFO 2017-06-30 02:54:07,727 main.py:52] epoch 2183, training loss: 6340.24, average training loss: 6542.63, base loss: 16000.23
[INFO 2017-06-30 02:54:10,956 main.py:52] epoch 2184, training loss: 6502.22, average training loss: 6542.42, base loss: 16000.26
[INFO 2017-06-30 02:54:14,137 main.py:52] epoch 2185, training loss: 6485.39, average training loss: 6541.39, base loss: 16000.73
[INFO 2017-06-30 02:54:17,297 main.py:52] epoch 2186, training loss: 6852.84, average training loss: 6541.59, base loss: 16001.63
[INFO 2017-06-30 02:54:20,479 main.py:52] epoch 2187, training loss: 6180.66, average training loss: 6540.98, base loss: 16001.81
[INFO 2017-06-30 02:54:23,683 main.py:52] epoch 2188, training loss: 5950.65, average training loss: 6540.00, base loss: 16001.66
[INFO 2017-06-30 02:54:26,857 main.py:52] epoch 2189, training loss: 6757.19, average training loss: 6540.31, base loss: 16002.57
[INFO 2017-06-30 02:54:30,004 main.py:52] epoch 2190, training loss: 6620.60, average training loss: 6539.89, base loss: 16003.21
[INFO 2017-06-30 02:54:33,148 main.py:52] epoch 2191, training loss: 6262.26, average training loss: 6539.73, base loss: 16002.35
[INFO 2017-06-30 02:54:36,341 main.py:52] epoch 2192, training loss: 6289.94, average training loss: 6539.12, base loss: 16002.09
[INFO 2017-06-30 02:54:39,506 main.py:52] epoch 2193, training loss: 6803.54, average training loss: 6538.77, base loss: 16003.29
[INFO 2017-06-30 02:54:42,650 main.py:52] epoch 2194, training loss: 5975.88, average training loss: 6537.60, base loss: 16002.46
[INFO 2017-06-30 02:54:45,867 main.py:52] epoch 2195, training loss: 6236.76, average training loss: 6537.36, base loss: 16002.03
[INFO 2017-06-30 02:54:49,056 main.py:52] epoch 2196, training loss: 6303.16, average training loss: 6536.94, base loss: 16002.43
[INFO 2017-06-30 02:54:52,255 main.py:52] epoch 2197, training loss: 5865.66, average training loss: 6536.28, base loss: 16001.63
[INFO 2017-06-30 02:54:55,417 main.py:52] epoch 2198, training loss: 5878.69, average training loss: 6535.00, base loss: 16000.99
[INFO 2017-06-30 02:54:58,558 main.py:52] epoch 2199, training loss: 7075.07, average training loss: 6534.73, base loss: 16002.10
[INFO 2017-06-30 02:54:58,559 main.py:54] epoch 2199, testing
[INFO 2017-06-30 02:55:11,944 main.py:97] average testing loss: 6326.83, base loss: 16257.71
[INFO 2017-06-30 02:55:11,944 main.py:98] improve_loss: 9930.88, improve_percent: 0.61
[INFO 2017-06-30 02:55:11,946 main.py:66] current best improved percent: 0.61
[INFO 2017-06-30 02:55:15,121 main.py:52] epoch 2200, training loss: 5854.28, average training loss: 6533.70, base loss: 16002.00
[INFO 2017-06-30 02:55:18,307 main.py:52] epoch 2201, training loss: 6378.59, average training loss: 6532.82, base loss: 16002.15
[INFO 2017-06-30 02:55:21,494 main.py:52] epoch 2202, training loss: 6473.06, average training loss: 6531.89, base loss: 16001.80
[INFO 2017-06-30 02:55:24,678 main.py:52] epoch 2203, training loss: 6150.06, average training loss: 6531.17, base loss: 16000.87
[INFO 2017-06-30 02:55:27,901 main.py:52] epoch 2204, training loss: 6699.28, average training loss: 6531.42, base loss: 16001.55
[INFO 2017-06-30 02:55:31,061 main.py:52] epoch 2205, training loss: 6409.77, average training loss: 6531.55, base loss: 16002.25
[INFO 2017-06-30 02:55:34,229 main.py:52] epoch 2206, training loss: 6458.15, average training loss: 6530.89, base loss: 16002.40
[INFO 2017-06-30 02:55:37,354 main.py:52] epoch 2207, training loss: 6132.56, average training loss: 6529.79, base loss: 16002.54
[INFO 2017-06-30 02:55:40,525 main.py:52] epoch 2208, training loss: 6065.80, average training loss: 6529.09, base loss: 16003.68
[INFO 2017-06-30 02:55:43,700 main.py:52] epoch 2209, training loss: 6243.13, average training loss: 6528.03, base loss: 16004.83
[INFO 2017-06-30 02:55:46,854 main.py:52] epoch 2210, training loss: 6726.44, average training loss: 6527.46, base loss: 16006.31
[INFO 2017-06-30 02:55:49,988 main.py:52] epoch 2211, training loss: 6038.12, average training loss: 6526.82, base loss: 16006.78
[INFO 2017-06-30 02:55:53,205 main.py:52] epoch 2212, training loss: 6364.88, average training loss: 6525.83, base loss: 16006.72
[INFO 2017-06-30 02:55:56,433 main.py:52] epoch 2213, training loss: 6204.41, average training loss: 6524.68, base loss: 16006.67
[INFO 2017-06-30 02:55:59,593 main.py:52] epoch 2214, training loss: 6131.86, average training loss: 6524.25, base loss: 16007.05
[INFO 2017-06-30 02:56:02,752 main.py:52] epoch 2215, training loss: 6464.27, average training loss: 6523.82, base loss: 16007.70
[INFO 2017-06-30 02:56:05,928 main.py:52] epoch 2216, training loss: 6317.53, average training loss: 6523.30, base loss: 16008.09
[INFO 2017-06-30 02:56:09,073 main.py:52] epoch 2217, training loss: 6340.86, average training loss: 6522.25, base loss: 16007.78
[INFO 2017-06-30 02:56:12,259 main.py:52] epoch 2218, training loss: 6378.83, average training loss: 6521.34, base loss: 16008.25
[INFO 2017-06-30 02:56:15,453 main.py:52] epoch 2219, training loss: 6969.07, average training loss: 6521.92, base loss: 16008.33
[INFO 2017-06-30 02:56:18,599 main.py:52] epoch 2220, training loss: 6538.64, average training loss: 6521.90, base loss: 16009.46
[INFO 2017-06-30 02:56:21,756 main.py:52] epoch 2221, training loss: 6284.53, average training loss: 6521.96, base loss: 16009.40
[INFO 2017-06-30 02:56:24,931 main.py:52] epoch 2222, training loss: 6074.33, average training loss: 6521.24, base loss: 16008.40
[INFO 2017-06-30 02:56:28,106 main.py:52] epoch 2223, training loss: 6361.83, average training loss: 6520.45, base loss: 16007.72
[INFO 2017-06-30 02:56:31,298 main.py:52] epoch 2224, training loss: 6823.07, average training loss: 6520.58, base loss: 16009.17
[INFO 2017-06-30 02:56:34,485 main.py:52] epoch 2225, training loss: 6152.04, average training loss: 6519.26, base loss: 16008.18
[INFO 2017-06-30 02:56:37,617 main.py:52] epoch 2226, training loss: 6432.87, average training loss: 6519.10, base loss: 16008.84
[INFO 2017-06-30 02:56:40,772 main.py:52] epoch 2227, training loss: 6372.49, average training loss: 6518.95, base loss: 16009.92
[INFO 2017-06-30 02:56:43,916 main.py:52] epoch 2228, training loss: 6125.46, average training loss: 6518.24, base loss: 16009.34
[INFO 2017-06-30 02:56:47,085 main.py:52] epoch 2229, training loss: 6505.51, average training loss: 6518.22, base loss: 16008.74
[INFO 2017-06-30 02:56:50,229 main.py:52] epoch 2230, training loss: 6630.78, average training loss: 6517.61, base loss: 16009.91
[INFO 2017-06-30 02:56:53,424 main.py:52] epoch 2231, training loss: 6271.02, average training loss: 6517.19, base loss: 16010.16
[INFO 2017-06-30 02:56:56,592 main.py:52] epoch 2232, training loss: 6146.34, average training loss: 6516.46, base loss: 16009.78
[INFO 2017-06-30 02:56:59,787 main.py:52] epoch 2233, training loss: 6528.61, average training loss: 6515.49, base loss: 16010.71
[INFO 2017-06-30 02:57:02,956 main.py:52] epoch 2234, training loss: 6484.25, average training loss: 6515.75, base loss: 16011.99
[INFO 2017-06-30 02:57:06,162 main.py:52] epoch 2235, training loss: 6436.90, average training loss: 6515.76, base loss: 16012.73
[INFO 2017-06-30 02:57:09,363 main.py:52] epoch 2236, training loss: 6305.31, average training loss: 6515.57, base loss: 16013.67
[INFO 2017-06-30 02:57:12,503 main.py:52] epoch 2237, training loss: 6595.91, average training loss: 6515.34, base loss: 16014.99
[INFO 2017-06-30 02:57:15,687 main.py:52] epoch 2238, training loss: 6278.98, average training loss: 6514.36, base loss: 16015.49
[INFO 2017-06-30 02:57:18,847 main.py:52] epoch 2239, training loss: 6513.95, average training loss: 6514.10, base loss: 16017.05
[INFO 2017-06-30 02:57:22,020 main.py:52] epoch 2240, training loss: 6100.85, average training loss: 6512.75, base loss: 16016.62
[INFO 2017-06-30 02:57:25,184 main.py:52] epoch 2241, training loss: 6518.26, average training loss: 6512.16, base loss: 16018.54
[INFO 2017-06-30 02:57:28,299 main.py:52] epoch 2242, training loss: 6464.33, average training loss: 6512.03, base loss: 16018.27
[INFO 2017-06-30 02:57:31,453 main.py:52] epoch 2243, training loss: 6097.68, average training loss: 6511.55, base loss: 16017.48
[INFO 2017-06-30 02:57:34,664 main.py:52] epoch 2244, training loss: 5913.05, average training loss: 6511.41, base loss: 16017.65
[INFO 2017-06-30 02:57:37,850 main.py:52] epoch 2245, training loss: 6508.22, average training loss: 6511.09, base loss: 16017.91
[INFO 2017-06-30 02:57:41,002 main.py:52] epoch 2246, training loss: 6410.64, average training loss: 6511.37, base loss: 16018.79
[INFO 2017-06-30 02:57:44,195 main.py:52] epoch 2247, training loss: 6041.67, average training loss: 6510.72, base loss: 16018.36
[INFO 2017-06-30 02:57:47,357 main.py:52] epoch 2248, training loss: 5844.73, average training loss: 6509.27, base loss: 16017.24
[INFO 2017-06-30 02:57:50,503 main.py:52] epoch 2249, training loss: 6002.56, average training loss: 6508.42, base loss: 16015.01
[INFO 2017-06-30 02:57:53,673 main.py:52] epoch 2250, training loss: 5967.87, average training loss: 6507.43, base loss: 16014.68
[INFO 2017-06-30 02:57:56,861 main.py:52] epoch 2251, training loss: 6817.54, average training loss: 6507.29, base loss: 16016.06
[INFO 2017-06-30 02:58:00,093 main.py:52] epoch 2252, training loss: 6176.49, average training loss: 6506.85, base loss: 16016.17
[INFO 2017-06-30 02:58:03,285 main.py:52] epoch 2253, training loss: 5779.64, average training loss: 6505.64, base loss: 16016.12
[INFO 2017-06-30 02:58:06,435 main.py:52] epoch 2254, training loss: 6430.08, average training loss: 6505.01, base loss: 16016.55
[INFO 2017-06-30 02:58:09,596 main.py:52] epoch 2255, training loss: 6033.99, average training loss: 6504.86, base loss: 16015.86
[INFO 2017-06-30 02:58:12,806 main.py:52] epoch 2256, training loss: 6294.42, average training loss: 6504.08, base loss: 16016.13
[INFO 2017-06-30 02:58:16,041 main.py:52] epoch 2257, training loss: 5908.47, average training loss: 6502.94, base loss: 16015.15
[INFO 2017-06-30 02:58:19,190 main.py:52] epoch 2258, training loss: 6042.80, average training loss: 6501.98, base loss: 16015.16
[INFO 2017-06-30 02:58:22,373 main.py:52] epoch 2259, training loss: 6181.06, average training loss: 6501.80, base loss: 16015.27
[INFO 2017-06-30 02:58:25,540 main.py:52] epoch 2260, training loss: 6168.57, average training loss: 6501.09, base loss: 16016.01
[INFO 2017-06-30 02:58:28,707 main.py:52] epoch 2261, training loss: 6834.57, average training loss: 6501.11, base loss: 16018.34
[INFO 2017-06-30 02:58:31,887 main.py:52] epoch 2262, training loss: 6374.54, average training loss: 6500.73, base loss: 16018.28
[INFO 2017-06-30 02:58:35,044 main.py:52] epoch 2263, training loss: 6229.39, average training loss: 6500.23, base loss: 16018.50
[INFO 2017-06-30 02:58:38,247 main.py:52] epoch 2264, training loss: 6017.50, average training loss: 6500.01, base loss: 16019.13
[INFO 2017-06-30 02:58:41,390 main.py:52] epoch 2265, training loss: 6447.16, average training loss: 6499.79, base loss: 16020.06
[INFO 2017-06-30 02:58:44,606 main.py:52] epoch 2266, training loss: 6204.65, average training loss: 6498.43, base loss: 16020.32
[INFO 2017-06-30 02:58:47,784 main.py:52] epoch 2267, training loss: 6193.39, average training loss: 6498.10, base loss: 16019.73
[INFO 2017-06-30 02:58:50,952 main.py:52] epoch 2268, training loss: 6657.11, average training loss: 6498.36, base loss: 16020.67
[INFO 2017-06-30 02:58:54,123 main.py:52] epoch 2269, training loss: 6569.20, average training loss: 6498.50, base loss: 16021.57
[INFO 2017-06-30 02:58:57,321 main.py:52] epoch 2270, training loss: 5981.73, average training loss: 6498.01, base loss: 16021.41
[INFO 2017-06-30 02:59:00,500 main.py:52] epoch 2271, training loss: 5283.17, average training loss: 6495.82, base loss: 16019.03
[INFO 2017-06-30 02:59:03,655 main.py:52] epoch 2272, training loss: 6096.29, average training loss: 6494.85, base loss: 16018.74
[INFO 2017-06-30 02:59:06,804 main.py:52] epoch 2273, training loss: 5998.90, average training loss: 6494.53, base loss: 16018.91
[INFO 2017-06-30 02:59:09,957 main.py:52] epoch 2274, training loss: 6462.96, average training loss: 6493.96, base loss: 16019.50
[INFO 2017-06-30 02:59:13,107 main.py:52] epoch 2275, training loss: 6275.09, average training loss: 6493.81, base loss: 16020.04
[INFO 2017-06-30 02:59:16,334 main.py:52] epoch 2276, training loss: 6905.90, average training loss: 6494.01, base loss: 16022.40
[INFO 2017-06-30 02:59:19,510 main.py:52] epoch 2277, training loss: 6260.90, average training loss: 6493.83, base loss: 16022.84
[INFO 2017-06-30 02:59:22,702 main.py:52] epoch 2278, training loss: 6191.87, average training loss: 6493.48, base loss: 16021.73
[INFO 2017-06-30 02:59:25,850 main.py:52] epoch 2279, training loss: 6272.53, average training loss: 6493.04, base loss: 16020.60
[INFO 2017-06-30 02:59:29,054 main.py:52] epoch 2280, training loss: 6247.55, average training loss: 6492.60, base loss: 16020.77
[INFO 2017-06-30 02:59:32,268 main.py:52] epoch 2281, training loss: 6161.11, average training loss: 6491.71, base loss: 16019.60
[INFO 2017-06-30 02:59:35,434 main.py:52] epoch 2282, training loss: 6145.01, average training loss: 6491.33, base loss: 16020.39
[INFO 2017-06-30 02:59:38,596 main.py:52] epoch 2283, training loss: 6634.61, average training loss: 6491.70, base loss: 16022.18
[INFO 2017-06-30 02:59:41,743 main.py:52] epoch 2284, training loss: 5972.74, average training loss: 6490.73, base loss: 16022.81
[INFO 2017-06-30 02:59:44,882 main.py:52] epoch 2285, training loss: 6616.88, average training loss: 6490.91, base loss: 16023.03
[INFO 2017-06-30 02:59:48,062 main.py:52] epoch 2286, training loss: 6188.99, average training loss: 6490.59, base loss: 16022.64
[INFO 2017-06-30 02:59:51,194 main.py:52] epoch 2287, training loss: 6322.04, average training loss: 6490.39, base loss: 16023.54
[INFO 2017-06-30 02:59:54,351 main.py:52] epoch 2288, training loss: 6441.17, average training loss: 6489.04, base loss: 16024.61
[INFO 2017-06-30 02:59:57,490 main.py:52] epoch 2289, training loss: 6137.15, average training loss: 6488.45, base loss: 16023.69
[INFO 2017-06-30 03:00:00,635 main.py:52] epoch 2290, training loss: 6294.77, average training loss: 6488.04, base loss: 16023.89
[INFO 2017-06-30 03:00:03,799 main.py:52] epoch 2291, training loss: 6085.02, average training loss: 6487.17, base loss: 16022.71
[INFO 2017-06-30 03:00:06,966 main.py:52] epoch 2292, training loss: 6500.48, average training loss: 6487.44, base loss: 16023.33
[INFO 2017-06-30 03:00:10,129 main.py:52] epoch 2293, training loss: 6509.99, average training loss: 6487.22, base loss: 16024.30
[INFO 2017-06-30 03:00:13,303 main.py:52] epoch 2294, training loss: 6351.90, average training loss: 6486.77, base loss: 16024.11
[INFO 2017-06-30 03:00:16,499 main.py:52] epoch 2295, training loss: 6321.20, average training loss: 6486.64, base loss: 16024.27
[INFO 2017-06-30 03:00:19,689 main.py:52] epoch 2296, training loss: 6134.01, average training loss: 6486.40, base loss: 16024.02
[INFO 2017-06-30 03:00:22,856 main.py:52] epoch 2297, training loss: 6139.25, average training loss: 6485.96, base loss: 16024.60
[INFO 2017-06-30 03:00:25,986 main.py:52] epoch 2298, training loss: 6559.35, average training loss: 6485.79, base loss: 16026.14
[INFO 2017-06-30 03:00:29,141 main.py:52] epoch 2299, training loss: 6012.62, average training loss: 6484.96, base loss: 16025.34
[INFO 2017-06-30 03:00:29,141 main.py:54] epoch 2299, testing
[INFO 2017-06-30 03:00:42,400 main.py:97] average testing loss: 6340.74, base loss: 15934.97
[INFO 2017-06-30 03:00:42,400 main.py:98] improve_loss: 9594.23, improve_percent: 0.60
[INFO 2017-06-30 03:00:42,401 main.py:66] current best improved percent: 0.61
[INFO 2017-06-30 03:00:45,578 main.py:52] epoch 2300, training loss: 6089.79, average training loss: 6484.97, base loss: 16025.10
[INFO 2017-06-30 03:00:48,770 main.py:52] epoch 2301, training loss: 6356.68, average training loss: 6484.49, base loss: 16025.34
[INFO 2017-06-30 03:00:51,955 main.py:52] epoch 2302, training loss: 6505.79, average training loss: 6484.42, base loss: 16025.04
[INFO 2017-06-30 03:00:55,109 main.py:52] epoch 2303, training loss: 6245.25, average training loss: 6483.86, base loss: 16024.80
[INFO 2017-06-30 03:00:58,260 main.py:52] epoch 2304, training loss: 6592.02, average training loss: 6483.42, base loss: 16025.80
[INFO 2017-06-30 03:01:01,420 main.py:52] epoch 2305, training loss: 6342.24, average training loss: 6482.94, base loss: 16025.61
[INFO 2017-06-30 03:01:04,548 main.py:52] epoch 2306, training loss: 6060.38, average training loss: 6481.92, base loss: 16026.16
[INFO 2017-06-30 03:01:07,713 main.py:52] epoch 2307, training loss: 6255.33, average training loss: 6481.41, base loss: 16025.08
[INFO 2017-06-30 03:01:10,874 main.py:52] epoch 2308, training loss: 6352.93, average training loss: 6480.06, base loss: 16024.83
[INFO 2017-06-30 03:01:14,029 main.py:52] epoch 2309, training loss: 5660.38, average training loss: 6479.07, base loss: 16023.65
[INFO 2017-06-30 03:01:17,203 main.py:52] epoch 2310, training loss: 6695.04, average training loss: 6479.42, base loss: 16025.28
[INFO 2017-06-30 03:01:20,362 main.py:52] epoch 2311, training loss: 7279.94, average training loss: 6479.91, base loss: 16027.26
[INFO 2017-06-30 03:01:23,508 main.py:52] epoch 2312, training loss: 6230.81, average training loss: 6479.57, base loss: 16027.60
[INFO 2017-06-30 03:01:26,693 main.py:52] epoch 2313, training loss: 6243.37, average training loss: 6478.88, base loss: 16027.49
[INFO 2017-06-30 03:01:29,890 main.py:52] epoch 2314, training loss: 6477.54, average training loss: 6478.67, base loss: 16027.22
[INFO 2017-06-30 03:01:33,089 main.py:52] epoch 2315, training loss: 6327.25, average training loss: 6477.84, base loss: 16026.72
[INFO 2017-06-30 03:01:36,216 main.py:52] epoch 2316, training loss: 6391.22, average training loss: 6477.30, base loss: 16027.49
[INFO 2017-06-30 03:01:39,398 main.py:52] epoch 2317, training loss: 5566.60, average training loss: 6475.82, base loss: 16026.07
[INFO 2017-06-30 03:01:42,606 main.py:52] epoch 2318, training loss: 6250.63, average training loss: 6475.50, base loss: 16026.37
[INFO 2017-06-30 03:01:45,763 main.py:52] epoch 2319, training loss: 5999.17, average training loss: 6474.95, base loss: 16026.11
[INFO 2017-06-30 03:01:48,929 main.py:52] epoch 2320, training loss: 6434.02, average training loss: 6474.92, base loss: 16027.20
[INFO 2017-06-30 03:01:52,114 main.py:52] epoch 2321, training loss: 6146.06, average training loss: 6474.18, base loss: 16026.57
[INFO 2017-06-30 03:01:55,274 main.py:52] epoch 2322, training loss: 5983.12, average training loss: 6473.72, base loss: 16026.13
[INFO 2017-06-30 03:01:58,474 main.py:52] epoch 2323, training loss: 6347.10, average training loss: 6473.67, base loss: 16026.16
[INFO 2017-06-30 03:02:01,671 main.py:52] epoch 2324, training loss: 6354.66, average training loss: 6473.41, base loss: 16026.98
[INFO 2017-06-30 03:02:04,823 main.py:52] epoch 2325, training loss: 6249.92, average training loss: 6473.34, base loss: 16027.80
[INFO 2017-06-30 03:02:07,985 main.py:52] epoch 2326, training loss: 5780.10, average training loss: 6472.29, base loss: 16026.90
[INFO 2017-06-30 03:02:11,132 main.py:52] epoch 2327, training loss: 6324.84, average training loss: 6471.98, base loss: 16028.18
[INFO 2017-06-30 03:02:14,292 main.py:52] epoch 2328, training loss: 6660.36, average training loss: 6472.24, base loss: 16028.71
[INFO 2017-06-30 03:02:17,503 main.py:52] epoch 2329, training loss: 6192.33, average training loss: 6471.39, base loss: 16028.30
[INFO 2017-06-30 03:02:20,638 main.py:52] epoch 2330, training loss: 6142.39, average training loss: 6471.15, base loss: 16027.75
[INFO 2017-06-30 03:02:23,808 main.py:52] epoch 2331, training loss: 6314.65, average training loss: 6470.84, base loss: 16028.06
[INFO 2017-06-30 03:02:27,012 main.py:52] epoch 2332, training loss: 6818.60, average training loss: 6470.89, base loss: 16029.47
[INFO 2017-06-30 03:02:30,166 main.py:52] epoch 2333, training loss: 6255.70, average training loss: 6470.91, base loss: 16029.38
[INFO 2017-06-30 03:02:33,342 main.py:52] epoch 2334, training loss: 6003.49, average training loss: 6469.91, base loss: 16028.73
[INFO 2017-06-30 03:02:36,519 main.py:52] epoch 2335, training loss: 5814.66, average training loss: 6468.75, base loss: 16028.02
[INFO 2017-06-30 03:02:39,726 main.py:52] epoch 2336, training loss: 5946.02, average training loss: 6467.64, base loss: 16027.66
[INFO 2017-06-30 03:02:42,925 main.py:52] epoch 2337, training loss: 6133.97, average training loss: 6466.77, base loss: 16027.35
[INFO 2017-06-30 03:02:46,085 main.py:52] epoch 2338, training loss: 5626.11, average training loss: 6465.77, base loss: 16026.60
[INFO 2017-06-30 03:02:49,268 main.py:52] epoch 2339, training loss: 6080.65, average training loss: 6465.49, base loss: 16026.33
[INFO 2017-06-30 03:02:52,413 main.py:52] epoch 2340, training loss: 6302.98, average training loss: 6465.56, base loss: 16025.61
[INFO 2017-06-30 03:02:55,558 main.py:52] epoch 2341, training loss: 5668.74, average training loss: 6464.25, base loss: 16024.89
[INFO 2017-06-30 03:02:58,724 main.py:52] epoch 2342, training loss: 6461.04, average training loss: 6463.98, base loss: 16024.36
[INFO 2017-06-30 03:03:01,891 main.py:52] epoch 2343, training loss: 5847.13, average training loss: 6463.16, base loss: 16023.65
[INFO 2017-06-30 03:03:05,068 main.py:52] epoch 2344, training loss: 5856.50, average training loss: 6461.92, base loss: 16022.44
[INFO 2017-06-30 03:03:08,303 main.py:52] epoch 2345, training loss: 6451.54, average training loss: 6461.62, base loss: 16023.49
[INFO 2017-06-30 03:03:11,453 main.py:52] epoch 2346, training loss: 7068.67, average training loss: 6462.01, base loss: 16025.08
[INFO 2017-06-30 03:03:14,631 main.py:52] epoch 2347, training loss: 6217.89, average training loss: 6462.00, base loss: 16024.77
[INFO 2017-06-30 03:03:17,799 main.py:52] epoch 2348, training loss: 5902.68, average training loss: 6460.98, base loss: 16024.96
[INFO 2017-06-30 03:03:20,992 main.py:52] epoch 2349, training loss: 6512.29, average training loss: 6460.29, base loss: 16026.18
[INFO 2017-06-30 03:03:24,186 main.py:52] epoch 2350, training loss: 6045.03, average training loss: 6458.90, base loss: 16024.94
[INFO 2017-06-30 03:03:27,384 main.py:52] epoch 2351, training loss: 6027.92, average training loss: 6457.77, base loss: 16024.11
[INFO 2017-06-30 03:03:30,564 main.py:52] epoch 2352, training loss: 6195.29, average training loss: 6457.57, base loss: 16024.64
[INFO 2017-06-30 03:03:33,706 main.py:52] epoch 2353, training loss: 6387.31, average training loss: 6457.49, base loss: 16024.32
[INFO 2017-06-30 03:03:36,900 main.py:52] epoch 2354, training loss: 6555.78, average training loss: 6457.39, base loss: 16025.17
[INFO 2017-06-30 03:03:40,089 main.py:52] epoch 2355, training loss: 6686.72, average training loss: 6457.69, base loss: 16025.47
[INFO 2017-06-30 03:03:43,260 main.py:52] epoch 2356, training loss: 6393.31, average training loss: 6457.29, base loss: 16025.64
[INFO 2017-06-30 03:03:46,412 main.py:52] epoch 2357, training loss: 6170.52, average training loss: 6456.77, base loss: 16026.16
[INFO 2017-06-30 03:03:49,576 main.py:52] epoch 2358, training loss: 6787.29, average training loss: 6457.02, base loss: 16027.90
[INFO 2017-06-30 03:03:52,770 main.py:52] epoch 2359, training loss: 5970.03, average training loss: 6456.44, base loss: 16027.59
[INFO 2017-06-30 03:03:55,952 main.py:52] epoch 2360, training loss: 6412.28, average training loss: 6455.90, base loss: 16028.69
[INFO 2017-06-30 03:03:59,164 main.py:52] epoch 2361, training loss: 6337.05, average training loss: 6455.04, base loss: 16029.36
[INFO 2017-06-30 03:04:02,355 main.py:52] epoch 2362, training loss: 6356.41, average training loss: 6454.14, base loss: 16031.10
[INFO 2017-06-30 03:04:05,528 main.py:52] epoch 2363, training loss: 6548.91, average training loss: 6454.48, base loss: 16032.42
[INFO 2017-06-30 03:04:08,647 main.py:52] epoch 2364, training loss: 5811.28, average training loss: 6454.21, base loss: 16030.74
[INFO 2017-06-30 03:04:11,822 main.py:52] epoch 2365, training loss: 6532.99, average training loss: 6454.56, base loss: 16031.01
[INFO 2017-06-30 03:04:14,989 main.py:52] epoch 2366, training loss: 5849.09, average training loss: 6454.00, base loss: 16029.60
[INFO 2017-06-30 03:04:18,144 main.py:52] epoch 2367, training loss: 6015.56, average training loss: 6453.60, base loss: 16027.23
[INFO 2017-06-30 03:04:21,299 main.py:52] epoch 2368, training loss: 5896.77, average training loss: 6453.04, base loss: 16026.46
[INFO 2017-06-30 03:04:24,423 main.py:52] epoch 2369, training loss: 6795.82, average training loss: 6453.59, base loss: 16027.87
[INFO 2017-06-30 03:04:27,600 main.py:52] epoch 2370, training loss: 6864.54, average training loss: 6453.50, base loss: 16028.84
[INFO 2017-06-30 03:04:30,760 main.py:52] epoch 2371, training loss: 6615.26, average training loss: 6453.44, base loss: 16029.83
[INFO 2017-06-30 03:04:33,915 main.py:52] epoch 2372, training loss: 5878.68, average training loss: 6452.80, base loss: 16028.22
[INFO 2017-06-30 03:04:37,090 main.py:52] epoch 2373, training loss: 5931.13, average training loss: 6452.01, base loss: 16026.13
[INFO 2017-06-30 03:04:40,305 main.py:52] epoch 2374, training loss: 5649.29, average training loss: 6450.66, base loss: 16025.43
[INFO 2017-06-30 03:04:43,528 main.py:52] epoch 2375, training loss: 6274.02, average training loss: 6450.24, base loss: 16025.47
[INFO 2017-06-30 03:04:46,653 main.py:52] epoch 2376, training loss: 6223.14, average training loss: 6450.37, base loss: 16024.55
[INFO 2017-06-30 03:04:49,797 main.py:52] epoch 2377, training loss: 6371.80, average training loss: 6449.73, base loss: 16023.29
[INFO 2017-06-30 03:04:52,965 main.py:52] epoch 2378, training loss: 6583.20, average training loss: 6449.95, base loss: 16024.73
[INFO 2017-06-30 03:04:56,157 main.py:52] epoch 2379, training loss: 6293.74, average training loss: 6449.75, base loss: 16024.82
[INFO 2017-06-30 03:04:59,317 main.py:52] epoch 2380, training loss: 6310.96, average training loss: 6449.80, base loss: 16023.41
[INFO 2017-06-30 03:05:02,456 main.py:52] epoch 2381, training loss: 6309.14, average training loss: 6449.60, base loss: 16023.22
[INFO 2017-06-30 03:05:05,629 main.py:52] epoch 2382, training loss: 6299.82, average training loss: 6449.43, base loss: 16023.27
[INFO 2017-06-30 03:05:08,791 main.py:52] epoch 2383, training loss: 6485.38, average training loss: 6448.39, base loss: 16023.70
[INFO 2017-06-30 03:05:11,961 main.py:52] epoch 2384, training loss: 6520.04, average training loss: 6448.50, base loss: 16023.66
[INFO 2017-06-30 03:05:15,113 main.py:52] epoch 2385, training loss: 6198.48, average training loss: 6448.70, base loss: 16022.16
[INFO 2017-06-30 03:05:18,278 main.py:52] epoch 2386, training loss: 5954.36, average training loss: 6447.79, base loss: 16021.43
[INFO 2017-06-30 03:05:21,476 main.py:52] epoch 2387, training loss: 6275.71, average training loss: 6447.44, base loss: 16020.74
[INFO 2017-06-30 03:05:24,652 main.py:52] epoch 2388, training loss: 6913.86, average training loss: 6448.36, base loss: 16022.77
[INFO 2017-06-30 03:05:27,822 main.py:52] epoch 2389, training loss: 6089.76, average training loss: 6447.71, base loss: 16022.28
[INFO 2017-06-30 03:05:30,987 main.py:52] epoch 2390, training loss: 7136.49, average training loss: 6447.87, base loss: 16023.93
[INFO 2017-06-30 03:05:34,170 main.py:52] epoch 2391, training loss: 6162.98, average training loss: 6447.43, base loss: 16023.79
[INFO 2017-06-30 03:05:37,355 main.py:52] epoch 2392, training loss: 6562.44, average training loss: 6447.16, base loss: 16024.44
[INFO 2017-06-30 03:05:40,525 main.py:52] epoch 2393, training loss: 6195.42, average training loss: 6446.93, base loss: 16024.22
[INFO 2017-06-30 03:05:43,666 main.py:52] epoch 2394, training loss: 5890.57, average training loss: 6446.17, base loss: 16022.99
[INFO 2017-06-30 03:05:46,853 main.py:52] epoch 2395, training loss: 6242.52, average training loss: 6445.30, base loss: 16022.97
[INFO 2017-06-30 03:05:50,032 main.py:52] epoch 2396, training loss: 5554.72, average training loss: 6444.73, base loss: 16020.93
[INFO 2017-06-30 03:05:53,238 main.py:52] epoch 2397, training loss: 6518.67, average training loss: 6445.18, base loss: 16020.22
[INFO 2017-06-30 03:05:56,427 main.py:52] epoch 2398, training loss: 6298.29, average training loss: 6444.75, base loss: 16021.70
[INFO 2017-06-30 03:05:59,562 main.py:52] epoch 2399, training loss: 6331.83, average training loss: 6444.66, base loss: 16022.14
[INFO 2017-06-30 03:05:59,562 main.py:54] epoch 2399, testing
[INFO 2017-06-30 03:06:12,860 main.py:97] average testing loss: 6168.75, base loss: 15751.76
[INFO 2017-06-30 03:06:12,860 main.py:98] improve_loss: 9583.01, improve_percent: 0.61
[INFO 2017-06-30 03:06:12,862 main.py:66] current best improved percent: 0.61
[INFO 2017-06-30 03:06:16,024 main.py:52] epoch 2400, training loss: 6485.88, average training loss: 6444.54, base loss: 16022.97
[INFO 2017-06-30 03:06:19,174 main.py:52] epoch 2401, training loss: 6203.11, average training loss: 6443.93, base loss: 16022.47
[INFO 2017-06-30 03:06:22,330 main.py:52] epoch 2402, training loss: 6041.37, average training loss: 6442.95, base loss: 16020.67
[INFO 2017-06-30 03:06:25,545 main.py:52] epoch 2403, training loss: 6142.46, average training loss: 6442.18, base loss: 16020.41
[INFO 2017-06-30 03:06:28,711 main.py:52] epoch 2404, training loss: 6158.00, average training loss: 6441.43, base loss: 16020.14
[INFO 2017-06-30 03:06:31,916 main.py:52] epoch 2405, training loss: 5830.09, average training loss: 6441.05, base loss: 16020.23
[INFO 2017-06-30 03:06:35,087 main.py:52] epoch 2406, training loss: 5875.39, average training loss: 6440.22, base loss: 16019.85
[INFO 2017-06-30 03:06:38,287 main.py:52] epoch 2407, training loss: 6365.33, average training loss: 6439.87, base loss: 16019.71
[INFO 2017-06-30 03:06:41,509 main.py:52] epoch 2408, training loss: 6785.01, average training loss: 6439.99, base loss: 16021.41
[INFO 2017-06-30 03:06:44,693 main.py:52] epoch 2409, training loss: 5805.90, average training loss: 6438.39, base loss: 16020.42
[INFO 2017-06-30 03:06:47,868 main.py:52] epoch 2410, training loss: 5846.52, average training loss: 6437.42, base loss: 16019.31
[INFO 2017-06-30 03:06:51,065 main.py:52] epoch 2411, training loss: 5938.98, average training loss: 6436.50, base loss: 16019.03
[INFO 2017-06-30 03:06:54,201 main.py:52] epoch 2412, training loss: 5872.79, average training loss: 6435.18, base loss: 16019.43
[INFO 2017-06-30 03:06:57,400 main.py:52] epoch 2413, training loss: 5668.33, average training loss: 6434.11, base loss: 16019.13
[INFO 2017-06-30 03:07:00,553 main.py:52] epoch 2414, training loss: 6079.99, average training loss: 6433.54, base loss: 16019.07
[INFO 2017-06-30 03:07:03,753 main.py:52] epoch 2415, training loss: 6307.77, average training loss: 6433.11, base loss: 16018.56
[INFO 2017-06-30 03:07:06,853 main.py:52] epoch 2416, training loss: 6261.83, average training loss: 6432.81, base loss: 16019.71
[INFO 2017-06-30 03:07:10,008 main.py:52] epoch 2417, training loss: 6319.89, average training loss: 6433.06, base loss: 16019.70
[INFO 2017-06-30 03:07:13,189 main.py:52] epoch 2418, training loss: 6664.71, average training loss: 6432.73, base loss: 16020.03
[INFO 2017-06-30 03:07:16,400 main.py:52] epoch 2419, training loss: 5880.23, average training loss: 6431.99, base loss: 16019.36
[INFO 2017-06-30 03:07:19,567 main.py:52] epoch 2420, training loss: 6370.13, average training loss: 6431.19, base loss: 16018.62
[INFO 2017-06-30 03:07:22,732 main.py:52] epoch 2421, training loss: 5517.75, average training loss: 6429.58, base loss: 16016.98
[INFO 2017-06-30 03:07:25,918 main.py:52] epoch 2422, training loss: 6425.00, average training loss: 6429.70, base loss: 16017.58
[INFO 2017-06-30 03:07:29,059 main.py:52] epoch 2423, training loss: 6155.01, average training loss: 6428.87, base loss: 16017.77
[INFO 2017-06-30 03:07:32,234 main.py:52] epoch 2424, training loss: 6164.42, average training loss: 6428.58, base loss: 16018.80
[INFO 2017-06-30 03:07:35,406 main.py:52] epoch 2425, training loss: 5963.35, average training loss: 6427.44, base loss: 16019.03
[INFO 2017-06-30 03:07:38,559 main.py:52] epoch 2426, training loss: 5941.45, average training loss: 6426.91, base loss: 16018.77
[INFO 2017-06-30 03:07:41,744 main.py:52] epoch 2427, training loss: 6166.67, average training loss: 6426.50, base loss: 16018.62
[INFO 2017-06-30 03:07:44,948 main.py:52] epoch 2428, training loss: 6285.03, average training loss: 6426.36, base loss: 16019.24
[INFO 2017-06-30 03:07:48,102 main.py:52] epoch 2429, training loss: 6155.64, average training loss: 6425.78, base loss: 16019.41
[INFO 2017-06-30 03:07:51,243 main.py:52] epoch 2430, training loss: 6019.57, average training loss: 6425.05, base loss: 16018.80
[INFO 2017-06-30 03:07:54,394 main.py:52] epoch 2431, training loss: 6136.63, average training loss: 6424.99, base loss: 16017.68
[INFO 2017-06-30 03:07:57,606 main.py:52] epoch 2432, training loss: 6282.02, average training loss: 6424.47, base loss: 16016.38
[INFO 2017-06-30 03:08:00,803 main.py:52] epoch 2433, training loss: 5986.79, average training loss: 6423.75, base loss: 16014.96
[INFO 2017-06-30 03:08:03,936 main.py:52] epoch 2434, training loss: 6328.76, average training loss: 6423.16, base loss: 16015.50
[INFO 2017-06-30 03:08:07,115 main.py:52] epoch 2435, training loss: 6566.52, average training loss: 6422.96, base loss: 16015.43
[INFO 2017-06-30 03:08:10,285 main.py:52] epoch 2436, training loss: 6522.85, average training loss: 6423.09, base loss: 16016.45
[INFO 2017-06-30 03:08:13,499 main.py:52] epoch 2437, training loss: 6331.20, average training loss: 6422.69, base loss: 16017.77
[INFO 2017-06-30 03:08:16,641 main.py:52] epoch 2438, training loss: 6118.06, average training loss: 6421.60, base loss: 16017.15
[INFO 2017-06-30 03:08:19,773 main.py:52] epoch 2439, training loss: 6461.07, average training loss: 6421.78, base loss: 16017.64
[INFO 2017-06-30 03:08:22,944 main.py:52] epoch 2440, training loss: 6153.00, average training loss: 6420.84, base loss: 16016.03
[INFO 2017-06-30 03:08:26,111 main.py:52] epoch 2441, training loss: 6074.87, average training loss: 6420.14, base loss: 16015.52
[INFO 2017-06-30 03:08:29,261 main.py:52] epoch 2442, training loss: 6597.43, average training loss: 6419.75, base loss: 16016.51
[INFO 2017-06-30 03:08:32,406 main.py:52] epoch 2443, training loss: 6178.87, average training loss: 6419.35, base loss: 16016.49
[INFO 2017-06-30 03:08:35,582 main.py:52] epoch 2444, training loss: 6498.10, average training loss: 6418.77, base loss: 16016.52
[INFO 2017-06-30 03:08:38,708 main.py:52] epoch 2445, training loss: 5844.44, average training loss: 6417.60, base loss: 16016.10
[INFO 2017-06-30 03:08:41,847 main.py:52] epoch 2446, training loss: 6203.97, average training loss: 6416.48, base loss: 16015.68
[INFO 2017-06-30 03:08:45,014 main.py:52] epoch 2447, training loss: 6031.07, average training loss: 6415.81, base loss: 16015.89
[INFO 2017-06-30 03:08:48,154 main.py:52] epoch 2448, training loss: 6264.54, average training loss: 6415.77, base loss: 16016.08
[INFO 2017-06-30 03:08:51,301 main.py:52] epoch 2449, training loss: 6265.01, average training loss: 6415.72, base loss: 16016.06
[INFO 2017-06-30 03:08:54,460 main.py:52] epoch 2450, training loss: 5968.77, average training loss: 6414.77, base loss: 16014.97
[INFO 2017-06-30 03:08:57,646 main.py:52] epoch 2451, training loss: 6733.45, average training loss: 6415.16, base loss: 16015.54
[INFO 2017-06-30 03:09:00,801 main.py:52] epoch 2452, training loss: 5733.28, average training loss: 6414.51, base loss: 16015.58
[INFO 2017-06-30 03:09:03,977 main.py:52] epoch 2453, training loss: 6364.18, average training loss: 6414.95, base loss: 16016.07
[INFO 2017-06-30 03:09:07,164 main.py:52] epoch 2454, training loss: 6098.30, average training loss: 6414.93, base loss: 16015.12
[INFO 2017-06-30 03:09:10,398 main.py:52] epoch 2455, training loss: 6074.00, average training loss: 6414.69, base loss: 16015.58
[INFO 2017-06-30 03:09:13,510 main.py:52] epoch 2456, training loss: 6097.29, average training loss: 6413.86, base loss: 16016.10
[INFO 2017-06-30 03:09:16,695 main.py:52] epoch 2457, training loss: 5907.67, average training loss: 6413.51, base loss: 16015.58
[INFO 2017-06-30 03:09:19,889 main.py:52] epoch 2458, training loss: 6144.10, average training loss: 6412.79, base loss: 16015.16
[INFO 2017-06-30 03:09:23,078 main.py:52] epoch 2459, training loss: 6468.96, average training loss: 6413.10, base loss: 16015.70
[INFO 2017-06-30 03:09:26,263 main.py:52] epoch 2460, training loss: 5953.80, average training loss: 6412.90, base loss: 16015.11
[INFO 2017-06-30 03:09:29,428 main.py:52] epoch 2461, training loss: 7144.43, average training loss: 6413.88, base loss: 16016.45
[INFO 2017-06-30 03:09:32,595 main.py:52] epoch 2462, training loss: 6289.42, average training loss: 6413.29, base loss: 16017.47
[INFO 2017-06-30 03:09:35,748 main.py:52] epoch 2463, training loss: 5811.71, average training loss: 6412.95, base loss: 16016.36
[INFO 2017-06-30 03:09:38,887 main.py:52] epoch 2464, training loss: 6699.47, average training loss: 6412.73, base loss: 16016.27
[INFO 2017-06-30 03:09:42,034 main.py:52] epoch 2465, training loss: 6340.87, average training loss: 6412.46, base loss: 16016.69
[INFO 2017-06-30 03:09:45,228 main.py:52] epoch 2466, training loss: 5707.77, average training loss: 6411.49, base loss: 16015.56
[INFO 2017-06-30 03:09:48,415 main.py:52] epoch 2467, training loss: 5484.53, average training loss: 6410.68, base loss: 16014.00
[INFO 2017-06-30 03:09:51,622 main.py:52] epoch 2468, training loss: 6452.86, average training loss: 6410.54, base loss: 16014.79
[INFO 2017-06-30 03:09:54,826 main.py:52] epoch 2469, training loss: 6422.54, average training loss: 6410.60, base loss: 16013.93
[INFO 2017-06-30 03:09:57,997 main.py:52] epoch 2470, training loss: 5945.90, average training loss: 6410.04, base loss: 16013.68
[INFO 2017-06-30 03:10:01,139 main.py:52] epoch 2471, training loss: 6135.69, average training loss: 6409.47, base loss: 16014.42
[INFO 2017-06-30 03:10:04,269 main.py:52] epoch 2472, training loss: 6049.08, average training loss: 6409.41, base loss: 16014.30
[INFO 2017-06-30 03:10:07,474 main.py:52] epoch 2473, training loss: 6431.61, average training loss: 6408.79, base loss: 16014.94
[INFO 2017-06-30 03:10:10,694 main.py:52] epoch 2474, training loss: 6489.47, average training loss: 6408.27, base loss: 16014.88
[INFO 2017-06-30 03:10:13,872 main.py:52] epoch 2475, training loss: 5696.43, average training loss: 6407.23, base loss: 16013.37
[INFO 2017-06-30 03:10:17,040 main.py:52] epoch 2476, training loss: 6347.86, average training loss: 6406.83, base loss: 16013.79
[INFO 2017-06-30 03:10:20,212 main.py:52] epoch 2477, training loss: 5944.60, average training loss: 6406.37, base loss: 16012.99
[INFO 2017-06-30 03:10:23,368 main.py:52] epoch 2478, training loss: 5738.77, average training loss: 6404.91, base loss: 16011.71
[INFO 2017-06-30 03:10:26,582 main.py:52] epoch 2479, training loss: 5580.70, average training loss: 6403.80, base loss: 16010.16
[INFO 2017-06-30 03:10:29,740 main.py:52] epoch 2480, training loss: 6146.11, average training loss: 6402.99, base loss: 16009.37
[INFO 2017-06-30 03:10:32,893 main.py:52] epoch 2481, training loss: 6185.91, average training loss: 6402.41, base loss: 16009.57
[INFO 2017-06-30 03:10:36,049 main.py:52] epoch 2482, training loss: 6130.94, average training loss: 6401.52, base loss: 16010.32
[INFO 2017-06-30 03:10:39,233 main.py:52] epoch 2483, training loss: 6542.81, average training loss: 6401.85, base loss: 16011.10
[INFO 2017-06-30 03:10:42,412 main.py:52] epoch 2484, training loss: 6333.14, average training loss: 6402.00, base loss: 16011.83
[INFO 2017-06-30 03:10:45,624 main.py:52] epoch 2485, training loss: 6472.38, average training loss: 6401.79, base loss: 16012.39
[INFO 2017-06-30 03:10:48,783 main.py:52] epoch 2486, training loss: 6272.50, average training loss: 6400.95, base loss: 16012.28
[INFO 2017-06-30 03:10:51,951 main.py:52] epoch 2487, training loss: 6482.68, average training loss: 6400.91, base loss: 16013.37
[INFO 2017-06-30 03:10:55,113 main.py:52] epoch 2488, training loss: 5924.31, average training loss: 6400.46, base loss: 16011.68
[INFO 2017-06-30 03:10:58,298 main.py:52] epoch 2489, training loss: 6313.64, average training loss: 6400.13, base loss: 16011.30
[INFO 2017-06-30 03:11:01,413 main.py:52] epoch 2490, training loss: 6050.21, average training loss: 6399.32, base loss: 16010.91
[INFO 2017-06-30 03:11:04,595 main.py:52] epoch 2491, training loss: 6323.52, average training loss: 6398.74, base loss: 16011.37
[INFO 2017-06-30 03:11:07,768 main.py:52] epoch 2492, training loss: 5813.16, average training loss: 6398.46, base loss: 16010.46
[INFO 2017-06-30 03:11:10,932 main.py:52] epoch 2493, training loss: 5857.10, average training loss: 6397.70, base loss: 16010.04
[INFO 2017-06-30 03:11:14,116 main.py:52] epoch 2494, training loss: 6180.36, average training loss: 6397.82, base loss: 16010.78
[INFO 2017-06-30 03:11:17,288 main.py:52] epoch 2495, training loss: 6555.49, average training loss: 6397.40, base loss: 16011.93
[INFO 2017-06-30 03:11:20,527 main.py:52] epoch 2496, training loss: 6260.79, average training loss: 6397.02, base loss: 16011.94
[INFO 2017-06-30 03:11:23,700 main.py:52] epoch 2497, training loss: 6209.95, average training loss: 6396.47, base loss: 16011.44
[INFO 2017-06-30 03:11:26,870 main.py:52] epoch 2498, training loss: 6115.72, average training loss: 6396.00, base loss: 16011.12
[INFO 2017-06-30 03:11:30,024 main.py:52] epoch 2499, training loss: 6581.95, average training loss: 6396.11, base loss: 16012.84
[INFO 2017-06-30 03:11:30,025 main.py:54] epoch 2499, testing
[INFO 2017-06-30 03:11:43,326 main.py:97] average testing loss: 6084.28, base loss: 15000.94
[INFO 2017-06-30 03:11:43,326 main.py:98] improve_loss: 8916.67, improve_percent: 0.59
[INFO 2017-06-30 03:11:43,328 main.py:66] current best improved percent: 0.61
[INFO 2017-06-30 03:11:46,492 main.py:52] epoch 2500, training loss: 6274.81, average training loss: 6395.65, base loss: 16013.30
[INFO 2017-06-30 03:11:49,624 main.py:52] epoch 2501, training loss: 5822.67, average training loss: 6394.86, base loss: 16012.93
[INFO 2017-06-30 03:11:52,751 main.py:52] epoch 2502, training loss: 6624.08, average training loss: 6394.45, base loss: 16013.27
[INFO 2017-06-30 03:11:55,908 main.py:52] epoch 2503, training loss: 6250.30, average training loss: 6393.52, base loss: 16012.46
[INFO 2017-06-30 03:11:59,099 main.py:52] epoch 2504, training loss: 7055.10, average training loss: 6394.49, base loss: 16014.31
[INFO 2017-06-30 03:12:02,255 main.py:52] epoch 2505, training loss: 6136.76, average training loss: 6393.65, base loss: 16014.33
[INFO 2017-06-30 03:12:05,416 main.py:52] epoch 2506, training loss: 6463.33, average training loss: 6393.83, base loss: 16013.39
[INFO 2017-06-30 03:12:08,601 main.py:52] epoch 2507, training loss: 5762.87, average training loss: 6393.16, base loss: 16012.55
[INFO 2017-06-30 03:12:11,818 main.py:52] epoch 2508, training loss: 6552.45, average training loss: 6393.06, base loss: 16012.75
[INFO 2017-06-30 03:12:14,960 main.py:52] epoch 2509, training loss: 6036.13, average training loss: 6392.86, base loss: 16011.59
[INFO 2017-06-30 03:12:18,159 main.py:52] epoch 2510, training loss: 6043.22, average training loss: 6391.82, base loss: 16011.68
[INFO 2017-06-30 03:12:21,346 main.py:52] epoch 2511, training loss: 6085.79, average training loss: 6391.39, base loss: 16012.42
[INFO 2017-06-30 03:12:24,511 main.py:52] epoch 2512, training loss: 6221.69, average training loss: 6391.24, base loss: 16012.45
[INFO 2017-06-30 03:12:27,665 main.py:52] epoch 2513, training loss: 6570.55, average training loss: 6391.28, base loss: 16012.59
[INFO 2017-06-30 03:12:30,868 main.py:52] epoch 2514, training loss: 6245.45, average training loss: 6390.60, base loss: 16012.44
[INFO 2017-06-30 03:12:34,026 main.py:52] epoch 2515, training loss: 6667.28, average training loss: 6390.72, base loss: 16014.66
[INFO 2017-06-30 03:12:37,181 main.py:52] epoch 2516, training loss: 6470.84, average training loss: 6390.71, base loss: 16015.17
[INFO 2017-06-30 03:12:40,372 main.py:52] epoch 2517, training loss: 6100.59, average training loss: 6390.32, base loss: 16015.28
[INFO 2017-06-30 03:12:43,572 main.py:52] epoch 2518, training loss: 6409.88, average training loss: 6390.22, base loss: 16015.08
[INFO 2017-06-30 03:12:46,718 main.py:52] epoch 2519, training loss: 6307.51, average training loss: 6390.46, base loss: 16014.21
[INFO 2017-06-30 03:12:49,937 main.py:52] epoch 2520, training loss: 6220.21, average training loss: 6389.92, base loss: 16014.48
[INFO 2017-06-30 03:12:53,086 main.py:52] epoch 2521, training loss: 6064.99, average training loss: 6389.50, base loss: 16014.02
[INFO 2017-06-30 03:12:56,268 main.py:52] epoch 2522, training loss: 6097.75, average training loss: 6388.95, base loss: 16013.95
[INFO 2017-06-30 03:12:59,442 main.py:52] epoch 2523, training loss: 6038.96, average training loss: 6388.75, base loss: 16013.98
[INFO 2017-06-30 03:13:02,590 main.py:52] epoch 2524, training loss: 7008.49, average training loss: 6389.44, base loss: 16015.77
[INFO 2017-06-30 03:13:05,739 main.py:52] epoch 2525, training loss: 5824.52, average training loss: 6389.14, base loss: 16015.66
[INFO 2017-06-30 03:13:08,939 main.py:52] epoch 2526, training loss: 6538.80, average training loss: 6389.12, base loss: 16017.00
[INFO 2017-06-30 03:13:12,118 main.py:52] epoch 2527, training loss: 5815.57, average training loss: 6388.03, base loss: 16017.07
[INFO 2017-06-30 03:13:15,260 main.py:52] epoch 2528, training loss: 5973.39, average training loss: 6387.54, base loss: 16016.42
[INFO 2017-06-30 03:13:18,441 main.py:52] epoch 2529, training loss: 6440.99, average training loss: 6387.35, base loss: 16016.20
[INFO 2017-06-30 03:13:21,610 main.py:52] epoch 2530, training loss: 5867.82, average training loss: 6386.76, base loss: 16014.76
[INFO 2017-06-30 03:13:24,727 main.py:52] epoch 2531, training loss: 6228.90, average training loss: 6386.49, base loss: 16014.33
[INFO 2017-06-30 03:13:27,913 main.py:52] epoch 2532, training loss: 6072.02, average training loss: 6386.23, base loss: 16013.39
[INFO 2017-06-30 03:13:31,096 main.py:52] epoch 2533, training loss: 6076.96, average training loss: 6385.53, base loss: 16012.30
[INFO 2017-06-30 03:13:34,291 main.py:52] epoch 2534, training loss: 6459.68, average training loss: 6386.00, base loss: 16012.98
[INFO 2017-06-30 03:13:37,486 main.py:52] epoch 2535, training loss: 6905.67, average training loss: 6385.62, base loss: 16014.58
[INFO 2017-06-30 03:13:40,655 main.py:52] epoch 2536, training loss: 6458.93, average training loss: 6385.41, base loss: 16015.76
[INFO 2017-06-30 03:13:43,822 main.py:52] epoch 2537, training loss: 5556.10, average training loss: 6384.41, base loss: 16014.43
[INFO 2017-06-30 03:13:46,967 main.py:52] epoch 2538, training loss: 6229.57, average training loss: 6383.98, base loss: 16013.98
[INFO 2017-06-30 03:13:50,152 main.py:52] epoch 2539, training loss: 6457.79, average training loss: 6384.15, base loss: 16015.50
[INFO 2017-06-30 03:13:53,360 main.py:52] epoch 2540, training loss: 6556.16, average training loss: 6384.29, base loss: 16016.35
[INFO 2017-06-30 03:13:56,529 main.py:52] epoch 2541, training loss: 6134.20, average training loss: 6383.49, base loss: 16016.92
[INFO 2017-06-30 03:13:59,709 main.py:52] epoch 2542, training loss: 6103.93, average training loss: 6383.20, base loss: 16016.35
[INFO 2017-06-30 03:14:02,883 main.py:52] epoch 2543, training loss: 6147.73, average training loss: 6382.25, base loss: 16016.52
[INFO 2017-06-30 03:14:06,069 main.py:52] epoch 2544, training loss: 6362.44, average training loss: 6381.71, base loss: 16016.74
[INFO 2017-06-30 03:14:09,251 main.py:52] epoch 2545, training loss: 6291.71, average training loss: 6381.02, base loss: 16017.32
[INFO 2017-06-30 03:14:12,415 main.py:52] epoch 2546, training loss: 6378.88, average training loss: 6380.86, base loss: 16018.37
[INFO 2017-06-30 03:14:15,535 main.py:52] epoch 2547, training loss: 5929.51, average training loss: 6379.88, base loss: 16018.63
[INFO 2017-06-30 03:14:18,719 main.py:52] epoch 2548, training loss: 6694.32, average training loss: 6380.01, base loss: 16019.38
[INFO 2017-06-30 03:14:21,913 main.py:52] epoch 2549, training loss: 5979.72, average training loss: 6379.71, base loss: 16019.01
[INFO 2017-06-30 03:14:25,133 main.py:52] epoch 2550, training loss: 6221.28, average training loss: 6379.21, base loss: 16019.15
[INFO 2017-06-30 03:14:28,270 main.py:52] epoch 2551, training loss: 5632.51, average training loss: 6378.30, base loss: 16018.20
[INFO 2017-06-30 03:14:31,384 main.py:52] epoch 2552, training loss: 6199.27, average training loss: 6378.42, base loss: 16018.61
[INFO 2017-06-30 03:14:34,523 main.py:52] epoch 2553, training loss: 5853.65, average training loss: 6378.15, base loss: 16018.62
[INFO 2017-06-30 03:14:37,698 main.py:52] epoch 2554, training loss: 6019.89, average training loss: 6377.94, base loss: 16017.63
[INFO 2017-06-30 03:14:40,843 main.py:52] epoch 2555, training loss: 6287.13, average training loss: 6377.30, base loss: 16017.65
[INFO 2017-06-30 03:14:43,986 main.py:52] epoch 2556, training loss: 6128.91, average training loss: 6376.67, base loss: 16016.18
[INFO 2017-06-30 03:14:47,147 main.py:52] epoch 2557, training loss: 6815.06, average training loss: 6377.10, base loss: 16017.58
[INFO 2017-06-30 03:14:50,273 main.py:52] epoch 2558, training loss: 6255.97, average training loss: 6376.88, base loss: 16018.04
[INFO 2017-06-30 03:14:53,429 main.py:52] epoch 2559, training loss: 6406.75, average training loss: 6376.73, base loss: 16018.96
[INFO 2017-06-30 03:14:56,615 main.py:52] epoch 2560, training loss: 6229.50, average training loss: 6376.20, base loss: 16019.71
[INFO 2017-06-30 03:14:59,795 main.py:52] epoch 2561, training loss: 6289.00, average training loss: 6376.18, base loss: 16021.15
[INFO 2017-06-30 03:15:02,964 main.py:52] epoch 2562, training loss: 6403.36, average training loss: 6375.95, base loss: 16020.84
[INFO 2017-06-30 03:15:06,116 main.py:52] epoch 2563, training loss: 5939.24, average training loss: 6375.33, base loss: 16019.52
[INFO 2017-06-30 03:15:09,243 main.py:52] epoch 2564, training loss: 6093.50, average training loss: 6375.08, base loss: 16019.48
[INFO 2017-06-30 03:15:12,448 main.py:52] epoch 2565, training loss: 6256.02, average training loss: 6374.17, base loss: 16019.61
[INFO 2017-06-30 03:15:15,622 main.py:52] epoch 2566, training loss: 6179.51, average training loss: 6373.30, base loss: 16018.77
[INFO 2017-06-30 03:15:18,761 main.py:52] epoch 2567, training loss: 6182.69, average training loss: 6373.05, base loss: 16017.95
[INFO 2017-06-30 03:15:21,945 main.py:52] epoch 2568, training loss: 6421.73, average training loss: 6373.50, base loss: 16018.92
[INFO 2017-06-30 03:15:25,137 main.py:52] epoch 2569, training loss: 6359.94, average training loss: 6373.94, base loss: 16019.09
[INFO 2017-06-30 03:15:28,387 main.py:52] epoch 2570, training loss: 5993.41, average training loss: 6373.11, base loss: 16018.77
[INFO 2017-06-30 03:15:31,586 main.py:52] epoch 2571, training loss: 6414.71, average training loss: 6373.01, base loss: 16019.12
[INFO 2017-06-30 03:15:34,741 main.py:52] epoch 2572, training loss: 5528.65, average training loss: 6371.63, base loss: 16018.62
[INFO 2017-06-30 03:15:37,874 main.py:52] epoch 2573, training loss: 6140.53, average training loss: 6371.28, base loss: 16018.96
[INFO 2017-06-30 03:15:41,027 main.py:52] epoch 2574, training loss: 6215.91, average training loss: 6371.06, base loss: 16018.15
[INFO 2017-06-30 03:15:44,198 main.py:52] epoch 2575, training loss: 5771.25, average training loss: 6370.16, base loss: 16017.78
[INFO 2017-06-30 03:15:47,383 main.py:52] epoch 2576, training loss: 6780.37, average training loss: 6370.63, base loss: 16018.13
[INFO 2017-06-30 03:15:50,540 main.py:52] epoch 2577, training loss: 6452.82, average training loss: 6369.96, base loss: 16017.97
[INFO 2017-06-30 03:15:53,714 main.py:52] epoch 2578, training loss: 5917.96, average training loss: 6369.60, base loss: 16017.57
[INFO 2017-06-30 03:15:56,872 main.py:52] epoch 2579, training loss: 6359.29, average training loss: 6369.23, base loss: 16018.77
[INFO 2017-06-30 03:16:00,073 main.py:52] epoch 2580, training loss: 6203.68, average training loss: 6368.51, base loss: 16018.12
[INFO 2017-06-30 03:16:03,287 main.py:52] epoch 2581, training loss: 6180.81, average training loss: 6367.95, base loss: 16017.61
[INFO 2017-06-30 03:16:06,443 main.py:52] epoch 2582, training loss: 6409.80, average training loss: 6367.92, base loss: 16018.09
[INFO 2017-06-30 03:16:09,653 main.py:52] epoch 2583, training loss: 6285.16, average training loss: 6367.49, base loss: 16018.11
[INFO 2017-06-30 03:16:12,810 main.py:52] epoch 2584, training loss: 6081.65, average training loss: 6367.07, base loss: 16017.44
[INFO 2017-06-30 03:16:15,947 main.py:52] epoch 2585, training loss: 6281.28, average training loss: 6367.06, base loss: 16016.96
[INFO 2017-06-30 03:16:19,139 main.py:52] epoch 2586, training loss: 6087.74, average training loss: 6366.08, base loss: 16016.81
[INFO 2017-06-30 03:16:22,308 main.py:52] epoch 2587, training loss: 6358.71, average training loss: 6366.37, base loss: 16017.10
[INFO 2017-06-30 03:16:25,515 main.py:52] epoch 2588, training loss: 6064.95, average training loss: 6365.58, base loss: 16017.20
[INFO 2017-06-30 03:16:28,693 main.py:52] epoch 2589, training loss: 6159.30, average training loss: 6364.75, base loss: 16016.87
[INFO 2017-06-30 03:16:31,881 main.py:52] epoch 2590, training loss: 6780.02, average training loss: 6364.25, base loss: 16017.95
[INFO 2017-06-30 03:16:35,050 main.py:52] epoch 2591, training loss: 6181.51, average training loss: 6364.03, base loss: 16017.83
[INFO 2017-06-30 03:16:38,238 main.py:52] epoch 2592, training loss: 6253.37, average training loss: 6362.87, base loss: 16017.86
[INFO 2017-06-30 03:16:41,405 main.py:52] epoch 2593, training loss: 6142.86, average training loss: 6362.91, base loss: 16017.81
[INFO 2017-06-30 03:16:44,574 main.py:52] epoch 2594, training loss: 5827.04, average training loss: 6362.08, base loss: 16017.61
[INFO 2017-06-30 03:16:47,793 main.py:52] epoch 2595, training loss: 5971.56, average training loss: 6362.03, base loss: 16017.37
[INFO 2017-06-30 03:16:51,006 main.py:52] epoch 2596, training loss: 5881.61, average training loss: 6361.05, base loss: 16017.24
[INFO 2017-06-30 03:16:54,175 main.py:52] epoch 2597, training loss: 5918.26, average training loss: 6360.35, base loss: 16017.04
[INFO 2017-06-30 03:16:57,360 main.py:52] epoch 2598, training loss: 6111.07, average training loss: 6359.71, base loss: 16017.68
[INFO 2017-06-30 03:17:00,484 main.py:52] epoch 2599, training loss: 6562.06, average training loss: 6359.76, base loss: 16018.29
[INFO 2017-06-30 03:17:00,484 main.py:54] epoch 2599, testing
[INFO 2017-06-30 03:17:13,772 main.py:97] average testing loss: 6377.64, base loss: 16560.02
[INFO 2017-06-30 03:17:13,772 main.py:98] improve_loss: 10182.38, improve_percent: 0.61
[INFO 2017-06-30 03:17:13,774 main.py:62] model save to ./model/final.pth
[INFO 2017-06-30 03:17:13,808 main.py:66] current best improved percent: 0.61
[INFO 2017-06-30 03:17:16,974 main.py:52] epoch 2600, training loss: 6169.79, average training loss: 6359.46, base loss: 16017.67
[INFO 2017-06-30 03:17:20,148 main.py:52] epoch 2601, training loss: 6490.55, average training loss: 6359.16, base loss: 16018.62
[INFO 2017-06-30 03:17:23,287 main.py:52] epoch 2602, training loss: 6506.40, average training loss: 6359.69, base loss: 16019.88
[INFO 2017-06-30 03:17:26,476 main.py:52] epoch 2603, training loss: 6017.12, average training loss: 6358.66, base loss: 16019.81
[INFO 2017-06-30 03:17:29,609 main.py:52] epoch 2604, training loss: 6604.33, average training loss: 6358.72, base loss: 16020.30
[INFO 2017-06-30 03:17:32,777 main.py:52] epoch 2605, training loss: 5994.07, average training loss: 6357.73, base loss: 16020.67
[INFO 2017-06-30 03:17:35,955 main.py:52] epoch 2606, training loss: 6132.25, average training loss: 6356.78, base loss: 16019.69
[INFO 2017-06-30 03:17:39,142 main.py:52] epoch 2607, training loss: 6015.99, average training loss: 6356.09, base loss: 16018.64
[INFO 2017-06-30 03:17:42,308 main.py:52] epoch 2608, training loss: 5700.59, average training loss: 6355.25, base loss: 16016.81
[INFO 2017-06-30 03:17:45,474 main.py:52] epoch 2609, training loss: 6090.51, average training loss: 6353.99, base loss: 16016.75
[INFO 2017-06-30 03:17:48,652 main.py:52] epoch 2610, training loss: 5997.15, average training loss: 6353.38, base loss: 16015.97
[INFO 2017-06-30 03:17:51,807 main.py:52] epoch 2611, training loss: 6171.00, average training loss: 6352.80, base loss: 16016.05
[INFO 2017-06-30 03:17:54,971 main.py:52] epoch 2612, training loss: 5673.96, average training loss: 6352.28, base loss: 16015.53
[INFO 2017-06-30 03:17:58,130 main.py:52] epoch 2613, training loss: 6315.71, average training loss: 6352.06, base loss: 16016.08
[INFO 2017-06-30 03:18:01,267 main.py:52] epoch 2614, training loss: 5554.44, average training loss: 6351.13, base loss: 16015.72
[INFO 2017-06-30 03:18:04,441 main.py:52] epoch 2615, training loss: 6531.00, average training loss: 6351.09, base loss: 16016.83
[INFO 2017-06-30 03:18:07,605 main.py:52] epoch 2616, training loss: 6192.09, average training loss: 6350.73, base loss: 16017.22
[INFO 2017-06-30 03:18:10,771 main.py:52] epoch 2617, training loss: 5973.40, average training loss: 6349.83, base loss: 16016.87
[INFO 2017-06-30 03:18:13,913 main.py:52] epoch 2618, training loss: 5757.06, average training loss: 6349.09, base loss: 16016.66
[INFO 2017-06-30 03:18:17,119 main.py:52] epoch 2619, training loss: 5914.66, average training loss: 6347.98, base loss: 16016.20
[INFO 2017-06-30 03:18:20,302 main.py:52] epoch 2620, training loss: 6158.29, average training loss: 6347.37, base loss: 16016.53
[INFO 2017-06-30 03:18:23,451 main.py:52] epoch 2621, training loss: 6466.02, average training loss: 6347.12, base loss: 16017.54
[INFO 2017-06-30 03:18:26,628 main.py:52] epoch 2622, training loss: 6162.26, average training loss: 6346.18, base loss: 16017.23
[INFO 2017-06-30 03:18:29,836 main.py:52] epoch 2623, training loss: 5768.27, average training loss: 6345.03, base loss: 16016.08
[INFO 2017-06-30 03:18:33,076 main.py:52] epoch 2624, training loss: 6159.68, average training loss: 6344.41, base loss: 16016.17
[INFO 2017-06-30 03:18:36,295 main.py:52] epoch 2625, training loss: 6340.90, average training loss: 6344.52, base loss: 16017.92
[INFO 2017-06-30 03:18:39,440 main.py:52] epoch 2626, training loss: 6083.11, average training loss: 6344.64, base loss: 16018.00
[INFO 2017-06-30 03:18:42,635 main.py:52] epoch 2627, training loss: 6304.46, average training loss: 6343.66, base loss: 16018.58
[INFO 2017-06-30 03:18:45,794 main.py:52] epoch 2628, training loss: 6501.39, average training loss: 6344.18, base loss: 16019.82
[INFO 2017-06-30 03:18:48,946 main.py:52] epoch 2629, training loss: 5814.77, average training loss: 6343.04, base loss: 16019.29
[INFO 2017-06-30 03:18:52,130 main.py:52] epoch 2630, training loss: 6311.60, average training loss: 6341.93, base loss: 16020.57
[INFO 2017-06-30 03:18:55,296 main.py:52] epoch 2631, training loss: 5877.76, average training loss: 6341.41, base loss: 16020.26
[INFO 2017-06-30 03:18:58,445 main.py:52] epoch 2632, training loss: 6542.45, average training loss: 6341.44, base loss: 16020.33
[INFO 2017-06-30 03:19:01,593 main.py:52] epoch 2633, training loss: 5652.22, average training loss: 6340.34, base loss: 16019.12
[INFO 2017-06-30 03:19:04,770 main.py:52] epoch 2634, training loss: 6178.89, average training loss: 6339.94, base loss: 16019.78
[INFO 2017-06-30 03:19:07,951 main.py:52] epoch 2635, training loss: 6295.54, average training loss: 6339.99, base loss: 16019.93
[INFO 2017-06-30 03:19:11,079 main.py:52] epoch 2636, training loss: 6632.97, average training loss: 6339.76, base loss: 16019.90
[INFO 2017-06-30 03:19:14,261 main.py:52] epoch 2637, training loss: 6207.56, average training loss: 6339.66, base loss: 16020.88
[INFO 2017-06-30 03:19:17,423 main.py:52] epoch 2638, training loss: 6074.32, average training loss: 6339.52, base loss: 16020.56
[INFO 2017-06-30 03:19:20,648 main.py:52] epoch 2639, training loss: 6466.91, average training loss: 6339.24, base loss: 16021.49
[INFO 2017-06-30 03:19:23,808 main.py:52] epoch 2640, training loss: 5974.22, average training loss: 6339.08, base loss: 16020.47
[INFO 2017-06-30 03:19:26,995 main.py:52] epoch 2641, training loss: 5704.17, average training loss: 6338.57, base loss: 16019.07
[INFO 2017-06-30 03:19:30,241 main.py:52] epoch 2642, training loss: 5816.95, average training loss: 6338.00, base loss: 16016.99
[INFO 2017-06-30 03:19:33,391 main.py:52] epoch 2643, training loss: 6291.50, average training loss: 6338.17, base loss: 16015.91
[INFO 2017-06-30 03:19:36,555 main.py:52] epoch 2644, training loss: 6537.98, average training loss: 6338.14, base loss: 16016.21
[INFO 2017-06-30 03:19:39,778 main.py:52] epoch 2645, training loss: 5868.90, average training loss: 6338.16, base loss: 16015.16
[INFO 2017-06-30 03:19:42,939 main.py:52] epoch 2646, training loss: 7020.73, average training loss: 6339.27, base loss: 16016.14
[INFO 2017-06-30 03:19:46,099 main.py:52] epoch 2647, training loss: 6398.16, average training loss: 6339.37, base loss: 16017.34
[INFO 2017-06-30 03:19:49,275 main.py:52] epoch 2648, training loss: 5629.96, average training loss: 6338.45, base loss: 16016.31
[INFO 2017-06-30 03:19:52,420 main.py:52] epoch 2649, training loss: 6132.16, average training loss: 6338.03, base loss: 16016.40
[INFO 2017-06-30 03:19:55,597 main.py:52] epoch 2650, training loss: 6702.95, average training loss: 6338.21, base loss: 16016.21
[INFO 2017-06-30 03:19:58,788 main.py:52] epoch 2651, training loss: 6127.06, average training loss: 6338.01, base loss: 16016.53
[INFO 2017-06-30 03:20:01,950 main.py:52] epoch 2652, training loss: 6236.37, average training loss: 6337.92, base loss: 16017.08
[INFO 2017-06-30 03:20:05,099 main.py:52] epoch 2653, training loss: 6493.23, average training loss: 6338.21, base loss: 16018.03
[INFO 2017-06-30 03:20:08,262 main.py:52] epoch 2654, training loss: 6056.90, average training loss: 6338.05, base loss: 16017.32
[INFO 2017-06-30 03:20:11,407 main.py:52] epoch 2655, training loss: 6057.56, average training loss: 6337.48, base loss: 16017.28
[INFO 2017-06-30 03:20:14,585 main.py:52] epoch 2656, training loss: 6134.92, average training loss: 6337.22, base loss: 16017.98
[INFO 2017-06-30 03:20:17,743 main.py:52] epoch 2657, training loss: 5894.61, average training loss: 6336.86, base loss: 16018.08
[INFO 2017-06-30 03:20:20,895 main.py:52] epoch 2658, training loss: 7068.99, average training loss: 6337.61, base loss: 16019.82
[INFO 2017-06-30 03:20:24,074 main.py:52] epoch 2659, training loss: 6151.07, average training loss: 6337.87, base loss: 16018.85
[INFO 2017-06-30 03:20:27,293 main.py:52] epoch 2660, training loss: 6442.85, average training loss: 6337.27, base loss: 16019.14
[INFO 2017-06-30 03:20:30,486 main.py:52] epoch 2661, training loss: 6484.08, average training loss: 6337.36, base loss: 16019.08
[INFO 2017-06-30 03:20:33,637 main.py:52] epoch 2662, training loss: 6180.37, average training loss: 6336.63, base loss: 16019.23
[INFO 2017-06-30 03:20:36,772 main.py:52] epoch 2663, training loss: 5969.64, average training loss: 6335.87, base loss: 16018.37
[INFO 2017-06-30 03:20:39,983 main.py:52] epoch 2664, training loss: 5980.99, average training loss: 6334.89, base loss: 16017.02
[INFO 2017-06-30 03:20:43,122 main.py:52] epoch 2665, training loss: 6742.57, average training loss: 6334.59, base loss: 16018.18
[INFO 2017-06-30 03:20:46,308 main.py:52] epoch 2666, training loss: 6060.81, average training loss: 6333.96, base loss: 16017.97
[INFO 2017-06-30 03:20:49,484 main.py:52] epoch 2667, training loss: 5527.11, average training loss: 6332.92, base loss: 16017.94
[INFO 2017-06-30 03:20:52,639 main.py:52] epoch 2668, training loss: 6336.99, average training loss: 6332.79, base loss: 16018.21
[INFO 2017-06-30 03:20:55,796 main.py:52] epoch 2669, training loss: 5568.43, average training loss: 6332.11, base loss: 16016.54
[INFO 2017-06-30 03:20:58,963 main.py:52] epoch 2670, training loss: 6055.39, average training loss: 6331.39, base loss: 16015.92
[INFO 2017-06-30 03:21:02,124 main.py:52] epoch 2671, training loss: 6213.12, average training loss: 6330.71, base loss: 16015.82
[INFO 2017-06-30 03:21:05,310 main.py:52] epoch 2672, training loss: 6060.64, average training loss: 6330.53, base loss: 16015.98
[INFO 2017-06-30 03:21:08,458 main.py:52] epoch 2673, training loss: 6407.26, average training loss: 6330.52, base loss: 16016.47
[INFO 2017-06-30 03:21:11,593 main.py:52] epoch 2674, training loss: 6111.23, average training loss: 6330.28, base loss: 16016.39
[INFO 2017-06-30 03:21:14,747 main.py:52] epoch 2675, training loss: 6036.06, average training loss: 6329.35, base loss: 16016.67
[INFO 2017-06-30 03:21:17,920 main.py:52] epoch 2676, training loss: 5758.02, average training loss: 6328.64, base loss: 16016.65
[INFO 2017-06-30 03:21:21,112 main.py:52] epoch 2677, training loss: 5831.03, average training loss: 6328.66, base loss: 16016.28
[INFO 2017-06-30 03:21:24,331 main.py:52] epoch 2678, training loss: 6025.55, average training loss: 6327.52, base loss: 16015.40
[INFO 2017-06-30 03:21:27,481 main.py:52] epoch 2679, training loss: 6065.38, average training loss: 6326.89, base loss: 16014.79
[INFO 2017-06-30 03:21:30,640 main.py:52] epoch 2680, training loss: 6352.54, average training loss: 6326.61, base loss: 16014.70
[INFO 2017-06-30 03:21:33,810 main.py:52] epoch 2681, training loss: 6031.85, average training loss: 6326.01, base loss: 16014.62
[INFO 2017-06-30 03:21:36,960 main.py:52] epoch 2682, training loss: 6165.81, average training loss: 6325.28, base loss: 16015.23
[INFO 2017-06-30 03:21:40,156 main.py:52] epoch 2683, training loss: 6204.99, average training loss: 6324.95, base loss: 16014.92
[INFO 2017-06-30 03:21:43,321 main.py:52] epoch 2684, training loss: 6232.21, average training loss: 6324.37, base loss: 16015.00
[INFO 2017-06-30 03:21:46,481 main.py:52] epoch 2685, training loss: 6600.99, average training loss: 6324.13, base loss: 16015.83
[INFO 2017-06-30 03:21:49,701 main.py:52] epoch 2686, training loss: 5997.17, average training loss: 6324.04, base loss: 16015.41
[INFO 2017-06-30 03:21:52,882 main.py:52] epoch 2687, training loss: 6375.69, average training loss: 6324.01, base loss: 16015.98
[INFO 2017-06-30 03:21:56,136 main.py:52] epoch 2688, training loss: 5608.97, average training loss: 6322.89, base loss: 16015.05
[INFO 2017-06-30 03:21:59,269 main.py:52] epoch 2689, training loss: 6117.64, average training loss: 6322.35, base loss: 16014.25
[INFO 2017-06-30 03:22:02,498 main.py:52] epoch 2690, training loss: 5474.19, average training loss: 6321.07, base loss: 16013.37
[INFO 2017-06-30 03:22:05,646 main.py:52] epoch 2691, training loss: 5987.19, average training loss: 6320.75, base loss: 16013.42
[INFO 2017-06-30 03:22:08,786 main.py:52] epoch 2692, training loss: 5652.38, average training loss: 6319.36, base loss: 16012.42
[INFO 2017-06-30 03:22:11,951 main.py:52] epoch 2693, training loss: 6229.49, average training loss: 6319.35, base loss: 16012.16
[INFO 2017-06-30 03:22:15,085 main.py:52] epoch 2694, training loss: 6407.82, average training loss: 6319.13, base loss: 16012.41
[INFO 2017-06-30 03:22:18,252 main.py:52] epoch 2695, training loss: 6202.26, average training loss: 6318.91, base loss: 16012.26
[INFO 2017-06-30 03:22:21,366 main.py:52] epoch 2696, training loss: 5944.00, average training loss: 6318.76, base loss: 16012.41
[INFO 2017-06-30 03:22:24,524 main.py:52] epoch 2697, training loss: 5903.50, average training loss: 6318.41, base loss: 16012.12
[INFO 2017-06-30 03:22:27,662 main.py:52] epoch 2698, training loss: 6036.10, average training loss: 6317.88, base loss: 16011.83
[INFO 2017-06-30 03:22:30,848 main.py:52] epoch 2699, training loss: 6278.60, average training loss: 6318.13, base loss: 16011.97
[INFO 2017-06-30 03:22:30,848 main.py:54] epoch 2699, testing
[INFO 2017-06-30 03:22:44,194 main.py:97] average testing loss: 6083.46, base loss: 15411.18
[INFO 2017-06-30 03:22:44,194 main.py:98] improve_loss: 9327.73, improve_percent: 0.61
[INFO 2017-06-30 03:22:44,196 main.py:66] current best improved percent: 0.61
[INFO 2017-06-30 03:22:47,394 main.py:52] epoch 2700, training loss: 6535.87, average training loss: 6317.85, base loss: 16012.13
[INFO 2017-06-30 03:22:50,530 main.py:52] epoch 2701, training loss: 6324.61, average training loss: 6317.87, base loss: 16012.23
[INFO 2017-06-30 03:22:53,705 main.py:52] epoch 2702, training loss: 6342.71, average training loss: 6317.63, base loss: 16012.88
[INFO 2017-06-30 03:22:56,862 main.py:52] epoch 2703, training loss: 6081.43, average training loss: 6316.96, base loss: 16012.62
[INFO 2017-06-30 03:23:00,034 main.py:52] epoch 2704, training loss: 5771.83, average training loss: 6316.28, base loss: 16011.31
[INFO 2017-06-30 03:23:03,169 main.py:52] epoch 2705, training loss: 5939.95, average training loss: 6316.34, base loss: 16010.45
[INFO 2017-06-30 03:23:06,329 main.py:52] epoch 2706, training loss: 5859.46, average training loss: 6315.74, base loss: 16009.90
[INFO 2017-06-30 03:23:09,528 main.py:52] epoch 2707, training loss: 5707.06, average training loss: 6315.33, base loss: 16008.99
[INFO 2017-06-30 03:23:12,719 main.py:52] epoch 2708, training loss: 5971.48, average training loss: 6314.09, base loss: 16008.86
[INFO 2017-06-30 03:23:15,907 main.py:52] epoch 2709, training loss: 5987.50, average training loss: 6312.82, base loss: 16008.30
[INFO 2017-06-30 03:23:19,077 main.py:52] epoch 2710, training loss: 6391.47, average training loss: 6312.92, base loss: 16009.06
[INFO 2017-06-30 03:23:22,266 main.py:52] epoch 2711, training loss: 5936.35, average training loss: 6312.14, base loss: 16008.53
[INFO 2017-06-30 03:23:25,467 main.py:52] epoch 2712, training loss: 6317.84, average training loss: 6312.39, base loss: 16008.92
[INFO 2017-06-30 03:23:28,666 main.py:52] epoch 2713, training loss: 5879.66, average training loss: 6311.52, base loss: 16008.20
[INFO 2017-06-30 03:23:31,891 main.py:52] epoch 2714, training loss: 6389.58, average training loss: 6310.91, base loss: 16009.18
[INFO 2017-06-30 03:23:35,035 main.py:52] epoch 2715, training loss: 6052.16, average training loss: 6310.32, base loss: 16009.53
[INFO 2017-06-30 03:23:38,172 main.py:52] epoch 2716, training loss: 6132.78, average training loss: 6309.66, base loss: 16009.75
[INFO 2017-06-30 03:23:41,341 main.py:52] epoch 2717, training loss: 6087.02, average training loss: 6309.01, base loss: 16009.10
[INFO 2017-06-30 03:23:44,510 main.py:52] epoch 2718, training loss: 5858.57, average training loss: 6308.35, base loss: 16009.38
[INFO 2017-06-30 03:23:47,658 main.py:52] epoch 2719, training loss: 6074.08, average training loss: 6308.09, base loss: 16009.25
[INFO 2017-06-30 03:23:50,840 main.py:52] epoch 2720, training loss: 6148.67, average training loss: 6308.37, base loss: 16009.56
[INFO 2017-06-30 03:23:54,027 main.py:52] epoch 2721, training loss: 6073.55, average training loss: 6308.35, base loss: 16010.18
[INFO 2017-06-30 03:23:57,180 main.py:52] epoch 2722, training loss: 6783.29, average training loss: 6308.91, base loss: 16011.90
[INFO 2017-06-30 03:24:00,342 main.py:52] epoch 2723, training loss: 5988.32, average training loss: 6308.56, base loss: 16011.39
[INFO 2017-06-30 03:24:03,520 main.py:52] epoch 2724, training loss: 5641.91, average training loss: 6307.15, base loss: 16010.23
[INFO 2017-06-30 03:24:06,688 main.py:52] epoch 2725, training loss: 6140.25, average training loss: 6306.82, base loss: 16009.76
[INFO 2017-06-30 03:24:09,876 main.py:52] epoch 2726, training loss: 6011.00, average training loss: 6306.33, base loss: 16010.38
[INFO 2017-06-30 03:24:13,049 main.py:52] epoch 2727, training loss: 6140.15, average training loss: 6305.68, base loss: 16009.55
[INFO 2017-06-30 03:24:16,234 main.py:52] epoch 2728, training loss: 5709.74, average training loss: 6304.70, base loss: 16009.19
[INFO 2017-06-30 03:24:19,418 main.py:52] epoch 2729, training loss: 6621.93, average training loss: 6305.07, base loss: 16010.24
[INFO 2017-06-30 03:24:22,582 main.py:52] epoch 2730, training loss: 6156.05, average training loss: 6304.45, base loss: 16009.86
[INFO 2017-06-30 03:24:25,761 main.py:52] epoch 2731, training loss: 6127.32, average training loss: 6304.25, base loss: 16009.78
[INFO 2017-06-30 03:24:28,957 main.py:52] epoch 2732, training loss: 6292.79, average training loss: 6304.39, base loss: 16009.83
[INFO 2017-06-30 03:24:32,126 main.py:52] epoch 2733, training loss: 6373.60, average training loss: 6304.27, base loss: 16009.94
[INFO 2017-06-30 03:24:35,357 main.py:52] epoch 2734, training loss: 6403.39, average training loss: 6303.79, base loss: 16010.13
[INFO 2017-06-30 03:24:38,520 main.py:52] epoch 2735, training loss: 6309.85, average training loss: 6303.46, base loss: 16010.61
[INFO 2017-06-30 03:24:41,738 main.py:52] epoch 2736, training loss: 6298.81, average training loss: 6302.68, base loss: 16011.21
[INFO 2017-06-30 03:24:44,932 main.py:52] epoch 2737, training loss: 6146.63, average training loss: 6302.15, base loss: 16011.59
[INFO 2017-06-30 03:24:48,083 main.py:52] epoch 2738, training loss: 6411.51, average training loss: 6302.53, base loss: 16011.39
[INFO 2017-06-30 03:24:51,258 main.py:52] epoch 2739, training loss: 6069.47, average training loss: 6302.50, base loss: 16011.33
[INFO 2017-06-30 03:24:54,443 main.py:52] epoch 2740, training loss: 6160.14, average training loss: 6302.37, base loss: 16011.94
[INFO 2017-06-30 03:24:57,598 main.py:52] epoch 2741, training loss: 6030.65, average training loss: 6302.26, base loss: 16011.81
[INFO 2017-06-30 03:25:00,758 main.py:52] epoch 2742, training loss: 6009.70, average training loss: 6302.01, base loss: 16011.41
[INFO 2017-06-30 03:25:03,928 main.py:52] epoch 2743, training loss: 6673.55, average training loss: 6302.36, base loss: 16012.23
[INFO 2017-06-30 03:25:07,088 main.py:52] epoch 2744, training loss: 5690.66, average training loss: 6301.43, base loss: 16011.72
[INFO 2017-06-30 03:25:10,234 main.py:52] epoch 2745, training loss: 5973.47, average training loss: 6301.57, base loss: 16012.08
[INFO 2017-06-30 03:25:13,391 main.py:52] epoch 2746, training loss: 6346.04, average training loss: 6301.21, base loss: 16012.75
[INFO 2017-06-30 03:25:16,567 main.py:52] epoch 2747, training loss: 6325.69, average training loss: 6301.17, base loss: 16013.22
[INFO 2017-06-30 03:25:19,737 main.py:52] epoch 2748, training loss: 6342.24, average training loss: 6301.55, base loss: 16012.64
[INFO 2017-06-30 03:25:22,931 main.py:52] epoch 2749, training loss: 6371.72, average training loss: 6301.34, base loss: 16012.39
[INFO 2017-06-30 03:25:26,053 main.py:52] epoch 2750, training loss: 5925.37, average training loss: 6300.49, base loss: 16011.81
[INFO 2017-06-30 03:25:29,238 main.py:52] epoch 2751, training loss: 6125.89, average training loss: 6300.00, base loss: 16012.03
[INFO 2017-06-30 03:25:32,400 main.py:52] epoch 2752, training loss: 5780.73, average training loss: 6299.08, base loss: 16011.40
[INFO 2017-06-30 03:25:35,588 main.py:52] epoch 2753, training loss: 6195.74, average training loss: 6298.58, base loss: 16011.49
[INFO 2017-06-30 03:25:38,748 main.py:52] epoch 2754, training loss: 5458.12, average training loss: 6297.63, base loss: 16010.33
[INFO 2017-06-30 03:25:41,887 main.py:52] epoch 2755, training loss: 5951.69, average training loss: 6297.56, base loss: 16009.98
[INFO 2017-06-30 03:25:45,038 main.py:52] epoch 2756, training loss: 7081.58, average training loss: 6298.32, base loss: 16011.46
[INFO 2017-06-30 03:25:48,176 main.py:52] epoch 2757, training loss: 6092.22, average training loss: 6298.16, base loss: 16012.10
[INFO 2017-06-30 03:25:51,377 main.py:52] epoch 2758, training loss: 6076.60, average training loss: 6297.54, base loss: 16012.49
[INFO 2017-06-30 03:25:54,546 main.py:52] epoch 2759, training loss: 6637.84, average training loss: 6298.09, base loss: 16013.68
[INFO 2017-06-30 03:25:57,731 main.py:52] epoch 2760, training loss: 6039.39, average training loss: 6297.84, base loss: 16013.72
[INFO 2017-06-30 03:26:00,946 main.py:52] epoch 2761, training loss: 5810.42, average training loss: 6297.28, base loss: 16013.13
[INFO 2017-06-30 03:26:04,132 main.py:52] epoch 2762, training loss: 6213.05, average training loss: 6297.24, base loss: 16013.15
[INFO 2017-06-30 03:26:07,280 main.py:52] epoch 2763, training loss: 6188.54, average training loss: 6296.29, base loss: 16013.56
[INFO 2017-06-30 03:26:10,441 main.py:52] epoch 2764, training loss: 6408.86, average training loss: 6295.70, base loss: 16013.23
[INFO 2017-06-30 03:26:13,591 main.py:52] epoch 2765, training loss: 6603.31, average training loss: 6295.83, base loss: 16013.66
[INFO 2017-06-30 03:26:16,776 main.py:52] epoch 2766, training loss: 5833.62, average training loss: 6295.24, base loss: 16012.21
[INFO 2017-06-30 03:26:19,943 main.py:52] epoch 2767, training loss: 6357.74, average training loss: 6295.05, base loss: 16011.95
[INFO 2017-06-30 03:26:23,133 main.py:52] epoch 2768, training loss: 5959.83, average training loss: 6294.57, base loss: 16010.80
[INFO 2017-06-30 03:26:26,368 main.py:52] epoch 2769, training loss: 6809.64, average training loss: 6295.57, base loss: 16012.04
[INFO 2017-06-30 03:26:29,538 main.py:52] epoch 2770, training loss: 6097.22, average training loss: 6294.94, base loss: 16012.14
[INFO 2017-06-30 03:26:32,754 main.py:52] epoch 2771, training loss: 6317.37, average training loss: 6294.63, base loss: 16012.48
[INFO 2017-06-30 03:26:35,901 main.py:52] epoch 2772, training loss: 5574.78, average training loss: 6293.53, base loss: 16011.02
[INFO 2017-06-30 03:26:39,040 main.py:52] epoch 2773, training loss: 6218.84, average training loss: 6293.69, base loss: 16010.75
[INFO 2017-06-30 03:26:42,219 main.py:52] epoch 2774, training loss: 5859.06, average training loss: 6293.10, base loss: 16010.75
[INFO 2017-06-30 03:26:45,457 main.py:52] epoch 2775, training loss: 5921.78, average training loss: 6292.75, base loss: 16011.09
[INFO 2017-06-30 03:26:48,652 main.py:52] epoch 2776, training loss: 6323.51, average training loss: 6292.47, base loss: 16011.78
[INFO 2017-06-30 03:26:51,843 main.py:52] epoch 2777, training loss: 6260.47, average training loss: 6292.94, base loss: 16011.71
[INFO 2017-06-30 03:26:55,024 main.py:52] epoch 2778, training loss: 5950.14, average training loss: 6292.36, base loss: 16012.17
[INFO 2017-06-30 03:26:58,187 main.py:52] epoch 2779, training loss: 6503.90, average training loss: 6291.95, base loss: 16013.17
[INFO 2017-06-30 03:27:01,336 main.py:52] epoch 2780, training loss: 6196.61, average training loss: 6291.19, base loss: 16012.94
[INFO 2017-06-30 03:27:04,502 main.py:52] epoch 2781, training loss: 6348.12, average training loss: 6291.13, base loss: 16013.01
[INFO 2017-06-30 03:27:07,655 main.py:52] epoch 2782, training loss: 6463.25, average training loss: 6291.45, base loss: 16013.51
[INFO 2017-06-30 03:27:10,819 main.py:52] epoch 2783, training loss: 6142.10, average training loss: 6290.90, base loss: 16012.26
[INFO 2017-06-30 03:27:13,975 main.py:52] epoch 2784, training loss: 5538.35, average training loss: 6289.75, base loss: 16012.54
[INFO 2017-06-30 03:27:17,120 main.py:52] epoch 2785, training loss: 6185.13, average training loss: 6289.27, base loss: 16012.21
[INFO 2017-06-30 03:27:20,280 main.py:52] epoch 2786, training loss: 5941.25, average training loss: 6288.52, base loss: 16011.72
[INFO 2017-06-30 03:27:23,432 main.py:52] epoch 2787, training loss: 5442.54, average training loss: 6287.25, base loss: 16010.60
[INFO 2017-06-30 03:27:26,559 main.py:52] epoch 2788, training loss: 6004.58, average training loss: 6286.79, base loss: 16011.15
[INFO 2017-06-30 03:27:29,725 main.py:52] epoch 2789, training loss: 5883.38, average training loss: 6286.72, base loss: 16011.30
[INFO 2017-06-30 03:27:32,870 main.py:52] epoch 2790, training loss: 6168.67, average training loss: 6286.47, base loss: 16011.41
[INFO 2017-06-30 03:27:36,047 main.py:52] epoch 2791, training loss: 6290.95, average training loss: 6286.27, base loss: 16011.76
[INFO 2017-06-30 03:27:39,220 main.py:52] epoch 2792, training loss: 5870.02, average training loss: 6285.68, base loss: 16011.08
[INFO 2017-06-30 03:27:42,377 main.py:52] epoch 2793, training loss: 5516.81, average training loss: 6284.64, base loss: 16010.26
[INFO 2017-06-30 03:27:45,537 main.py:52] epoch 2794, training loss: 6304.18, average training loss: 6284.52, base loss: 16010.60
[INFO 2017-06-30 03:27:48,711 main.py:52] epoch 2795, training loss: 5820.48, average training loss: 6283.66, base loss: 16010.56
[INFO 2017-06-30 03:27:51,876 main.py:52] epoch 2796, training loss: 5734.02, average training loss: 6283.52, base loss: 16009.74
[INFO 2017-06-30 03:27:55,088 main.py:52] epoch 2797, training loss: 6172.34, average training loss: 6283.40, base loss: 16009.21
[INFO 2017-06-30 03:27:58,218 main.py:52] epoch 2798, training loss: 6066.03, average training loss: 6282.51, base loss: 16009.75
[INFO 2017-06-30 03:28:01,438 main.py:52] epoch 2799, training loss: 5983.31, average training loss: 6282.29, base loss: 16009.70
[INFO 2017-06-30 03:28:01,439 main.py:54] epoch 2799, testing
[INFO 2017-06-30 03:28:14,858 main.py:97] average testing loss: 6174.33, base loss: 16252.07
[INFO 2017-06-30 03:28:14,858 main.py:98] improve_loss: 10077.74, improve_percent: 0.62
[INFO 2017-06-30 03:28:14,860 main.py:62] model save to ./model/final.pth
[INFO 2017-06-30 03:28:14,894 main.py:66] current best improved percent: 0.62
[INFO 2017-06-30 03:28:18,090 main.py:52] epoch 2800, training loss: 6440.48, average training loss: 6281.77, base loss: 16010.78
[INFO 2017-06-30 03:28:21,267 main.py:52] epoch 2801, training loss: 6025.72, average training loss: 6281.38, base loss: 16010.85
[INFO 2017-06-30 03:28:24,427 main.py:52] epoch 2802, training loss: 6369.68, average training loss: 6281.19, base loss: 16012.01
[INFO 2017-06-30 03:28:27,617 main.py:52] epoch 2803, training loss: 6727.07, average training loss: 6281.67, base loss: 16012.91
[INFO 2017-06-30 03:28:30,790 main.py:52] epoch 2804, training loss: 5835.71, average training loss: 6281.08, base loss: 16011.79
[INFO 2017-06-30 03:28:33,933 main.py:52] epoch 2805, training loss: 6396.08, average training loss: 6281.37, base loss: 16011.65
[INFO 2017-06-30 03:28:37,071 main.py:52] epoch 2806, training loss: 6289.99, average training loss: 6280.84, base loss: 16011.16
[INFO 2017-06-30 03:28:40,312 main.py:52] epoch 2807, training loss: 6295.67, average training loss: 6280.38, base loss: 16010.68
[INFO 2017-06-30 03:28:43,510 main.py:52] epoch 2808, training loss: 5987.74, average training loss: 6279.43, base loss: 16010.26
[INFO 2017-06-30 03:28:46,684 main.py:52] epoch 2809, training loss: 5943.73, average training loss: 6278.43, base loss: 16010.26
[INFO 2017-06-30 03:28:49,896 main.py:52] epoch 2810, training loss: 6169.40, average training loss: 6278.44, base loss: 16010.20
[INFO 2017-06-30 03:28:53,118 main.py:52] epoch 2811, training loss: 6198.11, average training loss: 6278.12, base loss: 16009.82
[INFO 2017-06-30 03:28:56,316 main.py:52] epoch 2812, training loss: 5960.99, average training loss: 6277.68, base loss: 16009.71
[INFO 2017-06-30 03:28:59,474 main.py:52] epoch 2813, training loss: 6433.24, average training loss: 6277.64, base loss: 16010.67
[INFO 2017-06-30 03:29:02,639 main.py:52] epoch 2814, training loss: 6286.17, average training loss: 6277.54, base loss: 16010.74
[INFO 2017-06-30 03:29:05,809 main.py:52] epoch 2815, training loss: 5714.76, average training loss: 6277.02, base loss: 16009.46
[INFO 2017-06-30 03:29:08,928 main.py:52] epoch 2816, training loss: 6393.13, average training loss: 6277.29, base loss: 16009.77
[INFO 2017-06-30 03:29:12,109 main.py:52] epoch 2817, training loss: 6413.56, average training loss: 6276.72, base loss: 16010.21
[INFO 2017-06-30 03:29:15,264 main.py:52] epoch 2818, training loss: 6031.94, average training loss: 6276.31, base loss: 16009.15
[INFO 2017-06-30 03:29:18,393 main.py:52] epoch 2819, training loss: 6308.32, average training loss: 6276.14, base loss: 16008.39
[INFO 2017-06-30 03:29:21,582 main.py:52] epoch 2820, training loss: 6242.87, average training loss: 6276.09, base loss: 16009.32
[INFO 2017-06-30 03:29:24,758 main.py:52] epoch 2821, training loss: 5995.72, average training loss: 6274.96, base loss: 16009.29
[INFO 2017-06-30 03:29:27,918 main.py:52] epoch 2822, training loss: 6548.08, average training loss: 6275.00, base loss: 16010.39
[INFO 2017-06-30 03:29:31,086 main.py:52] epoch 2823, training loss: 5795.76, average training loss: 6274.41, base loss: 16009.73
[INFO 2017-06-30 03:29:34,289 main.py:52] epoch 2824, training loss: 5887.74, average training loss: 6274.04, base loss: 16009.12
[INFO 2017-06-30 03:29:37,465 main.py:52] epoch 2825, training loss: 5615.91, average training loss: 6273.82, base loss: 16007.68
[INFO 2017-06-30 03:29:40,653 main.py:52] epoch 2826, training loss: 6154.52, average training loss: 6273.18, base loss: 16007.93
[INFO 2017-06-30 03:29:43,847 main.py:52] epoch 2827, training loss: 6013.91, average training loss: 6271.92, base loss: 16007.05
[INFO 2017-06-30 03:29:46,992 main.py:52] epoch 2828, training loss: 5944.80, average training loss: 6271.48, base loss: 16005.78
[INFO 2017-06-30 03:29:50,196 main.py:52] epoch 2829, training loss: 5883.96, average training loss: 6271.11, base loss: 16005.38
[INFO 2017-06-30 03:29:53,406 main.py:52] epoch 2830, training loss: 5664.69, average training loss: 6269.76, base loss: 16005.25
[INFO 2017-06-30 03:29:56,573 main.py:52] epoch 2831, training loss: 6344.84, average training loss: 6269.84, base loss: 16005.96
[INFO 2017-06-30 03:29:59,708 main.py:52] epoch 2832, training loss: 6397.13, average training loss: 6270.12, base loss: 16005.79
[INFO 2017-06-30 03:30:02,859 main.py:52] epoch 2833, training loss: 5986.31, average training loss: 6269.70, base loss: 16005.15
[INFO 2017-06-30 03:30:06,008 main.py:52] epoch 2834, training loss: 5927.31, average training loss: 6269.09, base loss: 16005.26
[INFO 2017-06-30 03:30:09,182 main.py:52] epoch 2835, training loss: 6034.37, average training loss: 6268.53, base loss: 16005.87
[INFO 2017-06-30 03:30:12,364 main.py:52] epoch 2836, training loss: 6260.67, average training loss: 6268.39, base loss: 16007.09
[INFO 2017-06-30 03:30:15,470 main.py:52] epoch 2837, training loss: 6456.17, average training loss: 6268.31, base loss: 16009.24
[INFO 2017-06-30 03:30:18,685 main.py:52] epoch 2838, training loss: 5989.75, average training loss: 6267.83, base loss: 16008.87
[INFO 2017-06-30 03:30:21,886 main.py:52] epoch 2839, training loss: 6588.82, average training loss: 6268.24, base loss: 16009.50
[INFO 2017-06-30 03:30:25,015 main.py:52] epoch 2840, training loss: 6245.94, average training loss: 6268.22, base loss: 16008.30
[INFO 2017-06-30 03:30:28,187 main.py:52] epoch 2841, training loss: 5921.81, average training loss: 6267.59, base loss: 16006.82
[INFO 2017-06-30 03:30:31,385 main.py:52] epoch 2842, training loss: 6151.29, average training loss: 6267.51, base loss: 16007.15
[INFO 2017-06-30 03:30:34,536 main.py:52] epoch 2843, training loss: 6364.48, average training loss: 6267.23, base loss: 16007.52
[INFO 2017-06-30 03:30:37,673 main.py:52] epoch 2844, training loss: 6196.46, average training loss: 6267.29, base loss: 16006.99
[INFO 2017-06-30 03:30:40,832 main.py:52] epoch 2845, training loss: 5976.86, average training loss: 6266.84, base loss: 16006.14
[INFO 2017-06-30 03:30:44,020 main.py:52] epoch 2846, training loss: 6433.99, average training loss: 6266.60, base loss: 16006.99
[INFO 2017-06-30 03:30:47,227 main.py:52] epoch 2847, training loss: 6041.77, average training loss: 6265.67, base loss: 16007.00
[INFO 2017-06-30 03:30:50,420 main.py:52] epoch 2848, training loss: 6192.28, average training loss: 6265.82, base loss: 16007.28
[INFO 2017-06-30 03:30:53,591 main.py:52] epoch 2849, training loss: 6932.05, average training loss: 6266.02, base loss: 16008.53
[INFO 2017-06-30 03:30:56,795 main.py:52] epoch 2850, training loss: 6228.01, average training loss: 6266.20, base loss: 16008.97
[INFO 2017-06-30 03:30:59,998 main.py:52] epoch 2851, training loss: 6439.64, average training loss: 6266.17, base loss: 16009.94
[INFO 2017-06-30 03:31:03,142 main.py:52] epoch 2852, training loss: 6013.51, average training loss: 6265.88, base loss: 16010.58
[INFO 2017-06-30 03:31:06,354 main.py:52] epoch 2853, training loss: 6062.62, average training loss: 6266.05, base loss: 16010.35
[INFO 2017-06-30 03:31:09,476 main.py:52] epoch 2854, training loss: 6126.42, average training loss: 6265.67, base loss: 16009.11
[INFO 2017-06-30 03:31:12,630 main.py:52] epoch 2855, training loss: 6114.79, average training loss: 6264.73, base loss: 16008.08
[INFO 2017-06-30 03:31:15,805 main.py:52] epoch 2856, training loss: 6197.81, average training loss: 6264.31, base loss: 16007.93
[INFO 2017-06-30 03:31:18,950 main.py:52] epoch 2857, training loss: 5810.46, average training loss: 6264.34, base loss: 16008.55
[INFO 2017-06-30 03:31:22,161 main.py:52] epoch 2858, training loss: 5690.35, average training loss: 6263.66, base loss: 16007.82
[INFO 2017-06-30 03:31:25,298 main.py:52] epoch 2859, training loss: 6124.80, average training loss: 6263.44, base loss: 16007.82
[INFO 2017-06-30 03:31:28,468 main.py:52] epoch 2860, training loss: 6229.72, average training loss: 6263.25, base loss: 16006.79
[INFO 2017-06-30 03:31:31,640 main.py:52] epoch 2861, training loss: 5785.86, average training loss: 6262.00, base loss: 16006.35
[INFO 2017-06-30 03:31:34,816 main.py:52] epoch 2862, training loss: 5908.60, average training loss: 6261.23, base loss: 16006.39
[INFO 2017-06-30 03:31:38,020 main.py:52] epoch 2863, training loss: 6001.74, average training loss: 6261.12, base loss: 16007.01
[INFO 2017-06-30 03:31:41,198 main.py:52] epoch 2864, training loss: 5996.22, average training loss: 6260.94, base loss: 16006.64
[INFO 2017-06-30 03:31:44,321 main.py:52] epoch 2865, training loss: 5820.19, average training loss: 6259.74, base loss: 16006.35
[INFO 2017-06-30 03:31:47,473 main.py:52] epoch 2866, training loss: 6337.99, average training loss: 6259.93, base loss: 16007.32
[INFO 2017-06-30 03:31:50,610 main.py:52] epoch 2867, training loss: 5864.37, average training loss: 6258.86, base loss: 16007.56
[INFO 2017-06-30 03:31:53,757 main.py:52] epoch 2868, training loss: 6201.35, average training loss: 6257.92, base loss: 16008.09
[INFO 2017-06-30 03:31:56,952 main.py:52] epoch 2869, training loss: 5907.90, average training loss: 6257.85, base loss: 16008.57
[INFO 2017-06-30 03:32:00,145 main.py:52] epoch 2870, training loss: 5968.44, average training loss: 6257.48, base loss: 16008.21
[INFO 2017-06-30 03:32:03,285 main.py:52] epoch 2871, training loss: 6615.06, average training loss: 6257.70, base loss: 16008.82
[INFO 2017-06-30 03:32:06,418 main.py:52] epoch 2872, training loss: 5814.02, average training loss: 6257.58, base loss: 16007.98
[INFO 2017-06-30 03:32:09,582 main.py:52] epoch 2873, training loss: 6423.52, average training loss: 6257.74, base loss: 16009.00
[INFO 2017-06-30 03:32:12,741 main.py:52] epoch 2874, training loss: 6320.42, average training loss: 6257.70, base loss: 16008.69
[INFO 2017-06-30 03:32:15,885 main.py:52] epoch 2875, training loss: 6081.30, average training loss: 6257.74, base loss: 16008.77
[INFO 2017-06-30 03:32:19,024 main.py:52] epoch 2876, training loss: 6123.36, average training loss: 6257.43, base loss: 16008.16
[INFO 2017-06-30 03:32:22,161 main.py:52] epoch 2877, training loss: 5731.73, average training loss: 6256.32, base loss: 16006.80
[INFO 2017-06-30 03:32:25,318 main.py:52] epoch 2878, training loss: 7058.97, average training loss: 6257.23, base loss: 16009.06
[INFO 2017-06-30 03:32:28,434 main.py:52] epoch 2879, training loss: 6379.77, average training loss: 6257.47, base loss: 16009.66
[INFO 2017-06-30 03:32:31,620 main.py:52] epoch 2880, training loss: 6291.48, average training loss: 6257.29, base loss: 16009.81
[INFO 2017-06-30 03:32:34,794 main.py:52] epoch 2881, training loss: 5922.71, average training loss: 6256.66, base loss: 16009.58
[INFO 2017-06-30 03:32:37,982 main.py:52] epoch 2882, training loss: 5840.89, average training loss: 6256.12, base loss: 16009.27
[INFO 2017-06-30 03:32:41,143 main.py:52] epoch 2883, training loss: 5911.28, average training loss: 6255.39, base loss: 16009.76
[INFO 2017-06-30 03:32:44,333 main.py:52] epoch 2884, training loss: 5786.13, average training loss: 6254.61, base loss: 16008.95
[INFO 2017-06-30 03:32:47,522 main.py:52] epoch 2885, training loss: 6511.83, average training loss: 6254.67, base loss: 16008.95
[INFO 2017-06-30 03:32:50,695 main.py:52] epoch 2886, training loss: 6249.69, average training loss: 6254.23, base loss: 16008.91
[INFO 2017-06-30 03:32:53,949 main.py:52] epoch 2887, training loss: 6500.48, average training loss: 6254.19, base loss: 16009.85
[INFO 2017-06-30 03:32:57,123 main.py:52] epoch 2888, training loss: 6121.98, average training loss: 6253.58, base loss: 16009.41
[INFO 2017-06-30 03:33:00,316 main.py:52] epoch 2889, training loss: 6032.09, average training loss: 6253.09, base loss: 16008.23
[INFO 2017-06-30 03:33:03,443 main.py:52] epoch 2890, training loss: 6392.35, average training loss: 6252.76, base loss: 16008.58
[INFO 2017-06-30 03:33:06,572 main.py:52] epoch 2891, training loss: 6758.63, average training loss: 6253.02, base loss: 16010.69
[INFO 2017-06-30 03:33:09,709 main.py:52] epoch 2892, training loss: 5991.34, average training loss: 6252.98, base loss: 16010.82
[INFO 2017-06-30 03:33:12,874 main.py:52] epoch 2893, training loss: 5873.57, average training loss: 6252.70, base loss: 16010.48
[INFO 2017-06-30 03:33:16,026 main.py:52] epoch 2894, training loss: 6707.59, average training loss: 6252.77, base loss: 16012.38
[INFO 2017-06-30 03:33:19,201 main.py:52] epoch 2895, training loss: 6163.56, average training loss: 6253.10, base loss: 16011.40
[INFO 2017-06-30 03:33:22,406 main.py:52] epoch 2896, training loss: 6132.67, average training loss: 6253.34, base loss: 16011.71
[INFO 2017-06-30 03:33:25,639 main.py:52] epoch 2897, training loss: 5797.36, average training loss: 6252.71, base loss: 16010.83
[INFO 2017-06-30 03:33:28,813 main.py:52] epoch 2898, training loss: 6050.94, average training loss: 6252.31, base loss: 16010.30
[INFO 2017-06-30 03:33:32,004 main.py:52] epoch 2899, training loss: 6180.79, average training loss: 6252.07, base loss: 16009.75
[INFO 2017-06-30 03:33:32,004 main.py:54] epoch 2899, testing
[INFO 2017-06-30 03:33:45,263 main.py:97] average testing loss: 6060.42, base loss: 16147.97
[INFO 2017-06-30 03:33:45,264 main.py:98] improve_loss: 10087.56, improve_percent: 0.62
[INFO 2017-06-30 03:33:45,265 main.py:62] model save to ./model/final.pth
[INFO 2017-06-30 03:33:45,300 main.py:66] current best improved percent: 0.62
[INFO 2017-06-30 03:33:48,405 main.py:52] epoch 2900, training loss: 6387.39, average training loss: 6252.44, base loss: 16010.00
[INFO 2017-06-30 03:33:51,547 main.py:52] epoch 2901, training loss: 5788.95, average training loss: 6251.56, base loss: 16009.63
[INFO 2017-06-30 03:33:54,725 main.py:52] epoch 2902, training loss: 5927.49, average training loss: 6250.94, base loss: 16007.78
[INFO 2017-06-30 03:33:57,922 main.py:52] epoch 2903, training loss: 5993.32, average training loss: 6250.04, base loss: 16006.94
[INFO 2017-06-30 03:34:01,064 main.py:52] epoch 2904, training loss: 6218.76, average training loss: 6249.69, base loss: 16006.02
[INFO 2017-06-30 03:34:04,259 main.py:52] epoch 2905, training loss: 6500.71, average training loss: 6249.78, base loss: 16006.29
[INFO 2017-06-30 03:34:07,406 main.py:52] epoch 2906, training loss: 5881.28, average training loss: 6248.96, base loss: 16006.17
[INFO 2017-06-30 03:34:10,593 main.py:52] epoch 2907, training loss: 5629.18, average training loss: 6247.66, base loss: 16005.30
[INFO 2017-06-30 03:34:13,763 main.py:52] epoch 2908, training loss: 5806.02, average training loss: 6247.05, base loss: 16004.78
[INFO 2017-06-30 03:34:16,916 main.py:52] epoch 2909, training loss: 6076.49, average training loss: 6246.44, base loss: 16004.96
[INFO 2017-06-30 03:34:20,075 main.py:52] epoch 2910, training loss: 5774.66, average training loss: 6245.36, base loss: 16004.82
[INFO 2017-06-30 03:34:23,219 main.py:52] epoch 2911, training loss: 5714.06, average training loss: 6244.20, base loss: 16004.63
[INFO 2017-06-30 03:34:26,380 main.py:52] epoch 2912, training loss: 6444.64, average training loss: 6244.63, base loss: 16005.23
[INFO 2017-06-30 03:34:29,552 main.py:52] epoch 2913, training loss: 6061.71, average training loss: 6244.19, base loss: 16004.61
[INFO 2017-06-30 03:34:32,739 main.py:52] epoch 2914, training loss: 6649.23, average training loss: 6244.83, base loss: 16006.26
[INFO 2017-06-30 03:34:35,903 main.py:52] epoch 2915, training loss: 5874.23, average training loss: 6244.30, base loss: 16006.06
[INFO 2017-06-30 03:34:39,025 main.py:52] epoch 2916, training loss: 5654.22, average training loss: 6243.40, base loss: 16006.01
[INFO 2017-06-30 03:34:42,232 main.py:52] epoch 2917, training loss: 6146.79, average training loss: 6242.98, base loss: 16006.00
[INFO 2017-06-30 03:34:45,377 main.py:52] epoch 2918, training loss: 6076.83, average training loss: 6242.53, base loss: 16005.78
[INFO 2017-06-30 03:34:48,538 main.py:52] epoch 2919, training loss: 6032.33, average training loss: 6242.04, base loss: 16005.30
[INFO 2017-06-30 03:34:51,735 main.py:52] epoch 2920, training loss: 6026.74, average training loss: 6241.87, base loss: 16005.19
[INFO 2017-06-30 03:34:54,939 main.py:52] epoch 2921, training loss: 6419.08, average training loss: 6242.48, base loss: 16005.51
[INFO 2017-06-30 03:34:58,078 main.py:52] epoch 2922, training loss: 6152.64, average training loss: 6241.79, base loss: 16005.64
[INFO 2017-06-30 03:35:01,263 main.py:52] epoch 2923, training loss: 5798.91, average training loss: 6241.16, base loss: 16005.14
[INFO 2017-06-30 03:35:04,460 main.py:52] epoch 2924, training loss: 6880.95, average training loss: 6241.86, base loss: 16006.53
[INFO 2017-06-30 03:35:07,608 main.py:52] epoch 2925, training loss: 5964.53, average training loss: 6241.45, base loss: 16005.93
[INFO 2017-06-30 03:35:10,795 main.py:52] epoch 2926, training loss: 6682.82, average training loss: 6242.04, base loss: 16006.12
[INFO 2017-06-30 03:35:13,990 main.py:52] epoch 2927, training loss: 6214.11, average training loss: 6242.08, base loss: 16005.24
[INFO 2017-06-30 03:35:17,172 main.py:52] epoch 2928, training loss: 5924.06, average training loss: 6241.79, base loss: 16005.52
[INFO 2017-06-30 03:35:20,339 main.py:52] epoch 2929, training loss: 6441.59, average training loss: 6241.89, base loss: 16006.94
[INFO 2017-06-30 03:35:23,544 main.py:52] epoch 2930, training loss: 5813.78, average training loss: 6241.15, base loss: 16007.06
[INFO 2017-06-30 03:35:26,710 main.py:52] epoch 2931, training loss: 5667.49, average training loss: 6240.78, base loss: 16006.65
[INFO 2017-06-30 03:35:29,901 main.py:52] epoch 2932, training loss: 6939.45, average training loss: 6241.70, base loss: 16007.97
[INFO 2017-06-30 03:35:33,021 main.py:52] epoch 2933, training loss: 6151.76, average training loss: 6241.52, base loss: 16007.42
[INFO 2017-06-30 03:35:36,200 main.py:52] epoch 2934, training loss: 6009.54, average training loss: 6241.36, base loss: 16006.85
[INFO 2017-06-30 03:35:39,382 main.py:52] epoch 2935, training loss: 6219.30, average training loss: 6240.97, base loss: 16006.17
[INFO 2017-06-30 03:35:42,584 main.py:52] epoch 2936, training loss: 6309.40, average training loss: 6240.24, base loss: 16006.07
[INFO 2017-06-30 03:35:45,746 main.py:52] epoch 2937, training loss: 5817.78, average training loss: 6240.33, base loss: 16005.80
[INFO 2017-06-30 03:35:48,907 main.py:52] epoch 2938, training loss: 5845.18, average training loss: 6239.54, base loss: 16005.32
[INFO 2017-06-30 03:35:52,041 main.py:52] epoch 2939, training loss: 5557.54, average training loss: 6239.12, base loss: 16004.70
[INFO 2017-06-30 03:35:55,167 main.py:52] epoch 2940, training loss: 6264.19, average training loss: 6239.18, base loss: 16005.30
[INFO 2017-06-30 03:35:58,292 main.py:52] epoch 2941, training loss: 6441.77, average training loss: 6239.22, base loss: 16005.82
[INFO 2017-06-30 03:36:01,467 main.py:52] epoch 2942, training loss: 6458.48, average training loss: 6238.83, base loss: 16007.65
[INFO 2017-06-30 03:36:04,682 main.py:52] epoch 2943, training loss: 6082.22, average training loss: 6238.66, base loss: 16008.21
[INFO 2017-06-30 03:36:07,883 main.py:52] epoch 2944, training loss: 6209.83, average training loss: 6238.90, base loss: 16009.13
[INFO 2017-06-30 03:36:11,050 main.py:52] epoch 2945, training loss: 6104.80, average training loss: 6238.28, base loss: 16009.07
[INFO 2017-06-30 03:36:14,214 main.py:52] epoch 2946, training loss: 6090.57, average training loss: 6237.77, base loss: 16008.32
[INFO 2017-06-30 03:36:17,386 main.py:52] epoch 2947, training loss: 6030.96, average training loss: 6237.54, base loss: 16007.31
[INFO 2017-06-30 03:36:20,570 main.py:52] epoch 2948, training loss: 5887.67, average training loss: 6237.15, base loss: 16006.82
[INFO 2017-06-30 03:36:23,730 main.py:52] epoch 2949, training loss: 5820.64, average training loss: 6236.78, base loss: 16006.75
[INFO 2017-06-30 03:36:26,909 main.py:52] epoch 2950, training loss: 6530.24, average training loss: 6236.75, base loss: 16007.12
[INFO 2017-06-30 03:36:30,131 main.py:52] epoch 2951, training loss: 6098.97, average training loss: 6237.08, base loss: 16006.96
[INFO 2017-06-30 03:36:33,347 main.py:52] epoch 2952, training loss: 5813.91, average training loss: 6236.74, base loss: 16006.61
[INFO 2017-06-30 03:36:36,511 main.py:52] epoch 2953, training loss: 5880.26, average training loss: 6236.43, base loss: 16006.93
[INFO 2017-06-30 03:36:39,657 main.py:52] epoch 2954, training loss: 6462.10, average training loss: 6236.30, base loss: 16008.23
[INFO 2017-06-30 03:36:42,824 main.py:52] epoch 2955, training loss: 5674.93, average training loss: 6235.37, base loss: 16007.65
[INFO 2017-06-30 03:36:45,961 main.py:52] epoch 2956, training loss: 5880.64, average training loss: 6234.99, base loss: 16008.07
[INFO 2017-06-30 03:36:49,144 main.py:52] epoch 2957, training loss: 5940.20, average training loss: 6234.42, base loss: 16007.72
[INFO 2017-06-30 03:36:52,295 main.py:52] epoch 2958, training loss: 6724.27, average training loss: 6234.04, base loss: 16008.11
[INFO 2017-06-30 03:36:55,469 main.py:52] epoch 2959, training loss: 5528.60, average training loss: 6233.29, base loss: 16006.55
[INFO 2017-06-30 03:36:58,616 main.py:52] epoch 2960, training loss: 6533.94, average training loss: 6233.42, base loss: 16007.17
[INFO 2017-06-30 03:37:01,756 main.py:52] epoch 2961, training loss: 5968.81, average training loss: 6232.95, base loss: 16006.43
[INFO 2017-06-30 03:37:04,908 main.py:52] epoch 2962, training loss: 6102.14, average training loss: 6232.38, base loss: 16006.89
[INFO 2017-06-30 03:37:08,074 main.py:52] epoch 2963, training loss: 5989.34, average training loss: 6232.33, base loss: 16006.76
[INFO 2017-06-30 03:37:11,237 main.py:52] epoch 2964, training loss: 6496.56, average training loss: 6232.50, base loss: 16007.77
[INFO 2017-06-30 03:37:14,376 main.py:52] epoch 2965, training loss: 6228.41, average training loss: 6232.06, base loss: 16007.89
[INFO 2017-06-30 03:37:17,529 main.py:52] epoch 2966, training loss: 6097.34, average training loss: 6232.32, base loss: 16007.29
[INFO 2017-06-30 03:37:20,692 main.py:52] epoch 2967, training loss: 5901.76, average training loss: 6231.36, base loss: 16006.17
[INFO 2017-06-30 03:37:23,883 main.py:52] epoch 2968, training loss: 5979.01, average training loss: 6231.00, base loss: 16005.95
[INFO 2017-06-30 03:37:27,119 main.py:52] epoch 2969, training loss: 6468.69, average training loss: 6231.05, base loss: 16006.49
[INFO 2017-06-30 03:37:30,273 main.py:52] epoch 2970, training loss: 6346.38, average training loss: 6231.19, base loss: 16007.65
[INFO 2017-06-30 03:37:33,437 main.py:52] epoch 2971, training loss: 6146.51, average training loss: 6230.98, base loss: 16007.92
[INFO 2017-06-30 03:37:36,618 main.py:52] epoch 2972, training loss: 6022.66, average training loss: 6230.64, base loss: 16007.92
[INFO 2017-06-30 03:37:39,808 main.py:52] epoch 2973, training loss: 6305.26, average training loss: 6230.48, base loss: 16008.41
[INFO 2017-06-30 03:37:42,985 main.py:52] epoch 2974, training loss: 6355.45, average training loss: 6230.47, base loss: 16009.29
[INFO 2017-06-30 03:37:46,167 main.py:52] epoch 2975, training loss: 6112.09, average training loss: 6230.49, base loss: 16010.17
[INFO 2017-06-30 03:37:49,320 main.py:52] epoch 2976, training loss: 5629.88, average training loss: 6229.29, base loss: 16008.90
[INFO 2017-06-30 03:37:52,505 main.py:52] epoch 2977, training loss: 5995.15, average training loss: 6229.17, base loss: 16008.24
[INFO 2017-06-30 03:37:55,677 main.py:52] epoch 2978, training loss: 5704.47, average training loss: 6228.39, base loss: 16008.15
[INFO 2017-06-30 03:37:58,831 main.py:52] epoch 2979, training loss: 6143.51, average training loss: 6227.74, base loss: 16008.84
[INFO 2017-06-30 03:38:01,983 main.py:52] epoch 2980, training loss: 6360.68, average training loss: 6227.86, base loss: 16009.31
[INFO 2017-06-30 03:38:05,119 main.py:52] epoch 2981, training loss: 5861.82, average training loss: 6227.18, base loss: 16008.25
[INFO 2017-06-30 03:38:08,292 main.py:52] epoch 2982, training loss: 5817.72, average training loss: 6226.25, base loss: 16007.77
[INFO 2017-06-30 03:38:11,407 main.py:52] epoch 2983, training loss: 5842.49, average training loss: 6225.98, base loss: 16008.07
[INFO 2017-06-30 03:38:14,641 main.py:52] epoch 2984, training loss: 5594.81, average training loss: 6225.55, base loss: 16006.76
[INFO 2017-06-30 03:38:17,824 main.py:52] epoch 2985, training loss: 5925.69, average training loss: 6224.96, base loss: 16007.13
[INFO 2017-06-30 03:38:20,984 main.py:52] epoch 2986, training loss: 6065.59, average training loss: 6224.99, base loss: 16006.62
[INFO 2017-06-30 03:38:24,130 main.py:52] epoch 2987, training loss: 6056.27, average training loss: 6224.46, base loss: 16006.45
[INFO 2017-06-30 03:38:27,315 main.py:52] epoch 2988, training loss: 5882.17, average training loss: 6224.22, base loss: 16007.04
[INFO 2017-06-30 03:38:30,502 main.py:52] epoch 2989, training loss: 5679.13, average training loss: 6223.81, base loss: 16007.06
[INFO 2017-06-30 03:38:33,660 main.py:52] epoch 2990, training loss: 6229.66, average training loss: 6224.07, base loss: 16007.09
[INFO 2017-06-30 03:38:36,811 main.py:52] epoch 2991, training loss: 5995.73, average training loss: 6223.22, base loss: 16006.86
[INFO 2017-06-30 03:38:39,969 main.py:52] epoch 2992, training loss: 6360.65, average training loss: 6223.31, base loss: 16007.00
[INFO 2017-06-30 03:38:43,136 main.py:52] epoch 2993, training loss: 6704.04, average training loss: 6222.82, base loss: 16008.72
[INFO 2017-06-30 03:38:46,330 main.py:52] epoch 2994, training loss: 5578.89, average training loss: 6222.51, base loss: 16008.12
[INFO 2017-06-30 03:38:49,499 main.py:52] epoch 2995, training loss: 6256.47, average training loss: 6222.62, base loss: 16007.78
[INFO 2017-06-30 03:38:52,673 main.py:52] epoch 2996, training loss: 6014.38, average training loss: 6222.67, base loss: 16007.58
[INFO 2017-06-30 03:38:55,851 main.py:52] epoch 2997, training loss: 5935.76, average training loss: 6222.41, base loss: 16006.66
[INFO 2017-06-30 03:38:59,029 main.py:52] epoch 2998, training loss: 6207.71, average training loss: 6221.62, base loss: 16006.31
[INFO 2017-06-30 03:39:02,197 main.py:52] epoch 2999, training loss: 5756.35, average training loss: 6220.93, base loss: 16005.86
[INFO 2017-06-30 03:39:02,197 main.py:54] epoch 2999, testing
[INFO 2017-06-30 03:39:15,426 main.py:97] average testing loss: 5865.06, base loss: 15421.11
[INFO 2017-06-30 03:39:15,426 main.py:98] improve_loss: 9556.06, improve_percent: 0.62
[INFO 2017-06-30 03:39:15,430 main.py:66] current best improved percent: 0.62
[INFO 2017-06-30 03:39:18,607 main.py:52] epoch 3000, training loss: 6140.07, average training loss: 6221.21, base loss: 16005.53
[INFO 2017-06-30 03:39:21,756 main.py:52] epoch 3001, training loss: 5869.68, average training loss: 6220.70, base loss: 16004.28
[INFO 2017-06-30 03:39:24,932 main.py:52] epoch 3002, training loss: 6170.69, average training loss: 6220.69, base loss: 16004.55
[INFO 2017-06-30 03:39:28,132 main.py:52] epoch 3003, training loss: 6119.72, average training loss: 6220.51, base loss: 16004.68
[INFO 2017-06-30 03:39:31,297 main.py:52] epoch 3004, training loss: 5563.94, average training loss: 6220.05, base loss: 16003.89
[INFO 2017-06-30 03:39:34,425 main.py:52] epoch 3005, training loss: 6245.05, average training loss: 6219.33, base loss: 16004.76
[INFO 2017-06-30 03:39:37,622 main.py:52] epoch 3006, training loss: 6204.36, average training loss: 6219.31, base loss: 16004.83
[INFO 2017-06-30 03:39:40,836 main.py:52] epoch 3007, training loss: 5688.56, average training loss: 6218.55, base loss: 16004.25
[INFO 2017-06-30 03:39:44,004 main.py:52] epoch 3008, training loss: 5942.82, average training loss: 6218.29, base loss: 16003.56
[INFO 2017-06-30 03:39:47,207 main.py:52] epoch 3009, training loss: 6046.46, average training loss: 6217.83, base loss: 16002.95
[INFO 2017-06-30 03:39:50,360 main.py:52] epoch 3010, training loss: 6089.63, average training loss: 6217.44, base loss: 16003.25
[INFO 2017-06-30 03:39:53,548 main.py:52] epoch 3011, training loss: 5884.05, average training loss: 6217.08, base loss: 16003.09
[INFO 2017-06-30 03:39:56,701 main.py:52] epoch 3012, training loss: 6205.94, average training loss: 6217.04, base loss: 16002.98
[INFO 2017-06-30 03:39:59,861 main.py:52] epoch 3013, training loss: 6135.19, average training loss: 6216.96, base loss: 16003.14
[INFO 2017-06-30 03:40:03,055 main.py:52] epoch 3014, training loss: 6539.33, average training loss: 6217.11, base loss: 16002.39
[INFO 2017-06-30 03:40:06,237 main.py:52] epoch 3015, training loss: 5878.68, average training loss: 6216.47, base loss: 16001.71
[INFO 2017-06-30 03:40:09,404 main.py:52] epoch 3016, training loss: 5945.81, average training loss: 6216.11, base loss: 16001.24
[INFO 2017-06-30 03:40:12,557 main.py:52] epoch 3017, training loss: 5791.92, average training loss: 6215.67, base loss: 16000.57
[INFO 2017-06-30 03:40:15,709 main.py:52] epoch 3018, training loss: 6386.05, average training loss: 6215.87, base loss: 16001.04
[INFO 2017-06-30 03:40:18,886 main.py:52] epoch 3019, training loss: 5789.91, average training loss: 6214.99, base loss: 16000.68
[INFO 2017-06-30 03:40:22,052 main.py:52] epoch 3020, training loss: 5930.44, average training loss: 6214.57, base loss: 16000.34
[INFO 2017-06-30 03:40:25,193 main.py:52] epoch 3021, training loss: 6087.23, average training loss: 6214.47, base loss: 16000.59
[INFO 2017-06-30 03:40:28,384 main.py:52] epoch 3022, training loss: 6411.18, average training loss: 6214.13, base loss: 16000.78
[INFO 2017-06-30 03:40:31,566 main.py:52] epoch 3023, training loss: 6045.91, average training loss: 6213.19, base loss: 16000.27
[INFO 2017-06-30 03:40:34,766 main.py:52] epoch 3024, training loss: 6133.70, average training loss: 6213.27, base loss: 16000.42
[INFO 2017-06-30 03:40:37,982 main.py:52] epoch 3025, training loss: 6291.70, average training loss: 6212.98, base loss: 16001.24
[INFO 2017-06-30 03:40:41,145 main.py:52] epoch 3026, training loss: 5895.81, average training loss: 6212.50, base loss: 16001.38
[INFO 2017-06-30 03:40:44,329 main.py:52] epoch 3027, training loss: 5856.55, average training loss: 6211.80, base loss: 16001.58
[INFO 2017-06-30 03:40:47,528 main.py:52] epoch 3028, training loss: 5923.33, average training loss: 6211.40, base loss: 16001.86
[INFO 2017-06-30 03:40:50,725 main.py:52] epoch 3029, training loss: 6623.61, average training loss: 6211.71, base loss: 16003.46
[INFO 2017-06-30 03:40:53,907 main.py:52] epoch 3030, training loss: 6013.37, average training loss: 6211.05, base loss: 16003.86
[INFO 2017-06-30 03:40:57,075 main.py:52] epoch 3031, training loss: 5971.36, average training loss: 6210.11, base loss: 16003.67
[INFO 2017-06-30 03:41:00,273 main.py:52] epoch 3032, training loss: 5977.40, average training loss: 6209.00, base loss: 16003.84
[INFO 2017-06-30 03:41:03,422 main.py:52] epoch 3033, training loss: 6375.34, average training loss: 6209.22, base loss: 16004.98
[INFO 2017-06-30 03:41:06,606 main.py:52] epoch 3034, training loss: 6025.61, average training loss: 6208.92, base loss: 16004.61
[INFO 2017-06-30 03:41:09,821 main.py:52] epoch 3035, training loss: 6229.67, average training loss: 6208.42, base loss: 16005.19
[INFO 2017-06-30 03:41:13,030 main.py:52] epoch 3036, training loss: 6481.69, average training loss: 6208.84, base loss: 16006.47
[INFO 2017-06-30 03:41:16,212 main.py:52] epoch 3037, training loss: 6618.17, average training loss: 6208.50, base loss: 16007.73
[INFO 2017-06-30 03:41:19,418 main.py:52] epoch 3038, training loss: 5995.47, average training loss: 6208.38, base loss: 16008.11
[INFO 2017-06-30 03:41:22,621 main.py:52] epoch 3039, training loss: 5901.79, average training loss: 6207.86, base loss: 16007.85
[INFO 2017-06-30 03:41:25,854 main.py:52] epoch 3040, training loss: 5892.56, average training loss: 6207.37, base loss: 16008.13
[INFO 2017-06-30 03:41:29,003 main.py:52] epoch 3041, training loss: 6082.70, average training loss: 6206.37, base loss: 16008.45
[INFO 2017-06-30 03:41:32,186 main.py:52] epoch 3042, training loss: 6065.45, average training loss: 6206.07, base loss: 16007.84
[INFO 2017-06-30 03:41:35,344 main.py:52] epoch 3043, training loss: 6227.20, average training loss: 6206.18, base loss: 16008.26
[INFO 2017-06-30 03:41:38,515 main.py:52] epoch 3044, training loss: 5747.13, average training loss: 6204.70, base loss: 16007.10
[INFO 2017-06-30 03:41:41,716 main.py:52] epoch 3045, training loss: 6093.85, average training loss: 6204.06, base loss: 16007.75
[INFO 2017-06-30 03:41:44,893 main.py:52] epoch 3046, training loss: 5707.52, average training loss: 6203.25, base loss: 16007.42
[INFO 2017-06-30 03:41:48,043 main.py:52] epoch 3047, training loss: 5894.03, average training loss: 6202.58, base loss: 16007.24
[INFO 2017-06-30 03:41:51,197 main.py:52] epoch 3048, training loss: 6117.92, average training loss: 6202.56, base loss: 16007.20
[INFO 2017-06-30 03:41:54,381 main.py:52] epoch 3049, training loss: 6069.40, average training loss: 6202.55, base loss: 16007.20
[INFO 2017-06-30 03:41:57,562 main.py:52] epoch 3050, training loss: 5659.45, average training loss: 6201.93, base loss: 16006.86
[INFO 2017-06-30 03:42:00,705 main.py:52] epoch 3051, training loss: 5971.25, average training loss: 6201.50, base loss: 16006.14
[INFO 2017-06-30 03:42:03,893 main.py:52] epoch 3052, training loss: 6117.12, average training loss: 6201.26, base loss: 16006.53
[INFO 2017-06-30 03:42:07,055 main.py:52] epoch 3053, training loss: 6349.49, average training loss: 6200.81, base loss: 16007.15
[INFO 2017-06-30 03:42:10,196 main.py:52] epoch 3054, training loss: 6038.26, average training loss: 6201.06, base loss: 16007.50
[INFO 2017-06-30 03:42:13,338 main.py:52] epoch 3055, training loss: 6316.27, average training loss: 6201.26, base loss: 16007.78
[INFO 2017-06-30 03:42:16,485 main.py:52] epoch 3056, training loss: 6275.25, average training loss: 6201.14, base loss: 16008.63
[INFO 2017-06-30 03:42:19,652 main.py:52] epoch 3057, training loss: 5742.11, average training loss: 6200.62, base loss: 16008.96
[INFO 2017-06-30 03:42:22,857 main.py:52] epoch 3058, training loss: 6646.24, average training loss: 6201.04, base loss: 16008.49
[INFO 2017-06-30 03:42:26,015 main.py:52] epoch 3059, training loss: 5917.77, average training loss: 6200.73, base loss: 16008.05
[INFO 2017-06-30 03:42:29,196 main.py:52] epoch 3060, training loss: 5809.57, average training loss: 6200.20, base loss: 16007.30
[INFO 2017-06-30 03:42:32,323 main.py:52] epoch 3061, training loss: 5822.88, average training loss: 6199.57, base loss: 16006.10
[INFO 2017-06-30 03:42:35,481 main.py:52] epoch 3062, training loss: 6092.63, average training loss: 6199.11, base loss: 16006.18
[INFO 2017-06-30 03:42:38,622 main.py:52] epoch 3063, training loss: 6186.69, average training loss: 6198.60, base loss: 16005.93
[INFO 2017-06-30 03:42:41,794 main.py:52] epoch 3064, training loss: 5884.50, average training loss: 6198.13, base loss: 16005.43
[INFO 2017-06-30 03:42:44,936 main.py:52] epoch 3065, training loss: 5898.98, average training loss: 6197.83, base loss: 16005.75
[INFO 2017-06-30 03:42:48,073 main.py:52] epoch 3066, training loss: 5702.52, average training loss: 6197.34, base loss: 16005.46
[INFO 2017-06-30 03:42:51,256 main.py:52] epoch 3067, training loss: 5924.19, average training loss: 6197.04, base loss: 16005.86
[INFO 2017-06-30 03:42:54,389 main.py:52] epoch 3068, training loss: 5739.48, average training loss: 6196.44, base loss: 16005.29
[INFO 2017-06-30 03:42:57,611 main.py:52] epoch 3069, training loss: 6052.68, average training loss: 6195.85, base loss: 16005.49
[INFO 2017-06-30 03:43:00,773 main.py:52] epoch 3070, training loss: 6284.84, average training loss: 6196.16, base loss: 16005.55
[INFO 2017-06-30 03:43:03,945 main.py:52] epoch 3071, training loss: 6183.23, average training loss: 6195.63, base loss: 16005.50
[INFO 2017-06-30 03:43:07,106 main.py:52] epoch 3072, training loss: 5850.99, average training loss: 6195.00, base loss: 16005.34
[INFO 2017-06-30 03:43:10,320 main.py:52] epoch 3073, training loss: 5758.26, average training loss: 6194.25, base loss: 16005.03
[INFO 2017-06-30 03:43:13,457 main.py:52] epoch 3074, training loss: 6010.97, average training loss: 6193.64, base loss: 16004.71
[INFO 2017-06-30 03:43:16,663 main.py:52] epoch 3075, training loss: 6066.78, average training loss: 6193.48, base loss: 16004.22
[INFO 2017-06-30 03:43:19,837 main.py:52] epoch 3076, training loss: 5617.11, average training loss: 6192.10, base loss: 16002.03
[INFO 2017-06-30 03:43:22,966 main.py:52] epoch 3077, training loss: 5933.97, average training loss: 6191.50, base loss: 16001.10
[INFO 2017-06-30 03:43:26,127 main.py:52] epoch 3078, training loss: 5898.35, average training loss: 6190.90, base loss: 16000.78
[INFO 2017-06-30 03:43:29,295 main.py:52] epoch 3079, training loss: 5926.64, average training loss: 6190.80, base loss: 16001.00
[INFO 2017-06-30 03:43:32,480 main.py:52] epoch 3080, training loss: 6385.61, average training loss: 6191.48, base loss: 16000.98
[INFO 2017-06-30 03:43:35,655 main.py:52] epoch 3081, training loss: 5921.06, average training loss: 6189.70, base loss: 16000.18
[INFO 2017-06-30 03:43:38,810 main.py:52] epoch 3082, training loss: 6045.73, average training loss: 6189.37, base loss: 15999.21
[INFO 2017-06-30 03:43:42,019 main.py:52] epoch 3083, training loss: 5912.55, average training loss: 6188.54, base loss: 15998.70
[INFO 2017-06-30 03:43:45,209 main.py:52] epoch 3084, training loss: 6216.07, average training loss: 6188.41, base loss: 15999.16
[INFO 2017-06-30 03:43:48,430 main.py:52] epoch 3085, training loss: 6364.48, average training loss: 6187.99, base loss: 15999.92
[INFO 2017-06-30 03:43:51,686 main.py:52] epoch 3086, training loss: 5962.75, average training loss: 6187.75, base loss: 15999.64
[INFO 2017-06-30 03:43:54,909 main.py:52] epoch 3087, training loss: 6281.07, average training loss: 6187.57, base loss: 15999.58
[INFO 2017-06-30 03:43:58,064 main.py:52] epoch 3088, training loss: 5764.34, average training loss: 6187.33, base loss: 15998.87
[INFO 2017-06-30 03:44:01,223 main.py:52] epoch 3089, training loss: 6312.12, average training loss: 6187.15, base loss: 15999.46
[INFO 2017-06-30 03:44:04,384 main.py:52] epoch 3090, training loss: 6123.32, average training loss: 6187.12, base loss: 15999.63
[INFO 2017-06-30 03:44:07,598 main.py:52] epoch 3091, training loss: 6091.26, average training loss: 6186.45, base loss: 15999.04
[INFO 2017-06-30 03:44:10,784 main.py:52] epoch 3092, training loss: 6420.71, average training loss: 6186.58, base loss: 16000.06
[INFO 2017-06-30 03:44:13,988 main.py:52] epoch 3093, training loss: 5643.98, average training loss: 6185.48, base loss: 15999.60
[INFO 2017-06-30 03:44:17,164 main.py:52] epoch 3094, training loss: 6159.05, average training loss: 6185.15, base loss: 15999.82
[INFO 2017-06-30 03:44:20,372 main.py:52] epoch 3095, training loss: 5969.92, average training loss: 6184.73, base loss: 15999.64
[INFO 2017-06-30 03:44:23,497 main.py:52] epoch 3096, training loss: 5983.11, average training loss: 6184.26, base loss: 15999.62
[INFO 2017-06-30 03:44:26,673 main.py:52] epoch 3097, training loss: 5872.57, average training loss: 6183.91, base loss: 15998.83
[INFO 2017-06-30 03:44:29,857 main.py:52] epoch 3098, training loss: 5925.38, average training loss: 6183.80, base loss: 15998.76
[INFO 2017-06-30 03:44:33,030 main.py:52] epoch 3099, training loss: 6075.61, average training loss: 6183.52, base loss: 15999.13
[INFO 2017-06-30 03:44:33,030 main.py:54] epoch 3099, testing
[INFO 2017-06-30 03:44:46,346 main.py:97] average testing loss: 6108.55, base loss: 16303.67
[INFO 2017-06-30 03:44:46,346 main.py:98] improve_loss: 10195.12, improve_percent: 0.63
[INFO 2017-06-30 03:44:46,350 main.py:62] model save to ./model/final.pth
[INFO 2017-06-30 03:44:46,384 main.py:66] current best improved percent: 0.63
[INFO 2017-06-30 03:44:49,593 main.py:52] epoch 3100, training loss: 6163.91, average training loss: 6183.10, base loss: 15999.69
[INFO 2017-06-30 03:44:52,780 main.py:52] epoch 3101, training loss: 6219.78, average training loss: 6183.17, base loss: 16000.65
[INFO 2017-06-30 03:44:55,970 main.py:52] epoch 3102, training loss: 5656.49, average training loss: 6182.70, base loss: 15999.93
[INFO 2017-06-30 03:44:59,150 main.py:52] epoch 3103, training loss: 5667.90, average training loss: 6181.20, base loss: 15999.51
[INFO 2017-06-30 03:45:02,335 main.py:52] epoch 3104, training loss: 6572.97, average training loss: 6180.97, base loss: 16000.79
[INFO 2017-06-30 03:45:05,532 main.py:52] epoch 3105, training loss: 5620.47, average training loss: 6180.87, base loss: 15999.79
[INFO 2017-06-30 03:45:08,668 main.py:52] epoch 3106, training loss: 6048.01, average training loss: 6180.28, base loss: 15999.77
[INFO 2017-06-30 03:45:11,811 main.py:52] epoch 3107, training loss: 6258.29, average training loss: 6180.15, base loss: 16000.16
[INFO 2017-06-30 03:45:14,957 main.py:52] epoch 3108, training loss: 6320.85, average training loss: 6179.76, base loss: 16000.86
[INFO 2017-06-30 03:45:18,191 main.py:52] epoch 3109, training loss: 6132.23, average training loss: 6179.66, base loss: 16001.38
[INFO 2017-06-30 03:45:21,353 main.py:52] epoch 3110, training loss: 6117.58, average training loss: 6179.26, base loss: 16000.85
[INFO 2017-06-30 03:45:24,512 main.py:52] epoch 3111, training loss: 6129.49, average training loss: 6179.02, base loss: 16000.95
[INFO 2017-06-30 03:45:27,641 main.py:52] epoch 3112, training loss: 6378.00, average training loss: 6178.99, base loss: 16000.74
[INFO 2017-06-30 03:45:30,786 main.py:52] epoch 3113, training loss: 5690.71, average training loss: 6178.56, base loss: 15999.26
[INFO 2017-06-30 03:45:33,985 main.py:52] epoch 3114, training loss: 6383.15, average training loss: 6178.27, base loss: 16000.23
[INFO 2017-06-30 03:45:37,203 main.py:52] epoch 3115, training loss: 6024.29, average training loss: 6178.19, base loss: 16000.28
[INFO 2017-06-30 03:45:40,383 main.py:52] epoch 3116, training loss: 6236.20, average training loss: 6178.28, base loss: 15999.93
[INFO 2017-06-30 03:45:43,564 main.py:52] epoch 3117, training loss: 6434.96, average training loss: 6178.69, base loss: 16000.54
[INFO 2017-06-30 03:45:46,755 main.py:52] epoch 3118, training loss: 5879.82, average training loss: 6178.19, base loss: 15999.74
[INFO 2017-06-30 03:45:49,886 main.py:52] epoch 3119, training loss: 5879.37, average training loss: 6177.30, base loss: 15999.55
[INFO 2017-06-30 03:45:53,071 main.py:52] epoch 3120, training loss: 5541.92, average training loss: 6176.54, base loss: 15998.53
[INFO 2017-06-30 03:45:56,222 main.py:52] epoch 3121, training loss: 6067.00, average training loss: 6176.40, base loss: 15998.97
[INFO 2017-06-30 03:45:59,381 main.py:52] epoch 3122, training loss: 6169.90, average training loss: 6176.42, base loss: 15998.56
[INFO 2017-06-30 03:46:02,549 main.py:52] epoch 3123, training loss: 6295.68, average training loss: 6175.89, base loss: 15999.11
[INFO 2017-06-30 03:46:05,723 main.py:52] epoch 3124, training loss: 5623.54, average training loss: 6175.28, base loss: 15998.25
[INFO 2017-06-30 03:46:08,918 main.py:52] epoch 3125, training loss: 6090.96, average training loss: 6175.22, base loss: 15998.72
[INFO 2017-06-30 03:46:12,072 main.py:52] epoch 3126, training loss: 5914.59, average training loss: 6175.16, base loss: 15998.50
[INFO 2017-06-30 03:46:15,236 main.py:52] epoch 3127, training loss: 6145.28, average training loss: 6174.93, base loss: 15998.48
[INFO 2017-06-30 03:46:18,388 main.py:52] epoch 3128, training loss: 5843.68, average training loss: 6174.50, base loss: 15998.22
[INFO 2017-06-30 03:46:21,590 main.py:52] epoch 3129, training loss: 6006.78, average training loss: 6173.97, base loss: 15998.62
[INFO 2017-06-30 03:46:24,758 main.py:52] epoch 3130, training loss: 6479.02, average training loss: 6174.28, base loss: 15999.33
[INFO 2017-06-30 03:46:27,961 main.py:52] epoch 3131, training loss: 6270.11, average training loss: 6174.39, base loss: 15999.27
[INFO 2017-06-30 03:46:31,167 main.py:52] epoch 3132, training loss: 6006.63, average training loss: 6174.07, base loss: 15999.16
[INFO 2017-06-30 03:46:34,326 main.py:52] epoch 3133, training loss: 5826.52, average training loss: 6173.79, base loss: 15998.64
[INFO 2017-06-30 03:46:37,498 main.py:52] epoch 3134, training loss: 6120.67, average training loss: 6173.38, base loss: 15998.21
[INFO 2017-06-30 03:46:40,665 main.py:52] epoch 3135, training loss: 5953.28, average training loss: 6172.86, base loss: 15998.03
[INFO 2017-06-30 03:46:43,792 main.py:52] epoch 3136, training loss: 6401.60, average training loss: 6172.75, base loss: 15999.08
[INFO 2017-06-30 03:46:46,972 main.py:52] epoch 3137, training loss: 5999.12, average training loss: 6172.22, base loss: 15999.69
[INFO 2017-06-30 03:46:50,137 main.py:52] epoch 3138, training loss: 6215.46, average training loss: 6171.70, base loss: 16000.01
[INFO 2017-06-30 03:46:53,288 main.py:52] epoch 3139, training loss: 6067.80, average training loss: 6171.62, base loss: 15999.81
[INFO 2017-06-30 03:46:56,492 main.py:52] epoch 3140, training loss: 5838.00, average training loss: 6171.83, base loss: 16000.31
[INFO 2017-06-30 03:46:59,657 main.py:52] epoch 3141, training loss: 6550.05, average training loss: 6172.22, base loss: 16001.97
[INFO 2017-06-30 03:47:02,809 main.py:52] epoch 3142, training loss: 5744.20, average training loss: 6171.46, base loss: 16001.57
[INFO 2017-06-30 03:47:05,985 main.py:52] epoch 3143, training loss: 5680.77, average training loss: 6171.15, base loss: 16000.48
[INFO 2017-06-30 03:47:09,141 main.py:52] epoch 3144, training loss: 6479.38, average training loss: 6171.50, base loss: 16001.46
[INFO 2017-06-30 03:47:12,314 main.py:52] epoch 3145, training loss: 6240.22, average training loss: 6171.46, base loss: 16001.97
[INFO 2017-06-30 03:47:15,509 main.py:52] epoch 3146, training loss: 5703.21, average training loss: 6170.96, base loss: 16001.32
[INFO 2017-06-30 03:47:18,711 main.py:52] epoch 3147, training loss: 6093.41, average training loss: 6170.72, base loss: 16002.39
[INFO 2017-06-30 03:47:21,898 main.py:52] epoch 3148, training loss: 6231.69, average training loss: 6170.07, base loss: 16003.10
[INFO 2017-06-30 03:47:25,067 main.py:52] epoch 3149, training loss: 6074.69, average training loss: 6169.93, base loss: 16002.57
[INFO 2017-06-30 03:47:28,235 main.py:52] epoch 3150, training loss: 5623.43, average training loss: 6169.38, base loss: 16002.57
[INFO 2017-06-30 03:47:31,396 main.py:52] epoch 3151, training loss: 6120.89, average training loss: 6169.23, base loss: 16003.08
[INFO 2017-06-30 03:47:34,614 main.py:52] epoch 3152, training loss: 5984.66, average training loss: 6169.13, base loss: 16003.25
[INFO 2017-06-30 03:47:37,771 main.py:52] epoch 3153, training loss: 6129.51, average training loss: 6168.86, base loss: 16003.32
[INFO 2017-06-30 03:47:40,933 main.py:52] epoch 3154, training loss: 5904.85, average training loss: 6168.27, base loss: 16003.48
[INFO 2017-06-30 03:47:44,083 main.py:52] epoch 3155, training loss: 5901.03, average training loss: 6168.03, base loss: 16003.81
[INFO 2017-06-30 03:47:47,228 main.py:52] epoch 3156, training loss: 5800.21, average training loss: 6167.41, base loss: 16003.77
[INFO 2017-06-30 03:47:50,382 main.py:52] epoch 3157, training loss: 5885.21, average training loss: 6166.86, base loss: 16004.08
[INFO 2017-06-30 03:47:53,565 main.py:52] epoch 3158, training loss: 6355.78, average training loss: 6166.90, base loss: 16004.18
[INFO 2017-06-30 03:47:56,698 main.py:52] epoch 3159, training loss: 5902.27, average training loss: 6166.49, base loss: 16003.27
[INFO 2017-06-30 03:47:59,918 main.py:52] epoch 3160, training loss: 6054.11, average training loss: 6166.16, base loss: 16003.26
[INFO 2017-06-30 03:48:03,132 main.py:52] epoch 3161, training loss: 6215.66, average training loss: 6166.39, base loss: 16003.81
[INFO 2017-06-30 03:48:06,285 main.py:52] epoch 3162, training loss: 6024.95, average training loss: 6165.63, base loss: 16003.95
[INFO 2017-06-30 03:48:09,484 main.py:52] epoch 3163, training loss: 6071.82, average training loss: 6165.70, base loss: 16004.68
[INFO 2017-06-30 03:48:12,700 main.py:52] epoch 3164, training loss: 5719.75, average training loss: 6165.16, base loss: 16004.46
[INFO 2017-06-30 03:48:15,927 main.py:52] epoch 3165, training loss: 5829.97, average training loss: 6164.61, base loss: 16003.81
[INFO 2017-06-30 03:48:19,089 main.py:52] epoch 3166, training loss: 6187.04, average training loss: 6164.88, base loss: 16004.41
[INFO 2017-06-30 03:48:22,233 main.py:52] epoch 3167, training loss: 5941.72, average training loss: 6164.86, base loss: 16004.51
[INFO 2017-06-30 03:48:25,391 main.py:52] epoch 3168, training loss: 5784.55, average training loss: 6164.44, base loss: 16003.37
[INFO 2017-06-30 03:48:28,580 main.py:52] epoch 3169, training loss: 5927.98, average training loss: 6164.00, base loss: 16003.38
[INFO 2017-06-30 03:48:31,749 main.py:52] epoch 3170, training loss: 6323.40, average training loss: 6163.85, base loss: 16003.32
[INFO 2017-06-30 03:48:34,941 main.py:52] epoch 3171, training loss: 6046.99, average training loss: 6163.47, base loss: 16002.83
[INFO 2017-06-30 03:48:38,133 main.py:52] epoch 3172, training loss: 6665.76, average training loss: 6163.47, base loss: 16003.18
[INFO 2017-06-30 03:48:41,295 main.py:52] epoch 3173, training loss: 6181.31, average training loss: 6163.21, base loss: 16002.89
[INFO 2017-06-30 03:48:44,509 main.py:52] epoch 3174, training loss: 5946.16, average training loss: 6162.46, base loss: 16002.37
[INFO 2017-06-30 03:48:47,685 main.py:52] epoch 3175, training loss: 6018.02, average training loss: 6161.69, base loss: 16002.11
[INFO 2017-06-30 03:48:50,838 main.py:52] epoch 3176, training loss: 6344.33, average training loss: 6162.00, base loss: 16002.56
[INFO 2017-06-30 03:48:54,003 main.py:52] epoch 3177, training loss: 5496.55, average training loss: 6161.08, base loss: 16000.95
[INFO 2017-06-30 03:48:57,163 main.py:52] epoch 3178, training loss: 5674.64, average training loss: 6160.51, base loss: 16000.41
[INFO 2017-06-30 03:49:00,314 main.py:52] epoch 3179, training loss: 6500.60, average training loss: 6160.88, base loss: 16000.94
[INFO 2017-06-30 03:49:03,500 main.py:52] epoch 3180, training loss: 6115.63, average training loss: 6160.76, base loss: 16002.10
[INFO 2017-06-30 03:49:06,658 main.py:52] epoch 3181, training loss: 5754.19, average training loss: 6160.36, base loss: 16002.15
[INFO 2017-06-30 03:49:09,836 main.py:52] epoch 3182, training loss: 6181.85, average training loss: 6159.80, base loss: 16002.07
[INFO 2017-06-30 03:49:13,084 main.py:52] epoch 3183, training loss: 6371.52, average training loss: 6159.83, base loss: 16001.99
[INFO 2017-06-30 03:49:16,277 main.py:52] epoch 3184, training loss: 5877.79, average training loss: 6159.21, base loss: 16001.73
[INFO 2017-06-30 03:49:19,462 main.py:52] epoch 3185, training loss: 5799.40, average training loss: 6158.52, base loss: 16001.12
[INFO 2017-06-30 03:49:22,574 main.py:52] epoch 3186, training loss: 5608.73, average training loss: 6157.28, base loss: 16001.36
[INFO 2017-06-30 03:49:25,841 main.py:52] epoch 3187, training loss: 6026.09, average training loss: 6157.12, base loss: 16001.33
[INFO 2017-06-30 03:49:29,027 main.py:52] epoch 3188, training loss: 5894.26, average training loss: 6157.06, base loss: 16000.57
[INFO 2017-06-30 03:49:32,206 main.py:52] epoch 3189, training loss: 6182.04, average training loss: 6156.49, base loss: 16000.14
[INFO 2017-06-30 03:49:35,388 main.py:52] epoch 3190, training loss: 5632.66, average training loss: 6155.50, base loss: 15998.97
[INFO 2017-06-30 03:49:38,578 main.py:52] epoch 3191, training loss: 6119.91, average training loss: 6155.36, base loss: 15999.06
[INFO 2017-06-30 03:49:41,766 main.py:52] epoch 3192, training loss: 5806.45, average training loss: 6154.88, base loss: 15999.15
[INFO 2017-06-30 03:49:44,945 main.py:52] epoch 3193, training loss: 6105.32, average training loss: 6154.18, base loss: 15999.23
[INFO 2017-06-30 03:49:48,128 main.py:52] epoch 3194, training loss: 5770.69, average training loss: 6153.97, base loss: 15999.40
[INFO 2017-06-30 03:49:51,272 main.py:52] epoch 3195, training loss: 5680.70, average training loss: 6153.42, base loss: 16000.07
[INFO 2017-06-30 03:49:54,423 main.py:52] epoch 3196, training loss: 6001.63, average training loss: 6153.11, base loss: 15999.84
[INFO 2017-06-30 03:49:57,542 main.py:52] epoch 3197, training loss: 6376.91, average training loss: 6153.63, base loss: 15999.85
[INFO 2017-06-30 03:50:00,740 main.py:52] epoch 3198, training loss: 6031.34, average training loss: 6153.78, base loss: 16000.23
[INFO 2017-06-30 03:50:03,945 main.py:52] epoch 3199, training loss: 6153.97, average training loss: 6152.86, base loss: 15999.60
[INFO 2017-06-30 03:50:03,946 main.py:54] epoch 3199, testing
[INFO 2017-06-30 03:50:17,317 main.py:97] average testing loss: 5974.71, base loss: 15808.42
[INFO 2017-06-30 03:50:17,317 main.py:98] improve_loss: 9833.71, improve_percent: 0.62
[INFO 2017-06-30 03:50:17,319 main.py:66] current best improved percent: 0.63
[INFO 2017-06-30 03:50:20,448 main.py:52] epoch 3200, training loss: 6273.40, average training loss: 6153.28, base loss: 15999.43
[INFO 2017-06-30 03:50:23,606 main.py:52] epoch 3201, training loss: 6396.57, average training loss: 6153.29, base loss: 15998.52
[INFO 2017-06-30 03:50:26,813 main.py:52] epoch 3202, training loss: 5844.19, average training loss: 6152.67, base loss: 15997.87
[INFO 2017-06-30 03:50:30,006 main.py:52] epoch 3203, training loss: 6209.25, average training loss: 6152.72, base loss: 15997.74
[INFO 2017-06-30 03:50:33,213 main.py:52] epoch 3204, training loss: 5571.29, average training loss: 6151.60, base loss: 15996.89
[INFO 2017-06-30 03:50:36,417 main.py:52] epoch 3205, training loss: 6319.62, average training loss: 6151.51, base loss: 15997.97
[INFO 2017-06-30 03:50:39,569 main.py:52] epoch 3206, training loss: 5965.15, average training loss: 6151.01, base loss: 15998.51
[INFO 2017-06-30 03:50:42,735 main.py:52] epoch 3207, training loss: 5754.06, average training loss: 6150.63, base loss: 15998.04
[INFO 2017-06-30 03:50:45,910 main.py:52] epoch 3208, training loss: 5759.46, average training loss: 6150.33, base loss: 15998.08
[INFO 2017-06-30 03:50:49,088 main.py:52] epoch 3209, training loss: 6455.81, average training loss: 6150.54, base loss: 15999.15
[INFO 2017-06-30 03:50:52,286 main.py:52] epoch 3210, training loss: 6291.02, average training loss: 6150.11, base loss: 16000.63
[INFO 2017-06-30 03:50:55,452 main.py:52] epoch 3211, training loss: 6157.43, average training loss: 6150.23, base loss: 16001.88
[INFO 2017-06-30 03:50:58,582 main.py:52] epoch 3212, training loss: 6152.00, average training loss: 6150.01, base loss: 16002.44
[INFO 2017-06-30 03:51:01,766 main.py:52] epoch 3213, training loss: 5931.16, average training loss: 6149.74, base loss: 16001.81
[INFO 2017-06-30 03:51:04,951 main.py:52] epoch 3214, training loss: 5962.86, average training loss: 6149.57, base loss: 16001.59
[INFO 2017-06-30 03:51:08,106 main.py:52] epoch 3215, training loss: 6876.18, average training loss: 6149.98, base loss: 16002.66
[INFO 2017-06-30 03:51:11,283 main.py:52] epoch 3216, training loss: 5949.74, average training loss: 6149.61, base loss: 16002.67
[INFO 2017-06-30 03:51:14,445 main.py:52] epoch 3217, training loss: 6149.59, average training loss: 6149.42, base loss: 16002.39
[INFO 2017-06-30 03:51:17,635 main.py:52] epoch 3218, training loss: 5775.62, average training loss: 6148.82, base loss: 16002.57
[INFO 2017-06-30 03:51:20,772 main.py:52] epoch 3219, training loss: 6625.51, average training loss: 6148.48, base loss: 16003.40
[INFO 2017-06-30 03:51:23,959 main.py:52] epoch 3220, training loss: 6079.61, average training loss: 6148.02, base loss: 16002.81
[INFO 2017-06-30 03:51:27,099 main.py:52] epoch 3221, training loss: 6048.28, average training loss: 6147.78, base loss: 16002.78
[INFO 2017-06-30 03:51:30,312 main.py:52] epoch 3222, training loss: 6241.48, average training loss: 6147.95, base loss: 16002.29
[INFO 2017-06-30 03:51:33,489 main.py:52] epoch 3223, training loss: 5835.72, average training loss: 6147.42, base loss: 16001.33
[INFO 2017-06-30 03:51:36,634 main.py:52] epoch 3224, training loss: 6138.23, average training loss: 6146.74, base loss: 16001.11
[INFO 2017-06-30 03:51:39,803 main.py:52] epoch 3225, training loss: 5815.55, average training loss: 6146.40, base loss: 16000.36
[INFO 2017-06-30 03:51:42,957 main.py:52] epoch 3226, training loss: 6154.25, average training loss: 6146.12, base loss: 16000.79
[INFO 2017-06-30 03:51:46,160 main.py:52] epoch 3227, training loss: 6031.06, average training loss: 6145.78, base loss: 16000.78
[INFO 2017-06-30 03:51:49,326 main.py:52] epoch 3228, training loss: 6007.06, average training loss: 6145.66, base loss: 16000.76
[INFO 2017-06-30 03:51:52,504 main.py:52] epoch 3229, training loss: 6141.29, average training loss: 6145.30, base loss: 16001.70
[INFO 2017-06-30 03:51:55,668 main.py:52] epoch 3230, training loss: 6039.73, average training loss: 6144.71, base loss: 16002.08
[INFO 2017-06-30 03:51:58,843 main.py:52] epoch 3231, training loss: 5624.41, average training loss: 6144.06, base loss: 16001.32
[INFO 2017-06-30 03:52:02,005 main.py:52] epoch 3232, training loss: 6307.58, average training loss: 6144.22, base loss: 16001.34
[INFO 2017-06-30 03:52:05,180 main.py:52] epoch 3233, training loss: 6459.83, average training loss: 6144.15, base loss: 16001.60
[INFO 2017-06-30 03:52:08,361 main.py:52] epoch 3234, training loss: 6235.72, average training loss: 6143.90, base loss: 16002.30
[INFO 2017-06-30 03:52:11,526 main.py:52] epoch 3235, training loss: 5256.41, average training loss: 6142.72, base loss: 16000.90
[INFO 2017-06-30 03:52:14,693 main.py:52] epoch 3236, training loss: 5855.44, average training loss: 6142.27, base loss: 16000.85
[INFO 2017-06-30 03:52:17,875 main.py:52] epoch 3237, training loss: 5857.49, average training loss: 6141.54, base loss: 16000.33
[INFO 2017-06-30 03:52:21,051 main.py:52] epoch 3238, training loss: 5910.58, average training loss: 6141.17, base loss: 16000.44
[INFO 2017-06-30 03:52:24,216 main.py:52] epoch 3239, training loss: 6154.75, average training loss: 6140.81, base loss: 16001.45
[INFO 2017-06-30 03:52:27,384 main.py:52] epoch 3240, training loss: 6327.93, average training loss: 6141.03, base loss: 16001.21
[INFO 2017-06-30 03:52:30,527 main.py:52] epoch 3241, training loss: 5761.34, average training loss: 6140.28, base loss: 16000.17
[INFO 2017-06-30 03:52:33,686 main.py:52] epoch 3242, training loss: 6423.57, average training loss: 6140.24, base loss: 16000.66
[INFO 2017-06-30 03:52:36,847 main.py:52] epoch 3243, training loss: 6103.20, average training loss: 6140.24, base loss: 16000.72
[INFO 2017-06-30 03:52:40,000 main.py:52] epoch 3244, training loss: 6137.06, average training loss: 6140.47, base loss: 16000.67
[INFO 2017-06-30 03:52:43,184 main.py:52] epoch 3245, training loss: 5842.99, average training loss: 6139.80, base loss: 16000.63
[INFO 2017-06-30 03:52:46,350 main.py:52] epoch 3246, training loss: 6108.49, average training loss: 6139.50, base loss: 16000.34
[INFO 2017-06-30 03:52:49,527 main.py:52] epoch 3247, training loss: 5722.52, average training loss: 6139.18, base loss: 15999.43
[INFO 2017-06-30 03:52:52,663 main.py:52] epoch 3248, training loss: 5879.17, average training loss: 6139.21, base loss: 15999.27
[INFO 2017-06-30 03:52:55,854 main.py:52] epoch 3249, training loss: 5973.27, average training loss: 6139.19, base loss: 15998.50
[INFO 2017-06-30 03:52:59,048 main.py:52] epoch 3250, training loss: 5600.73, average training loss: 6138.82, base loss: 15997.63
[INFO 2017-06-30 03:53:02,244 main.py:52] epoch 3251, training loss: 5983.09, average training loss: 6137.98, base loss: 15996.81
[INFO 2017-06-30 03:53:05,413 main.py:52] epoch 3252, training loss: 5343.16, average training loss: 6137.15, base loss: 15995.94
[INFO 2017-06-30 03:53:08,583 main.py:52] epoch 3253, training loss: 6351.14, average training loss: 6137.72, base loss: 15996.67
[INFO 2017-06-30 03:53:11,791 main.py:52] epoch 3254, training loss: 5657.92, average training loss: 6136.95, base loss: 15995.61
[INFO 2017-06-30 03:53:14,964 main.py:52] epoch 3255, training loss: 5986.88, average training loss: 6136.90, base loss: 15994.97
[INFO 2017-06-30 03:53:18,170 main.py:52] epoch 3256, training loss: 6213.40, average training loss: 6136.82, base loss: 15995.09
[INFO 2017-06-30 03:53:21,372 main.py:52] epoch 3257, training loss: 5861.55, average training loss: 6136.77, base loss: 15994.40
[INFO 2017-06-30 03:53:24,560 main.py:52] epoch 3258, training loss: 6085.04, average training loss: 6136.82, base loss: 15994.48
[INFO 2017-06-30 03:53:27,679 main.py:52] epoch 3259, training loss: 5604.78, average training loss: 6136.24, base loss: 15994.23
[INFO 2017-06-30 03:53:30,856 main.py:52] epoch 3260, training loss: 5730.16, average training loss: 6135.80, base loss: 15993.73
[INFO 2017-06-30 03:53:34,052 main.py:52] epoch 3261, training loss: 5706.50, average training loss: 6134.67, base loss: 15992.90
[INFO 2017-06-30 03:53:37,243 main.py:52] epoch 3262, training loss: 5872.22, average training loss: 6134.17, base loss: 15992.79
[INFO 2017-06-30 03:53:40,434 main.py:52] epoch 3263, training loss: 6383.51, average training loss: 6134.33, base loss: 15993.01
[INFO 2017-06-30 03:53:43,659 main.py:52] epoch 3264, training loss: 6043.90, average training loss: 6134.35, base loss: 15993.47
[INFO 2017-06-30 03:53:46,855 main.py:52] epoch 3265, training loss: 5891.32, average training loss: 6133.80, base loss: 15993.31
[INFO 2017-06-30 03:53:50,009 main.py:52] epoch 3266, training loss: 5901.28, average training loss: 6133.49, base loss: 15992.70
[INFO 2017-06-30 03:53:53,207 main.py:52] epoch 3267, training loss: 6010.35, average training loss: 6133.31, base loss: 15991.74
[INFO 2017-06-30 03:53:56,416 main.py:52] epoch 3268, training loss: 6315.49, average training loss: 6132.97, base loss: 15992.36
[INFO 2017-06-30 03:53:59,570 main.py:52] epoch 3269, training loss: 5861.21, average training loss: 6132.26, base loss: 15991.36
[INFO 2017-06-30 03:54:02,750 main.py:52] epoch 3270, training loss: 5808.88, average training loss: 6132.09, base loss: 15990.78
[INFO 2017-06-30 03:54:05,947 main.py:52] epoch 3271, training loss: 6118.09, average training loss: 6132.92, base loss: 15991.20
[INFO 2017-06-30 03:54:09,118 main.py:52] epoch 3272, training loss: 5885.88, average training loss: 6132.71, base loss: 15990.46
[INFO 2017-06-30 03:54:12,284 main.py:52] epoch 3273, training loss: 6065.85, average training loss: 6132.78, base loss: 15990.41
[INFO 2017-06-30 03:54:15,465 main.py:52] epoch 3274, training loss: 5789.61, average training loss: 6132.11, base loss: 15990.26
[INFO 2017-06-30 03:54:18,620 main.py:52] epoch 3275, training loss: 6127.42, average training loss: 6131.96, base loss: 15991.35
[INFO 2017-06-30 03:54:21,804 main.py:52] epoch 3276, training loss: 5850.45, average training loss: 6130.90, base loss: 15991.93
[INFO 2017-06-30 03:54:24,929 main.py:52] epoch 3277, training loss: 5848.55, average training loss: 6130.49, base loss: 15992.36
[INFO 2017-06-30 03:54:28,100 main.py:52] epoch 3278, training loss: 5984.87, average training loss: 6130.28, base loss: 15992.26
[INFO 2017-06-30 03:54:31,249 main.py:52] epoch 3279, training loss: 6097.15, average training loss: 6130.11, base loss: 15992.00
[INFO 2017-06-30 03:54:34,439 main.py:52] epoch 3280, training loss: 6352.53, average training loss: 6130.21, base loss: 15992.81
[INFO 2017-06-30 03:54:37,593 main.py:52] epoch 3281, training loss: 5828.80, average training loss: 6129.88, base loss: 15992.24
[INFO 2017-06-30 03:54:40,824 main.py:52] epoch 3282, training loss: 5816.67, average training loss: 6129.55, base loss: 15991.14
[INFO 2017-06-30 03:54:43,959 main.py:52] epoch 3283, training loss: 5922.96, average training loss: 6128.84, base loss: 15990.49
[INFO 2017-06-30 03:54:47,132 main.py:52] epoch 3284, training loss: 6168.36, average training loss: 6129.04, base loss: 15991.40
[INFO 2017-06-30 03:54:50,297 main.py:52] epoch 3285, training loss: 6388.16, average training loss: 6128.81, base loss: 15992.54
[INFO 2017-06-30 03:54:53,494 main.py:52] epoch 3286, training loss: 5566.83, average training loss: 6128.19, base loss: 15991.73
[INFO 2017-06-30 03:54:56,704 main.py:52] epoch 3287, training loss: 5609.65, average training loss: 6127.47, base loss: 15990.86
[INFO 2017-06-30 03:54:59,866 main.py:52] epoch 3288, training loss: 6478.34, average training loss: 6127.51, base loss: 15991.19
[INFO 2017-06-30 03:55:03,036 main.py:52] epoch 3289, training loss: 5587.04, average training loss: 6126.96, base loss: 15990.05
[INFO 2017-06-30 03:55:06,212 main.py:52] epoch 3290, training loss: 5914.91, average training loss: 6126.58, base loss: 15989.39
[INFO 2017-06-30 03:55:09,374 main.py:52] epoch 3291, training loss: 6225.73, average training loss: 6126.72, base loss: 15989.68
[INFO 2017-06-30 03:55:12,500 main.py:52] epoch 3292, training loss: 6174.63, average training loss: 6126.39, base loss: 15990.58
[INFO 2017-06-30 03:55:15,686 main.py:52] epoch 3293, training loss: 5957.28, average training loss: 6125.84, base loss: 15990.59
[INFO 2017-06-30 03:55:18,833 main.py:52] epoch 3294, training loss: 5932.36, average training loss: 6125.42, base loss: 15991.90
[INFO 2017-06-30 03:55:21,994 main.py:52] epoch 3295, training loss: 5748.83, average training loss: 6124.85, base loss: 15992.05
[INFO 2017-06-30 03:55:25,141 main.py:52] epoch 3296, training loss: 6197.84, average training loss: 6124.91, base loss: 15992.17
[INFO 2017-06-30 03:55:28,322 main.py:52] epoch 3297, training loss: 6061.72, average training loss: 6124.84, base loss: 15992.15
[INFO 2017-06-30 03:55:31,513 main.py:52] epoch 3298, training loss: 5886.35, average training loss: 6124.16, base loss: 15991.73
[INFO 2017-06-30 03:55:34,620 main.py:52] epoch 3299, training loss: 6243.16, average training loss: 6124.39, base loss: 15992.49
[INFO 2017-06-30 03:55:34,621 main.py:54] epoch 3299, testing
[INFO 2017-06-30 03:55:48,062 main.py:97] average testing loss: 5963.63, base loss: 15738.18
[INFO 2017-06-30 03:55:48,063 main.py:98] improve_loss: 9774.55, improve_percent: 0.62
[INFO 2017-06-30 03:55:48,064 main.py:66] current best improved percent: 0.63
[INFO 2017-06-30 03:55:51,242 main.py:52] epoch 3300, training loss: 6046.98, average training loss: 6124.35, base loss: 15992.93
[INFO 2017-06-30 03:55:54,460 main.py:52] epoch 3301, training loss: 5791.23, average training loss: 6123.79, base loss: 15992.04
[INFO 2017-06-30 03:55:57,646 main.py:52] epoch 3302, training loss: 5854.33, average training loss: 6123.13, base loss: 15991.90
[INFO 2017-06-30 03:56:00,805 main.py:52] epoch 3303, training loss: 5819.79, average training loss: 6122.71, base loss: 15991.91
[INFO 2017-06-30 03:56:03,935 main.py:52] epoch 3304, training loss: 5693.88, average training loss: 6121.81, base loss: 15990.43
[INFO 2017-06-30 03:56:07,135 main.py:52] epoch 3305, training loss: 6114.55, average training loss: 6121.58, base loss: 15989.68
[INFO 2017-06-30 03:56:10,267 main.py:52] epoch 3306, training loss: 6155.77, average training loss: 6121.68, base loss: 15990.05
[INFO 2017-06-30 03:56:13,389 main.py:52] epoch 3307, training loss: 6100.67, average training loss: 6121.52, base loss: 15990.53
[INFO 2017-06-30 03:56:16,556 main.py:52] epoch 3308, training loss: 6017.60, average training loss: 6121.19, base loss: 15990.91
[INFO 2017-06-30 03:56:19,709 main.py:52] epoch 3309, training loss: 6117.78, average training loss: 6121.65, base loss: 15991.09
[INFO 2017-06-30 03:56:22,902 main.py:52] epoch 3310, training loss: 6494.88, average training loss: 6121.45, base loss: 15992.54
[INFO 2017-06-30 03:56:26,047 main.py:52] epoch 3311, training loss: 6351.50, average training loss: 6120.52, base loss: 15993.48
[INFO 2017-06-30 03:56:29,263 main.py:52] epoch 3312, training loss: 6526.89, average training loss: 6120.81, base loss: 15994.78
[INFO 2017-06-30 03:56:32,395 main.py:52] epoch 3313, training loss: 5852.71, average training loss: 6120.42, base loss: 15995.18
[INFO 2017-06-30 03:56:35,521 main.py:52] epoch 3314, training loss: 5871.20, average training loss: 6119.82, base loss: 15994.90
[INFO 2017-06-30 03:56:38,674 main.py:52] epoch 3315, training loss: 5511.89, average training loss: 6119.00, base loss: 15994.07
[INFO 2017-06-30 03:56:41,818 main.py:52] epoch 3316, training loss: 5987.16, average training loss: 6118.60, base loss: 15994.31
[INFO 2017-06-30 03:56:44,998 main.py:52] epoch 3317, training loss: 5905.17, average training loss: 6118.94, base loss: 15993.96
[INFO 2017-06-30 03:56:48,193 main.py:52] epoch 3318, training loss: 5861.07, average training loss: 6118.55, base loss: 15992.71
[INFO 2017-06-30 03:56:51,344 main.py:52] epoch 3319, training loss: 6525.03, average training loss: 6119.07, base loss: 15992.90
[INFO 2017-06-30 03:56:54,499 main.py:52] epoch 3320, training loss: 5899.87, average training loss: 6118.54, base loss: 15992.13
[INFO 2017-06-30 03:56:57,689 main.py:52] epoch 3321, training loss: 5701.92, average training loss: 6118.09, base loss: 15991.58
[INFO 2017-06-30 03:57:00,829 main.py:52] epoch 3322, training loss: 5807.61, average training loss: 6117.92, base loss: 15991.64
[INFO 2017-06-30 03:57:04,007 main.py:52] epoch 3323, training loss: 5296.03, average training loss: 6116.87, base loss: 15990.75
[INFO 2017-06-30 03:57:07,168 main.py:52] epoch 3324, training loss: 5769.47, average training loss: 6116.28, base loss: 15990.37
[INFO 2017-06-30 03:57:10,357 main.py:52] epoch 3325, training loss: 6150.25, average training loss: 6116.18, base loss: 15990.33
[INFO 2017-06-30 03:57:13,506 main.py:52] epoch 3326, training loss: 6034.84, average training loss: 6116.44, base loss: 15989.48
[INFO 2017-06-30 03:57:16,649 main.py:52] epoch 3327, training loss: 5978.28, average training loss: 6116.09, base loss: 15989.02
[INFO 2017-06-30 03:57:19,803 main.py:52] epoch 3328, training loss: 5843.45, average training loss: 6115.27, base loss: 15988.34
[INFO 2017-06-30 03:57:22,961 main.py:52] epoch 3329, training loss: 6410.32, average training loss: 6115.49, base loss: 15988.71
[INFO 2017-06-30 03:57:26,135 main.py:52] epoch 3330, training loss: 5637.14, average training loss: 6114.99, base loss: 15988.82
[INFO 2017-06-30 03:57:29,297 main.py:52] epoch 3331, training loss: 6264.38, average training loss: 6114.94, base loss: 15989.95
[INFO 2017-06-30 03:57:32,491 main.py:52] epoch 3332, training loss: 6529.12, average training loss: 6114.65, base loss: 15991.23
[INFO 2017-06-30 03:57:35,677 main.py:52] epoch 3333, training loss: 6127.12, average training loss: 6114.52, base loss: 15992.75
[INFO 2017-06-30 03:57:38,810 main.py:52] epoch 3334, training loss: 5870.45, average training loss: 6114.38, base loss: 15993.22
[INFO 2017-06-30 03:57:41,983 main.py:52] epoch 3335, training loss: 6012.23, average training loss: 6114.58, base loss: 15993.68
[INFO 2017-06-30 03:57:45,220 main.py:52] epoch 3336, training loss: 5917.85, average training loss: 6114.55, base loss: 15993.99
[INFO 2017-06-30 03:57:48,427 main.py:52] epoch 3337, training loss: 6051.53, average training loss: 6114.47, base loss: 15993.31
[INFO 2017-06-30 03:57:51,610 main.py:52] epoch 3338, training loss: 6185.08, average training loss: 6115.03, base loss: 15993.86
[INFO 2017-06-30 03:57:54,856 main.py:52] epoch 3339, training loss: 6029.76, average training loss: 6114.98, base loss: 15993.45
[INFO 2017-06-30 03:57:58,019 main.py:52] epoch 3340, training loss: 5948.01, average training loss: 6114.62, base loss: 15993.07
[INFO 2017-06-30 03:58:01,224 main.py:52] epoch 3341, training loss: 5996.25, average training loss: 6114.95, base loss: 15992.54
[INFO 2017-06-30 03:58:04,382 main.py:52] epoch 3342, training loss: 6359.43, average training loss: 6114.85, base loss: 15992.61
[INFO 2017-06-30 03:58:07,564 main.py:52] epoch 3343, training loss: 5482.59, average training loss: 6114.49, base loss: 15991.48
[INFO 2017-06-30 03:58:10,720 main.py:52] epoch 3344, training loss: 6370.18, average training loss: 6115.00, base loss: 15992.83
[INFO 2017-06-30 03:58:13,910 main.py:52] epoch 3345, training loss: 6144.33, average training loss: 6114.69, base loss: 15993.76
[INFO 2017-06-30 03:58:17,082 main.py:52] epoch 3346, training loss: 6211.24, average training loss: 6113.84, base loss: 15994.26
[INFO 2017-06-30 03:58:20,242 main.py:52] epoch 3347, training loss: 5727.92, average training loss: 6113.35, base loss: 15993.94
[INFO 2017-06-30 03:58:23,410 main.py:52] epoch 3348, training loss: 6333.23, average training loss: 6113.78, base loss: 15994.10
[INFO 2017-06-30 03:58:26,558 main.py:52] epoch 3349, training loss: 5984.18, average training loss: 6113.25, base loss: 15994.59
[INFO 2017-06-30 03:58:29,720 main.py:52] epoch 3350, training loss: 6000.17, average training loss: 6113.20, base loss: 15993.92
[INFO 2017-06-30 03:58:32,944 main.py:52] epoch 3351, training loss: 5706.32, average training loss: 6112.88, base loss: 15993.05
[INFO 2017-06-30 03:58:36,120 main.py:52] epoch 3352, training loss: 5784.40, average training loss: 6112.47, base loss: 15992.74
[INFO 2017-06-30 03:58:39,275 main.py:52] epoch 3353, training loss: 6081.95, average training loss: 6112.17, base loss: 15993.31
[INFO 2017-06-30 03:58:42,507 main.py:52] epoch 3354, training loss: 5660.65, average training loss: 6111.27, base loss: 15992.69
[INFO 2017-06-30 03:58:45,687 main.py:52] epoch 3355, training loss: 5969.96, average training loss: 6110.55, base loss: 15992.83
[INFO 2017-06-30 03:58:48,880 main.py:52] epoch 3356, training loss: 5805.65, average training loss: 6109.97, base loss: 15993.11
[INFO 2017-06-30 03:58:52,010 main.py:52] epoch 3357, training loss: 5917.99, average training loss: 6109.71, base loss: 15993.30
[INFO 2017-06-30 03:58:55,177 main.py:52] epoch 3358, training loss: 5940.69, average training loss: 6108.87, base loss: 15992.16
[INFO 2017-06-30 03:58:58,327 main.py:52] epoch 3359, training loss: 5770.13, average training loss: 6108.67, base loss: 15991.31
[INFO 2017-06-30 03:59:01,512 main.py:52] epoch 3360, training loss: 5848.04, average training loss: 6108.10, base loss: 15991.34
[INFO 2017-06-30 03:59:04,671 main.py:52] epoch 3361, training loss: 5877.74, average training loss: 6107.64, base loss: 15990.91
[INFO 2017-06-30 03:59:07,864 main.py:52] epoch 3362, training loss: 6091.38, average training loss: 6107.38, base loss: 15991.84
[INFO 2017-06-30 03:59:11,050 main.py:52] epoch 3363, training loss: 5870.39, average training loss: 6106.70, base loss: 15992.14
[INFO 2017-06-30 03:59:14,221 main.py:52] epoch 3364, training loss: 6046.34, average training loss: 6106.93, base loss: 15992.55
[INFO 2017-06-30 03:59:17,359 main.py:52] epoch 3365, training loss: 6254.74, average training loss: 6106.66, base loss: 15992.45
[INFO 2017-06-30 03:59:20,537 main.py:52] epoch 3366, training loss: 6414.00, average training loss: 6107.22, base loss: 15992.70
[INFO 2017-06-30 03:59:23,688 main.py:52] epoch 3367, training loss: 6156.11, average training loss: 6107.36, base loss: 15992.82
[INFO 2017-06-30 03:59:26,866 main.py:52] epoch 3368, training loss: 5950.90, average training loss: 6107.42, base loss: 15992.34
[INFO 2017-06-30 03:59:30,029 main.py:52] epoch 3369, training loss: 6109.75, average training loss: 6106.73, base loss: 15992.45
[INFO 2017-06-30 03:59:33,178 main.py:52] epoch 3370, training loss: 6286.73, average training loss: 6106.15, base loss: 15993.46
[INFO 2017-06-30 03:59:36,335 main.py:52] epoch 3371, training loss: 5812.74, average training loss: 6105.35, base loss: 15993.08
[INFO 2017-06-30 03:59:39,555 main.py:52] epoch 3372, training loss: 6562.61, average training loss: 6106.03, base loss: 15994.28
[INFO 2017-06-30 03:59:42,709 main.py:52] epoch 3373, training loss: 6172.21, average training loss: 6106.27, base loss: 15994.92
[INFO 2017-06-30 03:59:45,879 main.py:52] epoch 3374, training loss: 6299.84, average training loss: 6106.92, base loss: 15995.10
[INFO 2017-06-30 03:59:49,046 main.py:52] epoch 3375, training loss: 5772.60, average training loss: 6106.42, base loss: 15993.81
[INFO 2017-06-30 03:59:52,233 main.py:52] epoch 3376, training loss: 5869.67, average training loss: 6106.07, base loss: 15993.15
[INFO 2017-06-30 03:59:55,377 main.py:52] epoch 3377, training loss: 5733.51, average training loss: 6105.43, base loss: 15992.48
[INFO 2017-06-30 03:59:58,527 main.py:52] epoch 3378, training loss: 5627.91, average training loss: 6104.48, base loss: 15991.50
[INFO 2017-06-30 04:00:01,658 main.py:52] epoch 3379, training loss: 5949.35, average training loss: 6104.13, base loss: 15991.48
[INFO 2017-06-30 04:00:04,797 main.py:52] epoch 3380, training loss: 5805.15, average training loss: 6103.63, base loss: 15990.97
[INFO 2017-06-30 04:00:07,970 main.py:52] epoch 3381, training loss: 6139.20, average training loss: 6103.46, base loss: 15990.45
[INFO 2017-06-30 04:00:11,113 main.py:52] epoch 3382, training loss: 5860.97, average training loss: 6103.02, base loss: 15990.03
[INFO 2017-06-30 04:00:14,273 main.py:52] epoch 3383, training loss: 5881.16, average training loss: 6102.41, base loss: 15989.19
[INFO 2017-06-30 04:00:17,455 main.py:52] epoch 3384, training loss: 6409.07, average training loss: 6102.30, base loss: 15989.40
[INFO 2017-06-30 04:00:20,602 main.py:52] epoch 3385, training loss: 5709.31, average training loss: 6101.81, base loss: 15988.97
[INFO 2017-06-30 04:00:23,728 main.py:52] epoch 3386, training loss: 5871.67, average training loss: 6101.73, base loss: 15989.01
[INFO 2017-06-30 04:00:26,909 main.py:52] epoch 3387, training loss: 5409.18, average training loss: 6100.86, base loss: 15989.16
[INFO 2017-06-30 04:00:30,075 main.py:52] epoch 3388, training loss: 5615.52, average training loss: 6099.57, base loss: 15988.04
[INFO 2017-06-30 04:00:33,233 main.py:52] epoch 3389, training loss: 6415.13, average training loss: 6099.89, base loss: 15988.42
[INFO 2017-06-30 04:00:36,438 main.py:52] epoch 3390, training loss: 6308.66, average training loss: 6099.06, base loss: 15988.93
[INFO 2017-06-30 04:00:39,572 main.py:52] epoch 3391, training loss: 6152.58, average training loss: 6099.05, base loss: 15989.01
[INFO 2017-06-30 04:00:42,745 main.py:52] epoch 3392, training loss: 5895.64, average training loss: 6098.39, base loss: 15989.00
[INFO 2017-06-30 04:00:45,922 main.py:52] epoch 3393, training loss: 5907.34, average training loss: 6098.10, base loss: 15988.08
[INFO 2017-06-30 04:00:49,075 main.py:52] epoch 3394, training loss: 5975.53, average training loss: 6098.18, base loss: 15988.08
[INFO 2017-06-30 04:00:52,229 main.py:52] epoch 3395, training loss: 6162.67, average training loss: 6098.10, base loss: 15988.65
[INFO 2017-06-30 04:00:55,342 main.py:52] epoch 3396, training loss: 6517.49, average training loss: 6099.07, base loss: 15989.25
[INFO 2017-06-30 04:00:58,519 main.py:52] epoch 3397, training loss: 6367.97, average training loss: 6098.91, base loss: 15989.22
[INFO 2017-06-30 04:01:01,667 main.py:52] epoch 3398, training loss: 5429.81, average training loss: 6098.05, base loss: 15988.06
[INFO 2017-06-30 04:01:04,866 main.py:52] epoch 3399, training loss: 6175.40, average training loss: 6097.89, base loss: 15988.20
[INFO 2017-06-30 04:01:04,866 main.py:54] epoch 3399, testing
[INFO 2017-06-30 04:01:18,213 main.py:97] average testing loss: 6110.73, base loss: 16274.73
[INFO 2017-06-30 04:01:18,213 main.py:98] improve_loss: 10164.00, improve_percent: 0.62
[INFO 2017-06-30 04:01:18,215 main.py:66] current best improved percent: 0.63
[INFO 2017-06-30 04:01:21,338 main.py:52] epoch 3400, training loss: 5787.66, average training loss: 6097.19, base loss: 15988.19
[INFO 2017-06-30 04:01:24,573 main.py:52] epoch 3401, training loss: 6338.93, average training loss: 6097.33, base loss: 15988.66
[INFO 2017-06-30 04:01:27,760 main.py:52] epoch 3402, training loss: 5772.27, average training loss: 6097.06, base loss: 15989.02
[INFO 2017-06-30 04:01:30,954 main.py:52] epoch 3403, training loss: 6029.48, average training loss: 6096.95, base loss: 15988.86
[INFO 2017-06-30 04:01:34,127 main.py:52] epoch 3404, training loss: 5970.82, average training loss: 6096.76, base loss: 15988.30
[INFO 2017-06-30 04:01:37,278 main.py:52] epoch 3405, training loss: 6637.18, average training loss: 6097.57, base loss: 15989.46
[INFO 2017-06-30 04:01:40,433 main.py:52] epoch 3406, training loss: 5769.88, average training loss: 6097.46, base loss: 15988.81
[INFO 2017-06-30 04:01:43,591 main.py:52] epoch 3407, training loss: 6129.85, average training loss: 6097.22, base loss: 15989.61
[INFO 2017-06-30 04:01:46,755 main.py:52] epoch 3408, training loss: 5858.57, average training loss: 6096.30, base loss: 15990.79
[INFO 2017-06-30 04:01:49,911 main.py:52] epoch 3409, training loss: 5699.01, average training loss: 6096.19, base loss: 15991.29
[INFO 2017-06-30 04:01:53,108 main.py:52] epoch 3410, training loss: 6506.48, average training loss: 6096.85, base loss: 15992.93
[INFO 2017-06-30 04:01:56,261 main.py:52] epoch 3411, training loss: 6141.89, average training loss: 6097.05, base loss: 15994.23
[INFO 2017-06-30 04:01:59,418 main.py:52] epoch 3412, training loss: 5960.53, average training loss: 6097.14, base loss: 15993.24
[INFO 2017-06-30 04:02:02,586 main.py:52] epoch 3413, training loss: 5914.23, average training loss: 6097.39, base loss: 15992.89
[INFO 2017-06-30 04:02:05,702 main.py:52] epoch 3414, training loss: 5775.08, average training loss: 6097.08, base loss: 15992.67
[INFO 2017-06-30 04:02:08,877 main.py:52] epoch 3415, training loss: 5967.68, average training loss: 6096.74, base loss: 15992.44
[INFO 2017-06-30 04:02:12,031 main.py:52] epoch 3416, training loss: 5571.64, average training loss: 6096.05, base loss: 15991.68
[INFO 2017-06-30 04:02:15,224 main.py:52] epoch 3417, training loss: 6009.40, average training loss: 6095.74, base loss: 15991.82
[INFO 2017-06-30 04:02:18,426 main.py:52] epoch 3418, training loss: 5605.05, average training loss: 6094.68, base loss: 15991.06
[INFO 2017-06-30 04:02:21,640 main.py:52] epoch 3419, training loss: 5886.07, average training loss: 6094.69, base loss: 15991.20
[INFO 2017-06-30 04:02:24,832 main.py:52] epoch 3420, training loss: 5727.91, average training loss: 6094.05, base loss: 15991.21
[INFO 2017-06-30 04:02:28,033 main.py:52] epoch 3421, training loss: 5755.96, average training loss: 6094.28, base loss: 15991.35
[INFO 2017-06-30 04:02:31,175 main.py:52] epoch 3422, training loss: 5650.03, average training loss: 6093.51, base loss: 15991.20
[INFO 2017-06-30 04:02:34,328 main.py:52] epoch 3423, training loss: 6405.91, average training loss: 6093.76, base loss: 15992.26
[INFO 2017-06-30 04:02:37,496 main.py:52] epoch 3424, training loss: 5691.89, average training loss: 6093.29, base loss: 15991.27
[INFO 2017-06-30 04:02:40,671 main.py:52] epoch 3425, training loss: 5575.87, average training loss: 6092.90, base loss: 15990.45
[INFO 2017-06-30 04:02:43,859 main.py:52] epoch 3426, training loss: 6100.55, average training loss: 6093.06, base loss: 15991.47
[INFO 2017-06-30 04:02:47,035 main.py:52] epoch 3427, training loss: 6490.55, average training loss: 6093.38, base loss: 15992.84
[INFO 2017-06-30 04:02:50,231 main.py:52] epoch 3428, training loss: 6223.70, average training loss: 6093.32, base loss: 15992.64
[INFO 2017-06-30 04:02:53,398 main.py:52] epoch 3429, training loss: 5871.48, average training loss: 6093.04, base loss: 15991.69
[INFO 2017-06-30 04:02:56,541 main.py:52] epoch 3430, training loss: 5946.03, average training loss: 6092.96, base loss: 15991.38
[INFO 2017-06-30 04:02:59,708 main.py:52] epoch 3431, training loss: 6075.57, average training loss: 6092.90, base loss: 15992.17
[INFO 2017-06-30 04:03:02,861 main.py:52] epoch 3432, training loss: 6410.94, average training loss: 6093.03, base loss: 15993.38
[INFO 2017-06-30 04:03:06,029 main.py:52] epoch 3433, training loss: 5879.79, average training loss: 6092.92, base loss: 15992.67
[INFO 2017-06-30 04:03:09,223 main.py:52] epoch 3434, training loss: 5502.59, average training loss: 6092.10, base loss: 15991.90
[INFO 2017-06-30 04:03:12,352 main.py:52] epoch 3435, training loss: 6678.09, average training loss: 6092.21, base loss: 15993.53
[INFO 2017-06-30 04:03:15,538 main.py:52] epoch 3436, training loss: 6001.54, average training loss: 6091.69, base loss: 15994.05
[INFO 2017-06-30 04:03:18,694 main.py:52] epoch 3437, training loss: 6118.45, average training loss: 6091.48, base loss: 15994.33
[INFO 2017-06-30 04:03:21,888 main.py:52] epoch 3438, training loss: 5996.63, average training loss: 6091.35, base loss: 15994.63
[INFO 2017-06-30 04:03:25,073 main.py:52] epoch 3439, training loss: 6691.96, average training loss: 6091.59, base loss: 15995.43
[INFO 2017-06-30 04:03:28,216 main.py:52] epoch 3440, training loss: 5921.54, average training loss: 6091.35, base loss: 15995.08
[INFO 2017-06-30 04:03:31,395 main.py:52] epoch 3441, training loss: 6619.33, average training loss: 6091.90, base loss: 15996.38
[INFO 2017-06-30 04:03:34,574 main.py:52] epoch 3442, training loss: 5652.84, average training loss: 6090.95, base loss: 15996.07
[INFO 2017-06-30 04:03:37,737 main.py:52] epoch 3443, training loss: 5780.73, average training loss: 6090.56, base loss: 15995.48
[INFO 2017-06-30 04:03:40,935 main.py:52] epoch 3444, training loss: 6112.74, average training loss: 6090.17, base loss: 15995.31
[INFO 2017-06-30 04:03:44,159 main.py:52] epoch 3445, training loss: 6112.04, average training loss: 6090.44, base loss: 15995.38
[INFO 2017-06-30 04:03:47,312 main.py:52] epoch 3446, training loss: 6128.60, average training loss: 6090.36, base loss: 15995.85
[INFO 2017-06-30 04:03:50,513 main.py:52] epoch 3447, training loss: 6446.83, average training loss: 6090.78, base loss: 15996.11
[INFO 2017-06-30 04:03:53,675 main.py:52] epoch 3448, training loss: 6016.57, average training loss: 6090.53, base loss: 15995.61
[INFO 2017-06-30 04:03:56,828 main.py:52] epoch 3449, training loss: 5485.00, average training loss: 6089.75, base loss: 15995.23
[INFO 2017-06-30 04:04:00,006 main.py:52] epoch 3450, training loss: 5790.51, average training loss: 6089.57, base loss: 15995.23
[INFO 2017-06-30 04:04:03,170 main.py:52] epoch 3451, training loss: 6257.04, average training loss: 6089.10, base loss: 15995.39
[INFO 2017-06-30 04:04:06,360 main.py:52] epoch 3452, training loss: 6131.47, average training loss: 6089.49, base loss: 15995.56
[INFO 2017-06-30 04:04:09,524 main.py:52] epoch 3453, training loss: 5621.60, average training loss: 6088.75, base loss: 15994.67
[INFO 2017-06-30 04:04:12,717 main.py:52] epoch 3454, training loss: 6038.14, average training loss: 6088.69, base loss: 15994.17
[INFO 2017-06-30 04:04:15,896 main.py:52] epoch 3455, training loss: 5743.48, average training loss: 6088.36, base loss: 15993.62
[INFO 2017-06-30 04:04:19,085 main.py:52] epoch 3456, training loss: 6421.43, average training loss: 6088.69, base loss: 15993.95
[INFO 2017-06-30 04:04:22,259 main.py:52] epoch 3457, training loss: 5388.18, average training loss: 6088.17, base loss: 15992.74
[INFO 2017-06-30 04:04:25,448 main.py:52] epoch 3458, training loss: 6155.90, average training loss: 6088.18, base loss: 15993.03
[INFO 2017-06-30 04:04:28,660 main.py:52] epoch 3459, training loss: 6020.04, average training loss: 6087.73, base loss: 15993.45
[INFO 2017-06-30 04:04:31,793 main.py:52] epoch 3460, training loss: 5608.63, average training loss: 6087.38, base loss: 15992.73
[INFO 2017-06-30 04:04:34,935 main.py:52] epoch 3461, training loss: 5851.77, average training loss: 6086.09, base loss: 15992.61
[INFO 2017-06-30 04:04:38,106 main.py:52] epoch 3462, training loss: 5568.87, average training loss: 6085.37, base loss: 15992.16
[INFO 2017-06-30 04:04:41,290 main.py:52] epoch 3463, training loss: 6232.83, average training loss: 6085.79, base loss: 15991.50
[INFO 2017-06-30 04:04:44,458 main.py:52] epoch 3464, training loss: 5577.80, average training loss: 6084.67, base loss: 15990.62
[INFO 2017-06-30 04:04:47,629 main.py:52] epoch 3465, training loss: 5978.27, average training loss: 6084.31, base loss: 15990.63
[INFO 2017-06-30 04:04:50,824 main.py:52] epoch 3466, training loss: 6323.11, average training loss: 6084.92, base loss: 15991.54
[INFO 2017-06-30 04:04:53,992 main.py:52] epoch 3467, training loss: 6188.55, average training loss: 6085.63, base loss: 15991.71
[INFO 2017-06-30 04:04:57,167 main.py:52] epoch 3468, training loss: 5765.31, average training loss: 6084.94, base loss: 15991.60
[INFO 2017-06-30 04:05:00,308 main.py:52] epoch 3469, training loss: 6304.44, average training loss: 6084.82, base loss: 15991.54
[INFO 2017-06-30 04:05:03,478 main.py:52] epoch 3470, training loss: 5764.13, average training loss: 6084.64, base loss: 15991.17
[INFO 2017-06-30 04:05:06,641 main.py:52] epoch 3471, training loss: 5740.06, average training loss: 6084.24, base loss: 15990.48
[INFO 2017-06-30 04:05:09,780 main.py:52] epoch 3472, training loss: 5880.70, average training loss: 6084.07, base loss: 15989.49
[INFO 2017-06-30 04:05:12,983 main.py:52] epoch 3473, training loss: 5827.10, average training loss: 6083.47, base loss: 15988.98
[INFO 2017-06-30 04:05:16,161 main.py:52] epoch 3474, training loss: 5916.46, average training loss: 6082.90, base loss: 15988.93
[INFO 2017-06-30 04:05:19,327 main.py:52] epoch 3475, training loss: 5604.06, average training loss: 6082.80, base loss: 15987.93
[INFO 2017-06-30 04:05:22,482 main.py:52] epoch 3476, training loss: 6214.89, average training loss: 6082.67, base loss: 15988.78
[INFO 2017-06-30 04:05:25,642 main.py:52] epoch 3477, training loss: 5555.85, average training loss: 6082.28, base loss: 15988.59
[INFO 2017-06-30 04:05:28,801 main.py:52] epoch 3478, training loss: 5510.12, average training loss: 6082.05, base loss: 15988.01
[INFO 2017-06-30 04:05:31,954 main.py:52] epoch 3479, training loss: 6362.01, average training loss: 6082.84, base loss: 15988.72
[INFO 2017-06-30 04:05:35,193 main.py:52] epoch 3480, training loss: 6483.83, average training loss: 6083.17, base loss: 15989.71
[INFO 2017-06-30 04:05:38,372 main.py:52] epoch 3481, training loss: 6175.66, average training loss: 6083.16, base loss: 15990.33
[INFO 2017-06-30 04:05:41,523 main.py:52] epoch 3482, training loss: 6181.37, average training loss: 6083.21, base loss: 15990.84
[INFO 2017-06-30 04:05:44,701 main.py:52] epoch 3483, training loss: 5824.66, average training loss: 6082.50, base loss: 15989.85
[INFO 2017-06-30 04:05:47,941 main.py:52] epoch 3484, training loss: 5986.59, average training loss: 6082.15, base loss: 15990.43
[INFO 2017-06-30 04:05:51,121 main.py:52] epoch 3485, training loss: 5741.04, average training loss: 6081.42, base loss: 15990.24
[INFO 2017-06-30 04:05:54,292 main.py:52] epoch 3486, training loss: 5843.07, average training loss: 6080.99, base loss: 15990.37
[INFO 2017-06-30 04:05:57,471 main.py:52] epoch 3487, training loss: 5940.35, average training loss: 6080.45, base loss: 15990.18
[INFO 2017-06-30 04:06:00,600 main.py:52] epoch 3488, training loss: 6115.39, average training loss: 6080.64, base loss: 15990.85
[INFO 2017-06-30 04:06:03,771 main.py:52] epoch 3489, training loss: 5863.59, average training loss: 6080.19, base loss: 15990.43
[INFO 2017-06-30 04:06:06,944 main.py:52] epoch 3490, training loss: 5344.58, average training loss: 6079.48, base loss: 15988.99
[INFO 2017-06-30 04:06:10,076 main.py:52] epoch 3491, training loss: 6357.93, average training loss: 6079.52, base loss: 15989.21
[INFO 2017-06-30 04:06:13,258 main.py:52] epoch 3492, training loss: 6056.93, average training loss: 6079.76, base loss: 15989.44
[INFO 2017-06-30 04:06:16,421 main.py:52] epoch 3493, training loss: 6136.01, average training loss: 6080.04, base loss: 15989.15
[INFO 2017-06-30 04:06:19,586 main.py:52] epoch 3494, training loss: 5513.30, average training loss: 6079.37, base loss: 15988.23
[INFO 2017-06-30 04:06:22,742 main.py:52] epoch 3495, training loss: 5955.07, average training loss: 6078.77, base loss: 15988.08
[INFO 2017-06-30 04:06:25,908 main.py:52] epoch 3496, training loss: 6096.03, average training loss: 6078.61, base loss: 15988.56
[INFO 2017-06-30 04:06:29,060 main.py:52] epoch 3497, training loss: 5785.56, average training loss: 6078.18, base loss: 15987.51
[INFO 2017-06-30 04:06:32,229 main.py:52] epoch 3498, training loss: 6011.13, average training loss: 6078.08, base loss: 15987.30
[INFO 2017-06-30 04:06:35,422 main.py:52] epoch 3499, training loss: 6092.00, average training loss: 6077.59, base loss: 15987.42
[INFO 2017-06-30 04:06:35,423 main.py:54] epoch 3499, testing
[INFO 2017-06-30 04:06:48,782 main.py:97] average testing loss: 5931.86, base loss: 15624.25
[INFO 2017-06-30 04:06:48,782 main.py:98] improve_loss: 9692.39, improve_percent: 0.62
[INFO 2017-06-30 04:06:48,784 main.py:66] current best improved percent: 0.63
[INFO 2017-06-30 04:06:51,940 main.py:52] epoch 3500, training loss: 5668.88, average training loss: 6076.98, base loss: 15986.57
[INFO 2017-06-30 04:06:55,144 main.py:52] epoch 3501, training loss: 5852.90, average training loss: 6077.01, base loss: 15986.16
[INFO 2017-06-30 04:06:58,318 main.py:52] epoch 3502, training loss: 5801.45, average training loss: 6076.19, base loss: 15986.17
[INFO 2017-06-30 04:07:01,477 main.py:52] epoch 3503, training loss: 5672.59, average training loss: 6075.61, base loss: 15985.78
[INFO 2017-06-30 04:07:04,634 main.py:52] epoch 3504, training loss: 6058.95, average training loss: 6074.62, base loss: 15986.08
[INFO 2017-06-30 04:07:07,800 main.py:52] epoch 3505, training loss: 5497.14, average training loss: 6073.98, base loss: 15984.34
[INFO 2017-06-30 04:07:10,964 main.py:52] epoch 3506, training loss: 5884.29, average training loss: 6073.40, base loss: 15983.86
[INFO 2017-06-30 04:07:14,138 main.py:52] epoch 3507, training loss: 6040.63, average training loss: 6073.67, base loss: 15983.28
[INFO 2017-06-30 04:07:17,315 main.py:52] epoch 3508, training loss: 6301.14, average training loss: 6073.42, base loss: 15983.91
[INFO 2017-06-30 04:07:20,469 main.py:52] epoch 3509, training loss: 5450.52, average training loss: 6072.84, base loss: 15983.20
[INFO 2017-06-30 04:07:23,606 main.py:52] epoch 3510, training loss: 6384.48, average training loss: 6073.18, base loss: 15983.46
[INFO 2017-06-30 04:07:26,793 main.py:52] epoch 3511, training loss: 5950.50, average training loss: 6073.04, base loss: 15983.27
[INFO 2017-06-30 04:07:29,959 main.py:52] epoch 3512, training loss: 6005.57, average training loss: 6072.83, base loss: 15983.42
[INFO 2017-06-30 04:07:33,130 main.py:52] epoch 3513, training loss: 6488.71, average training loss: 6072.75, base loss: 15984.95
[INFO 2017-06-30 04:07:36,274 main.py:52] epoch 3514, training loss: 5916.04, average training loss: 6072.42, base loss: 15985.73
[INFO 2017-06-30 04:07:39,435 main.py:52] epoch 3515, training loss: 5546.91, average training loss: 6071.30, base loss: 15985.78
[INFO 2017-06-30 04:07:42,614 main.py:52] epoch 3516, training loss: 6064.01, average training loss: 6070.89, base loss: 15986.66
[INFO 2017-06-30 04:07:45,765 main.py:52] epoch 3517, training loss: 5867.76, average training loss: 6070.66, base loss: 15986.71
[INFO 2017-06-30 04:07:48,951 main.py:52] epoch 3518, training loss: 5558.15, average training loss: 6069.80, base loss: 15985.92
[INFO 2017-06-30 04:07:52,098 main.py:52] epoch 3519, training loss: 5805.19, average training loss: 6069.30, base loss: 15985.71
[INFO 2017-06-30 04:07:55,304 main.py:52] epoch 3520, training loss: 6280.98, average training loss: 6069.36, base loss: 15986.23
[INFO 2017-06-30 04:07:58,438 main.py:52] epoch 3521, training loss: 5977.21, average training loss: 6069.27, base loss: 15986.15
[INFO 2017-06-30 04:08:01,609 main.py:52] epoch 3522, training loss: 5684.11, average training loss: 6068.86, base loss: 15985.59
[INFO 2017-06-30 04:08:04,736 main.py:52] epoch 3523, training loss: 5836.34, average training loss: 6068.66, base loss: 15985.33
[INFO 2017-06-30 04:08:07,932 main.py:52] epoch 3524, training loss: 5571.22, average training loss: 6067.22, base loss: 15985.05
[INFO 2017-06-30 04:08:11,129 main.py:52] epoch 3525, training loss: 5834.83, average training loss: 6067.23, base loss: 15985.53
[INFO 2017-06-30 04:08:14,270 main.py:52] epoch 3526, training loss: 6042.86, average training loss: 6066.74, base loss: 15985.53
[INFO 2017-06-30 04:08:17,451 main.py:52] epoch 3527, training loss: 6114.57, average training loss: 6067.03, base loss: 15985.47
[INFO 2017-06-30 04:08:20,612 main.py:52] epoch 3528, training loss: 5966.49, average training loss: 6067.03, base loss: 15985.36
[INFO 2017-06-30 04:08:23,782 main.py:52] epoch 3529, training loss: 5951.98, average training loss: 6066.54, base loss: 15984.71
[INFO 2017-06-30 04:08:26,964 main.py:52] epoch 3530, training loss: 5995.08, average training loss: 6066.67, base loss: 15984.45
[INFO 2017-06-30 04:08:30,106 main.py:52] epoch 3531, training loss: 5982.34, average training loss: 6066.42, base loss: 15984.78
[INFO 2017-06-30 04:08:33,323 main.py:52] epoch 3532, training loss: 6209.29, average training loss: 6066.56, base loss: 15985.18
[INFO 2017-06-30 04:08:36,518 main.py:52] epoch 3533, training loss: 6515.63, average training loss: 6067.00, base loss: 15986.41
[INFO 2017-06-30 04:08:39,728 main.py:52] epoch 3534, training loss: 5819.19, average training loss: 6066.35, base loss: 15986.87
[INFO 2017-06-30 04:08:42,886 main.py:52] epoch 3535, training loss: 5775.18, average training loss: 6065.22, base loss: 15986.69
[INFO 2017-06-30 04:08:46,043 main.py:52] epoch 3536, training loss: 5995.03, average training loss: 6064.76, base loss: 15986.65
[INFO 2017-06-30 04:08:49,190 main.py:52] epoch 3537, training loss: 6043.42, average training loss: 6065.25, base loss: 15986.68
[INFO 2017-06-30 04:08:52,369 main.py:52] epoch 3538, training loss: 6138.77, average training loss: 6065.16, base loss: 15986.69
[INFO 2017-06-30 04:08:55,534 main.py:52] epoch 3539, training loss: 6643.61, average training loss: 6065.34, base loss: 15988.30
[INFO 2017-06-30 04:08:58,700 main.py:52] epoch 3540, training loss: 6063.55, average training loss: 6064.85, base loss: 15988.71
[INFO 2017-06-30 04:09:01,848 main.py:52] epoch 3541, training loss: 5952.42, average training loss: 6064.67, base loss: 15988.88
[INFO 2017-06-30 04:09:04,993 main.py:52] epoch 3542, training loss: 5529.39, average training loss: 6064.09, base loss: 15988.17
[INFO 2017-06-30 04:09:08,141 main.py:52] epoch 3543, training loss: 6043.66, average training loss: 6063.99, base loss: 15988.55
[INFO 2017-06-30 04:09:11,275 main.py:52] epoch 3544, training loss: 5433.05, average training loss: 6063.06, base loss: 15988.03
[INFO 2017-06-30 04:09:14,475 main.py:52] epoch 3545, training loss: 6286.88, average training loss: 6063.06, base loss: 15988.73
[INFO 2017-06-30 04:09:17,622 main.py:52] epoch 3546, training loss: 6090.81, average training loss: 6062.77, base loss: 15989.09
[INFO 2017-06-30 04:09:20,851 main.py:52] epoch 3547, training loss: 6226.49, average training loss: 6063.06, base loss: 15989.47
[INFO 2017-06-30 04:09:23,982 main.py:52] epoch 3548, training loss: 6151.69, average training loss: 6062.52, base loss: 15990.16
[INFO 2017-06-30 04:09:27,124 main.py:52] epoch 3549, training loss: 5636.50, average training loss: 6062.18, base loss: 15989.89
[INFO 2017-06-30 04:09:30,259 main.py:52] epoch 3550, training loss: 5889.21, average training loss: 6061.85, base loss: 15989.79
[INFO 2017-06-30 04:09:33,429 main.py:52] epoch 3551, training loss: 6055.64, average training loss: 6062.27, base loss: 15990.07
[INFO 2017-06-30 04:09:36,609 main.py:52] epoch 3552, training loss: 5880.68, average training loss: 6061.95, base loss: 15990.17
[INFO 2017-06-30 04:09:39,788 main.py:52] epoch 3553, training loss: 6267.08, average training loss: 6062.36, base loss: 15990.67
[INFO 2017-06-30 04:09:42,935 main.py:52] epoch 3554, training loss: 5543.04, average training loss: 6061.89, base loss: 15989.86
[INFO 2017-06-30 04:09:46,115 main.py:52] epoch 3555, training loss: 5811.28, average training loss: 6061.41, base loss: 15989.40
[INFO 2017-06-30 04:09:49,319 main.py:52] epoch 3556, training loss: 5861.78, average training loss: 6061.14, base loss: 15989.58
[INFO 2017-06-30 04:09:52,498 main.py:52] epoch 3557, training loss: 5853.54, average training loss: 6060.18, base loss: 15989.04
[INFO 2017-06-30 04:09:55,697 main.py:52] epoch 3558, training loss: 5671.85, average training loss: 6059.60, base loss: 15989.50
[INFO 2017-06-30 04:09:58,908 main.py:52] epoch 3559, training loss: 5914.85, average training loss: 6059.11, base loss: 15989.81
[INFO 2017-06-30 04:10:02,114 main.py:52] epoch 3560, training loss: 6062.46, average training loss: 6058.94, base loss: 15990.51
[INFO 2017-06-30 04:10:05,291 main.py:52] epoch 3561, training loss: 5866.90, average training loss: 6058.52, base loss: 15990.21
[INFO 2017-06-30 04:10:08,459 main.py:52] epoch 3562, training loss: 5796.00, average training loss: 6057.91, base loss: 15989.96
[INFO 2017-06-30 04:10:11,604 main.py:52] epoch 3563, training loss: 6122.10, average training loss: 6058.09, base loss: 15990.57
[INFO 2017-06-30 04:10:14,824 main.py:52] epoch 3564, training loss: 5450.61, average training loss: 6057.45, base loss: 15990.09
[INFO 2017-06-30 04:10:17,972 main.py:52] epoch 3565, training loss: 6352.20, average training loss: 6057.55, base loss: 15990.98
[INFO 2017-06-30 04:10:21,189 main.py:52] epoch 3566, training loss: 5783.73, average training loss: 6057.15, base loss: 15990.84
[INFO 2017-06-30 04:10:24,375 main.py:52] epoch 3567, training loss: 5820.46, average training loss: 6056.79, base loss: 15990.65
[INFO 2017-06-30 04:10:27,553 main.py:52] epoch 3568, training loss: 6178.62, average training loss: 6056.55, base loss: 15991.95
[INFO 2017-06-30 04:10:30,663 main.py:52] epoch 3569, training loss: 6249.72, average training loss: 6056.44, base loss: 15992.72
[INFO 2017-06-30 04:10:33,848 main.py:52] epoch 3570, training loss: 5490.47, average training loss: 6055.93, base loss: 15992.10
[INFO 2017-06-30 04:10:37,033 main.py:52] epoch 3571, training loss: 5924.35, average training loss: 6055.44, base loss: 15991.95
[INFO 2017-06-30 04:10:40,201 main.py:52] epoch 3572, training loss: 5946.75, average training loss: 6055.86, base loss: 15991.63
[INFO 2017-06-30 04:10:43,324 main.py:52] epoch 3573, training loss: 5757.95, average training loss: 6055.48, base loss: 15990.96
[INFO 2017-06-30 04:10:46,515 main.py:52] epoch 3574, training loss: 5661.22, average training loss: 6054.92, base loss: 15990.95
[INFO 2017-06-30 04:10:49,632 main.py:52] epoch 3575, training loss: 6119.61, average training loss: 6055.27, base loss: 15991.04
[INFO 2017-06-30 04:10:52,836 main.py:52] epoch 3576, training loss: 6046.92, average training loss: 6054.54, base loss: 15991.08
[INFO 2017-06-30 04:10:56,006 main.py:52] epoch 3577, training loss: 5896.43, average training loss: 6053.98, base loss: 15991.04
[INFO 2017-06-30 04:10:59,149 main.py:52] epoch 3578, training loss: 6161.25, average training loss: 6054.22, base loss: 15991.21
[INFO 2017-06-30 04:11:02,346 main.py:52] epoch 3579, training loss: 6178.16, average training loss: 6054.04, base loss: 15991.26
[INFO 2017-06-30 04:11:05,536 main.py:52] epoch 3580, training loss: 6178.58, average training loss: 6054.02, base loss: 15991.89
[INFO 2017-06-30 04:11:08,701 main.py:52] epoch 3581, training loss: 6046.05, average training loss: 6053.88, base loss: 15992.49
[INFO 2017-06-30 04:11:11,863 main.py:52] epoch 3582, training loss: 6012.29, average training loss: 6053.49, base loss: 15992.53
[INFO 2017-06-30 04:11:15,026 main.py:52] epoch 3583, training loss: 5978.65, average training loss: 6053.18, base loss: 15992.04
[INFO 2017-06-30 04:11:18,204 main.py:52] epoch 3584, training loss: 5398.45, average training loss: 6052.50, base loss: 15991.36
[INFO 2017-06-30 04:11:21,343 main.py:52] epoch 3585, training loss: 5969.13, average training loss: 6052.18, base loss: 15991.40
[INFO 2017-06-30 04:11:24,507 main.py:52] epoch 3586, training loss: 5540.34, average training loss: 6051.64, base loss: 15989.78
[INFO 2017-06-30 04:11:27,665 main.py:52] epoch 3587, training loss: 5969.56, average training loss: 6051.25, base loss: 15989.02
[INFO 2017-06-30 04:11:30,859 main.py:52] epoch 3588, training loss: 5525.47, average training loss: 6050.71, base loss: 15988.86
[INFO 2017-06-30 04:11:34,014 main.py:52] epoch 3589, training loss: 5745.92, average training loss: 6050.30, base loss: 15988.71
[INFO 2017-06-30 04:11:37,185 main.py:52] epoch 3590, training loss: 6042.47, average training loss: 6049.56, base loss: 15988.07
[INFO 2017-06-30 04:11:40,355 main.py:52] epoch 3591, training loss: 6044.54, average training loss: 6049.42, base loss: 15987.33
[INFO 2017-06-30 04:11:43,550 main.py:52] epoch 3592, training loss: 5844.53, average training loss: 6049.01, base loss: 15987.24
[INFO 2017-06-30 04:11:46,783 main.py:52] epoch 3593, training loss: 5729.12, average training loss: 6048.60, base loss: 15987.00
[INFO 2017-06-30 04:11:49,973 main.py:52] epoch 3594, training loss: 6312.07, average training loss: 6049.08, base loss: 15987.81
[INFO 2017-06-30 04:11:53,179 main.py:52] epoch 3595, training loss: 5524.03, average training loss: 6048.64, base loss: 15986.70
[INFO 2017-06-30 04:11:56,376 main.py:52] epoch 3596, training loss: 6038.99, average training loss: 6048.79, base loss: 15987.13
[INFO 2017-06-30 04:11:59,550 main.py:52] epoch 3597, training loss: 6055.85, average training loss: 6048.93, base loss: 15987.51
[INFO 2017-06-30 04:12:02,679 main.py:52] epoch 3598, training loss: 6778.76, average training loss: 6049.60, base loss: 15989.79
[INFO 2017-06-30 04:12:05,862 main.py:52] epoch 3599, training loss: 6027.14, average training loss: 6049.06, base loss: 15990.29
[INFO 2017-06-30 04:12:05,863 main.py:54] epoch 3599, testing
[INFO 2017-06-30 04:12:19,058 main.py:97] average testing loss: 5870.32, base loss: 15771.27
[INFO 2017-06-30 04:12:19,058 main.py:98] improve_loss: 9900.94, improve_percent: 0.63
[INFO 2017-06-30 04:12:19,060 main.py:62] model save to ./model/final.pth
[INFO 2017-06-30 04:12:19,094 main.py:66] current best improved percent: 0.63
[INFO 2017-06-30 04:12:22,257 main.py:52] epoch 3600, training loss: 6035.90, average training loss: 6048.93, base loss: 15990.10
[INFO 2017-06-30 04:12:25,407 main.py:52] epoch 3601, training loss: 5656.25, average training loss: 6048.09, base loss: 15989.89
[INFO 2017-06-30 04:12:28,580 main.py:52] epoch 3602, training loss: 6197.42, average training loss: 6047.79, base loss: 15990.44
[INFO 2017-06-30 04:12:31,730 main.py:52] epoch 3603, training loss: 5817.67, average training loss: 6047.59, base loss: 15990.36
[INFO 2017-06-30 04:12:34,874 main.py:52] epoch 3604, training loss: 6112.75, average training loss: 6047.09, base loss: 15990.19
[INFO 2017-06-30 04:12:38,055 main.py:52] epoch 3605, training loss: 5833.67, average training loss: 6046.93, base loss: 15989.79
[INFO 2017-06-30 04:12:41,214 main.py:52] epoch 3606, training loss: 5785.73, average training loss: 6046.59, base loss: 15989.60
[INFO 2017-06-30 04:12:44,359 main.py:52] epoch 3607, training loss: 5676.18, average training loss: 6046.25, base loss: 15989.10
[INFO 2017-06-30 04:12:47,524 main.py:52] epoch 3608, training loss: 6089.68, average training loss: 6046.64, base loss: 15989.45
[INFO 2017-06-30 04:12:50,649 main.py:52] epoch 3609, training loss: 5588.70, average training loss: 6046.14, base loss: 15989.16
[INFO 2017-06-30 04:12:53,797 main.py:52] epoch 3610, training loss: 5774.69, average training loss: 6045.91, base loss: 15988.67
[INFO 2017-06-30 04:12:56,969 main.py:52] epoch 3611, training loss: 5628.67, average training loss: 6045.37, base loss: 15988.63
[INFO 2017-06-30 04:13:00,130 main.py:52] epoch 3612, training loss: 5879.56, average training loss: 6045.58, base loss: 15988.82
[INFO 2017-06-30 04:13:03,278 main.py:52] epoch 3613, training loss: 6083.93, average training loss: 6045.34, base loss: 15989.61
[INFO 2017-06-30 04:13:06,444 main.py:52] epoch 3614, training loss: 6129.83, average training loss: 6045.92, base loss: 15990.40
[INFO 2017-06-30 04:13:09,588 main.py:52] epoch 3615, training loss: 6272.00, average training loss: 6045.66, base loss: 15991.47
[INFO 2017-06-30 04:13:12,794 main.py:52] epoch 3616, training loss: 5759.76, average training loss: 6045.23, base loss: 15991.33
[INFO 2017-06-30 04:13:15,979 main.py:52] epoch 3617, training loss: 6016.55, average training loss: 6045.27, base loss: 15991.16
[INFO 2017-06-30 04:13:19,144 main.py:52] epoch 3618, training loss: 5805.54, average training loss: 6045.32, base loss: 15991.53
[INFO 2017-06-30 04:13:22,338 main.py:52] epoch 3619, training loss: 5948.13, average training loss: 6045.35, base loss: 15992.09
[INFO 2017-06-30 04:13:25,502 main.py:52] epoch 3620, training loss: 6238.38, average training loss: 6045.43, base loss: 15991.80
[INFO 2017-06-30 04:13:28,707 main.py:52] epoch 3621, training loss: 6072.74, average training loss: 6045.04, base loss: 15992.10
[INFO 2017-06-30 04:13:31,908 main.py:52] epoch 3622, training loss: 5566.34, average training loss: 6044.44, base loss: 15991.42
[INFO 2017-06-30 04:13:35,083 main.py:52] epoch 3623, training loss: 6119.71, average training loss: 6044.80, base loss: 15991.88
[INFO 2017-06-30 04:13:38,251 main.py:52] epoch 3624, training loss: 5952.55, average training loss: 6044.59, base loss: 15991.94
[INFO 2017-06-30 04:13:41,458 main.py:52] epoch 3625, training loss: 5769.24, average training loss: 6044.02, base loss: 15991.86
[INFO 2017-06-30 04:13:44,678 main.py:52] epoch 3626, training loss: 6065.93, average training loss: 6044.00, base loss: 15992.15
[INFO 2017-06-30 04:13:47,860 main.py:52] epoch 3627, training loss: 6189.90, average training loss: 6043.89, base loss: 15992.90
[INFO 2017-06-30 04:13:51,026 main.py:52] epoch 3628, training loss: 5739.35, average training loss: 6043.12, base loss: 15993.00
[INFO 2017-06-30 04:13:54,203 main.py:52] epoch 3629, training loss: 5936.17, average training loss: 6043.24, base loss: 15993.24
[INFO 2017-06-30 04:13:57,340 main.py:52] epoch 3630, training loss: 5653.45, average training loss: 6042.59, base loss: 15993.20
[INFO 2017-06-30 04:14:00,530 main.py:52] epoch 3631, training loss: 5463.55, average training loss: 6042.17, base loss: 15992.45
[INFO 2017-06-30 04:14:03,752 main.py:52] epoch 3632, training loss: 6078.88, average training loss: 6041.71, base loss: 15992.45
[INFO 2017-06-30 04:14:06,947 main.py:52] epoch 3633, training loss: 5752.91, average training loss: 6041.81, base loss: 15991.79
[INFO 2017-06-30 04:14:10,116 main.py:52] epoch 3634, training loss: 5557.51, average training loss: 6041.19, base loss: 15990.45
[INFO 2017-06-30 04:14:13,293 main.py:52] epoch 3635, training loss: 6219.82, average training loss: 6041.11, base loss: 15990.85
[INFO 2017-06-30 04:14:16,459 main.py:52] epoch 3636, training loss: 5818.51, average training loss: 6040.30, base loss: 15990.70
[INFO 2017-06-30 04:14:19,627 main.py:52] epoch 3637, training loss: 6264.28, average training loss: 6040.35, base loss: 15991.32
[INFO 2017-06-30 04:14:22,763 main.py:52] epoch 3638, training loss: 5967.53, average training loss: 6040.25, base loss: 15992.23
[INFO 2017-06-30 04:14:25,913 main.py:52] epoch 3639, training loss: 6006.35, average training loss: 6039.79, base loss: 15992.81
[INFO 2017-06-30 04:14:29,077 main.py:52] epoch 3640, training loss: 6235.92, average training loss: 6040.05, base loss: 15992.36
[INFO 2017-06-30 04:14:32,258 main.py:52] epoch 3641, training loss: 5710.31, average training loss: 6040.06, base loss: 15992.15
[INFO 2017-06-30 04:14:35,475 main.py:52] epoch 3642, training loss: 5861.22, average training loss: 6040.10, base loss: 15992.62
[INFO 2017-06-30 04:14:38,637 main.py:52] epoch 3643, training loss: 5823.04, average training loss: 6039.63, base loss: 15992.24
[INFO 2017-06-30 04:14:41,855 main.py:52] epoch 3644, training loss: 5944.69, average training loss: 6039.04, base loss: 15991.86
[INFO 2017-06-30 04:14:45,062 main.py:52] epoch 3645, training loss: 5994.29, average training loss: 6039.16, base loss: 15991.90
[INFO 2017-06-30 04:14:48,250 main.py:52] epoch 3646, training loss: 5941.75, average training loss: 6038.08, base loss: 15991.89
[INFO 2017-06-30 04:14:51,414 main.py:52] epoch 3647, training loss: 6177.93, average training loss: 6037.86, base loss: 15992.31
[INFO 2017-06-30 04:14:54,597 main.py:52] epoch 3648, training loss: 5924.85, average training loss: 6038.16, base loss: 15992.34
[INFO 2017-06-30 04:14:57,795 main.py:52] epoch 3649, training loss: 5559.69, average training loss: 6037.59, base loss: 15992.33
[INFO 2017-06-30 04:15:00,952 main.py:52] epoch 3650, training loss: 5695.70, average training loss: 6036.58, base loss: 15992.24
[INFO 2017-06-30 04:15:04,109 main.py:52] epoch 3651, training loss: 6513.24, average training loss: 6036.97, base loss: 15993.31
[INFO 2017-06-30 04:15:07,252 main.py:52] epoch 3652, training loss: 6452.02, average training loss: 6037.18, base loss: 15994.60
[INFO 2017-06-30 04:15:10,469 main.py:52] epoch 3653, training loss: 5857.54, average training loss: 6036.55, base loss: 15994.58
[INFO 2017-06-30 04:15:13,645 main.py:52] epoch 3654, training loss: 5525.74, average training loss: 6036.01, base loss: 15993.30
[INFO 2017-06-30 04:15:16,844 main.py:52] epoch 3655, training loss: 5877.58, average training loss: 6035.83, base loss: 15992.48
[INFO 2017-06-30 04:15:20,031 main.py:52] epoch 3656, training loss: 6089.01, average training loss: 6035.79, base loss: 15992.60
[INFO 2017-06-30 04:15:23,179 main.py:52] epoch 3657, training loss: 6071.81, average training loss: 6035.97, base loss: 15992.43
[INFO 2017-06-30 04:15:26,377 main.py:52] epoch 3658, training loss: 5830.07, average training loss: 6034.73, base loss: 15991.85
[INFO 2017-06-30 04:15:29,584 main.py:52] epoch 3659, training loss: 6305.50, average training loss: 6034.88, base loss: 15992.06
[INFO 2017-06-30 04:15:32,724 main.py:52] epoch 3660, training loss: 5759.41, average training loss: 6034.20, base loss: 15992.47
[INFO 2017-06-30 04:15:35,897 main.py:52] epoch 3661, training loss: 6134.99, average training loss: 6033.85, base loss: 15992.65
[INFO 2017-06-30 04:15:39,074 main.py:52] epoch 3662, training loss: 5739.73, average training loss: 6033.41, base loss: 15992.39
[INFO 2017-06-30 04:15:42,260 main.py:52] epoch 3663, training loss: 5914.73, average training loss: 6033.35, base loss: 15992.31
[INFO 2017-06-30 04:15:45,435 main.py:52] epoch 3664, training loss: 5514.75, average training loss: 6032.89, base loss: 15991.44
[INFO 2017-06-30 04:15:48,634 main.py:52] epoch 3665, training loss: 5567.05, average training loss: 6031.71, base loss: 15990.93
[INFO 2017-06-30 04:15:51,774 main.py:52] epoch 3666, training loss: 6196.35, average training loss: 6031.85, base loss: 15991.08
[INFO 2017-06-30 04:15:54,969 main.py:52] epoch 3667, training loss: 5523.97, average training loss: 6031.84, base loss: 15990.56
[INFO 2017-06-30 04:15:58,138 main.py:52] epoch 3668, training loss: 5913.84, average training loss: 6031.42, base loss: 15990.16
[INFO 2017-06-30 04:16:01,309 main.py:52] epoch 3669, training loss: 6163.12, average training loss: 6032.02, base loss: 15990.01
[INFO 2017-06-30 04:16:04,453 main.py:52] epoch 3670, training loss: 5810.74, average training loss: 6031.77, base loss: 15989.99
[INFO 2017-06-30 04:16:07,649 main.py:52] epoch 3671, training loss: 6140.92, average training loss: 6031.70, base loss: 15990.80
[INFO 2017-06-30 04:16:10,803 main.py:52] epoch 3672, training loss: 5692.22, average training loss: 6031.33, base loss: 15989.57
[INFO 2017-06-30 04:16:13,962 main.py:52] epoch 3673, training loss: 5973.27, average training loss: 6030.90, base loss: 15989.35
[INFO 2017-06-30 04:16:17,117 main.py:52] epoch 3674, training loss: 6221.52, average training loss: 6031.01, base loss: 15989.51
[INFO 2017-06-30 04:16:20,234 main.py:52] epoch 3675, training loss: 5341.08, average training loss: 6030.31, base loss: 15988.65
[INFO 2017-06-30 04:16:23,422 main.py:52] epoch 3676, training loss: 5651.56, average training loss: 6030.20, base loss: 15987.86
[INFO 2017-06-30 04:16:26,592 main.py:52] epoch 3677, training loss: 5872.96, average training loss: 6030.25, base loss: 15987.78
[INFO 2017-06-30 04:16:29,800 main.py:52] epoch 3678, training loss: 5918.04, average training loss: 6030.14, base loss: 15986.61
[INFO 2017-06-30 04:16:32,943 main.py:52] epoch 3679, training loss: 5650.22, average training loss: 6029.72, base loss: 15985.38
[INFO 2017-06-30 04:16:36,103 main.py:52] epoch 3680, training loss: 6027.93, average training loss: 6029.40, base loss: 15984.70
[INFO 2017-06-30 04:16:39,262 main.py:52] epoch 3681, training loss: 5684.55, average training loss: 6029.05, base loss: 15984.41
[INFO 2017-06-30 04:16:42,439 main.py:52] epoch 3682, training loss: 5692.77, average training loss: 6028.58, base loss: 15984.23
[INFO 2017-06-30 04:16:45,593 main.py:52] epoch 3683, training loss: 6414.50, average training loss: 6028.79, base loss: 15985.15
[INFO 2017-06-30 04:16:48,790 main.py:52] epoch 3684, training loss: 6134.41, average training loss: 6028.69, base loss: 15985.04
[INFO 2017-06-30 04:16:51,910 main.py:52] epoch 3685, training loss: 6277.11, average training loss: 6028.37, base loss: 15985.30
[INFO 2017-06-30 04:16:55,104 main.py:52] epoch 3686, training loss: 6225.12, average training loss: 6028.59, base loss: 15986.50
[INFO 2017-06-30 04:16:58,244 main.py:52] epoch 3687, training loss: 5720.94, average training loss: 6027.94, base loss: 15986.30
[INFO 2017-06-30 04:17:01,403 main.py:52] epoch 3688, training loss: 5777.75, average training loss: 6028.11, base loss: 15985.81
[INFO 2017-06-30 04:17:04,545 main.py:52] epoch 3689, training loss: 5761.53, average training loss: 6027.75, base loss: 15985.04
[INFO 2017-06-30 04:17:07,678 main.py:52] epoch 3690, training loss: 5687.84, average training loss: 6027.97, base loss: 15985.70
[INFO 2017-06-30 04:17:10,853 main.py:52] epoch 3691, training loss: 5972.93, average training loss: 6027.95, base loss: 15985.87
[INFO 2017-06-30 04:17:14,038 main.py:52] epoch 3692, training loss: 5460.19, average training loss: 6027.76, base loss: 15984.93
[INFO 2017-06-30 04:17:17,176 main.py:52] epoch 3693, training loss: 6255.59, average training loss: 6027.79, base loss: 15985.52
[INFO 2017-06-30 04:17:20,369 main.py:52] epoch 3694, training loss: 5925.41, average training loss: 6027.30, base loss: 15984.99
[INFO 2017-06-30 04:17:23,614 main.py:52] epoch 3695, training loss: 5825.94, average training loss: 6026.93, base loss: 15984.82
[INFO 2017-06-30 04:17:26,915 main.py:52] epoch 3696, training loss: 5782.76, average training loss: 6026.77, base loss: 15984.26
[INFO 2017-06-30 04:17:30,067 main.py:52] epoch 3697, training loss: 5878.61, average training loss: 6026.74, base loss: 15984.45
[INFO 2017-06-30 04:17:33,228 main.py:52] epoch 3698, training loss: 6102.62, average training loss: 6026.81, base loss: 15985.18
[INFO 2017-06-30 04:17:36,450 main.py:52] epoch 3699, training loss: 5679.00, average training loss: 6026.21, base loss: 15984.94
[INFO 2017-06-30 04:17:36,450 main.py:54] epoch 3699, testing
[INFO 2017-06-30 04:17:49,703 main.py:97] average testing loss: 5762.72, base loss: 15456.98
[INFO 2017-06-30 04:17:49,703 main.py:98] improve_loss: 9694.26, improve_percent: 0.63
[INFO 2017-06-30 04:17:49,706 main.py:66] current best improved percent: 0.63
[INFO 2017-06-30 04:17:52,878 main.py:52] epoch 3700, training loss: 6135.87, average training loss: 6025.81, base loss: 15985.18
[INFO 2017-06-30 04:17:56,052 main.py:52] epoch 3701, training loss: 5668.71, average training loss: 6025.15, base loss: 15985.15
[INFO 2017-06-30 04:17:59,215 main.py:52] epoch 3702, training loss: 6230.27, average training loss: 6025.04, base loss: 15985.43
[INFO 2017-06-30 04:18:02,396 main.py:52] epoch 3703, training loss: 5612.85, average training loss: 6024.57, base loss: 15984.67
[INFO 2017-06-30 04:18:05,568 main.py:52] epoch 3704, training loss: 5745.52, average training loss: 6024.54, base loss: 15984.86
[INFO 2017-06-30 04:18:08,742 main.py:52] epoch 3705, training loss: 6362.67, average training loss: 6024.97, base loss: 15986.52
[INFO 2017-06-30 04:18:11,921 main.py:52] epoch 3706, training loss: 6155.92, average training loss: 6025.26, base loss: 15986.54
[INFO 2017-06-30 04:18:15,087 main.py:52] epoch 3707, training loss: 5625.37, average training loss: 6025.18, base loss: 15986.01
[INFO 2017-06-30 04:18:18,256 main.py:52] epoch 3708, training loss: 5881.13, average training loss: 6025.09, base loss: 15985.68
[INFO 2017-06-30 04:18:21,438 main.py:52] epoch 3709, training loss: 5605.82, average training loss: 6024.71, base loss: 15985.44
[INFO 2017-06-30 04:18:24,577 main.py:52] epoch 3710, training loss: 5973.69, average training loss: 6024.29, base loss: 15985.57
[INFO 2017-06-30 04:18:27,736 main.py:52] epoch 3711, training loss: 5465.11, average training loss: 6023.82, base loss: 15984.37
[INFO 2017-06-30 04:18:30,886 main.py:52] epoch 3712, training loss: 6099.90, average training loss: 6023.60, base loss: 15984.69
[INFO 2017-06-30 04:18:34,045 main.py:52] epoch 3713, training loss: 5601.90, average training loss: 6023.33, base loss: 15983.87
[INFO 2017-06-30 04:18:37,209 main.py:52] epoch 3714, training loss: 6092.62, average training loss: 6023.03, base loss: 15984.25
[INFO 2017-06-30 04:18:40,358 main.py:52] epoch 3715, training loss: 5331.15, average training loss: 6022.31, base loss: 15983.18
[INFO 2017-06-30 04:18:43,549 main.py:52] epoch 3716, training loss: 6401.94, average training loss: 6022.58, base loss: 15984.11
[INFO 2017-06-30 04:18:46,724 main.py:52] epoch 3717, training loss: 6000.13, average training loss: 6022.49, base loss: 15983.88
[INFO 2017-06-30 04:18:49,911 main.py:52] epoch 3718, training loss: 6151.62, average training loss: 6022.78, base loss: 15983.44
[INFO 2017-06-30 04:18:53,081 main.py:52] epoch 3719, training loss: 5956.43, average training loss: 6022.67, base loss: 15983.03
[INFO 2017-06-30 04:18:56,243 main.py:52] epoch 3720, training loss: 5648.72, average training loss: 6022.17, base loss: 15982.13
[INFO 2017-06-30 04:18:59,389 main.py:52] epoch 3721, training loss: 5995.23, average training loss: 6022.09, base loss: 15982.36
[INFO 2017-06-30 04:19:02,541 main.py:52] epoch 3722, training loss: 5999.42, average training loss: 6021.30, base loss: 15982.22
[INFO 2017-06-30 04:19:05,743 main.py:52] epoch 3723, training loss: 5823.00, average training loss: 6021.14, base loss: 15982.19
[INFO 2017-06-30 04:19:08,912 main.py:52] epoch 3724, training loss: 5839.58, average training loss: 6021.34, base loss: 15982.69
[INFO 2017-06-30 04:19:12,095 main.py:52] epoch 3725, training loss: 6029.26, average training loss: 6021.22, base loss: 15983.54
[INFO 2017-06-30 04:19:15,250 main.py:52] epoch 3726, training loss: 5432.98, average training loss: 6020.65, base loss: 15982.98
[INFO 2017-06-30 04:19:18,456 main.py:52] epoch 3727, training loss: 5535.19, average training loss: 6020.04, base loss: 15982.19
[INFO 2017-06-30 04:19:21,664 main.py:52] epoch 3728, training loss: 6041.39, average training loss: 6020.37, base loss: 15982.14
[INFO 2017-06-30 04:19:24,831 main.py:52] epoch 3729, training loss: 6264.43, average training loss: 6020.02, base loss: 15982.30
[INFO 2017-06-30 04:19:27,976 main.py:52] epoch 3730, training loss: 6193.47, average training loss: 6020.05, base loss: 15983.10
[INFO 2017-06-30 04:19:31,151 main.py:52] epoch 3731, training loss: 5909.69, average training loss: 6019.84, base loss: 15982.62
[INFO 2017-06-30 04:19:34,310 main.py:52] epoch 3732, training loss: 5561.49, average training loss: 6019.10, base loss: 15981.89
[INFO 2017-06-30 04:19:37,450 main.py:52] epoch 3733, training loss: 5915.98, average training loss: 6018.65, base loss: 15981.00
[INFO 2017-06-30 04:19:40,629 main.py:52] epoch 3734, training loss: 6164.82, average training loss: 6018.41, base loss: 15981.67
[INFO 2017-06-30 04:19:43,833 main.py:52] epoch 3735, training loss: 5897.70, average training loss: 6018.00, base loss: 15980.96
[INFO 2017-06-30 04:19:47,047 main.py:52] epoch 3736, training loss: 5675.27, average training loss: 6017.37, base loss: 15980.26
[INFO 2017-06-30 04:19:50,209 main.py:52] epoch 3737, training loss: 5936.44, average training loss: 6017.16, base loss: 15979.34
[INFO 2017-06-30 04:19:53,379 main.py:52] epoch 3738, training loss: 5626.11, average training loss: 6016.38, base loss: 15978.46
[INFO 2017-06-30 04:19:56,582 main.py:52] epoch 3739, training loss: 6017.07, average training loss: 6016.32, base loss: 15978.09
[INFO 2017-06-30 04:19:59,754 main.py:52] epoch 3740, training loss: 5880.66, average training loss: 6016.04, base loss: 15977.90
[INFO 2017-06-30 04:20:02,927 main.py:52] epoch 3741, training loss: 5918.84, average training loss: 6015.93, base loss: 15977.90
[INFO 2017-06-30 04:20:06,079 main.py:52] epoch 3742, training loss: 6034.61, average training loss: 6015.96, base loss: 15977.92
[INFO 2017-06-30 04:20:09,221 main.py:52] epoch 3743, training loss: 6025.22, average training loss: 6015.31, base loss: 15978.47
[INFO 2017-06-30 04:20:12,415 main.py:52] epoch 3744, training loss: 6286.59, average training loss: 6015.91, base loss: 15978.96
[INFO 2017-06-30 04:20:15,584 main.py:52] epoch 3745, training loss: 6219.19, average training loss: 6016.15, base loss: 15979.53
[INFO 2017-06-30 04:20:18,793 main.py:52] epoch 3746, training loss: 6003.07, average training loss: 6015.81, base loss: 15979.89
[INFO 2017-06-30 04:20:21,952 main.py:52] epoch 3747, training loss: 5361.05, average training loss: 6014.84, base loss: 15978.96
[INFO 2017-06-30 04:20:25,097 main.py:52] epoch 3748, training loss: 6509.95, average training loss: 6015.01, base loss: 15979.81
[INFO 2017-06-30 04:20:28,261 main.py:52] epoch 3749, training loss: 6052.15, average training loss: 6014.69, base loss: 15979.56
[INFO 2017-06-30 04:20:31,497 main.py:52] epoch 3750, training loss: 6251.31, average training loss: 6015.02, base loss: 15979.62
[INFO 2017-06-30 04:20:34,656 main.py:52] epoch 3751, training loss: 5584.52, average training loss: 6014.48, base loss: 15979.32
[INFO 2017-06-30 04:20:37,791 main.py:52] epoch 3752, training loss: 5757.99, average training loss: 6014.45, base loss: 15979.64
[INFO 2017-06-30 04:20:40,956 main.py:52] epoch 3753, training loss: 5483.76, average training loss: 6013.74, base loss: 15979.60
[INFO 2017-06-30 04:20:44,091 main.py:52] epoch 3754, training loss: 6330.92, average training loss: 6014.61, base loss: 15979.99
[INFO 2017-06-30 04:20:47,222 main.py:52] epoch 3755, training loss: 5806.11, average training loss: 6014.47, base loss: 15979.43
[INFO 2017-06-30 04:20:50,415 main.py:52] epoch 3756, training loss: 5959.22, average training loss: 6013.35, base loss: 15980.05
[INFO 2017-06-30 04:20:53,576 main.py:52] epoch 3757, training loss: 6596.74, average training loss: 6013.85, base loss: 15981.56
[INFO 2017-06-30 04:20:56,738 main.py:52] epoch 3758, training loss: 5954.69, average training loss: 6013.73, base loss: 15981.92
[INFO 2017-06-30 04:20:59,887 main.py:52] epoch 3759, training loss: 5963.14, average training loss: 6013.05, base loss: 15981.43
[INFO 2017-06-30 04:21:03,060 main.py:52] epoch 3760, training loss: 6134.16, average training loss: 6013.15, base loss: 15981.50
[INFO 2017-06-30 04:21:06,182 main.py:52] epoch 3761, training loss: 5850.58, average training loss: 6013.19, base loss: 15980.76
[INFO 2017-06-30 04:21:09,350 main.py:52] epoch 3762, training loss: 5557.12, average training loss: 6012.53, base loss: 15980.78
[INFO 2017-06-30 04:21:12,517 main.py:52] epoch 3763, training loss: 5966.63, average training loss: 6012.31, base loss: 15980.84
[INFO 2017-06-30 04:21:15,630 main.py:52] epoch 3764, training loss: 5652.22, average training loss: 6011.55, base loss: 15980.88
[INFO 2017-06-30 04:21:18,798 main.py:52] epoch 3765, training loss: 6046.23, average training loss: 6011.00, base loss: 15980.91
[INFO 2017-06-30 04:21:21,965 main.py:52] epoch 3766, training loss: 5405.60, average training loss: 6010.57, base loss: 15979.45
[INFO 2017-06-30 04:21:25,150 main.py:52] epoch 3767, training loss: 5835.67, average training loss: 6010.05, base loss: 15979.35
[INFO 2017-06-30 04:21:28,348 main.py:52] epoch 3768, training loss: 5904.95, average training loss: 6009.99, base loss: 15979.18
[INFO 2017-06-30 04:21:31,551 main.py:52] epoch 3769, training loss: 6101.36, average training loss: 6009.28, base loss: 15979.10
[INFO 2017-06-30 04:21:34,805 main.py:52] epoch 3770, training loss: 5693.68, average training loss: 6008.88, base loss: 15979.30
[INFO 2017-06-30 04:21:37,938 main.py:52] epoch 3771, training loss: 5786.98, average training loss: 6008.35, base loss: 15979.58
[INFO 2017-06-30 04:21:41,137 main.py:52] epoch 3772, training loss: 5673.57, average training loss: 6008.45, base loss: 15979.32
[INFO 2017-06-30 04:21:44,374 main.py:52] epoch 3773, training loss: 6189.92, average training loss: 6008.42, base loss: 15980.11
[INFO 2017-06-30 04:21:47,543 main.py:52] epoch 3774, training loss: 6300.89, average training loss: 6008.86, base loss: 15980.89
[INFO 2017-06-30 04:21:50,707 main.py:52] epoch 3775, training loss: 5924.29, average training loss: 6008.86, base loss: 15981.61
[INFO 2017-06-30 04:21:53,845 main.py:52] epoch 3776, training loss: 6026.14, average training loss: 6008.57, base loss: 15981.93
[INFO 2017-06-30 04:21:57,000 main.py:52] epoch 3777, training loss: 6201.77, average training loss: 6008.51, base loss: 15982.30
[INFO 2017-06-30 04:22:00,175 main.py:52] epoch 3778, training loss: 5682.93, average training loss: 6008.24, base loss: 15982.57
[INFO 2017-06-30 04:22:03,402 main.py:52] epoch 3779, training loss: 5964.39, average training loss: 6007.70, base loss: 15982.27
[INFO 2017-06-30 04:22:06,573 main.py:52] epoch 3780, training loss: 5657.68, average training loss: 6007.16, base loss: 15982.33
[INFO 2017-06-30 04:22:09,765 main.py:52] epoch 3781, training loss: 5394.79, average training loss: 6006.21, base loss: 15982.08
[INFO 2017-06-30 04:22:12,931 main.py:52] epoch 3782, training loss: 6366.92, average training loss: 6006.11, base loss: 15983.28
[INFO 2017-06-30 04:22:16,090 main.py:52] epoch 3783, training loss: 6410.11, average training loss: 6006.38, base loss: 15983.59
[INFO 2017-06-30 04:22:19,262 main.py:52] epoch 3784, training loss: 5546.53, average training loss: 6006.39, base loss: 15982.96
[INFO 2017-06-30 04:22:22,446 main.py:52] epoch 3785, training loss: 5763.83, average training loss: 6005.97, base loss: 15982.93
[INFO 2017-06-30 04:22:25,593 main.py:52] epoch 3786, training loss: 5804.71, average training loss: 6005.83, base loss: 15983.31
[INFO 2017-06-30 04:22:28,735 main.py:52] epoch 3787, training loss: 5951.91, average training loss: 6006.34, base loss: 15984.01
[INFO 2017-06-30 04:22:31,845 main.py:52] epoch 3788, training loss: 6158.12, average training loss: 6006.49, base loss: 15984.39
[INFO 2017-06-30 04:22:35,015 main.py:52] epoch 3789, training loss: 5621.18, average training loss: 6006.23, base loss: 15983.77
[INFO 2017-06-30 04:22:38,174 main.py:52] epoch 3790, training loss: 5761.57, average training loss: 6005.83, base loss: 15984.19
[INFO 2017-06-30 04:22:41,375 main.py:52] epoch 3791, training loss: 5889.49, average training loss: 6005.42, base loss: 15984.48
[INFO 2017-06-30 04:22:44,548 main.py:52] epoch 3792, training loss: 5698.92, average training loss: 6005.25, base loss: 15984.16
[INFO 2017-06-30 04:22:47,730 main.py:52] epoch 3793, training loss: 5681.13, average training loss: 6005.42, base loss: 15984.58
[INFO 2017-06-30 04:22:50,914 main.py:52] epoch 3794, training loss: 5589.49, average training loss: 6004.70, base loss: 15985.03
[INFO 2017-06-30 04:22:54,117 main.py:52] epoch 3795, training loss: 5677.73, average training loss: 6004.56, base loss: 15984.76
[INFO 2017-06-30 04:22:57,289 main.py:52] epoch 3796, training loss: 5974.81, average training loss: 6004.80, base loss: 15985.05
[INFO 2017-06-30 04:23:00,433 main.py:52] epoch 3797, training loss: 5852.80, average training loss: 6004.48, base loss: 15984.83
[INFO 2017-06-30 04:23:03,613 main.py:52] epoch 3798, training loss: 6089.45, average training loss: 6004.50, base loss: 15985.42
[INFO 2017-06-30 04:23:06,756 main.py:52] epoch 3799, training loss: 6062.39, average training loss: 6004.58, base loss: 15985.85
[INFO 2017-06-30 04:23:06,756 main.py:54] epoch 3799, testing
[INFO 2017-06-30 04:23:20,126 main.py:97] average testing loss: 5805.11, base loss: 15757.62
[INFO 2017-06-30 04:23:20,126 main.py:98] improve_loss: 9952.52, improve_percent: 0.63
[INFO 2017-06-30 04:23:20,127 main.py:62] model save to ./model/final.pth
[INFO 2017-06-30 04:23:20,162 main.py:66] current best improved percent: 0.63
[INFO 2017-06-30 04:23:23,365 main.py:52] epoch 3800, training loss: 5987.69, average training loss: 6004.13, base loss: 15985.98
[INFO 2017-06-30 04:23:26,550 main.py:52] epoch 3801, training loss: 6060.70, average training loss: 6004.17, base loss: 15986.63
[INFO 2017-06-30 04:23:29,731 main.py:52] epoch 3802, training loss: 6121.30, average training loss: 6003.92, base loss: 15987.07
[INFO 2017-06-30 04:23:32,915 main.py:52] epoch 3803, training loss: 6498.76, average training loss: 6003.69, base loss: 15987.87
[INFO 2017-06-30 04:23:36,046 main.py:52] epoch 3804, training loss: 5550.39, average training loss: 6003.40, base loss: 15987.19
[INFO 2017-06-30 04:23:39,233 main.py:52] epoch 3805, training loss: 5717.55, average training loss: 6002.72, base loss: 15986.94
[INFO 2017-06-30 04:23:42,391 main.py:52] epoch 3806, training loss: 5823.69, average training loss: 6002.26, base loss: 15987.43
[INFO 2017-06-30 04:23:45,588 main.py:52] epoch 3807, training loss: 5867.67, average training loss: 6001.83, base loss: 15987.81
[INFO 2017-06-30 04:23:48,781 main.py:52] epoch 3808, training loss: 5897.49, average training loss: 6001.74, base loss: 15988.60
[INFO 2017-06-30 04:23:51,956 main.py:52] epoch 3809, training loss: 6196.30, average training loss: 6001.99, base loss: 15989.16
[INFO 2017-06-30 04:23:55,164 main.py:52] epoch 3810, training loss: 6512.97, average training loss: 6002.34, base loss: 15990.27
[INFO 2017-06-30 04:23:58,339 main.py:52] epoch 3811, training loss: 5972.19, average training loss: 6002.11, base loss: 15990.35
[INFO 2017-06-30 04:24:01,508 main.py:52] epoch 3812, training loss: 5731.06, average training loss: 6001.88, base loss: 15990.13
[INFO 2017-06-30 04:24:04,660 main.py:52] epoch 3813, training loss: 5654.55, average training loss: 6001.10, base loss: 15990.03
[INFO 2017-06-30 04:24:07,844 main.py:52] epoch 3814, training loss: 5706.65, average training loss: 6000.52, base loss: 15990.23
[INFO 2017-06-30 04:24:11,007 main.py:52] epoch 3815, training loss: 5678.07, average training loss: 6000.49, base loss: 15990.41
[INFO 2017-06-30 04:24:14,165 main.py:52] epoch 3816, training loss: 5942.69, average training loss: 6000.04, base loss: 15991.28
[INFO 2017-06-30 04:24:17,336 main.py:52] epoch 3817, training loss: 6307.55, average training loss: 5999.93, base loss: 15992.11
[INFO 2017-06-30 04:24:20,551 main.py:52] epoch 3818, training loss: 5433.85, average training loss: 5999.33, base loss: 15991.39
[INFO 2017-06-30 04:24:23,746 main.py:52] epoch 3819, training loss: 6044.16, average training loss: 5999.07, base loss: 15991.71
[INFO 2017-06-30 04:24:26,955 main.py:52] epoch 3820, training loss: 6080.29, average training loss: 5998.90, base loss: 15991.51
[INFO 2017-06-30 04:24:30,155 main.py:52] epoch 3821, training loss: 5568.06, average training loss: 5998.48, base loss: 15990.52
[INFO 2017-06-30 04:24:33,375 main.py:52] epoch 3822, training loss: 6442.44, average training loss: 5998.37, base loss: 15990.78
[INFO 2017-06-30 04:24:36,528 main.py:52] epoch 3823, training loss: 5660.38, average training loss: 5998.24, base loss: 15989.53
[INFO 2017-06-30 04:24:39,671 main.py:52] epoch 3824, training loss: 5968.02, average training loss: 5998.32, base loss: 15989.50
[INFO 2017-06-30 04:24:42,867 main.py:52] epoch 3825, training loss: 5580.87, average training loss: 5998.28, base loss: 15988.46
[INFO 2017-06-30 04:24:46,058 main.py:52] epoch 3826, training loss: 5963.83, average training loss: 5998.09, base loss: 15988.30
[INFO 2017-06-30 04:24:49,183 main.py:52] epoch 3827, training loss: 6081.50, average training loss: 5998.16, base loss: 15988.21
[INFO 2017-06-30 04:24:52,359 main.py:52] epoch 3828, training loss: 6182.67, average training loss: 5998.40, base loss: 15988.19
[INFO 2017-06-30 04:24:55,496 main.py:52] epoch 3829, training loss: 5974.29, average training loss: 5998.49, base loss: 15988.05
[INFO 2017-06-30 04:24:58,691 main.py:52] epoch 3830, training loss: 6374.07, average training loss: 5999.20, base loss: 15988.47
[INFO 2017-06-30 04:25:01,875 main.py:52] epoch 3831, training loss: 6275.23, average training loss: 5999.13, base loss: 15989.26
[INFO 2017-06-30 04:25:05,021 main.py:52] epoch 3832, training loss: 5563.24, average training loss: 5998.29, base loss: 15988.34
[INFO 2017-06-30 04:25:08,169 main.py:52] epoch 3833, training loss: 5769.80, average training loss: 5998.08, base loss: 15988.18
[INFO 2017-06-30 04:25:11,364 main.py:52] epoch 3834, training loss: 6183.85, average training loss: 5998.33, base loss: 15989.06
[INFO 2017-06-30 04:25:14,552 main.py:52] epoch 3835, training loss: 5934.42, average training loss: 5998.23, base loss: 15988.83
[INFO 2017-06-30 04:25:17,750 main.py:52] epoch 3836, training loss: 6090.44, average training loss: 5998.06, base loss: 15989.08
[INFO 2017-06-30 04:25:20,901 main.py:52] epoch 3837, training loss: 5862.86, average training loss: 5997.47, base loss: 15989.19
[INFO 2017-06-30 04:25:24,063 main.py:52] epoch 3838, training loss: 5837.11, average training loss: 5997.32, base loss: 15989.21
[INFO 2017-06-30 04:25:27,261 main.py:52] epoch 3839, training loss: 5788.85, average training loss: 5996.52, base loss: 15988.99
[INFO 2017-06-30 04:25:30,395 main.py:52] epoch 3840, training loss: 5938.52, average training loss: 5996.21, base loss: 15989.13
[INFO 2017-06-30 04:25:33,576 main.py:52] epoch 3841, training loss: 5565.20, average training loss: 5995.85, base loss: 15988.44
[INFO 2017-06-30 04:25:36,737 main.py:52] epoch 3842, training loss: 6147.09, average training loss: 5995.85, base loss: 15988.71
[INFO 2017-06-30 04:25:39,889 main.py:52] epoch 3843, training loss: 5664.19, average training loss: 5995.15, base loss: 15987.63
[INFO 2017-06-30 04:25:43,108 main.py:52] epoch 3844, training loss: 5714.35, average training loss: 5994.67, base loss: 15987.14
[INFO 2017-06-30 04:25:46,279 main.py:52] epoch 3845, training loss: 6299.56, average training loss: 5994.99, base loss: 15988.35
[INFO 2017-06-30 04:25:49,433 main.py:52] epoch 3846, training loss: 5885.79, average training loss: 5994.44, base loss: 15988.80
[INFO 2017-06-30 04:25:52,598 main.py:52] epoch 3847, training loss: 5678.34, average training loss: 5994.08, base loss: 15988.81
[INFO 2017-06-30 04:25:55,809 main.py:52] epoch 3848, training loss: 5618.07, average training loss: 5993.50, base loss: 15988.14
[INFO 2017-06-30 04:25:58,973 main.py:52] epoch 3849, training loss: 5660.06, average training loss: 5992.23, base loss: 15987.89
[INFO 2017-06-30 04:26:02,137 main.py:52] epoch 3850, training loss: 5880.42, average training loss: 5991.88, base loss: 15987.57
[INFO 2017-06-30 04:26:05,288 main.py:52] epoch 3851, training loss: 5830.98, average training loss: 5991.27, base loss: 15986.87
[INFO 2017-06-30 04:26:08,440 main.py:52] epoch 3852, training loss: 5630.91, average training loss: 5990.89, base loss: 15986.92
[INFO 2017-06-30 04:26:11,632 main.py:52] epoch 3853, training loss: 5978.80, average training loss: 5990.81, base loss: 15987.35
[INFO 2017-06-30 04:26:14,802 main.py:52] epoch 3854, training loss: 5866.65, average training loss: 5990.55, base loss: 15988.27
[INFO 2017-06-30 04:26:17,972 main.py:52] epoch 3855, training loss: 6051.68, average training loss: 5990.48, base loss: 15988.92
[INFO 2017-06-30 04:26:21,106 main.py:52] epoch 3856, training loss: 6180.33, average training loss: 5990.47, base loss: 15989.07
[INFO 2017-06-30 04:26:24,277 main.py:52] epoch 3857, training loss: 5904.35, average training loss: 5990.56, base loss: 15988.76
[INFO 2017-06-30 04:26:27,410 main.py:52] epoch 3858, training loss: 5783.05, average training loss: 5990.65, base loss: 15988.63
[INFO 2017-06-30 04:26:30,624 main.py:52] epoch 3859, training loss: 5831.52, average training loss: 5990.36, base loss: 15988.64
[INFO 2017-06-30 04:26:33,843 main.py:52] epoch 3860, training loss: 5809.84, average training loss: 5989.94, base loss: 15988.52
[INFO 2017-06-30 04:26:36,996 main.py:52] epoch 3861, training loss: 5906.03, average training loss: 5990.06, base loss: 15988.33
[INFO 2017-06-30 04:26:40,112 main.py:52] epoch 3862, training loss: 5597.63, average training loss: 5989.75, base loss: 15988.15
[INFO 2017-06-30 04:26:43,269 main.py:52] epoch 3863, training loss: 6028.53, average training loss: 5989.78, base loss: 15988.31
[INFO 2017-06-30 04:26:46,428 main.py:52] epoch 3864, training loss: 5696.33, average training loss: 5989.48, base loss: 15987.19
[INFO 2017-06-30 04:26:49,583 main.py:52] epoch 3865, training loss: 6330.69, average training loss: 5989.99, base loss: 15987.80
[INFO 2017-06-30 04:26:52,794 main.py:52] epoch 3866, training loss: 6064.66, average training loss: 5989.71, base loss: 15988.02
[INFO 2017-06-30 04:26:55,978 main.py:52] epoch 3867, training loss: 6008.56, average training loss: 5989.86, base loss: 15988.27
[INFO 2017-06-30 04:26:59,115 main.py:52] epoch 3868, training loss: 5891.69, average training loss: 5989.55, base loss: 15988.75
[INFO 2017-06-30 04:27:02,275 main.py:52] epoch 3869, training loss: 5356.41, average training loss: 5989.00, base loss: 15987.74
[INFO 2017-06-30 04:27:05,410 main.py:52] epoch 3870, training loss: 5735.71, average training loss: 5988.76, base loss: 15987.19
[INFO 2017-06-30 04:27:08,554 main.py:52] epoch 3871, training loss: 5627.97, average training loss: 5987.78, base loss: 15986.20
[INFO 2017-06-30 04:27:11,747 main.py:52] epoch 3872, training loss: 6410.54, average training loss: 5988.37, base loss: 15987.00
[INFO 2017-06-30 04:27:14,924 main.py:52] epoch 3873, training loss: 5992.67, average training loss: 5987.94, base loss: 15988.00
[INFO 2017-06-30 04:27:18,129 main.py:52] epoch 3874, training loss: 5673.71, average training loss: 5987.30, base loss: 15987.83
[INFO 2017-06-30 04:27:21,294 main.py:52] epoch 3875, training loss: 6003.26, average training loss: 5987.22, base loss: 15988.16
[INFO 2017-06-30 04:27:24,428 main.py:52] epoch 3876, training loss: 5827.96, average training loss: 5986.92, base loss: 15987.95
[INFO 2017-06-30 04:27:27,583 main.py:52] epoch 3877, training loss: 5861.46, average training loss: 5987.05, base loss: 15987.44
[INFO 2017-06-30 04:27:30,754 main.py:52] epoch 3878, training loss: 5035.59, average training loss: 5985.03, base loss: 15985.95
[INFO 2017-06-30 04:27:33,897 main.py:52] epoch 3879, training loss: 5752.21, average training loss: 5984.40, base loss: 15985.58
[INFO 2017-06-30 04:27:37,095 main.py:52] epoch 3880, training loss: 6022.24, average training loss: 5984.13, base loss: 15986.15
[INFO 2017-06-30 04:27:40,289 main.py:52] epoch 3881, training loss: 5468.73, average training loss: 5983.68, base loss: 15985.13
[INFO 2017-06-30 04:27:43,466 main.py:52] epoch 3882, training loss: 6980.71, average training loss: 5984.82, base loss: 15986.87
[INFO 2017-06-30 04:27:46,665 main.py:52] epoch 3883, training loss: 5868.04, average training loss: 5984.77, base loss: 15987.45
[INFO 2017-06-30 04:27:49,836 main.py:52] epoch 3884, training loss: 5607.49, average training loss: 5984.60, base loss: 15986.96
[INFO 2017-06-30 04:27:52,991 main.py:52] epoch 3885, training loss: 6070.26, average training loss: 5984.15, base loss: 15986.82
[INFO 2017-06-30 04:27:56,124 main.py:52] epoch 3886, training loss: 5585.97, average training loss: 5983.49, base loss: 15986.35
[INFO 2017-06-30 04:27:59,263 main.py:52] epoch 3887, training loss: 6095.66, average training loss: 5983.09, base loss: 15986.64
[INFO 2017-06-30 04:28:02,458 main.py:52] epoch 3888, training loss: 5864.53, average training loss: 5982.83, base loss: 15986.31
[INFO 2017-06-30 04:28:05,628 main.py:52] epoch 3889, training loss: 6013.70, average training loss: 5982.81, base loss: 15986.08
[INFO 2017-06-30 04:28:08,797 main.py:52] epoch 3890, training loss: 5797.25, average training loss: 5982.22, base loss: 15985.60
[INFO 2017-06-30 04:28:11,997 main.py:52] epoch 3891, training loss: 5733.38, average training loss: 5981.19, base loss: 15985.09
[INFO 2017-06-30 04:28:15,135 main.py:52] epoch 3892, training loss: 5899.98, average training loss: 5981.10, base loss: 15985.13
[INFO 2017-06-30 04:28:18,296 main.py:52] epoch 3893, training loss: 5698.27, average training loss: 5980.92, base loss: 15984.69
[INFO 2017-06-30 04:28:21,511 main.py:52] epoch 3894, training loss: 6020.36, average training loss: 5980.24, base loss: 15985.07
[INFO 2017-06-30 04:28:24,713 main.py:52] epoch 3895, training loss: 5535.83, average training loss: 5979.61, base loss: 15984.86
[INFO 2017-06-30 04:28:27,866 main.py:52] epoch 3896, training loss: 6510.29, average training loss: 5979.99, base loss: 15986.01
[INFO 2017-06-30 04:28:31,049 main.py:52] epoch 3897, training loss: 5935.65, average training loss: 5980.12, base loss: 15986.27
[INFO 2017-06-30 04:28:34,221 main.py:52] epoch 3898, training loss: 6087.90, average training loss: 5980.16, base loss: 15986.39
[INFO 2017-06-30 04:28:37,408 main.py:52] epoch 3899, training loss: 5513.04, average training loss: 5979.49, base loss: 15986.19
[INFO 2017-06-30 04:28:37,408 main.py:54] epoch 3899, testing
[INFO 2017-06-30 04:28:50,883 main.py:97] average testing loss: 5897.35, base loss: 16128.75
[INFO 2017-06-30 04:28:50,883 main.py:98] improve_loss: 10231.40, improve_percent: 0.63
[INFO 2017-06-30 04:28:50,886 main.py:62] model save to ./model/final.pth
[INFO 2017-06-30 04:28:50,920 main.py:66] current best improved percent: 0.63
[INFO 2017-06-30 04:28:54,066 main.py:52] epoch 3900, training loss: 5751.27, average training loss: 5978.86, base loss: 15985.64
[INFO 2017-06-30 04:28:57,254 main.py:52] epoch 3901, training loss: 6180.07, average training loss: 5979.25, base loss: 15985.88
[INFO 2017-06-30 04:29:00,445 main.py:52] epoch 3902, training loss: 5902.67, average training loss: 5979.22, base loss: 15985.93
[INFO 2017-06-30 04:29:03,637 main.py:52] epoch 3903, training loss: 5924.24, average training loss: 5979.15, base loss: 15985.85
[INFO 2017-06-30 04:29:06,819 main.py:52] epoch 3904, training loss: 5985.77, average training loss: 5978.92, base loss: 15986.45
[INFO 2017-06-30 04:29:10,034 main.py:52] epoch 3905, training loss: 5964.58, average training loss: 5978.39, base loss: 15986.08
[INFO 2017-06-30 04:29:13,217 main.py:52] epoch 3906, training loss: 5907.23, average training loss: 5978.41, base loss: 15986.42
[INFO 2017-06-30 04:29:16,370 main.py:52] epoch 3907, training loss: 5809.83, average training loss: 5978.59, base loss: 15986.47
[INFO 2017-06-30 04:29:19,533 main.py:52] epoch 3908, training loss: 6065.29, average training loss: 5978.85, base loss: 15987.08
[INFO 2017-06-30 04:29:22,721 main.py:52] epoch 3909, training loss: 6194.89, average training loss: 5978.97, base loss: 15987.48
[INFO 2017-06-30 04:29:25,907 main.py:52] epoch 3910, training loss: 5834.42, average training loss: 5979.03, base loss: 15987.97
[INFO 2017-06-30 04:29:29,101 main.py:52] epoch 3911, training loss: 6062.54, average training loss: 5979.38, base loss: 15989.24
[INFO 2017-06-30 04:29:32,300 main.py:52] epoch 3912, training loss: 5925.04, average training loss: 5978.86, base loss: 15989.06
[INFO 2017-06-30 04:29:35,476 main.py:52] epoch 3913, training loss: 5926.85, average training loss: 5978.72, base loss: 15989.34
[INFO 2017-06-30 04:29:38,663 main.py:52] epoch 3914, training loss: 6054.05, average training loss: 5978.13, base loss: 15989.79
[INFO 2017-06-30 04:29:41,853 main.py:52] epoch 3915, training loss: 6066.19, average training loss: 5978.32, base loss: 15989.52
[INFO 2017-06-30 04:29:45,061 main.py:52] epoch 3916, training loss: 6167.79, average training loss: 5978.83, base loss: 15989.54
[INFO 2017-06-30 04:29:48,224 main.py:52] epoch 3917, training loss: 6416.21, average training loss: 5979.10, base loss: 15990.29
[INFO 2017-06-30 04:29:51,407 main.py:52] epoch 3918, training loss: 5637.48, average training loss: 5978.66, base loss: 15989.18
[INFO 2017-06-30 04:29:54,570 main.py:52] epoch 3919, training loss: 5846.85, average training loss: 5978.48, base loss: 15988.30
[INFO 2017-06-30 04:29:57,730 main.py:52] epoch 3920, training loss: 5895.10, average training loss: 5978.35, base loss: 15988.03
[INFO 2017-06-30 04:30:00,922 main.py:52] epoch 3921, training loss: 6304.01, average training loss: 5978.23, base loss: 15988.70
[INFO 2017-06-30 04:30:04,093 main.py:52] epoch 3922, training loss: 5865.81, average training loss: 5977.94, base loss: 15988.37
[INFO 2017-06-30 04:30:07,283 main.py:52] epoch 3923, training loss: 5910.52, average training loss: 5978.06, base loss: 15988.13
[INFO 2017-06-30 04:30:10,485 main.py:52] epoch 3924, training loss: 5972.49, average training loss: 5977.15, base loss: 15988.55
[INFO 2017-06-30 04:30:13,684 main.py:52] epoch 3925, training loss: 5690.61, average training loss: 5976.87, base loss: 15987.94
[INFO 2017-06-30 04:30:16,841 main.py:52] epoch 3926, training loss: 5645.96, average training loss: 5975.84, base loss: 15987.63
[INFO 2017-06-30 04:30:20,012 main.py:52] epoch 3927, training loss: 5675.97, average training loss: 5975.30, base loss: 15987.13
[INFO 2017-06-30 04:30:23,162 main.py:52] epoch 3928, training loss: 6262.05, average training loss: 5975.64, base loss: 15987.49
[INFO 2017-06-30 04:30:26,334 main.py:52] epoch 3929, training loss: 5846.81, average training loss: 5975.04, base loss: 15986.77
[INFO 2017-06-30 04:30:29,467 main.py:52] epoch 3930, training loss: 5617.15, average training loss: 5974.85, base loss: 15986.47
[INFO 2017-06-30 04:30:32,664 main.py:52] epoch 3931, training loss: 5737.02, average training loss: 5974.92, base loss: 15985.42
[INFO 2017-06-30 04:30:35,820 main.py:52] epoch 3932, training loss: 5810.97, average training loss: 5973.79, base loss: 15984.95
[INFO 2017-06-30 04:30:38,995 main.py:52] epoch 3933, training loss: 5865.55, average training loss: 5973.50, base loss: 15983.65
[INFO 2017-06-30 04:30:42,223 main.py:52] epoch 3934, training loss: 6023.73, average training loss: 5973.51, base loss: 15983.17
[INFO 2017-06-30 04:30:45,401 main.py:52] epoch 3935, training loss: 6086.64, average training loss: 5973.38, base loss: 15983.17
[INFO 2017-06-30 04:30:48,604 main.py:52] epoch 3936, training loss: 5922.83, average training loss: 5973.00, base loss: 15983.78
[INFO 2017-06-30 04:30:51,761 main.py:52] epoch 3937, training loss: 5972.95, average training loss: 5973.15, base loss: 15984.74
[INFO 2017-06-30 04:30:54,942 main.py:52] epoch 3938, training loss: 5751.64, average training loss: 5973.06, base loss: 15984.66
[INFO 2017-06-30 04:30:58,122 main.py:52] epoch 3939, training loss: 6021.37, average training loss: 5973.52, base loss: 15984.92
[INFO 2017-06-30 04:31:01,261 main.py:52] epoch 3940, training loss: 5525.77, average training loss: 5972.78, base loss: 15984.59
[INFO 2017-06-30 04:31:04,465 main.py:52] epoch 3941, training loss: 5736.08, average training loss: 5972.08, base loss: 15984.49
[INFO 2017-06-30 04:31:07,613 main.py:52] epoch 3942, training loss: 5893.88, average training loss: 5971.51, base loss: 15984.95
[INFO 2017-06-30 04:31:10,784 main.py:52] epoch 3943, training loss: 5480.35, average training loss: 5970.91, base loss: 15984.43
[INFO 2017-06-30 04:31:13,949 main.py:52] epoch 3944, training loss: 6129.28, average training loss: 5970.83, base loss: 15984.89
[INFO 2017-06-30 04:31:17,138 main.py:52] epoch 3945, training loss: 5943.54, average training loss: 5970.67, base loss: 15985.02
[INFO 2017-06-30 04:31:20,352 main.py:52] epoch 3946, training loss: 6226.73, average training loss: 5970.80, base loss: 15985.91
[INFO 2017-06-30 04:31:23,548 main.py:52] epoch 3947, training loss: 5996.05, average training loss: 5970.77, base loss: 15986.16
[INFO 2017-06-30 04:31:26,722 main.py:52] epoch 3948, training loss: 6120.09, average training loss: 5971.00, base loss: 15986.05
[INFO 2017-06-30 04:31:29,902 main.py:52] epoch 3949, training loss: 5668.43, average training loss: 5970.85, base loss: 15985.54
[INFO 2017-06-30 04:31:33,110 main.py:52] epoch 3950, training loss: 6266.24, average training loss: 5970.59, base loss: 15986.53
[INFO 2017-06-30 04:31:36,280 main.py:52] epoch 3951, training loss: 5745.46, average training loss: 5970.23, base loss: 15987.03
[INFO 2017-06-30 04:31:39,452 main.py:52] epoch 3952, training loss: 5908.44, average training loss: 5970.33, base loss: 15987.40
[INFO 2017-06-30 04:31:42,613 main.py:52] epoch 3953, training loss: 6046.07, average training loss: 5970.49, base loss: 15988.56
[INFO 2017-06-30 04:31:45,768 main.py:52] epoch 3954, training loss: 6083.34, average training loss: 5970.11, base loss: 15989.16
[INFO 2017-06-30 04:31:48,931 main.py:52] epoch 3955, training loss: 5963.11, average training loss: 5970.40, base loss: 15988.76
[INFO 2017-06-30 04:31:52,077 main.py:52] epoch 3956, training loss: 5836.74, average training loss: 5970.36, base loss: 15987.71
[INFO 2017-06-30 04:31:55,287 main.py:52] epoch 3957, training loss: 5625.97, average training loss: 5970.04, base loss: 15987.42
[INFO 2017-06-30 04:31:58,483 main.py:52] epoch 3958, training loss: 5705.52, average training loss: 5969.03, base loss: 15986.89
[INFO 2017-06-30 04:32:01,666 main.py:52] epoch 3959, training loss: 5438.70, average training loss: 5968.94, base loss: 15986.76
[INFO 2017-06-30 04:32:04,830 main.py:52] epoch 3960, training loss: 5592.81, average training loss: 5967.99, base loss: 15986.10
[INFO 2017-06-30 04:32:07,985 main.py:52] epoch 3961, training loss: 5712.33, average training loss: 5967.74, base loss: 15984.71
[INFO 2017-06-30 04:32:11,140 main.py:52] epoch 3962, training loss: 6155.63, average training loss: 5967.79, base loss: 15985.66
[INFO 2017-06-30 04:32:14,299 main.py:52] epoch 3963, training loss: 6184.06, average training loss: 5967.99, base loss: 15986.39
[INFO 2017-06-30 04:32:17,441 main.py:52] epoch 3964, training loss: 5975.40, average training loss: 5967.46, base loss: 15987.21
[INFO 2017-06-30 04:32:20,645 main.py:52] epoch 3965, training loss: 5885.48, average training loss: 5967.12, base loss: 15987.43
[INFO 2017-06-30 04:32:23,835 main.py:52] epoch 3966, training loss: 5928.16, average training loss: 5966.95, base loss: 15987.46
[INFO 2017-06-30 04:32:26,999 main.py:52] epoch 3967, training loss: 6272.50, average training loss: 5967.32, base loss: 15988.68
[INFO 2017-06-30 04:32:30,143 main.py:52] epoch 3968, training loss: 6071.05, average training loss: 5967.42, base loss: 15987.91
[INFO 2017-06-30 04:32:33,296 main.py:52] epoch 3969, training loss: 5826.91, average training loss: 5966.77, base loss: 15988.13
[INFO 2017-06-30 04:32:36,514 main.py:52] epoch 3970, training loss: 6054.01, average training loss: 5966.48, base loss: 15988.25
[INFO 2017-06-30 04:32:39,661 main.py:52] epoch 3971, training loss: 5567.26, average training loss: 5965.90, base loss: 15987.17
[INFO 2017-06-30 04:32:42,822 main.py:52] epoch 3972, training loss: 5760.98, average training loss: 5965.64, base loss: 15986.44
[INFO 2017-06-30 04:32:46,005 main.py:52] epoch 3973, training loss: 5686.57, average training loss: 5965.02, base loss: 15985.63
[INFO 2017-06-30 04:32:49,166 main.py:52] epoch 3974, training loss: 5781.27, average training loss: 5964.45, base loss: 15985.38
[INFO 2017-06-30 04:32:52,367 main.py:52] epoch 3975, training loss: 5986.58, average training loss: 5964.32, base loss: 15986.06
[INFO 2017-06-30 04:32:55,499 main.py:52] epoch 3976, training loss: 5972.11, average training loss: 5964.66, base loss: 15986.55
[INFO 2017-06-30 04:32:58,671 main.py:52] epoch 3977, training loss: 6133.19, average training loss: 5964.80, base loss: 15987.66
[INFO 2017-06-30 04:33:01,866 main.py:52] epoch 3978, training loss: 5938.45, average training loss: 5965.04, base loss: 15988.19
[INFO 2017-06-30 04:33:05,080 main.py:52] epoch 3979, training loss: 6039.67, average training loss: 5964.93, base loss: 15988.36
[INFO 2017-06-30 04:33:08,289 main.py:52] epoch 3980, training loss: 5710.67, average training loss: 5964.28, base loss: 15988.08
[INFO 2017-06-30 04:33:11,480 main.py:52] epoch 3981, training loss: 6133.48, average training loss: 5964.55, base loss: 15988.37
[INFO 2017-06-30 04:33:14,644 main.py:52] epoch 3982, training loss: 5896.78, average training loss: 5964.63, base loss: 15988.51
[INFO 2017-06-30 04:33:17,795 main.py:52] epoch 3983, training loss: 6206.39, average training loss: 5965.00, base loss: 15989.16
[INFO 2017-06-30 04:33:20,998 main.py:52] epoch 3984, training loss: 5676.51, average training loss: 5965.08, base loss: 15989.03
[INFO 2017-06-30 04:33:24,217 main.py:52] epoch 3985, training loss: 5719.78, average training loss: 5964.87, base loss: 15989.10
[INFO 2017-06-30 04:33:27,396 main.py:52] epoch 3986, training loss: 5706.54, average training loss: 5964.51, base loss: 15988.96
[INFO 2017-06-30 04:33:30,560 main.py:52] epoch 3987, training loss: 5638.16, average training loss: 5964.10, base loss: 15988.27
[INFO 2017-06-30 04:33:33,792 main.py:52] epoch 3988, training loss: 5728.55, average training loss: 5963.94, base loss: 15987.49
[INFO 2017-06-30 04:33:37,015 main.py:52] epoch 3989, training loss: 5605.77, average training loss: 5963.87, base loss: 15986.97
[INFO 2017-06-30 04:33:40,165 main.py:52] epoch 3990, training loss: 6358.79, average training loss: 5964.00, base loss: 15988.06
[INFO 2017-06-30 04:33:43,359 main.py:52] epoch 3991, training loss: 6032.76, average training loss: 5964.03, base loss: 15989.04
[INFO 2017-06-30 04:33:46,552 main.py:52] epoch 3992, training loss: 6449.06, average training loss: 5964.12, base loss: 15989.38
[INFO 2017-06-30 04:33:49,726 main.py:52] epoch 3993, training loss: 5838.50, average training loss: 5963.26, base loss: 15988.82
[INFO 2017-06-30 04:33:52,904 main.py:52] epoch 3994, training loss: 6053.45, average training loss: 5963.73, base loss: 15989.52
[INFO 2017-06-30 04:33:56,078 main.py:52] epoch 3995, training loss: 6003.73, average training loss: 5963.48, base loss: 15989.85
[INFO 2017-06-30 04:33:59,280 main.py:52] epoch 3996, training loss: 5545.73, average training loss: 5963.01, base loss: 15989.88
[INFO 2017-06-30 04:34:02,437 main.py:52] epoch 3997, training loss: 5734.97, average training loss: 5962.81, base loss: 15989.90
[INFO 2017-06-30 04:34:05,597 main.py:52] epoch 3998, training loss: 6200.41, average training loss: 5962.80, base loss: 15990.35
[INFO 2017-06-30 04:34:08,791 main.py:52] epoch 3999, training loss: 6265.86, average training loss: 5963.31, base loss: 15991.09
[INFO 2017-06-30 04:34:08,791 main.py:54] epoch 3999, testing
[INFO 2017-06-30 04:34:22,127 main.py:97] average testing loss: 5919.26, base loss: 16205.39
[INFO 2017-06-30 04:34:22,128 main.py:98] improve_loss: 10286.13, improve_percent: 0.63
[INFO 2017-06-30 04:34:22,129 main.py:62] model save to ./model/final.pth
[INFO 2017-06-30 04:34:22,163 main.py:66] current best improved percent: 0.63
[INFO 2017-06-30 04:34:25,295 main.py:52] epoch 4000, training loss: 5781.59, average training loss: 5962.95, base loss: 15990.84
[INFO 2017-06-30 04:34:28,496 main.py:52] epoch 4001, training loss: 5869.15, average training loss: 5962.95, base loss: 15990.01
[INFO 2017-06-30 04:34:31,676 main.py:52] epoch 4002, training loss: 5665.29, average training loss: 5962.45, base loss: 15989.43
[INFO 2017-06-30 04:34:34,828 main.py:52] epoch 4003, training loss: 5846.25, average training loss: 5962.17, base loss: 15989.44
[INFO 2017-06-30 04:34:38,012 main.py:52] epoch 4004, training loss: 5958.14, average training loss: 5962.57, base loss: 15989.41
[INFO 2017-06-30 04:34:41,225 main.py:52] epoch 4005, training loss: 5758.71, average training loss: 5962.08, base loss: 15988.73
[INFO 2017-06-30 04:34:44,376 main.py:52] epoch 4006, training loss: 5509.41, average training loss: 5961.39, base loss: 15987.56
[INFO 2017-06-30 04:34:47,508 main.py:52] epoch 4007, training loss: 5590.74, average training loss: 5961.29, base loss: 15986.94
[INFO 2017-06-30 04:34:50,691 main.py:52] epoch 4008, training loss: 5965.39, average training loss: 5961.31, base loss: 15987.33
[INFO 2017-06-30 04:34:53,873 main.py:52] epoch 4009, training loss: 6078.33, average training loss: 5961.34, base loss: 15987.87
[INFO 2017-06-30 04:34:57,003 main.py:52] epoch 4010, training loss: 5851.54, average training loss: 5961.11, base loss: 15987.97
[INFO 2017-06-30 04:35:00,157 main.py:52] epoch 4011, training loss: 5756.57, average training loss: 5960.98, base loss: 15988.61
[INFO 2017-06-30 04:35:03,329 main.py:52] epoch 4012, training loss: 5625.78, average training loss: 5960.40, base loss: 15988.07
[INFO 2017-06-30 04:35:06,458 main.py:52] epoch 4013, training loss: 5313.71, average training loss: 5959.58, base loss: 15987.22
[INFO 2017-06-30 04:35:09,624 main.py:52] epoch 4014, training loss: 6033.26, average training loss: 5959.07, base loss: 15987.49
[INFO 2017-06-30 04:35:12,781 main.py:52] epoch 4015, training loss: 5656.53, average training loss: 5958.85, base loss: 15987.13
[INFO 2017-06-30 04:35:15,945 main.py:52] epoch 4016, training loss: 5927.75, average training loss: 5958.83, base loss: 15987.13
[INFO 2017-06-30 04:35:19,105 main.py:52] epoch 4017, training loss: 6006.39, average training loss: 5959.04, base loss: 15986.95
[INFO 2017-06-30 04:35:22,290 main.py:52] epoch 4018, training loss: 5555.23, average training loss: 5958.21, base loss: 15986.35
[INFO 2017-06-30 04:35:25,463 main.py:52] epoch 4019, training loss: 5940.42, average training loss: 5958.36, base loss: 15986.82
[INFO 2017-06-30 04:35:28,607 main.py:52] epoch 4020, training loss: 6132.84, average training loss: 5958.57, base loss: 15987.65
[INFO 2017-06-30 04:35:31,782 main.py:52] epoch 4021, training loss: 5902.85, average training loss: 5958.38, base loss: 15988.36
[INFO 2017-06-30 04:35:34,978 main.py:52] epoch 4022, training loss: 5658.21, average training loss: 5957.63, base loss: 15988.65
[INFO 2017-06-30 04:35:38,119 main.py:52] epoch 4023, training loss: 5679.66, average training loss: 5957.26, base loss: 15988.41
[INFO 2017-06-30 04:35:41,294 main.py:52] epoch 4024, training loss: 5374.25, average training loss: 5956.50, base loss: 15988.06
[INFO 2017-06-30 04:35:44,477 main.py:52] epoch 4025, training loss: 6163.16, average training loss: 5956.38, base loss: 15988.63
[INFO 2017-06-30 04:35:47,642 main.py:52] epoch 4026, training loss: 6162.95, average training loss: 5956.64, base loss: 15988.86
[INFO 2017-06-30 04:35:50,797 main.py:52] epoch 4027, training loss: 6555.23, average training loss: 5957.34, base loss: 15988.74
[INFO 2017-06-30 04:35:53,970 main.py:52] epoch 4028, training loss: 5863.50, average training loss: 5957.28, base loss: 15988.43
[INFO 2017-06-30 04:35:57,163 main.py:52] epoch 4029, training loss: 5800.72, average training loss: 5956.46, base loss: 15987.97
[INFO 2017-06-30 04:36:00,306 main.py:52] epoch 4030, training loss: 6193.17, average training loss: 5956.64, base loss: 15988.84
[INFO 2017-06-30 04:36:03,478 main.py:52] epoch 4031, training loss: 5661.85, average training loss: 5956.33, base loss: 15988.49
[INFO 2017-06-30 04:36:06,612 main.py:52] epoch 4032, training loss: 5543.45, average training loss: 5955.89, base loss: 15987.61
[INFO 2017-06-30 04:36:09,774 main.py:52] epoch 4033, training loss: 6095.11, average training loss: 5955.61, base loss: 15987.08
[INFO 2017-06-30 04:36:12,931 main.py:52] epoch 4034, training loss: 5681.89, average training loss: 5955.27, base loss: 15986.69
[INFO 2017-06-30 04:36:16,091 main.py:52] epoch 4035, training loss: 5737.57, average training loss: 5954.78, base loss: 15987.17
[INFO 2017-06-30 04:36:19,291 main.py:52] epoch 4036, training loss: 6031.11, average training loss: 5954.33, base loss: 15987.74
[INFO 2017-06-30 04:36:22,463 main.py:52] epoch 4037, training loss: 6244.01, average training loss: 5953.95, base loss: 15988.48
[INFO 2017-06-30 04:36:25,616 main.py:52] epoch 4038, training loss: 5564.36, average training loss: 5953.52, base loss: 15989.00
[INFO 2017-06-30 04:36:28,809 main.py:52] epoch 4039, training loss: 5363.27, average training loss: 5952.98, base loss: 15988.46
[INFO 2017-06-30 04:36:31,971 main.py:52] epoch 4040, training loss: 5889.64, average training loss: 5952.98, base loss: 15988.11
[INFO 2017-06-30 04:36:35,134 main.py:52] epoch 4041, training loss: 5186.04, average training loss: 5952.08, base loss: 15987.50
[INFO 2017-06-30 04:36:38,299 main.py:52] epoch 4042, training loss: 5689.72, average training loss: 5951.71, base loss: 15987.04
[INFO 2017-06-30 04:36:41,479 main.py:52] epoch 4043, training loss: 6322.59, average training loss: 5951.80, base loss: 15987.30
[INFO 2017-06-30 04:36:44,693 main.py:52] epoch 4044, training loss: 5692.49, average training loss: 5951.75, base loss: 15987.22
[INFO 2017-06-30 04:36:47,881 main.py:52] epoch 4045, training loss: 6064.78, average training loss: 5951.72, base loss: 15987.77
[INFO 2017-06-30 04:36:51,094 main.py:52] epoch 4046, training loss: 5926.00, average training loss: 5951.94, base loss: 15988.43
[INFO 2017-06-30 04:36:54,261 main.py:52] epoch 4047, training loss: 5846.86, average training loss: 5951.89, base loss: 15988.73
[INFO 2017-06-30 04:36:57,433 main.py:52] epoch 4048, training loss: 5760.95, average training loss: 5951.54, base loss: 15987.96
[INFO 2017-06-30 04:37:00,562 main.py:52] epoch 4049, training loss: 5989.55, average training loss: 5951.46, base loss: 15987.70
[INFO 2017-06-30 04:37:03,739 main.py:52] epoch 4050, training loss: 5686.53, average training loss: 5951.48, base loss: 15986.53
[INFO 2017-06-30 04:37:06,878 main.py:52] epoch 4051, training loss: 6390.03, average training loss: 5951.90, base loss: 15987.66
[INFO 2017-06-30 04:37:10,046 main.py:52] epoch 4052, training loss: 5782.75, average training loss: 5951.57, base loss: 15987.36
[INFO 2017-06-30 04:37:13,196 main.py:52] epoch 4053, training loss: 5849.32, average training loss: 5951.07, base loss: 15987.22
[INFO 2017-06-30 04:37:16,382 main.py:52] epoch 4054, training loss: 5589.25, average training loss: 5950.62, base loss: 15987.18
[INFO 2017-06-30 04:37:19,588 main.py:52] epoch 4055, training loss: 5540.46, average training loss: 5949.84, base loss: 15987.25
[INFO 2017-06-30 04:37:22,746 main.py:52] epoch 4056, training loss: 6082.65, average training loss: 5949.65, base loss: 15987.32
[INFO 2017-06-30 04:37:25,887 main.py:52] epoch 4057, training loss: 6308.73, average training loss: 5950.22, base loss: 15987.32
[INFO 2017-06-30 04:37:29,076 main.py:52] epoch 4058, training loss: 5578.34, average training loss: 5949.15, base loss: 15986.64
[INFO 2017-06-30 04:37:32,264 main.py:52] epoch 4059, training loss: 5611.52, average training loss: 5948.84, base loss: 15986.27
[INFO 2017-06-30 04:37:35,419 main.py:52] epoch 4060, training loss: 5919.96, average training loss: 5948.95, base loss: 15985.56
[INFO 2017-06-30 04:37:38,576 main.py:52] epoch 4061, training loss: 6211.15, average training loss: 5949.34, base loss: 15985.59
[INFO 2017-06-30 04:37:41,752 main.py:52] epoch 4062, training loss: 5915.04, average training loss: 5949.16, base loss: 15985.52
[INFO 2017-06-30 04:37:44,930 main.py:52] epoch 4063, training loss: 5930.07, average training loss: 5948.91, base loss: 15985.98
[INFO 2017-06-30 04:37:48,111 main.py:52] epoch 4064, training loss: 5337.45, average training loss: 5948.36, base loss: 15985.72
[INFO 2017-06-30 04:37:51,270 main.py:52] epoch 4065, training loss: 6333.83, average training loss: 5948.79, base loss: 15986.51
[INFO 2017-06-30 04:37:54,456 main.py:52] epoch 4066, training loss: 5528.12, average training loss: 5948.62, base loss: 15985.15
[INFO 2017-06-30 04:37:57,641 main.py:52] epoch 4067, training loss: 5995.65, average training loss: 5948.69, base loss: 15985.43
[INFO 2017-06-30 04:38:00,810 main.py:52] epoch 4068, training loss: 5538.08, average training loss: 5948.49, base loss: 15985.00
[INFO 2017-06-30 04:38:03,991 main.py:52] epoch 4069, training loss: 5619.36, average training loss: 5948.06, base loss: 15984.45
[INFO 2017-06-30 04:38:07,158 main.py:52] epoch 4070, training loss: 5714.62, average training loss: 5947.49, base loss: 15984.16
[INFO 2017-06-30 04:38:10,325 main.py:52] epoch 4071, training loss: 6032.96, average training loss: 5947.34, base loss: 15983.61
[INFO 2017-06-30 04:38:13,463 main.py:52] epoch 4072, training loss: 5366.09, average training loss: 5946.85, base loss: 15982.42
[INFO 2017-06-30 04:38:16,613 main.py:52] epoch 4073, training loss: 5684.02, average training loss: 5946.78, base loss: 15981.83
[INFO 2017-06-30 04:38:19,781 main.py:52] epoch 4074, training loss: 6010.24, average training loss: 5946.78, base loss: 15982.07
[INFO 2017-06-30 04:38:22,962 main.py:52] epoch 4075, training loss: 5712.00, average training loss: 5946.42, base loss: 15981.94
[INFO 2017-06-30 04:38:26,163 main.py:52] epoch 4076, training loss: 5876.76, average training loss: 5946.68, base loss: 15981.36
[INFO 2017-06-30 04:38:29,342 main.py:52] epoch 4077, training loss: 5858.39, average training loss: 5946.61, base loss: 15981.51
[INFO 2017-06-30 04:38:32,527 main.py:52] epoch 4078, training loss: 5335.38, average training loss: 5946.04, base loss: 15980.81
[INFO 2017-06-30 04:38:35,698 main.py:52] epoch 4079, training loss: 6036.27, average training loss: 5946.15, base loss: 15981.36
[INFO 2017-06-30 04:38:38,891 main.py:52] epoch 4080, training loss: 5531.10, average training loss: 5945.30, base loss: 15980.66
[INFO 2017-06-30 04:38:42,069 main.py:52] epoch 4081, training loss: 5755.34, average training loss: 5945.13, base loss: 15980.74
[INFO 2017-06-30 04:38:45,270 main.py:52] epoch 4082, training loss: 5770.93, average training loss: 5944.86, base loss: 15980.20
[INFO 2017-06-30 04:38:48,419 main.py:52] epoch 4083, training loss: 5489.83, average training loss: 5944.43, base loss: 15979.76
[INFO 2017-06-30 04:38:51,589 main.py:52] epoch 4084, training loss: 5679.77, average training loss: 5943.90, base loss: 15979.53
[INFO 2017-06-30 04:38:54,766 main.py:52] epoch 4085, training loss: 5789.35, average training loss: 5943.32, base loss: 15978.66
[INFO 2017-06-30 04:38:57,922 main.py:52] epoch 4086, training loss: 5578.58, average training loss: 5942.94, base loss: 15978.22
[INFO 2017-06-30 04:39:01,083 main.py:52] epoch 4087, training loss: 5675.88, average training loss: 5942.33, base loss: 15978.11
[INFO 2017-06-30 04:39:04,251 main.py:52] epoch 4088, training loss: 5861.07, average training loss: 5942.43, base loss: 15977.99
[INFO 2017-06-30 04:39:07,420 main.py:52] epoch 4089, training loss: 6181.50, average training loss: 5942.30, base loss: 15978.48
[INFO 2017-06-30 04:39:10,570 main.py:52] epoch 4090, training loss: 5746.32, average training loss: 5941.92, base loss: 15977.79
[INFO 2017-06-30 04:39:13,723 main.py:52] epoch 4091, training loss: 5619.17, average training loss: 5941.45, base loss: 15977.45
[INFO 2017-06-30 04:39:16,890 main.py:52] epoch 4092, training loss: 5922.19, average training loss: 5940.95, base loss: 15977.45
[INFO 2017-06-30 04:39:20,067 main.py:52] epoch 4093, training loss: 5680.89, average training loss: 5940.99, base loss: 15978.04
[INFO 2017-06-30 04:39:23,250 main.py:52] epoch 4094, training loss: 5249.08, average training loss: 5940.08, base loss: 15977.59
[INFO 2017-06-30 04:39:26,406 main.py:52] epoch 4095, training loss: 5911.49, average training loss: 5940.02, base loss: 15978.11
[INFO 2017-06-30 04:39:29,607 main.py:52] epoch 4096, training loss: 5731.81, average training loss: 5939.77, base loss: 15977.47
[INFO 2017-06-30 04:39:32,742 main.py:52] epoch 4097, training loss: 5802.06, average training loss: 5939.70, base loss: 15977.08
[INFO 2017-06-30 04:39:35,883 main.py:52] epoch 4098, training loss: 6196.59, average training loss: 5939.97, base loss: 15978.00
[INFO 2017-06-30 04:39:39,058 main.py:52] epoch 4099, training loss: 5748.78, average training loss: 5939.64, base loss: 15978.38
[INFO 2017-06-30 04:39:39,059 main.py:54] epoch 4099, testing
[INFO 2017-06-30 04:39:52,287 main.py:97] average testing loss: 5863.89, base loss: 15716.43
[INFO 2017-06-30 04:39:52,287 main.py:98] improve_loss: 9852.54, improve_percent: 0.63
[INFO 2017-06-30 04:39:52,290 main.py:66] current best improved percent: 0.63
[INFO 2017-06-30 04:39:55,587 main.py:52] epoch 4100, training loss: 5751.83, average training loss: 5939.23, base loss: 15977.74
[INFO 2017-06-30 04:39:58,777 main.py:52] epoch 4101, training loss: 5652.64, average training loss: 5938.66, base loss: 15977.06
[INFO 2017-06-30 04:40:01,966 main.py:52] epoch 4102, training loss: 5782.56, average training loss: 5938.79, base loss: 15977.30
[INFO 2017-06-30 04:40:05,121 main.py:52] epoch 4103, training loss: 6240.23, average training loss: 5939.36, base loss: 15977.66
[INFO 2017-06-30 04:40:08,265 main.py:52] epoch 4104, training loss: 5813.31, average training loss: 5938.60, base loss: 15977.14
[INFO 2017-06-30 04:40:11,416 main.py:52] epoch 4105, training loss: 6195.14, average training loss: 5939.18, base loss: 15977.28
[INFO 2017-06-30 04:40:14,567 main.py:52] epoch 4106, training loss: 5891.31, average training loss: 5939.02, base loss: 15977.19
[INFO 2017-06-30 04:40:17,715 main.py:52] epoch 4107, training loss: 5715.56, average training loss: 5938.48, base loss: 15977.08
[INFO 2017-06-30 04:40:20,880 main.py:52] epoch 4108, training loss: 5907.55, average training loss: 5938.06, base loss: 15977.58
[INFO 2017-06-30 04:40:24,044 main.py:52] epoch 4109, training loss: 5775.27, average training loss: 5937.71, base loss: 15978.34
[INFO 2017-06-30 04:40:27,228 main.py:52] epoch 4110, training loss: 5619.23, average training loss: 5937.21, base loss: 15977.63
[INFO 2017-06-30 04:40:30,378 main.py:52] epoch 4111, training loss: 6085.88, average training loss: 5937.17, base loss: 15978.21
[INFO 2017-06-30 04:40:33,616 main.py:52] epoch 4112, training loss: 5079.08, average training loss: 5935.87, base loss: 15976.62
[INFO 2017-06-30 04:40:36,791 main.py:52] epoch 4113, training loss: 5224.42, average training loss: 5935.40, base loss: 15975.34
[INFO 2017-06-30 04:40:39,955 main.py:52] epoch 4114, training loss: 5559.01, average training loss: 5934.58, base loss: 15974.28
[INFO 2017-06-30 04:40:43,127 main.py:52] epoch 4115, training loss: 6048.15, average training loss: 5934.60, base loss: 15974.08
[INFO 2017-06-30 04:40:46,237 main.py:52] epoch 4116, training loss: 5521.72, average training loss: 5933.89, base loss: 15973.18
[INFO 2017-06-30 04:40:49,441 main.py:52] epoch 4117, training loss: 5876.91, average training loss: 5933.33, base loss: 15972.86
[INFO 2017-06-30 04:40:52,629 main.py:52] epoch 4118, training loss: 6305.91, average training loss: 5933.75, base loss: 15973.28
[INFO 2017-06-30 04:40:55,797 main.py:52] epoch 4119, training loss: 5930.27, average training loss: 5933.80, base loss: 15972.83
[INFO 2017-06-30 04:40:58,957 main.py:52] epoch 4120, training loss: 5971.97, average training loss: 5934.23, base loss: 15972.87
[INFO 2017-06-30 04:41:02,091 main.py:52] epoch 4121, training loss: 5774.84, average training loss: 5933.94, base loss: 15972.66
[INFO 2017-06-30 04:41:05,236 main.py:52] epoch 4122, training loss: 5542.28, average training loss: 5933.31, base loss: 15971.80
[INFO 2017-06-30 04:41:08,379 main.py:52] epoch 4123, training loss: 6462.61, average training loss: 5933.48, base loss: 15972.22
[INFO 2017-06-30 04:41:11,586 main.py:52] epoch 4124, training loss: 5603.90, average training loss: 5933.46, base loss: 15971.86
[INFO 2017-06-30 04:41:14,779 main.py:52] epoch 4125, training loss: 5938.96, average training loss: 5933.31, base loss: 15971.96
[INFO 2017-06-30 04:41:17,990 main.py:52] epoch 4126, training loss: 6280.67, average training loss: 5933.68, base loss: 15972.48
[INFO 2017-06-30 04:41:21,195 main.py:52] epoch 4127, training loss: 5299.22, average training loss: 5932.83, base loss: 15971.81
[INFO 2017-06-30 04:41:24,365 main.py:52] epoch 4128, training loss: 5685.88, average training loss: 5932.67, base loss: 15971.19
[INFO 2017-06-30 04:41:27,528 main.py:52] epoch 4129, training loss: 5672.10, average training loss: 5932.34, base loss: 15971.15
[INFO 2017-06-30 04:41:30,732 main.py:52] epoch 4130, training loss: 6035.97, average training loss: 5931.89, base loss: 15971.25
[INFO 2017-06-30 04:41:33,872 main.py:52] epoch 4131, training loss: 5884.23, average training loss: 5931.51, base loss: 15971.42
[INFO 2017-06-30 04:41:37,006 main.py:52] epoch 4132, training loss: 5957.46, average training loss: 5931.46, base loss: 15971.83
[INFO 2017-06-30 04:41:40,167 main.py:52] epoch 4133, training loss: 6200.31, average training loss: 5931.83, base loss: 15972.54
[INFO 2017-06-30 04:41:43,329 main.py:52] epoch 4134, training loss: 6043.99, average training loss: 5931.76, base loss: 15972.77
[INFO 2017-06-30 04:41:46,569 main.py:52] epoch 4135, training loss: 5734.51, average training loss: 5931.54, base loss: 15972.61
[INFO 2017-06-30 04:41:49,742 main.py:52] epoch 4136, training loss: 6137.56, average training loss: 5931.27, base loss: 15972.85
[INFO 2017-06-30 04:41:52,922 main.py:52] epoch 4137, training loss: 5904.88, average training loss: 5931.18, base loss: 15972.92
[INFO 2017-06-30 04:41:56,121 main.py:52] epoch 4138, training loss: 5815.79, average training loss: 5930.78, base loss: 15972.72
[INFO 2017-06-30 04:41:59,308 main.py:52] epoch 4139, training loss: 5999.41, average training loss: 5930.71, base loss: 15972.38
[INFO 2017-06-30 04:42:02,485 main.py:52] epoch 4140, training loss: 5830.33, average training loss: 5930.70, base loss: 15972.11
[INFO 2017-06-30 04:42:05,639 main.py:52] epoch 4141, training loss: 6026.23, average training loss: 5930.18, base loss: 15972.57
[INFO 2017-06-30 04:42:08,844 main.py:52] epoch 4142, training loss: 6147.66, average training loss: 5930.58, base loss: 15972.11
[INFO 2017-06-30 04:42:12,045 main.py:52] epoch 4143, training loss: 5743.34, average training loss: 5930.65, base loss: 15971.70
[INFO 2017-06-30 04:42:15,218 main.py:52] epoch 4144, training loss: 5656.54, average training loss: 5929.82, base loss: 15971.44
[INFO 2017-06-30 04:42:18,416 main.py:52] epoch 4145, training loss: 5738.08, average training loss: 5929.32, base loss: 15971.49
[INFO 2017-06-30 04:42:21,533 main.py:52] epoch 4146, training loss: 5533.81, average training loss: 5929.15, base loss: 15971.85
[INFO 2017-06-30 04:42:24,713 main.py:52] epoch 4147, training loss: 5426.65, average training loss: 5928.48, base loss: 15971.25
[INFO 2017-06-30 04:42:27,863 main.py:52] epoch 4148, training loss: 5647.95, average training loss: 5927.90, base loss: 15970.61
[INFO 2017-06-30 04:42:31,043 main.py:52] epoch 4149, training loss: 5829.42, average training loss: 5927.66, base loss: 15970.61
[INFO 2017-06-30 04:42:34,214 main.py:52] epoch 4150, training loss: 5535.82, average training loss: 5927.57, base loss: 15970.58
[INFO 2017-06-30 04:42:37,412 main.py:52] epoch 4151, training loss: 6057.26, average training loss: 5927.50, base loss: 15971.27
[INFO 2017-06-30 04:42:40,583 main.py:52] epoch 4152, training loss: 5624.40, average training loss: 5927.14, base loss: 15970.99
[INFO 2017-06-30 04:42:43,745 main.py:52] epoch 4153, training loss: 5517.34, average training loss: 5926.53, base loss: 15969.66
[INFO 2017-06-30 04:42:46,948 main.py:52] epoch 4154, training loss: 5819.56, average training loss: 5926.45, base loss: 15969.05
[INFO 2017-06-30 04:42:50,109 main.py:52] epoch 4155, training loss: 5664.76, average training loss: 5926.21, base loss: 15968.39
[INFO 2017-06-30 04:42:53,295 main.py:52] epoch 4156, training loss: 5467.13, average training loss: 5925.88, base loss: 15968.23
[INFO 2017-06-30 04:42:56,431 main.py:52] epoch 4157, training loss: 6074.15, average training loss: 5926.07, base loss: 15967.63
[INFO 2017-06-30 04:42:59,615 main.py:52] epoch 4158, training loss: 5402.43, average training loss: 5925.11, base loss: 15967.70
[INFO 2017-06-30 04:43:02,772 main.py:52] epoch 4159, training loss: 5592.99, average training loss: 5924.80, base loss: 15967.56
[INFO 2017-06-30 04:43:05,931 main.py:52] epoch 4160, training loss: 5477.19, average training loss: 5924.23, base loss: 15966.16
[INFO 2017-06-30 04:43:09,186 main.py:52] epoch 4161, training loss: 6014.15, average training loss: 5924.03, base loss: 15965.83
[INFO 2017-06-30 04:43:12,375 main.py:52] epoch 4162, training loss: 5449.25, average training loss: 5923.45, base loss: 15965.86
[INFO 2017-06-30 04:43:15,579 main.py:52] epoch 4163, training loss: 5828.55, average training loss: 5923.21, base loss: 15966.28
[INFO 2017-06-30 04:43:18,733 main.py:52] epoch 4164, training loss: 5971.45, average training loss: 5923.46, base loss: 15966.49
[INFO 2017-06-30 04:43:21,913 main.py:52] epoch 4165, training loss: 5730.42, average training loss: 5923.36, base loss: 15966.03
[INFO 2017-06-30 04:43:25,049 main.py:52] epoch 4166, training loss: 5679.78, average training loss: 5922.85, base loss: 15966.15
[INFO 2017-06-30 04:43:28,240 main.py:52] epoch 4167, training loss: 5487.97, average training loss: 5922.40, base loss: 15965.67
[INFO 2017-06-30 04:43:31,372 main.py:52] epoch 4168, training loss: 5708.68, average training loss: 5922.32, base loss: 15965.48
[INFO 2017-06-30 04:43:34,540 main.py:52] epoch 4169, training loss: 5615.64, average training loss: 5922.01, base loss: 15965.17
[INFO 2017-06-30 04:43:37,688 main.py:52] epoch 4170, training loss: 5848.94, average training loss: 5921.53, base loss: 15964.88
[INFO 2017-06-30 04:43:40,886 main.py:52] epoch 4171, training loss: 6014.90, average training loss: 5921.50, base loss: 15964.53
[INFO 2017-06-30 04:43:44,074 main.py:52] epoch 4172, training loss: 5930.62, average training loss: 5920.77, base loss: 15964.19
[INFO 2017-06-30 04:43:47,236 main.py:52] epoch 4173, training loss: 5988.62, average training loss: 5920.57, base loss: 15964.26
[INFO 2017-06-30 04:43:50,379 main.py:52] epoch 4174, training loss: 5861.91, average training loss: 5920.49, base loss: 15964.31
[INFO 2017-06-30 04:43:53,523 main.py:52] epoch 4175, training loss: 6117.93, average training loss: 5920.59, base loss: 15964.49
[INFO 2017-06-30 04:43:56,682 main.py:52] epoch 4176, training loss: 6071.49, average training loss: 5920.32, base loss: 15963.99
[INFO 2017-06-30 04:43:59,851 main.py:52] epoch 4177, training loss: 5819.95, average training loss: 5920.64, base loss: 15963.48
[INFO 2017-06-30 04:44:03,016 main.py:52] epoch 4178, training loss: 5560.20, average training loss: 5920.53, base loss: 15962.74
[INFO 2017-06-30 04:44:06,168 main.py:52] epoch 4179, training loss: 5692.90, average training loss: 5919.72, base loss: 15961.93
[INFO 2017-06-30 04:44:09,347 main.py:52] epoch 4180, training loss: 5469.58, average training loss: 5919.07, base loss: 15961.15
[INFO 2017-06-30 04:44:12,514 main.py:52] epoch 4181, training loss: 5747.77, average training loss: 5919.07, base loss: 15961.24
[INFO 2017-06-30 04:44:15,656 main.py:52] epoch 4182, training loss: 5594.20, average training loss: 5918.48, base loss: 15960.63
[INFO 2017-06-30 04:44:18,844 main.py:52] epoch 4183, training loss: 5717.02, average training loss: 5917.82, base loss: 15960.06
[INFO 2017-06-30 04:44:22,041 main.py:52] epoch 4184, training loss: 5704.25, average training loss: 5917.65, base loss: 15959.58
[INFO 2017-06-30 04:44:25,218 main.py:52] epoch 4185, training loss: 5813.45, average training loss: 5917.66, base loss: 15960.27
[INFO 2017-06-30 04:44:28,398 main.py:52] epoch 4186, training loss: 5857.67, average training loss: 5917.91, base loss: 15960.79
[INFO 2017-06-30 04:44:31,552 main.py:52] epoch 4187, training loss: 6170.29, average training loss: 5918.06, base loss: 15960.65
[INFO 2017-06-30 04:44:34,720 main.py:52] epoch 4188, training loss: 5842.75, average training loss: 5918.01, base loss: 15961.29
[INFO 2017-06-30 04:44:37,911 main.py:52] epoch 4189, training loss: 6178.17, average training loss: 5918.00, base loss: 15962.70
[INFO 2017-06-30 04:44:41,088 main.py:52] epoch 4190, training loss: 5748.07, average training loss: 5918.12, base loss: 15962.94
[INFO 2017-06-30 04:44:44,248 main.py:52] epoch 4191, training loss: 5877.98, average training loss: 5917.88, base loss: 15962.49
[INFO 2017-06-30 04:44:47,398 main.py:52] epoch 4192, training loss: 5675.50, average training loss: 5917.75, base loss: 15961.65
[INFO 2017-06-30 04:44:50,603 main.py:52] epoch 4193, training loss: 5597.11, average training loss: 5917.24, base loss: 15961.14
[INFO 2017-06-30 04:44:53,752 main.py:52] epoch 4194, training loss: 6064.78, average training loss: 5917.53, base loss: 15961.34
[INFO 2017-06-30 04:44:56,917 main.py:52] epoch 4195, training loss: 6139.60, average training loss: 5917.99, base loss: 15961.62
[INFO 2017-06-30 04:45:00,093 main.py:52] epoch 4196, training loss: 5962.22, average training loss: 5917.95, base loss: 15961.91
[INFO 2017-06-30 04:45:03,290 main.py:52] epoch 4197, training loss: 5626.06, average training loss: 5917.20, base loss: 15961.57
[INFO 2017-06-30 04:45:06,423 main.py:52] epoch 4198, training loss: 6078.94, average training loss: 5917.25, base loss: 15962.12
[INFO 2017-06-30 04:45:09,591 main.py:52] epoch 4199, training loss: 5980.42, average training loss: 5917.07, base loss: 15962.22
[INFO 2017-06-30 04:45:09,592 main.py:54] epoch 4199, testing
[INFO 2017-06-30 04:45:23,239 main.py:97] average testing loss: 5894.44, base loss: 16213.83
[INFO 2017-06-30 04:45:23,239 main.py:98] improve_loss: 10319.39, improve_percent: 0.64
[INFO 2017-06-30 04:45:23,242 main.py:62] model save to ./model/final.pth
[INFO 2017-06-30 04:45:23,276 main.py:66] current best improved percent: 0.64
[INFO 2017-06-30 04:45:26,590 main.py:52] epoch 4200, training loss: 5728.55, average training loss: 5916.53, base loss: 15962.35
[INFO 2017-06-30 04:45:29,799 main.py:52] epoch 4201, training loss: 6242.24, average training loss: 5916.37, base loss: 15962.88
[INFO 2017-06-30 04:45:33,030 main.py:52] epoch 4202, training loss: 5735.40, average training loss: 5916.27, base loss: 15962.78
[INFO 2017-06-30 04:45:36,222 main.py:52] epoch 4203, training loss: 5672.46, average training loss: 5915.73, base loss: 15962.47
[INFO 2017-06-30 04:45:39,446 main.py:52] epoch 4204, training loss: 5965.70, average training loss: 5916.12, base loss: 15962.70
[INFO 2017-06-30 04:45:42,606 main.py:52] epoch 4205, training loss: 5604.62, average training loss: 5915.41, base loss: 15962.48
[INFO 2017-06-30 04:45:45,832 main.py:52] epoch 4206, training loss: 5654.19, average training loss: 5915.10, base loss: 15962.59
[INFO 2017-06-30 04:45:49,002 main.py:52] epoch 4207, training loss: 5923.25, average training loss: 5915.27, base loss: 15963.16
[INFO 2017-06-30 04:45:52,210 main.py:52] epoch 4208, training loss: 5446.24, average training loss: 5914.95, base loss: 15962.61
[INFO 2017-06-30 04:45:55,369 main.py:52] epoch 4209, training loss: 5960.89, average training loss: 5914.46, base loss: 15963.17
[INFO 2017-06-30 04:45:58,588 main.py:52] epoch 4210, training loss: 5528.86, average training loss: 5913.70, base loss: 15962.23
[INFO 2017-06-30 04:46:01,759 main.py:52] epoch 4211, training loss: 6430.77, average training loss: 5913.97, base loss: 15962.91
[INFO 2017-06-30 04:46:04,893 main.py:52] epoch 4212, training loss: 5775.39, average training loss: 5913.59, base loss: 15962.80
[INFO 2017-06-30 04:46:08,053 main.py:52] epoch 4213, training loss: 5558.50, average training loss: 5913.22, base loss: 15962.10
[INFO 2017-06-30 04:46:11,247 main.py:52] epoch 4214, training loss: 6048.43, average training loss: 5913.31, base loss: 15963.23
[INFO 2017-06-30 04:46:14,426 main.py:52] epoch 4215, training loss: 5952.68, average training loss: 5912.38, base loss: 15964.03
[INFO 2017-06-30 04:46:17,625 main.py:52] epoch 4216, training loss: 5732.91, average training loss: 5912.17, base loss: 15963.51
[INFO 2017-06-30 04:46:20,768 main.py:52] epoch 4217, training loss: 5582.99, average training loss: 5911.60, base loss: 15963.53
[INFO 2017-06-30 04:46:23,923 main.py:52] epoch 4218, training loss: 5785.85, average training loss: 5911.61, base loss: 15963.55
[INFO 2017-06-30 04:46:27,090 main.py:52] epoch 4219, training loss: 5837.65, average training loss: 5910.82, base loss: 15964.10
[INFO 2017-06-30 04:46:30,249 main.py:52] epoch 4220, training loss: 5525.67, average training loss: 5910.27, base loss: 15963.17
[INFO 2017-06-30 04:46:33,391 main.py:52] epoch 4221, training loss: 5705.67, average training loss: 5909.92, base loss: 15962.89
[INFO 2017-06-30 04:46:36,535 main.py:52] epoch 4222, training loss: 5768.97, average training loss: 5909.45, base loss: 15962.34
[INFO 2017-06-30 04:46:39,718 main.py:52] epoch 4223, training loss: 5784.10, average training loss: 5909.40, base loss: 15961.89
[INFO 2017-06-30 04:46:42,881 main.py:52] epoch 4224, training loss: 5686.52, average training loss: 5908.95, base loss: 15961.06
[INFO 2017-06-30 04:46:46,048 main.py:52] epoch 4225, training loss: 5264.25, average training loss: 5908.40, base loss: 15959.62
[INFO 2017-06-30 04:46:49,245 main.py:52] epoch 4226, training loss: 5764.80, average training loss: 5908.01, base loss: 15959.48
[INFO 2017-06-30 04:46:52,418 main.py:52] epoch 4227, training loss: 5719.59, average training loss: 5907.70, base loss: 15959.73
[INFO 2017-06-30 04:46:55,613 main.py:52] epoch 4228, training loss: 6049.81, average training loss: 5907.74, base loss: 15960.07
[INFO 2017-06-30 04:46:58,769 main.py:52] epoch 4229, training loss: 5650.95, average training loss: 5907.25, base loss: 15960.13
[INFO 2017-06-30 04:47:01,951 main.py:52] epoch 4230, training loss: 5755.24, average training loss: 5906.96, base loss: 15959.54
[INFO 2017-06-30 04:47:05,087 main.py:52] epoch 4231, training loss: 5855.45, average training loss: 5907.20, base loss: 15958.45
[INFO 2017-06-30 04:47:08,248 main.py:52] epoch 4232, training loss: 5717.72, average training loss: 5906.61, base loss: 15958.04
[INFO 2017-06-30 04:47:11,499 main.py:52] epoch 4233, training loss: 5649.21, average training loss: 5905.80, base loss: 15958.12
[INFO 2017-06-30 04:47:14,708 main.py:52] epoch 4234, training loss: 5704.87, average training loss: 5905.26, base loss: 15957.54
[INFO 2017-06-30 04:47:17,905 main.py:52] epoch 4235, training loss: 5895.54, average training loss: 5905.90, base loss: 15957.72
[INFO 2017-06-30 04:47:21,057 main.py:52] epoch 4236, training loss: 6007.10, average training loss: 5906.06, base loss: 15958.02
[INFO 2017-06-30 04:47:24,231 main.py:52] epoch 4237, training loss: 6083.11, average training loss: 5906.28, base loss: 15958.36
[INFO 2017-06-30 04:47:27,399 main.py:52] epoch 4238, training loss: 5871.89, average training loss: 5906.24, base loss: 15958.54
[INFO 2017-06-30 04:47:30,551 main.py:52] epoch 4239, training loss: 6290.52, average training loss: 5906.38, base loss: 15958.91
[INFO 2017-06-30 04:47:33,737 main.py:52] epoch 4240, training loss: 5690.68, average training loss: 5905.74, base loss: 15958.59
[INFO 2017-06-30 04:47:36,932 main.py:52] epoch 4241, training loss: 5735.74, average training loss: 5905.72, base loss: 15958.84
[INFO 2017-06-30 04:47:40,116 main.py:52] epoch 4242, training loss: 6373.28, average training loss: 5905.66, base loss: 15959.68
[INFO 2017-06-30 04:47:43,286 main.py:52] epoch 4243, training loss: 6231.52, average training loss: 5905.79, base loss: 15960.47
[INFO 2017-06-30 04:47:46,466 main.py:52] epoch 4244, training loss: 5714.07, average training loss: 5905.37, base loss: 15960.46
[INFO 2017-06-30 04:47:49,642 main.py:52] epoch 4245, training loss: 5567.18, average training loss: 5905.09, base loss: 15960.63
[INFO 2017-06-30 04:47:52,865 main.py:52] epoch 4246, training loss: 5925.41, average training loss: 5904.91, base loss: 15960.84
[INFO 2017-06-30 04:47:56,028 main.py:52] epoch 4247, training loss: 5082.19, average training loss: 5904.27, base loss: 15959.71
[INFO 2017-06-30 04:47:59,188 main.py:52] epoch 4248, training loss: 6182.71, average training loss: 5904.57, base loss: 15960.49
[INFO 2017-06-30 04:48:02,395 main.py:52] epoch 4249, training loss: 5828.53, average training loss: 5904.43, base loss: 15960.53
[INFO 2017-06-30 04:48:05,560 main.py:52] epoch 4250, training loss: 5528.79, average training loss: 5904.36, base loss: 15960.65
[INFO 2017-06-30 04:48:08,720 main.py:52] epoch 4251, training loss: 5699.18, average training loss: 5904.07, base loss: 15961.23
[INFO 2017-06-30 04:48:11,902 main.py:52] epoch 4252, training loss: 5781.66, average training loss: 5904.51, base loss: 15961.23
[INFO 2017-06-30 04:48:15,076 main.py:52] epoch 4253, training loss: 6250.21, average training loss: 5904.41, base loss: 15961.87
[INFO 2017-06-30 04:48:18,238 main.py:52] epoch 4254, training loss: 6144.28, average training loss: 5904.90, base loss: 15961.15
[INFO 2017-06-30 04:48:21,383 main.py:52] epoch 4255, training loss: 6131.24, average training loss: 5905.04, base loss: 15961.06
[INFO 2017-06-30 04:48:24,537 main.py:52] epoch 4256, training loss: 5478.18, average training loss: 5904.31, base loss: 15960.73
[INFO 2017-06-30 04:48:27,713 main.py:52] epoch 4257, training loss: 5551.79, average training loss: 5904.00, base loss: 15960.83
[INFO 2017-06-30 04:48:30,854 main.py:52] epoch 4258, training loss: 6021.87, average training loss: 5903.93, base loss: 15961.01
[INFO 2017-06-30 04:48:34,057 main.py:52] epoch 4259, training loss: 6048.55, average training loss: 5904.38, base loss: 15961.86
[INFO 2017-06-30 04:48:37,289 main.py:52] epoch 4260, training loss: 5636.06, average training loss: 5904.28, base loss: 15961.83
[INFO 2017-06-30 04:48:40,428 main.py:52] epoch 4261, training loss: 6373.95, average training loss: 5904.95, base loss: 15962.94
[INFO 2017-06-30 04:48:43,555 main.py:52] epoch 4262, training loss: 6253.75, average training loss: 5905.33, base loss: 15963.50
[INFO 2017-06-30 04:48:46,725 main.py:52] epoch 4263, training loss: 6153.32, average training loss: 5905.10, base loss: 15963.74
[INFO 2017-06-30 04:48:49,860 main.py:52] epoch 4264, training loss: 5640.38, average training loss: 5904.70, base loss: 15963.60
[INFO 2017-06-30 04:48:53,054 main.py:52] epoch 4265, training loss: 5562.87, average training loss: 5904.37, base loss: 15962.83
[INFO 2017-06-30 04:48:56,254 main.py:52] epoch 4266, training loss: 6060.72, average training loss: 5904.53, base loss: 15963.09
[INFO 2017-06-30 04:48:59,404 main.py:52] epoch 4267, training loss: 5606.61, average training loss: 5904.13, base loss: 15962.89
[INFO 2017-06-30 04:49:02,530 main.py:52] epoch 4268, training loss: 6139.39, average training loss: 5903.95, base loss: 15963.78
[INFO 2017-06-30 04:49:05,698 main.py:52] epoch 4269, training loss: 5636.59, average training loss: 5903.73, base loss: 15963.08
[INFO 2017-06-30 04:49:08,863 main.py:52] epoch 4270, training loss: 5739.14, average training loss: 5903.66, base loss: 15962.40
[INFO 2017-06-30 04:49:12,082 main.py:52] epoch 4271, training loss: 5931.48, average training loss: 5903.47, base loss: 15962.42
[INFO 2017-06-30 04:49:15,289 main.py:52] epoch 4272, training loss: 6284.38, average training loss: 5903.87, base loss: 15962.80
[INFO 2017-06-30 04:49:18,490 main.py:52] epoch 4273, training loss: 5881.42, average training loss: 5903.68, base loss: 15963.18
[INFO 2017-06-30 04:49:21,698 main.py:52] epoch 4274, training loss: 5794.39, average training loss: 5903.69, base loss: 15962.67
[INFO 2017-06-30 04:49:24,893 main.py:52] epoch 4275, training loss: 5626.67, average training loss: 5903.19, base loss: 15962.44
[INFO 2017-06-30 04:49:28,056 main.py:52] epoch 4276, training loss: 6247.26, average training loss: 5903.58, base loss: 15961.76
[INFO 2017-06-30 04:49:31,243 main.py:52] epoch 4277, training loss: 5634.40, average training loss: 5903.37, base loss: 15961.54
[INFO 2017-06-30 04:49:34,388 main.py:52] epoch 4278, training loss: 6090.80, average training loss: 5903.48, base loss: 15961.73
[INFO 2017-06-30 04:49:37,545 main.py:52] epoch 4279, training loss: 5849.72, average training loss: 5903.23, base loss: 15961.70
[INFO 2017-06-30 04:49:40,714 main.py:52] epoch 4280, training loss: 5802.42, average training loss: 5902.68, base loss: 15960.87
[INFO 2017-06-30 04:49:43,885 main.py:52] epoch 4281, training loss: 6003.51, average training loss: 5902.85, base loss: 15960.94
[INFO 2017-06-30 04:49:47,056 main.py:52] epoch 4282, training loss: 6101.23, average training loss: 5903.14, base loss: 15961.56
[INFO 2017-06-30 04:49:50,217 main.py:52] epoch 4283, training loss: 5805.99, average training loss: 5903.02, base loss: 15961.96
[INFO 2017-06-30 04:49:53,370 main.py:52] epoch 4284, training loss: 5882.05, average training loss: 5902.73, base loss: 15962.44
[INFO 2017-06-30 04:49:56,544 main.py:52] epoch 4285, training loss: 6316.36, average training loss: 5902.66, base loss: 15963.50
[INFO 2017-06-30 04:49:59,675 main.py:52] epoch 4286, training loss: 5723.94, average training loss: 5902.82, base loss: 15963.33
[INFO 2017-06-30 04:50:02,819 main.py:52] epoch 4287, training loss: 5647.64, average training loss: 5902.86, base loss: 15962.70
[INFO 2017-06-30 04:50:05,978 main.py:52] epoch 4288, training loss: 5692.51, average training loss: 5902.07, base loss: 15962.49
[INFO 2017-06-30 04:50:09,195 main.py:52] epoch 4289, training loss: 5788.92, average training loss: 5902.27, base loss: 15962.56
[INFO 2017-06-30 04:50:12,420 main.py:52] epoch 4290, training loss: 6029.11, average training loss: 5902.39, base loss: 15963.38
[INFO 2017-06-30 04:50:15,591 main.py:52] epoch 4291, training loss: 5443.49, average training loss: 5901.61, base loss: 15963.27
[INFO 2017-06-30 04:50:18,729 main.py:52] epoch 4292, training loss: 6037.83, average training loss: 5901.47, base loss: 15964.25
[INFO 2017-06-30 04:50:21,957 main.py:52] epoch 4293, training loss: 5689.65, average training loss: 5901.20, base loss: 15965.32
[INFO 2017-06-30 04:50:25,153 main.py:52] epoch 4294, training loss: 5983.35, average training loss: 5901.25, base loss: 15965.66
[INFO 2017-06-30 04:50:28,329 main.py:52] epoch 4295, training loss: 5733.11, average training loss: 5901.24, base loss: 15965.82
[INFO 2017-06-30 04:50:31,529 main.py:52] epoch 4296, training loss: 6224.96, average training loss: 5901.26, base loss: 15966.80
[INFO 2017-06-30 04:50:34,701 main.py:52] epoch 4297, training loss: 6121.60, average training loss: 5901.32, base loss: 15967.40
[INFO 2017-06-30 04:50:37,863 main.py:52] epoch 4298, training loss: 5480.32, average training loss: 5900.92, base loss: 15967.09
[INFO 2017-06-30 04:50:41,033 main.py:52] epoch 4299, training loss: 5553.81, average training loss: 5900.23, base loss: 15966.50
[INFO 2017-06-30 04:50:41,033 main.py:54] epoch 4299, testing
[INFO 2017-06-30 04:50:54,369 main.py:97] average testing loss: 5732.28, base loss: 15830.54
[INFO 2017-06-30 04:50:54,369 main.py:98] improve_loss: 10098.26, improve_percent: 0.64
[INFO 2017-06-30 04:50:54,371 main.py:62] model save to ./model/final.pth
[INFO 2017-06-30 04:50:54,405 main.py:66] current best improved percent: 0.64
[INFO 2017-06-30 04:50:57,567 main.py:52] epoch 4300, training loss: 5774.88, average training loss: 5899.96, base loss: 15966.58
[INFO 2017-06-30 04:51:00,734 main.py:52] epoch 4301, training loss: 5860.81, average training loss: 5900.03, base loss: 15966.86
[INFO 2017-06-30 04:51:03,941 main.py:52] epoch 4302, training loss: 5336.12, average training loss: 5899.51, base loss: 15966.11
[INFO 2017-06-30 04:51:07,107 main.py:52] epoch 4303, training loss: 5980.88, average training loss: 5899.67, base loss: 15965.74
[INFO 2017-06-30 04:51:10,257 main.py:52] epoch 4304, training loss: 5575.76, average training loss: 5899.55, base loss: 15965.19
[INFO 2017-06-30 04:51:13,418 main.py:52] epoch 4305, training loss: 6056.29, average training loss: 5899.49, base loss: 15965.47
[INFO 2017-06-30 04:51:16,553 main.py:52] epoch 4306, training loss: 5803.80, average training loss: 5899.14, base loss: 15964.69
[INFO 2017-06-30 04:51:19,737 main.py:52] epoch 4307, training loss: 5999.88, average training loss: 5899.04, base loss: 15964.80
[INFO 2017-06-30 04:51:22,916 main.py:52] epoch 4308, training loss: 6510.79, average training loss: 5899.53, base loss: 15966.27
[INFO 2017-06-30 04:51:26,094 main.py:52] epoch 4309, training loss: 6145.47, average training loss: 5899.56, base loss: 15966.69
[INFO 2017-06-30 04:51:29,209 main.py:52] epoch 4310, training loss: 5762.21, average training loss: 5898.83, base loss: 15966.41
[INFO 2017-06-30 04:51:32,418 main.py:52] epoch 4311, training loss: 6160.39, average training loss: 5898.64, base loss: 15966.81
[INFO 2017-06-30 04:51:35,602 main.py:52] epoch 4312, training loss: 5921.79, average training loss: 5898.03, base loss: 15966.61
[INFO 2017-06-30 04:51:38,769 main.py:52] epoch 4313, training loss: 5941.27, average training loss: 5898.12, base loss: 15966.64
[INFO 2017-06-30 04:51:41,940 main.py:52] epoch 4314, training loss: 5863.59, average training loss: 5898.11, base loss: 15966.63
[INFO 2017-06-30 04:51:45,124 main.py:52] epoch 4315, training loss: 5668.07, average training loss: 5898.27, base loss: 15966.45
[INFO 2017-06-30 04:51:48,325 main.py:52] epoch 4316, training loss: 5369.27, average training loss: 5897.65, base loss: 15965.87
[INFO 2017-06-30 04:51:51,481 main.py:52] epoch 4317, training loss: 5831.25, average training loss: 5897.58, base loss: 15965.94
[INFO 2017-06-30 04:51:54,648 main.py:52] epoch 4318, training loss: 5570.59, average training loss: 5897.29, base loss: 15965.74
[INFO 2017-06-30 04:51:57,777 main.py:52] epoch 4319, training loss: 5409.71, average training loss: 5896.17, base loss: 15964.75
[INFO 2017-06-30 04:52:00,923 main.py:52] epoch 4320, training loss: 5839.92, average training loss: 5896.11, base loss: 15964.64
[INFO 2017-06-30 04:52:04,053 main.py:52] epoch 4321, training loss: 5490.72, average training loss: 5895.90, base loss: 15964.49
[INFO 2017-06-30 04:52:07,178 main.py:52] epoch 4322, training loss: 5402.12, average training loss: 5895.49, base loss: 15964.21
[INFO 2017-06-30 04:52:10,326 main.py:52] epoch 4323, training loss: 5945.69, average training loss: 5896.14, base loss: 15964.19
[INFO 2017-06-30 04:52:13,497 main.py:52] epoch 4324, training loss: 6113.22, average training loss: 5896.49, base loss: 15965.75
[INFO 2017-06-30 04:52:16,637 main.py:52] epoch 4325, training loss: 5862.11, average training loss: 5896.20, base loss: 15966.37
[INFO 2017-06-30 04:52:19,801 main.py:52] epoch 4326, training loss: 6183.64, average training loss: 5896.35, base loss: 15967.37
[INFO 2017-06-30 04:52:22,941 main.py:52] epoch 4327, training loss: 5853.61, average training loss: 5896.22, base loss: 15967.28
[INFO 2017-06-30 04:52:26,099 main.py:52] epoch 4328, training loss: 5476.86, average training loss: 5895.86, base loss: 15967.19
[INFO 2017-06-30 04:52:29,262 main.py:52] epoch 4329, training loss: 5749.93, average training loss: 5895.20, base loss: 15967.78
[INFO 2017-06-30 04:52:32,404 main.py:52] epoch 4330, training loss: 5408.14, average training loss: 5894.97, base loss: 15967.61
[INFO 2017-06-30 04:52:35,546 main.py:52] epoch 4331, training loss: 5888.97, average training loss: 5894.59, base loss: 15967.69
[INFO 2017-06-30 04:52:38,701 main.py:52] epoch 4332, training loss: 5913.89, average training loss: 5893.98, base loss: 15968.42
[INFO 2017-06-30 04:52:41,865 main.py:52] epoch 4333, training loss: 5850.92, average training loss: 5893.70, base loss: 15968.46
[INFO 2017-06-30 04:52:45,068 main.py:52] epoch 4334, training loss: 5948.06, average training loss: 5893.78, base loss: 15968.54
[INFO 2017-06-30 04:52:48,239 main.py:52] epoch 4335, training loss: 5863.99, average training loss: 5893.63, base loss: 15968.56
[INFO 2017-06-30 04:52:51,395 main.py:52] epoch 4336, training loss: 5851.50, average training loss: 5893.56, base loss: 15968.54
[INFO 2017-06-30 04:52:54,561 main.py:52] epoch 4337, training loss: 5864.70, average training loss: 5893.38, base loss: 15967.86
[INFO 2017-06-30 04:52:57,716 main.py:52] epoch 4338, training loss: 5771.02, average training loss: 5892.96, base loss: 15967.91
[INFO 2017-06-30 04:53:00,935 main.py:52] epoch 4339, training loss: 5716.18, average training loss: 5892.65, base loss: 15968.39
[INFO 2017-06-30 04:53:04,104 main.py:52] epoch 4340, training loss: 5795.18, average training loss: 5892.50, base loss: 15968.73
[INFO 2017-06-30 04:53:07,315 main.py:52] epoch 4341, training loss: 5586.29, average training loss: 5892.09, base loss: 15968.37
[INFO 2017-06-30 04:53:10,465 main.py:52] epoch 4342, training loss: 6151.51, average training loss: 5891.88, base loss: 15968.46
[INFO 2017-06-30 04:53:13,636 main.py:52] epoch 4343, training loss: 5639.57, average training loss: 5892.04, base loss: 15969.02
[INFO 2017-06-30 04:53:16,801 main.py:52] epoch 4344, training loss: 5435.72, average training loss: 5891.10, base loss: 15967.98
[INFO 2017-06-30 04:53:20,004 main.py:52] epoch 4345, training loss: 5981.79, average training loss: 5890.94, base loss: 15968.14
[INFO 2017-06-30 04:53:23,156 main.py:52] epoch 4346, training loss: 5563.17, average training loss: 5890.29, base loss: 15968.27
[INFO 2017-06-30 04:53:26,339 main.py:52] epoch 4347, training loss: 5899.97, average training loss: 5890.46, base loss: 15968.04
[INFO 2017-06-30 04:53:29,503 main.py:52] epoch 4348, training loss: 5701.44, average training loss: 5889.83, base loss: 15967.97
[INFO 2017-06-30 04:53:32,669 main.py:52] epoch 4349, training loss: 6203.89, average training loss: 5890.05, base loss: 15968.03
[INFO 2017-06-30 04:53:35,820 main.py:52] epoch 4350, training loss: 6021.25, average training loss: 5890.07, base loss: 15968.74
[INFO 2017-06-30 04:53:38,962 main.py:52] epoch 4351, training loss: 5841.51, average training loss: 5890.21, base loss: 15969.21
[INFO 2017-06-30 04:53:42,148 main.py:52] epoch 4352, training loss: 5688.73, average training loss: 5890.11, base loss: 15969.19
[INFO 2017-06-30 04:53:45,349 main.py:52] epoch 4353, training loss: 6043.61, average training loss: 5890.07, base loss: 15969.74
[INFO 2017-06-30 04:53:48,480 main.py:52] epoch 4354, training loss: 5878.23, average training loss: 5890.29, base loss: 15970.56
[INFO 2017-06-30 04:53:51,670 main.py:52] epoch 4355, training loss: 5770.93, average training loss: 5890.09, base loss: 15970.61
[INFO 2017-06-30 04:53:54,892 main.py:52] epoch 4356, training loss: 5823.70, average training loss: 5890.11, base loss: 15970.72
[INFO 2017-06-30 04:53:58,134 main.py:52] epoch 4357, training loss: 5943.14, average training loss: 5890.13, base loss: 15971.16
[INFO 2017-06-30 04:54:01,272 main.py:52] epoch 4358, training loss: 5522.03, average training loss: 5889.72, base loss: 15970.84
[INFO 2017-06-30 04:54:04,503 main.py:52] epoch 4359, training loss: 5903.60, average training loss: 5889.85, base loss: 15970.64
[INFO 2017-06-30 04:54:07,660 main.py:52] epoch 4360, training loss: 5955.84, average training loss: 5889.96, base loss: 15970.48
[INFO 2017-06-30 04:54:10,841 main.py:52] epoch 4361, training loss: 6057.23, average training loss: 5890.14, base loss: 15970.40
[INFO 2017-06-30 04:54:14,045 main.py:52] epoch 4362, training loss: 5796.83, average training loss: 5889.84, base loss: 15971.53
[INFO 2017-06-30 04:54:17,184 main.py:52] epoch 4363, training loss: 5699.60, average training loss: 5889.67, base loss: 15972.04
[INFO 2017-06-30 04:54:20,319 main.py:52] epoch 4364, training loss: 5563.40, average training loss: 5889.19, base loss: 15971.93
[INFO 2017-06-30 04:54:23,543 main.py:52] epoch 4365, training loss: 5926.09, average training loss: 5888.86, base loss: 15972.37
[INFO 2017-06-30 04:54:26,697 main.py:52] epoch 4366, training loss: 5782.96, average training loss: 5888.23, base loss: 15972.29
[INFO 2017-06-30 04:54:29,878 main.py:52] epoch 4367, training loss: 5727.26, average training loss: 5887.80, base loss: 15972.42
[INFO 2017-06-30 04:54:33,051 main.py:52] epoch 4368, training loss: 5720.12, average training loss: 5887.57, base loss: 15972.02
[INFO 2017-06-30 04:54:36,231 main.py:52] epoch 4369, training loss: 5498.19, average training loss: 5886.96, base loss: 15971.67
[INFO 2017-06-30 04:54:39,434 main.py:52] epoch 4370, training loss: 6122.89, average training loss: 5886.79, base loss: 15971.92
[INFO 2017-06-30 04:54:42,655 main.py:52] epoch 4371, training loss: 5182.78, average training loss: 5886.16, base loss: 15971.27
[INFO 2017-06-30 04:54:45,847 main.py:52] epoch 4372, training loss: 5783.86, average training loss: 5885.38, base loss: 15970.70
[INFO 2017-06-30 04:54:49,031 main.py:52] epoch 4373, training loss: 5534.51, average training loss: 5884.75, base loss: 15970.24
[INFO 2017-06-30 04:54:52,191 main.py:52] epoch 4374, training loss: 5915.98, average training loss: 5884.36, base loss: 15970.64
[INFO 2017-06-30 04:54:55,323 main.py:52] epoch 4375, training loss: 5568.99, average training loss: 5884.16, base loss: 15970.67
[INFO 2017-06-30 04:54:58,482 main.py:52] epoch 4376, training loss: 5276.90, average training loss: 5883.57, base loss: 15969.97
[INFO 2017-06-30 04:55:01,632 main.py:52] epoch 4377, training loss: 5748.79, average training loss: 5883.58, base loss: 15969.45
[INFO 2017-06-30 04:55:04,772 main.py:52] epoch 4378, training loss: 5510.36, average training loss: 5883.46, base loss: 15969.52
[INFO 2017-06-30 04:55:07,962 main.py:52] epoch 4379, training loss: 5662.51, average training loss: 5883.18, base loss: 15969.38
[INFO 2017-06-30 04:55:11,095 main.py:52] epoch 4380, training loss: 5542.32, average training loss: 5882.91, base loss: 15968.91
[INFO 2017-06-30 04:55:14,318 main.py:52] epoch 4381, training loss: 5970.92, average training loss: 5882.75, base loss: 15969.65
[INFO 2017-06-30 04:55:17,464 main.py:52] epoch 4382, training loss: 5836.40, average training loss: 5882.72, base loss: 15969.66
[INFO 2017-06-30 04:55:20,633 main.py:52] epoch 4383, training loss: 6066.69, average training loss: 5882.91, base loss: 15970.01
[INFO 2017-06-30 04:55:23,832 main.py:52] epoch 4384, training loss: 5478.13, average training loss: 5881.98, base loss: 15969.07
[INFO 2017-06-30 04:55:27,030 main.py:52] epoch 4385, training loss: 6126.24, average training loss: 5882.39, base loss: 15968.98
[INFO 2017-06-30 04:55:30,208 main.py:52] epoch 4386, training loss: 6016.86, average training loss: 5882.54, base loss: 15969.69
[INFO 2017-06-30 04:55:33,352 main.py:52] epoch 4387, training loss: 6045.23, average training loss: 5883.17, base loss: 15970.10
[INFO 2017-06-30 04:55:36,566 main.py:52] epoch 4388, training loss: 6365.41, average training loss: 5883.92, base loss: 15971.54
[INFO 2017-06-30 04:55:39,738 main.py:52] epoch 4389, training loss: 5652.05, average training loss: 5883.16, base loss: 15971.89
[INFO 2017-06-30 04:55:42,914 main.py:52] epoch 4390, training loss: 5936.66, average training loss: 5882.79, base loss: 15971.49
[INFO 2017-06-30 04:55:46,046 main.py:52] epoch 4391, training loss: 5637.41, average training loss: 5882.27, base loss: 15971.01
[INFO 2017-06-30 04:55:49,265 main.py:52] epoch 4392, training loss: 6178.78, average training loss: 5882.56, base loss: 15971.37
[INFO 2017-06-30 04:55:52,396 main.py:52] epoch 4393, training loss: 5781.44, average training loss: 5882.43, base loss: 15971.32
[INFO 2017-06-30 04:55:55,560 main.py:52] epoch 4394, training loss: 5860.49, average training loss: 5882.32, base loss: 15971.54
[INFO 2017-06-30 04:55:58,756 main.py:52] epoch 4395, training loss: 5386.37, average training loss: 5881.54, base loss: 15970.81
[INFO 2017-06-30 04:56:01,906 main.py:52] epoch 4396, training loss: 5971.48, average training loss: 5880.99, base loss: 15971.40
[INFO 2017-06-30 04:56:05,086 main.py:52] epoch 4397, training loss: 6224.65, average training loss: 5880.85, base loss: 15972.18
[INFO 2017-06-30 04:56:08,246 main.py:52] epoch 4398, training loss: 5856.39, average training loss: 5881.28, base loss: 15972.22
[INFO 2017-06-30 04:56:11,405 main.py:52] epoch 4399, training loss: 5759.22, average training loss: 5880.86, base loss: 15972.12
[INFO 2017-06-30 04:56:11,405 main.py:54] epoch 4399, testing
[INFO 2017-06-30 04:56:24,778 main.py:97] average testing loss: 5709.96, base loss: 15356.13
[INFO 2017-06-30 04:56:24,779 main.py:98] improve_loss: 9646.17, improve_percent: 0.63
[INFO 2017-06-30 04:56:24,780 main.py:66] current best improved percent: 0.64
[INFO 2017-06-30 04:56:27,973 main.py:52] epoch 4400, training loss: 5741.97, average training loss: 5880.82, base loss: 15971.06
[INFO 2017-06-30 04:56:31,127 main.py:52] epoch 4401, training loss: 5552.84, average training loss: 5880.03, base loss: 15970.16
[INFO 2017-06-30 04:56:34,268 main.py:52] epoch 4402, training loss: 6013.41, average training loss: 5880.27, base loss: 15970.85
[INFO 2017-06-30 04:56:37,438 main.py:52] epoch 4403, training loss: 5878.25, average training loss: 5880.12, base loss: 15971.06
[INFO 2017-06-30 04:56:40,612 main.py:52] epoch 4404, training loss: 6597.04, average training loss: 5880.75, base loss: 15971.88
[INFO 2017-06-30 04:56:43,761 main.py:52] epoch 4405, training loss: 5914.04, average training loss: 5880.02, base loss: 15971.39
[INFO 2017-06-30 04:56:46,896 main.py:52] epoch 4406, training loss: 5736.10, average training loss: 5879.99, base loss: 15971.14
[INFO 2017-06-30 04:56:50,084 main.py:52] epoch 4407, training loss: 5575.39, average training loss: 5879.43, base loss: 15971.11
[INFO 2017-06-30 04:56:53,276 main.py:52] epoch 4408, training loss: 5662.52, average training loss: 5879.24, base loss: 15970.73
[INFO 2017-06-30 04:56:56,454 main.py:52] epoch 4409, training loss: 5776.47, average training loss: 5879.32, base loss: 15970.78
[INFO 2017-06-30 04:56:59,612 main.py:52] epoch 4410, training loss: 5903.73, average training loss: 5878.71, base loss: 15970.48
[INFO 2017-06-30 04:57:02,758 main.py:52] epoch 4411, training loss: 5739.23, average training loss: 5878.31, base loss: 15970.78
[INFO 2017-06-30 04:57:05,966 main.py:52] epoch 4412, training loss: 6410.09, average training loss: 5878.76, base loss: 15971.79
[INFO 2017-06-30 04:57:09,127 main.py:52] epoch 4413, training loss: 5775.27, average training loss: 5878.62, base loss: 15971.72
[INFO 2017-06-30 04:57:12,269 main.py:52] epoch 4414, training loss: 6137.81, average training loss: 5878.98, base loss: 15972.61
[INFO 2017-06-30 04:57:15,459 main.py:52] epoch 4415, training loss: 5626.12, average training loss: 5878.64, base loss: 15972.31
[INFO 2017-06-30 04:57:18,602 main.py:52] epoch 4416, training loss: 5564.71, average training loss: 5878.64, base loss: 15971.63
[INFO 2017-06-30 04:57:21,720 main.py:52] epoch 4417, training loss: 6135.31, average training loss: 5878.76, base loss: 15971.61
[INFO 2017-06-30 04:57:24,876 main.py:52] epoch 4418, training loss: 5837.18, average training loss: 5878.99, base loss: 15971.80
[INFO 2017-06-30 04:57:28,011 main.py:52] epoch 4419, training loss: 5740.73, average training loss: 5878.85, base loss: 15970.97
[INFO 2017-06-30 04:57:31,188 main.py:52] epoch 4420, training loss: 5941.28, average training loss: 5879.06, base loss: 15971.47
[INFO 2017-06-30 04:57:34,399 main.py:52] epoch 4421, training loss: 5583.37, average training loss: 5878.89, base loss: 15971.05
[INFO 2017-06-30 04:57:37,527 main.py:52] epoch 4422, training loss: 5805.43, average training loss: 5879.04, base loss: 15971.33
[INFO 2017-06-30 04:57:40,729 main.py:52] epoch 4423, training loss: 5750.29, average training loss: 5878.39, base loss: 15972.08
[INFO 2017-06-30 04:57:43,907 main.py:52] epoch 4424, training loss: 5786.72, average training loss: 5878.48, base loss: 15971.73
[INFO 2017-06-30 04:57:47,063 main.py:52] epoch 4425, training loss: 5730.68, average training loss: 5878.64, base loss: 15970.70
[INFO 2017-06-30 04:57:50,236 main.py:52] epoch 4426, training loss: 5727.89, average training loss: 5878.27, base loss: 15970.91
[INFO 2017-06-30 04:57:53,402 main.py:52] epoch 4427, training loss: 5991.79, average training loss: 5877.77, base loss: 15971.40
[INFO 2017-06-30 04:57:56,565 main.py:52] epoch 4428, training loss: 5888.81, average training loss: 5877.43, base loss: 15971.46
[INFO 2017-06-30 04:57:59,734 main.py:52] epoch 4429, training loss: 5650.46, average training loss: 5877.21, base loss: 15971.88
[INFO 2017-06-30 04:58:02,945 main.py:52] epoch 4430, training loss: 5361.02, average training loss: 5876.63, base loss: 15970.99
[INFO 2017-06-30 04:58:06,094 main.py:52] epoch 4431, training loss: 5733.93, average training loss: 5876.28, base loss: 15970.60
[INFO 2017-06-30 04:58:09,272 main.py:52] epoch 4432, training loss: 5679.69, average training loss: 5875.55, base loss: 15970.92
[INFO 2017-06-30 04:58:12,449 main.py:52] epoch 4433, training loss: 5428.12, average training loss: 5875.10, base loss: 15971.16
[INFO 2017-06-30 04:58:15,639 main.py:52] epoch 4434, training loss: 5991.16, average training loss: 5875.59, base loss: 15971.27
[INFO 2017-06-30 04:58:18,826 main.py:52] epoch 4435, training loss: 5901.44, average training loss: 5874.81, base loss: 15971.50
[INFO 2017-06-30 04:58:22,015 main.py:52] epoch 4436, training loss: 5896.19, average training loss: 5874.71, base loss: 15971.18
[INFO 2017-06-30 04:58:25,180 main.py:52] epoch 4437, training loss: 6108.26, average training loss: 5874.70, base loss: 15971.72
[INFO 2017-06-30 04:58:28,309 main.py:52] epoch 4438, training loss: 6334.76, average training loss: 5875.04, base loss: 15972.77
[INFO 2017-06-30 04:58:31,445 main.py:52] epoch 4439, training loss: 5576.81, average training loss: 5873.92, base loss: 15972.17
[INFO 2017-06-30 04:58:34,620 main.py:52] epoch 4440, training loss: 5411.93, average training loss: 5873.41, base loss: 15972.21
[INFO 2017-06-30 04:58:37,825 main.py:52] epoch 4441, training loss: 6149.96, average training loss: 5872.94, base loss: 15972.91
[INFO 2017-06-30 04:58:40,995 main.py:52] epoch 4442, training loss: 5947.40, average training loss: 5873.24, base loss: 15973.21
[INFO 2017-06-30 04:58:44,197 main.py:52] epoch 4443, training loss: 6063.64, average training loss: 5873.52, base loss: 15973.65
[INFO 2017-06-30 04:58:47,375 main.py:52] epoch 4444, training loss: 5576.79, average training loss: 5872.98, base loss: 15973.60
[INFO 2017-06-30 04:58:50,565 main.py:52] epoch 4445, training loss: 5320.88, average training loss: 5872.19, base loss: 15972.89
[INFO 2017-06-30 04:58:53,800 main.py:52] epoch 4446, training loss: 5952.30, average training loss: 5872.02, base loss: 15972.53
[INFO 2017-06-30 04:58:56,947 main.py:52] epoch 4447, training loss: 5811.20, average training loss: 5871.38, base loss: 15972.94
[INFO 2017-06-30 04:59:00,087 main.py:52] epoch 4448, training loss: 5501.43, average training loss: 5870.86, base loss: 15971.85
[INFO 2017-06-30 04:59:03,288 main.py:52] epoch 4449, training loss: 6092.74, average training loss: 5871.47, base loss: 15972.10
[INFO 2017-06-30 04:59:06,439 main.py:52] epoch 4450, training loss: 6099.61, average training loss: 5871.78, base loss: 15973.10
[INFO 2017-06-30 04:59:09,628 main.py:52] epoch 4451, training loss: 5833.51, average training loss: 5871.36, base loss: 15973.40
[INFO 2017-06-30 04:59:12,785 main.py:52] epoch 4452, training loss: 5901.51, average training loss: 5871.13, base loss: 15973.20
[INFO 2017-06-30 04:59:15,990 main.py:52] epoch 4453, training loss: 5249.42, average training loss: 5870.76, base loss: 15972.59
[INFO 2017-06-30 04:59:19,177 main.py:52] epoch 4454, training loss: 5855.44, average training loss: 5870.57, base loss: 15972.90
[INFO 2017-06-30 04:59:22,364 main.py:52] epoch 4455, training loss: 5906.46, average training loss: 5870.74, base loss: 15972.63
[INFO 2017-06-30 04:59:25,529 main.py:52] epoch 4456, training loss: 5937.59, average training loss: 5870.25, base loss: 15972.83
[INFO 2017-06-30 04:59:28,701 main.py:52] epoch 4457, training loss: 6229.64, average training loss: 5871.09, base loss: 15972.79
[INFO 2017-06-30 04:59:31,891 main.py:52] epoch 4458, training loss: 5467.59, average training loss: 5870.41, base loss: 15972.34
[INFO 2017-06-30 04:59:35,075 main.py:52] epoch 4459, training loss: 5717.17, average training loss: 5870.10, base loss: 15972.32
[INFO 2017-06-30 04:59:38,230 main.py:52] epoch 4460, training loss: 6546.25, average training loss: 5871.04, base loss: 15973.81
[INFO 2017-06-30 04:59:41,372 main.py:52] epoch 4461, training loss: 6035.08, average training loss: 5871.22, base loss: 15974.11
[INFO 2017-06-30 04:59:44,504 main.py:52] epoch 4462, training loss: 5792.76, average training loss: 5871.45, base loss: 15973.69
[INFO 2017-06-30 04:59:47,658 main.py:52] epoch 4463, training loss: 5859.26, average training loss: 5871.07, base loss: 15973.20
[INFO 2017-06-30 04:59:50,811 main.py:52] epoch 4464, training loss: 6078.44, average training loss: 5871.57, base loss: 15973.67
[INFO 2017-06-30 04:59:54,002 main.py:52] epoch 4465, training loss: 5835.97, average training loss: 5871.43, base loss: 15973.97
[INFO 2017-06-30 04:59:57,170 main.py:52] epoch 4466, training loss: 5563.82, average training loss: 5870.67, base loss: 15973.31
[INFO 2017-06-30 05:00:00,357 main.py:52] epoch 4467, training loss: 5558.45, average training loss: 5870.04, base loss: 15972.79
[INFO 2017-06-30 05:00:03,553 main.py:52] epoch 4468, training loss: 5617.12, average training loss: 5869.89, base loss: 15972.57
[INFO 2017-06-30 05:00:06,709 main.py:52] epoch 4469, training loss: 5697.76, average training loss: 5869.29, base loss: 15972.68
[INFO 2017-06-30 05:00:09,881 main.py:52] epoch 4470, training loss: 5519.74, average training loss: 5869.04, base loss: 15972.43
[INFO 2017-06-30 05:00:13,079 main.py:52] epoch 4471, training loss: 5395.07, average training loss: 5868.70, base loss: 15972.12
[INFO 2017-06-30 05:00:16,225 main.py:52] epoch 4472, training loss: 5597.70, average training loss: 5868.42, base loss: 15972.45
[INFO 2017-06-30 05:00:19,387 main.py:52] epoch 4473, training loss: 5701.20, average training loss: 5868.29, base loss: 15972.59
[INFO 2017-06-30 05:00:22,547 main.py:52] epoch 4474, training loss: 6350.78, average training loss: 5868.72, base loss: 15973.27
[INFO 2017-06-30 05:00:25,690 main.py:52] epoch 4475, training loss: 5959.80, average training loss: 5869.08, base loss: 15973.24
[INFO 2017-06-30 05:00:28,826 main.py:52] epoch 4476, training loss: 5593.82, average training loss: 5868.46, base loss: 15972.48
[INFO 2017-06-30 05:00:31,983 main.py:52] epoch 4477, training loss: 5650.80, average training loss: 5868.55, base loss: 15972.42
[INFO 2017-06-30 05:00:35,136 main.py:52] epoch 4478, training loss: 5865.39, average training loss: 5868.91, base loss: 15972.51
[INFO 2017-06-30 05:00:38,315 main.py:52] epoch 4479, training loss: 5734.58, average training loss: 5868.28, base loss: 15972.11
[INFO 2017-06-30 05:00:41,500 main.py:52] epoch 4480, training loss: 5918.14, average training loss: 5867.72, base loss: 15971.99
[INFO 2017-06-30 05:00:44,700 main.py:52] epoch 4481, training loss: 5548.00, average training loss: 5867.09, base loss: 15971.46
[INFO 2017-06-30 05:00:47,901 main.py:52] epoch 4482, training loss: 6028.43, average training loss: 5866.94, base loss: 15971.88
[INFO 2017-06-30 05:00:51,087 main.py:52] epoch 4483, training loss: 5514.49, average training loss: 5866.63, base loss: 15971.11
[INFO 2017-06-30 05:00:54,265 main.py:52] epoch 4484, training loss: 5811.49, average training loss: 5866.45, base loss: 15971.20
[INFO 2017-06-30 05:00:57,431 main.py:52] epoch 4485, training loss: 5794.32, average training loss: 5866.50, base loss: 15971.97
[INFO 2017-06-30 05:01:00,623 main.py:52] epoch 4486, training loss: 5638.35, average training loss: 5866.30, base loss: 15972.24
[INFO 2017-06-30 05:01:03,791 main.py:52] epoch 4487, training loss: 5754.40, average training loss: 5866.11, base loss: 15971.75
[INFO 2017-06-30 05:01:06,968 main.py:52] epoch 4488, training loss: 6009.62, average training loss: 5866.01, base loss: 15972.21
[INFO 2017-06-30 05:01:10,129 main.py:52] epoch 4489, training loss: 5196.68, average training loss: 5865.34, base loss: 15971.62
[INFO 2017-06-30 05:01:13,295 main.py:52] epoch 4490, training loss: 5757.45, average training loss: 5865.75, base loss: 15972.00
[INFO 2017-06-30 05:01:16,451 main.py:52] epoch 4491, training loss: 5825.17, average training loss: 5865.22, base loss: 15972.23
[INFO 2017-06-30 05:01:19,604 main.py:52] epoch 4492, training loss: 5609.28, average training loss: 5864.77, base loss: 15972.49
[INFO 2017-06-30 05:01:22,780 main.py:52] epoch 4493, training loss: 6072.45, average training loss: 5864.71, base loss: 15972.74
[INFO 2017-06-30 05:01:25,956 main.py:52] epoch 4494, training loss: 5455.04, average training loss: 5864.65, base loss: 15972.79
[INFO 2017-06-30 05:01:29,163 main.py:52] epoch 4495, training loss: 5502.96, average training loss: 5864.20, base loss: 15972.22
[INFO 2017-06-30 05:01:32,382 main.py:52] epoch 4496, training loss: 5960.90, average training loss: 5864.06, base loss: 15972.72
[INFO 2017-06-30 05:01:35,575 main.py:52] epoch 4497, training loss: 5611.42, average training loss: 5863.89, base loss: 15972.23
[INFO 2017-06-30 05:01:38,725 main.py:52] epoch 4498, training loss: 5630.21, average training loss: 5863.51, base loss: 15971.56
[INFO 2017-06-30 05:01:41,928 main.py:52] epoch 4499, training loss: 5803.34, average training loss: 5863.22, base loss: 15971.08
[INFO 2017-06-30 05:01:41,928 main.py:54] epoch 4499, testing
[INFO 2017-06-30 05:01:55,228 main.py:97] average testing loss: 5949.65, base loss: 16371.76
[INFO 2017-06-30 05:01:55,228 main.py:98] improve_loss: 10422.11, improve_percent: 0.64
[INFO 2017-06-30 05:01:55,230 main.py:66] current best improved percent: 0.64
[INFO 2017-06-30 05:01:58,356 main.py:52] epoch 4500, training loss: 5900.05, average training loss: 5863.45, base loss: 15971.84
[INFO 2017-06-30 05:02:01,519 main.py:52] epoch 4501, training loss: 5764.50, average training loss: 5863.36, base loss: 15972.32
[INFO 2017-06-30 05:02:04,681 main.py:52] epoch 4502, training loss: 5572.10, average training loss: 5863.13, base loss: 15971.33
[INFO 2017-06-30 05:02:07,869 main.py:52] epoch 4503, training loss: 6447.64, average training loss: 5863.91, base loss: 15972.33
[INFO 2017-06-30 05:02:11,076 main.py:52] epoch 4504, training loss: 5967.38, average training loss: 5863.82, base loss: 15972.44
[INFO 2017-06-30 05:02:14,204 main.py:52] epoch 4505, training loss: 5716.67, average training loss: 5864.04, base loss: 15972.52
[INFO 2017-06-30 05:02:17,378 main.py:52] epoch 4506, training loss: 6340.56, average training loss: 5864.49, base loss: 15973.50
[INFO 2017-06-30 05:02:20,546 main.py:52] epoch 4507, training loss: 5628.97, average training loss: 5864.08, base loss: 15973.04
[INFO 2017-06-30 05:02:23,746 main.py:52] epoch 4508, training loss: 5706.37, average training loss: 5863.49, base loss: 15972.69
[INFO 2017-06-30 05:02:26,938 main.py:52] epoch 4509, training loss: 5299.41, average training loss: 5863.33, base loss: 15972.15
[INFO 2017-06-30 05:02:30,081 main.py:52] epoch 4510, training loss: 6135.77, average training loss: 5863.09, base loss: 15972.05
[INFO 2017-06-30 05:02:33,234 main.py:52] epoch 4511, training loss: 5965.65, average training loss: 5863.10, base loss: 15971.19
[INFO 2017-06-30 05:02:36,414 main.py:52] epoch 4512, training loss: 5441.11, average training loss: 5862.54, base loss: 15970.91
[INFO 2017-06-30 05:02:39,602 main.py:52] epoch 4513, training loss: 5665.48, average training loss: 5861.71, base loss: 15970.54
[INFO 2017-06-30 05:02:42,797 main.py:52] epoch 4514, training loss: 5416.31, average training loss: 5861.21, base loss: 15969.87
[INFO 2017-06-30 05:02:45,949 main.py:52] epoch 4515, training loss: 5817.15, average training loss: 5861.48, base loss: 15970.14
[INFO 2017-06-30 05:02:49,113 main.py:52] epoch 4516, training loss: 5584.84, average training loss: 5861.00, base loss: 15970.22
[INFO 2017-06-30 05:02:52,246 main.py:52] epoch 4517, training loss: 5591.32, average training loss: 5860.73, base loss: 15969.82
[INFO 2017-06-30 05:02:55,425 main.py:52] epoch 4518, training loss: 5400.94, average training loss: 5860.57, base loss: 15969.33
[INFO 2017-06-30 05:02:58,589 main.py:52] epoch 4519, training loss: 6096.79, average training loss: 5860.86, base loss: 15969.94
[INFO 2017-06-30 05:03:01,777 main.py:52] epoch 4520, training loss: 5904.77, average training loss: 5860.49, base loss: 15970.55
[INFO 2017-06-30 05:03:04,952 main.py:52] epoch 4521, training loss: 5956.89, average training loss: 5860.47, base loss: 15971.30
[INFO 2017-06-30 05:03:08,131 main.py:52] epoch 4522, training loss: 5938.89, average training loss: 5860.72, base loss: 15971.43
[INFO 2017-06-30 05:03:11,293 main.py:52] epoch 4523, training loss: 5653.91, average training loss: 5860.54, base loss: 15971.09
[INFO 2017-06-30 05:03:14,482 main.py:52] epoch 4524, training loss: 5530.63, average training loss: 5860.50, base loss: 15970.95
[INFO 2017-06-30 05:03:17,650 main.py:52] epoch 4525, training loss: 5308.51, average training loss: 5859.97, base loss: 15970.60
[INFO 2017-06-30 05:03:20,863 main.py:52] epoch 4526, training loss: 5903.86, average training loss: 5859.83, base loss: 15971.31
[INFO 2017-06-30 05:03:24,054 main.py:52] epoch 4527, training loss: 5517.29, average training loss: 5859.24, base loss: 15970.82
[INFO 2017-06-30 05:03:27,263 main.py:52] epoch 4528, training loss: 6052.92, average training loss: 5859.32, base loss: 15970.72
[INFO 2017-06-30 05:03:30,426 main.py:52] epoch 4529, training loss: 6148.71, average training loss: 5859.52, base loss: 15971.02
[INFO 2017-06-30 05:03:33,575 main.py:52] epoch 4530, training loss: 5495.30, average training loss: 5859.02, base loss: 15970.10
[INFO 2017-06-30 05:03:36,739 main.py:52] epoch 4531, training loss: 5741.47, average training loss: 5858.78, base loss: 15969.86
[INFO 2017-06-30 05:03:39,887 main.py:52] epoch 4532, training loss: 5732.40, average training loss: 5858.30, base loss: 15969.62
[INFO 2017-06-30 05:03:43,017 main.py:52] epoch 4533, training loss: 6005.08, average training loss: 5857.79, base loss: 15969.75
[INFO 2017-06-30 05:03:46,231 main.py:52] epoch 4534, training loss: 5824.08, average training loss: 5857.80, base loss: 15969.65
[INFO 2017-06-30 05:03:49,391 main.py:52] epoch 4535, training loss: 5879.04, average training loss: 5857.90, base loss: 15969.98
[INFO 2017-06-30 05:03:52,562 main.py:52] epoch 4536, training loss: 5682.41, average training loss: 5857.59, base loss: 15970.21
[INFO 2017-06-30 05:03:55,743 main.py:52] epoch 4537, training loss: 5923.37, average training loss: 5857.47, base loss: 15970.62
[INFO 2017-06-30 05:03:58,934 main.py:52] epoch 4538, training loss: 5717.56, average training loss: 5857.05, base loss: 15970.69
[INFO 2017-06-30 05:04:02,112 main.py:52] epoch 4539, training loss: 5649.65, average training loss: 5856.05, base loss: 15970.97
[INFO 2017-06-30 05:04:05,305 main.py:52] epoch 4540, training loss: 5710.40, average training loss: 5855.70, base loss: 15970.12
[INFO 2017-06-30 05:04:08,433 main.py:52] epoch 4541, training loss: 5915.84, average training loss: 5855.66, base loss: 15970.31
[INFO 2017-06-30 05:04:11,614 main.py:52] epoch 4542, training loss: 5754.03, average training loss: 5855.89, base loss: 15970.36
[INFO 2017-06-30 05:04:14,794 main.py:52] epoch 4543, training loss: 5940.29, average training loss: 5855.78, base loss: 15970.74
[INFO 2017-06-30 05:04:17,964 main.py:52] epoch 4544, training loss: 5769.86, average training loss: 5856.12, base loss: 15970.66
[INFO 2017-06-30 05:04:21,167 main.py:52] epoch 4545, training loss: 5417.97, average training loss: 5855.25, base loss: 15969.51
[INFO 2017-06-30 05:04:24,314 main.py:52] epoch 4546, training loss: 6124.18, average training loss: 5855.28, base loss: 15969.63
[INFO 2017-06-30 05:04:27,490 main.py:52] epoch 4547, training loss: 5549.00, average training loss: 5854.61, base loss: 15969.29
[INFO 2017-06-30 05:04:30,667 main.py:52] epoch 4548, training loss: 5884.68, average training loss: 5854.34, base loss: 15969.98
[INFO 2017-06-30 05:04:33,842 main.py:52] epoch 4549, training loss: 6023.17, average training loss: 5854.73, base loss: 15970.64
[INFO 2017-06-30 05:04:37,024 main.py:52] epoch 4550, training loss: 6410.57, average training loss: 5855.25, base loss: 15971.42
[INFO 2017-06-30 05:04:40,195 main.py:52] epoch 4551, training loss: 5711.17, average training loss: 5854.90, base loss: 15971.43
[INFO 2017-06-30 05:04:43,354 main.py:52] epoch 4552, training loss: 6053.83, average training loss: 5855.08, base loss: 15971.67
[INFO 2017-06-30 05:04:46,506 main.py:52] epoch 4553, training loss: 5431.32, average training loss: 5854.24, base loss: 15971.11
[INFO 2017-06-30 05:04:49,697 main.py:52] epoch 4554, training loss: 5632.05, average training loss: 5854.33, base loss: 15970.82
[INFO 2017-06-30 05:04:52,873 main.py:52] epoch 4555, training loss: 5451.96, average training loss: 5853.97, base loss: 15970.53
[INFO 2017-06-30 05:04:55,987 main.py:52] epoch 4556, training loss: 5729.21, average training loss: 5853.84, base loss: 15970.73
[INFO 2017-06-30 05:04:59,161 main.py:52] epoch 4557, training loss: 6311.01, average training loss: 5854.30, base loss: 15971.21
[INFO 2017-06-30 05:05:02,344 main.py:52] epoch 4558, training loss: 5774.13, average training loss: 5854.40, base loss: 15970.94
[INFO 2017-06-30 05:05:05,483 main.py:52] epoch 4559, training loss: 5823.14, average training loss: 5854.31, base loss: 15970.77
[INFO 2017-06-30 05:05:08,645 main.py:52] epoch 4560, training loss: 5759.92, average training loss: 5854.00, base loss: 15970.58
[INFO 2017-06-30 05:05:11,784 main.py:52] epoch 4561, training loss: 5636.69, average training loss: 5853.77, base loss: 15970.16
[INFO 2017-06-30 05:05:14,966 main.py:52] epoch 4562, training loss: 5626.38, average training loss: 5853.60, base loss: 15969.06
[INFO 2017-06-30 05:05:18,149 main.py:52] epoch 4563, training loss: 5691.78, average training loss: 5853.17, base loss: 15968.64
[INFO 2017-06-30 05:05:21,322 main.py:52] epoch 4564, training loss: 5389.16, average training loss: 5853.11, base loss: 15967.85
[INFO 2017-06-30 05:05:24,451 main.py:52] epoch 4565, training loss: 5774.84, average training loss: 5852.53, base loss: 15967.59
[INFO 2017-06-30 05:05:27,656 main.py:52] epoch 4566, training loss: 5550.35, average training loss: 5852.30, base loss: 15967.92
[INFO 2017-06-30 05:05:30,868 main.py:52] epoch 4567, training loss: 5772.23, average training loss: 5852.25, base loss: 15968.05
[INFO 2017-06-30 05:05:34,036 main.py:52] epoch 4568, training loss: 6379.26, average training loss: 5852.45, base loss: 15968.90
[INFO 2017-06-30 05:05:37,246 main.py:52] epoch 4569, training loss: 5625.21, average training loss: 5851.83, base loss: 15968.55
[INFO 2017-06-30 05:05:40,390 main.py:52] epoch 4570, training loss: 5560.76, average training loss: 5851.90, base loss: 15968.06
[INFO 2017-06-30 05:05:43,561 main.py:52] epoch 4571, training loss: 5875.34, average training loss: 5851.85, base loss: 15968.24
[INFO 2017-06-30 05:05:46,745 main.py:52] epoch 4572, training loss: 5797.81, average training loss: 5851.70, base loss: 15968.60
[INFO 2017-06-30 05:05:49,873 main.py:52] epoch 4573, training loss: 6067.69, average training loss: 5852.01, base loss: 15969.05
[INFO 2017-06-30 05:05:53,066 main.py:52] epoch 4574, training loss: 6218.62, average training loss: 5852.57, base loss: 15969.25
[INFO 2017-06-30 05:05:56,215 main.py:52] epoch 4575, training loss: 5622.29, average training loss: 5852.07, base loss: 15968.93
[INFO 2017-06-30 05:05:59,453 main.py:52] epoch 4576, training loss: 6114.34, average training loss: 5852.14, base loss: 15970.10
[INFO 2017-06-30 05:06:02,642 main.py:52] epoch 4577, training loss: 5747.37, average training loss: 5851.99, base loss: 15970.30
[INFO 2017-06-30 05:06:05,815 main.py:52] epoch 4578, training loss: 5987.66, average training loss: 5851.82, base loss: 15970.71
[INFO 2017-06-30 05:06:09,017 main.py:52] epoch 4579, training loss: 6124.51, average training loss: 5851.76, base loss: 15971.43
[INFO 2017-06-30 05:06:12,208 main.py:52] epoch 4580, training loss: 5613.21, average training loss: 5851.20, base loss: 15971.23
[INFO 2017-06-30 05:06:15,419 main.py:52] epoch 4581, training loss: 5816.95, average training loss: 5850.97, base loss: 15971.11
[INFO 2017-06-30 05:06:18,620 main.py:52] epoch 4582, training loss: 5581.75, average training loss: 5850.54, base loss: 15970.61
[INFO 2017-06-30 05:06:21,754 main.py:52] epoch 4583, training loss: 5886.05, average training loss: 5850.44, base loss: 15970.60
[INFO 2017-06-30 05:06:24,911 main.py:52] epoch 4584, training loss: 6072.09, average training loss: 5851.12, base loss: 15971.49
[INFO 2017-06-30 05:06:28,106 main.py:52] epoch 4585, training loss: 5925.32, average training loss: 5851.07, base loss: 15972.17
[INFO 2017-06-30 05:06:31,274 main.py:52] epoch 4586, training loss: 5471.58, average training loss: 5851.01, base loss: 15971.94
[INFO 2017-06-30 05:06:34,435 main.py:52] epoch 4587, training loss: 5523.98, average training loss: 5850.56, base loss: 15972.21
[INFO 2017-06-30 05:06:37,667 main.py:52] epoch 4588, training loss: 5628.27, average training loss: 5850.66, base loss: 15971.32
[INFO 2017-06-30 05:06:40,875 main.py:52] epoch 4589, training loss: 5704.92, average training loss: 5850.62, base loss: 15970.63
[INFO 2017-06-30 05:06:44,098 main.py:52] epoch 4590, training loss: 6303.68, average training loss: 5850.88, base loss: 15971.03
[INFO 2017-06-30 05:06:47,222 main.py:52] epoch 4591, training loss: 6029.84, average training loss: 5850.87, base loss: 15970.66
[INFO 2017-06-30 05:06:50,370 main.py:52] epoch 4592, training loss: 5809.46, average training loss: 5850.83, base loss: 15970.52
[INFO 2017-06-30 05:06:53,497 main.py:52] epoch 4593, training loss: 6395.57, average training loss: 5851.50, base loss: 15970.68
[INFO 2017-06-30 05:06:56,627 main.py:52] epoch 4594, training loss: 5900.86, average training loss: 5851.09, base loss: 15971.07
[INFO 2017-06-30 05:06:59,828 main.py:52] epoch 4595, training loss: 5900.05, average training loss: 5851.46, base loss: 15971.27
[INFO 2017-06-30 05:07:03,014 main.py:52] epoch 4596, training loss: 5900.14, average training loss: 5851.33, base loss: 15971.63
[INFO 2017-06-30 05:07:06,209 main.py:52] epoch 4597, training loss: 5533.82, average training loss: 5850.80, base loss: 15971.59
[INFO 2017-06-30 05:07:09,389 main.py:52] epoch 4598, training loss: 5307.29, average training loss: 5849.33, base loss: 15971.53
[INFO 2017-06-30 05:07:12,531 main.py:52] epoch 4599, training loss: 5634.86, average training loss: 5848.94, base loss: 15971.81
[INFO 2017-06-30 05:07:12,531 main.py:54] epoch 4599, testing
[INFO 2017-06-30 05:07:25,831 main.py:97] average testing loss: 5745.91, base loss: 16025.76
[INFO 2017-06-30 05:07:25,831 main.py:98] improve_loss: 10279.85, improve_percent: 0.64
[INFO 2017-06-30 05:07:25,833 main.py:62] model save to ./model/final.pth
[INFO 2017-06-30 05:07:25,868 main.py:66] current best improved percent: 0.64
[INFO 2017-06-30 05:07:29,040 main.py:52] epoch 4600, training loss: 5774.29, average training loss: 5848.68, base loss: 15971.48
[INFO 2017-06-30 05:07:32,213 main.py:52] epoch 4601, training loss: 5688.48, average training loss: 5848.71, base loss: 15971.96
[INFO 2017-06-30 05:07:35,403 main.py:52] epoch 4602, training loss: 5237.89, average training loss: 5847.75, base loss: 15971.68
[INFO 2017-06-30 05:07:38,558 main.py:52] epoch 4603, training loss: 6359.07, average training loss: 5848.29, base loss: 15972.61
[INFO 2017-06-30 05:07:41,725 main.py:52] epoch 4604, training loss: 6102.59, average training loss: 5848.28, base loss: 15973.14
[INFO 2017-06-30 05:07:44,875 main.py:52] epoch 4605, training loss: 5811.94, average training loss: 5848.26, base loss: 15973.58
[INFO 2017-06-30 05:07:48,026 main.py:52] epoch 4606, training loss: 5875.02, average training loss: 5848.35, base loss: 15972.98
[INFO 2017-06-30 05:07:51,225 main.py:52] epoch 4607, training loss: 5829.47, average training loss: 5848.50, base loss: 15972.98
[INFO 2017-06-30 05:07:54,429 main.py:52] epoch 4608, training loss: 5797.26, average training loss: 5848.21, base loss: 15973.30
[INFO 2017-06-30 05:07:57,658 main.py:52] epoch 4609, training loss: 5791.17, average training loss: 5848.41, base loss: 15973.82
[INFO 2017-06-30 05:08:00,809 main.py:52] epoch 4610, training loss: 5720.59, average training loss: 5848.36, base loss: 15973.48
[INFO 2017-06-30 05:08:03,965 main.py:52] epoch 4611, training loss: 5738.75, average training loss: 5848.47, base loss: 15973.45
[INFO 2017-06-30 05:08:07,128 main.py:52] epoch 4612, training loss: 5811.08, average training loss: 5848.40, base loss: 15973.23
[INFO 2017-06-30 05:08:10,323 main.py:52] epoch 4613, training loss: 5587.91, average training loss: 5847.90, base loss: 15972.34
[INFO 2017-06-30 05:08:13,488 main.py:52] epoch 4614, training loss: 5692.40, average training loss: 5847.47, base loss: 15972.78
[INFO 2017-06-30 05:08:16,636 main.py:52] epoch 4615, training loss: 5883.93, average training loss: 5847.08, base loss: 15973.25
[INFO 2017-06-30 05:08:19,817 main.py:52] epoch 4616, training loss: 5715.83, average training loss: 5847.04, base loss: 15973.35
[INFO 2017-06-30 05:08:23,001 main.py:52] epoch 4617, training loss: 5632.05, average training loss: 5846.65, base loss: 15973.21
[INFO 2017-06-30 05:08:26,172 main.py:52] epoch 4618, training loss: 5399.35, average training loss: 5846.24, base loss: 15972.81
[INFO 2017-06-30 05:08:29,308 main.py:52] epoch 4619, training loss: 6336.63, average training loss: 5846.63, base loss: 15973.93
[INFO 2017-06-30 05:08:32,459 main.py:52] epoch 4620, training loss: 5731.61, average training loss: 5846.13, base loss: 15973.66
[INFO 2017-06-30 05:08:35,619 main.py:52] epoch 4621, training loss: 5932.09, average training loss: 5845.99, base loss: 15974.20
[INFO 2017-06-30 05:08:38,788 main.py:52] epoch 4622, training loss: 5888.17, average training loss: 5846.31, base loss: 15974.24
[INFO 2017-06-30 05:08:41,964 main.py:52] epoch 4623, training loss: 5631.40, average training loss: 5845.82, base loss: 15973.73
[INFO 2017-06-30 05:08:45,146 main.py:52] epoch 4624, training loss: 5741.38, average training loss: 5845.61, base loss: 15974.46
[INFO 2017-06-30 05:08:48,311 main.py:52] epoch 4625, training loss: 5899.29, average training loss: 5845.74, base loss: 15975.01
[INFO 2017-06-30 05:08:51,502 main.py:52] epoch 4626, training loss: 5902.74, average training loss: 5845.57, base loss: 15975.06
[INFO 2017-06-30 05:08:54,679 main.py:52] epoch 4627, training loss: 6293.85, average training loss: 5845.68, base loss: 15975.55
[INFO 2017-06-30 05:08:57,867 main.py:52] epoch 4628, training loss: 6070.71, average training loss: 5846.01, base loss: 15975.98
[INFO 2017-06-30 05:09:01,062 main.py:52] epoch 4629, training loss: 5532.98, average training loss: 5845.61, base loss: 15976.07
[INFO 2017-06-30 05:09:04,242 main.py:52] epoch 4630, training loss: 5413.09, average training loss: 5845.37, base loss: 15976.08
[INFO 2017-06-30 05:09:07,413 main.py:52] epoch 4631, training loss: 5929.93, average training loss: 5845.83, base loss: 15976.46
[INFO 2017-06-30 05:09:10,594 main.py:52] epoch 4632, training loss: 6020.13, average training loss: 5845.77, base loss: 15976.91
[INFO 2017-06-30 05:09:13,750 main.py:52] epoch 4633, training loss: 5597.63, average training loss: 5845.62, base loss: 15976.89
[INFO 2017-06-30 05:09:16,928 main.py:52] epoch 4634, training loss: 6068.85, average training loss: 5846.13, base loss: 15977.90
[INFO 2017-06-30 05:09:20,106 main.py:52] epoch 4635, training loss: 5784.20, average training loss: 5845.69, base loss: 15978.33
[INFO 2017-06-30 05:09:23,263 main.py:52] epoch 4636, training loss: 5208.87, average training loss: 5845.09, base loss: 15978.07
[INFO 2017-06-30 05:09:26,441 main.py:52] epoch 4637, training loss: 5692.88, average training loss: 5844.51, base loss: 15978.13
[INFO 2017-06-30 05:09:29,640 main.py:52] epoch 4638, training loss: 5648.10, average training loss: 5844.19, base loss: 15977.50
[INFO 2017-06-30 05:09:32,832 main.py:52] epoch 4639, training loss: 5511.12, average training loss: 5843.70, base loss: 15976.43
[INFO 2017-06-30 05:09:36,040 main.py:52] epoch 4640, training loss: 5910.51, average training loss: 5843.37, base loss: 15977.04
[INFO 2017-06-30 05:09:39,185 main.py:52] epoch 4641, training loss: 5683.38, average training loss: 5843.35, base loss: 15977.17
[INFO 2017-06-30 05:09:42,366 main.py:52] epoch 4642, training loss: 5323.63, average training loss: 5842.81, base loss: 15976.42
[INFO 2017-06-30 05:09:45,549 main.py:52] epoch 4643, training loss: 5939.44, average training loss: 5842.93, base loss: 15976.52
[INFO 2017-06-30 05:09:48,704 main.py:52] epoch 4644, training loss: 5444.49, average training loss: 5842.43, base loss: 15975.97
[INFO 2017-06-30 05:09:51,866 main.py:52] epoch 4645, training loss: 6029.23, average training loss: 5842.46, base loss: 15976.44
[INFO 2017-06-30 05:09:55,034 main.py:52] epoch 4646, training loss: 5798.22, average training loss: 5842.32, base loss: 15976.38
[INFO 2017-06-30 05:09:58,229 main.py:52] epoch 4647, training loss: 5620.97, average training loss: 5841.76, base loss: 15975.66
[INFO 2017-06-30 05:10:01,411 main.py:52] epoch 4648, training loss: 5594.74, average training loss: 5841.43, base loss: 15975.62
[INFO 2017-06-30 05:10:04,580 main.py:52] epoch 4649, training loss: 6199.06, average training loss: 5842.07, base loss: 15975.87
[INFO 2017-06-30 05:10:07,737 main.py:52] epoch 4650, training loss: 5820.27, average training loss: 5842.19, base loss: 15975.74
[INFO 2017-06-30 05:10:10,906 main.py:52] epoch 4651, training loss: 5999.68, average training loss: 5841.68, base loss: 15976.53
[INFO 2017-06-30 05:10:14,078 main.py:52] epoch 4652, training loss: 5650.26, average training loss: 5840.88, base loss: 15975.61
[INFO 2017-06-30 05:10:17,241 main.py:52] epoch 4653, training loss: 5708.12, average training loss: 5840.73, base loss: 15974.82
[INFO 2017-06-30 05:10:20,403 main.py:52] epoch 4654, training loss: 6002.09, average training loss: 5841.21, base loss: 15974.49
[INFO 2017-06-30 05:10:23,547 main.py:52] epoch 4655, training loss: 5570.38, average training loss: 5840.90, base loss: 15973.70
[INFO 2017-06-30 05:10:26,679 main.py:52] epoch 4656, training loss: 5513.89, average training loss: 5840.32, base loss: 15973.66
[INFO 2017-06-30 05:10:29,837 main.py:52] epoch 4657, training loss: 5909.20, average training loss: 5840.16, base loss: 15973.60
[INFO 2017-06-30 05:10:32,995 main.py:52] epoch 4658, training loss: 5836.29, average training loss: 5840.17, base loss: 15974.37
[INFO 2017-06-30 05:10:36,161 main.py:52] epoch 4659, training loss: 6079.84, average training loss: 5839.94, base loss: 15975.19
[INFO 2017-06-30 05:10:39,299 main.py:52] epoch 4660, training loss: 5178.89, average training loss: 5839.36, base loss: 15975.10
[INFO 2017-06-30 05:10:42,465 main.py:52] epoch 4661, training loss: 5585.95, average training loss: 5838.81, base loss: 15975.63
[INFO 2017-06-30 05:10:45,616 main.py:52] epoch 4662, training loss: 5823.73, average training loss: 5838.90, base loss: 15974.94
[INFO 2017-06-30 05:10:48,788 main.py:52] epoch 4663, training loss: 6157.48, average training loss: 5839.14, base loss: 15974.72
[INFO 2017-06-30 05:10:51,933 main.py:52] epoch 4664, training loss: 5576.97, average training loss: 5839.20, base loss: 15974.66
[INFO 2017-06-30 05:10:55,073 main.py:52] epoch 4665, training loss: 6081.60, average training loss: 5839.71, base loss: 15974.80
[INFO 2017-06-30 05:10:58,247 main.py:52] epoch 4666, training loss: 5661.73, average training loss: 5839.18, base loss: 15974.55
[INFO 2017-06-30 05:11:01,439 main.py:52] epoch 4667, training loss: 5856.60, average training loss: 5839.51, base loss: 15974.31
[INFO 2017-06-30 05:11:04,591 main.py:52] epoch 4668, training loss: 5908.57, average training loss: 5839.51, base loss: 15974.12
[INFO 2017-06-30 05:11:07,816 main.py:52] epoch 4669, training loss: 6021.24, average training loss: 5839.37, base loss: 15974.02
[INFO 2017-06-30 05:11:10,958 main.py:52] epoch 4670, training loss: 5698.71, average training loss: 5839.25, base loss: 15974.13
[INFO 2017-06-30 05:11:14,115 main.py:52] epoch 4671, training loss: 6183.52, average training loss: 5839.30, base loss: 15974.55
[INFO 2017-06-30 05:11:17,264 main.py:52] epoch 4672, training loss: 5743.19, average training loss: 5839.35, base loss: 15974.48
[INFO 2017-06-30 05:11:20,448 main.py:52] epoch 4673, training loss: 5778.11, average training loss: 5839.15, base loss: 15974.92
[INFO 2017-06-30 05:11:23,658 main.py:52] epoch 4674, training loss: 5746.80, average training loss: 5838.68, base loss: 15975.03
[INFO 2017-06-30 05:11:26,809 main.py:52] epoch 4675, training loss: 5664.56, average training loss: 5839.00, base loss: 15974.94
[INFO 2017-06-30 05:11:29,979 main.py:52] epoch 4676, training loss: 5553.75, average training loss: 5838.90, base loss: 15975.14
[INFO 2017-06-30 05:11:33,129 main.py:52] epoch 4677, training loss: 6028.37, average training loss: 5839.06, base loss: 15976.25
[INFO 2017-06-30 05:11:36,275 main.py:52] epoch 4678, training loss: 5642.76, average training loss: 5838.78, base loss: 15976.00
[INFO 2017-06-30 05:11:39,489 main.py:52] epoch 4679, training loss: 5985.44, average training loss: 5839.12, base loss: 15976.36
[INFO 2017-06-30 05:11:42,682 main.py:52] epoch 4680, training loss: 5480.71, average training loss: 5838.57, base loss: 15975.93
[INFO 2017-06-30 05:11:45,848 main.py:52] epoch 4681, training loss: 6331.57, average training loss: 5839.22, base loss: 15976.65
[INFO 2017-06-30 05:11:48,979 main.py:52] epoch 4682, training loss: 5706.15, average training loss: 5839.23, base loss: 15976.54
[INFO 2017-06-30 05:11:52,158 main.py:52] epoch 4683, training loss: 5753.55, average training loss: 5838.57, base loss: 15976.92
[INFO 2017-06-30 05:11:55,337 main.py:52] epoch 4684, training loss: 5586.38, average training loss: 5838.02, base loss: 15976.86
[INFO 2017-06-30 05:11:58,494 main.py:52] epoch 4685, training loss: 5600.69, average training loss: 5837.35, base loss: 15976.92
[INFO 2017-06-30 05:12:01,653 main.py:52] epoch 4686, training loss: 6227.30, average training loss: 5837.35, base loss: 15977.49
[INFO 2017-06-30 05:12:04,847 main.py:52] epoch 4687, training loss: 5914.77, average training loss: 5837.54, base loss: 15977.59
[INFO 2017-06-30 05:12:08,003 main.py:52] epoch 4688, training loss: 5466.19, average training loss: 5837.23, base loss: 15977.42
[INFO 2017-06-30 05:12:11,171 main.py:52] epoch 4689, training loss: 6156.20, average training loss: 5837.63, base loss: 15977.59
[INFO 2017-06-30 05:12:14,350 main.py:52] epoch 4690, training loss: 5942.13, average training loss: 5837.88, base loss: 15977.04
[INFO 2017-06-30 05:12:17,535 main.py:52] epoch 4691, training loss: 5521.21, average training loss: 5837.43, base loss: 15976.33
[INFO 2017-06-30 05:12:20,661 main.py:52] epoch 4692, training loss: 6137.45, average training loss: 5838.10, base loss: 15976.44
[INFO 2017-06-30 05:12:23,849 main.py:52] epoch 4693, training loss: 5765.80, average training loss: 5837.62, base loss: 15975.92
[INFO 2017-06-30 05:12:27,053 main.py:52] epoch 4694, training loss: 5161.83, average training loss: 5836.85, base loss: 15974.76
[INFO 2017-06-30 05:12:30,211 main.py:52] epoch 4695, training loss: 5392.60, average training loss: 5836.42, base loss: 15974.32
[INFO 2017-06-30 05:12:33,376 main.py:52] epoch 4696, training loss: 5997.78, average training loss: 5836.63, base loss: 15974.46
[INFO 2017-06-30 05:12:36,550 main.py:52] epoch 4697, training loss: 5769.69, average training loss: 5836.52, base loss: 15974.26
[INFO 2017-06-30 05:12:39,730 main.py:52] epoch 4698, training loss: 5716.66, average training loss: 5836.14, base loss: 15974.24
[INFO 2017-06-30 05:12:42,910 main.py:52] epoch 4699, training loss: 5629.25, average training loss: 5836.09, base loss: 15974.19
[INFO 2017-06-30 05:12:42,910 main.py:54] epoch 4699, testing
[INFO 2017-06-30 05:12:56,295 main.py:97] average testing loss: 5732.74, base loss: 15976.67
[INFO 2017-06-30 05:12:56,295 main.py:98] improve_loss: 10243.93, improve_percent: 0.64
[INFO 2017-06-30 05:12:56,297 main.py:66] current best improved percent: 0.64
[INFO 2017-06-30 05:12:59,467 main.py:52] epoch 4700, training loss: 6402.22, average training loss: 5836.35, base loss: 15974.66
[INFO 2017-06-30 05:13:02,626 main.py:52] epoch 4701, training loss: 5162.73, average training loss: 5835.85, base loss: 15973.87
[INFO 2017-06-30 05:13:05,793 main.py:52] epoch 4702, training loss: 5502.96, average training loss: 5835.12, base loss: 15973.81
[INFO 2017-06-30 05:13:08,966 main.py:52] epoch 4703, training loss: 5961.74, average training loss: 5835.47, base loss: 15974.09
[INFO 2017-06-30 05:13:12,100 main.py:52] epoch 4704, training loss: 6062.11, average training loss: 5835.79, base loss: 15974.36
[INFO 2017-06-30 05:13:15,259 main.py:52] epoch 4705, training loss: 5864.53, average training loss: 5835.29, base loss: 15974.37
[INFO 2017-06-30 05:13:18,437 main.py:52] epoch 4706, training loss: 5527.54, average training loss: 5834.66, base loss: 15974.08
[INFO 2017-06-30 05:13:21,584 main.py:52] epoch 4707, training loss: 6052.65, average training loss: 5835.09, base loss: 15974.01
[INFO 2017-06-30 05:13:24,759 main.py:52] epoch 4708, training loss: 5750.22, average training loss: 5834.96, base loss: 15973.46
[INFO 2017-06-30 05:13:27,920 main.py:52] epoch 4709, training loss: 6012.56, average training loss: 5835.36, base loss: 15973.37
[INFO 2017-06-30 05:13:31,091 main.py:52] epoch 4710, training loss: 5874.51, average training loss: 5835.26, base loss: 15973.87
[INFO 2017-06-30 05:13:34,276 main.py:52] epoch 4711, training loss: 5874.37, average training loss: 5835.67, base loss: 15974.47
[INFO 2017-06-30 05:13:37,446 main.py:52] epoch 4712, training loss: 6078.37, average training loss: 5835.65, base loss: 15973.96
[INFO 2017-06-30 05:13:40,647 main.py:52] epoch 4713, training loss: 5742.71, average training loss: 5835.79, base loss: 15973.89
[INFO 2017-06-30 05:13:43,859 main.py:52] epoch 4714, training loss: 5513.42, average training loss: 5835.21, base loss: 15973.44
[INFO 2017-06-30 05:13:47,018 main.py:52] epoch 4715, training loss: 5832.69, average training loss: 5835.72, base loss: 15973.44
[INFO 2017-06-30 05:13:50,186 main.py:52] epoch 4716, training loss: 5301.91, average training loss: 5834.62, base loss: 15972.99
[INFO 2017-06-30 05:13:53,332 main.py:52] epoch 4717, training loss: 5302.51, average training loss: 5833.92, base loss: 15972.09
[INFO 2017-06-30 05:13:56,528 main.py:52] epoch 4718, training loss: 5704.32, average training loss: 5833.47, base loss: 15971.88
[INFO 2017-06-30 05:13:59,717 main.py:52] epoch 4719, training loss: 5813.22, average training loss: 5833.33, base loss: 15972.23
[INFO 2017-06-30 05:14:02,902 main.py:52] epoch 4720, training loss: 6061.11, average training loss: 5833.74, base loss: 15972.09
[INFO 2017-06-30 05:14:06,025 main.py:52] epoch 4721, training loss: 5902.29, average training loss: 5833.65, base loss: 15972.26
[INFO 2017-06-30 05:14:09,213 main.py:52] epoch 4722, training loss: 5353.94, average training loss: 5833.00, base loss: 15971.23
[INFO 2017-06-30 05:14:12,403 main.py:52] epoch 4723, training loss: 5678.23, average training loss: 5832.86, base loss: 15971.41
[INFO 2017-06-30 05:14:15,580 main.py:52] epoch 4724, training loss: 5444.11, average training loss: 5832.46, base loss: 15971.18
[INFO 2017-06-30 05:14:18,757 main.py:52] epoch 4725, training loss: 5785.16, average training loss: 5832.22, base loss: 15971.63
[INFO 2017-06-30 05:14:21,946 main.py:52] epoch 4726, training loss: 5863.79, average training loss: 5832.65, base loss: 15971.99
[INFO 2017-06-30 05:14:25,133 main.py:52] epoch 4727, training loss: 5723.75, average training loss: 5832.84, base loss: 15971.93
[INFO 2017-06-30 05:14:28,306 main.py:52] epoch 4728, training loss: 5386.63, average training loss: 5832.18, base loss: 15971.52
[INFO 2017-06-30 05:14:31,508 main.py:52] epoch 4729, training loss: 5836.52, average training loss: 5831.75, base loss: 15971.40
[INFO 2017-06-30 05:14:34,703 main.py:52] epoch 4730, training loss: 5641.03, average training loss: 5831.20, base loss: 15971.12
[INFO 2017-06-30 05:14:37,898 main.py:52] epoch 4731, training loss: 5474.11, average training loss: 5830.77, base loss: 15970.56
[INFO 2017-06-30 05:14:41,094 main.py:52] epoch 4732, training loss: 5391.32, average training loss: 5830.60, base loss: 15970.02
[INFO 2017-06-30 05:14:44,265 main.py:52] epoch 4733, training loss: 5845.02, average training loss: 5830.52, base loss: 15970.00
[INFO 2017-06-30 05:14:47,471 main.py:52] epoch 4734, training loss: 5431.51, average training loss: 5829.79, base loss: 15969.44
[INFO 2017-06-30 05:14:50,702 main.py:52] epoch 4735, training loss: 5841.03, average training loss: 5829.73, base loss: 15969.70
[INFO 2017-06-30 05:14:53,881 main.py:52] epoch 4736, training loss: 5873.44, average training loss: 5829.93, base loss: 15969.70
[INFO 2017-06-30 05:14:57,042 main.py:52] epoch 4737, training loss: 5721.70, average training loss: 5829.72, base loss: 15969.86
[INFO 2017-06-30 05:15:00,290 main.py:52] epoch 4738, training loss: 5163.65, average training loss: 5829.26, base loss: 15968.70
[INFO 2017-06-30 05:15:03,505 main.py:52] epoch 4739, training loss: 5585.41, average training loss: 5828.82, base loss: 15968.12
[INFO 2017-06-30 05:15:06,737 main.py:52] epoch 4740, training loss: 5584.87, average training loss: 5828.53, base loss: 15967.79
[INFO 2017-06-30 05:15:09,876 main.py:52] epoch 4741, training loss: 5711.75, average training loss: 5828.32, base loss: 15967.22
[INFO 2017-06-30 05:15:13,062 main.py:52] epoch 4742, training loss: 6010.94, average training loss: 5828.30, base loss: 15967.16
[INFO 2017-06-30 05:15:16,241 main.py:52] epoch 4743, training loss: 5729.21, average training loss: 5828.00, base loss: 15967.31
[INFO 2017-06-30 05:15:19,415 main.py:52] epoch 4744, training loss: 5434.90, average training loss: 5827.15, base loss: 15967.20
[INFO 2017-06-30 05:15:22,581 main.py:52] epoch 4745, training loss: 5525.98, average training loss: 5826.46, base loss: 15966.55
[INFO 2017-06-30 05:15:25,774 main.py:52] epoch 4746, training loss: 5657.47, average training loss: 5826.11, base loss: 15966.08
[INFO 2017-06-30 05:15:28,919 main.py:52] epoch 4747, training loss: 5740.71, average training loss: 5826.49, base loss: 15965.81
[INFO 2017-06-30 05:15:32,104 main.py:52] epoch 4748, training loss: 5375.49, average training loss: 5825.36, base loss: 15965.77
[INFO 2017-06-30 05:15:35,260 main.py:52] epoch 4749, training loss: 5705.60, average training loss: 5825.01, base loss: 15965.69
[INFO 2017-06-30 05:15:38,489 main.py:52] epoch 4750, training loss: 5534.39, average training loss: 5824.29, base loss: 15965.84
[INFO 2017-06-30 05:15:41,665 main.py:52] epoch 4751, training loss: 5448.18, average training loss: 5824.16, base loss: 15965.74
[INFO 2017-06-30 05:15:44,878 main.py:52] epoch 4752, training loss: 5628.86, average training loss: 5824.03, base loss: 15965.43
[INFO 2017-06-30 05:15:48,069 main.py:52] epoch 4753, training loss: 6207.90, average training loss: 5824.75, base loss: 15966.44
[INFO 2017-06-30 05:15:51,276 main.py:52] epoch 4754, training loss: 5553.16, average training loss: 5823.97, base loss: 15965.82
[INFO 2017-06-30 05:15:54,462 main.py:52] epoch 4755, training loss: 5769.89, average training loss: 5823.94, base loss: 15965.62
[INFO 2017-06-30 05:15:57,577 main.py:52] epoch 4756, training loss: 5669.99, average training loss: 5823.65, base loss: 15965.54
[INFO 2017-06-30 05:16:00,789 main.py:52] epoch 4757, training loss: 5459.08, average training loss: 5822.51, base loss: 15965.25
[INFO 2017-06-30 05:16:04,002 main.py:52] epoch 4758, training loss: 5507.75, average training loss: 5822.06, base loss: 15965.00
[INFO 2017-06-30 05:16:07,161 main.py:52] epoch 4759, training loss: 6228.49, average training loss: 5822.33, base loss: 15965.81
[INFO 2017-06-30 05:16:10,349 main.py:52] epoch 4760, training loss: 5648.17, average training loss: 5821.84, base loss: 15966.23
[INFO 2017-06-30 05:16:13,514 main.py:52] epoch 4761, training loss: 5667.64, average training loss: 5821.66, base loss: 15965.90
[INFO 2017-06-30 05:16:16,699 main.py:52] epoch 4762, training loss: 6070.67, average training loss: 5822.17, base loss: 15966.50
[INFO 2017-06-30 05:16:19,880 main.py:52] epoch 4763, training loss: 5603.10, average training loss: 5821.81, base loss: 15966.61
[INFO 2017-06-30 05:16:23,034 main.py:52] epoch 4764, training loss: 5738.27, average training loss: 5821.90, base loss: 15966.67
[INFO 2017-06-30 05:16:26,200 main.py:52] epoch 4765, training loss: 5691.64, average training loss: 5821.54, base loss: 15966.49
[INFO 2017-06-30 05:16:29,348 main.py:52] epoch 4766, training loss: 5660.06, average training loss: 5821.80, base loss: 15966.40
[INFO 2017-06-30 05:16:32,505 main.py:52] epoch 4767, training loss: 5580.03, average training loss: 5821.54, base loss: 15966.51
[INFO 2017-06-30 05:16:35,692 main.py:52] epoch 4768, training loss: 5511.96, average training loss: 5821.15, base loss: 15965.91
[INFO 2017-06-30 05:16:38,884 main.py:52] epoch 4769, training loss: 5663.03, average training loss: 5820.71, base loss: 15965.56
[INFO 2017-06-30 05:16:42,089 main.py:52] epoch 4770, training loss: 5711.38, average training loss: 5820.73, base loss: 15965.21
[INFO 2017-06-30 05:16:45,270 main.py:52] epoch 4771, training loss: 6039.37, average training loss: 5820.98, base loss: 15965.53
[INFO 2017-06-30 05:16:48,422 main.py:52] epoch 4772, training loss: 5810.07, average training loss: 5821.12, base loss: 15965.30
[INFO 2017-06-30 05:16:51,608 main.py:52] epoch 4773, training loss: 5847.69, average training loss: 5820.77, base loss: 15965.03
[INFO 2017-06-30 05:16:54,787 main.py:52] epoch 4774, training loss: 5934.84, average training loss: 5820.41, base loss: 15964.63
[INFO 2017-06-30 05:16:58,025 main.py:52] epoch 4775, training loss: 5624.85, average training loss: 5820.11, base loss: 15964.02
[INFO 2017-06-30 05:17:01,192 main.py:52] epoch 4776, training loss: 5888.73, average training loss: 5819.97, base loss: 15964.08
[INFO 2017-06-30 05:17:04,362 main.py:52] epoch 4777, training loss: 5288.87, average training loss: 5819.06, base loss: 15963.25
[INFO 2017-06-30 05:17:07,479 main.py:52] epoch 4778, training loss: 5792.30, average training loss: 5819.17, base loss: 15963.54
[INFO 2017-06-30 05:17:10,626 main.py:52] epoch 4779, training loss: 5920.64, average training loss: 5819.12, base loss: 15963.94
[INFO 2017-06-30 05:17:13,830 main.py:52] epoch 4780, training loss: 5522.86, average training loss: 5818.99, base loss: 15963.76
[INFO 2017-06-30 05:17:16,997 main.py:52] epoch 4781, training loss: 5987.99, average training loss: 5819.58, base loss: 15964.23
[INFO 2017-06-30 05:17:20,139 main.py:52] epoch 4782, training loss: 5939.76, average training loss: 5819.15, base loss: 15964.50
[INFO 2017-06-30 05:17:23,276 main.py:52] epoch 4783, training loss: 5638.78, average training loss: 5818.38, base loss: 15964.30
[INFO 2017-06-30 05:17:26,447 main.py:52] epoch 4784, training loss: 6029.27, average training loss: 5818.87, base loss: 15965.01
[INFO 2017-06-30 05:17:29,617 main.py:52] epoch 4785, training loss: 5567.00, average training loss: 5818.67, base loss: 15964.93
[INFO 2017-06-30 05:17:32,799 main.py:52] epoch 4786, training loss: 5709.40, average training loss: 5818.57, base loss: 15964.67
[INFO 2017-06-30 05:17:35,986 main.py:52] epoch 4787, training loss: 5458.22, average training loss: 5818.08, base loss: 15964.47
[INFO 2017-06-30 05:17:39,214 main.py:52] epoch 4788, training loss: 6112.82, average training loss: 5818.03, base loss: 15965.28
[INFO 2017-06-30 05:17:42,369 main.py:52] epoch 4789, training loss: 5596.36, average training loss: 5818.01, base loss: 15965.15
[INFO 2017-06-30 05:17:45,535 main.py:52] epoch 4790, training loss: 5467.92, average training loss: 5817.72, base loss: 15965.15
[INFO 2017-06-30 05:17:48,704 main.py:52] epoch 4791, training loss: 5885.75, average training loss: 5817.71, base loss: 15965.37
[INFO 2017-06-30 05:17:51,869 main.py:52] epoch 4792, training loss: 5695.64, average training loss: 5817.71, base loss: 15965.42
[INFO 2017-06-30 05:17:55,015 main.py:52] epoch 4793, training loss: 5312.92, average training loss: 5817.34, base loss: 15965.38
[INFO 2017-06-30 05:17:58,156 main.py:52] epoch 4794, training loss: 5486.09, average training loss: 5817.24, base loss: 15965.36
[INFO 2017-06-30 05:18:01,331 main.py:52] epoch 4795, training loss: 5883.76, average training loss: 5817.44, base loss: 15965.46
[INFO 2017-06-30 05:18:04,475 main.py:52] epoch 4796, training loss: 5723.47, average training loss: 5817.19, base loss: 15964.46
[INFO 2017-06-30 05:18:07,650 main.py:52] epoch 4797, training loss: 5258.31, average training loss: 5816.60, base loss: 15963.79
[INFO 2017-06-30 05:18:10,813 main.py:52] epoch 4798, training loss: 5632.23, average training loss: 5816.14, base loss: 15963.60
[INFO 2017-06-30 05:18:14,024 main.py:52] epoch 4799, training loss: 5407.66, average training loss: 5815.49, base loss: 15963.26
[INFO 2017-06-30 05:18:14,024 main.py:54] epoch 4799, testing
[INFO 2017-06-30 05:18:27,387 main.py:97] average testing loss: 5774.67, base loss: 16335.82
[INFO 2017-06-30 05:18:27,387 main.py:98] improve_loss: 10561.14, improve_percent: 0.65
[INFO 2017-06-30 05:18:27,389 main.py:62] model save to ./model/final.pth
[INFO 2017-06-30 05:18:27,423 main.py:66] current best improved percent: 0.65
[INFO 2017-06-30 05:18:30,571 main.py:52] epoch 4800, training loss: 6035.86, average training loss: 5815.53, base loss: 15964.05
[INFO 2017-06-30 05:18:33,743 main.py:52] epoch 4801, training loss: 6174.98, average training loss: 5815.65, base loss: 15964.93
[INFO 2017-06-30 05:18:36,876 main.py:52] epoch 4802, training loss: 5733.62, average training loss: 5815.26, base loss: 15964.82
[INFO 2017-06-30 05:18:40,052 main.py:52] epoch 4803, training loss: 6010.87, average training loss: 5814.77, base loss: 15965.00
[INFO 2017-06-30 05:18:43,247 main.py:52] epoch 4804, training loss: 5941.10, average training loss: 5815.16, base loss: 15965.91
[INFO 2017-06-30 05:18:46,430 main.py:52] epoch 4805, training loss: 6128.71, average training loss: 5815.57, base loss: 15967.23
[INFO 2017-06-30 05:18:49,614 main.py:52] epoch 4806, training loss: 5492.39, average training loss: 5815.24, base loss: 15967.30
[INFO 2017-06-30 05:18:52,786 main.py:52] epoch 4807, training loss: 5544.90, average training loss: 5814.92, base loss: 15966.98
[INFO 2017-06-30 05:18:55,943 main.py:52] epoch 4808, training loss: 5236.61, average training loss: 5814.26, base loss: 15966.14
[INFO 2017-06-30 05:18:59,164 main.py:52] epoch 4809, training loss: 5795.26, average training loss: 5813.86, base loss: 15966.60
[INFO 2017-06-30 05:19:02,344 main.py:52] epoch 4810, training loss: 5550.33, average training loss: 5812.90, base loss: 15966.62
[INFO 2017-06-30 05:19:05,517 main.py:52] epoch 4811, training loss: 5868.43, average training loss: 5812.79, base loss: 15967.21
[INFO 2017-06-30 05:19:08,686 main.py:52] epoch 4812, training loss: 5551.69, average training loss: 5812.61, base loss: 15967.03
[INFO 2017-06-30 05:19:11,862 main.py:52] epoch 4813, training loss: 5926.64, average training loss: 5812.88, base loss: 15967.08
[INFO 2017-06-30 05:19:15,039 main.py:52] epoch 4814, training loss: 5731.26, average training loss: 5812.91, base loss: 15966.85
[INFO 2017-06-30 05:19:18,215 main.py:52] epoch 4815, training loss: 5579.65, average training loss: 5812.81, base loss: 15966.71
[INFO 2017-06-30 05:19:21,388 main.py:52] epoch 4816, training loss: 5720.33, average training loss: 5812.59, base loss: 15967.14
[INFO 2017-06-30 05:19:24,580 main.py:52] epoch 4817, training loss: 6237.25, average training loss: 5812.52, base loss: 15967.80
[INFO 2017-06-30 05:19:27,764 main.py:52] epoch 4818, training loss: 5673.68, average training loss: 5812.76, base loss: 15967.78
[INFO 2017-06-30 05:19:30,940 main.py:52] epoch 4819, training loss: 5846.84, average training loss: 5812.56, base loss: 15968.28
[INFO 2017-06-30 05:19:34,106 main.py:52] epoch 4820, training loss: 5521.69, average training loss: 5812.00, base loss: 15967.74
[INFO 2017-06-30 05:19:37,273 main.py:52] epoch 4821, training loss: 5619.21, average training loss: 5812.05, base loss: 15967.60
[INFO 2017-06-30 05:19:40,476 main.py:52] epoch 4822, training loss: 5902.43, average training loss: 5811.51, base loss: 15967.60
[INFO 2017-06-30 05:19:43,661 main.py:52] epoch 4823, training loss: 5690.66, average training loss: 5811.54, base loss: 15966.87
[INFO 2017-06-30 05:19:46,809 main.py:52] epoch 4824, training loss: 6025.99, average training loss: 5811.60, base loss: 15967.08
[INFO 2017-06-30 05:19:49,987 main.py:52] epoch 4825, training loss: 5948.35, average training loss: 5811.97, base loss: 15967.36
[INFO 2017-06-30 05:19:53,177 main.py:52] epoch 4826, training loss: 5710.29, average training loss: 5811.72, base loss: 15968.01
[INFO 2017-06-30 05:19:56,398 main.py:52] epoch 4827, training loss: 5872.84, average training loss: 5811.51, base loss: 15968.74
[INFO 2017-06-30 05:19:59,585 main.py:52] epoch 4828, training loss: 5950.15, average training loss: 5811.27, base loss: 15968.75
[INFO 2017-06-30 05:20:02,708 main.py:52] epoch 4829, training loss: 5720.79, average training loss: 5811.02, base loss: 15969.23
[INFO 2017-06-30 05:20:05,878 main.py:52] epoch 4830, training loss: 5751.65, average training loss: 5810.40, base loss: 15969.19
[INFO 2017-06-30 05:20:09,045 main.py:52] epoch 4831, training loss: 5871.74, average training loss: 5809.99, base loss: 15969.86
[INFO 2017-06-30 05:20:12,237 main.py:52] epoch 4832, training loss: 5401.33, average training loss: 5809.83, base loss: 15969.86
[INFO 2017-06-30 05:20:15,468 main.py:52] epoch 4833, training loss: 6046.78, average training loss: 5810.11, base loss: 15970.58
[INFO 2017-06-30 05:20:18,645 main.py:52] epoch 4834, training loss: 5411.74, average training loss: 5809.34, base loss: 15970.68
[INFO 2017-06-30 05:20:21,775 main.py:52] epoch 4835, training loss: 5688.09, average training loss: 5809.09, base loss: 15970.87
[INFO 2017-06-30 05:20:24,981 main.py:52] epoch 4836, training loss: 5630.25, average training loss: 5808.63, base loss: 15970.81
[INFO 2017-06-30 05:20:28,156 main.py:52] epoch 4837, training loss: 5787.24, average training loss: 5808.56, base loss: 15971.53
[INFO 2017-06-30 05:20:31,267 main.py:52] epoch 4838, training loss: 5814.05, average training loss: 5808.53, base loss: 15971.70
[INFO 2017-06-30 05:20:34,463 main.py:52] epoch 4839, training loss: 5720.35, average training loss: 5808.46, base loss: 15971.15
[INFO 2017-06-30 05:20:37,613 main.py:52] epoch 4840, training loss: 5439.50, average training loss: 5807.97, base loss: 15970.85
[INFO 2017-06-30 05:20:40,799 main.py:52] epoch 4841, training loss: 6152.87, average training loss: 5808.55, base loss: 15971.33
[INFO 2017-06-30 05:20:43,982 main.py:52] epoch 4842, training loss: 5436.58, average training loss: 5807.84, base loss: 15970.93
[INFO 2017-06-30 05:20:47,137 main.py:52] epoch 4843, training loss: 5704.72, average training loss: 5807.88, base loss: 15970.39
[INFO 2017-06-30 05:20:50,315 main.py:52] epoch 4844, training loss: 6060.19, average training loss: 5808.23, base loss: 15970.60
[INFO 2017-06-30 05:20:53,534 main.py:52] epoch 4845, training loss: 5863.51, average training loss: 5807.79, base loss: 15970.40
[INFO 2017-06-30 05:20:56,687 main.py:52] epoch 4846, training loss: 5498.04, average training loss: 5807.40, base loss: 15970.14
[INFO 2017-06-30 05:20:59,828 main.py:52] epoch 4847, training loss: 5063.76, average training loss: 5806.79, base loss: 15969.29
[INFO 2017-06-30 05:21:03,000 main.py:52] epoch 4848, training loss: 5780.92, average training loss: 5806.95, base loss: 15969.19
[INFO 2017-06-30 05:21:06,137 main.py:52] epoch 4849, training loss: 5649.47, average training loss: 5806.94, base loss: 15968.46
[INFO 2017-06-30 05:21:09,294 main.py:52] epoch 4850, training loss: 5590.02, average training loss: 5806.65, base loss: 15968.27
[INFO 2017-06-30 05:21:12,469 main.py:52] epoch 4851, training loss: 5886.82, average training loss: 5806.71, base loss: 15968.66
[INFO 2017-06-30 05:21:15,628 main.py:52] epoch 4852, training loss: 5888.27, average training loss: 5806.97, base loss: 15969.42
[INFO 2017-06-30 05:21:18,849 main.py:52] epoch 4853, training loss: 5773.82, average training loss: 5806.76, base loss: 15969.88
[INFO 2017-06-30 05:21:21,997 main.py:52] epoch 4854, training loss: 5965.17, average training loss: 5806.86, base loss: 15969.80
[INFO 2017-06-30 05:21:25,167 main.py:52] epoch 4855, training loss: 5655.83, average training loss: 5806.46, base loss: 15970.02
[INFO 2017-06-30 05:21:28,336 main.py:52] epoch 4856, training loss: 6185.06, average training loss: 5806.47, base loss: 15970.63
[INFO 2017-06-30 05:21:31,473 main.py:52] epoch 4857, training loss: 5563.20, average training loss: 5806.13, base loss: 15970.45
[INFO 2017-06-30 05:21:34,636 main.py:52] epoch 4858, training loss: 6067.02, average training loss: 5806.41, base loss: 15971.08
[INFO 2017-06-30 05:21:37,829 main.py:52] epoch 4859, training loss: 5550.31, average training loss: 5806.13, base loss: 15971.44
[INFO 2017-06-30 05:21:41,012 main.py:52] epoch 4860, training loss: 5924.89, average training loss: 5806.24, base loss: 15971.17
[INFO 2017-06-30 05:21:44,166 main.py:52] epoch 4861, training loss: 5527.42, average training loss: 5805.87, base loss: 15970.90
[INFO 2017-06-30 05:21:47,368 main.py:52] epoch 4862, training loss: 5960.47, average training loss: 5806.23, base loss: 15971.20
[INFO 2017-06-30 05:21:50,533 main.py:52] epoch 4863, training loss: 5430.78, average training loss: 5805.63, base loss: 15971.16
[INFO 2017-06-30 05:21:53,726 main.py:52] epoch 4864, training loss: 5875.87, average training loss: 5805.81, base loss: 15970.77
[INFO 2017-06-30 05:21:56,881 main.py:52] epoch 4865, training loss: 5453.54, average training loss: 5804.93, base loss: 15970.10
[INFO 2017-06-30 05:22:00,045 main.py:52] epoch 4866, training loss: 5613.93, average training loss: 5804.48, base loss: 15969.56
[INFO 2017-06-30 05:22:03,219 main.py:52] epoch 4867, training loss: 5575.75, average training loss: 5804.05, base loss: 15969.22
[INFO 2017-06-30 05:22:06,393 main.py:52] epoch 4868, training loss: 5497.30, average training loss: 5803.66, base loss: 15968.62
[INFO 2017-06-30 05:22:09,581 main.py:52] epoch 4869, training loss: 5535.70, average training loss: 5803.83, base loss: 15968.50
[INFO 2017-06-30 05:22:12,761 main.py:52] epoch 4870, training loss: 5709.77, average training loss: 5803.81, base loss: 15968.31
[INFO 2017-06-30 05:22:15,943 main.py:52] epoch 4871, training loss: 5501.20, average training loss: 5803.68, base loss: 15968.06
[INFO 2017-06-30 05:22:19,087 main.py:52] epoch 4872, training loss: 5302.65, average training loss: 5802.57, base loss: 15967.69
[INFO 2017-06-30 05:22:22,246 main.py:52] epoch 4873, training loss: 5633.06, average training loss: 5802.21, base loss: 15967.30
[INFO 2017-06-30 05:22:25,396 main.py:52] epoch 4874, training loss: 5983.94, average training loss: 5802.52, base loss: 15967.47
[INFO 2017-06-30 05:22:28,575 main.py:52] epoch 4875, training loss: 5802.80, average training loss: 5802.32, base loss: 15967.08
[INFO 2017-06-30 05:22:31,703 main.py:52] epoch 4876, training loss: 5793.85, average training loss: 5802.29, base loss: 15967.17
[INFO 2017-06-30 05:22:34,896 main.py:52] epoch 4877, training loss: 6001.72, average training loss: 5802.43, base loss: 15967.71
[INFO 2017-06-30 05:22:38,082 main.py:52] epoch 4878, training loss: 5384.82, average training loss: 5802.78, base loss: 15967.21
[INFO 2017-06-30 05:22:41,287 main.py:52] epoch 4879, training loss: 5882.92, average training loss: 5802.91, base loss: 15966.88
[INFO 2017-06-30 05:22:44,453 main.py:52] epoch 4880, training loss: 5517.93, average training loss: 5802.41, base loss: 15966.18
[INFO 2017-06-30 05:22:47,653 main.py:52] epoch 4881, training loss: 5360.56, average training loss: 5802.30, base loss: 15965.69
[INFO 2017-06-30 05:22:50,809 main.py:52] epoch 4882, training loss: 6105.22, average training loss: 5801.42, base loss: 15966.57
[INFO 2017-06-30 05:22:53,952 main.py:52] epoch 4883, training loss: 5928.68, average training loss: 5801.48, base loss: 15966.83
[INFO 2017-06-30 05:22:57,079 main.py:52] epoch 4884, training loss: 5891.12, average training loss: 5801.77, base loss: 15967.22
[INFO 2017-06-30 05:23:00,219 main.py:52] epoch 4885, training loss: 5470.58, average training loss: 5801.17, base loss: 15967.01
[INFO 2017-06-30 05:23:03,431 main.py:52] epoch 4886, training loss: 5505.68, average training loss: 5801.09, base loss: 15966.83
[INFO 2017-06-30 05:23:06,577 main.py:52] epoch 4887, training loss: 5596.25, average training loss: 5800.59, base loss: 15966.57
[INFO 2017-06-30 05:23:09,717 main.py:52] epoch 4888, training loss: 5874.87, average training loss: 5800.60, base loss: 15966.57
[INFO 2017-06-30 05:23:12,871 main.py:52] epoch 4889, training loss: 5491.85, average training loss: 5800.08, base loss: 15966.44
[INFO 2017-06-30 05:23:16,060 main.py:52] epoch 4890, training loss: 5885.08, average training loss: 5800.16, base loss: 15966.42
[INFO 2017-06-30 05:23:19,229 main.py:52] epoch 4891, training loss: 5761.82, average training loss: 5800.19, base loss: 15966.74
[INFO 2017-06-30 05:23:22,426 main.py:52] epoch 4892, training loss: 5455.22, average training loss: 5799.75, base loss: 15966.61
[INFO 2017-06-30 05:23:25,592 main.py:52] epoch 4893, training loss: 5795.26, average training loss: 5799.84, base loss: 15966.90
[INFO 2017-06-30 05:23:28,856 main.py:52] epoch 4894, training loss: 5684.91, average training loss: 5799.51, base loss: 15967.08
[INFO 2017-06-30 05:23:32,006 main.py:52] epoch 4895, training loss: 5867.08, average training loss: 5799.84, base loss: 15967.58
[INFO 2017-06-30 05:23:35,167 main.py:52] epoch 4896, training loss: 5246.56, average training loss: 5798.58, base loss: 15967.43
[INFO 2017-06-30 05:23:38,371 main.py:52] epoch 4897, training loss: 5462.92, average training loss: 5798.10, base loss: 15967.09
[INFO 2017-06-30 05:23:41,612 main.py:52] epoch 4898, training loss: 5775.25, average training loss: 5797.79, base loss: 15966.77
[INFO 2017-06-30 05:23:44,761 main.py:52] epoch 4899, training loss: 5499.21, average training loss: 5797.78, base loss: 15966.04
[INFO 2017-06-30 05:23:44,762 main.py:54] epoch 4899, testing
[INFO 2017-06-30 05:23:58,076 main.py:97] average testing loss: 5735.82, base loss: 16135.89
[INFO 2017-06-30 05:23:58,077 main.py:98] improve_loss: 10400.07, improve_percent: 0.64
[INFO 2017-06-30 05:23:58,078 main.py:66] current best improved percent: 0.65
[INFO 2017-06-30 05:24:01,276 main.py:52] epoch 4900, training loss: 5594.59, average training loss: 5797.62, base loss: 15965.85
[INFO 2017-06-30 05:24:04,484 main.py:52] epoch 4901, training loss: 5342.92, average training loss: 5796.78, base loss: 15965.38
[INFO 2017-06-30 05:24:07,657 main.py:52] epoch 4902, training loss: 5520.67, average training loss: 5796.40, base loss: 15965.07
[INFO 2017-06-30 05:24:10,830 main.py:52] epoch 4903, training loss: 5274.45, average training loss: 5795.75, base loss: 15964.26
[INFO 2017-06-30 05:24:14,007 main.py:52] epoch 4904, training loss: 5559.70, average training loss: 5795.33, base loss: 15964.04
[INFO 2017-06-30 05:24:17,153 main.py:52] epoch 4905, training loss: 5498.83, average training loss: 5794.86, base loss: 15963.38
[INFO 2017-06-30 05:24:20,336 main.py:52] epoch 4906, training loss: 5488.41, average training loss: 5794.44, base loss: 15963.48
[INFO 2017-06-30 05:24:23,577 main.py:52] epoch 4907, training loss: 5751.52, average training loss: 5794.38, base loss: 15964.19
[INFO 2017-06-30 05:24:26,792 main.py:52] epoch 4908, training loss: 5448.67, average training loss: 5793.77, base loss: 15963.60
[INFO 2017-06-30 05:24:29,950 main.py:52] epoch 4909, training loss: 5615.52, average training loss: 5793.19, base loss: 15963.64
[INFO 2017-06-30 05:24:33,126 main.py:52] epoch 4910, training loss: 5402.49, average training loss: 5792.75, base loss: 15962.72
[INFO 2017-06-30 05:24:36,268 main.py:52] epoch 4911, training loss: 5706.88, average training loss: 5792.40, base loss: 15962.58
[INFO 2017-06-30 05:24:39,479 main.py:52] epoch 4912, training loss: 5838.12, average training loss: 5792.31, base loss: 15962.67
[INFO 2017-06-30 05:24:42,647 main.py:52] epoch 4913, training loss: 5743.13, average training loss: 5792.13, base loss: 15962.28
[INFO 2017-06-30 05:24:45,775 main.py:52] epoch 4914, training loss: 6097.81, average training loss: 5792.17, base loss: 15962.26
[INFO 2017-06-30 05:24:48,920 main.py:52] epoch 4915, training loss: 5420.92, average training loss: 5791.53, base loss: 15961.92
[INFO 2017-06-30 05:24:52,073 main.py:52] epoch 4916, training loss: 5163.32, average training loss: 5790.52, base loss: 15961.27
[INFO 2017-06-30 05:24:55,257 main.py:52] epoch 4917, training loss: 5524.41, average training loss: 5789.63, base loss: 15960.71
[INFO 2017-06-30 05:24:58,422 main.py:52] epoch 4918, training loss: 5658.27, average training loss: 5789.65, base loss: 15960.69
[INFO 2017-06-30 05:25:01,595 main.py:52] epoch 4919, training loss: 5434.89, average training loss: 5789.24, base loss: 15960.73
[INFO 2017-06-30 05:25:04,802 main.py:52] epoch 4920, training loss: 5651.87, average training loss: 5789.00, base loss: 15960.42
[INFO 2017-06-30 05:25:07,995 main.py:52] epoch 4921, training loss: 6193.42, average training loss: 5788.89, base loss: 15960.71
[INFO 2017-06-30 05:25:11,156 main.py:52] epoch 4922, training loss: 5540.07, average training loss: 5788.56, base loss: 15960.40
[INFO 2017-06-30 05:25:14,310 main.py:52] epoch 4923, training loss: 6106.79, average training loss: 5788.76, base loss: 15961.17
[INFO 2017-06-30 05:25:17,471 main.py:52] epoch 4924, training loss: 6035.22, average training loss: 5788.82, base loss: 15961.52
[INFO 2017-06-30 05:25:20,697 main.py:52] epoch 4925, training loss: 5413.99, average training loss: 5788.54, base loss: 15961.37
[INFO 2017-06-30 05:25:23,889 main.py:52] epoch 4926, training loss: 5856.54, average training loss: 5788.75, base loss: 15961.36
[INFO 2017-06-30 05:25:27,057 main.py:52] epoch 4927, training loss: 6391.51, average training loss: 5789.47, base loss: 15961.65
[INFO 2017-06-30 05:25:30,240 main.py:52] epoch 4928, training loss: 5505.15, average training loss: 5788.71, base loss: 15961.07
[INFO 2017-06-30 05:25:33,412 main.py:52] epoch 4929, training loss: 5863.17, average training loss: 5788.73, base loss: 15960.75
[INFO 2017-06-30 05:25:36,545 main.py:52] epoch 4930, training loss: 5730.08, average training loss: 5788.84, base loss: 15960.75
[INFO 2017-06-30 05:25:39,674 main.py:52] epoch 4931, training loss: 6390.01, average training loss: 5789.49, base loss: 15961.15
[INFO 2017-06-30 05:25:42,837 main.py:52] epoch 4932, training loss: 5374.50, average training loss: 5789.06, base loss: 15960.21
[INFO 2017-06-30 05:25:45,998 main.py:52] epoch 4933, training loss: 5659.93, average training loss: 5788.85, base loss: 15960.21
[INFO 2017-06-30 05:25:49,174 main.py:52] epoch 4934, training loss: 6692.76, average training loss: 5789.52, base loss: 15961.26
[INFO 2017-06-30 05:25:52,336 main.py:52] epoch 4935, training loss: 5585.64, average training loss: 5789.02, base loss: 15960.97
[INFO 2017-06-30 05:25:55,435 main.py:52] epoch 4936, training loss: 5792.60, average training loss: 5788.89, base loss: 15961.08
[INFO 2017-06-30 05:25:58,622 main.py:52] epoch 4937, training loss: 5587.35, average training loss: 5788.50, base loss: 15961.06
[INFO 2017-06-30 05:26:01,782 main.py:52] epoch 4938, training loss: 5416.63, average training loss: 5788.17, base loss: 15961.04
[INFO 2017-06-30 05:26:04,952 main.py:52] epoch 4939, training loss: 5575.20, average training loss: 5787.72, base loss: 15960.92
[INFO 2017-06-30 05:26:08,145 main.py:52] epoch 4940, training loss: 5742.65, average training loss: 5787.94, base loss: 15960.48
[INFO 2017-06-30 05:26:11,286 main.py:52] epoch 4941, training loss: 5497.34, average training loss: 5787.70, base loss: 15959.87
[INFO 2017-06-30 05:26:14,425 main.py:52] epoch 4942, training loss: 5394.78, average training loss: 5787.20, base loss: 15959.57
[INFO 2017-06-30 05:26:17,625 main.py:52] epoch 4943, training loss: 5408.61, average training loss: 5787.13, base loss: 15959.23
[INFO 2017-06-30 05:26:20,793 main.py:52] epoch 4944, training loss: 6332.57, average training loss: 5787.33, base loss: 15960.66
[INFO 2017-06-30 05:26:23,972 main.py:52] epoch 4945, training loss: 5632.11, average training loss: 5787.02, base loss: 15960.84
[INFO 2017-06-30 05:26:27,116 main.py:52] epoch 4946, training loss: 5402.54, average training loss: 5786.20, base loss: 15960.69
[INFO 2017-06-30 05:26:30,260 main.py:52] epoch 4947, training loss: 5699.64, average training loss: 5785.90, base loss: 15960.88
[INFO 2017-06-30 05:26:33,467 main.py:52] epoch 4948, training loss: 5934.33, average training loss: 5785.72, base loss: 15960.52
[INFO 2017-06-30 05:26:36,658 main.py:52] epoch 4949, training loss: 5541.75, average training loss: 5785.59, base loss: 15959.86
[INFO 2017-06-30 05:26:39,787 main.py:52] epoch 4950, training loss: 5410.10, average training loss: 5784.73, base loss: 15959.43
[INFO 2017-06-30 05:26:42,948 main.py:52] epoch 4951, training loss: 5796.39, average training loss: 5784.78, base loss: 15959.76
[INFO 2017-06-30 05:26:46,101 main.py:52] epoch 4952, training loss: 5678.46, average training loss: 5784.55, base loss: 15959.58
[INFO 2017-06-30 05:26:49,250 main.py:52] epoch 4953, training loss: 5855.77, average training loss: 5784.36, base loss: 15960.00
[INFO 2017-06-30 05:26:52,405 main.py:52] epoch 4954, training loss: 5456.06, average training loss: 5783.74, base loss: 15960.02
[INFO 2017-06-30 05:26:55,602 main.py:52] epoch 4955, training loss: 6062.91, average training loss: 5783.84, base loss: 15960.65
[INFO 2017-06-30 05:26:58,775 main.py:52] epoch 4956, training loss: 5966.08, average training loss: 5783.97, base loss: 15960.93
[INFO 2017-06-30 05:27:01,937 main.py:52] epoch 4957, training loss: 5770.29, average training loss: 5784.11, base loss: 15960.74
[INFO 2017-06-30 05:27:05,100 main.py:52] epoch 4958, training loss: 5447.84, average training loss: 5783.85, base loss: 15960.66
[INFO 2017-06-30 05:27:08,351 main.py:52] epoch 4959, training loss: 5971.72, average training loss: 5784.38, base loss: 15961.09
[INFO 2017-06-30 05:27:11,502 main.py:52] epoch 4960, training loss: 6276.46, average training loss: 5785.07, base loss: 15961.43
[INFO 2017-06-30 05:27:14,704 main.py:52] epoch 4961, training loss: 5904.94, average training loss: 5785.26, base loss: 15961.28
[INFO 2017-06-30 05:27:17,879 main.py:52] epoch 4962, training loss: 5741.89, average training loss: 5784.85, base loss: 15960.74
[INFO 2017-06-30 05:27:21,018 main.py:52] epoch 4963, training loss: 5641.44, average training loss: 5784.30, base loss: 15960.13
[INFO 2017-06-30 05:27:24,167 main.py:52] epoch 4964, training loss: 5426.10, average training loss: 5783.76, base loss: 15959.88
[INFO 2017-06-30 05:27:27,335 main.py:52] epoch 4965, training loss: 5890.80, average training loss: 5783.76, base loss: 15960.95
[INFO 2017-06-30 05:27:30,472 main.py:52] epoch 4966, training loss: 5492.07, average training loss: 5783.32, base loss: 15960.06
[INFO 2017-06-30 05:27:33,642 main.py:52] epoch 4967, training loss: 5641.54, average training loss: 5782.69, base loss: 15959.73
[INFO 2017-06-30 05:27:36,812 main.py:52] epoch 4968, training loss: 5691.02, average training loss: 5782.31, base loss: 15959.33
[INFO 2017-06-30 05:27:39,994 main.py:52] epoch 4969, training loss: 5676.01, average training loss: 5782.16, base loss: 15959.38
[INFO 2017-06-30 05:27:43,140 main.py:52] epoch 4970, training loss: 5418.52, average training loss: 5781.53, base loss: 15958.86
[INFO 2017-06-30 05:27:46,323 main.py:52] epoch 4971, training loss: 5560.80, average training loss: 5781.52, base loss: 15958.64
[INFO 2017-06-30 05:27:49,497 main.py:52] epoch 4972, training loss: 6406.04, average training loss: 5782.17, base loss: 15958.88
[INFO 2017-06-30 05:27:52,664 main.py:52] epoch 4973, training loss: 5861.14, average training loss: 5782.34, base loss: 15959.33
[INFO 2017-06-30 05:27:55,816 main.py:52] epoch 4974, training loss: 5726.60, average training loss: 5782.29, base loss: 15959.04
[INFO 2017-06-30 05:27:58,992 main.py:52] epoch 4975, training loss: 5926.40, average training loss: 5782.23, base loss: 15959.25
[INFO 2017-06-30 05:28:02,218 main.py:52] epoch 4976, training loss: 5675.32, average training loss: 5781.93, base loss: 15959.30
[INFO 2017-06-30 05:28:05,366 main.py:52] epoch 4977, training loss: 5809.58, average training loss: 5781.61, base loss: 15959.89
[INFO 2017-06-30 05:28:08,517 main.py:52] epoch 4978, training loss: 5344.89, average training loss: 5781.01, base loss: 15959.07
[INFO 2017-06-30 05:28:11,722 main.py:52] epoch 4979, training loss: 5684.63, average training loss: 5780.66, base loss: 15958.75
[INFO 2017-06-30 05:28:14,879 main.py:52] epoch 4980, training loss: 5848.31, average training loss: 5780.79, base loss: 15958.87
[INFO 2017-06-30 05:28:18,050 main.py:52] epoch 4981, training loss: 5764.90, average training loss: 5780.43, base loss: 15958.92
[INFO 2017-06-30 05:28:21,204 main.py:52] epoch 4982, training loss: 5097.66, average training loss: 5779.63, base loss: 15957.94
[INFO 2017-06-30 05:28:24,324 main.py:52] epoch 4983, training loss: 5211.34, average training loss: 5778.63, base loss: 15957.25
[INFO 2017-06-30 05:28:27,492 main.py:52] epoch 4984, training loss: 5422.26, average training loss: 5778.38, base loss: 15957.10
[INFO 2017-06-30 05:28:30,612 main.py:52] epoch 4985, training loss: 5693.46, average training loss: 5778.35, base loss: 15957.18
[INFO 2017-06-30 05:28:33,756 main.py:52] epoch 4986, training loss: 6145.73, average training loss: 5778.79, base loss: 15958.24
[INFO 2017-06-30 05:28:36,920 main.py:52] epoch 4987, training loss: 5711.12, average training loss: 5778.86, base loss: 15958.21
[INFO 2017-06-30 05:28:40,103 main.py:52] epoch 4988, training loss: 5356.70, average training loss: 5778.49, base loss: 15957.65
[INFO 2017-06-30 05:28:43,277 main.py:52] epoch 4989, training loss: 5842.06, average training loss: 5778.73, base loss: 15958.19
[INFO 2017-06-30 05:28:46,463 main.py:52] epoch 4990, training loss: 5726.14, average training loss: 5778.09, base loss: 15958.42
[INFO 2017-06-30 05:28:49,617 main.py:52] epoch 4991, training loss: 5520.54, average training loss: 5777.58, base loss: 15958.06
[INFO 2017-06-30 05:28:52,816 main.py:52] epoch 4992, training loss: 5542.80, average training loss: 5776.68, base loss: 15957.43
[INFO 2017-06-30 05:28:55,972 main.py:52] epoch 4993, training loss: 5733.06, average training loss: 5776.57, base loss: 15957.39
[INFO 2017-06-30 05:28:59,146 main.py:52] epoch 4994, training loss: 5682.51, average training loss: 5776.20, base loss: 15957.76
[INFO 2017-06-30 05:29:02,321 main.py:52] epoch 4995, training loss: 5764.65, average training loss: 5775.96, base loss: 15957.95
[INFO 2017-06-30 05:29:05,522 main.py:52] epoch 4996, training loss: 5505.65, average training loss: 5775.92, base loss: 15957.57
[INFO 2017-06-30 05:29:08,665 main.py:52] epoch 4997, training loss: 5741.94, average training loss: 5775.93, base loss: 15957.60
[INFO 2017-06-30 05:29:11,829 main.py:52] epoch 4998, training loss: 5577.41, average training loss: 5775.30, base loss: 15957.57
[INFO 2017-06-30 05:29:14,978 main.py:52] epoch 4999, training loss: 5793.80, average training loss: 5774.83, base loss: 15957.91
[INFO 2017-06-30 05:29:14,979 main.py:54] epoch 4999, testing
[INFO 2017-06-30 05:29:28,498 main.py:97] average testing loss: 5781.06, base loss: 16269.32
[INFO 2017-06-30 05:29:28,499 main.py:98] improve_loss: 10488.26, improve_percent: 0.64
[INFO 2017-06-30 05:29:28,502 main.py:66] current best improved percent: 0.65
[INFO 2017-06-30 05:29:31,691 main.py:52] epoch 5000, training loss: 5582.75, average training loss: 5774.63, base loss: 15958.08
[INFO 2017-06-30 05:29:34,876 main.py:52] epoch 5001, training loss: 5599.40, average training loss: 5774.36, base loss: 15957.99
[INFO 2017-06-30 05:29:38,076 main.py:52] epoch 5002, training loss: 5849.27, average training loss: 5774.55, base loss: 15957.47
[INFO 2017-06-30 05:29:41,257 main.py:52] epoch 5003, training loss: 5701.30, average training loss: 5774.40, base loss: 15957.84
[INFO 2017-06-30 05:29:44,531 main.py:52] epoch 5004, training loss: 5565.80, average training loss: 5774.01, base loss: 15957.78
[INFO 2017-06-30 05:29:47,717 main.py:52] epoch 5005, training loss: 5338.28, average training loss: 5773.59, base loss: 15957.31
[INFO 2017-06-30 05:29:50,873 main.py:52] epoch 5006, training loss: 5525.32, average training loss: 5773.61, base loss: 15957.05
[INFO 2017-06-30 05:29:54,022 main.py:52] epoch 5007, training loss: 5501.89, average training loss: 5773.52, base loss: 15957.18
[INFO 2017-06-30 05:29:57,169 main.py:52] epoch 5008, training loss: 5589.02, average training loss: 5773.14, base loss: 15956.44
[INFO 2017-06-30 05:30:00,353 main.py:52] epoch 5009, training loss: 5747.12, average training loss: 5772.81, base loss: 15956.36
[INFO 2017-06-30 05:30:03,504 main.py:52] epoch 5010, training loss: 5210.45, average training loss: 5772.17, base loss: 15956.25
[INFO 2017-06-30 05:30:06,646 main.py:52] epoch 5011, training loss: 5152.33, average training loss: 5771.56, base loss: 15956.44
[INFO 2017-06-30 05:30:09,796 main.py:52] epoch 5012, training loss: 5943.15, average training loss: 5771.88, base loss: 15956.64
[INFO 2017-06-30 05:30:12,939 main.py:52] epoch 5013, training loss: 5451.79, average training loss: 5772.02, base loss: 15956.14
[INFO 2017-06-30 05:30:16,093 main.py:52] epoch 5014, training loss: 5451.73, average training loss: 5771.44, base loss: 15955.83
[INFO 2017-06-30 05:30:19,254 main.py:52] epoch 5015, training loss: 6021.18, average training loss: 5771.80, base loss: 15956.37
[INFO 2017-06-30 05:30:22,441 main.py:52] epoch 5016, training loss: 5664.24, average training loss: 5771.54, base loss: 15956.20
[INFO 2017-06-30 05:30:25,596 main.py:52] epoch 5017, training loss: 5883.06, average training loss: 5771.42, base loss: 15956.40
[INFO 2017-06-30 05:30:28,749 main.py:52] epoch 5018, training loss: 5829.03, average training loss: 5771.69, base loss: 15955.88
[INFO 2017-06-30 05:30:31,972 main.py:52] epoch 5019, training loss: 5634.61, average training loss: 5771.38, base loss: 15956.08
[INFO 2017-06-30 05:30:35,160 main.py:52] epoch 5020, training loss: 5422.12, average training loss: 5770.67, base loss: 15955.45
[INFO 2017-06-30 05:30:38,341 main.py:52] epoch 5021, training loss: 5729.00, average training loss: 5770.50, base loss: 15955.83
[INFO 2017-06-30 05:30:41,536 main.py:52] epoch 5022, training loss: 5889.23, average training loss: 5770.73, base loss: 15955.92
[INFO 2017-06-30 05:30:44,724 main.py:52] epoch 5023, training loss: 6026.13, average training loss: 5771.08, base loss: 15955.80
[INFO 2017-06-30 05:30:47,880 main.py:52] epoch 5024, training loss: 5282.03, average training loss: 5770.98, base loss: 15954.93
[INFO 2017-06-30 05:30:51,062 main.py:52] epoch 5025, training loss: 5585.84, average training loss: 5770.41, base loss: 15955.03
[INFO 2017-06-30 05:30:54,225 main.py:52] epoch 5026, training loss: 5506.35, average training loss: 5769.75, base loss: 15954.45
[INFO 2017-06-30 05:30:57,375 main.py:52] epoch 5027, training loss: 5549.81, average training loss: 5768.75, base loss: 15954.27
[INFO 2017-06-30 05:31:00,514 main.py:52] epoch 5028, training loss: 5822.32, average training loss: 5768.70, base loss: 15954.18
[INFO 2017-06-30 05:31:03,657 main.py:52] epoch 5029, training loss: 5409.83, average training loss: 5768.31, base loss: 15953.59
[INFO 2017-06-30 05:31:06,860 main.py:52] epoch 5030, training loss: 5036.40, average training loss: 5767.16, base loss: 15952.78
[INFO 2017-06-30 05:31:10,008 main.py:52] epoch 5031, training loss: 5967.29, average training loss: 5767.46, base loss: 15952.70
[INFO 2017-06-30 05:31:13,157 main.py:52] epoch 5032, training loss: 5723.22, average training loss: 5767.64, base loss: 15952.45
[INFO 2017-06-30 05:31:16,387 main.py:52] epoch 5033, training loss: 5629.56, average training loss: 5767.18, base loss: 15952.39
[INFO 2017-06-30 05:31:19,571 main.py:52] epoch 5034, training loss: 6142.93, average training loss: 5767.64, base loss: 15953.12
[INFO 2017-06-30 05:31:22,766 main.py:52] epoch 5035, training loss: 5702.85, average training loss: 5767.60, base loss: 15953.42
[INFO 2017-06-30 05:31:25,923 main.py:52] epoch 5036, training loss: 5404.70, average training loss: 5766.98, base loss: 15952.87
[INFO 2017-06-30 05:31:29,084 main.py:52] epoch 5037, training loss: 5303.13, average training loss: 5766.04, base loss: 15952.43
[INFO 2017-06-30 05:31:32,249 main.py:52] epoch 5038, training loss: 5786.01, average training loss: 5766.26, base loss: 15952.04
[INFO 2017-06-30 05:31:35,397 main.py:52] epoch 5039, training loss: 5588.08, average training loss: 5766.48, base loss: 15951.75
[INFO 2017-06-30 05:31:38,550 main.py:52] epoch 5040, training loss: 5614.04, average training loss: 5766.21, base loss: 15951.69
[INFO 2017-06-30 05:31:41,730 main.py:52] epoch 5041, training loss: 5971.84, average training loss: 5766.99, base loss: 15951.89
[INFO 2017-06-30 05:31:44,941 main.py:52] epoch 5042, training loss: 5611.00, average training loss: 5766.91, base loss: 15952.05
[INFO 2017-06-30 05:31:48,123 main.py:52] epoch 5043, training loss: 5467.09, average training loss: 5766.06, base loss: 15952.45
[INFO 2017-06-30 05:31:51,314 main.py:52] epoch 5044, training loss: 6160.98, average training loss: 5766.53, base loss: 15952.83
[INFO 2017-06-30 05:31:54,525 main.py:52] epoch 5045, training loss: 5857.63, average training loss: 5766.32, base loss: 15953.28
[INFO 2017-06-30 05:31:57,707 main.py:52] epoch 5046, training loss: 5282.41, average training loss: 5765.68, base loss: 15952.74
[INFO 2017-06-30 05:32:00,851 main.py:52] epoch 5047, training loss: 5732.13, average training loss: 5765.56, base loss: 15952.85
[INFO 2017-06-30 05:32:04,004 main.py:52] epoch 5048, training loss: 5430.25, average training loss: 5765.23, base loss: 15952.32
[INFO 2017-06-30 05:32:07,153 main.py:52] epoch 5049, training loss: 6334.53, average training loss: 5765.58, base loss: 15952.72
[INFO 2017-06-30 05:32:10,327 main.py:52] epoch 5050, training loss: 5852.69, average training loss: 5765.74, base loss: 15952.90
[INFO 2017-06-30 05:32:13,513 main.py:52] epoch 5051, training loss: 6244.62, average training loss: 5765.60, base loss: 15953.54
[INFO 2017-06-30 05:32:16,706 main.py:52] epoch 5052, training loss: 5394.10, average training loss: 5765.21, base loss: 15952.29
[INFO 2017-06-30 05:32:19,871 main.py:52] epoch 5053, training loss: 5840.04, average training loss: 5765.20, base loss: 15952.10
[INFO 2017-06-30 05:32:23,050 main.py:52] epoch 5054, training loss: 5700.28, average training loss: 5765.31, base loss: 15951.82
[INFO 2017-06-30 05:32:26,234 main.py:52] epoch 5055, training loss: 5467.22, average training loss: 5765.24, base loss: 15951.28
[INFO 2017-06-30 05:32:29,400 main.py:52] epoch 5056, training loss: 5901.96, average training loss: 5765.05, base loss: 15951.91
[INFO 2017-06-30 05:32:32,570 main.py:52] epoch 5057, training loss: 5635.24, average training loss: 5764.38, base loss: 15951.55
[INFO 2017-06-30 05:32:35,722 main.py:52] epoch 5058, training loss: 6194.34, average training loss: 5765.00, base loss: 15952.10
[INFO 2017-06-30 05:32:38,854 main.py:52] epoch 5059, training loss: 5676.45, average training loss: 5765.06, base loss: 15952.01
[INFO 2017-06-30 05:32:41,998 main.py:52] epoch 5060, training loss: 5825.75, average training loss: 5764.97, base loss: 15951.90
[INFO 2017-06-30 05:32:45,182 main.py:52] epoch 5061, training loss: 5884.14, average training loss: 5764.64, base loss: 15951.64
[INFO 2017-06-30 05:32:48,335 main.py:52] epoch 5062, training loss: 5746.81, average training loss: 5764.47, base loss: 15951.65
[INFO 2017-06-30 05:32:51,508 main.py:52] epoch 5063, training loss: 5579.71, average training loss: 5764.12, base loss: 15951.78
[INFO 2017-06-30 05:32:54,712 main.py:52] epoch 5064, training loss: 5681.06, average training loss: 5764.47, base loss: 15951.55
[INFO 2017-06-30 05:32:57,888 main.py:52] epoch 5065, training loss: 5962.67, average training loss: 5764.10, base loss: 15951.81
[INFO 2017-06-30 05:33:01,034 main.py:52] epoch 5066, training loss: 5253.27, average training loss: 5763.82, base loss: 15951.14
[INFO 2017-06-30 05:33:04,167 main.py:52] epoch 5067, training loss: 5678.42, average training loss: 5763.50, base loss: 15950.39
[INFO 2017-06-30 05:33:07,325 main.py:52] epoch 5068, training loss: 5547.67, average training loss: 5763.51, base loss: 15950.05
[INFO 2017-06-30 05:33:10,560 main.py:52] epoch 5069, training loss: 5611.26, average training loss: 5763.50, base loss: 15950.15
[INFO 2017-06-30 05:33:13,688 main.py:52] epoch 5070, training loss: 5884.38, average training loss: 5763.67, base loss: 15950.25
[INFO 2017-06-30 05:33:16,828 main.py:52] epoch 5071, training loss: 5695.57, average training loss: 5763.34, base loss: 15950.87
[INFO 2017-06-30 05:33:20,025 main.py:52] epoch 5072, training loss: 5445.26, average training loss: 5763.42, base loss: 15950.70
[INFO 2017-06-30 05:33:23,202 main.py:52] epoch 5073, training loss: 5648.60, average training loss: 5763.38, base loss: 15950.90
[INFO 2017-06-30 05:33:26,369 main.py:52] epoch 5074, training loss: 5369.47, average training loss: 5762.74, base loss: 15950.46
[INFO 2017-06-30 05:33:29,542 main.py:52] epoch 5075, training loss: 5688.78, average training loss: 5762.72, base loss: 15950.20
[INFO 2017-06-30 05:33:32,708 main.py:52] epoch 5076, training loss: 5722.69, average training loss: 5762.56, base loss: 15950.31
[INFO 2017-06-30 05:33:35,883 main.py:52] epoch 5077, training loss: 5671.41, average training loss: 5762.38, base loss: 15950.18
[INFO 2017-06-30 05:33:39,048 main.py:52] epoch 5078, training loss: 5544.42, average training loss: 5762.58, base loss: 15950.52
[INFO 2017-06-30 05:33:42,203 main.py:52] epoch 5079, training loss: 5974.13, average training loss: 5762.52, base loss: 15951.32
[INFO 2017-06-30 05:33:45,422 main.py:52] epoch 5080, training loss: 5661.38, average training loss: 5762.65, base loss: 15951.27
[INFO 2017-06-30 05:33:48,563 main.py:52] epoch 5081, training loss: 5570.58, average training loss: 5762.47, base loss: 15950.86
[INFO 2017-06-30 05:33:51,716 main.py:52] epoch 5082, training loss: 5075.35, average training loss: 5761.77, base loss: 15950.30
[INFO 2017-06-30 05:33:54,898 main.py:52] epoch 5083, training loss: 5949.03, average training loss: 5762.23, base loss: 15950.85
[INFO 2017-06-30 05:33:58,088 main.py:52] epoch 5084, training loss: 5756.91, average training loss: 5762.31, base loss: 15950.98
[INFO 2017-06-30 05:34:01,290 main.py:52] epoch 5085, training loss: 5744.42, average training loss: 5762.26, base loss: 15950.90
[INFO 2017-06-30 05:34:04,416 main.py:52] epoch 5086, training loss: 5561.32, average training loss: 5762.25, base loss: 15950.85
[INFO 2017-06-30 05:34:07,595 main.py:52] epoch 5087, training loss: 5592.61, average training loss: 5762.16, base loss: 15950.79
[INFO 2017-06-30 05:34:10,801 main.py:52] epoch 5088, training loss: 5834.85, average training loss: 5762.14, base loss: 15951.49
[INFO 2017-06-30 05:34:13,983 main.py:52] epoch 5089, training loss: 5932.35, average training loss: 5761.89, base loss: 15952.52
[INFO 2017-06-30 05:34:17,156 main.py:52] epoch 5090, training loss: 5609.69, average training loss: 5761.75, base loss: 15952.35
[INFO 2017-06-30 05:34:20,310 main.py:52] epoch 5091, training loss: 5387.06, average training loss: 5761.52, base loss: 15951.73
[INFO 2017-06-30 05:34:23,487 main.py:52] epoch 5092, training loss: 5088.28, average training loss: 5760.69, base loss: 15950.78
[INFO 2017-06-30 05:34:26,666 main.py:52] epoch 5093, training loss: 5777.57, average training loss: 5760.78, base loss: 15951.22
[INFO 2017-06-30 05:34:29,859 main.py:52] epoch 5094, training loss: 5829.27, average training loss: 5761.36, base loss: 15951.23
[INFO 2017-06-30 05:34:33,044 main.py:52] epoch 5095, training loss: 5450.49, average training loss: 5760.90, base loss: 15950.80
[INFO 2017-06-30 05:34:36,219 main.py:52] epoch 5096, training loss: 5921.56, average training loss: 5761.09, base loss: 15950.49
[INFO 2017-06-30 05:34:39,391 main.py:52] epoch 5097, training loss: 5700.72, average training loss: 5760.99, base loss: 15950.52
[INFO 2017-06-30 05:34:42,624 main.py:52] epoch 5098, training loss: 5673.41, average training loss: 5760.47, base loss: 15950.15
[INFO 2017-06-30 05:34:45,800 main.py:52] epoch 5099, training loss: 5353.49, average training loss: 5760.07, base loss: 15949.91
[INFO 2017-06-30 05:34:45,800 main.py:54] epoch 5099, testing
[INFO 2017-06-30 05:34:59,033 main.py:97] average testing loss: 5663.31, base loss: 15978.55
[INFO 2017-06-30 05:34:59,033 main.py:98] improve_loss: 10315.25, improve_percent: 0.65
[INFO 2017-06-30 05:34:59,034 main.py:66] current best improved percent: 0.65
[INFO 2017-06-30 05:35:02,240 main.py:52] epoch 5100, training loss: 5635.71, average training loss: 5759.95, base loss: 15949.79
[INFO 2017-06-30 05:35:05,402 main.py:52] epoch 5101, training loss: 5933.46, average training loss: 5760.24, base loss: 15949.85
[INFO 2017-06-30 05:35:08,557 main.py:52] epoch 5102, training loss: 5987.19, average training loss: 5760.44, base loss: 15949.76
[INFO 2017-06-30 05:35:11,706 main.py:52] epoch 5103, training loss: 5399.07, average training loss: 5759.60, base loss: 15949.85
[INFO 2017-06-30 05:35:14,862 main.py:52] epoch 5104, training loss: 5667.42, average training loss: 5759.45, base loss: 15949.90
[INFO 2017-06-30 05:35:18,060 main.py:52] epoch 5105, training loss: 5371.99, average training loss: 5758.63, base loss: 15949.52
[INFO 2017-06-30 05:35:21,211 main.py:52] epoch 5106, training loss: 5609.51, average training loss: 5758.35, base loss: 15949.70
[INFO 2017-06-30 05:35:24,363 main.py:52] epoch 5107, training loss: 5796.73, average training loss: 5758.43, base loss: 15950.16
[INFO 2017-06-30 05:35:27,560 main.py:52] epoch 5108, training loss: 5963.79, average training loss: 5758.49, base loss: 15950.46
[INFO 2017-06-30 05:35:30,728 main.py:52] epoch 5109, training loss: 5699.60, average training loss: 5758.41, base loss: 15949.78
[INFO 2017-06-30 05:35:33,914 main.py:52] epoch 5110, training loss: 5641.76, average training loss: 5758.43, base loss: 15949.57
[INFO 2017-06-30 05:35:37,098 main.py:52] epoch 5111, training loss: 5629.20, average training loss: 5757.98, base loss: 15949.29
[INFO 2017-06-30 05:35:40,280 main.py:52] epoch 5112, training loss: 5479.91, average training loss: 5758.38, base loss: 15948.89
[INFO 2017-06-30 05:35:43,473 main.py:52] epoch 5113, training loss: 6199.19, average training loss: 5759.35, base loss: 15949.73
[INFO 2017-06-30 05:35:46,617 main.py:52] epoch 5114, training loss: 5690.90, average training loss: 5759.48, base loss: 15950.42
[INFO 2017-06-30 05:35:49,783 main.py:52] epoch 5115, training loss: 5769.83, average training loss: 5759.21, base loss: 15951.25
[INFO 2017-06-30 05:35:52,929 main.py:52] epoch 5116, training loss: 5875.11, average training loss: 5759.56, base loss: 15951.69
[INFO 2017-06-30 05:35:56,111 main.py:52] epoch 5117, training loss: 5524.26, average training loss: 5759.21, base loss: 15951.95
[INFO 2017-06-30 05:35:59,254 main.py:52] epoch 5118, training loss: 5581.30, average training loss: 5758.48, base loss: 15952.16
[INFO 2017-06-30 05:36:02,442 main.py:52] epoch 5119, training loss: 5920.53, average training loss: 5758.47, base loss: 15952.62
[INFO 2017-06-30 05:36:05,660 main.py:52] epoch 5120, training loss: 5939.46, average training loss: 5758.44, base loss: 15952.61
[INFO 2017-06-30 05:36:08,809 main.py:52] epoch 5121, training loss: 5528.85, average training loss: 5758.19, base loss: 15952.39
[INFO 2017-06-30 05:36:11,976 main.py:52] epoch 5122, training loss: 5534.05, average training loss: 5758.18, base loss: 15951.66
[INFO 2017-06-30 05:36:15,199 main.py:52] epoch 5123, training loss: 5621.64, average training loss: 5757.34, base loss: 15951.13
[INFO 2017-06-30 05:36:18,349 main.py:52] epoch 5124, training loss: 5377.08, average training loss: 5757.12, base loss: 15950.92
[INFO 2017-06-30 05:36:21,500 main.py:52] epoch 5125, training loss: 5565.16, average training loss: 5756.74, base loss: 15950.83
[INFO 2017-06-30 05:36:24,685 main.py:52] epoch 5126, training loss: 5729.40, average training loss: 5756.19, base loss: 15951.20
[INFO 2017-06-30 05:36:27,833 main.py:52] epoch 5127, training loss: 5666.31, average training loss: 5756.56, base loss: 15950.94
[INFO 2017-06-30 05:36:30,976 main.py:52] epoch 5128, training loss: 5506.59, average training loss: 5756.38, base loss: 15950.71
[INFO 2017-06-30 05:36:34,122 main.py:52] epoch 5129, training loss: 5593.04, average training loss: 5756.30, base loss: 15950.55
[INFO 2017-06-30 05:36:37,268 main.py:52] epoch 5130, training loss: 5776.47, average training loss: 5756.04, base loss: 15950.92
[INFO 2017-06-30 05:36:40,461 main.py:52] epoch 5131, training loss: 5678.58, average training loss: 5755.84, base loss: 15951.24
[INFO 2017-06-30 05:36:43,599 main.py:52] epoch 5132, training loss: 5609.12, average training loss: 5755.49, base loss: 15951.31
[INFO 2017-06-30 05:36:46,785 main.py:52] epoch 5133, training loss: 5725.75, average training loss: 5755.01, base loss: 15951.26
[INFO 2017-06-30 05:36:49,895 main.py:52] epoch 5134, training loss: 5808.92, average training loss: 5754.78, base loss: 15952.12
[INFO 2017-06-30 05:36:53,054 main.py:52] epoch 5135, training loss: 6081.83, average training loss: 5755.12, base loss: 15953.18
[INFO 2017-06-30 05:36:56,228 main.py:52] epoch 5136, training loss: 5802.87, average training loss: 5754.79, base loss: 15953.16
[INFO 2017-06-30 05:36:59,372 main.py:52] epoch 5137, training loss: 5347.78, average training loss: 5754.23, base loss: 15952.56
[INFO 2017-06-30 05:37:02,515 main.py:52] epoch 5138, training loss: 5331.88, average training loss: 5753.75, base loss: 15952.33
[INFO 2017-06-30 05:37:05,680 main.py:52] epoch 5139, training loss: 5658.91, average training loss: 5753.41, base loss: 15952.48
[INFO 2017-06-30 05:37:08,862 main.py:52] epoch 5140, training loss: 5787.32, average training loss: 5753.37, base loss: 15952.27
[INFO 2017-06-30 05:37:12,006 main.py:52] epoch 5141, training loss: 5538.21, average training loss: 5752.88, base loss: 15951.80
[INFO 2017-06-30 05:37:15,140 main.py:52] epoch 5142, training loss: 5769.89, average training loss: 5752.50, base loss: 15951.58
[INFO 2017-06-30 05:37:18,340 main.py:52] epoch 5143, training loss: 5795.37, average training loss: 5752.55, base loss: 15951.92
[INFO 2017-06-30 05:37:21,502 main.py:52] epoch 5144, training loss: 5639.92, average training loss: 5752.54, base loss: 15951.89
[INFO 2017-06-30 05:37:24,621 main.py:52] epoch 5145, training loss: 5521.35, average training loss: 5752.32, base loss: 15951.65
[INFO 2017-06-30 05:37:27,781 main.py:52] epoch 5146, training loss: 5842.23, average training loss: 5752.63, base loss: 15951.61
[INFO 2017-06-30 05:37:30,986 main.py:52] epoch 5147, training loss: 5690.55, average training loss: 5752.89, base loss: 15951.61
[INFO 2017-06-30 05:37:34,157 main.py:52] epoch 5148, training loss: 5483.59, average training loss: 5752.73, base loss: 15951.27
[INFO 2017-06-30 05:37:37,378 main.py:52] epoch 5149, training loss: 5691.71, average training loss: 5752.59, base loss: 15951.11
[INFO 2017-06-30 05:37:40,576 main.py:52] epoch 5150, training loss: 5648.81, average training loss: 5752.70, base loss: 15950.99
[INFO 2017-06-30 05:37:43,740 main.py:52] epoch 5151, training loss: 5729.48, average training loss: 5752.37, base loss: 15950.65
[INFO 2017-06-30 05:37:46,912 main.py:52] epoch 5152, training loss: 5666.53, average training loss: 5752.42, base loss: 15950.93
[INFO 2017-06-30 05:37:50,046 main.py:52] epoch 5153, training loss: 5597.57, average training loss: 5752.50, base loss: 15950.86
[INFO 2017-06-30 05:37:53,183 main.py:52] epoch 5154, training loss: 6036.05, average training loss: 5752.71, base loss: 15951.26
[INFO 2017-06-30 05:37:56,359 main.py:52] epoch 5155, training loss: 5882.18, average training loss: 5752.93, base loss: 15951.21
[INFO 2017-06-30 05:37:59,539 main.py:52] epoch 5156, training loss: 5858.98, average training loss: 5753.32, base loss: 15950.91
[INFO 2017-06-30 05:38:02,679 main.py:52] epoch 5157, training loss: 5421.71, average training loss: 5752.67, base loss: 15950.11
[INFO 2017-06-30 05:38:05,832 main.py:52] epoch 5158, training loss: 5564.33, average training loss: 5752.83, base loss: 15950.38
[INFO 2017-06-30 05:38:08,975 main.py:52] epoch 5159, training loss: 5224.23, average training loss: 5752.46, base loss: 15950.01
[INFO 2017-06-30 05:38:12,173 main.py:52] epoch 5160, training loss: 5401.19, average training loss: 5752.39, base loss: 15950.12
[INFO 2017-06-30 05:38:15,331 main.py:52] epoch 5161, training loss: 5903.48, average training loss: 5752.28, base loss: 15950.35
[INFO 2017-06-30 05:38:18,497 main.py:52] epoch 5162, training loss: 5238.87, average training loss: 5752.07, base loss: 15949.84
[INFO 2017-06-30 05:38:21,641 main.py:52] epoch 5163, training loss: 5887.33, average training loss: 5752.12, base loss: 15949.96
[INFO 2017-06-30 05:38:24,792 main.py:52] epoch 5164, training loss: 5701.31, average training loss: 5751.85, base loss: 15949.80
[INFO 2017-06-30 05:38:27,931 main.py:52] epoch 5165, training loss: 5744.15, average training loss: 5751.87, base loss: 15949.32
[INFO 2017-06-30 05:38:31,091 main.py:52] epoch 5166, training loss: 5765.16, average training loss: 5751.95, base loss: 15949.69
[INFO 2017-06-30 05:38:34,285 main.py:52] epoch 5167, training loss: 6119.73, average training loss: 5752.59, base loss: 15949.99
[INFO 2017-06-30 05:38:37,424 main.py:52] epoch 5168, training loss: 5567.69, average training loss: 5752.44, base loss: 15949.57
[INFO 2017-06-30 05:38:40,609 main.py:52] epoch 5169, training loss: 5649.53, average training loss: 5752.48, base loss: 15949.94
[INFO 2017-06-30 05:38:43,879 main.py:52] epoch 5170, training loss: 5405.26, average training loss: 5752.03, base loss: 15949.50
[INFO 2017-06-30 05:38:47,084 main.py:52] epoch 5171, training loss: 5592.36, average training loss: 5751.61, base loss: 15949.45
[INFO 2017-06-30 05:38:50,260 main.py:52] epoch 5172, training loss: 5727.96, average training loss: 5751.41, base loss: 15949.54
[INFO 2017-06-30 05:38:53,442 main.py:52] epoch 5173, training loss: 5420.09, average training loss: 5750.84, base loss: 15948.46
[INFO 2017-06-30 05:38:56,605 main.py:52] epoch 5174, training loss: 5449.12, average training loss: 5750.43, base loss: 15948.35
[INFO 2017-06-30 05:38:59,745 main.py:52] epoch 5175, training loss: 5374.69, average training loss: 5749.68, base loss: 15948.25
[INFO 2017-06-30 05:39:02,908 main.py:52] epoch 5176, training loss: 5726.39, average training loss: 5749.34, base loss: 15948.48
[INFO 2017-06-30 05:39:06,121 main.py:52] epoch 5177, training loss: 5559.54, average training loss: 5749.08, base loss: 15948.60
[INFO 2017-06-30 05:39:09,332 main.py:52] epoch 5178, training loss: 6013.84, average training loss: 5749.53, base loss: 15949.10
[INFO 2017-06-30 05:39:12,475 main.py:52] epoch 5179, training loss: 6175.22, average training loss: 5750.02, base loss: 15950.12
[INFO 2017-06-30 05:39:15,635 main.py:52] epoch 5180, training loss: 5865.59, average training loss: 5750.41, base loss: 15950.27
[INFO 2017-06-30 05:39:18,776 main.py:52] epoch 5181, training loss: 5662.08, average training loss: 5750.33, base loss: 15950.61
[INFO 2017-06-30 05:39:21,952 main.py:52] epoch 5182, training loss: 5679.89, average training loss: 5750.41, base loss: 15950.88
[INFO 2017-06-30 05:39:25,152 main.py:52] epoch 5183, training loss: 5814.37, average training loss: 5750.51, base loss: 15951.44
[INFO 2017-06-30 05:39:28,285 main.py:52] epoch 5184, training loss: 5704.69, average training loss: 5750.51, base loss: 15952.02
[INFO 2017-06-30 05:39:31,438 main.py:52] epoch 5185, training loss: 5573.28, average training loss: 5750.27, base loss: 15952.12
[INFO 2017-06-30 05:39:34,636 main.py:52] epoch 5186, training loss: 6600.58, average training loss: 5751.01, base loss: 15953.47
[INFO 2017-06-30 05:39:37,771 main.py:52] epoch 5187, training loss: 5846.67, average training loss: 5750.69, base loss: 15954.26
[INFO 2017-06-30 05:39:40,914 main.py:52] epoch 5188, training loss: 5558.78, average training loss: 5750.40, base loss: 15954.10
[INFO 2017-06-30 05:39:44,063 main.py:52] epoch 5189, training loss: 5387.12, average training loss: 5749.61, base loss: 15953.73
[INFO 2017-06-30 05:39:47,227 main.py:52] epoch 5190, training loss: 5910.74, average training loss: 5749.78, base loss: 15953.78
[INFO 2017-06-30 05:39:50,424 main.py:52] epoch 5191, training loss: 6149.41, average training loss: 5750.05, base loss: 15953.95
[INFO 2017-06-30 05:39:53,538 main.py:52] epoch 5192, training loss: 5812.33, average training loss: 5750.18, base loss: 15953.49
[INFO 2017-06-30 05:39:56,798 main.py:52] epoch 5193, training loss: 5944.22, average training loss: 5750.53, base loss: 15953.32
[INFO 2017-06-30 05:39:59,969 main.py:52] epoch 5194, training loss: 5880.11, average training loss: 5750.35, base loss: 15953.20
[INFO 2017-06-30 05:40:03,098 main.py:52] epoch 5195, training loss: 6049.27, average training loss: 5750.26, base loss: 15953.66
[INFO 2017-06-30 05:40:06,264 main.py:52] epoch 5196, training loss: 5521.71, average training loss: 5749.82, base loss: 15953.34
[INFO 2017-06-30 05:40:09,438 main.py:52] epoch 5197, training loss: 6263.53, average training loss: 5750.45, base loss: 15954.46
[INFO 2017-06-30 05:40:12,618 main.py:52] epoch 5198, training loss: 5554.24, average training loss: 5749.93, base loss: 15953.46
[INFO 2017-06-30 05:40:15,750 main.py:52] epoch 5199, training loss: 5721.42, average training loss: 5749.67, base loss: 15953.02
[INFO 2017-06-30 05:40:15,750 main.py:54] epoch 5199, testing
[INFO 2017-06-30 05:40:29,109 main.py:97] average testing loss: 5715.49, base loss: 15953.61
[INFO 2017-06-30 05:40:29,109 main.py:98] improve_loss: 10238.12, improve_percent: 0.64
[INFO 2017-06-30 05:40:29,111 main.py:66] current best improved percent: 0.65
[INFO 2017-06-30 05:40:32,304 main.py:52] epoch 5200, training loss: 5950.33, average training loss: 5749.89, base loss: 15953.26
[INFO 2017-06-30 05:40:35,462 main.py:52] epoch 5201, training loss: 5456.93, average training loss: 5749.11, base loss: 15953.17
[INFO 2017-06-30 05:40:38,605 main.py:52] epoch 5202, training loss: 5823.12, average training loss: 5749.19, base loss: 15953.36
[INFO 2017-06-30 05:40:41,759 main.py:52] epoch 5203, training loss: 5541.92, average training loss: 5749.06, base loss: 15953.32
[INFO 2017-06-30 05:40:44,905 main.py:52] epoch 5204, training loss: 5769.69, average training loss: 5748.87, base loss: 15953.10
[INFO 2017-06-30 05:40:48,071 main.py:52] epoch 5205, training loss: 5522.53, average training loss: 5748.79, base loss: 15952.54
[INFO 2017-06-30 05:40:51,242 main.py:52] epoch 5206, training loss: 5871.37, average training loss: 5749.00, base loss: 15952.47
[INFO 2017-06-30 05:40:54,464 main.py:52] epoch 5207, training loss: 5634.23, average training loss: 5748.71, base loss: 15952.43
[INFO 2017-06-30 05:40:57,589 main.py:52] epoch 5208, training loss: 5912.44, average training loss: 5749.18, base loss: 15952.24
[INFO 2017-06-30 05:41:00,718 main.py:52] epoch 5209, training loss: 5369.76, average training loss: 5748.59, base loss: 15951.40
[INFO 2017-06-30 05:41:03,912 main.py:52] epoch 5210, training loss: 6029.85, average training loss: 5749.09, base loss: 15951.69
[INFO 2017-06-30 05:41:07,065 main.py:52] epoch 5211, training loss: 5896.80, average training loss: 5748.56, base loss: 15951.56
[INFO 2017-06-30 05:41:10,225 main.py:52] epoch 5212, training loss: 5542.25, average training loss: 5748.32, base loss: 15951.36
[INFO 2017-06-30 05:41:13,377 main.py:52] epoch 5213, training loss: 5347.41, average training loss: 5748.11, base loss: 15951.05
[INFO 2017-06-30 05:41:16,531 main.py:52] epoch 5214, training loss: 5689.72, average training loss: 5747.75, base loss: 15951.04
[INFO 2017-06-30 05:41:19,720 main.py:52] epoch 5215, training loss: 6389.00, average training loss: 5748.19, base loss: 15951.52
[INFO 2017-06-30 05:41:22,870 main.py:52] epoch 5216, training loss: 5619.02, average training loss: 5748.07, base loss: 15950.64
[INFO 2017-06-30 05:41:26,047 main.py:52] epoch 5217, training loss: 5257.20, average training loss: 5747.75, base loss: 15949.72
[INFO 2017-06-30 05:41:29,282 main.py:52] epoch 5218, training loss: 5926.01, average training loss: 5747.89, base loss: 15949.78
[INFO 2017-06-30 05:41:32,455 main.py:52] epoch 5219, training loss: 5646.80, average training loss: 5747.70, base loss: 15949.63
[INFO 2017-06-30 05:41:35,622 main.py:52] epoch 5220, training loss: 5949.89, average training loss: 5748.12, base loss: 15949.97
[INFO 2017-06-30 05:41:38,801 main.py:52] epoch 5221, training loss: 6122.86, average training loss: 5748.54, base loss: 15949.93
[INFO 2017-06-30 05:41:41,971 main.py:52] epoch 5222, training loss: 5798.05, average training loss: 5748.57, base loss: 15949.54
[INFO 2017-06-30 05:41:45,113 main.py:52] epoch 5223, training loss: 5610.06, average training loss: 5748.39, base loss: 15949.34
[INFO 2017-06-30 05:41:48,317 main.py:52] epoch 5224, training loss: 5803.53, average training loss: 5748.51, base loss: 15948.93
[INFO 2017-06-30 05:41:51,508 main.py:52] epoch 5225, training loss: 5942.59, average training loss: 5749.19, base loss: 15949.30
[INFO 2017-06-30 05:41:54,688 main.py:52] epoch 5226, training loss: 5866.84, average training loss: 5749.29, base loss: 15949.68
[INFO 2017-06-30 05:41:57,849 main.py:52] epoch 5227, training loss: 5523.97, average training loss: 5749.10, base loss: 15949.19
[INFO 2017-06-30 05:42:01,013 main.py:52] epoch 5228, training loss: 5225.75, average training loss: 5748.27, base loss: 15948.59
[INFO 2017-06-30 05:42:04,181 main.py:52] epoch 5229, training loss: 5777.06, average training loss: 5748.40, base loss: 15949.17
[INFO 2017-06-30 05:42:07,435 main.py:52] epoch 5230, training loss: 5185.51, average training loss: 5747.83, base loss: 15948.33
[INFO 2017-06-30 05:42:10,603 main.py:52] epoch 5231, training loss: 5844.67, average training loss: 5747.82, base loss: 15948.18
[INFO 2017-06-30 05:42:13,759 main.py:52] epoch 5232, training loss: 5419.83, average training loss: 5747.52, base loss: 15947.82
[INFO 2017-06-30 05:42:16,945 main.py:52] epoch 5233, training loss: 5790.22, average training loss: 5747.66, base loss: 15947.96
[INFO 2017-06-30 05:42:20,111 main.py:52] epoch 5234, training loss: 5727.24, average training loss: 5747.68, base loss: 15948.44
[INFO 2017-06-30 05:42:23,258 main.py:52] epoch 5235, training loss: 5995.57, average training loss: 5747.78, base loss: 15949.29
[INFO 2017-06-30 05:42:26,428 main.py:52] epoch 5236, training loss: 5745.95, average training loss: 5747.52, base loss: 15950.01
[INFO 2017-06-30 05:42:29,600 main.py:52] epoch 5237, training loss: 5829.88, average training loss: 5747.27, base loss: 15950.80
[INFO 2017-06-30 05:42:32,746 main.py:52] epoch 5238, training loss: 5306.13, average training loss: 5746.70, base loss: 15950.01
[INFO 2017-06-30 05:42:35,890 main.py:52] epoch 5239, training loss: 5913.72, average training loss: 5746.33, base loss: 15950.26
[INFO 2017-06-30 05:42:39,068 main.py:52] epoch 5240, training loss: 5541.48, average training loss: 5746.18, base loss: 15950.23
[INFO 2017-06-30 05:42:42,226 main.py:52] epoch 5241, training loss: 5907.54, average training loss: 5746.35, base loss: 15950.34
[INFO 2017-06-30 05:42:45,366 main.py:52] epoch 5242, training loss: 5790.20, average training loss: 5745.77, base loss: 15950.20
[INFO 2017-06-30 05:42:48,563 main.py:52] epoch 5243, training loss: 5418.50, average training loss: 5744.95, base loss: 15950.01
[INFO 2017-06-30 05:42:51,696 main.py:52] epoch 5244, training loss: 5981.48, average training loss: 5745.22, base loss: 15950.71
[INFO 2017-06-30 05:42:54,882 main.py:52] epoch 5245, training loss: 5535.79, average training loss: 5745.19, base loss: 15950.41
[INFO 2017-06-30 05:42:58,062 main.py:52] epoch 5246, training loss: 5797.89, average training loss: 5745.06, base loss: 15951.10
[INFO 2017-06-30 05:43:01,206 main.py:52] epoch 5247, training loss: 5474.11, average training loss: 5745.45, base loss: 15951.00
[INFO 2017-06-30 05:43:04,448 main.py:52] epoch 5248, training loss: 5602.53, average training loss: 5744.87, base loss: 15951.27
[INFO 2017-06-30 05:43:07,640 main.py:52] epoch 5249, training loss: 5647.00, average training loss: 5744.69, base loss: 15951.21
[INFO 2017-06-30 05:43:10,822 main.py:52] epoch 5250, training loss: 5620.48, average training loss: 5744.78, base loss: 15951.45
[INFO 2017-06-30 05:43:14,035 main.py:52] epoch 5251, training loss: 5472.73, average training loss: 5744.56, base loss: 15950.89
[INFO 2017-06-30 05:43:17,226 main.py:52] epoch 5252, training loss: 5845.44, average training loss: 5744.62, base loss: 15950.70
[INFO 2017-06-30 05:43:20,394 main.py:52] epoch 5253, training loss: 5788.80, average training loss: 5744.16, base loss: 15951.19
[INFO 2017-06-30 05:43:23,562 main.py:52] epoch 5254, training loss: 5351.37, average training loss: 5743.37, base loss: 15951.03
[INFO 2017-06-30 05:43:26,709 main.py:52] epoch 5255, training loss: 5410.57, average training loss: 5742.65, base loss: 15951.04
[INFO 2017-06-30 05:43:29,875 main.py:52] epoch 5256, training loss: 6042.16, average training loss: 5743.21, base loss: 15951.09
[INFO 2017-06-30 05:43:33,019 main.py:52] epoch 5257, training loss: 5699.80, average training loss: 5743.36, base loss: 15950.36
[INFO 2017-06-30 05:43:36,178 main.py:52] epoch 5258, training loss: 6200.49, average training loss: 5743.54, base loss: 15950.79
[INFO 2017-06-30 05:43:39,358 main.py:52] epoch 5259, training loss: 5417.95, average training loss: 5742.91, base loss: 15950.44
[INFO 2017-06-30 05:43:42,546 main.py:52] epoch 5260, training loss: 5233.84, average training loss: 5742.50, base loss: 15949.86
[INFO 2017-06-30 05:43:45,715 main.py:52] epoch 5261, training loss: 5541.43, average training loss: 5741.67, base loss: 15949.93
[INFO 2017-06-30 05:43:48,847 main.py:52] epoch 5262, training loss: 6146.77, average training loss: 5741.56, base loss: 15949.61
[INFO 2017-06-30 05:43:52,069 main.py:52] epoch 5263, training loss: 5902.21, average training loss: 5741.31, base loss: 15949.79
[INFO 2017-06-30 05:43:55,217 main.py:52] epoch 5264, training loss: 5878.74, average training loss: 5741.55, base loss: 15950.33
[INFO 2017-06-30 05:43:58,340 main.py:52] epoch 5265, training loss: 6068.48, average training loss: 5742.06, base loss: 15951.07
[INFO 2017-06-30 05:44:01,465 main.py:52] epoch 5266, training loss: 5870.39, average training loss: 5741.87, base loss: 15951.12
[INFO 2017-06-30 05:44:04,687 main.py:52] epoch 5267, training loss: 5758.00, average training loss: 5742.02, base loss: 15950.90
[INFO 2017-06-30 05:44:07,882 main.py:52] epoch 5268, training loss: 5802.90, average training loss: 5741.68, base loss: 15950.91
[INFO 2017-06-30 05:44:11,055 main.py:52] epoch 5269, training loss: 5702.03, average training loss: 5741.75, base loss: 15950.85
[INFO 2017-06-30 05:44:14,267 main.py:52] epoch 5270, training loss: 5387.89, average training loss: 5741.40, base loss: 15950.55
[INFO 2017-06-30 05:44:17,454 main.py:52] epoch 5271, training loss: 5572.90, average training loss: 5741.04, base loss: 15950.75
[INFO 2017-06-30 05:44:20,626 main.py:52] epoch 5272, training loss: 5243.43, average training loss: 5740.00, base loss: 15950.45
[INFO 2017-06-30 05:44:23,799 main.py:52] epoch 5273, training loss: 5687.66, average training loss: 5739.80, base loss: 15950.67
[INFO 2017-06-30 05:44:26,974 main.py:52] epoch 5274, training loss: 5709.73, average training loss: 5739.72, base loss: 15950.37
[INFO 2017-06-30 05:44:30,171 main.py:52] epoch 5275, training loss: 5911.11, average training loss: 5740.00, base loss: 15950.52
[INFO 2017-06-30 05:44:33,310 main.py:52] epoch 5276, training loss: 5530.82, average training loss: 5739.29, base loss: 15950.01
[INFO 2017-06-30 05:44:36,496 main.py:52] epoch 5277, training loss: 5657.02, average training loss: 5739.31, base loss: 15949.29
[INFO 2017-06-30 05:44:39,611 main.py:52] epoch 5278, training loss: 5620.88, average training loss: 5738.84, base loss: 15949.15
[INFO 2017-06-30 05:44:42,815 main.py:52] epoch 5279, training loss: 5910.69, average training loss: 5738.90, base loss: 15950.12
[INFO 2017-06-30 05:44:45,969 main.py:52] epoch 5280, training loss: 5801.68, average training loss: 5738.90, base loss: 15949.92
[INFO 2017-06-30 05:44:49,116 main.py:52] epoch 5281, training loss: 5937.18, average training loss: 5738.83, base loss: 15949.68
[INFO 2017-06-30 05:44:52,271 main.py:52] epoch 5282, training loss: 5153.60, average training loss: 5737.88, base loss: 15948.93
[INFO 2017-06-30 05:44:55,429 main.py:52] epoch 5283, training loss: 5701.26, average training loss: 5737.78, base loss: 15949.15
[INFO 2017-06-30 05:44:58,597 main.py:52] epoch 5284, training loss: 5382.53, average training loss: 5737.28, base loss: 15948.56
[INFO 2017-06-30 05:45:01,739 main.py:52] epoch 5285, training loss: 5642.01, average training loss: 5736.61, base loss: 15948.73
[INFO 2017-06-30 05:45:04,927 main.py:52] epoch 5286, training loss: 5764.33, average training loss: 5736.65, base loss: 15948.73
[INFO 2017-06-30 05:45:08,120 main.py:52] epoch 5287, training loss: 5527.11, average training loss: 5736.53, base loss: 15948.83
[INFO 2017-06-30 05:45:11,300 main.py:52] epoch 5288, training loss: 5817.90, average training loss: 5736.65, base loss: 15949.13
[INFO 2017-06-30 05:45:14,454 main.py:52] epoch 5289, training loss: 5537.17, average training loss: 5736.40, base loss: 15948.79
[INFO 2017-06-30 05:45:17,583 main.py:52] epoch 5290, training loss: 5573.99, average training loss: 5735.94, base loss: 15948.68
[INFO 2017-06-30 05:45:20,744 main.py:52] epoch 5291, training loss: 5382.65, average training loss: 5735.88, base loss: 15948.41
[INFO 2017-06-30 05:45:23,920 main.py:52] epoch 5292, training loss: 6184.70, average training loss: 5736.03, base loss: 15949.47
[INFO 2017-06-30 05:45:27,052 main.py:52] epoch 5293, training loss: 5433.59, average training loss: 5735.77, base loss: 15949.56
[INFO 2017-06-30 05:45:30,157 main.py:52] epoch 5294, training loss: 5962.88, average training loss: 5735.75, base loss: 15950.03
[INFO 2017-06-30 05:45:33,363 main.py:52] epoch 5295, training loss: 5639.66, average training loss: 5735.66, base loss: 15950.29
[INFO 2017-06-30 05:45:36,535 main.py:52] epoch 5296, training loss: 5942.58, average training loss: 5735.38, base loss: 15950.45
[INFO 2017-06-30 05:45:39,678 main.py:52] epoch 5297, training loss: 5657.94, average training loss: 5734.91, base loss: 15950.44
[INFO 2017-06-30 05:45:42,821 main.py:52] epoch 5298, training loss: 5633.35, average training loss: 5735.07, base loss: 15950.20
[INFO 2017-06-30 05:45:45,971 main.py:52] epoch 5299, training loss: 6062.88, average training loss: 5735.58, base loss: 15950.76
[INFO 2017-06-30 05:45:45,971 main.py:54] epoch 5299, testing
[INFO 2017-06-30 05:45:59,155 main.py:97] average testing loss: 5608.43, base loss: 15856.35
[INFO 2017-06-30 05:45:59,155 main.py:98] improve_loss: 10247.92, improve_percent: 0.65
[INFO 2017-06-30 05:45:59,157 main.py:66] current best improved percent: 0.65
[INFO 2017-06-30 05:46:02,368 main.py:52] epoch 5300, training loss: 5526.06, average training loss: 5735.33, base loss: 15950.21
[INFO 2017-06-30 05:46:05,544 main.py:52] epoch 5301, training loss: 6363.12, average training loss: 5735.83, base loss: 15951.02
[INFO 2017-06-30 05:46:08,713 main.py:52] epoch 5302, training loss: 5698.35, average training loss: 5736.19, base loss: 15951.48
[INFO 2017-06-30 05:46:11,891 main.py:52] epoch 5303, training loss: 5868.11, average training loss: 5736.08, base loss: 15951.78
[INFO 2017-06-30 05:46:15,044 main.py:52] epoch 5304, training loss: 5563.82, average training loss: 5736.07, base loss: 15952.09
[INFO 2017-06-30 05:46:18,177 main.py:52] epoch 5305, training loss: 5821.96, average training loss: 5735.83, base loss: 15952.57
[INFO 2017-06-30 05:46:21,334 main.py:52] epoch 5306, training loss: 5769.92, average training loss: 5735.80, base loss: 15952.28
[INFO 2017-06-30 05:46:24,488 main.py:52] epoch 5307, training loss: 5982.39, average training loss: 5735.78, base loss: 15952.84
[INFO 2017-06-30 05:46:27,679 main.py:52] epoch 5308, training loss: 5647.77, average training loss: 5734.92, base loss: 15952.43
[INFO 2017-06-30 05:46:30,831 main.py:52] epoch 5309, training loss: 5507.34, average training loss: 5734.28, base loss: 15952.30
[INFO 2017-06-30 05:46:33,933 main.py:52] epoch 5310, training loss: 6107.74, average training loss: 5734.63, base loss: 15953.24
[INFO 2017-06-30 05:46:37,088 main.py:52] epoch 5311, training loss: 5410.10, average training loss: 5733.88, base loss: 15953.20
[INFO 2017-06-30 05:46:40,265 main.py:52] epoch 5312, training loss: 6441.34, average training loss: 5734.40, base loss: 15954.57
[INFO 2017-06-30 05:46:43,404 main.py:52] epoch 5313, training loss: 5463.48, average training loss: 5733.92, base loss: 15954.93
[INFO 2017-06-30 05:46:46,549 main.py:52] epoch 5314, training loss: 5611.69, average training loss: 5733.67, base loss: 15955.15
[INFO 2017-06-30 05:46:49,703 main.py:52] epoch 5315, training loss: 5459.02, average training loss: 5733.46, base loss: 15955.13
[INFO 2017-06-30 05:46:52,901 main.py:52] epoch 5316, training loss: 5670.62, average training loss: 5733.76, base loss: 15954.66
[INFO 2017-06-30 05:46:56,063 main.py:52] epoch 5317, training loss: 5210.98, average training loss: 5733.14, base loss: 15953.93
[INFO 2017-06-30 05:46:59,192 main.py:52] epoch 5318, training loss: 5635.71, average training loss: 5733.20, base loss: 15953.87
[INFO 2017-06-30 05:47:02,362 main.py:52] epoch 5319, training loss: 5259.73, average training loss: 5733.05, base loss: 15953.49
[INFO 2017-06-30 05:47:05,519 main.py:52] epoch 5320, training loss: 5851.54, average training loss: 5733.06, base loss: 15953.74
[INFO 2017-06-30 05:47:08,699 main.py:52] epoch 5321, training loss: 5760.64, average training loss: 5733.33, base loss: 15953.91
[INFO 2017-06-30 05:47:11,829 main.py:52] epoch 5322, training loss: 5451.13, average training loss: 5733.38, base loss: 15953.36
[INFO 2017-06-30 05:47:14,987 main.py:52] epoch 5323, training loss: 5978.51, average training loss: 5733.42, base loss: 15953.93
[INFO 2017-06-30 05:47:18,150 main.py:52] epoch 5324, training loss: 5806.59, average training loss: 5733.11, base loss: 15954.12
[INFO 2017-06-30 05:47:21,328 main.py:52] epoch 5325, training loss: 5458.92, average training loss: 5732.71, base loss: 15953.76
[INFO 2017-06-30 05:47:24,485 main.py:52] epoch 5326, training loss: 5210.81, average training loss: 5731.73, base loss: 15953.34
[INFO 2017-06-30 05:47:27,652 main.py:52] epoch 5327, training loss: 5602.41, average training loss: 5731.48, base loss: 15953.51
[INFO 2017-06-30 05:47:30,859 main.py:52] epoch 5328, training loss: 6008.62, average training loss: 5732.01, base loss: 15953.48
[INFO 2017-06-30 05:47:34,048 main.py:52] epoch 5329, training loss: 5942.92, average training loss: 5732.21, base loss: 15953.71
[INFO 2017-06-30 05:47:37,242 main.py:52] epoch 5330, training loss: 5921.39, average training loss: 5732.72, base loss: 15954.16
[INFO 2017-06-30 05:47:40,410 main.py:52] epoch 5331, training loss: 6182.55, average training loss: 5733.01, base loss: 15954.62
[INFO 2017-06-30 05:47:43,600 main.py:52] epoch 5332, training loss: 5471.92, average training loss: 5732.57, base loss: 15954.26
[INFO 2017-06-30 05:47:46,754 main.py:52] epoch 5333, training loss: 5707.71, average training loss: 5732.43, base loss: 15953.78
[INFO 2017-06-30 05:47:49,941 main.py:52] epoch 5334, training loss: 6050.34, average training loss: 5732.53, base loss: 15953.82
[INFO 2017-06-30 05:47:53,132 main.py:52] epoch 5335, training loss: 5617.31, average training loss: 5732.28, base loss: 15954.05
[INFO 2017-06-30 05:47:56,312 main.py:52] epoch 5336, training loss: 5856.08, average training loss: 5732.29, base loss: 15954.00
[INFO 2017-06-30 05:47:59,449 main.py:52] epoch 5337, training loss: 5970.13, average training loss: 5732.39, base loss: 15954.62
[INFO 2017-06-30 05:48:02,610 main.py:52] epoch 5338, training loss: 5793.93, average training loss: 5732.42, base loss: 15954.76
[INFO 2017-06-30 05:48:05,787 main.py:52] epoch 5339, training loss: 5768.74, average training loss: 5732.47, base loss: 15955.13
[INFO 2017-06-30 05:48:08,927 main.py:52] epoch 5340, training loss: 5602.91, average training loss: 5732.28, base loss: 15954.74
[INFO 2017-06-30 05:48:12,154 main.py:52] epoch 5341, training loss: 6102.21, average training loss: 5732.79, base loss: 15955.41
[INFO 2017-06-30 05:48:15,318 main.py:52] epoch 5342, training loss: 5533.98, average training loss: 5732.18, base loss: 15955.38
[INFO 2017-06-30 05:48:18,561 main.py:52] epoch 5343, training loss: 5375.49, average training loss: 5731.91, base loss: 15954.84
[INFO 2017-06-30 05:48:21,696 main.py:52] epoch 5344, training loss: 5327.13, average training loss: 5731.80, base loss: 15954.01
[INFO 2017-06-30 05:48:24,851 main.py:52] epoch 5345, training loss: 5718.92, average training loss: 5731.54, base loss: 15953.96
[INFO 2017-06-30 05:48:28,065 main.py:52] epoch 5346, training loss: 5642.81, average training loss: 5731.62, base loss: 15953.67
[INFO 2017-06-30 05:48:31,265 main.py:52] epoch 5347, training loss: 5535.08, average training loss: 5731.26, base loss: 15953.19
[INFO 2017-06-30 05:48:34,416 main.py:52] epoch 5348, training loss: 5487.99, average training loss: 5731.04, base loss: 15952.37
[INFO 2017-06-30 05:48:37,586 main.py:52] epoch 5349, training loss: 5808.99, average training loss: 5730.65, base loss: 15952.51
[INFO 2017-06-30 05:48:40,803 main.py:52] epoch 5350, training loss: 5788.94, average training loss: 5730.41, base loss: 15952.38
[INFO 2017-06-30 05:48:43,996 main.py:52] epoch 5351, training loss: 5509.07, average training loss: 5730.08, base loss: 15952.37
[INFO 2017-06-30 05:48:47,126 main.py:52] epoch 5352, training loss: 5486.04, average training loss: 5729.88, base loss: 15951.75
[INFO 2017-06-30 05:48:50,272 main.py:52] epoch 5353, training loss: 5808.98, average training loss: 5729.65, base loss: 15952.04
[INFO 2017-06-30 05:48:53,436 main.py:52] epoch 5354, training loss: 6112.73, average training loss: 5729.88, base loss: 15952.50
[INFO 2017-06-30 05:48:56,621 main.py:52] epoch 5355, training loss: 5722.75, average training loss: 5729.83, base loss: 15952.51
[INFO 2017-06-30 05:48:59,838 main.py:52] epoch 5356, training loss: 5711.81, average training loss: 5729.72, base loss: 15952.29
[INFO 2017-06-30 05:49:02,983 main.py:52] epoch 5357, training loss: 5554.36, average training loss: 5729.33, base loss: 15952.02
[INFO 2017-06-30 05:49:06,133 main.py:52] epoch 5358, training loss: 5573.34, average training loss: 5729.38, base loss: 15951.80
[INFO 2017-06-30 05:49:09,283 main.py:52] epoch 5359, training loss: 5736.91, average training loss: 5729.22, base loss: 15951.93
[INFO 2017-06-30 05:49:12,485 main.py:52] epoch 5360, training loss: 5598.30, average training loss: 5728.86, base loss: 15952.16
[INFO 2017-06-30 05:49:15,661 main.py:52] epoch 5361, training loss: 5360.40, average training loss: 5728.16, base loss: 15951.80
[INFO 2017-06-30 05:49:18,835 main.py:52] epoch 5362, training loss: 5624.25, average training loss: 5727.99, base loss: 15951.94
[INFO 2017-06-30 05:49:22,012 main.py:52] epoch 5363, training loss: 5616.75, average training loss: 5727.91, base loss: 15951.73
[INFO 2017-06-30 05:49:25,165 main.py:52] epoch 5364, training loss: 5389.53, average training loss: 5727.73, base loss: 15951.06
[INFO 2017-06-30 05:49:28,331 main.py:52] epoch 5365, training loss: 5554.26, average training loss: 5727.36, base loss: 15951.12
[INFO 2017-06-30 05:49:31,496 main.py:52] epoch 5366, training loss: 5901.50, average training loss: 5727.48, base loss: 15951.56
[INFO 2017-06-30 05:49:34,695 main.py:52] epoch 5367, training loss: 5663.61, average training loss: 5727.41, base loss: 15951.89
[INFO 2017-06-30 05:49:37,898 main.py:52] epoch 5368, training loss: 5807.22, average training loss: 5727.50, base loss: 15952.05
[INFO 2017-06-30 05:49:41,075 main.py:52] epoch 5369, training loss: 5531.23, average training loss: 5727.53, base loss: 15951.47
[INFO 2017-06-30 05:49:44,226 main.py:52] epoch 5370, training loss: 5944.88, average training loss: 5727.36, base loss: 15951.56
[INFO 2017-06-30 05:49:47,401 main.py:52] epoch 5371, training loss: 5722.13, average training loss: 5727.90, base loss: 15950.99
[INFO 2017-06-30 05:49:50,545 main.py:52] epoch 5372, training loss: 5942.31, average training loss: 5728.05, base loss: 15951.07
[INFO 2017-06-30 05:49:53,701 main.py:52] epoch 5373, training loss: 5573.57, average training loss: 5728.09, base loss: 15950.97
[INFO 2017-06-30 05:49:56,898 main.py:52] epoch 5374, training loss: 5608.58, average training loss: 5727.79, base loss: 15950.46
[INFO 2017-06-30 05:50:00,087 main.py:52] epoch 5375, training loss: 5610.52, average training loss: 5727.83, base loss: 15949.87
[INFO 2017-06-30 05:50:03,256 main.py:52] epoch 5376, training loss: 5743.04, average training loss: 5728.29, base loss: 15950.02
[INFO 2017-06-30 05:50:06,376 main.py:52] epoch 5377, training loss: 5385.81, average training loss: 5727.93, base loss: 15949.70
[INFO 2017-06-30 05:50:09,548 main.py:52] epoch 5378, training loss: 5760.96, average training loss: 5728.18, base loss: 15949.91
[INFO 2017-06-30 05:50:12,711 main.py:52] epoch 5379, training loss: 5693.85, average training loss: 5728.21, base loss: 15949.91
[INFO 2017-06-30 05:50:15,888 main.py:52] epoch 5380, training loss: 5443.44, average training loss: 5728.11, base loss: 15949.87
[INFO 2017-06-30 05:50:19,028 main.py:52] epoch 5381, training loss: 5167.02, average training loss: 5727.31, base loss: 15949.40
[INFO 2017-06-30 05:50:22,209 main.py:52] epoch 5382, training loss: 6035.81, average training loss: 5727.51, base loss: 15949.99
[INFO 2017-06-30 05:50:25,392 main.py:52] epoch 5383, training loss: 5308.77, average training loss: 5726.75, base loss: 15949.59
[INFO 2017-06-30 05:50:28,622 main.py:52] epoch 5384, training loss: 5687.54, average training loss: 5726.96, base loss: 15948.87
[INFO 2017-06-30 05:50:31,769 main.py:52] epoch 5385, training loss: 5475.33, average training loss: 5726.31, base loss: 15948.46
[INFO 2017-06-30 05:50:34,925 main.py:52] epoch 5386, training loss: 5931.06, average training loss: 5726.22, base loss: 15949.10
[INFO 2017-06-30 05:50:38,099 main.py:52] epoch 5387, training loss: 5649.65, average training loss: 5725.83, base loss: 15948.88
[INFO 2017-06-30 05:50:41,253 main.py:52] epoch 5388, training loss: 5500.48, average training loss: 5724.96, base loss: 15948.53
[INFO 2017-06-30 05:50:44,420 main.py:52] epoch 5389, training loss: 5527.22, average training loss: 5724.84, base loss: 15948.68
[INFO 2017-06-30 05:50:47,594 main.py:52] epoch 5390, training loss: 5693.27, average training loss: 5724.60, base loss: 15948.82
[INFO 2017-06-30 05:50:50,749 main.py:52] epoch 5391, training loss: 5761.67, average training loss: 5724.72, base loss: 15948.95
[INFO 2017-06-30 05:50:53,899 main.py:52] epoch 5392, training loss: 5752.13, average training loss: 5724.29, base loss: 15949.10
[INFO 2017-06-30 05:50:57,038 main.py:52] epoch 5393, training loss: 5315.46, average training loss: 5723.83, base loss: 15948.85
[INFO 2017-06-30 05:51:00,226 main.py:52] epoch 5394, training loss: 5885.73, average training loss: 5723.85, base loss: 15949.47
[INFO 2017-06-30 05:51:03,389 main.py:52] epoch 5395, training loss: 5689.50, average training loss: 5724.16, base loss: 15949.57
[INFO 2017-06-30 05:51:06,603 main.py:52] epoch 5396, training loss: 5951.82, average training loss: 5724.14, base loss: 15949.85
[INFO 2017-06-30 05:51:09,775 main.py:52] epoch 5397, training loss: 5629.64, average training loss: 5723.54, base loss: 15949.69
[INFO 2017-06-30 05:51:12,902 main.py:52] epoch 5398, training loss: 5573.20, average training loss: 5723.26, base loss: 15949.63
[INFO 2017-06-30 05:51:16,036 main.py:52] epoch 5399, training loss: 6036.84, average training loss: 5723.54, base loss: 15950.03
[INFO 2017-06-30 05:51:16,036 main.py:54] epoch 5399, testing
[INFO 2017-06-30 05:51:29,345 main.py:97] average testing loss: 5479.07, base loss: 15599.14
[INFO 2017-06-30 05:51:29,345 main.py:98] improve_loss: 10120.07, improve_percent: 0.65
[INFO 2017-06-30 05:51:29,347 main.py:62] model save to ./model/final.pth
[INFO 2017-06-30 05:51:29,381 main.py:66] current best improved percent: 0.65
[INFO 2017-06-30 05:51:32,516 main.py:52] epoch 5400, training loss: 5579.86, average training loss: 5723.37, base loss: 15949.77
[INFO 2017-06-30 05:51:35,637 main.py:52] epoch 5401, training loss: 5619.33, average training loss: 5723.44, base loss: 15949.82
[INFO 2017-06-30 05:51:38,786 main.py:52] epoch 5402, training loss: 5905.38, average training loss: 5723.33, base loss: 15950.10
[INFO 2017-06-30 05:51:41,907 main.py:52] epoch 5403, training loss: 5601.72, average training loss: 5723.06, base loss: 15949.88
[INFO 2017-06-30 05:51:45,044 main.py:52] epoch 5404, training loss: 5537.64, average training loss: 5722.00, base loss: 15949.78
[INFO 2017-06-30 05:51:48,189 main.py:52] epoch 5405, training loss: 5726.40, average training loss: 5721.81, base loss: 15949.95
[INFO 2017-06-30 05:51:51,333 main.py:52] epoch 5406, training loss: 5337.68, average training loss: 5721.41, base loss: 15949.39
[INFO 2017-06-30 05:51:54,498 main.py:52] epoch 5407, training loss: 5487.36, average training loss: 5721.32, base loss: 15949.29
[INFO 2017-06-30 05:51:57,609 main.py:52] epoch 5408, training loss: 5582.46, average training loss: 5721.24, base loss: 15949.18
[INFO 2017-06-30 05:52:00,783 main.py:52] epoch 5409, training loss: 5829.01, average training loss: 5721.29, base loss: 15949.58
[INFO 2017-06-30 05:52:03,948 main.py:52] epoch 5410, training loss: 5413.23, average training loss: 5720.80, base loss: 15949.75
[INFO 2017-06-30 05:52:07,077 main.py:52] epoch 5411, training loss: 5272.85, average training loss: 5720.34, base loss: 15949.07
[INFO 2017-06-30 05:52:10,252 main.py:52] epoch 5412, training loss: 5619.24, average training loss: 5719.55, base loss: 15948.90
[INFO 2017-06-30 05:52:13,412 main.py:52] epoch 5413, training loss: 6089.70, average training loss: 5719.86, base loss: 15949.11
[INFO 2017-06-30 05:52:16,534 main.py:52] epoch 5414, training loss: 5394.36, average training loss: 5719.12, base loss: 15949.16
[INFO 2017-06-30 05:52:19,703 main.py:52] epoch 5415, training loss: 5437.73, average training loss: 5718.93, base loss: 15948.91
[INFO 2017-06-30 05:52:22,816 main.py:52] epoch 5416, training loss: 5888.07, average training loss: 5719.25, base loss: 15949.72
[INFO 2017-06-30 05:52:25,944 main.py:52] epoch 5417, training loss: 5656.60, average training loss: 5718.77, base loss: 15949.81
[INFO 2017-06-30 05:52:29,087 main.py:52] epoch 5418, training loss: 5445.65, average training loss: 5718.38, base loss: 15949.08
[INFO 2017-06-30 05:52:32,257 main.py:52] epoch 5419, training loss: 5221.71, average training loss: 5717.86, base loss: 15948.67
[INFO 2017-06-30 05:52:35,378 main.py:52] epoch 5420, training loss: 6022.49, average training loss: 5717.94, base loss: 15949.87
[INFO 2017-06-30 05:52:38,537 main.py:52] epoch 5421, training loss: 5686.62, average training loss: 5718.05, base loss: 15950.18
[INFO 2017-06-30 05:52:41,683 main.py:52] epoch 5422, training loss: 5531.00, average training loss: 5717.77, base loss: 15949.65
[INFO 2017-06-30 05:52:44,861 main.py:52] epoch 5423, training loss: 5334.67, average training loss: 5717.36, base loss: 15949.15
[INFO 2017-06-30 05:52:48,026 main.py:52] epoch 5424, training loss: 5554.13, average training loss: 5717.13, base loss: 15949.19
[INFO 2017-06-30 05:52:51,166 main.py:52] epoch 5425, training loss: 5893.47, average training loss: 5717.29, base loss: 15949.44
[INFO 2017-06-30 05:52:54,347 main.py:52] epoch 5426, training loss: 5809.32, average training loss: 5717.37, base loss: 15950.24
[INFO 2017-06-30 05:52:57,474 main.py:52] epoch 5427, training loss: 5824.08, average training loss: 5717.20, base loss: 15951.27
[INFO 2017-06-30 05:53:00,638 main.py:52] epoch 5428, training loss: 5015.87, average training loss: 5716.33, base loss: 15950.98
[INFO 2017-06-30 05:53:03,775 main.py:52] epoch 5429, training loss: 5608.53, average training loss: 5716.29, base loss: 15950.31
[INFO 2017-06-30 05:53:06,944 main.py:52] epoch 5430, training loss: 5449.98, average training loss: 5716.38, base loss: 15949.58
[INFO 2017-06-30 05:53:10,110 main.py:52] epoch 5431, training loss: 5409.44, average training loss: 5716.05, base loss: 15948.81
[INFO 2017-06-30 05:53:13,286 main.py:52] epoch 5432, training loss: 6123.37, average training loss: 5716.49, base loss: 15949.64
[INFO 2017-06-30 05:53:16,451 main.py:52] epoch 5433, training loss: 5884.95, average training loss: 5716.95, base loss: 15950.46
[INFO 2017-06-30 05:53:19,588 main.py:52] epoch 5434, training loss: 5902.03, average training loss: 5716.86, base loss: 15951.20
[INFO 2017-06-30 05:53:22,726 main.py:52] epoch 5435, training loss: 5473.43, average training loss: 5716.43, base loss: 15951.22
[INFO 2017-06-30 05:53:25,885 main.py:52] epoch 5436, training loss: 5639.75, average training loss: 5716.18, base loss: 15950.41
[INFO 2017-06-30 05:53:29,058 main.py:52] epoch 5437, training loss: 5474.38, average training loss: 5715.54, base loss: 15950.07
[INFO 2017-06-30 05:53:32,186 main.py:52] epoch 5438, training loss: 5789.64, average training loss: 5715.00, base loss: 15950.18
[INFO 2017-06-30 05:53:35,336 main.py:52] epoch 5439, training loss: 5623.04, average training loss: 5715.05, base loss: 15950.22
[INFO 2017-06-30 05:53:38,515 main.py:52] epoch 5440, training loss: 5470.15, average training loss: 5715.10, base loss: 15950.05
[INFO 2017-06-30 05:53:41,697 main.py:52] epoch 5441, training loss: 5834.36, average training loss: 5714.79, base loss: 15950.59
[INFO 2017-06-30 05:53:44,857 main.py:52] epoch 5442, training loss: 5729.92, average training loss: 5714.57, base loss: 15950.52
[INFO 2017-06-30 05:53:48,014 main.py:52] epoch 5443, training loss: 5807.63, average training loss: 5714.31, base loss: 15950.46
[INFO 2017-06-30 05:53:51,170 main.py:52] epoch 5444, training loss: 5501.09, average training loss: 5714.24, base loss: 15950.11
[INFO 2017-06-30 05:53:54,283 main.py:52] epoch 5445, training loss: 5416.58, average training loss: 5714.33, base loss: 15949.62
[INFO 2017-06-30 05:53:57,419 main.py:52] epoch 5446, training loss: 5915.01, average training loss: 5714.30, base loss: 15949.62
[INFO 2017-06-30 05:54:00,596 main.py:52] epoch 5447, training loss: 5397.67, average training loss: 5713.88, base loss: 15949.39
[INFO 2017-06-30 05:54:03,750 main.py:52] epoch 5448, training loss: 5466.67, average training loss: 5713.85, base loss: 15949.10
[INFO 2017-06-30 05:54:06,879 main.py:52] epoch 5449, training loss: 6045.64, average training loss: 5713.80, base loss: 15949.74
[INFO 2017-06-30 05:54:09,996 main.py:52] epoch 5450, training loss: 6018.47, average training loss: 5713.72, base loss: 15950.47
[INFO 2017-06-30 05:54:13,155 main.py:52] epoch 5451, training loss: 5518.03, average training loss: 5713.41, base loss: 15950.87
[INFO 2017-06-30 05:54:16,281 main.py:52] epoch 5452, training loss: 5559.52, average training loss: 5713.06, base loss: 15950.17
[INFO 2017-06-30 05:54:19,440 main.py:52] epoch 5453, training loss: 5711.57, average training loss: 5713.53, base loss: 15950.15
[INFO 2017-06-30 05:54:22,564 main.py:52] epoch 5454, training loss: 5891.21, average training loss: 5713.56, base loss: 15950.77
[INFO 2017-06-30 05:54:25,715 main.py:52] epoch 5455, training loss: 5478.61, average training loss: 5713.13, base loss: 15950.73
[INFO 2017-06-30 05:54:28,866 main.py:52] epoch 5456, training loss: 5866.58, average training loss: 5713.06, base loss: 15951.06
[INFO 2017-06-30 05:54:32,012 main.py:52] epoch 5457, training loss: 5603.39, average training loss: 5712.44, base loss: 15951.56
[INFO 2017-06-30 05:54:35,177 main.py:52] epoch 5458, training loss: 5981.27, average training loss: 5712.95, base loss: 15952.18
[INFO 2017-06-30 05:54:38,349 main.py:52] epoch 5459, training loss: 5465.42, average training loss: 5712.70, base loss: 15952.59
[INFO 2017-06-30 05:54:41,503 main.py:52] epoch 5460, training loss: 5800.17, average training loss: 5711.95, base loss: 15953.03
[INFO 2017-06-30 05:54:44,667 main.py:52] epoch 5461, training loss: 6114.88, average training loss: 5712.03, base loss: 15953.41
[INFO 2017-06-30 05:54:47,852 main.py:52] epoch 5462, training loss: 5463.40, average training loss: 5711.70, base loss: 15953.02
[INFO 2017-06-30 05:54:50,991 main.py:52] epoch 5463, training loss: 6127.37, average training loss: 5711.97, base loss: 15953.75
[INFO 2017-06-30 05:54:54,205 main.py:52] epoch 5464, training loss: 5910.80, average training loss: 5711.80, base loss: 15954.01
[INFO 2017-06-30 05:54:57,366 main.py:52] epoch 5465, training loss: 5322.33, average training loss: 5711.29, base loss: 15953.57
[INFO 2017-06-30 05:55:00,508 main.py:52] epoch 5466, training loss: 5687.63, average training loss: 5711.41, base loss: 15953.55
[INFO 2017-06-30 05:55:03,609 main.py:52] epoch 5467, training loss: 5655.34, average training loss: 5711.51, base loss: 15953.40
[INFO 2017-06-30 05:55:06,693 main.py:52] epoch 5468, training loss: 5760.34, average training loss: 5711.65, base loss: 15953.76
[INFO 2017-06-30 05:55:09,812 main.py:52] epoch 5469, training loss: 5637.14, average training loss: 5711.59, base loss: 15953.50
[INFO 2017-06-30 05:55:12,968 main.py:52] epoch 5470, training loss: 5581.39, average training loss: 5711.65, base loss: 15952.97
[INFO 2017-06-30 05:55:16,101 main.py:52] epoch 5471, training loss: 5783.54, average training loss: 5712.04, base loss: 15953.16
[INFO 2017-06-30 05:55:19,232 main.py:52] epoch 5472, training loss: 5360.85, average training loss: 5711.81, base loss: 15952.80
[INFO 2017-06-30 05:55:22,346 main.py:52] epoch 5473, training loss: 5818.06, average training loss: 5711.92, base loss: 15952.57
[INFO 2017-06-30 05:55:25,449 main.py:52] epoch 5474, training loss: 5805.80, average training loss: 5711.38, base loss: 15952.48
[INFO 2017-06-30 05:55:28,627 main.py:52] epoch 5475, training loss: 5384.69, average training loss: 5710.80, base loss: 15952.20
[INFO 2017-06-30 05:55:31,731 main.py:52] epoch 5476, training loss: 5905.41, average training loss: 5711.11, base loss: 15951.99
[INFO 2017-06-30 05:55:34,857 main.py:52] epoch 5477, training loss: 5650.07, average training loss: 5711.11, base loss: 15952.27
[INFO 2017-06-30 05:55:37,992 main.py:52] epoch 5478, training loss: 5623.79, average training loss: 5710.87, base loss: 15952.49
[INFO 2017-06-30 05:55:41,162 main.py:52] epoch 5479, training loss: 6131.98, average training loss: 5711.27, base loss: 15953.21
[INFO 2017-06-30 05:55:44,326 main.py:52] epoch 5480, training loss: 5346.35, average training loss: 5710.70, base loss: 15952.81
[INFO 2017-06-30 05:55:47,509 main.py:52] epoch 5481, training loss: 5479.40, average training loss: 5710.63, base loss: 15953.25
[INFO 2017-06-30 05:55:50,639 main.py:52] epoch 5482, training loss: 5752.10, average training loss: 5710.35, base loss: 15953.39
[INFO 2017-06-30 05:55:53,738 main.py:52] epoch 5483, training loss: 5728.99, average training loss: 5710.57, base loss: 15953.74
[INFO 2017-06-30 05:55:56,885 main.py:52] epoch 5484, training loss: 5766.87, average training loss: 5710.52, base loss: 15954.06
[INFO 2017-06-30 05:56:00,025 main.py:52] epoch 5485, training loss: 5854.63, average training loss: 5710.58, base loss: 15954.74
[INFO 2017-06-30 05:56:03,171 main.py:52] epoch 5486, training loss: 5937.70, average training loss: 5710.88, base loss: 15954.49
[INFO 2017-06-30 05:56:06,278 main.py:52] epoch 5487, training loss: 6054.66, average training loss: 5711.18, base loss: 15955.14
[INFO 2017-06-30 05:56:09,427 main.py:52] epoch 5488, training loss: 5802.30, average training loss: 5710.97, base loss: 15955.25
[INFO 2017-06-30 05:56:12,603 main.py:52] epoch 5489, training loss: 6182.82, average training loss: 5711.96, base loss: 15955.84
[INFO 2017-06-30 05:56:15,741 main.py:52] epoch 5490, training loss: 5537.62, average training loss: 5711.74, base loss: 15955.93
[INFO 2017-06-30 05:56:18,893 main.py:52] epoch 5491, training loss: 5801.68, average training loss: 5711.72, base loss: 15955.90
[INFO 2017-06-30 05:56:22,062 main.py:52] epoch 5492, training loss: 5628.50, average training loss: 5711.74, base loss: 15955.41
[INFO 2017-06-30 05:56:25,189 main.py:52] epoch 5493, training loss: 6354.71, average training loss: 5712.02, base loss: 15956.38
[INFO 2017-06-30 05:56:28,383 main.py:52] epoch 5494, training loss: 5421.39, average training loss: 5711.99, base loss: 15956.15
[INFO 2017-06-30 05:56:31,551 main.py:52] epoch 5495, training loss: 5530.39, average training loss: 5712.01, base loss: 15955.85
[INFO 2017-06-30 05:56:34,747 main.py:52] epoch 5496, training loss: 5920.59, average training loss: 5711.97, base loss: 15956.24
[INFO 2017-06-30 05:56:37,899 main.py:52] epoch 5497, training loss: 5690.88, average training loss: 5712.05, base loss: 15956.35
[INFO 2017-06-30 05:56:41,021 main.py:52] epoch 5498, training loss: 5658.90, average training loss: 5712.08, base loss: 15956.25
[INFO 2017-06-30 05:56:44,173 main.py:52] epoch 5499, training loss: 6085.57, average training loss: 5712.36, base loss: 15956.38
[INFO 2017-06-30 05:56:44,173 main.py:54] epoch 5499, testing
[INFO 2017-06-30 05:56:57,201 main.py:97] average testing loss: 5826.90, base loss: 16407.47
[INFO 2017-06-30 05:56:57,201 main.py:98] improve_loss: 10580.58, improve_percent: 0.64
[INFO 2017-06-30 05:56:57,202 main.py:66] current best improved percent: 0.65
[INFO 2017-06-30 05:57:00,375 main.py:52] epoch 5500, training loss: 5740.29, average training loss: 5712.20, base loss: 15956.20
[INFO 2017-06-30 05:57:03,548 main.py:52] epoch 5501, training loss: 6143.25, average training loss: 5712.58, base loss: 15956.66
[INFO 2017-06-30 05:57:06,777 main.py:52] epoch 5502, training loss: 6013.60, average training loss: 5713.02, base loss: 15957.51
[INFO 2017-06-30 05:57:09,922 main.py:52] epoch 5503, training loss: 5579.23, average training loss: 5712.16, base loss: 15957.62
[INFO 2017-06-30 05:57:13,044 main.py:52] epoch 5504, training loss: 5727.84, average training loss: 5711.92, base loss: 15957.38
[INFO 2017-06-30 05:57:16,190 main.py:52] epoch 5505, training loss: 5764.24, average training loss: 5711.96, base loss: 15956.84
[INFO 2017-06-30 05:57:19,340 main.py:52] epoch 5506, training loss: 5985.90, average training loss: 5711.61, base loss: 15956.91
[INFO 2017-06-30 05:57:22,449 main.py:52] epoch 5507, training loss: 6308.14, average training loss: 5712.29, base loss: 15958.09
[INFO 2017-06-30 05:57:25,581 main.py:52] epoch 5508, training loss: 5284.82, average training loss: 5711.87, base loss: 15957.06
[INFO 2017-06-30 05:57:28,710 main.py:52] epoch 5509, training loss: 6031.25, average training loss: 5712.60, base loss: 15957.06
[INFO 2017-06-30 05:57:31,849 main.py:52] epoch 5510, training loss: 5857.94, average training loss: 5712.32, base loss: 15957.40
[INFO 2017-06-30 05:57:34,999 main.py:52] epoch 5511, training loss: 5630.10, average training loss: 5711.98, base loss: 15957.16
[INFO 2017-06-30 05:57:38,144 main.py:52] epoch 5512, training loss: 5667.56, average training loss: 5712.21, base loss: 15957.77
[INFO 2017-06-30 05:57:41,289 main.py:52] epoch 5513, training loss: 5590.66, average training loss: 5712.14, base loss: 15957.71
[INFO 2017-06-30 05:57:44,434 main.py:52] epoch 5514, training loss: 5616.87, average training loss: 5712.34, base loss: 15957.58
[INFO 2017-06-30 05:57:47,602 main.py:52] epoch 5515, training loss: 5691.96, average training loss: 5712.21, base loss: 15957.81
[INFO 2017-06-30 05:57:50,748 main.py:52] epoch 5516, training loss: 5175.11, average training loss: 5711.80, base loss: 15957.01
[INFO 2017-06-30 05:57:53,945 main.py:52] epoch 5517, training loss: 5559.39, average training loss: 5711.77, base loss: 15956.45
[INFO 2017-06-30 05:57:57,115 main.py:52] epoch 5518, training loss: 6076.50, average training loss: 5712.45, base loss: 15957.20
[INFO 2017-06-30 05:58:00,221 main.py:52] epoch 5519, training loss: 5496.14, average training loss: 5711.84, base loss: 15957.34
[INFO 2017-06-30 05:58:03,357 main.py:52] epoch 5520, training loss: 6166.37, average training loss: 5712.11, base loss: 15958.00
[INFO 2017-06-30 05:58:06,477 main.py:52] epoch 5521, training loss: 6208.78, average training loss: 5712.36, base loss: 15958.87
[INFO 2017-06-30 05:58:09,624 main.py:52] epoch 5522, training loss: 5769.12, average training loss: 5712.19, base loss: 15958.89
[INFO 2017-06-30 05:58:12,785 main.py:52] epoch 5523, training loss: 5726.79, average training loss: 5712.26, base loss: 15959.03
[INFO 2017-06-30 05:58:15,939 main.py:52] epoch 5524, training loss: 6016.24, average training loss: 5712.75, base loss: 15959.08
[INFO 2017-06-30 05:58:19,031 main.py:52] epoch 5525, training loss: 5854.63, average training loss: 5713.29, base loss: 15959.40
[INFO 2017-06-30 05:58:22,212 main.py:52] epoch 5526, training loss: 5789.39, average training loss: 5713.18, base loss: 15959.35
[INFO 2017-06-30 05:58:25,355 main.py:52] epoch 5527, training loss: 5363.19, average training loss: 5713.02, base loss: 15958.89
[INFO 2017-06-30 05:58:28,499 main.py:52] epoch 5528, training loss: 5602.18, average training loss: 5712.57, base loss: 15959.31
[INFO 2017-06-30 05:58:31,633 main.py:52] epoch 5529, training loss: 5711.59, average training loss: 5712.14, base loss: 15959.55
[INFO 2017-06-30 05:58:34,779 main.py:52] epoch 5530, training loss: 5462.63, average training loss: 5712.10, base loss: 15958.71
[INFO 2017-06-30 05:58:37,976 main.py:52] epoch 5531, training loss: 5587.13, average training loss: 5711.95, base loss: 15958.18
[INFO 2017-06-30 05:58:41,097 main.py:52] epoch 5532, training loss: 6446.10, average training loss: 5712.66, base loss: 15959.07
[INFO 2017-06-30 05:58:44,284 main.py:52] epoch 5533, training loss: 5802.40, average training loss: 5712.46, base loss: 15959.02
[INFO 2017-06-30 05:58:47,456 main.py:52] epoch 5534, training loss: 5665.45, average training loss: 5712.30, base loss: 15959.04
[INFO 2017-06-30 05:58:50,606 main.py:52] epoch 5535, training loss: 5903.34, average training loss: 5712.33, base loss: 15959.42
[INFO 2017-06-30 05:58:53,741 main.py:52] epoch 5536, training loss: 5518.03, average training loss: 5712.16, base loss: 15959.27
[INFO 2017-06-30 05:58:56,897 main.py:52] epoch 5537, training loss: 5667.70, average training loss: 5711.91, base loss: 15959.12
[INFO 2017-06-30 05:59:00,069 main.py:52] epoch 5538, training loss: 5706.33, average training loss: 5711.90, base loss: 15959.23
[INFO 2017-06-30 05:59:03,229 main.py:52] epoch 5539, training loss: 5829.70, average training loss: 5712.08, base loss: 15959.38
[INFO 2017-06-30 05:59:06,385 main.py:52] epoch 5540, training loss: 5454.21, average training loss: 5711.82, base loss: 15959.00
[INFO 2017-06-30 05:59:09,522 main.py:52] epoch 5541, training loss: 5771.37, average training loss: 5711.67, base loss: 15959.23
[INFO 2017-06-30 05:59:12,641 main.py:52] epoch 5542, training loss: 5627.01, average training loss: 5711.55, base loss: 15959.07
[INFO 2017-06-30 05:59:15,777 main.py:52] epoch 5543, training loss: 5567.60, average training loss: 5711.17, base loss: 15959.10
[INFO 2017-06-30 05:59:18,934 main.py:52] epoch 5544, training loss: 5740.92, average training loss: 5711.15, base loss: 15959.32
[INFO 2017-06-30 05:59:22,040 main.py:52] epoch 5545, training loss: 5594.69, average training loss: 5711.32, base loss: 15959.01
[INFO 2017-06-30 05:59:25,180 main.py:52] epoch 5546, training loss: 6002.69, average training loss: 5711.20, base loss: 15959.59
[INFO 2017-06-30 05:59:28,316 main.py:52] epoch 5547, training loss: 5402.93, average training loss: 5711.06, base loss: 15959.34
[INFO 2017-06-30 05:59:31,461 main.py:52] epoch 5548, training loss: 5699.40, average training loss: 5710.87, base loss: 15958.76
[INFO 2017-06-30 05:59:34,637 main.py:52] epoch 5549, training loss: 5653.64, average training loss: 5710.50, base loss: 15958.41
[INFO 2017-06-30 05:59:37,787 main.py:52] epoch 5550, training loss: 5395.13, average training loss: 5709.48, base loss: 15958.27
[INFO 2017-06-30 05:59:40,899 main.py:52] epoch 5551, training loss: 5494.43, average training loss: 5709.27, base loss: 15958.19
[INFO 2017-06-30 05:59:44,081 main.py:52] epoch 5552, training loss: 5281.97, average training loss: 5708.50, base loss: 15957.36
[INFO 2017-06-30 05:59:47,214 main.py:52] epoch 5553, training loss: 5384.11, average training loss: 5708.45, base loss: 15956.96
[INFO 2017-06-30 05:59:50,306 main.py:52] epoch 5554, training loss: 5344.45, average training loss: 5708.16, base loss: 15956.54
[INFO 2017-06-30 05:59:53,417 main.py:52] epoch 5555, training loss: 5556.97, average training loss: 5708.27, base loss: 15956.73
[INFO 2017-06-30 05:59:56,589 main.py:52] epoch 5556, training loss: 6063.09, average training loss: 5708.60, base loss: 15957.35
[INFO 2017-06-30 05:59:59,753 main.py:52] epoch 5557, training loss: 5215.45, average training loss: 5707.50, base loss: 15956.83
[INFO 2017-06-30 06:00:02,898 main.py:52] epoch 5558, training loss: 5855.39, average training loss: 5707.59, base loss: 15957.04
[INFO 2017-06-30 06:00:06,043 main.py:52] epoch 5559, training loss: 5576.62, average training loss: 5707.34, base loss: 15956.92
[INFO 2017-06-30 06:00:09,155 main.py:52] epoch 5560, training loss: 5855.38, average training loss: 5707.43, base loss: 15957.33
[INFO 2017-06-30 06:00:12,295 main.py:52] epoch 5561, training loss: 5578.11, average training loss: 5707.38, base loss: 15957.78
[INFO 2017-06-30 06:00:15,407 main.py:52] epoch 5562, training loss: 5539.33, average training loss: 5707.29, base loss: 15957.50
[INFO 2017-06-30 06:00:18,548 main.py:52] epoch 5563, training loss: 5661.96, average training loss: 5707.26, base loss: 15957.39
[INFO 2017-06-30 06:00:21,650 main.py:52] epoch 5564, training loss: 5861.46, average training loss: 5707.73, base loss: 15957.97
[INFO 2017-06-30 06:00:24,792 main.py:52] epoch 5565, training loss: 5806.38, average training loss: 5707.76, base loss: 15958.41
[INFO 2017-06-30 06:00:27,965 main.py:52] epoch 5566, training loss: 5672.15, average training loss: 5707.89, base loss: 15958.82
[INFO 2017-06-30 06:00:31,107 main.py:52] epoch 5567, training loss: 5732.20, average training loss: 5707.85, base loss: 15958.60
[INFO 2017-06-30 06:00:34,314 main.py:52] epoch 5568, training loss: 5160.80, average training loss: 5706.63, base loss: 15957.97
[INFO 2017-06-30 06:00:37,473 main.py:52] epoch 5569, training loss: 6034.86, average training loss: 5707.04, base loss: 15958.08
[INFO 2017-06-30 06:00:40,623 main.py:52] epoch 5570, training loss: 5969.89, average training loss: 5707.45, base loss: 15957.73
[INFO 2017-06-30 06:00:43,767 main.py:52] epoch 5571, training loss: 5728.28, average training loss: 5707.30, base loss: 15957.58
[INFO 2017-06-30 06:00:46,908 main.py:52] epoch 5572, training loss: 5198.29, average training loss: 5706.70, base loss: 15957.21
[INFO 2017-06-30 06:00:50,038 main.py:52] epoch 5573, training loss: 5379.14, average training loss: 5706.01, base loss: 15956.47
[INFO 2017-06-30 06:00:53,164 main.py:52] epoch 5574, training loss: 5953.63, average training loss: 5705.75, base loss: 15957.13
[INFO 2017-06-30 06:00:56,269 main.py:52] epoch 5575, training loss: 5398.12, average training loss: 5705.52, base loss: 15957.28
[INFO 2017-06-30 06:00:59,367 main.py:52] epoch 5576, training loss: 5545.57, average training loss: 5704.95, base loss: 15957.36
[INFO 2017-06-30 06:01:02,520 main.py:52] epoch 5577, training loss: 5300.94, average training loss: 5704.51, base loss: 15957.54
[INFO 2017-06-30 06:01:05,683 main.py:52] epoch 5578, training loss: 5523.30, average training loss: 5704.04, base loss: 15957.35
[INFO 2017-06-30 06:01:08,786 main.py:52] epoch 5579, training loss: 5706.53, average training loss: 5703.62, base loss: 15957.67
[INFO 2017-06-30 06:01:11,891 main.py:52] epoch 5580, training loss: 6030.62, average training loss: 5704.04, base loss: 15958.14
[INFO 2017-06-30 06:01:15,011 main.py:52] epoch 5581, training loss: 5942.86, average training loss: 5704.17, base loss: 15958.57
[INFO 2017-06-30 06:01:18,105 main.py:52] epoch 5582, training loss: 5608.27, average training loss: 5704.19, base loss: 15958.53
[INFO 2017-06-30 06:01:21,226 main.py:52] epoch 5583, training loss: 5663.01, average training loss: 5703.97, base loss: 15958.58
[INFO 2017-06-30 06:01:24,347 main.py:52] epoch 5584, training loss: 5472.41, average training loss: 5703.37, base loss: 15958.69
[INFO 2017-06-30 06:01:27,478 main.py:52] epoch 5585, training loss: 5672.42, average training loss: 5703.12, base loss: 15958.57
[INFO 2017-06-30 06:01:30,632 main.py:52] epoch 5586, training loss: 5556.22, average training loss: 5703.20, base loss: 15958.64
[INFO 2017-06-30 06:01:33,770 main.py:52] epoch 5587, training loss: 6259.50, average training loss: 5703.94, base loss: 15958.92
[INFO 2017-06-30 06:01:36,909 main.py:52] epoch 5588, training loss: 5686.50, average training loss: 5704.00, base loss: 15958.70
[INFO 2017-06-30 06:01:40,063 main.py:52] epoch 5589, training loss: 5156.49, average training loss: 5703.45, base loss: 15958.14
[INFO 2017-06-30 06:01:43,178 main.py:52] epoch 5590, training loss: 5687.05, average training loss: 5702.83, base loss: 15957.84
[INFO 2017-06-30 06:01:46,304 main.py:52] epoch 5591, training loss: 5518.25, average training loss: 5702.32, base loss: 15957.60
[INFO 2017-06-30 06:01:49,463 main.py:52] epoch 5592, training loss: 5956.02, average training loss: 5702.47, base loss: 15958.05
[INFO 2017-06-30 06:01:52,630 main.py:52] epoch 5593, training loss: 6228.66, average training loss: 5702.30, base loss: 15958.50
[INFO 2017-06-30 06:01:55,742 main.py:52] epoch 5594, training loss: 5930.56, average training loss: 5702.33, base loss: 15959.22
[INFO 2017-06-30 06:01:58,844 main.py:52] epoch 5595, training loss: 5325.23, average training loss: 5701.75, base loss: 15959.28
[INFO 2017-06-30 06:02:01,974 main.py:52] epoch 5596, training loss: 5586.96, average training loss: 5701.44, base loss: 15958.52
[INFO 2017-06-30 06:02:05,118 main.py:52] epoch 5597, training loss: 5176.72, average training loss: 5701.08, base loss: 15958.08
[INFO 2017-06-30 06:02:08,230 main.py:52] epoch 5598, training loss: 5333.79, average training loss: 5701.11, base loss: 15957.54
[INFO 2017-06-30 06:02:11,369 main.py:52] epoch 5599, training loss: 5556.81, average training loss: 5701.03, base loss: 15957.51
[INFO 2017-06-30 06:02:11,370 main.py:54] epoch 5599, testing
[INFO 2017-06-30 06:02:24,384 main.py:97] average testing loss: 5840.35, base loss: 16589.78
[INFO 2017-06-30 06:02:24,385 main.py:98] improve_loss: 10749.43, improve_percent: 0.65
[INFO 2017-06-30 06:02:24,386 main.py:66] current best improved percent: 0.65
[INFO 2017-06-30 06:02:27,505 main.py:52] epoch 5600, training loss: 5519.74, average training loss: 5700.78, base loss: 15957.52
[INFO 2017-06-30 06:02:30,651 main.py:52] epoch 5601, training loss: 5463.97, average training loss: 5700.55, base loss: 15956.91
[INFO 2017-06-30 06:02:33,784 main.py:52] epoch 5602, training loss: 5824.42, average training loss: 5701.14, base loss: 15957.37
[INFO 2017-06-30 06:02:36,948 main.py:52] epoch 5603, training loss: 5704.92, average training loss: 5700.49, base loss: 15957.61
[INFO 2017-06-30 06:02:40,141 main.py:52] epoch 5604, training loss: 5760.11, average training loss: 5700.14, base loss: 15957.72
[INFO 2017-06-30 06:02:43,265 main.py:52] epoch 5605, training loss: 5679.05, average training loss: 5700.01, base loss: 15957.88
[INFO 2017-06-30 06:02:46,398 main.py:52] epoch 5606, training loss: 5717.04, average training loss: 5699.85, base loss: 15958.04
[INFO 2017-06-30 06:02:49,574 main.py:52] epoch 5607, training loss: 6212.37, average training loss: 5700.24, base loss: 15958.89
[INFO 2017-06-30 06:02:52,751 main.py:52] epoch 5608, training loss: 5699.67, average training loss: 5700.14, base loss: 15958.85
[INFO 2017-06-30 06:02:55,894 main.py:52] epoch 5609, training loss: 5733.57, average training loss: 5700.08, base loss: 15959.27
[INFO 2017-06-30 06:02:59,012 main.py:52] epoch 5610, training loss: 5588.97, average training loss: 5699.95, base loss: 15959.32
[INFO 2017-06-30 06:03:02,197 main.py:52] epoch 5611, training loss: 5890.73, average training loss: 5700.10, base loss: 15959.12
[INFO 2017-06-30 06:03:05,311 main.py:52] epoch 5612, training loss: 5526.86, average training loss: 5699.82, base loss: 15959.14
[INFO 2017-06-30 06:03:08,502 main.py:52] epoch 5613, training loss: 6042.92, average training loss: 5700.27, base loss: 15959.95
[INFO 2017-06-30 06:03:11,649 main.py:52] epoch 5614, training loss: 5655.14, average training loss: 5700.23, base loss: 15959.39
[INFO 2017-06-30 06:03:14,791 main.py:52] epoch 5615, training loss: 5534.38, average training loss: 5699.88, base loss: 15958.40
[INFO 2017-06-30 06:03:17,978 main.py:52] epoch 5616, training loss: 5554.70, average training loss: 5699.72, base loss: 15957.98
[INFO 2017-06-30 06:03:21,149 main.py:52] epoch 5617, training loss: 5307.29, average training loss: 5699.40, base loss: 15957.62
[INFO 2017-06-30 06:03:24,311 main.py:52] epoch 5618, training loss: 5853.16, average training loss: 5699.85, base loss: 15957.75
[INFO 2017-06-30 06:03:27,484 main.py:52] epoch 5619, training loss: 5958.67, average training loss: 5699.47, base loss: 15957.91
[INFO 2017-06-30 06:03:30,602 main.py:52] epoch 5620, training loss: 6153.64, average training loss: 5699.90, base loss: 15958.47
[INFO 2017-06-30 06:03:33,738 main.py:52] epoch 5621, training loss: 5662.17, average training loss: 5699.63, base loss: 15958.58
[INFO 2017-06-30 06:03:36,881 main.py:52] epoch 5622, training loss: 5423.25, average training loss: 5699.16, base loss: 15958.45
[INFO 2017-06-30 06:03:40,011 main.py:52] epoch 5623, training loss: 5628.45, average training loss: 5699.16, base loss: 15958.21
[INFO 2017-06-30 06:03:43,157 main.py:52] epoch 5624, training loss: 5468.77, average training loss: 5698.89, base loss: 15957.56
[INFO 2017-06-30 06:03:46,389 main.py:52] epoch 5625, training loss: 5405.88, average training loss: 5698.39, base loss: 15957.08
[INFO 2017-06-30 06:03:49,565 main.py:52] epoch 5626, training loss: 5441.17, average training loss: 5697.93, base loss: 15957.22
[INFO 2017-06-30 06:03:52,695 main.py:52] epoch 5627, training loss: 5755.53, average training loss: 5697.39, base loss: 15957.40
[INFO 2017-06-30 06:03:55,832 main.py:52] epoch 5628, training loss: 5977.55, average training loss: 5697.30, base loss: 15957.71
[INFO 2017-06-30 06:03:58,967 main.py:52] epoch 5629, training loss: 5934.94, average training loss: 5697.70, base loss: 15957.90
[INFO 2017-06-30 06:04:02,101 main.py:52] epoch 5630, training loss: 6099.53, average training loss: 5698.39, base loss: 15958.74
[INFO 2017-06-30 06:04:05,260 main.py:52] epoch 5631, training loss: 5588.94, average training loss: 5698.05, base loss: 15958.64
[INFO 2017-06-30 06:04:08,380 main.py:52] epoch 5632, training loss: 5464.45, average training loss: 5697.49, base loss: 15958.17
[INFO 2017-06-30 06:04:11,504 main.py:52] epoch 5633, training loss: 6053.28, average training loss: 5697.95, base loss: 15958.87
[INFO 2017-06-30 06:04:14,640 main.py:52] epoch 5634, training loss: 5700.31, average training loss: 5697.58, base loss: 15959.29
[INFO 2017-06-30 06:04:17,810 main.py:52] epoch 5635, training loss: 5443.76, average training loss: 5697.24, base loss: 15959.17
[INFO 2017-06-30 06:04:20,959 main.py:52] epoch 5636, training loss: 5603.24, average training loss: 5697.63, base loss: 15959.34
[INFO 2017-06-30 06:04:24,070 main.py:52] epoch 5637, training loss: 5539.74, average training loss: 5697.48, base loss: 15958.82
[INFO 2017-06-30 06:04:27,226 main.py:52] epoch 5638, training loss: 5537.19, average training loss: 5697.37, base loss: 15958.78
[INFO 2017-06-30 06:04:30,381 main.py:52] epoch 5639, training loss: 5686.98, average training loss: 5697.54, base loss: 15958.52
[INFO 2017-06-30 06:04:33,528 main.py:52] epoch 5640, training loss: 5833.68, average training loss: 5697.47, base loss: 15958.78
[INFO 2017-06-30 06:04:36,668 main.py:52] epoch 5641, training loss: 5794.57, average training loss: 5697.58, base loss: 15959.32
[INFO 2017-06-30 06:04:39,829 main.py:52] epoch 5642, training loss: 5388.44, average training loss: 5697.64, base loss: 15959.07
[INFO 2017-06-30 06:04:42,906 main.py:52] epoch 5643, training loss: 5779.62, average training loss: 5697.48, base loss: 15959.31
[INFO 2017-06-30 06:04:45,999 main.py:52] epoch 5644, training loss: 5520.85, average training loss: 5697.56, base loss: 15959.42
[INFO 2017-06-30 06:04:49,168 main.py:52] epoch 5645, training loss: 5562.36, average training loss: 5697.09, base loss: 15959.30
[INFO 2017-06-30 06:04:52,290 main.py:52] epoch 5646, training loss: 5913.97, average training loss: 5697.21, base loss: 15959.61
[INFO 2017-06-30 06:04:55,390 main.py:52] epoch 5647, training loss: 5761.01, average training loss: 5697.35, base loss: 15959.83
[INFO 2017-06-30 06:04:58,544 main.py:52] epoch 5648, training loss: 5682.01, average training loss: 5697.44, base loss: 15959.88
[INFO 2017-06-30 06:05:01,693 main.py:52] epoch 5649, training loss: 5421.57, average training loss: 5696.66, base loss: 15959.98
[INFO 2017-06-30 06:05:04,883 main.py:52] epoch 5650, training loss: 5858.04, average training loss: 5696.70, base loss: 15960.38
[INFO 2017-06-30 06:05:08,066 main.py:52] epoch 5651, training loss: 5654.22, average training loss: 5696.35, base loss: 15960.47
[INFO 2017-06-30 06:05:11,239 main.py:52] epoch 5652, training loss: 5463.50, average training loss: 5696.16, base loss: 15960.24
[INFO 2017-06-30 06:05:14,372 main.py:52] epoch 5653, training loss: 5377.81, average training loss: 5695.83, base loss: 15959.77
[INFO 2017-06-30 06:05:17,497 main.py:52] epoch 5654, training loss: 5703.88, average training loss: 5695.54, base loss: 15959.68
[INFO 2017-06-30 06:05:20,615 main.py:52] epoch 5655, training loss: 5893.48, average training loss: 5695.86, base loss: 15960.19
[INFO 2017-06-30 06:05:23,731 main.py:52] epoch 5656, training loss: 5593.67, average training loss: 5695.94, base loss: 15960.59
[INFO 2017-06-30 06:05:26,849 main.py:52] epoch 5657, training loss: 5504.84, average training loss: 5695.53, base loss: 15960.63
[INFO 2017-06-30 06:05:29,994 main.py:52] epoch 5658, training loss: 5122.30, average training loss: 5694.82, base loss: 15959.40
[INFO 2017-06-30 06:05:33,164 main.py:52] epoch 5659, training loss: 5955.13, average training loss: 5694.70, base loss: 15959.82
[INFO 2017-06-30 06:05:36,315 main.py:52] epoch 5660, training loss: 5351.55, average training loss: 5694.87, base loss: 15959.59
[INFO 2017-06-30 06:05:39,476 main.py:52] epoch 5661, training loss: 5690.63, average training loss: 5694.97, base loss: 15959.80
[INFO 2017-06-30 06:05:42,608 main.py:52] epoch 5662, training loss: 5424.05, average training loss: 5694.57, base loss: 15959.59
[INFO 2017-06-30 06:05:45,759 main.py:52] epoch 5663, training loss: 5751.77, average training loss: 5694.17, base loss: 15959.70
[INFO 2017-06-30 06:05:48,888 main.py:52] epoch 5664, training loss: 5616.63, average training loss: 5694.21, base loss: 15959.67
[INFO 2017-06-30 06:05:52,035 main.py:52] epoch 5665, training loss: 5444.00, average training loss: 5693.57, base loss: 15959.74
[INFO 2017-06-30 06:05:55,203 main.py:52] epoch 5666, training loss: 5191.85, average training loss: 5693.10, base loss: 15959.28
[INFO 2017-06-30 06:05:58,372 main.py:52] epoch 5667, training loss: 5346.52, average training loss: 5692.59, base loss: 15959.34
[INFO 2017-06-30 06:06:01,498 main.py:52] epoch 5668, training loss: 5768.03, average training loss: 5692.45, base loss: 15959.64
[INFO 2017-06-30 06:06:04,656 main.py:52] epoch 5669, training loss: 5910.08, average training loss: 5692.34, base loss: 15960.23
[INFO 2017-06-30 06:06:07,796 main.py:52] epoch 5670, training loss: 5607.76, average training loss: 5692.25, base loss: 15959.74
[INFO 2017-06-30 06:06:10,958 main.py:52] epoch 5671, training loss: 5783.70, average training loss: 5691.85, base loss: 15959.98
[INFO 2017-06-30 06:06:14,101 main.py:52] epoch 5672, training loss: 5366.90, average training loss: 5691.47, base loss: 15959.81
[INFO 2017-06-30 06:06:17,217 main.py:52] epoch 5673, training loss: 5879.90, average training loss: 5691.57, base loss: 15960.08
[INFO 2017-06-30 06:06:20,375 main.py:52] epoch 5674, training loss: 5466.69, average training loss: 5691.29, base loss: 15959.81
[INFO 2017-06-30 06:06:23,567 main.py:52] epoch 5675, training loss: 5948.26, average training loss: 5691.58, base loss: 15959.97
[INFO 2017-06-30 06:06:26,760 main.py:52] epoch 5676, training loss: 5832.47, average training loss: 5691.85, base loss: 15960.02
[INFO 2017-06-30 06:06:29,886 main.py:52] epoch 5677, training loss: 5507.15, average training loss: 5691.33, base loss: 15959.80
[INFO 2017-06-30 06:06:33,024 main.py:52] epoch 5678, training loss: 5407.85, average training loss: 5691.10, base loss: 15959.67
[INFO 2017-06-30 06:06:36,131 main.py:52] epoch 5679, training loss: 5901.11, average training loss: 5691.01, base loss: 15960.27
[INFO 2017-06-30 06:06:39,271 main.py:52] epoch 5680, training loss: 5172.20, average training loss: 5690.71, base loss: 15959.60
[INFO 2017-06-30 06:06:42,421 main.py:52] epoch 5681, training loss: 5865.67, average training loss: 5690.24, base loss: 15959.45
[INFO 2017-06-30 06:06:45,560 main.py:52] epoch 5682, training loss: 5510.34, average training loss: 5690.04, base loss: 15959.48
[INFO 2017-06-30 06:06:48,713 main.py:52] epoch 5683, training loss: 5592.60, average training loss: 5689.88, base loss: 15959.73
[INFO 2017-06-30 06:06:51,879 main.py:52] epoch 5684, training loss: 5751.19, average training loss: 5690.05, base loss: 15959.56
[INFO 2017-06-30 06:06:55,022 main.py:52] epoch 5685, training loss: 5726.48, average training loss: 5690.17, base loss: 15959.74
[INFO 2017-06-30 06:06:58,161 main.py:52] epoch 5686, training loss: 5483.30, average training loss: 5689.43, base loss: 15959.51
[INFO 2017-06-30 06:07:01,289 main.py:52] epoch 5687, training loss: 5606.03, average training loss: 5689.12, base loss: 15959.41
[INFO 2017-06-30 06:07:04,458 main.py:52] epoch 5688, training loss: 5402.23, average training loss: 5689.06, base loss: 15959.05
[INFO 2017-06-30 06:07:07,629 main.py:52] epoch 5689, training loss: 6168.10, average training loss: 5689.07, base loss: 15959.68
[INFO 2017-06-30 06:07:10,759 main.py:52] epoch 5690, training loss: 5358.31, average training loss: 5688.49, base loss: 15959.45
[INFO 2017-06-30 06:07:13,889 main.py:52] epoch 5691, training loss: 5528.97, average training loss: 5688.49, base loss: 15959.68
[INFO 2017-06-30 06:07:17,019 main.py:52] epoch 5692, training loss: 5688.01, average training loss: 5688.04, base loss: 15959.47
[INFO 2017-06-30 06:07:20,132 main.py:52] epoch 5693, training loss: 5375.29, average training loss: 5687.65, base loss: 15958.48
[INFO 2017-06-30 06:07:23,289 main.py:52] epoch 5694, training loss: 5446.75, average training loss: 5687.94, base loss: 15958.16
[INFO 2017-06-30 06:07:26,437 main.py:52] epoch 5695, training loss: 5968.44, average training loss: 5688.51, base loss: 15958.59
[INFO 2017-06-30 06:07:29,543 main.py:52] epoch 5696, training loss: 6009.90, average training loss: 5688.53, base loss: 15959.27
[INFO 2017-06-30 06:07:32,692 main.py:52] epoch 5697, training loss: 5489.41, average training loss: 5688.25, base loss: 15959.07
[INFO 2017-06-30 06:07:35,830 main.py:52] epoch 5698, training loss: 5512.45, average training loss: 5688.04, base loss: 15958.82
[INFO 2017-06-30 06:07:38,972 main.py:52] epoch 5699, training loss: 5112.08, average training loss: 5687.52, base loss: 15957.83
[INFO 2017-06-30 06:07:38,972 main.py:54] epoch 5699, testing
[INFO 2017-06-30 06:07:52,041 main.py:97] average testing loss: 5603.75, base loss: 15784.59
[INFO 2017-06-30 06:07:52,041 main.py:98] improve_loss: 10180.84, improve_percent: 0.64
[INFO 2017-06-30 06:07:52,043 main.py:66] current best improved percent: 0.65
[INFO 2017-06-30 06:07:55,221 main.py:52] epoch 5700, training loss: 5404.98, average training loss: 5686.53, base loss: 15957.52
[INFO 2017-06-30 06:07:58,369 main.py:52] epoch 5701, training loss: 5807.62, average training loss: 5687.17, base loss: 15957.58
[INFO 2017-06-30 06:08:01,514 main.py:52] epoch 5702, training loss: 5861.24, average training loss: 5687.53, base loss: 15956.99
[INFO 2017-06-30 06:08:04,611 main.py:52] epoch 5703, training loss: 5609.95, average training loss: 5687.18, base loss: 15956.73
[INFO 2017-06-30 06:08:07,785 main.py:52] epoch 5704, training loss: 5929.41, average training loss: 5687.05, base loss: 15956.58
[INFO 2017-06-30 06:08:10,952 main.py:52] epoch 5705, training loss: 5739.84, average training loss: 5686.92, base loss: 15956.04
[INFO 2017-06-30 06:08:14,077 main.py:52] epoch 5706, training loss: 5906.89, average training loss: 5687.30, base loss: 15955.93
[INFO 2017-06-30 06:08:17,252 main.py:52] epoch 5707, training loss: 5900.14, average training loss: 5687.15, base loss: 15956.60
[INFO 2017-06-30 06:08:20,396 main.py:52] epoch 5708, training loss: 6127.75, average training loss: 5687.53, base loss: 15956.82
[INFO 2017-06-30 06:08:23,512 main.py:52] epoch 5709, training loss: 5808.76, average training loss: 5687.32, base loss: 15956.56
[INFO 2017-06-30 06:08:26,663 main.py:52] epoch 5710, training loss: 5588.11, average training loss: 5687.03, base loss: 15956.40
[INFO 2017-06-30 06:08:29,813 main.py:52] epoch 5711, training loss: 5712.09, average training loss: 5686.87, base loss: 15956.34
[INFO 2017-06-30 06:08:32,955 main.py:52] epoch 5712, training loss: 5200.00, average training loss: 5685.99, base loss: 15956.02
[INFO 2017-06-30 06:08:36,116 main.py:52] epoch 5713, training loss: 5183.11, average training loss: 5685.43, base loss: 15955.36
[INFO 2017-06-30 06:08:39,237 main.py:52] epoch 5714, training loss: 5592.87, average training loss: 5685.51, base loss: 15954.96
[INFO 2017-06-30 06:08:42,384 main.py:52] epoch 5715, training loss: 5704.96, average training loss: 5685.39, base loss: 15955.16
[INFO 2017-06-30 06:08:45,557 main.py:52] epoch 5716, training loss: 5758.44, average training loss: 5685.84, base loss: 15956.05
[INFO 2017-06-30 06:08:48,717 main.py:52] epoch 5717, training loss: 5736.38, average training loss: 5686.28, base loss: 15956.41
[INFO 2017-06-30 06:08:51,859 main.py:52] epoch 5718, training loss: 5983.12, average training loss: 5686.56, base loss: 15956.46
[INFO 2017-06-30 06:08:54,999 main.py:52] epoch 5719, training loss: 5712.93, average training loss: 5686.46, base loss: 15956.66
[INFO 2017-06-30 06:08:58,132 main.py:52] epoch 5720, training loss: 5601.16, average training loss: 5686.00, base loss: 15956.97
[INFO 2017-06-30 06:09:01,291 main.py:52] epoch 5721, training loss: 5579.71, average training loss: 5685.67, base loss: 15957.10
[INFO 2017-06-30 06:09:04,407 main.py:52] epoch 5722, training loss: 5800.97, average training loss: 5686.12, base loss: 15957.27
[INFO 2017-06-30 06:09:07,520 main.py:52] epoch 5723, training loss: 5850.14, average training loss: 5686.29, base loss: 15957.41
[INFO 2017-06-30 06:09:10,648 main.py:52] epoch 5724, training loss: 5527.73, average training loss: 5686.38, base loss: 15957.37
[INFO 2017-06-30 06:09:13,780 main.py:52] epoch 5725, training loss: 5486.37, average training loss: 5686.08, base loss: 15957.28
[INFO 2017-06-30 06:09:16,901 main.py:52] epoch 5726, training loss: 5999.74, average training loss: 5686.21, base loss: 15957.33
[INFO 2017-06-30 06:09:20,088 main.py:52] epoch 5727, training loss: 5787.43, average training loss: 5686.28, base loss: 15957.41
[INFO 2017-06-30 06:09:23,229 main.py:52] epoch 5728, training loss: 6121.80, average training loss: 5687.01, base loss: 15957.83
[INFO 2017-06-30 06:09:26,378 main.py:52] epoch 5729, training loss: 5357.57, average training loss: 5686.53, base loss: 15957.30
[INFO 2017-06-30 06:09:29,494 main.py:52] epoch 5730, training loss: 5246.04, average training loss: 5686.14, base loss: 15956.50
[INFO 2017-06-30 06:09:32,606 main.py:52] epoch 5731, training loss: 5448.44, average training loss: 5686.11, base loss: 15955.81
[INFO 2017-06-30 06:09:35,762 main.py:52] epoch 5732, training loss: 5790.27, average training loss: 5686.51, base loss: 15955.81
[INFO 2017-06-30 06:09:38,903 main.py:52] epoch 5733, training loss: 5609.57, average training loss: 5686.28, base loss: 15956.01
[INFO 2017-06-30 06:09:42,031 main.py:52] epoch 5734, training loss: 5537.83, average training loss: 5686.38, base loss: 15955.83
[INFO 2017-06-30 06:09:45,151 main.py:52] epoch 5735, training loss: 5250.00, average training loss: 5685.79, base loss: 15955.47
[INFO 2017-06-30 06:09:48,287 main.py:52] epoch 5736, training loss: 5869.72, average training loss: 5685.79, base loss: 15955.97
[INFO 2017-06-30 06:09:51,469 main.py:52] epoch 5737, training loss: 5765.77, average training loss: 5685.83, base loss: 15956.09
[INFO 2017-06-30 06:09:54,609 main.py:52] epoch 5738, training loss: 5947.99, average training loss: 5686.62, base loss: 15956.98
[INFO 2017-06-30 06:09:57,745 main.py:52] epoch 5739, training loss: 5352.99, average training loss: 5686.38, base loss: 15957.05
[INFO 2017-06-30 06:10:00,871 main.py:52] epoch 5740, training loss: 5718.43, average training loss: 5686.52, base loss: 15956.71
[INFO 2017-06-30 06:10:04,009 main.py:52] epoch 5741, training loss: 5766.97, average training loss: 5686.57, base loss: 15956.73
[INFO 2017-06-30 06:10:07,165 main.py:52] epoch 5742, training loss: 5694.68, average training loss: 5686.26, base loss: 15956.76
[INFO 2017-06-30 06:10:10,322 main.py:52] epoch 5743, training loss: 5582.51, average training loss: 5686.11, base loss: 15956.66
[INFO 2017-06-30 06:10:13,478 main.py:52] epoch 5744, training loss: 5365.34, average training loss: 5686.04, base loss: 15956.66
[INFO 2017-06-30 06:10:16,642 main.py:52] epoch 5745, training loss: 5572.29, average training loss: 5686.09, base loss: 15956.42
[INFO 2017-06-30 06:10:19,781 main.py:52] epoch 5746, training loss: 5630.82, average training loss: 5686.06, base loss: 15956.81
[INFO 2017-06-30 06:10:22,952 main.py:52] epoch 5747, training loss: 5835.92, average training loss: 5686.15, base loss: 15957.08
[INFO 2017-06-30 06:10:26,057 main.py:52] epoch 5748, training loss: 5294.83, average training loss: 5686.07, base loss: 15956.62
[INFO 2017-06-30 06:10:29,225 main.py:52] epoch 5749, training loss: 5530.32, average training loss: 5685.90, base loss: 15956.90
[INFO 2017-06-30 06:10:32,346 main.py:52] epoch 5750, training loss: 5542.66, average training loss: 5685.91, base loss: 15956.73
[INFO 2017-06-30 06:10:35,444 main.py:52] epoch 5751, training loss: 5591.55, average training loss: 5686.05, base loss: 15956.41
[INFO 2017-06-30 06:10:38,594 main.py:52] epoch 5752, training loss: 5754.62, average training loss: 5686.18, base loss: 15956.63
[INFO 2017-06-30 06:10:41,726 main.py:52] epoch 5753, training loss: 5864.90, average training loss: 5685.83, base loss: 15957.05
[INFO 2017-06-30 06:10:44,886 main.py:52] epoch 5754, training loss: 5342.76, average training loss: 5685.62, base loss: 15956.91
[INFO 2017-06-30 06:10:48,027 main.py:52] epoch 5755, training loss: 5799.88, average training loss: 5685.65, base loss: 15956.69
[INFO 2017-06-30 06:10:51,167 main.py:52] epoch 5756, training loss: 5478.97, average training loss: 5685.46, base loss: 15956.49
[INFO 2017-06-30 06:10:54,268 main.py:52] epoch 5757, training loss: 5374.46, average training loss: 5685.38, base loss: 15956.01
[INFO 2017-06-30 06:10:57,409 main.py:52] epoch 5758, training loss: 5413.25, average training loss: 5685.28, base loss: 15956.17
[INFO 2017-06-30 06:11:00,524 main.py:52] epoch 5759, training loss: 5960.60, average training loss: 5685.01, base loss: 15956.69
[INFO 2017-06-30 06:11:03,639 main.py:52] epoch 5760, training loss: 5643.75, average training loss: 5685.01, base loss: 15956.37
[INFO 2017-06-30 06:11:06,723 main.py:52] epoch 5761, training loss: 5318.90, average training loss: 5684.66, base loss: 15956.03
[INFO 2017-06-30 06:11:09,827 main.py:52] epoch 5762, training loss: 5522.96, average training loss: 5684.11, base loss: 15955.79
[INFO 2017-06-30 06:11:12,947 main.py:52] epoch 5763, training loss: 5015.93, average training loss: 5683.53, base loss: 15954.93
[INFO 2017-06-30 06:11:16,084 main.py:52] epoch 5764, training loss: 5421.99, average training loss: 5683.21, base loss: 15954.90
[INFO 2017-06-30 06:11:19,243 main.py:52] epoch 5765, training loss: 5615.97, average training loss: 5683.13, base loss: 15955.00
[INFO 2017-06-30 06:11:22,376 main.py:52] epoch 5766, training loss: 4990.57, average training loss: 5682.46, base loss: 15954.36
[INFO 2017-06-30 06:11:25,580 main.py:52] epoch 5767, training loss: 6050.25, average training loss: 5682.93, base loss: 15955.27
[INFO 2017-06-30 06:11:28,696 main.py:52] epoch 5768, training loss: 5670.40, average training loss: 5683.09, base loss: 15955.40
[INFO 2017-06-30 06:11:31,801 main.py:52] epoch 5769, training loss: 5823.46, average training loss: 5683.25, base loss: 15955.39
[INFO 2017-06-30 06:11:34,960 main.py:52] epoch 5770, training loss: 5705.44, average training loss: 5683.25, base loss: 15955.52
[INFO 2017-06-30 06:11:38,113 main.py:52] epoch 5771, training loss: 5363.84, average training loss: 5682.57, base loss: 15955.06
[INFO 2017-06-30 06:11:41,259 main.py:52] epoch 5772, training loss: 5395.42, average training loss: 5682.16, base loss: 15954.93
[INFO 2017-06-30 06:11:44,375 main.py:52] epoch 5773, training loss: 5547.69, average training loss: 5681.86, base loss: 15954.65
[INFO 2017-06-30 06:11:47,530 main.py:52] epoch 5774, training loss: 5322.40, average training loss: 5681.25, base loss: 15954.20
[INFO 2017-06-30 06:11:50,690 main.py:52] epoch 5775, training loss: 5777.99, average training loss: 5681.40, base loss: 15954.11
[INFO 2017-06-30 06:11:53,830 main.py:52] epoch 5776, training loss: 6207.29, average training loss: 5681.72, base loss: 15954.93
[INFO 2017-06-30 06:11:57,011 main.py:52] epoch 5777, training loss: 5907.96, average training loss: 5682.34, base loss: 15955.17
[INFO 2017-06-30 06:12:00,159 main.py:52] epoch 5778, training loss: 5864.94, average training loss: 5682.41, base loss: 15955.02
[INFO 2017-06-30 06:12:03,306 main.py:52] epoch 5779, training loss: 5383.83, average training loss: 5681.87, base loss: 15954.71
[INFO 2017-06-30 06:12:06,450 main.py:52] epoch 5780, training loss: 5411.47, average training loss: 5681.76, base loss: 15954.27
[INFO 2017-06-30 06:12:09,571 main.py:52] epoch 5781, training loss: 5461.01, average training loss: 5681.23, base loss: 15954.32
[INFO 2017-06-30 06:12:12,727 main.py:52] epoch 5782, training loss: 5400.37, average training loss: 5680.69, base loss: 15954.58
[INFO 2017-06-30 06:12:15,902 main.py:52] epoch 5783, training loss: 5632.94, average training loss: 5680.69, base loss: 15955.15
[INFO 2017-06-30 06:12:19,063 main.py:52] epoch 5784, training loss: 5898.20, average training loss: 5680.56, base loss: 15955.74
[INFO 2017-06-30 06:12:22,176 main.py:52] epoch 5785, training loss: 5716.76, average training loss: 5680.71, base loss: 15956.06
[INFO 2017-06-30 06:12:25,274 main.py:52] epoch 5786, training loss: 5233.75, average training loss: 5680.23, base loss: 15955.31
[INFO 2017-06-30 06:12:28,396 main.py:52] epoch 5787, training loss: 5580.05, average training loss: 5680.35, base loss: 15955.23
[INFO 2017-06-30 06:12:31,513 main.py:52] epoch 5788, training loss: 5836.80, average training loss: 5680.08, base loss: 15955.30
[INFO 2017-06-30 06:12:34,678 main.py:52] epoch 5789, training loss: 5909.39, average training loss: 5680.39, base loss: 15955.58
[INFO 2017-06-30 06:12:37,766 main.py:52] epoch 5790, training loss: 5244.95, average training loss: 5680.17, base loss: 15955.29
[INFO 2017-06-30 06:12:40,922 main.py:52] epoch 5791, training loss: 5361.43, average training loss: 5679.64, base loss: 15955.23
[INFO 2017-06-30 06:12:44,085 main.py:52] epoch 5792, training loss: 5770.76, average training loss: 5679.72, base loss: 15955.50
[INFO 2017-06-30 06:12:47,250 main.py:52] epoch 5793, training loss: 5496.13, average training loss: 5679.90, base loss: 15955.40
[INFO 2017-06-30 06:12:50,402 main.py:52] epoch 5794, training loss: 5910.48, average training loss: 5680.33, base loss: 15955.52
[INFO 2017-06-30 06:12:53,520 main.py:52] epoch 5795, training loss: 6079.12, average training loss: 5680.52, base loss: 15955.48
[INFO 2017-06-30 06:12:56,688 main.py:52] epoch 5796, training loss: 5501.04, average training loss: 5680.30, base loss: 15955.41
[INFO 2017-06-30 06:12:59,910 main.py:52] epoch 5797, training loss: 6107.20, average training loss: 5681.15, base loss: 15956.07
[INFO 2017-06-30 06:13:03,082 main.py:52] epoch 5798, training loss: 5641.90, average training loss: 5681.16, base loss: 15955.90
[INFO 2017-06-30 06:13:06,255 main.py:52] epoch 5799, training loss: 5382.68, average training loss: 5681.13, base loss: 15955.63
[INFO 2017-06-30 06:13:06,255 main.py:54] epoch 5799, testing
[INFO 2017-06-30 06:13:19,265 main.py:97] average testing loss: 5483.85, base loss: 15875.21
[INFO 2017-06-30 06:13:19,265 main.py:98] improve_loss: 10391.35, improve_percent: 0.65
[INFO 2017-06-30 06:13:19,266 main.py:62] model save to ./model/final.pth
[INFO 2017-06-30 06:13:19,301 main.py:66] current best improved percent: 0.65
[INFO 2017-06-30 06:13:22,414 main.py:52] epoch 5800, training loss: 5759.49, average training loss: 5680.86, base loss: 15956.17
[INFO 2017-06-30 06:13:25,536 main.py:52] epoch 5801, training loss: 5337.20, average training loss: 5680.02, base loss: 15955.75
[INFO 2017-06-30 06:13:28,672 main.py:52] epoch 5802, training loss: 5763.01, average training loss: 5680.05, base loss: 15955.79
[INFO 2017-06-30 06:13:31,789 main.py:52] epoch 5803, training loss: 5709.69, average training loss: 5679.75, base loss: 15955.80
[INFO 2017-06-30 06:13:34,918 main.py:52] epoch 5804, training loss: 5470.62, average training loss: 5679.28, base loss: 15955.83
[INFO 2017-06-30 06:13:38,078 main.py:52] epoch 5805, training loss: 5310.05, average training loss: 5678.46, base loss: 15955.51
[INFO 2017-06-30 06:13:41,191 main.py:52] epoch 5806, training loss: 5466.54, average training loss: 5678.43, base loss: 15955.12
[INFO 2017-06-30 06:13:44,362 main.py:52] epoch 5807, training loss: 5492.69, average training loss: 5678.38, base loss: 15954.79
[INFO 2017-06-30 06:13:47,503 main.py:52] epoch 5808, training loss: 5607.89, average training loss: 5678.75, base loss: 15954.74
[INFO 2017-06-30 06:13:50,670 main.py:52] epoch 5809, training loss: 5590.62, average training loss: 5678.55, base loss: 15955.23
[INFO 2017-06-30 06:13:53,804 main.py:52] epoch 5810, training loss: 5365.87, average training loss: 5678.36, base loss: 15955.22
[INFO 2017-06-30 06:13:56,958 main.py:52] epoch 5811, training loss: 5645.70, average training loss: 5678.14, base loss: 15954.94
[INFO 2017-06-30 06:14:00,104 main.py:52] epoch 5812, training loss: 6009.08, average training loss: 5678.60, base loss: 15955.41
[INFO 2017-06-30 06:14:03,249 main.py:52] epoch 5813, training loss: 5510.64, average training loss: 5678.18, base loss: 15955.06
[INFO 2017-06-30 06:14:06,354 main.py:52] epoch 5814, training loss: 5607.07, average training loss: 5678.06, base loss: 15955.01
[INFO 2017-06-30 06:14:09,494 main.py:52] epoch 5815, training loss: 5459.56, average training loss: 5677.94, base loss: 15954.90
[INFO 2017-06-30 06:14:12,671 main.py:52] epoch 5816, training loss: 5621.32, average training loss: 5677.84, base loss: 15954.12
[INFO 2017-06-30 06:14:15,832 main.py:52] epoch 5817, training loss: 5157.52, average training loss: 5676.76, base loss: 15953.01
[INFO 2017-06-30 06:14:19,009 main.py:52] epoch 5818, training loss: 5392.08, average training loss: 5676.47, base loss: 15953.08
[INFO 2017-06-30 06:14:22,144 main.py:52] epoch 5819, training loss: 5667.32, average training loss: 5676.30, base loss: 15953.16
[INFO 2017-06-30 06:14:25,271 main.py:52] epoch 5820, training loss: 5774.70, average training loss: 5676.55, base loss: 15953.36
[INFO 2017-06-30 06:14:28,430 main.py:52] epoch 5821, training loss: 5566.60, average training loss: 5676.50, base loss: 15952.89
[INFO 2017-06-30 06:14:31,599 main.py:52] epoch 5822, training loss: 5887.44, average training loss: 5676.48, base loss: 15953.19
[INFO 2017-06-30 06:14:34,739 main.py:52] epoch 5823, training loss: 5623.21, average training loss: 5676.41, base loss: 15953.13
[INFO 2017-06-30 06:14:37,889 main.py:52] epoch 5824, training loss: 5648.65, average training loss: 5676.04, base loss: 15953.15
[INFO 2017-06-30 06:14:41,012 main.py:52] epoch 5825, training loss: 6110.72, average training loss: 5676.20, base loss: 15953.80
[INFO 2017-06-30 06:14:44,185 main.py:52] epoch 5826, training loss: 5716.92, average training loss: 5676.21, base loss: 15954.23
[INFO 2017-06-30 06:14:47,341 main.py:52] epoch 5827, training loss: 5903.41, average training loss: 5676.24, base loss: 15954.52
[INFO 2017-06-30 06:14:50,474 main.py:52] epoch 5828, training loss: 5911.90, average training loss: 5676.20, base loss: 15954.62
[INFO 2017-06-30 06:14:53,635 main.py:52] epoch 5829, training loss: 5923.77, average training loss: 5676.40, base loss: 15955.57
[INFO 2017-06-30 06:14:56,762 main.py:52] epoch 5830, training loss: 6126.73, average training loss: 5676.78, base loss: 15955.88
[INFO 2017-06-30 06:14:59,897 main.py:52] epoch 5831, training loss: 5653.40, average training loss: 5676.56, base loss: 15955.72
[INFO 2017-06-30 06:15:03,050 main.py:52] epoch 5832, training loss: 5672.53, average training loss: 5676.83, base loss: 15956.43
[INFO 2017-06-30 06:15:06,195 main.py:52] epoch 5833, training loss: 5712.98, average training loss: 5676.49, base loss: 15956.35
[INFO 2017-06-30 06:15:09,317 main.py:52] epoch 5834, training loss: 5463.87, average training loss: 5676.55, base loss: 15956.29
[INFO 2017-06-30 06:15:12,480 main.py:52] epoch 5835, training loss: 5300.69, average training loss: 5676.16, base loss: 15956.26
[INFO 2017-06-30 06:15:15,640 main.py:52] epoch 5836, training loss: 5580.26, average training loss: 5676.11, base loss: 15955.74
[INFO 2017-06-30 06:15:18,794 main.py:52] epoch 5837, training loss: 4937.77, average training loss: 5675.26, base loss: 15955.14
[INFO 2017-06-30 06:15:21,904 main.py:52] epoch 5838, training loss: 5429.63, average training loss: 5674.88, base loss: 15955.66
[INFO 2017-06-30 06:15:25,103 main.py:52] epoch 5839, training loss: 5714.62, average training loss: 5674.87, base loss: 15956.05
[INFO 2017-06-30 06:15:28,255 main.py:52] epoch 5840, training loss: 5810.16, average training loss: 5675.24, base loss: 15955.93
[INFO 2017-06-30 06:15:31,400 main.py:52] epoch 5841, training loss: 5734.00, average training loss: 5674.82, base loss: 15956.38
[INFO 2017-06-30 06:15:34,601 main.py:52] epoch 5842, training loss: 5539.14, average training loss: 5674.92, base loss: 15956.07
[INFO 2017-06-30 06:15:37,780 main.py:52] epoch 5843, training loss: 5403.62, average training loss: 5674.62, base loss: 15955.74
[INFO 2017-06-30 06:15:40,925 main.py:52] epoch 5844, training loss: 5866.96, average training loss: 5674.43, base loss: 15955.87
[INFO 2017-06-30 06:15:44,054 main.py:52] epoch 5845, training loss: 5519.23, average training loss: 5674.09, base loss: 15955.83
[INFO 2017-06-30 06:15:47,198 main.py:52] epoch 5846, training loss: 6049.95, average training loss: 5674.64, base loss: 15955.92
[INFO 2017-06-30 06:15:50,373 main.py:52] epoch 5847, training loss: 5510.97, average training loss: 5675.08, base loss: 15955.91
[INFO 2017-06-30 06:15:53,515 main.py:52] epoch 5848, training loss: 5456.60, average training loss: 5674.76, base loss: 15955.99
[INFO 2017-06-30 06:15:56,647 main.py:52] epoch 5849, training loss: 5796.50, average training loss: 5674.91, base loss: 15956.28
[INFO 2017-06-30 06:15:59,808 main.py:52] epoch 5850, training loss: 6209.46, average training loss: 5675.53, base loss: 15956.72
[INFO 2017-06-30 06:16:02,959 main.py:52] epoch 5851, training loss: 5610.93, average training loss: 5675.25, base loss: 15956.62
[INFO 2017-06-30 06:16:06,109 main.py:52] epoch 5852, training loss: 5751.07, average training loss: 5675.11, base loss: 15956.55
[INFO 2017-06-30 06:16:09,258 main.py:52] epoch 5853, training loss: 5734.45, average training loss: 5675.07, base loss: 15957.00
[INFO 2017-06-30 06:16:12,438 main.py:52] epoch 5854, training loss: 5855.94, average training loss: 5674.96, base loss: 15957.56
[INFO 2017-06-30 06:16:15,559 main.py:52] epoch 5855, training loss: 5721.85, average training loss: 5675.03, base loss: 15957.84
[INFO 2017-06-30 06:16:18,763 main.py:52] epoch 5856, training loss: 5278.45, average training loss: 5674.12, base loss: 15957.50
[INFO 2017-06-30 06:16:21,926 main.py:52] epoch 5857, training loss: 5265.37, average training loss: 5673.83, base loss: 15957.36
[INFO 2017-06-30 06:16:25,083 main.py:52] epoch 5858, training loss: 5699.09, average training loss: 5673.46, base loss: 15957.18
[INFO 2017-06-30 06:16:28,221 main.py:52] epoch 5859, training loss: 5476.15, average training loss: 5673.38, base loss: 15957.19
[INFO 2017-06-30 06:16:31,403 main.py:52] epoch 5860, training loss: 5611.75, average training loss: 5673.07, base loss: 15957.06
[INFO 2017-06-30 06:16:34,541 main.py:52] epoch 5861, training loss: 5679.75, average training loss: 5673.22, base loss: 15957.29
[INFO 2017-06-30 06:16:37,680 main.py:52] epoch 5862, training loss: 5804.33, average training loss: 5673.07, base loss: 15957.62
[INFO 2017-06-30 06:16:40,850 main.py:52] epoch 5863, training loss: 5615.44, average training loss: 5673.25, base loss: 15957.13
[INFO 2017-06-30 06:16:43,979 main.py:52] epoch 5864, training loss: 5302.64, average training loss: 5672.68, base loss: 15955.95
[INFO 2017-06-30 06:16:47,085 main.py:52] epoch 5865, training loss: 5988.44, average training loss: 5673.21, base loss: 15956.47
[INFO 2017-06-30 06:16:50,206 main.py:52] epoch 5866, training loss: 5754.86, average training loss: 5673.35, base loss: 15956.66
[INFO 2017-06-30 06:16:53,404 main.py:52] epoch 5867, training loss: 5350.75, average training loss: 5673.13, base loss: 15956.11
[INFO 2017-06-30 06:16:56,539 main.py:52] epoch 5868, training loss: 5487.52, average training loss: 5673.12, base loss: 15956.00
[INFO 2017-06-30 06:16:59,696 main.py:52] epoch 5869, training loss: 5993.61, average training loss: 5673.58, base loss: 15956.33
[INFO 2017-06-30 06:17:02,869 main.py:52] epoch 5870, training loss: 5670.96, average training loss: 5673.54, base loss: 15955.49
[INFO 2017-06-30 06:17:06,040 main.py:52] epoch 5871, training loss: 5462.92, average training loss: 5673.50, base loss: 15955.18
[INFO 2017-06-30 06:17:09,153 main.py:52] epoch 5872, training loss: 6107.25, average training loss: 5674.31, base loss: 15955.79
[INFO 2017-06-30 06:17:12,308 main.py:52] epoch 5873, training loss: 5481.43, average training loss: 5674.15, base loss: 15956.01
[INFO 2017-06-30 06:17:15,504 main.py:52] epoch 5874, training loss: 5576.63, average training loss: 5673.75, base loss: 15955.90
[INFO 2017-06-30 06:17:18,665 main.py:52] epoch 5875, training loss: 5387.77, average training loss: 5673.33, base loss: 15955.22
[INFO 2017-06-30 06:17:21,817 main.py:52] epoch 5876, training loss: 6026.45, average training loss: 5673.56, base loss: 15956.05
[INFO 2017-06-30 06:17:24,942 main.py:52] epoch 5877, training loss: 5654.36, average training loss: 5673.22, base loss: 15956.25
[INFO 2017-06-30 06:17:28,104 main.py:52] epoch 5878, training loss: 5346.24, average training loss: 5673.18, base loss: 15956.21
[INFO 2017-06-30 06:17:31,264 main.py:52] epoch 5879, training loss: 5337.35, average training loss: 5672.63, base loss: 15956.41
[INFO 2017-06-30 06:17:34,405 main.py:52] epoch 5880, training loss: 6054.08, average training loss: 5673.17, base loss: 15956.99
[INFO 2017-06-30 06:17:37,566 main.py:52] epoch 5881, training loss: 5695.49, average training loss: 5673.50, base loss: 15956.97
[INFO 2017-06-30 06:17:40,739 main.py:52] epoch 5882, training loss: 5617.53, average training loss: 5673.02, base loss: 15957.27
[INFO 2017-06-30 06:17:43,908 main.py:52] epoch 5883, training loss: 5509.82, average training loss: 5672.60, base loss: 15957.11
[INFO 2017-06-30 06:17:47,048 main.py:52] epoch 5884, training loss: 5761.77, average training loss: 5672.47, base loss: 15957.44
[INFO 2017-06-30 06:17:50,200 main.py:52] epoch 5885, training loss: 5882.46, average training loss: 5672.88, base loss: 15958.14
[INFO 2017-06-30 06:17:53,309 main.py:52] epoch 5886, training loss: 5536.53, average training loss: 5672.91, base loss: 15958.32
[INFO 2017-06-30 06:17:56,463 main.py:52] epoch 5887, training loss: 5638.41, average training loss: 5672.95, base loss: 15958.52
[INFO 2017-06-30 06:17:59,634 main.py:52] epoch 5888, training loss: 5464.09, average training loss: 5672.54, base loss: 15957.55
[INFO 2017-06-30 06:18:02,761 main.py:52] epoch 5889, training loss: 5710.40, average training loss: 5672.76, base loss: 15957.34
[INFO 2017-06-30 06:18:05,850 main.py:52] epoch 5890, training loss: 5493.05, average training loss: 5672.37, base loss: 15957.15
[INFO 2017-06-30 06:18:09,005 main.py:52] epoch 5891, training loss: 5695.55, average training loss: 5672.30, base loss: 15956.88
[INFO 2017-06-30 06:18:12,124 main.py:52] epoch 5892, training loss: 5731.69, average training loss: 5672.58, base loss: 15957.23
[INFO 2017-06-30 06:18:15,289 main.py:52] epoch 5893, training loss: 5188.61, average training loss: 5671.97, base loss: 15957.13
[INFO 2017-06-30 06:18:18,398 main.py:52] epoch 5894, training loss: 5503.27, average training loss: 5671.79, base loss: 15957.07
[INFO 2017-06-30 06:18:21,529 main.py:52] epoch 5895, training loss: 5609.83, average training loss: 5671.53, base loss: 15957.09
[INFO 2017-06-30 06:18:24,730 main.py:52] epoch 5896, training loss: 5609.92, average training loss: 5671.90, base loss: 15957.22
[INFO 2017-06-30 06:18:27,909 main.py:52] epoch 5897, training loss: 5603.19, average training loss: 5672.04, base loss: 15957.19
[INFO 2017-06-30 06:18:31,038 main.py:52] epoch 5898, training loss: 5716.08, average training loss: 5671.98, base loss: 15956.93
[INFO 2017-06-30 06:18:34,196 main.py:52] epoch 5899, training loss: 5949.76, average training loss: 5672.43, base loss: 15957.38
[INFO 2017-06-30 06:18:34,196 main.py:54] epoch 5899, testing
[INFO 2017-06-30 06:18:47,265 main.py:97] average testing loss: 5515.12, base loss: 15428.05
[INFO 2017-06-30 06:18:47,265 main.py:98] improve_loss: 9912.94, improve_percent: 0.64
[INFO 2017-06-30 06:18:47,267 main.py:66] current best improved percent: 0.65
[INFO 2017-06-30 06:18:50,405 main.py:52] epoch 5900, training loss: 5455.26, average training loss: 5672.29, base loss: 15956.79
[INFO 2017-06-30 06:18:53,533 main.py:52] epoch 5901, training loss: 5660.83, average training loss: 5672.61, base loss: 15956.76
[INFO 2017-06-30 06:18:56,648 main.py:52] epoch 5902, training loss: 5485.59, average training loss: 5672.57, base loss: 15956.54
[INFO 2017-06-30 06:18:59,803 main.py:52] epoch 5903, training loss: 5901.47, average training loss: 5673.20, base loss: 15956.91
[INFO 2017-06-30 06:19:03,001 main.py:52] epoch 5904, training loss: 5898.75, average training loss: 5673.54, base loss: 15957.01
[INFO 2017-06-30 06:19:06,132 main.py:52] epoch 5905, training loss: 5419.91, average training loss: 5673.46, base loss: 15956.77
[INFO 2017-06-30 06:19:09,291 main.py:52] epoch 5906, training loss: 5670.28, average training loss: 5673.64, base loss: 15956.90
[INFO 2017-06-30 06:19:12,450 main.py:52] epoch 5907, training loss: 5206.22, average training loss: 5673.10, base loss: 15956.71
[INFO 2017-06-30 06:19:15,627 main.py:52] epoch 5908, training loss: 5561.05, average training loss: 5673.21, base loss: 15956.87
[INFO 2017-06-30 06:19:18,804 main.py:52] epoch 5909, training loss: 5773.71, average training loss: 5673.37, base loss: 15957.44
[INFO 2017-06-30 06:19:22,005 main.py:52] epoch 5910, training loss: 6127.99, average training loss: 5674.09, base loss: 15958.19
[INFO 2017-06-30 06:19:25,125 main.py:52] epoch 5911, training loss: 5875.78, average training loss: 5674.26, base loss: 15958.75
[INFO 2017-06-30 06:19:28,283 main.py:52] epoch 5912, training loss: 5261.15, average training loss: 5673.68, base loss: 15958.01
[INFO 2017-06-30 06:19:31,395 main.py:52] epoch 5913, training loss: 5940.13, average training loss: 5673.88, base loss: 15958.19
[INFO 2017-06-30 06:19:34,511 main.py:52] epoch 5914, training loss: 5728.06, average training loss: 5673.51, base loss: 15958.70
[INFO 2017-06-30 06:19:37,676 main.py:52] epoch 5915, training loss: 5432.91, average training loss: 5673.52, base loss: 15958.84
[INFO 2017-06-30 06:19:40,817 main.py:52] epoch 5916, training loss: 5132.17, average training loss: 5673.49, base loss: 15958.62
[INFO 2017-06-30 06:19:43,916 main.py:52] epoch 5917, training loss: 5810.28, average training loss: 5673.78, base loss: 15958.70
[INFO 2017-06-30 06:19:47,060 main.py:52] epoch 5918, training loss: 5232.65, average training loss: 5673.35, base loss: 15958.19
[INFO 2017-06-30 06:19:50,239 main.py:52] epoch 5919, training loss: 5126.53, average training loss: 5673.04, base loss: 15957.62
[INFO 2017-06-30 06:19:53,378 main.py:52] epoch 5920, training loss: 5436.94, average training loss: 5672.83, base loss: 15957.63
[INFO 2017-06-30 06:19:56,522 main.py:52] epoch 5921, training loss: 5719.58, average training loss: 5672.35, base loss: 15957.40
[INFO 2017-06-30 06:19:59,662 main.py:52] epoch 5922, training loss: 5616.12, average training loss: 5672.43, base loss: 15957.49
[INFO 2017-06-30 06:20:02,811 main.py:52] epoch 5923, training loss: 5676.12, average training loss: 5672.00, base loss: 15957.85
[INFO 2017-06-30 06:20:05,993 main.py:52] epoch 5924, training loss: 5607.55, average training loss: 5671.57, base loss: 15957.67
[INFO 2017-06-30 06:20:09,108 main.py:52] epoch 5925, training loss: 5690.93, average training loss: 5671.85, base loss: 15957.10
[INFO 2017-06-30 06:20:12,251 main.py:52] epoch 5926, training loss: 6016.63, average training loss: 5672.01, base loss: 15957.43
[INFO 2017-06-30 06:20:15,388 main.py:52] epoch 5927, training loss: 5248.15, average training loss: 5670.87, base loss: 15957.15
[INFO 2017-06-30 06:20:18,557 main.py:52] epoch 5928, training loss: 5751.35, average training loss: 5671.11, base loss: 15957.22
[INFO 2017-06-30 06:20:21,705 main.py:52] epoch 5929, training loss: 5648.13, average training loss: 5670.90, base loss: 15957.30
[INFO 2017-06-30 06:20:24,813 main.py:52] epoch 5930, training loss: 5296.85, average training loss: 5670.46, base loss: 15956.54
[INFO 2017-06-30 06:20:27,903 main.py:52] epoch 5931, training loss: 5623.09, average training loss: 5669.70, base loss: 15956.67
[INFO 2017-06-30 06:20:31,005 main.py:52] epoch 5932, training loss: 5826.44, average training loss: 5670.15, base loss: 15957.19
[INFO 2017-06-30 06:20:34,122 main.py:52] epoch 5933, training loss: 5856.22, average training loss: 5670.35, base loss: 15957.58
[INFO 2017-06-30 06:20:37,304 main.py:52] epoch 5934, training loss: 5307.93, average training loss: 5668.96, base loss: 15957.57
[INFO 2017-06-30 06:20:40,431 main.py:52] epoch 5935, training loss: 5069.66, average training loss: 5668.44, base loss: 15957.48
[INFO 2017-06-30 06:20:43,598 main.py:52] epoch 5936, training loss: 5640.42, average training loss: 5668.29, base loss: 15957.65
[INFO 2017-06-30 06:20:46,736 main.py:52] epoch 5937, training loss: 5627.77, average training loss: 5668.33, base loss: 15957.43
[INFO 2017-06-30 06:20:49,872 main.py:52] epoch 5938, training loss: 5489.93, average training loss: 5668.41, base loss: 15957.23
[INFO 2017-06-30 06:20:53,027 main.py:52] epoch 5939, training loss: 5671.38, average training loss: 5668.50, base loss: 15957.35
[INFO 2017-06-30 06:20:56,147 main.py:52] epoch 5940, training loss: 5102.26, average training loss: 5667.86, base loss: 15956.81
[INFO 2017-06-30 06:20:59,244 main.py:52] epoch 5941, training loss: 5822.29, average training loss: 5668.19, base loss: 15957.47
[INFO 2017-06-30 06:21:02,436 main.py:52] epoch 5942, training loss: 5565.46, average training loss: 5668.36, base loss: 15957.61
[INFO 2017-06-30 06:21:05,611 main.py:52] epoch 5943, training loss: 5838.42, average training loss: 5668.79, base loss: 15957.72
[INFO 2017-06-30 06:21:08,761 main.py:52] epoch 5944, training loss: 5282.16, average training loss: 5667.74, base loss: 15957.11
[INFO 2017-06-30 06:21:11,959 main.py:52] epoch 5945, training loss: 5812.03, average training loss: 5667.92, base loss: 15956.94
[INFO 2017-06-30 06:21:15,109 main.py:52] epoch 5946, training loss: 5339.74, average training loss: 5667.85, base loss: 15957.14
[INFO 2017-06-30 06:21:18,250 main.py:52] epoch 5947, training loss: 5715.67, average training loss: 5667.87, base loss: 15957.57
[INFO 2017-06-30 06:21:21,406 main.py:52] epoch 5948, training loss: 5483.37, average training loss: 5667.42, base loss: 15957.79
[INFO 2017-06-30 06:21:24,552 main.py:52] epoch 5949, training loss: 5456.19, average training loss: 5667.33, base loss: 15958.14
[INFO 2017-06-30 06:21:27,713 main.py:52] epoch 5950, training loss: 4977.46, average training loss: 5666.90, base loss: 15957.47
[INFO 2017-06-30 06:21:30,857 main.py:52] epoch 5951, training loss: 5445.87, average training loss: 5666.55, base loss: 15957.01
[INFO 2017-06-30 06:21:34,003 main.py:52] epoch 5952, training loss: 5489.70, average training loss: 5666.36, base loss: 15957.08
[INFO 2017-06-30 06:21:37,115 main.py:52] epoch 5953, training loss: 6201.23, average training loss: 5666.71, base loss: 15957.92
[INFO 2017-06-30 06:21:40,273 main.py:52] epoch 5954, training loss: 5596.88, average training loss: 5666.85, base loss: 15958.46
[INFO 2017-06-30 06:21:43,448 main.py:52] epoch 5955, training loss: 5350.71, average training loss: 5666.14, base loss: 15958.43
[INFO 2017-06-30 06:21:46,563 main.py:52] epoch 5956, training loss: 5954.13, average training loss: 5666.12, base loss: 15958.74
[INFO 2017-06-30 06:21:49,664 main.py:52] epoch 5957, training loss: 5634.88, average training loss: 5665.99, base loss: 15959.07
[INFO 2017-06-30 06:21:52,793 main.py:52] epoch 5958, training loss: 5622.36, average training loss: 5666.16, base loss: 15958.59
[INFO 2017-06-30 06:21:55,972 main.py:52] epoch 5959, training loss: 5770.99, average training loss: 5665.96, base loss: 15958.04
[INFO 2017-06-30 06:21:59,099 main.py:52] epoch 5960, training loss: 5838.49, average training loss: 5665.52, base loss: 15958.40
[INFO 2017-06-30 06:22:02,276 main.py:52] epoch 5961, training loss: 5822.78, average training loss: 5665.44, base loss: 15959.01
[INFO 2017-06-30 06:22:05,429 main.py:52] epoch 5962, training loss: 5592.78, average training loss: 5665.29, base loss: 15958.86
[INFO 2017-06-30 06:22:08,544 main.py:52] epoch 5963, training loss: 5553.91, average training loss: 5665.21, base loss: 15959.10
[INFO 2017-06-30 06:22:11,699 main.py:52] epoch 5964, training loss: 5422.38, average training loss: 5665.20, base loss: 15958.80
[INFO 2017-06-30 06:22:14,830 main.py:52] epoch 5965, training loss: 5409.18, average training loss: 5664.72, base loss: 15958.39
[INFO 2017-06-30 06:22:17,975 main.py:52] epoch 5966, training loss: 5833.33, average training loss: 5665.06, base loss: 15958.29
[INFO 2017-06-30 06:22:21,073 main.py:52] epoch 5967, training loss: 5635.08, average training loss: 5665.05, base loss: 15958.28
[INFO 2017-06-30 06:22:24,221 main.py:52] epoch 5968, training loss: 5888.27, average training loss: 5665.25, base loss: 15958.53
[INFO 2017-06-30 06:22:27,357 main.py:52] epoch 5969, training loss: 6054.11, average training loss: 5665.63, base loss: 15958.96
[INFO 2017-06-30 06:22:30,520 main.py:52] epoch 5970, training loss: 5200.45, average training loss: 5665.41, base loss: 15958.12
[INFO 2017-06-30 06:22:33,661 main.py:52] epoch 5971, training loss: 5462.28, average training loss: 5665.31, base loss: 15957.82
[INFO 2017-06-30 06:22:36,793 main.py:52] epoch 5972, training loss: 5550.82, average training loss: 5664.46, base loss: 15958.01
[INFO 2017-06-30 06:22:39,923 main.py:52] epoch 5973, training loss: 6063.65, average training loss: 5664.66, base loss: 15958.00
[INFO 2017-06-30 06:22:43,081 main.py:52] epoch 5974, training loss: 5803.45, average training loss: 5664.74, base loss: 15958.44
[INFO 2017-06-30 06:22:46,297 main.py:52] epoch 5975, training loss: 5587.55, average training loss: 5664.40, base loss: 15958.64
[INFO 2017-06-30 06:22:49,447 main.py:52] epoch 5976, training loss: 5675.22, average training loss: 5664.40, base loss: 15958.87
[INFO 2017-06-30 06:22:52,566 main.py:52] epoch 5977, training loss: 5672.12, average training loss: 5664.26, base loss: 15959.23
[INFO 2017-06-30 06:22:55,729 main.py:52] epoch 5978, training loss: 5971.50, average training loss: 5664.89, base loss: 15959.47
[INFO 2017-06-30 06:22:58,832 main.py:52] epoch 5979, training loss: 5495.33, average training loss: 5664.70, base loss: 15959.44
[INFO 2017-06-30 06:23:01,942 main.py:52] epoch 5980, training loss: 5459.71, average training loss: 5664.31, base loss: 15958.91
[INFO 2017-06-30 06:23:05,061 main.py:52] epoch 5981, training loss: 5690.19, average training loss: 5664.24, base loss: 15958.96
[INFO 2017-06-30 06:23:08,225 main.py:52] epoch 5982, training loss: 5869.19, average training loss: 5665.01, base loss: 15959.11
[INFO 2017-06-30 06:23:11,329 main.py:52] epoch 5983, training loss: 5610.62, average training loss: 5665.41, base loss: 15958.89
[INFO 2017-06-30 06:23:14,464 main.py:52] epoch 5984, training loss: 5897.01, average training loss: 5665.88, base loss: 15959.03
[INFO 2017-06-30 06:23:17,610 main.py:52] epoch 5985, training loss: 5077.73, average training loss: 5665.26, base loss: 15958.69
[INFO 2017-06-30 06:23:20,746 main.py:52] epoch 5986, training loss: 5614.62, average training loss: 5664.73, base loss: 15958.18
[INFO 2017-06-30 06:23:23,848 main.py:52] epoch 5987, training loss: 5363.25, average training loss: 5664.39, base loss: 15957.35
[INFO 2017-06-30 06:23:26,985 main.py:52] epoch 5988, training loss: 5998.87, average training loss: 5665.03, base loss: 15957.81
[INFO 2017-06-30 06:23:30,123 main.py:52] epoch 5989, training loss: 5394.90, average training loss: 5664.58, base loss: 15957.61
[INFO 2017-06-30 06:23:33,287 main.py:52] epoch 5990, training loss: 5693.49, average training loss: 5664.55, base loss: 15957.64
[INFO 2017-06-30 06:23:36,436 main.py:52] epoch 5991, training loss: 5439.67, average training loss: 5664.47, base loss: 15957.12
[INFO 2017-06-30 06:23:39,600 main.py:52] epoch 5992, training loss: 5780.65, average training loss: 5664.71, base loss: 15956.85
[INFO 2017-06-30 06:23:42,799 main.py:52] epoch 5993, training loss: 5604.80, average training loss: 5664.58, base loss: 15956.84
[INFO 2017-06-30 06:23:45,912 main.py:52] epoch 5994, training loss: 5928.45, average training loss: 5664.82, base loss: 15956.88
[INFO 2017-06-30 06:23:49,079 main.py:52] epoch 5995, training loss: 5588.55, average training loss: 5664.65, base loss: 15956.15
[INFO 2017-06-30 06:23:52,247 main.py:52] epoch 5996, training loss: 5508.51, average training loss: 5664.65, base loss: 15956.35
[INFO 2017-06-30 06:23:55,381 main.py:52] epoch 5997, training loss: 6033.64, average training loss: 5664.94, base loss: 15956.95
[INFO 2017-06-30 06:23:58,517 main.py:52] epoch 5998, training loss: 5409.76, average training loss: 5664.77, base loss: 15956.49
[INFO 2017-06-30 06:24:01,665 main.py:52] epoch 5999, training loss: 5901.53, average training loss: 5664.88, base loss: 15956.58
[INFO 2017-06-30 06:24:01,665 main.py:54] epoch 5999, testing
[INFO 2017-06-30 06:24:14,672 main.py:97] average testing loss: 5666.04, base loss: 16204.82
[INFO 2017-06-30 06:24:14,672 main.py:98] improve_loss: 10538.79, improve_percent: 0.65
[INFO 2017-06-30 06:24:14,675 main.py:66] current best improved percent: 0.65
[INFO 2017-06-30 06:24:17,792 main.py:52] epoch 6000, training loss: 5500.23, average training loss: 5664.80, base loss: 15956.43
[INFO 2017-06-30 06:24:20,924 main.py:52] epoch 6001, training loss: 5510.78, average training loss: 5664.71, base loss: 15956.29
[INFO 2017-06-30 06:24:24,093 main.py:52] epoch 6002, training loss: 5693.47, average training loss: 5664.55, base loss: 15956.29
[INFO 2017-06-30 06:24:27,261 main.py:52] epoch 6003, training loss: 5349.09, average training loss: 5664.20, base loss: 15955.85
[INFO 2017-06-30 06:24:30,349 main.py:52] epoch 6004, training loss: 5570.10, average training loss: 5664.21, base loss: 15956.15
[INFO 2017-06-30 06:24:33,512 main.py:52] epoch 6005, training loss: 5356.47, average training loss: 5664.22, base loss: 15956.00
[INFO 2017-06-30 06:24:36,728 main.py:52] epoch 6006, training loss: 5974.94, average training loss: 5664.67, base loss: 15956.78
[INFO 2017-06-30 06:24:39,849 main.py:52] epoch 6007, training loss: 5934.31, average training loss: 5665.11, base loss: 15956.75
[INFO 2017-06-30 06:24:42,974 main.py:52] epoch 6008, training loss: 5448.66, average training loss: 5664.97, base loss: 15956.97
[INFO 2017-06-30 06:24:46,134 main.py:52] epoch 6009, training loss: 5522.97, average training loss: 5664.74, base loss: 15957.39
[INFO 2017-06-30 06:24:49,262 main.py:52] epoch 6010, training loss: 5732.69, average training loss: 5665.26, base loss: 15957.44
[INFO 2017-06-30 06:24:52,408 main.py:52] epoch 6011, training loss: 5468.30, average training loss: 5665.58, base loss: 15957.20
[INFO 2017-06-30 06:24:55,550 main.py:52] epoch 6012, training loss: 5577.30, average training loss: 5665.21, base loss: 15956.96
[INFO 2017-06-30 06:24:58,706 main.py:52] epoch 6013, training loss: 5944.30, average training loss: 5665.71, base loss: 15957.19
[INFO 2017-06-30 06:25:01,848 main.py:52] epoch 6014, training loss: 5549.54, average training loss: 5665.80, base loss: 15957.15
[INFO 2017-06-30 06:25:05,019 main.py:52] epoch 6015, training loss: 5667.62, average training loss: 5665.45, base loss: 15957.25
[INFO 2017-06-30 06:25:08,138 main.py:52] epoch 6016, training loss: 5390.04, average training loss: 5665.18, base loss: 15956.77
[INFO 2017-06-30 06:25:11,288 main.py:52] epoch 6017, training loss: 5554.67, average training loss: 5664.85, base loss: 15956.53
[INFO 2017-06-30 06:25:14,405 main.py:52] epoch 6018, training loss: 5795.37, average training loss: 5664.82, base loss: 15956.70
[INFO 2017-06-30 06:25:17,565 main.py:52] epoch 6019, training loss: 5642.76, average training loss: 5664.82, base loss: 15956.60
[INFO 2017-06-30 06:25:20,721 main.py:52] epoch 6020, training loss: 5355.27, average training loss: 5664.76, base loss: 15956.74
[INFO 2017-06-30 06:25:23,867 main.py:52] epoch 6021, training loss: 5746.33, average training loss: 5664.77, base loss: 15956.97
[INFO 2017-06-30 06:25:27,017 main.py:52] epoch 6022, training loss: 5624.05, average training loss: 5664.51, base loss: 15957.52
[INFO 2017-06-30 06:25:30,224 main.py:52] epoch 6023, training loss: 5472.14, average training loss: 5663.95, base loss: 15957.50
[INFO 2017-06-30 06:25:33,414 main.py:52] epoch 6024, training loss: 5391.05, average training loss: 5664.06, base loss: 15957.42
[INFO 2017-06-30 06:25:36,582 main.py:52] epoch 6025, training loss: 5410.62, average training loss: 5663.89, base loss: 15957.23
[INFO 2017-06-30 06:25:39,733 main.py:52] epoch 6026, training loss: 6073.21, average training loss: 5664.46, base loss: 15957.89
[INFO 2017-06-30 06:25:42,897 main.py:52] epoch 6027, training loss: 5623.47, average training loss: 5664.53, base loss: 15957.66
[INFO 2017-06-30 06:25:46,004 main.py:52] epoch 6028, training loss: 5449.51, average training loss: 5664.16, base loss: 15957.35
[INFO 2017-06-30 06:25:49,142 main.py:52] epoch 6029, training loss: 5708.30, average training loss: 5664.45, base loss: 15957.57
[INFO 2017-06-30 06:25:52,279 main.py:52] epoch 6030, training loss: 5191.40, average training loss: 5664.61, base loss: 15956.79
[INFO 2017-06-30 06:25:55,418 main.py:52] epoch 6031, training loss: 5310.69, average training loss: 5663.95, base loss: 15956.10
[INFO 2017-06-30 06:25:58,550 main.py:52] epoch 6032, training loss: 5447.92, average training loss: 5663.68, base loss: 15956.11
[INFO 2017-06-30 06:26:01,694 main.py:52] epoch 6033, training loss: 5760.86, average training loss: 5663.81, base loss: 15956.42
[INFO 2017-06-30 06:26:04,846 main.py:52] epoch 6034, training loss: 5450.19, average training loss: 5663.12, base loss: 15956.42
[INFO 2017-06-30 06:26:07,957 main.py:52] epoch 6035, training loss: 5627.42, average training loss: 5663.04, base loss: 15956.71
[INFO 2017-06-30 06:26:11,089 main.py:52] epoch 6036, training loss: 5505.96, average training loss: 5663.14, base loss: 15956.97
[INFO 2017-06-30 06:26:14,218 main.py:52] epoch 6037, training loss: 6025.11, average training loss: 5663.86, base loss: 15957.23
[INFO 2017-06-30 06:26:17,348 main.py:52] epoch 6038, training loss: 5612.03, average training loss: 5663.69, base loss: 15956.64
[INFO 2017-06-30 06:26:20,536 main.py:52] epoch 6039, training loss: 5642.55, average training loss: 5663.74, base loss: 15956.71
[INFO 2017-06-30 06:26:23,701 main.py:52] epoch 6040, training loss: 5676.37, average training loss: 5663.81, base loss: 15956.89
[INFO 2017-06-30 06:26:26,886 main.py:52] epoch 6041, training loss: 5792.32, average training loss: 5663.63, base loss: 15957.05
[INFO 2017-06-30 06:26:30,056 main.py:52] epoch 6042, training loss: 5162.59, average training loss: 5663.18, base loss: 15956.72
[INFO 2017-06-30 06:26:33,243 main.py:52] epoch 6043, training loss: 5492.13, average training loss: 5663.20, base loss: 15956.61
[INFO 2017-06-30 06:26:36,421 main.py:52] epoch 6044, training loss: 5911.75, average training loss: 5662.95, base loss: 15957.54
[INFO 2017-06-30 06:26:39,606 main.py:52] epoch 6045, training loss: 5378.49, average training loss: 5662.48, base loss: 15957.67
[INFO 2017-06-30 06:26:42,761 main.py:52] epoch 6046, training loss: 5930.24, average training loss: 5663.12, base loss: 15957.65
[INFO 2017-06-30 06:26:45,914 main.py:52] epoch 6047, training loss: 5382.64, average training loss: 5662.77, base loss: 15957.35
[INFO 2017-06-30 06:26:49,029 main.py:52] epoch 6048, training loss: 5541.75, average training loss: 5662.89, base loss: 15957.71
[INFO 2017-06-30 06:26:52,141 main.py:52] epoch 6049, training loss: 6162.72, average training loss: 5662.71, base loss: 15958.42
[INFO 2017-06-30 06:26:55,258 main.py:52] epoch 6050, training loss: 5563.51, average training loss: 5662.42, base loss: 15958.58
[INFO 2017-06-30 06:26:58,349 main.py:52] epoch 6051, training loss: 5407.38, average training loss: 5661.59, base loss: 15958.53
[INFO 2017-06-30 06:27:01,459 main.py:52] epoch 6052, training loss: 5653.76, average training loss: 5661.85, base loss: 15958.10
[INFO 2017-06-30 06:27:04,607 main.py:52] epoch 6053, training loss: 5537.47, average training loss: 5661.54, base loss: 15957.84
[INFO 2017-06-30 06:27:07,736 main.py:52] epoch 6054, training loss: 5757.01, average training loss: 5661.60, base loss: 15958.17
[INFO 2017-06-30 06:27:10,872 main.py:52] epoch 6055, training loss: 5389.37, average training loss: 5661.52, base loss: 15957.88
[INFO 2017-06-30 06:27:13,952 main.py:52] epoch 6056, training loss: 5299.05, average training loss: 5660.92, base loss: 15957.45
[INFO 2017-06-30 06:27:17,068 main.py:52] epoch 6057, training loss: 5493.78, average training loss: 5660.78, base loss: 15957.12
[INFO 2017-06-30 06:27:20,234 main.py:52] epoch 6058, training loss: 5475.62, average training loss: 5660.06, base loss: 15956.75
[INFO 2017-06-30 06:27:23,351 main.py:52] epoch 6059, training loss: 5641.15, average training loss: 5660.02, base loss: 15956.38
[INFO 2017-06-30 06:27:26,479 main.py:52] epoch 6060, training loss: 6021.36, average training loss: 5660.22, base loss: 15957.12
[INFO 2017-06-30 06:27:29,622 main.py:52] epoch 6061, training loss: 5444.46, average training loss: 5659.78, base loss: 15956.83
[INFO 2017-06-30 06:27:32,779 main.py:52] epoch 6062, training loss: 5626.53, average training loss: 5659.66, base loss: 15957.03
[INFO 2017-06-30 06:27:35,923 main.py:52] epoch 6063, training loss: 5860.26, average training loss: 5659.94, base loss: 15957.35
[INFO 2017-06-30 06:27:39,052 main.py:52] epoch 6064, training loss: 5661.44, average training loss: 5659.92, base loss: 15957.42
[INFO 2017-06-30 06:27:42,184 main.py:52] epoch 6065, training loss: 5537.24, average training loss: 5659.50, base loss: 15957.22
[INFO 2017-06-30 06:27:45,342 main.py:52] epoch 6066, training loss: 5354.02, average training loss: 5659.60, base loss: 15957.29
[INFO 2017-06-30 06:27:48,519 main.py:52] epoch 6067, training loss: 5310.56, average training loss: 5659.23, base loss: 15957.01
[INFO 2017-06-30 06:27:51,647 main.py:52] epoch 6068, training loss: 5662.19, average training loss: 5659.34, base loss: 15956.53
[INFO 2017-06-30 06:27:54,791 main.py:52] epoch 6069, training loss: 5378.57, average training loss: 5659.11, base loss: 15956.70
[INFO 2017-06-30 06:27:57,929 main.py:52] epoch 6070, training loss: 5363.64, average training loss: 5658.59, base loss: 15956.63
[INFO 2017-06-30 06:28:01,081 main.py:52] epoch 6071, training loss: 5378.79, average training loss: 5658.27, base loss: 15956.44
[INFO 2017-06-30 06:28:04,197 main.py:52] epoch 6072, training loss: 5700.83, average training loss: 5658.53, base loss: 15956.74
[INFO 2017-06-30 06:28:07,341 main.py:52] epoch 6073, training loss: 5595.95, average training loss: 5658.48, base loss: 15957.19
[INFO 2017-06-30 06:28:10,485 main.py:52] epoch 6074, training loss: 5532.59, average training loss: 5658.64, base loss: 15957.56
[INFO 2017-06-30 06:28:13,657 main.py:52] epoch 6075, training loss: 5352.59, average training loss: 5658.30, base loss: 15957.67
[INFO 2017-06-30 06:28:16,809 main.py:52] epoch 6076, training loss: 5544.66, average training loss: 5658.12, base loss: 15957.17
[INFO 2017-06-30 06:28:19,974 main.py:52] epoch 6077, training loss: 5616.21, average training loss: 5658.07, base loss: 15956.94
[INFO 2017-06-30 06:28:23,105 main.py:52] epoch 6078, training loss: 5605.48, average training loss: 5658.13, base loss: 15956.99
[INFO 2017-06-30 06:28:26,207 main.py:52] epoch 6079, training loss: 5715.70, average training loss: 5657.87, base loss: 15957.44
[INFO 2017-06-30 06:28:29,358 main.py:52] epoch 6080, training loss: 6002.10, average training loss: 5658.21, base loss: 15957.88
[INFO 2017-06-30 06:28:32,502 main.py:52] epoch 6081, training loss: 5136.75, average training loss: 5657.78, base loss: 15957.15
[INFO 2017-06-30 06:28:35,639 main.py:52] epoch 6082, training loss: 5424.27, average training loss: 5658.13, base loss: 15956.89
[INFO 2017-06-30 06:28:38,816 main.py:52] epoch 6083, training loss: 5517.05, average training loss: 5657.70, base loss: 15956.53
[INFO 2017-06-30 06:28:41,981 main.py:52] epoch 6084, training loss: 5945.76, average training loss: 5657.89, base loss: 15956.96
[INFO 2017-06-30 06:28:45,106 main.py:52] epoch 6085, training loss: 5537.16, average training loss: 5657.68, base loss: 15957.06
[INFO 2017-06-30 06:28:48,272 main.py:52] epoch 6086, training loss: 5318.14, average training loss: 5657.43, base loss: 15956.96
[INFO 2017-06-30 06:28:51,416 main.py:52] epoch 6087, training loss: 5776.49, average training loss: 5657.62, base loss: 15957.55
[INFO 2017-06-30 06:28:54,544 main.py:52] epoch 6088, training loss: 5370.12, average training loss: 5657.15, base loss: 15957.38
[INFO 2017-06-30 06:28:57,775 main.py:52] epoch 6089, training loss: 5344.52, average training loss: 5656.57, base loss: 15957.16
[INFO 2017-06-30 06:29:00,898 main.py:52] epoch 6090, training loss: 5472.66, average training loss: 5656.43, base loss: 15957.32
[INFO 2017-06-30 06:29:04,026 main.py:52] epoch 6091, training loss: 5495.44, average training loss: 5656.54, base loss: 15957.50
[INFO 2017-06-30 06:29:07,187 main.py:52] epoch 6092, training loss: 5618.44, average training loss: 5657.07, base loss: 15958.22
[INFO 2017-06-30 06:29:10,339 main.py:52] epoch 6093, training loss: 5448.53, average training loss: 5656.74, base loss: 15958.37
[INFO 2017-06-30 06:29:13,473 main.py:52] epoch 6094, training loss: 5640.00, average training loss: 5656.55, base loss: 15958.67
[INFO 2017-06-30 06:29:16,617 main.py:52] epoch 6095, training loss: 5780.28, average training loss: 5656.88, base loss: 15958.76
[INFO 2017-06-30 06:29:19,786 main.py:52] epoch 6096, training loss: 5576.08, average training loss: 5656.53, base loss: 15958.27
[INFO 2017-06-30 06:29:22,963 main.py:52] epoch 6097, training loss: 5385.89, average training loss: 5656.22, base loss: 15958.05
[INFO 2017-06-30 06:29:26,094 main.py:52] epoch 6098, training loss: 5761.65, average training loss: 5656.31, base loss: 15958.15
[INFO 2017-06-30 06:29:29,233 main.py:52] epoch 6099, training loss: 5300.93, average training loss: 5656.25, base loss: 15958.14
[INFO 2017-06-30 06:29:29,233 main.py:54] epoch 6099, testing
[INFO 2017-06-30 06:29:42,341 main.py:97] average testing loss: 5370.02, base loss: 15238.42
[INFO 2017-06-30 06:29:42,341 main.py:98] improve_loss: 9868.41, improve_percent: 0.65
[INFO 2017-06-30 06:29:42,343 main.py:66] current best improved percent: 0.65
[INFO 2017-06-30 06:29:45,515 main.py:52] epoch 6100, training loss: 5563.30, average training loss: 5656.18, base loss: 15957.87
[INFO 2017-06-30 06:29:48,697 main.py:52] epoch 6101, training loss: 5239.40, average training loss: 5655.49, base loss: 15957.42
[INFO 2017-06-30 06:29:51,880 main.py:52] epoch 6102, training loss: 5881.66, average training loss: 5655.38, base loss: 15957.55
[INFO 2017-06-30 06:29:55,038 main.py:52] epoch 6103, training loss: 5893.05, average training loss: 5655.88, base loss: 15957.85
[INFO 2017-06-30 06:29:58,158 main.py:52] epoch 6104, training loss: 5486.69, average training loss: 5655.70, base loss: 15957.59
[INFO 2017-06-30 06:30:01,292 main.py:52] epoch 6105, training loss: 5923.97, average training loss: 5656.25, base loss: 15957.11
[INFO 2017-06-30 06:30:04,495 main.py:52] epoch 6106, training loss: 5219.17, average training loss: 5655.86, base loss: 15956.41
[INFO 2017-06-30 06:30:07,616 main.py:52] epoch 6107, training loss: 5490.38, average training loss: 5655.55, base loss: 15956.25
[INFO 2017-06-30 06:30:10,748 main.py:52] epoch 6108, training loss: 5584.01, average training loss: 5655.17, base loss: 15956.76
[INFO 2017-06-30 06:30:13,900 main.py:52] epoch 6109, training loss: 5519.39, average training loss: 5654.99, base loss: 15957.12
[INFO 2017-06-30 06:30:17,068 main.py:52] epoch 6110, training loss: 5846.34, average training loss: 5655.20, base loss: 15957.74
[INFO 2017-06-30 06:30:20,234 main.py:52] epoch 6111, training loss: 5544.70, average training loss: 5655.11, base loss: 15957.47
[INFO 2017-06-30 06:30:23,399 main.py:52] epoch 6112, training loss: 6090.50, average training loss: 5655.72, base loss: 15958.10
[INFO 2017-06-30 06:30:26,507 main.py:52] epoch 6113, training loss: 5253.51, average training loss: 5654.78, base loss: 15957.71
[INFO 2017-06-30 06:30:29,687 main.py:52] epoch 6114, training loss: 5589.53, average training loss: 5654.67, base loss: 15957.67
[INFO 2017-06-30 06:30:32,828 main.py:52] epoch 6115, training loss: 5674.96, average training loss: 5654.58, base loss: 15958.05
[INFO 2017-06-30 06:30:35,979 main.py:52] epoch 6116, training loss: 5456.89, average training loss: 5654.16, base loss: 15958.21
[INFO 2017-06-30 06:30:39,132 main.py:52] epoch 6117, training loss: 5435.32, average training loss: 5654.07, base loss: 15958.00
[INFO 2017-06-30 06:30:42,223 main.py:52] epoch 6118, training loss: 5799.56, average training loss: 5654.29, base loss: 15958.29
[INFO 2017-06-30 06:30:45,346 main.py:52] epoch 6119, training loss: 5445.92, average training loss: 5653.82, base loss: 15957.83
[INFO 2017-06-30 06:30:48,462 main.py:52] epoch 6120, training loss: 5344.16, average training loss: 5653.22, base loss: 15957.57
[INFO 2017-06-30 06:30:51,585 main.py:52] epoch 6121, training loss: 5668.98, average training loss: 5653.36, base loss: 15957.76
[INFO 2017-06-30 06:30:54,748 main.py:52] epoch 6122, training loss: 5721.82, average training loss: 5653.55, base loss: 15957.86
[INFO 2017-06-30 06:30:57,878 main.py:52] epoch 6123, training loss: 5736.29, average training loss: 5653.66, base loss: 15958.08
[INFO 2017-06-30 06:31:01,048 main.py:52] epoch 6124, training loss: 5610.99, average training loss: 5653.90, base loss: 15958.14
[INFO 2017-06-30 06:31:04,204 main.py:52] epoch 6125, training loss: 5809.97, average training loss: 5654.14, base loss: 15958.31
[INFO 2017-06-30 06:31:07,365 main.py:52] epoch 6126, training loss: 5307.40, average training loss: 5653.72, base loss: 15958.06
[INFO 2017-06-30 06:31:10,487 main.py:52] epoch 6127, training loss: 5781.33, average training loss: 5653.84, base loss: 15957.87
[INFO 2017-06-30 06:31:13,592 main.py:52] epoch 6128, training loss: 5337.30, average training loss: 5653.67, base loss: 15957.49
[INFO 2017-06-30 06:31:16,730 main.py:52] epoch 6129, training loss: 5664.40, average training loss: 5653.74, base loss: 15957.23
[INFO 2017-06-30 06:31:19,848 main.py:52] epoch 6130, training loss: 5344.88, average training loss: 5653.31, base loss: 15957.55
[INFO 2017-06-30 06:31:23,055 main.py:52] epoch 6131, training loss: 5446.83, average training loss: 5653.07, base loss: 15957.79
[INFO 2017-06-30 06:31:26,176 main.py:52] epoch 6132, training loss: 5389.70, average training loss: 5652.85, base loss: 15957.21
[INFO 2017-06-30 06:31:29,310 main.py:52] epoch 6133, training loss: 6010.01, average training loss: 5653.14, base loss: 15957.27
[INFO 2017-06-30 06:31:32,409 main.py:52] epoch 6134, training loss: 5562.50, average training loss: 5652.89, base loss: 15957.56
[INFO 2017-06-30 06:31:35,545 main.py:52] epoch 6135, training loss: 5675.52, average training loss: 5652.49, base loss: 15957.66
[INFO 2017-06-30 06:31:38,717 main.py:52] epoch 6136, training loss: 5530.42, average training loss: 5652.21, base loss: 15957.49
[INFO 2017-06-30 06:31:41,906 main.py:52] epoch 6137, training loss: 5488.16, average training loss: 5652.35, base loss: 15958.01
[INFO 2017-06-30 06:31:45,065 main.py:52] epoch 6138, training loss: 5467.40, average training loss: 5652.49, base loss: 15958.08
[INFO 2017-06-30 06:31:48,199 main.py:52] epoch 6139, training loss: 5446.66, average training loss: 5652.28, base loss: 15957.95
[INFO 2017-06-30 06:31:51,367 main.py:52] epoch 6140, training loss: 6089.54, average training loss: 5652.58, base loss: 15958.04
[INFO 2017-06-30 06:31:54,487 main.py:52] epoch 6141, training loss: 5518.32, average training loss: 5652.56, base loss: 15957.57
[INFO 2017-06-30 06:31:57,651 main.py:52] epoch 6142, training loss: 5457.42, average training loss: 5652.25, base loss: 15957.77
[INFO 2017-06-30 06:32:00,831 main.py:52] epoch 6143, training loss: 5933.30, average training loss: 5652.38, base loss: 15958.01
[INFO 2017-06-30 06:32:03,984 main.py:52] epoch 6144, training loss: 5404.92, average training loss: 5652.15, base loss: 15957.79
[INFO 2017-06-30 06:32:07,150 main.py:52] epoch 6145, training loss: 5433.83, average training loss: 5652.06, base loss: 15957.15
[INFO 2017-06-30 06:32:10,284 main.py:52] epoch 6146, training loss: 5294.55, average training loss: 5651.51, base loss: 15956.84
[INFO 2017-06-30 06:32:13,446 main.py:52] epoch 6147, training loss: 5576.21, average training loss: 5651.40, base loss: 15956.55
[INFO 2017-06-30 06:32:16,665 main.py:52] epoch 6148, training loss: 5563.74, average training loss: 5651.48, base loss: 15956.43
[INFO 2017-06-30 06:32:19,812 main.py:52] epoch 6149, training loss: 5308.52, average training loss: 5651.10, base loss: 15956.25
[INFO 2017-06-30 06:32:22,913 main.py:52] epoch 6150, training loss: 5715.93, average training loss: 5651.16, base loss: 15956.89
[INFO 2017-06-30 06:32:26,046 main.py:52] epoch 6151, training loss: 5966.31, average training loss: 5651.40, base loss: 15957.83
[INFO 2017-06-30 06:32:29,186 main.py:52] epoch 6152, training loss: 5655.56, average training loss: 5651.39, base loss: 15957.37
[INFO 2017-06-30 06:32:32,332 main.py:52] epoch 6153, training loss: 5770.32, average training loss: 5651.56, base loss: 15956.97
[INFO 2017-06-30 06:32:35,486 main.py:52] epoch 6154, training loss: 5514.12, average training loss: 5651.04, base loss: 15956.84
[INFO 2017-06-30 06:32:38,665 main.py:52] epoch 6155, training loss: 5547.00, average training loss: 5650.71, base loss: 15956.87
[INFO 2017-06-30 06:32:41,817 main.py:52] epoch 6156, training loss: 5477.55, average training loss: 5650.32, base loss: 15956.50
[INFO 2017-06-30 06:32:44,978 main.py:52] epoch 6157, training loss: 5619.12, average training loss: 5650.52, base loss: 15956.64
[INFO 2017-06-30 06:32:48,104 main.py:52] epoch 6158, training loss: 5610.78, average training loss: 5650.57, base loss: 15956.89
[INFO 2017-06-30 06:32:51,252 main.py:52] epoch 6159, training loss: 5794.22, average training loss: 5651.14, base loss: 15956.94
[INFO 2017-06-30 06:32:54,379 main.py:52] epoch 6160, training loss: 5471.14, average training loss: 5651.21, base loss: 15956.79
[INFO 2017-06-30 06:32:57,558 main.py:52] epoch 6161, training loss: 5914.19, average training loss: 5651.22, base loss: 15957.46
[INFO 2017-06-30 06:33:00,724 main.py:52] epoch 6162, training loss: 5757.45, average training loss: 5651.74, base loss: 15957.33
[INFO 2017-06-30 06:33:03,850 main.py:52] epoch 6163, training loss: 5571.33, average training loss: 5651.42, base loss: 15957.10
[INFO 2017-06-30 06:33:06,997 main.py:52] epoch 6164, training loss: 5593.85, average training loss: 5651.31, base loss: 15957.11
[INFO 2017-06-30 06:33:10,157 main.py:52] epoch 6165, training loss: 5775.03, average training loss: 5651.35, base loss: 15957.23
[INFO 2017-06-30 06:33:13,320 main.py:52] epoch 6166, training loss: 5906.12, average training loss: 5651.49, base loss: 15957.55
[INFO 2017-06-30 06:33:16,524 main.py:52] epoch 6167, training loss: 5736.92, average training loss: 5651.10, base loss: 15957.46
[INFO 2017-06-30 06:33:19,670 main.py:52] epoch 6168, training loss: 5589.71, average training loss: 5651.13, base loss: 15957.14
[INFO 2017-06-30 06:33:22,845 main.py:52] epoch 6169, training loss: 5921.02, average training loss: 5651.40, base loss: 15957.69
[INFO 2017-06-30 06:33:25,995 main.py:52] epoch 6170, training loss: 5479.86, average training loss: 5651.47, base loss: 15957.11
[INFO 2017-06-30 06:33:29,117 main.py:52] epoch 6171, training loss: 5536.17, average training loss: 5651.42, base loss: 15956.54
[INFO 2017-06-30 06:33:32,253 main.py:52] epoch 6172, training loss: 5749.60, average training loss: 5651.44, base loss: 15956.51
[INFO 2017-06-30 06:33:35,428 main.py:52] epoch 6173, training loss: 5852.81, average training loss: 5651.87, base loss: 15956.96
[INFO 2017-06-30 06:33:38,561 main.py:52] epoch 6174, training loss: 5470.12, average training loss: 5651.89, base loss: 15955.94
[INFO 2017-06-30 06:33:41,732 main.py:52] epoch 6175, training loss: 5638.37, average training loss: 5652.15, base loss: 15955.98
[INFO 2017-06-30 06:33:44,904 main.py:52] epoch 6176, training loss: 5734.38, average training loss: 5652.16, base loss: 15956.53
[INFO 2017-06-30 06:33:48,042 main.py:52] epoch 6177, training loss: 5949.71, average training loss: 5652.55, base loss: 15957.38
[INFO 2017-06-30 06:33:51,192 main.py:52] epoch 6178, training loss: 5823.44, average training loss: 5652.36, base loss: 15957.83
[INFO 2017-06-30 06:33:54,356 main.py:52] epoch 6179, training loss: 5068.01, average training loss: 5651.25, base loss: 15957.32
[INFO 2017-06-30 06:33:57,506 main.py:52] epoch 6180, training loss: 5154.81, average training loss: 5650.54, base loss: 15957.15
[INFO 2017-06-30 06:34:00,666 main.py:52] epoch 6181, training loss: 5802.74, average training loss: 5650.68, base loss: 15957.35
[INFO 2017-06-30 06:34:03,828 main.py:52] epoch 6182, training loss: 5604.80, average training loss: 5650.61, base loss: 15957.58
[INFO 2017-06-30 06:34:07,003 main.py:52] epoch 6183, training loss: 5774.53, average training loss: 5650.57, base loss: 15957.89
[INFO 2017-06-30 06:34:10,193 main.py:52] epoch 6184, training loss: 5842.28, average training loss: 5650.71, base loss: 15958.22
[INFO 2017-06-30 06:34:13,372 main.py:52] epoch 6185, training loss: 5566.92, average training loss: 5650.70, base loss: 15958.21
[INFO 2017-06-30 06:34:16,529 main.py:52] epoch 6186, training loss: 5682.95, average training loss: 5649.78, base loss: 15958.26
[INFO 2017-06-30 06:34:19,675 main.py:52] epoch 6187, training loss: 5482.35, average training loss: 5649.42, base loss: 15958.23
[INFO 2017-06-30 06:34:22,855 main.py:52] epoch 6188, training loss: 5166.36, average training loss: 5649.03, base loss: 15957.46
[INFO 2017-06-30 06:34:26,033 main.py:52] epoch 6189, training loss: 5579.66, average training loss: 5649.22, base loss: 15957.15
[INFO 2017-06-30 06:34:29,175 main.py:52] epoch 6190, training loss: 6113.31, average training loss: 5649.42, base loss: 15957.78
[INFO 2017-06-30 06:34:32,368 main.py:52] epoch 6191, training loss: 5656.56, average training loss: 5648.93, base loss: 15958.04
[INFO 2017-06-30 06:34:35,468 main.py:52] epoch 6192, training loss: 5567.89, average training loss: 5648.68, base loss: 15957.66
[INFO 2017-06-30 06:34:38,572 main.py:52] epoch 6193, training loss: 5502.49, average training loss: 5648.24, base loss: 15957.26
[INFO 2017-06-30 06:34:41,770 main.py:52] epoch 6194, training loss: 5592.02, average training loss: 5647.95, base loss: 15957.17
[INFO 2017-06-30 06:34:44,912 main.py:52] epoch 6195, training loss: 5749.91, average training loss: 5647.66, base loss: 15957.87
[INFO 2017-06-30 06:34:48,051 main.py:52] epoch 6196, training loss: 5614.92, average training loss: 5647.75, base loss: 15958.08
[INFO 2017-06-30 06:34:51,191 main.py:52] epoch 6197, training loss: 5270.46, average training loss: 5646.76, base loss: 15957.68
[INFO 2017-06-30 06:34:54,336 main.py:52] epoch 6198, training loss: 5415.50, average training loss: 5646.62, base loss: 15957.15
[INFO 2017-06-30 06:34:57,486 main.py:52] epoch 6199, training loss: 5680.89, average training loss: 5646.58, base loss: 15956.55
[INFO 2017-06-30 06:34:57,486 main.py:54] epoch 6199, testing
[INFO 2017-06-30 06:35:10,628 main.py:97] average testing loss: 5621.10, base loss: 16031.63
[INFO 2017-06-30 06:35:10,629 main.py:98] improve_loss: 10410.52, improve_percent: 0.65
[INFO 2017-06-30 06:35:10,631 main.py:66] current best improved percent: 0.65
[INFO 2017-06-30 06:35:13,786 main.py:52] epoch 6200, training loss: 6131.23, average training loss: 5646.76, base loss: 15956.80
[INFO 2017-06-30 06:35:16,905 main.py:52] epoch 6201, training loss: 5724.23, average training loss: 5647.02, base loss: 15957.19
[INFO 2017-06-30 06:35:20,053 main.py:52] epoch 6202, training loss: 5578.90, average training loss: 5646.78, base loss: 15957.00
[INFO 2017-06-30 06:35:23,203 main.py:52] epoch 6203, training loss: 5437.77, average training loss: 5646.68, base loss: 15956.93
[INFO 2017-06-30 06:35:26,369 main.py:52] epoch 6204, training loss: 5380.46, average training loss: 5646.29, base loss: 15956.48
[INFO 2017-06-30 06:35:29,563 main.py:52] epoch 6205, training loss: 5625.44, average training loss: 5646.39, base loss: 15956.38
[INFO 2017-06-30 06:35:32,721 main.py:52] epoch 6206, training loss: 5441.23, average training loss: 5645.96, base loss: 15956.11
[INFO 2017-06-30 06:35:35,861 main.py:52] epoch 6207, training loss: 5727.93, average training loss: 5646.05, base loss: 15956.16
[INFO 2017-06-30 06:35:39,047 main.py:52] epoch 6208, training loss: 5493.98, average training loss: 5645.63, base loss: 15956.41
[INFO 2017-06-30 06:35:42,176 main.py:52] epoch 6209, training loss: 5810.92, average training loss: 5646.08, base loss: 15956.71
[INFO 2017-06-30 06:35:45,313 main.py:52] epoch 6210, training loss: 5298.04, average training loss: 5645.34, base loss: 15956.72
[INFO 2017-06-30 06:35:48,525 main.py:52] epoch 6211, training loss: 5555.30, average training loss: 5645.00, base loss: 15956.29
[INFO 2017-06-30 06:35:51,636 main.py:52] epoch 6212, training loss: 5847.54, average training loss: 5645.31, base loss: 15956.13
[INFO 2017-06-30 06:35:54,809 main.py:52] epoch 6213, training loss: 5844.41, average training loss: 5645.80, base loss: 15956.50
[INFO 2017-06-30 06:35:57,926 main.py:52] epoch 6214, training loss: 5110.93, average training loss: 5645.23, base loss: 15956.11
[INFO 2017-06-30 06:36:01,052 main.py:52] epoch 6215, training loss: 5200.38, average training loss: 5644.04, base loss: 15955.44
[INFO 2017-06-30 06:36:04,209 main.py:52] epoch 6216, training loss: 5628.15, average training loss: 5644.05, base loss: 15955.69
[INFO 2017-06-30 06:36:07,379 main.py:52] epoch 6217, training loss: 5745.96, average training loss: 5644.54, base loss: 15955.63
[INFO 2017-06-30 06:36:10,537 main.py:52] epoch 6218, training loss: 5661.80, average training loss: 5644.27, base loss: 15955.55
[INFO 2017-06-30 06:36:13,682 main.py:52] epoch 6219, training loss: 5826.85, average training loss: 5644.45, base loss: 15955.98
[INFO 2017-06-30 06:36:16,865 main.py:52] epoch 6220, training loss: 5552.02, average training loss: 5644.05, base loss: 15955.99
[INFO 2017-06-30 06:36:19,995 main.py:52] epoch 6221, training loss: 5723.44, average training loss: 5643.65, base loss: 15956.58
[INFO 2017-06-30 06:36:23,146 main.py:52] epoch 6222, training loss: 5973.92, average training loss: 5643.83, base loss: 15957.18
[INFO 2017-06-30 06:36:26,307 main.py:52] epoch 6223, training loss: 6205.44, average training loss: 5644.43, base loss: 15957.70
[INFO 2017-06-30 06:36:29,430 main.py:52] epoch 6224, training loss: 5746.91, average training loss: 5644.37, base loss: 15958.31
[INFO 2017-06-30 06:36:32,529 main.py:52] epoch 6225, training loss: 5577.83, average training loss: 5644.00, base loss: 15958.50
[INFO 2017-06-30 06:36:35,654 main.py:52] epoch 6226, training loss: 5456.65, average training loss: 5643.59, base loss: 15958.17
[INFO 2017-06-30 06:36:38,803 main.py:52] epoch 6227, training loss: 5959.79, average training loss: 5644.03, base loss: 15958.64
[INFO 2017-06-30 06:36:41,966 main.py:52] epoch 6228, training loss: 5388.00, average training loss: 5644.19, base loss: 15958.65
[INFO 2017-06-30 06:36:45,132 main.py:52] epoch 6229, training loss: 5566.49, average training loss: 5643.98, base loss: 15958.60
[INFO 2017-06-30 06:36:48,243 main.py:52] epoch 6230, training loss: 5665.65, average training loss: 5644.46, base loss: 15958.82
[INFO 2017-06-30 06:36:51,393 main.py:52] epoch 6231, training loss: 5248.58, average training loss: 5643.86, base loss: 15958.35
[INFO 2017-06-30 06:36:54,565 main.py:52] epoch 6232, training loss: 5099.20, average training loss: 5643.54, base loss: 15958.11
[INFO 2017-06-30 06:36:57,713 main.py:52] epoch 6233, training loss: 5483.59, average training loss: 5643.24, base loss: 15958.22
[INFO 2017-06-30 06:37:00,854 main.py:52] epoch 6234, training loss: 5392.50, average training loss: 5642.90, base loss: 15957.78
[INFO 2017-06-30 06:37:04,021 main.py:52] epoch 6235, training loss: 5222.15, average training loss: 5642.13, base loss: 15957.21
[INFO 2017-06-30 06:37:07,180 main.py:52] epoch 6236, training loss: 5409.49, average training loss: 5641.79, base loss: 15957.17
[INFO 2017-06-30 06:37:10,332 main.py:52] epoch 6237, training loss: 5116.95, average training loss: 5641.08, base loss: 15956.47
[INFO 2017-06-30 06:37:13,454 main.py:52] epoch 6238, training loss: 5831.92, average training loss: 5641.61, base loss: 15956.96
[INFO 2017-06-30 06:37:16,590 main.py:52] epoch 6239, training loss: 5870.68, average training loss: 5641.56, base loss: 15957.21
[INFO 2017-06-30 06:37:19,760 main.py:52] epoch 6240, training loss: 5511.14, average training loss: 5641.53, base loss: 15957.14
[INFO 2017-06-30 06:37:22,882 main.py:52] epoch 6241, training loss: 5662.33, average training loss: 5641.29, base loss: 15957.11
[INFO 2017-06-30 06:37:26,068 main.py:52] epoch 6242, training loss: 5611.44, average training loss: 5641.11, base loss: 15957.10
[INFO 2017-06-30 06:37:29,232 main.py:52] epoch 6243, training loss: 5552.43, average training loss: 5641.24, base loss: 15956.34
[INFO 2017-06-30 06:37:32,408 main.py:52] epoch 6244, training loss: 5937.72, average training loss: 5641.20, base loss: 15957.12
[INFO 2017-06-30 06:37:35,519 main.py:52] epoch 6245, training loss: 5612.47, average training loss: 5641.28, base loss: 15957.28
[INFO 2017-06-30 06:37:38,691 main.py:52] epoch 6246, training loss: 5085.45, average training loss: 5640.56, base loss: 15956.96
[INFO 2017-06-30 06:37:41,824 main.py:52] epoch 6247, training loss: 5581.18, average training loss: 5640.67, base loss: 15956.58
[INFO 2017-06-30 06:37:44,960 main.py:52] epoch 6248, training loss: 5216.86, average training loss: 5640.28, base loss: 15956.72
[INFO 2017-06-30 06:37:48,102 main.py:52] epoch 6249, training loss: 5564.15, average training loss: 5640.20, base loss: 15956.65
[INFO 2017-06-30 06:37:51,246 main.py:52] epoch 6250, training loss: 5670.57, average training loss: 5640.25, base loss: 15956.78
[INFO 2017-06-30 06:37:54,398 main.py:52] epoch 6251, training loss: 5658.66, average training loss: 5640.44, base loss: 15957.06
[INFO 2017-06-30 06:37:57,500 main.py:52] epoch 6252, training loss: 5948.82, average training loss: 5640.54, base loss: 15957.91
[INFO 2017-06-30 06:38:00,646 main.py:52] epoch 6253, training loss: 5262.95, average training loss: 5640.02, base loss: 15958.04
[INFO 2017-06-30 06:38:03,751 main.py:52] epoch 6254, training loss: 5150.04, average training loss: 5639.81, base loss: 15957.97
[INFO 2017-06-30 06:38:06,889 main.py:52] epoch 6255, training loss: 5338.64, average training loss: 5639.74, base loss: 15958.15
[INFO 2017-06-30 06:38:10,054 main.py:52] epoch 6256, training loss: 5391.68, average training loss: 5639.09, base loss: 15957.80
[INFO 2017-06-30 06:38:13,196 main.py:52] epoch 6257, training loss: 5796.98, average training loss: 5639.19, base loss: 15957.75
[INFO 2017-06-30 06:38:16,398 main.py:52] epoch 6258, training loss: 5643.90, average training loss: 5638.63, base loss: 15957.77
[INFO 2017-06-30 06:38:19,521 main.py:52] epoch 6259, training loss: 5683.40, average training loss: 5638.90, base loss: 15957.93
[INFO 2017-06-30 06:38:22,692 main.py:52] epoch 6260, training loss: 5240.17, average training loss: 5638.90, base loss: 15957.61
[INFO 2017-06-30 06:38:25,814 main.py:52] epoch 6261, training loss: 5359.78, average training loss: 5638.72, base loss: 15957.27
[INFO 2017-06-30 06:38:28,987 main.py:52] epoch 6262, training loss: 5529.59, average training loss: 5638.10, base loss: 15957.43
[INFO 2017-06-30 06:38:32,146 main.py:52] epoch 6263, training loss: 5809.26, average training loss: 5638.01, base loss: 15957.60
[INFO 2017-06-30 06:38:35,311 main.py:52] epoch 6264, training loss: 5665.08, average training loss: 5637.80, base loss: 15958.00
[INFO 2017-06-30 06:38:38,460 main.py:52] epoch 6265, training loss: 6068.00, average training loss: 5637.80, base loss: 15958.81
[INFO 2017-06-30 06:38:41,594 main.py:52] epoch 6266, training loss: 5679.58, average training loss: 5637.61, base loss: 15959.05
[INFO 2017-06-30 06:38:44,750 main.py:52] epoch 6267, training loss: 5717.39, average training loss: 5637.57, base loss: 15959.31
[INFO 2017-06-30 06:38:47,849 main.py:52] epoch 6268, training loss: 5706.78, average training loss: 5637.47, base loss: 15959.96
[INFO 2017-06-30 06:38:51,008 main.py:52] epoch 6269, training loss: 5835.01, average training loss: 5637.60, base loss: 15960.23
[INFO 2017-06-30 06:38:54,167 main.py:52] epoch 6270, training loss: 5726.63, average training loss: 5637.94, base loss: 15960.66
[INFO 2017-06-30 06:38:57,368 main.py:52] epoch 6271, training loss: 5857.56, average training loss: 5638.23, base loss: 15961.07
[INFO 2017-06-30 06:39:00,460 main.py:52] epoch 6272, training loss: 5669.60, average training loss: 5638.65, base loss: 15961.29
[INFO 2017-06-30 06:39:03,601 main.py:52] epoch 6273, training loss: 5377.09, average training loss: 5638.34, base loss: 15960.90
[INFO 2017-06-30 06:39:06,721 main.py:52] epoch 6274, training loss: 5403.34, average training loss: 5638.04, base loss: 15960.66
[INFO 2017-06-30 06:39:09,864 main.py:52] epoch 6275, training loss: 5453.71, average training loss: 5637.58, base loss: 15960.39
[INFO 2017-06-30 06:39:12,986 main.py:52] epoch 6276, training loss: 5383.26, average training loss: 5637.43, base loss: 15960.01
[INFO 2017-06-30 06:39:16,112 main.py:52] epoch 6277, training loss: 5536.35, average training loss: 5637.31, base loss: 15960.06
[INFO 2017-06-30 06:39:19,235 main.py:52] epoch 6278, training loss: 5626.54, average training loss: 5637.32, base loss: 15960.17
[INFO 2017-06-30 06:39:22,386 main.py:52] epoch 6279, training loss: 5890.43, average training loss: 5637.30, base loss: 15961.05
[INFO 2017-06-30 06:39:25,533 main.py:52] epoch 6280, training loss: 5606.69, average training loss: 5637.10, base loss: 15961.08
[INFO 2017-06-30 06:39:28,667 main.py:52] epoch 6281, training loss: 5383.59, average training loss: 5636.55, base loss: 15961.27
[INFO 2017-06-30 06:39:31,792 main.py:52] epoch 6282, training loss: 5560.73, average training loss: 5636.95, base loss: 15960.98
[INFO 2017-06-30 06:39:34,936 main.py:52] epoch 6283, training loss: 5357.30, average training loss: 5636.61, base loss: 15960.44
[INFO 2017-06-30 06:39:38,055 main.py:52] epoch 6284, training loss: 5478.84, average training loss: 5636.71, base loss: 15960.33
[INFO 2017-06-30 06:39:41,235 main.py:52] epoch 6285, training loss: 5332.90, average training loss: 5636.40, base loss: 15960.16
[INFO 2017-06-30 06:39:44,405 main.py:52] epoch 6286, training loss: 5401.11, average training loss: 5636.03, base loss: 15960.08
[INFO 2017-06-30 06:39:47,555 main.py:52] epoch 6287, training loss: 5757.42, average training loss: 5636.26, base loss: 15959.93
[INFO 2017-06-30 06:39:50,713 main.py:52] epoch 6288, training loss: 5116.99, average training loss: 5635.56, base loss: 15959.07
[INFO 2017-06-30 06:39:53,817 main.py:52] epoch 6289, training loss: 5507.23, average training loss: 5635.53, base loss: 15958.56
[INFO 2017-06-30 06:39:56,980 main.py:52] epoch 6290, training loss: 5500.59, average training loss: 5635.46, base loss: 15958.64
[INFO 2017-06-30 06:40:00,085 main.py:52] epoch 6291, training loss: 5283.06, average training loss: 5635.36, base loss: 15958.66
[INFO 2017-06-30 06:40:03,239 main.py:52] epoch 6292, training loss: 5312.00, average training loss: 5634.49, base loss: 15958.23
[INFO 2017-06-30 06:40:06,390 main.py:52] epoch 6293, training loss: 5435.42, average training loss: 5634.49, base loss: 15957.83
[INFO 2017-06-30 06:40:09,515 main.py:52] epoch 6294, training loss: 5857.77, average training loss: 5634.38, base loss: 15958.18
[INFO 2017-06-30 06:40:12,640 main.py:52] epoch 6295, training loss: 5145.93, average training loss: 5633.89, base loss: 15957.82
[INFO 2017-06-30 06:40:15,786 main.py:52] epoch 6296, training loss: 5556.01, average training loss: 5633.50, base loss: 15958.03
[INFO 2017-06-30 06:40:18,889 main.py:52] epoch 6297, training loss: 5650.52, average training loss: 5633.50, base loss: 15958.25
[INFO 2017-06-30 06:40:22,031 main.py:52] epoch 6298, training loss: 5666.13, average training loss: 5633.53, base loss: 15957.95
[INFO 2017-06-30 06:40:25,197 main.py:52] epoch 6299, training loss: 5682.80, average training loss: 5633.15, base loss: 15958.07
[INFO 2017-06-30 06:40:25,197 main.py:54] epoch 6299, testing
[INFO 2017-06-30 06:40:38,192 main.py:97] average testing loss: 5648.07, base loss: 16492.36
[INFO 2017-06-30 06:40:38,192 main.py:98] improve_loss: 10844.29, improve_percent: 0.66
[INFO 2017-06-30 06:40:38,193 main.py:62] model save to ./model/final.pth
[INFO 2017-06-30 06:40:38,228 main.py:66] current best improved percent: 0.66
[INFO 2017-06-30 06:40:41,356 main.py:52] epoch 6300, training loss: 5471.91, average training loss: 5633.10, base loss: 15958.02
[INFO 2017-06-30 06:40:44,504 main.py:52] epoch 6301, training loss: 5545.04, average training loss: 5632.28, base loss: 15957.90
[INFO 2017-06-30 06:40:47,632 main.py:52] epoch 6302, training loss: 5181.18, average training loss: 5631.76, base loss: 15957.65
[INFO 2017-06-30 06:40:50,785 main.py:52] epoch 6303, training loss: 5814.04, average training loss: 5631.71, base loss: 15957.90
[INFO 2017-06-30 06:40:53,887 main.py:52] epoch 6304, training loss: 5542.35, average training loss: 5631.68, base loss: 15958.05
[INFO 2017-06-30 06:40:56,990 main.py:52] epoch 6305, training loss: 5308.64, average training loss: 5631.17, base loss: 15958.29
[INFO 2017-06-30 06:41:00,187 main.py:52] epoch 6306, training loss: 5635.29, average training loss: 5631.04, base loss: 15958.68
[INFO 2017-06-30 06:41:03,319 main.py:52] epoch 6307, training loss: 5573.91, average training loss: 5630.63, base loss: 15958.54
[INFO 2017-06-30 06:41:06,468 main.py:52] epoch 6308, training loss: 5633.38, average training loss: 5630.61, base loss: 15958.84
[INFO 2017-06-30 06:41:09,594 main.py:52] epoch 6309, training loss: 5822.25, average training loss: 5630.93, base loss: 15959.28
[INFO 2017-06-30 06:41:12,732 main.py:52] epoch 6310, training loss: 5688.88, average training loss: 5630.51, base loss: 15959.22
[INFO 2017-06-30 06:41:15,873 main.py:52] epoch 6311, training loss: 5313.95, average training loss: 5630.41, base loss: 15958.98
[INFO 2017-06-30 06:41:19,033 main.py:52] epoch 6312, training loss: 5626.13, average training loss: 5629.60, base loss: 15958.76
[INFO 2017-06-30 06:41:22,225 main.py:52] epoch 6313, training loss: 5889.18, average training loss: 5630.02, base loss: 15959.38
[INFO 2017-06-30 06:41:25,351 main.py:52] epoch 6314, training loss: 5682.13, average training loss: 5630.09, base loss: 15960.14
[INFO 2017-06-30 06:41:28,514 main.py:52] epoch 6315, training loss: 5576.57, average training loss: 5630.21, base loss: 15959.90
[INFO 2017-06-30 06:41:31,626 main.py:52] epoch 6316, training loss: 5347.54, average training loss: 5629.89, base loss: 15959.90
[INFO 2017-06-30 06:41:34,786 main.py:52] epoch 6317, training loss: 5724.47, average training loss: 5630.40, base loss: 15960.36
[INFO 2017-06-30 06:41:37,926 main.py:52] epoch 6318, training loss: 5291.13, average training loss: 5630.06, base loss: 15959.94
[INFO 2017-06-30 06:41:41,051 main.py:52] epoch 6319, training loss: 6064.21, average training loss: 5630.86, base loss: 15960.28
[INFO 2017-06-30 06:41:44,260 main.py:52] epoch 6320, training loss: 5872.24, average training loss: 5630.88, base loss: 15960.61
[INFO 2017-06-30 06:41:47,368 main.py:52] epoch 6321, training loss: 5460.96, average training loss: 5630.58, base loss: 15960.73
[INFO 2017-06-30 06:41:50,504 main.py:52] epoch 6322, training loss: 5518.10, average training loss: 5630.65, base loss: 15960.55
[INFO 2017-06-30 06:41:53,664 main.py:52] epoch 6323, training loss: 5475.84, average training loss: 5630.15, base loss: 15960.79
[INFO 2017-06-30 06:41:56,772 main.py:52] epoch 6324, training loss: 5401.30, average training loss: 5629.74, base loss: 15960.49
[INFO 2017-06-30 06:41:59,890 main.py:52] epoch 6325, training loss: 5522.87, average training loss: 5629.81, base loss: 15960.39
[INFO 2017-06-30 06:42:03,070 main.py:52] epoch 6326, training loss: 5299.35, average training loss: 5629.90, base loss: 15960.00
[INFO 2017-06-30 06:42:06,238 main.py:52] epoch 6327, training loss: 5200.03, average training loss: 5629.49, base loss: 15959.32
[INFO 2017-06-30 06:42:09,404 main.py:52] epoch 6328, training loss: 5274.14, average training loss: 5628.76, base loss: 15958.66
[INFO 2017-06-30 06:42:12,545 main.py:52] epoch 6329, training loss: 5607.16, average training loss: 5628.42, base loss: 15957.98
[INFO 2017-06-30 06:42:15,723 main.py:52] epoch 6330, training loss: 5585.39, average training loss: 5628.09, base loss: 15957.58
[INFO 2017-06-30 06:42:18,862 main.py:52] epoch 6331, training loss: 5736.63, average training loss: 5627.64, base loss: 15957.79
[INFO 2017-06-30 06:42:22,009 main.py:52] epoch 6332, training loss: 6083.81, average training loss: 5628.25, base loss: 15958.36
[INFO 2017-06-30 06:42:25,176 main.py:52] epoch 6333, training loss: 5621.77, average training loss: 5628.17, base loss: 15958.39
[INFO 2017-06-30 06:42:28,337 main.py:52] epoch 6334, training loss: 5673.86, average training loss: 5627.79, base loss: 15958.34
[INFO 2017-06-30 06:42:31,495 main.py:52] epoch 6335, training loss: 5487.22, average training loss: 5627.66, base loss: 15958.06
[INFO 2017-06-30 06:42:34,608 main.py:52] epoch 6336, training loss: 5464.10, average training loss: 5627.27, base loss: 15958.16
[INFO 2017-06-30 06:42:37,772 main.py:52] epoch 6337, training loss: 5545.49, average training loss: 5626.84, base loss: 15958.38
[INFO 2017-06-30 06:42:40,905 main.py:52] epoch 6338, training loss: 5676.97, average training loss: 5626.73, base loss: 15958.52
[INFO 2017-06-30 06:42:44,081 main.py:52] epoch 6339, training loss: 5293.02, average training loss: 5626.25, base loss: 15958.23
[INFO 2017-06-30 06:42:47,213 main.py:52] epoch 6340, training loss: 5525.77, average training loss: 5626.17, base loss: 15957.58
[INFO 2017-06-30 06:42:50,388 main.py:52] epoch 6341, training loss: 5505.94, average training loss: 5625.58, base loss: 15957.80
[INFO 2017-06-30 06:42:53,528 main.py:52] epoch 6342, training loss: 5416.03, average training loss: 5625.46, base loss: 15957.96
[INFO 2017-06-30 06:42:56,640 main.py:52] epoch 6343, training loss: 5476.08, average training loss: 5625.56, base loss: 15957.85
[INFO 2017-06-30 06:42:59,803 main.py:52] epoch 6344, training loss: 5644.73, average training loss: 5625.88, base loss: 15958.28
[INFO 2017-06-30 06:43:02,943 main.py:52] epoch 6345, training loss: 5376.67, average training loss: 5625.54, base loss: 15958.34
[INFO 2017-06-30 06:43:06,084 main.py:52] epoch 6346, training loss: 5685.35, average training loss: 5625.58, base loss: 15958.29
[INFO 2017-06-30 06:43:09,218 main.py:52] epoch 6347, training loss: 5424.18, average training loss: 5625.47, base loss: 15958.09
[INFO 2017-06-30 06:43:12,373 main.py:52] epoch 6348, training loss: 5572.37, average training loss: 5625.55, base loss: 15957.60
[INFO 2017-06-30 06:43:15,499 main.py:52] epoch 6349, training loss: 5581.55, average training loss: 5625.32, base loss: 15957.32
[INFO 2017-06-30 06:43:18,653 main.py:52] epoch 6350, training loss: 5602.74, average training loss: 5625.14, base loss: 15957.70
[INFO 2017-06-30 06:43:21,817 main.py:52] epoch 6351, training loss: 5792.48, average training loss: 5625.42, base loss: 15958.11
[INFO 2017-06-30 06:43:24,959 main.py:52] epoch 6352, training loss: 5420.95, average training loss: 5625.36, base loss: 15957.77
[INFO 2017-06-30 06:43:28,088 main.py:52] epoch 6353, training loss: 5659.11, average training loss: 5625.21, base loss: 15957.92
[INFO 2017-06-30 06:43:31,269 main.py:52] epoch 6354, training loss: 6069.84, average training loss: 5625.16, base loss: 15958.22
[INFO 2017-06-30 06:43:34,472 main.py:52] epoch 6355, training loss: 5588.96, average training loss: 5625.03, base loss: 15958.10
[INFO 2017-06-30 06:43:37,664 main.py:52] epoch 6356, training loss: 5569.41, average training loss: 5624.89, base loss: 15958.43
[INFO 2017-06-30 06:43:40,828 main.py:52] epoch 6357, training loss: 5523.29, average training loss: 5624.86, base loss: 15958.34
[INFO 2017-06-30 06:43:43,971 main.py:52] epoch 6358, training loss: 5878.13, average training loss: 5625.16, base loss: 15958.43
[INFO 2017-06-30 06:43:47,110 main.py:52] epoch 6359, training loss: 5701.66, average training loss: 5625.13, base loss: 15958.23
[INFO 2017-06-30 06:43:50,246 main.py:52] epoch 6360, training loss: 5406.28, average training loss: 5624.93, base loss: 15958.14
[INFO 2017-06-30 06:43:53,354 main.py:52] epoch 6361, training loss: 5735.97, average training loss: 5625.31, base loss: 15958.35
[INFO 2017-06-30 06:43:56,507 main.py:52] epoch 6362, training loss: 5631.75, average training loss: 5625.32, base loss: 15958.69
[INFO 2017-06-30 06:43:59,616 main.py:52] epoch 6363, training loss: 5277.83, average training loss: 5624.98, base loss: 15958.70
[INFO 2017-06-30 06:44:02,812 main.py:52] epoch 6364, training loss: 5535.27, average training loss: 5625.12, base loss: 15958.46
[INFO 2017-06-30 06:44:06,011 main.py:52] epoch 6365, training loss: 5573.64, average training loss: 5625.14, base loss: 15958.01
[INFO 2017-06-30 06:44:09,124 main.py:52] epoch 6366, training loss: 5419.28, average training loss: 5624.66, base loss: 15957.94
[INFO 2017-06-30 06:44:12,293 main.py:52] epoch 6367, training loss: 5414.49, average training loss: 5624.41, base loss: 15957.84
[INFO 2017-06-30 06:44:15,447 main.py:52] epoch 6368, training loss: 5996.08, average training loss: 5624.60, base loss: 15958.05
[INFO 2017-06-30 06:44:18,571 main.py:52] epoch 6369, training loss: 5969.75, average training loss: 5625.04, base loss: 15958.37
[INFO 2017-06-30 06:44:21,707 main.py:52] epoch 6370, training loss: 5531.87, average training loss: 5624.63, base loss: 15958.25
[INFO 2017-06-30 06:44:24,890 main.py:52] epoch 6371, training loss: 5687.89, average training loss: 5624.59, base loss: 15958.17
[INFO 2017-06-30 06:44:28,050 main.py:52] epoch 6372, training loss: 5580.70, average training loss: 5624.23, base loss: 15958.48
[INFO 2017-06-30 06:44:31,177 main.py:52] epoch 6373, training loss: 5705.81, average training loss: 5624.36, base loss: 15958.46
[INFO 2017-06-30 06:44:34,353 main.py:52] epoch 6374, training loss: 5600.29, average training loss: 5624.35, base loss: 15958.46
[INFO 2017-06-30 06:44:37,503 main.py:52] epoch 6375, training loss: 5674.80, average training loss: 5624.42, base loss: 15958.73
[INFO 2017-06-30 06:44:40,596 main.py:52] epoch 6376, training loss: 5332.30, average training loss: 5624.01, base loss: 15959.04
[INFO 2017-06-30 06:44:43,754 main.py:52] epoch 6377, training loss: 5896.97, average training loss: 5624.52, base loss: 15959.89
[INFO 2017-06-30 06:44:46,875 main.py:52] epoch 6378, training loss: 5431.89, average training loss: 5624.19, base loss: 15960.04
[INFO 2017-06-30 06:44:50,005 main.py:52] epoch 6379, training loss: 5300.87, average training loss: 5623.80, base loss: 15960.16
[INFO 2017-06-30 06:44:53,179 main.py:52] epoch 6380, training loss: 5604.71, average training loss: 5623.96, base loss: 15959.60
[INFO 2017-06-30 06:44:56,388 main.py:52] epoch 6381, training loss: 5731.25, average training loss: 5624.52, base loss: 15959.63
[INFO 2017-06-30 06:44:59,487 main.py:52] epoch 6382, training loss: 5705.65, average training loss: 5624.19, base loss: 15959.72
[INFO 2017-06-30 06:45:02,643 main.py:52] epoch 6383, training loss: 5735.15, average training loss: 5624.62, base loss: 15959.57
[INFO 2017-06-30 06:45:05,765 main.py:52] epoch 6384, training loss: 5328.14, average training loss: 5624.26, base loss: 15959.20
[INFO 2017-06-30 06:45:08,892 main.py:52] epoch 6385, training loss: 5669.73, average training loss: 5624.45, base loss: 15958.70
[INFO 2017-06-30 06:45:12,045 main.py:52] epoch 6386, training loss: 5559.14, average training loss: 5624.08, base loss: 15958.46
[INFO 2017-06-30 06:45:15,172 main.py:52] epoch 6387, training loss: 5315.32, average training loss: 5623.75, base loss: 15958.30
[INFO 2017-06-30 06:45:18,295 main.py:52] epoch 6388, training loss: 5695.32, average training loss: 5623.94, base loss: 15958.47
[INFO 2017-06-30 06:45:21,426 main.py:52] epoch 6389, training loss: 5421.44, average training loss: 5623.84, base loss: 15958.17
[INFO 2017-06-30 06:45:24,570 main.py:52] epoch 6390, training loss: 5128.03, average training loss: 5623.27, base loss: 15957.75
[INFO 2017-06-30 06:45:27,716 main.py:52] epoch 6391, training loss: 5599.85, average training loss: 5623.11, base loss: 15957.71
[INFO 2017-06-30 06:45:30,895 main.py:52] epoch 6392, training loss: 5136.85, average training loss: 5622.49, base loss: 15957.58
[INFO 2017-06-30 06:45:34,031 main.py:52] epoch 6393, training loss: 5549.89, average training loss: 5622.73, base loss: 15958.20
[INFO 2017-06-30 06:45:37,195 main.py:52] epoch 6394, training loss: 5693.42, average training loss: 5622.54, base loss: 15958.68
[INFO 2017-06-30 06:45:40,331 main.py:52] epoch 6395, training loss: 5469.44, average training loss: 5622.32, base loss: 15958.71
[INFO 2017-06-30 06:45:43,491 main.py:52] epoch 6396, training loss: 5668.34, average training loss: 5622.03, base loss: 15958.87
[INFO 2017-06-30 06:45:46,667 main.py:52] epoch 6397, training loss: 5594.77, average training loss: 5622.00, base loss: 15959.66
[INFO 2017-06-30 06:45:49,813 main.py:52] epoch 6398, training loss: 5485.50, average training loss: 5621.91, base loss: 15959.72
[INFO 2017-06-30 06:45:52,953 main.py:52] epoch 6399, training loss: 5071.28, average training loss: 5620.94, base loss: 15959.34
[INFO 2017-06-30 06:45:52,954 main.py:54] epoch 6399, testing
[INFO 2017-06-30 06:46:05,994 main.py:97] average testing loss: 5668.49, base loss: 16268.65
[INFO 2017-06-30 06:46:05,994 main.py:98] improve_loss: 10600.15, improve_percent: 0.65
[INFO 2017-06-30 06:46:05,997 main.py:66] current best improved percent: 0.66
[INFO 2017-06-30 06:46:09,169 main.py:52] epoch 6400, training loss: 5571.30, average training loss: 5620.94, base loss: 15959.33
[INFO 2017-06-30 06:46:12,311 main.py:52] epoch 6401, training loss: 5631.90, average training loss: 5620.95, base loss: 15959.29
[INFO 2017-06-30 06:46:15,457 main.py:52] epoch 6402, training loss: 6173.23, average training loss: 5621.22, base loss: 15960.10
[INFO 2017-06-30 06:46:18,578 main.py:52] epoch 6403, training loss: 5880.18, average training loss: 5621.49, base loss: 15960.03
[INFO 2017-06-30 06:46:21,768 main.py:52] epoch 6404, training loss: 5617.08, average training loss: 5621.57, base loss: 15960.27
[INFO 2017-06-30 06:46:24,931 main.py:52] epoch 6405, training loss: 5751.42, average training loss: 5621.60, base loss: 15960.71
[INFO 2017-06-30 06:46:28,028 main.py:52] epoch 6406, training loss: 6034.67, average training loss: 5622.30, base loss: 15960.72
[INFO 2017-06-30 06:46:31,204 main.py:52] epoch 6407, training loss: 5682.95, average training loss: 5622.49, base loss: 15960.06
[INFO 2017-06-30 06:46:34,352 main.py:52] epoch 6408, training loss: 5992.29, average training loss: 5622.90, base loss: 15960.12
[INFO 2017-06-30 06:46:37,450 main.py:52] epoch 6409, training loss: 5450.92, average training loss: 5622.52, base loss: 15960.06
[INFO 2017-06-30 06:46:40,597 main.py:52] epoch 6410, training loss: 5511.51, average training loss: 5622.62, base loss: 15960.37
[INFO 2017-06-30 06:46:43,699 main.py:52] epoch 6411, training loss: 5476.48, average training loss: 5622.83, base loss: 15960.54
[INFO 2017-06-30 06:46:46,838 main.py:52] epoch 6412, training loss: 5518.99, average training loss: 5622.72, base loss: 15960.79
[INFO 2017-06-30 06:46:49,954 main.py:52] epoch 6413, training loss: 5539.96, average training loss: 5622.18, base loss: 15960.31
[INFO 2017-06-30 06:46:53,072 main.py:52] epoch 6414, training loss: 6116.29, average training loss: 5622.90, base loss: 15961.04
[INFO 2017-06-30 06:46:56,187 main.py:52] epoch 6415, training loss: 5540.24, average training loss: 5623.00, base loss: 15961.31
[INFO 2017-06-30 06:46:59,296 main.py:52] epoch 6416, training loss: 5752.58, average training loss: 5622.86, base loss: 15960.61
[INFO 2017-06-30 06:47:02,416 main.py:52] epoch 6417, training loss: 5498.94, average training loss: 5622.71, base loss: 15960.39
[INFO 2017-06-30 06:47:05,583 main.py:52] epoch 6418, training loss: 5574.93, average training loss: 5622.84, base loss: 15960.21
[INFO 2017-06-30 06:47:08,712 main.py:52] epoch 6419, training loss: 5686.72, average training loss: 5623.30, base loss: 15960.36
[INFO 2017-06-30 06:47:11,820 main.py:52] epoch 6420, training loss: 5451.99, average training loss: 5622.73, base loss: 15959.82
[INFO 2017-06-30 06:47:14,932 main.py:52] epoch 6421, training loss: 5256.71, average training loss: 5622.30, base loss: 15958.93
[INFO 2017-06-30 06:47:18,090 main.py:52] epoch 6422, training loss: 5576.40, average training loss: 5622.35, base loss: 15958.95
[INFO 2017-06-30 06:47:21,207 main.py:52] epoch 6423, training loss: 5417.84, average training loss: 5622.43, base loss: 15958.59
[INFO 2017-06-30 06:47:24,331 main.py:52] epoch 6424, training loss: 5450.42, average training loss: 5622.33, base loss: 15958.49
[INFO 2017-06-30 06:47:27,469 main.py:52] epoch 6425, training loss: 5810.32, average training loss: 5622.24, base loss: 15959.14
[INFO 2017-06-30 06:47:30,644 main.py:52] epoch 6426, training loss: 5546.78, average training loss: 5621.98, base loss: 15959.02
[INFO 2017-06-30 06:47:33,808 main.py:52] epoch 6427, training loss: 5376.94, average training loss: 5621.53, base loss: 15958.69
[INFO 2017-06-30 06:47:36,963 main.py:52] epoch 6428, training loss: 6031.43, average training loss: 5622.55, base loss: 15959.05
[INFO 2017-06-30 06:47:40,124 main.py:52] epoch 6429, training loss: 5365.43, average training loss: 5622.30, base loss: 15958.91
[INFO 2017-06-30 06:47:43,249 main.py:52] epoch 6430, training loss: 5360.30, average training loss: 5622.22, base loss: 15959.00
[INFO 2017-06-30 06:47:46,388 main.py:52] epoch 6431, training loss: 5260.47, average training loss: 5622.07, base loss: 15958.60
[INFO 2017-06-30 06:47:49,528 main.py:52] epoch 6432, training loss: 5618.74, average training loss: 5621.56, base loss: 15958.71
[INFO 2017-06-30 06:47:52,646 main.py:52] epoch 6433, training loss: 5290.08, average training loss: 5620.97, base loss: 15958.45
[INFO 2017-06-30 06:47:55,786 main.py:52] epoch 6434, training loss: 5774.19, average training loss: 5620.84, base loss: 15958.85
[INFO 2017-06-30 06:47:58,917 main.py:52] epoch 6435, training loss: 5629.42, average training loss: 5620.99, base loss: 15959.10
[INFO 2017-06-30 06:48:02,059 main.py:52] epoch 6436, training loss: 5835.05, average training loss: 5621.19, base loss: 15959.35
[INFO 2017-06-30 06:48:05,202 main.py:52] epoch 6437, training loss: 5648.03, average training loss: 5621.36, base loss: 15959.79
[INFO 2017-06-30 06:48:08,313 main.py:52] epoch 6438, training loss: 5582.30, average training loss: 5621.16, base loss: 15959.52
[INFO 2017-06-30 06:48:11,465 main.py:52] epoch 6439, training loss: 5556.92, average training loss: 5621.09, base loss: 15959.44
[INFO 2017-06-30 06:48:14,617 main.py:52] epoch 6440, training loss: 5724.90, average training loss: 5621.34, base loss: 15959.20
[INFO 2017-06-30 06:48:17,806 main.py:52] epoch 6441, training loss: 5506.28, average training loss: 5621.02, base loss: 15958.68
[INFO 2017-06-30 06:48:20,946 main.py:52] epoch 6442, training loss: 5370.40, average training loss: 5620.66, base loss: 15958.44
[INFO 2017-06-30 06:48:24,104 main.py:52] epoch 6443, training loss: 5344.50, average training loss: 5620.19, base loss: 15958.36
[INFO 2017-06-30 06:48:27,276 main.py:52] epoch 6444, training loss: 5537.52, average training loss: 5620.23, base loss: 15958.29
[INFO 2017-06-30 06:48:30,396 main.py:52] epoch 6445, training loss: 5462.14, average training loss: 5620.28, base loss: 15958.43
[INFO 2017-06-30 06:48:33,525 main.py:52] epoch 6446, training loss: 5244.35, average training loss: 5619.61, base loss: 15958.47
[INFO 2017-06-30 06:48:36,630 main.py:52] epoch 6447, training loss: 5588.30, average training loss: 5619.80, base loss: 15958.50
[INFO 2017-06-30 06:48:39,766 main.py:52] epoch 6448, training loss: 5739.16, average training loss: 5620.07, base loss: 15958.34
[INFO 2017-06-30 06:48:42,865 main.py:52] epoch 6449, training loss: 5430.41, average training loss: 5619.45, base loss: 15957.98
[INFO 2017-06-30 06:48:46,042 main.py:52] epoch 6450, training loss: 5701.60, average training loss: 5619.14, base loss: 15957.91
[INFO 2017-06-30 06:48:49,195 main.py:52] epoch 6451, training loss: 5419.22, average training loss: 5619.04, base loss: 15957.99
[INFO 2017-06-30 06:48:52,387 main.py:52] epoch 6452, training loss: 5285.87, average training loss: 5618.76, base loss: 15957.90
[INFO 2017-06-30 06:48:55,536 main.py:52] epoch 6453, training loss: 5253.02, average training loss: 5618.31, base loss: 15957.75
[INFO 2017-06-30 06:48:58,766 main.py:52] epoch 6454, training loss: 5314.96, average training loss: 5617.73, base loss: 15957.57
[INFO 2017-06-30 06:49:01,908 main.py:52] epoch 6455, training loss: 5486.16, average training loss: 5617.74, base loss: 15957.56
[INFO 2017-06-30 06:49:05,074 main.py:52] epoch 6456, training loss: 5154.31, average training loss: 5617.02, base loss: 15957.15
[INFO 2017-06-30 06:49:08,224 main.py:52] epoch 6457, training loss: 5489.31, average training loss: 5616.91, base loss: 15957.00
[INFO 2017-06-30 06:49:11,366 main.py:52] epoch 6458, training loss: 5973.19, average training loss: 5616.90, base loss: 15957.17
[INFO 2017-06-30 06:49:14,505 main.py:52] epoch 6459, training loss: 5371.38, average training loss: 5616.81, base loss: 15956.41
[INFO 2017-06-30 06:49:17,669 main.py:52] epoch 6460, training loss: 5362.75, average training loss: 5616.37, base loss: 15956.05
[INFO 2017-06-30 06:49:20,827 main.py:52] epoch 6461, training loss: 5563.78, average training loss: 5615.82, base loss: 15956.02
[INFO 2017-06-30 06:49:23,935 main.py:52] epoch 6462, training loss: 5294.40, average training loss: 5615.65, base loss: 15956.22
[INFO 2017-06-30 06:49:27,080 main.py:52] epoch 6463, training loss: 5765.98, average training loss: 5615.29, base loss: 15956.44
[INFO 2017-06-30 06:49:30,187 main.py:52] epoch 6464, training loss: 5734.68, average training loss: 5615.11, base loss: 15956.45
[INFO 2017-06-30 06:49:33,382 main.py:52] epoch 6465, training loss: 5508.69, average training loss: 5615.30, base loss: 15956.41
[INFO 2017-06-30 06:49:36,512 main.py:52] epoch 6466, training loss: 5387.55, average training loss: 5615.00, base loss: 15955.96
[INFO 2017-06-30 06:49:39,603 main.py:52] epoch 6467, training loss: 5342.02, average training loss: 5614.69, base loss: 15955.80
[INFO 2017-06-30 06:49:42,749 main.py:52] epoch 6468, training loss: 5743.80, average training loss: 5614.67, base loss: 15956.20
[INFO 2017-06-30 06:49:45,868 main.py:52] epoch 6469, training loss: 5345.29, average training loss: 5614.38, base loss: 15956.42
[INFO 2017-06-30 06:49:49,005 main.py:52] epoch 6470, training loss: 5606.81, average training loss: 5614.40, base loss: 15956.35
[INFO 2017-06-30 06:49:52,156 main.py:52] epoch 6471, training loss: 6076.56, average training loss: 5614.70, base loss: 15956.75
[INFO 2017-06-30 06:49:55,299 main.py:52] epoch 6472, training loss: 5695.64, average training loss: 5615.03, base loss: 15957.00
[INFO 2017-06-30 06:49:58,476 main.py:52] epoch 6473, training loss: 5294.86, average training loss: 5614.51, base loss: 15956.79
[INFO 2017-06-30 06:50:01,654 main.py:52] epoch 6474, training loss: 5453.45, average training loss: 5614.16, base loss: 15956.87
[INFO 2017-06-30 06:50:04,787 main.py:52] epoch 6475, training loss: 5444.75, average training loss: 5614.22, base loss: 15956.83
[INFO 2017-06-30 06:50:07,949 main.py:52] epoch 6476, training loss: 5829.85, average training loss: 5614.14, base loss: 15957.17
[INFO 2017-06-30 06:50:11,145 main.py:52] epoch 6477, training loss: 5597.46, average training loss: 5614.09, base loss: 15957.42
[INFO 2017-06-30 06:50:14,258 main.py:52] epoch 6478, training loss: 5416.80, average training loss: 5613.88, base loss: 15957.67
[INFO 2017-06-30 06:50:17,419 main.py:52] epoch 6479, training loss: 5778.89, average training loss: 5613.53, base loss: 15958.20
[INFO 2017-06-30 06:50:20,527 main.py:52] epoch 6480, training loss: 5889.33, average training loss: 5614.07, base loss: 15958.39
[INFO 2017-06-30 06:50:23,632 main.py:52] epoch 6481, training loss: 5523.43, average training loss: 5614.11, base loss: 15958.46
[INFO 2017-06-30 06:50:26,789 main.py:52] epoch 6482, training loss: 5455.80, average training loss: 5613.82, base loss: 15958.44
[INFO 2017-06-30 06:50:29,941 main.py:52] epoch 6483, training loss: 5522.97, average training loss: 5613.61, base loss: 15958.19
[INFO 2017-06-30 06:50:33,108 main.py:52] epoch 6484, training loss: 5878.50, average training loss: 5613.72, base loss: 15958.74
[INFO 2017-06-30 06:50:36,256 main.py:52] epoch 6485, training loss: 5658.81, average training loss: 5613.53, base loss: 15959.11
[INFO 2017-06-30 06:50:39,421 main.py:52] epoch 6486, training loss: 5818.50, average training loss: 5613.41, base loss: 15958.61
[INFO 2017-06-30 06:50:42,559 main.py:52] epoch 6487, training loss: 5614.06, average training loss: 5612.97, base loss: 15958.39
[INFO 2017-06-30 06:50:45,746 main.py:52] epoch 6488, training loss: 5443.13, average training loss: 5612.61, base loss: 15958.05
[INFO 2017-06-30 06:50:48,881 main.py:52] epoch 6489, training loss: 6012.98, average training loss: 5612.44, base loss: 15958.58
[INFO 2017-06-30 06:50:52,058 main.py:52] epoch 6490, training loss: 5587.84, average training loss: 5612.49, base loss: 15958.73
[INFO 2017-06-30 06:50:55,230 main.py:52] epoch 6491, training loss: 5921.20, average training loss: 5612.61, base loss: 15959.02
[INFO 2017-06-30 06:50:58,369 main.py:52] epoch 6492, training loss: 5854.66, average training loss: 5612.83, base loss: 15959.30
[INFO 2017-06-30 06:51:01,570 main.py:52] epoch 6493, training loss: 5414.11, average training loss: 5611.89, base loss: 15958.96
[INFO 2017-06-30 06:51:04,703 main.py:52] epoch 6494, training loss: 5214.73, average training loss: 5611.69, base loss: 15958.71
[INFO 2017-06-30 06:51:07,873 main.py:52] epoch 6495, training loss: 5689.92, average training loss: 5611.85, base loss: 15958.91
[INFO 2017-06-30 06:51:11,031 main.py:52] epoch 6496, training loss: 6163.58, average training loss: 5612.09, base loss: 15959.27
[INFO 2017-06-30 06:51:14,151 main.py:52] epoch 6497, training loss: 5698.78, average training loss: 5612.10, base loss: 15959.56
[INFO 2017-06-30 06:51:17,284 main.py:52] epoch 6498, training loss: 5564.50, average training loss: 5612.00, base loss: 15959.55
[INFO 2017-06-30 06:51:20,439 main.py:52] epoch 6499, training loss: 5822.69, average training loss: 5611.74, base loss: 15960.12
[INFO 2017-06-30 06:51:20,439 main.py:54] epoch 6499, testing
[INFO 2017-06-30 06:51:33,388 main.py:97] average testing loss: 5687.72, base loss: 16341.68
[INFO 2017-06-30 06:51:33,388 main.py:98] improve_loss: 10653.95, improve_percent: 0.65
[INFO 2017-06-30 06:51:33,389 main.py:66] current best improved percent: 0.66
[INFO 2017-06-30 06:51:36,596 main.py:52] epoch 6500, training loss: 5699.06, average training loss: 5611.70, base loss: 15960.21
[INFO 2017-06-30 06:51:39,741 main.py:52] epoch 6501, training loss: 5755.82, average training loss: 5611.31, base loss: 15960.53
[INFO 2017-06-30 06:51:42,865 main.py:52] epoch 6502, training loss: 6032.41, average training loss: 5611.33, base loss: 15960.91
[INFO 2017-06-30 06:51:46,029 main.py:52] epoch 6503, training loss: 5752.30, average training loss: 5611.50, base loss: 15961.39
[INFO 2017-06-30 06:51:49,165 main.py:52] epoch 6504, training loss: 5749.32, average training loss: 5611.53, base loss: 15961.90
[INFO 2017-06-30 06:51:52,287 main.py:52] epoch 6505, training loss: 6115.15, average training loss: 5611.88, base loss: 15962.53
[INFO 2017-06-30 06:51:55,408 main.py:52] epoch 6506, training loss: 5419.51, average training loss: 5611.31, base loss: 15961.96
[INFO 2017-06-30 06:51:58,540 main.py:52] epoch 6507, training loss: 5206.04, average training loss: 5610.21, base loss: 15961.48
[INFO 2017-06-30 06:52:01,715 main.py:52] epoch 6508, training loss: 5469.33, average training loss: 5610.39, base loss: 15961.56
[INFO 2017-06-30 06:52:04,876 main.py:52] epoch 6509, training loss: 5496.48, average training loss: 5609.86, base loss: 15961.60
[INFO 2017-06-30 06:52:08,004 main.py:52] epoch 6510, training loss: 5674.37, average training loss: 5609.67, base loss: 15961.34
[INFO 2017-06-30 06:52:11,104 main.py:52] epoch 6511, training loss: 5838.98, average training loss: 5609.88, base loss: 15961.20
[INFO 2017-06-30 06:52:14,263 main.py:52] epoch 6512, training loss: 5321.73, average training loss: 5609.54, base loss: 15960.91
[INFO 2017-06-30 06:52:17,399 main.py:52] epoch 6513, training loss: 5851.52, average training loss: 5609.80, base loss: 15961.13
[INFO 2017-06-30 06:52:20,538 main.py:52] epoch 6514, training loss: 5266.29, average training loss: 5609.45, base loss: 15961.19
[INFO 2017-06-30 06:52:23,703 main.py:52] epoch 6515, training loss: 5668.74, average training loss: 5609.42, base loss: 15961.23
[INFO 2017-06-30 06:52:26,874 main.py:52] epoch 6516, training loss: 5576.47, average training loss: 5609.83, base loss: 15961.37
[INFO 2017-06-30 06:52:30,032 main.py:52] epoch 6517, training loss: 5993.20, average training loss: 5610.26, base loss: 15961.88
[INFO 2017-06-30 06:52:33,198 main.py:52] epoch 6518, training loss: 5660.09, average training loss: 5609.84, base loss: 15961.96
[INFO 2017-06-30 06:52:36,345 main.py:52] epoch 6519, training loss: 5276.73, average training loss: 5609.62, base loss: 15961.62
[INFO 2017-06-30 06:52:39,489 main.py:52] epoch 6520, training loss: 5431.21, average training loss: 5608.89, base loss: 15961.31
[INFO 2017-06-30 06:52:42,610 main.py:52] epoch 6521, training loss: 5590.60, average training loss: 5608.27, base loss: 15961.57
[INFO 2017-06-30 06:52:45,761 main.py:52] epoch 6522, training loss: 5636.26, average training loss: 5608.14, base loss: 15961.67
[INFO 2017-06-30 06:52:48,917 main.py:52] epoch 6523, training loss: 5471.42, average training loss: 5607.88, base loss: 15961.56
[INFO 2017-06-30 06:52:52,048 main.py:52] epoch 6524, training loss: 5412.91, average training loss: 5607.28, base loss: 15961.53
[INFO 2017-06-30 06:52:55,149 main.py:52] epoch 6525, training loss: 5314.73, average training loss: 5606.74, base loss: 15961.21
[INFO 2017-06-30 06:52:58,339 main.py:52] epoch 6526, training loss: 5104.00, average training loss: 5606.05, base loss: 15960.94
[INFO 2017-06-30 06:53:01,479 main.py:52] epoch 6527, training loss: 5474.14, average training loss: 5606.16, base loss: 15961.14
[INFO 2017-06-30 06:53:04,613 main.py:52] epoch 6528, training loss: 4973.40, average training loss: 5605.54, base loss: 15960.28
[INFO 2017-06-30 06:53:07,800 main.py:52] epoch 6529, training loss: 5362.71, average training loss: 5605.19, base loss: 15960.19
[INFO 2017-06-30 06:53:10,941 main.py:52] epoch 6530, training loss: 5146.06, average training loss: 5604.87, base loss: 15959.78
[INFO 2017-06-30 06:53:14,143 main.py:52] epoch 6531, training loss: 5512.82, average training loss: 5604.80, base loss: 15960.24
[INFO 2017-06-30 06:53:17,243 main.py:52] epoch 6532, training loss: 6057.34, average training loss: 5604.41, base loss: 15960.97
[INFO 2017-06-30 06:53:20,401 main.py:52] epoch 6533, training loss: 5677.04, average training loss: 5604.28, base loss: 15961.10
[INFO 2017-06-30 06:53:23,540 main.py:52] epoch 6534, training loss: 5886.19, average training loss: 5604.50, base loss: 15961.21
[INFO 2017-06-30 06:53:26,687 main.py:52] epoch 6535, training loss: 5350.04, average training loss: 5603.95, base loss: 15961.51
[INFO 2017-06-30 06:53:29,826 main.py:52] epoch 6536, training loss: 5757.07, average training loss: 5604.19, base loss: 15961.46
[INFO 2017-06-30 06:53:32,963 main.py:52] epoch 6537, training loss: 5980.61, average training loss: 5604.50, base loss: 15961.95
[INFO 2017-06-30 06:53:36,086 main.py:52] epoch 6538, training loss: 5502.38, average training loss: 5604.30, base loss: 15961.65
[INFO 2017-06-30 06:53:39,217 main.py:52] epoch 6539, training loss: 5750.13, average training loss: 5604.22, base loss: 15961.81
[INFO 2017-06-30 06:53:42,302 main.py:52] epoch 6540, training loss: 5543.78, average training loss: 5604.31, base loss: 15961.40
[INFO 2017-06-30 06:53:45,427 main.py:52] epoch 6541, training loss: 6288.67, average training loss: 5604.82, base loss: 15962.07
[INFO 2017-06-30 06:53:48,593 main.py:52] epoch 6542, training loss: 5777.40, average training loss: 5604.97, base loss: 15961.88
[INFO 2017-06-30 06:53:51,732 main.py:52] epoch 6543, training loss: 5525.74, average training loss: 5604.93, base loss: 15961.74
[INFO 2017-06-30 06:53:54,842 main.py:52] epoch 6544, training loss: 5680.35, average training loss: 5604.87, base loss: 15961.76
[INFO 2017-06-30 06:53:57,956 main.py:52] epoch 6545, training loss: 5269.13, average training loss: 5604.55, base loss: 15961.16
[INFO 2017-06-30 06:54:01,076 main.py:52] epoch 6546, training loss: 5548.95, average training loss: 5604.09, base loss: 15961.14
[INFO 2017-06-30 06:54:04,186 main.py:52] epoch 6547, training loss: 5687.72, average training loss: 5604.38, base loss: 15961.11
[INFO 2017-06-30 06:54:07,335 main.py:52] epoch 6548, training loss: 5739.64, average training loss: 5604.42, base loss: 15961.01
[INFO 2017-06-30 06:54:10,479 main.py:52] epoch 6549, training loss: 5642.46, average training loss: 5604.41, base loss: 15960.78
[INFO 2017-06-30 06:54:13,575 main.py:52] epoch 6550, training loss: 5639.52, average training loss: 5604.65, base loss: 15960.53
[INFO 2017-06-30 06:54:16,717 main.py:52] epoch 6551, training loss: 5440.98, average training loss: 5604.60, base loss: 15960.34
[INFO 2017-06-30 06:54:19,886 main.py:52] epoch 6552, training loss: 5502.00, average training loss: 5604.82, base loss: 15960.51
[INFO 2017-06-30 06:54:23,025 main.py:52] epoch 6553, training loss: 6179.71, average training loss: 5605.61, base loss: 15960.75
[INFO 2017-06-30 06:54:26,158 main.py:52] epoch 6554, training loss: 5471.66, average training loss: 5605.74, base loss: 15960.35
[INFO 2017-06-30 06:54:29,359 main.py:52] epoch 6555, training loss: 5797.60, average training loss: 5605.98, base loss: 15960.38
[INFO 2017-06-30 06:54:32,490 main.py:52] epoch 6556, training loss: 5998.71, average training loss: 5605.92, base loss: 15960.89
[INFO 2017-06-30 06:54:35,690 main.py:52] epoch 6557, training loss: 5854.26, average training loss: 5606.56, base loss: 15961.83
[INFO 2017-06-30 06:54:38,817 main.py:52] epoch 6558, training loss: 5215.95, average training loss: 5605.92, base loss: 15961.28
[INFO 2017-06-30 06:54:41,971 main.py:52] epoch 6559, training loss: 5481.81, average training loss: 5605.82, base loss: 15960.37
[INFO 2017-06-30 06:54:45,087 main.py:52] epoch 6560, training loss: 5578.55, average training loss: 5605.54, base loss: 15960.11
[INFO 2017-06-30 06:54:48,249 main.py:52] epoch 6561, training loss: 6103.97, average training loss: 5606.07, base loss: 15960.33
[INFO 2017-06-30 06:54:51,432 main.py:52] epoch 6562, training loss: 5017.34, average training loss: 5605.55, base loss: 15959.75
[INFO 2017-06-30 06:54:54,626 main.py:52] epoch 6563, training loss: 5760.93, average training loss: 5605.65, base loss: 15959.97
[INFO 2017-06-30 06:54:57,740 main.py:52] epoch 6564, training loss: 5337.40, average training loss: 5605.12, base loss: 15959.87
[INFO 2017-06-30 06:55:00,880 main.py:52] epoch 6565, training loss: 5590.97, average training loss: 5604.91, base loss: 15959.96
[INFO 2017-06-30 06:55:04,006 main.py:52] epoch 6566, training loss: 5730.23, average training loss: 5604.97, base loss: 15959.34
[INFO 2017-06-30 06:55:07,115 main.py:52] epoch 6567, training loss: 5594.70, average training loss: 5604.83, base loss: 15959.31
[INFO 2017-06-30 06:55:10,275 main.py:52] epoch 6568, training loss: 5379.89, average training loss: 5605.05, base loss: 15959.53
[INFO 2017-06-30 06:55:13,455 main.py:52] epoch 6569, training loss: 5891.32, average training loss: 5604.90, base loss: 15960.15
[INFO 2017-06-30 06:55:16,608 main.py:52] epoch 6570, training loss: 5560.83, average training loss: 5604.50, base loss: 15960.41
[INFO 2017-06-30 06:55:19,727 main.py:52] epoch 6571, training loss: 5436.88, average training loss: 5604.20, base loss: 15960.20
[INFO 2017-06-30 06:55:22,896 main.py:52] epoch 6572, training loss: 5258.87, average training loss: 5604.26, base loss: 15959.54
[INFO 2017-06-30 06:55:26,062 main.py:52] epoch 6573, training loss: 5720.66, average training loss: 5604.61, base loss: 15959.14
[INFO 2017-06-30 06:55:29,239 main.py:52] epoch 6574, training loss: 5296.96, average training loss: 5603.95, base loss: 15958.07
[INFO 2017-06-30 06:55:32,362 main.py:52] epoch 6575, training loss: 5870.60, average training loss: 5604.42, base loss: 15957.73
[INFO 2017-06-30 06:55:35,521 main.py:52] epoch 6576, training loss: 5647.08, average training loss: 5604.52, base loss: 15957.65
[INFO 2017-06-30 06:55:38,647 main.py:52] epoch 6577, training loss: 5277.09, average training loss: 5604.50, base loss: 15957.44
[INFO 2017-06-30 06:55:41,809 main.py:52] epoch 6578, training loss: 5481.73, average training loss: 5604.46, base loss: 15957.44
[INFO 2017-06-30 06:55:44,945 main.py:52] epoch 6579, training loss: 5154.23, average training loss: 5603.91, base loss: 15957.17
[INFO 2017-06-30 06:55:48,088 main.py:52] epoch 6580, training loss: 5700.55, average training loss: 5603.58, base loss: 15957.16
[INFO 2017-06-30 06:55:51,250 main.py:52] epoch 6581, training loss: 5225.53, average training loss: 5602.86, base loss: 15957.08
[INFO 2017-06-30 06:55:54,405 main.py:52] epoch 6582, training loss: 6136.10, average training loss: 5603.39, base loss: 15957.37
[INFO 2017-06-30 06:55:57,516 main.py:52] epoch 6583, training loss: 6087.60, average training loss: 5603.81, base loss: 15957.74
[INFO 2017-06-30 06:56:00,675 main.py:52] epoch 6584, training loss: 5369.67, average training loss: 5603.71, base loss: 15957.72
[INFO 2017-06-30 06:56:03,842 main.py:52] epoch 6585, training loss: 5449.82, average training loss: 5603.49, base loss: 15957.83
[INFO 2017-06-30 06:56:06,963 main.py:52] epoch 6586, training loss: 5664.96, average training loss: 5603.59, base loss: 15957.55
[INFO 2017-06-30 06:56:10,184 main.py:52] epoch 6587, training loss: 5586.72, average training loss: 5602.92, base loss: 15957.38
[INFO 2017-06-30 06:56:13,367 main.py:52] epoch 6588, training loss: 5519.48, average training loss: 5602.75, base loss: 15957.82
[INFO 2017-06-30 06:56:16,516 main.py:52] epoch 6589, training loss: 5456.21, average training loss: 5603.05, base loss: 15957.87
[INFO 2017-06-30 06:56:19,635 main.py:52] epoch 6590, training loss: 5807.66, average training loss: 5603.17, base loss: 15958.09
[INFO 2017-06-30 06:56:22,771 main.py:52] epoch 6591, training loss: 5623.56, average training loss: 5603.28, base loss: 15957.84
[INFO 2017-06-30 06:56:25,985 main.py:52] epoch 6592, training loss: 5269.28, average training loss: 5602.59, base loss: 15957.45
[INFO 2017-06-30 06:56:29,155 main.py:52] epoch 6593, training loss: 5499.76, average training loss: 5601.86, base loss: 15957.37
[INFO 2017-06-30 06:56:32,285 main.py:52] epoch 6594, training loss: 5621.82, average training loss: 5601.56, base loss: 15957.30
[INFO 2017-06-30 06:56:35,477 main.py:52] epoch 6595, training loss: 5585.79, average training loss: 5601.82, base loss: 15956.96
[INFO 2017-06-30 06:56:38,607 main.py:52] epoch 6596, training loss: 5121.12, average training loss: 5601.35, base loss: 15956.85
[INFO 2017-06-30 06:56:41,762 main.py:52] epoch 6597, training loss: 5809.98, average training loss: 5601.98, base loss: 15957.13
[INFO 2017-06-30 06:56:44,928 main.py:52] epoch 6598, training loss: 5593.47, average training loss: 5602.24, base loss: 15957.39
[INFO 2017-06-30 06:56:48,073 main.py:52] epoch 6599, training loss: 5827.31, average training loss: 5602.51, base loss: 15957.89
[INFO 2017-06-30 06:56:48,074 main.py:54] epoch 6599, testing
[INFO 2017-06-30 06:57:01,238 main.py:97] average testing loss: 5537.75, base loss: 15683.60
[INFO 2017-06-30 06:57:01,238 main.py:98] improve_loss: 10145.85, improve_percent: 0.65
[INFO 2017-06-30 06:57:01,239 main.py:66] current best improved percent: 0.66
[INFO 2017-06-30 06:57:04,404 main.py:52] epoch 6600, training loss: 5485.31, average training loss: 5602.48, base loss: 15957.70
[INFO 2017-06-30 06:57:07,567 main.py:52] epoch 6601, training loss: 5566.07, average training loss: 5602.58, base loss: 15957.82
[INFO 2017-06-30 06:57:10,712 main.py:52] epoch 6602, training loss: 5042.70, average training loss: 5601.80, base loss: 15957.81
[INFO 2017-06-30 06:57:13,843 main.py:52] epoch 6603, training loss: 5776.49, average training loss: 5601.87, base loss: 15958.38
[INFO 2017-06-30 06:57:16,940 main.py:52] epoch 6604, training loss: 5776.88, average training loss: 5601.89, base loss: 15958.80
[INFO 2017-06-30 06:57:20,074 main.py:52] epoch 6605, training loss: 5494.13, average training loss: 5601.70, base loss: 15958.73
[INFO 2017-06-30 06:57:23,242 main.py:52] epoch 6606, training loss: 5873.19, average training loss: 5601.86, base loss: 15958.66
[INFO 2017-06-30 06:57:26,379 main.py:52] epoch 6607, training loss: 5219.24, average training loss: 5600.87, base loss: 15958.00
[INFO 2017-06-30 06:57:29,513 main.py:52] epoch 6608, training loss: 5586.87, average training loss: 5600.75, base loss: 15957.84
[INFO 2017-06-30 06:57:32,643 main.py:52] epoch 6609, training loss: 5760.48, average training loss: 5600.78, base loss: 15957.62
[INFO 2017-06-30 06:57:35,784 main.py:52] epoch 6610, training loss: 5569.08, average training loss: 5600.76, base loss: 15957.14
[INFO 2017-06-30 06:57:38,955 main.py:52] epoch 6611, training loss: 5755.06, average training loss: 5600.62, base loss: 15957.39
[INFO 2017-06-30 06:57:42,045 main.py:52] epoch 6612, training loss: 5481.59, average training loss: 5600.58, base loss: 15957.13
[INFO 2017-06-30 06:57:45,186 main.py:52] epoch 6613, training loss: 5741.71, average training loss: 5600.28, base loss: 15957.26
[INFO 2017-06-30 06:57:48,360 main.py:52] epoch 6614, training loss: 6236.30, average training loss: 5600.86, base loss: 15957.99
[INFO 2017-06-30 06:57:51,503 main.py:52] epoch 6615, training loss: 5295.41, average training loss: 5600.62, base loss: 15957.53
[INFO 2017-06-30 06:57:54,642 main.py:52] epoch 6616, training loss: 5839.42, average training loss: 5600.90, base loss: 15957.94
[INFO 2017-06-30 06:57:57,783 main.py:52] epoch 6617, training loss: 5288.44, average training loss: 5600.89, base loss: 15957.52
[INFO 2017-06-30 06:58:00,954 main.py:52] epoch 6618, training loss: 5363.33, average training loss: 5600.40, base loss: 15957.37
[INFO 2017-06-30 06:58:04,088 main.py:52] epoch 6619, training loss: 5601.40, average training loss: 5600.04, base loss: 15957.41
[INFO 2017-06-30 06:58:07,264 main.py:52] epoch 6620, training loss: 5571.97, average training loss: 5599.46, base loss: 15957.66
[INFO 2017-06-30 06:58:10,418 main.py:52] epoch 6621, training loss: 5386.79, average training loss: 5599.18, base loss: 15957.58
[INFO 2017-06-30 06:58:13,514 main.py:52] epoch 6622, training loss: 5580.22, average training loss: 5599.34, base loss: 15958.26
[INFO 2017-06-30 06:58:16,657 main.py:52] epoch 6623, training loss: 5363.38, average training loss: 5599.07, base loss: 15958.03
[INFO 2017-06-30 06:58:19,811 main.py:52] epoch 6624, training loss: 5661.50, average training loss: 5599.27, base loss: 15957.73
[INFO 2017-06-30 06:58:22,951 main.py:52] epoch 6625, training loss: 5284.96, average training loss: 5599.15, base loss: 15957.74
[INFO 2017-06-30 06:58:26,067 main.py:52] epoch 6626, training loss: 5336.17, average training loss: 5599.04, base loss: 15958.03
[INFO 2017-06-30 06:58:29,229 main.py:52] epoch 6627, training loss: 5292.89, average training loss: 5598.58, base loss: 15957.68
[INFO 2017-06-30 06:58:32,377 main.py:52] epoch 6628, training loss: 5639.76, average training loss: 5598.24, base loss: 15957.91
[INFO 2017-06-30 06:58:35,542 main.py:52] epoch 6629, training loss: 5294.85, average training loss: 5597.60, base loss: 15957.77
[INFO 2017-06-30 06:58:38,747 main.py:52] epoch 6630, training loss: 5377.32, average training loss: 5596.88, base loss: 15957.58
[INFO 2017-06-30 06:58:41,899 main.py:52] epoch 6631, training loss: 5684.96, average training loss: 5596.97, base loss: 15957.50
[INFO 2017-06-30 06:58:45,047 main.py:52] epoch 6632, training loss: 5665.95, average training loss: 5597.18, base loss: 15957.80
[INFO 2017-06-30 06:58:48,203 main.py:52] epoch 6633, training loss: 5792.45, average training loss: 5596.91, base loss: 15958.15
[INFO 2017-06-30 06:58:51,354 main.py:52] epoch 6634, training loss: 5044.12, average training loss: 5596.26, base loss: 15957.74
[INFO 2017-06-30 06:58:54,510 main.py:52] epoch 6635, training loss: 5540.14, average training loss: 5596.35, base loss: 15957.93
[INFO 2017-06-30 06:58:57,650 main.py:52] epoch 6636, training loss: 5467.71, average training loss: 5596.22, base loss: 15957.86
[INFO 2017-06-30 06:59:00,800 main.py:52] epoch 6637, training loss: 5881.16, average training loss: 5596.56, base loss: 15958.52
[INFO 2017-06-30 06:59:03,943 main.py:52] epoch 6638, training loss: 5649.24, average training loss: 5596.67, base loss: 15958.62
[INFO 2017-06-30 06:59:07,092 main.py:52] epoch 6639, training loss: 5906.80, average training loss: 5596.89, base loss: 15958.61
[INFO 2017-06-30 06:59:10,256 main.py:52] epoch 6640, training loss: 5799.14, average training loss: 5596.86, base loss: 15958.80
[INFO 2017-06-30 06:59:13,369 main.py:52] epoch 6641, training loss: 5700.49, average training loss: 5596.76, base loss: 15958.98
[INFO 2017-06-30 06:59:16,460 main.py:52] epoch 6642, training loss: 5671.70, average training loss: 5597.05, base loss: 15959.42
[INFO 2017-06-30 06:59:19,597 main.py:52] epoch 6643, training loss: 5709.66, average training loss: 5596.98, base loss: 15959.46
[INFO 2017-06-30 06:59:22,741 main.py:52] epoch 6644, training loss: 5618.58, average training loss: 5597.07, base loss: 15959.89
[INFO 2017-06-30 06:59:25,868 main.py:52] epoch 6645, training loss: 5785.59, average training loss: 5597.30, base loss: 15960.36
[INFO 2017-06-30 06:59:29,061 main.py:52] epoch 6646, training loss: 5929.47, average training loss: 5597.31, base loss: 15960.88
[INFO 2017-06-30 06:59:32,196 main.py:52] epoch 6647, training loss: 5274.32, average training loss: 5596.83, base loss: 15960.59
[INFO 2017-06-30 06:59:35,344 main.py:52] epoch 6648, training loss: 5559.83, average training loss: 5596.70, base loss: 15960.69
[INFO 2017-06-30 06:59:38,457 main.py:52] epoch 6649, training loss: 5715.29, average training loss: 5597.00, base loss: 15960.89
[INFO 2017-06-30 06:59:41,606 main.py:52] epoch 6650, training loss: 6113.80, average training loss: 5597.25, base loss: 15961.79
[INFO 2017-06-30 06:59:44,732 main.py:52] epoch 6651, training loss: 5577.64, average training loss: 5597.18, base loss: 15961.56
[INFO 2017-06-30 06:59:47,957 main.py:52] epoch 6652, training loss: 5645.07, average training loss: 5597.36, base loss: 15962.31
[INFO 2017-06-30 06:59:51,129 main.py:52] epoch 6653, training loss: 5601.91, average training loss: 5597.58, base loss: 15962.78
[INFO 2017-06-30 06:59:54,296 main.py:52] epoch 6654, training loss: 5423.79, average training loss: 5597.30, base loss: 15963.00
[INFO 2017-06-30 06:59:57,465 main.py:52] epoch 6655, training loss: 5542.94, average training loss: 5596.95, base loss: 15962.81
[INFO 2017-06-30 07:00:00,613 main.py:52] epoch 6656, training loss: 5689.56, average training loss: 5597.05, base loss: 15963.19
[INFO 2017-06-30 07:00:03,769 main.py:52] epoch 6657, training loss: 5817.59, average training loss: 5597.36, base loss: 15963.72
[INFO 2017-06-30 07:00:06,932 main.py:52] epoch 6658, training loss: 5788.22, average training loss: 5598.03, base loss: 15963.77
[INFO 2017-06-30 07:00:10,132 main.py:52] epoch 6659, training loss: 5574.86, average training loss: 5597.65, base loss: 15963.83
[INFO 2017-06-30 07:00:13,306 main.py:52] epoch 6660, training loss: 5318.51, average training loss: 5597.61, base loss: 15963.24
[INFO 2017-06-30 07:00:16,455 main.py:52] epoch 6661, training loss: 5952.73, average training loss: 5597.88, base loss: 15963.64
[INFO 2017-06-30 07:00:19,597 main.py:52] epoch 6662, training loss: 5491.89, average training loss: 5597.94, base loss: 15963.72
[INFO 2017-06-30 07:00:22,719 main.py:52] epoch 6663, training loss: 5499.30, average training loss: 5597.69, base loss: 15963.86
[INFO 2017-06-30 07:00:25,853 main.py:52] epoch 6664, training loss: 5650.91, average training loss: 5597.73, base loss: 15964.14
[INFO 2017-06-30 07:00:29,035 main.py:52] epoch 6665, training loss: 5357.06, average training loss: 5597.64, base loss: 15964.32
[INFO 2017-06-30 07:00:32,165 main.py:52] epoch 6666, training loss: 5442.18, average training loss: 5597.89, base loss: 15964.10
[INFO 2017-06-30 07:00:35,346 main.py:52] epoch 6667, training loss: 5011.86, average training loss: 5597.55, base loss: 15963.49
[INFO 2017-06-30 07:00:38,473 main.py:52] epoch 6668, training loss: 5667.27, average training loss: 5597.45, base loss: 15963.39
[INFO 2017-06-30 07:00:41,721 main.py:52] epoch 6669, training loss: 5832.25, average training loss: 5597.38, base loss: 15963.82
[INFO 2017-06-30 07:00:44,872 main.py:52] epoch 6670, training loss: 5297.91, average training loss: 5597.07, base loss: 15963.40
[INFO 2017-06-30 07:00:48,052 main.py:52] epoch 6671, training loss: 5744.78, average training loss: 5597.03, base loss: 15963.79
[INFO 2017-06-30 07:00:51,184 main.py:52] epoch 6672, training loss: 5679.76, average training loss: 5597.34, base loss: 15963.84
[INFO 2017-06-30 07:00:54,354 main.py:52] epoch 6673, training loss: 5304.90, average training loss: 5596.76, base loss: 15963.85
[INFO 2017-06-30 07:00:57,477 main.py:52] epoch 6674, training loss: 5108.15, average training loss: 5596.41, base loss: 15963.61
[INFO 2017-06-30 07:01:00,622 main.py:52] epoch 6675, training loss: 5496.05, average training loss: 5595.95, base loss: 15963.43
[INFO 2017-06-30 07:01:03,777 main.py:52] epoch 6676, training loss: 5893.27, average training loss: 5596.01, base loss: 15963.96
[INFO 2017-06-30 07:01:06,910 main.py:52] epoch 6677, training loss: 5353.27, average training loss: 5595.86, base loss: 15964.05
[INFO 2017-06-30 07:01:10,054 main.py:52] epoch 6678, training loss: 5872.85, average training loss: 5596.33, base loss: 15964.12
[INFO 2017-06-30 07:01:13,180 main.py:52] epoch 6679, training loss: 5979.69, average training loss: 5596.40, base loss: 15964.36
[INFO 2017-06-30 07:01:16,336 main.py:52] epoch 6680, training loss: 5390.57, average training loss: 5596.62, base loss: 15964.43
[INFO 2017-06-30 07:01:19,458 main.py:52] epoch 6681, training loss: 5257.98, average training loss: 5596.02, base loss: 15964.01
[INFO 2017-06-30 07:01:22,669 main.py:52] epoch 6682, training loss: 5410.84, average training loss: 5595.92, base loss: 15964.26
[INFO 2017-06-30 07:01:25,814 main.py:52] epoch 6683, training loss: 5467.19, average training loss: 5595.79, base loss: 15964.27
[INFO 2017-06-30 07:01:28,934 main.py:52] epoch 6684, training loss: 5122.61, average training loss: 5595.16, base loss: 15963.57
[INFO 2017-06-30 07:01:32,144 main.py:52] epoch 6685, training loss: 5745.83, average training loss: 5595.18, base loss: 15963.53
[INFO 2017-06-30 07:01:35,263 main.py:52] epoch 6686, training loss: 5301.36, average training loss: 5595.00, base loss: 15963.67
[INFO 2017-06-30 07:01:38,392 main.py:52] epoch 6687, training loss: 5561.67, average training loss: 5594.95, base loss: 15963.95
[INFO 2017-06-30 07:01:41,564 main.py:52] epoch 6688, training loss: 5811.62, average training loss: 5595.36, base loss: 15964.45
[INFO 2017-06-30 07:01:44,707 main.py:52] epoch 6689, training loss: 5525.44, average training loss: 5594.72, base loss: 15964.47
[INFO 2017-06-30 07:01:47,810 main.py:52] epoch 6690, training loss: 5354.77, average training loss: 5594.72, base loss: 15964.26
[INFO 2017-06-30 07:01:50,981 main.py:52] epoch 6691, training loss: 5791.94, average training loss: 5594.98, base loss: 15964.49
[INFO 2017-06-30 07:01:54,090 main.py:52] epoch 6692, training loss: 5422.32, average training loss: 5594.72, base loss: 15964.29
[INFO 2017-06-30 07:01:57,225 main.py:52] epoch 6693, training loss: 5437.77, average training loss: 5594.78, base loss: 15964.32
[INFO 2017-06-30 07:02:00,367 main.py:52] epoch 6694, training loss: 5390.41, average training loss: 5594.72, base loss: 15964.29
[INFO 2017-06-30 07:02:03,551 main.py:52] epoch 6695, training loss: 5904.06, average training loss: 5594.66, base loss: 15965.00
[INFO 2017-06-30 07:02:06,656 main.py:52] epoch 6696, training loss: 5290.51, average training loss: 5593.94, base loss: 15964.95
[INFO 2017-06-30 07:02:09,825 main.py:52] epoch 6697, training loss: 5261.52, average training loss: 5593.71, base loss: 15964.43
[INFO 2017-06-30 07:02:12,960 main.py:52] epoch 6698, training loss: 5995.39, average training loss: 5594.19, base loss: 15964.83
[INFO 2017-06-30 07:02:16,134 main.py:52] epoch 6699, training loss: 6113.03, average training loss: 5595.19, base loss: 15965.47
[INFO 2017-06-30 07:02:16,134 main.py:54] epoch 6699, testing
[INFO 2017-06-30 07:02:29,005 main.py:97] average testing loss: 5438.79, base loss: 15257.40
[INFO 2017-06-30 07:02:29,006 main.py:98] improve_loss: 9818.61, improve_percent: 0.64
[INFO 2017-06-30 07:02:29,007 main.py:66] current best improved percent: 0.66
[INFO 2017-06-30 07:02:32,095 main.py:52] epoch 6700, training loss: 5538.09, average training loss: 5595.33, base loss: 15965.65
[INFO 2017-06-30 07:02:35,220 main.py:52] epoch 6701, training loss: 5859.18, average training loss: 5595.38, base loss: 15965.92
[INFO 2017-06-30 07:02:38,360 main.py:52] epoch 6702, training loss: 5739.44, average training loss: 5595.26, base loss: 15966.37
[INFO 2017-06-30 07:02:41,539 main.py:52] epoch 6703, training loss: 5255.13, average training loss: 5594.90, base loss: 15966.10
[INFO 2017-06-30 07:02:44,660 main.py:52] epoch 6704, training loss: 5593.49, average training loss: 5594.57, base loss: 15966.36
[INFO 2017-06-30 07:02:47,867 main.py:52] epoch 6705, training loss: 5870.68, average training loss: 5594.70, base loss: 15966.88
[INFO 2017-06-30 07:02:50,976 main.py:52] epoch 6706, training loss: 5811.95, average training loss: 5594.60, base loss: 15967.33
[INFO 2017-06-30 07:02:54,094 main.py:52] epoch 6707, training loss: 5789.62, average training loss: 5594.49, base loss: 15967.56
[INFO 2017-06-30 07:02:57,273 main.py:52] epoch 6708, training loss: 5873.65, average training loss: 5594.24, base loss: 15967.55
[INFO 2017-06-30 07:03:00,382 main.py:52] epoch 6709, training loss: 5457.77, average training loss: 5593.89, base loss: 15967.04
[INFO 2017-06-30 07:03:03,509 main.py:52] epoch 6710, training loss: 5503.69, average training loss: 5593.80, base loss: 15966.87
[INFO 2017-06-30 07:03:06,669 main.py:52] epoch 6711, training loss: 5374.96, average training loss: 5593.46, base loss: 15967.00
[INFO 2017-06-30 07:03:09,833 main.py:52] epoch 6712, training loss: 5511.45, average training loss: 5593.78, base loss: 15967.02
[INFO 2017-06-30 07:03:13,011 main.py:52] epoch 6713, training loss: 5594.49, average training loss: 5594.19, base loss: 15967.11
[INFO 2017-06-30 07:03:16,202 main.py:52] epoch 6714, training loss: 5242.75, average training loss: 5593.84, base loss: 15966.82
[INFO 2017-06-30 07:03:19,305 main.py:52] epoch 6715, training loss: 5390.53, average training loss: 5593.52, base loss: 15966.68
[INFO 2017-06-30 07:03:22,457 main.py:52] epoch 6716, training loss: 5146.86, average training loss: 5592.91, base loss: 15966.14
[INFO 2017-06-30 07:03:25,599 main.py:52] epoch 6717, training loss: 5408.67, average training loss: 5592.58, base loss: 15965.63
[INFO 2017-06-30 07:03:28,731 main.py:52] epoch 6718, training loss: 5429.40, average training loss: 5592.03, base loss: 15965.50
[INFO 2017-06-30 07:03:31,870 main.py:52] epoch 6719, training loss: 5166.43, average training loss: 5591.48, base loss: 15965.07
[INFO 2017-06-30 07:03:35,046 main.py:52] epoch 6720, training loss: 5635.65, average training loss: 5591.52, base loss: 15965.40
[INFO 2017-06-30 07:03:38,171 main.py:52] epoch 6721, training loss: 6021.90, average training loss: 5591.96, base loss: 15966.02
[INFO 2017-06-30 07:03:41,330 main.py:52] epoch 6722, training loss: 5997.00, average training loss: 5592.16, base loss: 15966.36
[INFO 2017-06-30 07:03:44,487 main.py:52] epoch 6723, training loss: 5649.71, average training loss: 5591.96, base loss: 15966.77
[INFO 2017-06-30 07:03:47,673 main.py:52] epoch 6724, training loss: 5331.52, average training loss: 5591.76, base loss: 15966.76
[INFO 2017-06-30 07:03:50,828 main.py:52] epoch 6725, training loss: 5105.23, average training loss: 5591.38, base loss: 15966.25
[INFO 2017-06-30 07:03:53,963 main.py:52] epoch 6726, training loss: 6215.43, average training loss: 5591.59, base loss: 15967.19
[INFO 2017-06-30 07:03:57,107 main.py:52] epoch 6727, training loss: 5084.00, average training loss: 5590.89, base loss: 15967.12
[INFO 2017-06-30 07:04:00,257 main.py:52] epoch 6728, training loss: 5494.18, average training loss: 5590.26, base loss: 15967.22
[INFO 2017-06-30 07:04:03,396 main.py:52] epoch 6729, training loss: 5635.07, average training loss: 5590.54, base loss: 15967.13
[INFO 2017-06-30 07:04:06,526 main.py:52] epoch 6730, training loss: 5512.05, average training loss: 5590.81, base loss: 15967.29
[INFO 2017-06-30 07:04:09,691 main.py:52] epoch 6731, training loss: 5422.83, average training loss: 5590.78, base loss: 15966.88
[INFO 2017-06-30 07:04:12,845 main.py:52] epoch 6732, training loss: 5656.75, average training loss: 5590.65, base loss: 15966.80
[INFO 2017-06-30 07:04:16,007 main.py:52] epoch 6733, training loss: 5865.53, average training loss: 5590.90, base loss: 15967.21
[INFO 2017-06-30 07:04:19,157 main.py:52] epoch 6734, training loss: 5746.61, average training loss: 5591.11, base loss: 15966.79
[INFO 2017-06-30 07:04:22,294 main.py:52] epoch 6735, training loss: 5760.47, average training loss: 5591.62, base loss: 15966.74
[INFO 2017-06-30 07:04:25,456 main.py:52] epoch 6736, training loss: 5610.66, average training loss: 5591.36, base loss: 15966.77
[INFO 2017-06-30 07:04:28,612 main.py:52] epoch 6737, training loss: 5233.18, average training loss: 5590.83, base loss: 15966.69
[INFO 2017-06-30 07:04:31,761 main.py:52] epoch 6738, training loss: 5665.05, average training loss: 5590.55, base loss: 15966.81
[INFO 2017-06-30 07:04:34,896 main.py:52] epoch 6739, training loss: 5539.66, average training loss: 5590.73, base loss: 15967.05
[INFO 2017-06-30 07:04:38,052 main.py:52] epoch 6740, training loss: 5590.07, average training loss: 5590.61, base loss: 15967.26
[INFO 2017-06-30 07:04:41,188 main.py:52] epoch 6741, training loss: 5543.39, average training loss: 5590.38, base loss: 15967.50
[INFO 2017-06-30 07:04:44,311 main.py:52] epoch 6742, training loss: 5499.86, average training loss: 5590.19, base loss: 15967.15
[INFO 2017-06-30 07:04:47,514 main.py:52] epoch 6743, training loss: 5561.79, average training loss: 5590.17, base loss: 15967.34
[INFO 2017-06-30 07:04:50,666 main.py:52] epoch 6744, training loss: 5507.08, average training loss: 5590.31, base loss: 15967.42
[INFO 2017-06-30 07:04:53,813 main.py:52] epoch 6745, training loss: 5176.56, average training loss: 5589.91, base loss: 15967.08
[INFO 2017-06-30 07:04:56,948 main.py:52] epoch 6746, training loss: 5996.51, average training loss: 5590.28, base loss: 15967.56
[INFO 2017-06-30 07:05:00,102 main.py:52] epoch 6747, training loss: 5590.85, average training loss: 5590.03, base loss: 15967.15
[INFO 2017-06-30 07:05:03,256 main.py:52] epoch 6748, training loss: 5215.71, average training loss: 5589.95, base loss: 15967.43
[INFO 2017-06-30 07:05:06,398 main.py:52] epoch 6749, training loss: 5940.10, average training loss: 5590.36, base loss: 15968.10
[INFO 2017-06-30 07:05:09,585 main.py:52] epoch 6750, training loss: 5257.79, average training loss: 5590.08, base loss: 15967.72
[INFO 2017-06-30 07:05:12,756 main.py:52] epoch 6751, training loss: 5418.54, average training loss: 5589.91, base loss: 15967.66
[INFO 2017-06-30 07:05:15,935 main.py:52] epoch 6752, training loss: 6178.70, average training loss: 5590.33, base loss: 15967.91
[INFO 2017-06-30 07:05:19,088 main.py:52] epoch 6753, training loss: 5616.47, average training loss: 5590.08, base loss: 15967.70
[INFO 2017-06-30 07:05:22,217 main.py:52] epoch 6754, training loss: 5573.27, average training loss: 5590.31, base loss: 15967.61
[INFO 2017-06-30 07:05:25,340 main.py:52] epoch 6755, training loss: 5577.70, average training loss: 5590.09, base loss: 15967.90
[INFO 2017-06-30 07:05:28,494 main.py:52] epoch 6756, training loss: 5512.40, average training loss: 5590.12, base loss: 15967.85
[INFO 2017-06-30 07:05:31,605 main.py:52] epoch 6757, training loss: 5839.46, average training loss: 5590.59, base loss: 15968.16
[INFO 2017-06-30 07:05:34,749 main.py:52] epoch 6758, training loss: 5525.41, average training loss: 5590.70, base loss: 15968.28
[INFO 2017-06-30 07:05:37,903 main.py:52] epoch 6759, training loss: 5533.61, average training loss: 5590.27, base loss: 15968.15
[INFO 2017-06-30 07:05:41,052 main.py:52] epoch 6760, training loss: 5753.69, average training loss: 5590.38, base loss: 15967.47
[INFO 2017-06-30 07:05:44,199 main.py:52] epoch 6761, training loss: 5711.34, average training loss: 5590.78, base loss: 15967.63
[INFO 2017-06-30 07:05:47,334 main.py:52] epoch 6762, training loss: 5566.76, average training loss: 5590.82, base loss: 15967.69
[INFO 2017-06-30 07:05:50,489 main.py:52] epoch 6763, training loss: 5682.41, average training loss: 5591.49, base loss: 15967.54
[INFO 2017-06-30 07:05:53,640 main.py:52] epoch 6764, training loss: 5361.06, average training loss: 5591.43, base loss: 15966.94
[INFO 2017-06-30 07:05:56,771 main.py:52] epoch 6765, training loss: 5513.77, average training loss: 5591.32, base loss: 15966.67
[INFO 2017-06-30 07:05:59,905 main.py:52] epoch 6766, training loss: 5122.75, average training loss: 5591.46, base loss: 15966.19
[INFO 2017-06-30 07:06:03,070 main.py:52] epoch 6767, training loss: 5652.94, average training loss: 5591.06, base loss: 15966.59
[INFO 2017-06-30 07:06:06,209 main.py:52] epoch 6768, training loss: 5758.42, average training loss: 5591.15, base loss: 15966.77
[INFO 2017-06-30 07:06:09,354 main.py:52] epoch 6769, training loss: 5282.29, average training loss: 5590.61, base loss: 15966.73
[INFO 2017-06-30 07:06:12,478 main.py:52] epoch 6770, training loss: 6106.30, average training loss: 5591.01, base loss: 15967.49
[INFO 2017-06-30 07:06:15,598 main.py:52] epoch 6771, training loss: 5477.90, average training loss: 5591.12, base loss: 15967.27
[INFO 2017-06-30 07:06:18,751 main.py:52] epoch 6772, training loss: 5223.38, average training loss: 5590.95, base loss: 15966.46
[INFO 2017-06-30 07:06:21,891 main.py:52] epoch 6773, training loss: 5687.72, average training loss: 5591.09, base loss: 15966.26
[INFO 2017-06-30 07:06:25,010 main.py:52] epoch 6774, training loss: 5548.16, average training loss: 5591.31, base loss: 15965.91
[INFO 2017-06-30 07:06:28,157 main.py:52] epoch 6775, training loss: 5627.62, average training loss: 5591.16, base loss: 15966.35
[INFO 2017-06-30 07:06:31,338 main.py:52] epoch 6776, training loss: 5547.56, average training loss: 5590.50, base loss: 15966.33
[INFO 2017-06-30 07:06:34,563 main.py:52] epoch 6777, training loss: 5165.80, average training loss: 5589.76, base loss: 15965.76
[INFO 2017-06-30 07:06:37,709 main.py:52] epoch 6778, training loss: 5637.40, average training loss: 5589.53, base loss: 15965.71
[INFO 2017-06-30 07:06:40,834 main.py:52] epoch 6779, training loss: 5362.92, average training loss: 5589.51, base loss: 15965.82
[INFO 2017-06-30 07:06:44,020 main.py:52] epoch 6780, training loss: 5373.99, average training loss: 5589.48, base loss: 15965.53
[INFO 2017-06-30 07:06:47,158 main.py:52] epoch 6781, training loss: 5489.78, average training loss: 5589.50, base loss: 15965.69
[INFO 2017-06-30 07:06:50,344 main.py:52] epoch 6782, training loss: 5385.10, average training loss: 5589.49, base loss: 15965.22
[INFO 2017-06-30 07:06:53,500 main.py:52] epoch 6783, training loss: 5589.30, average training loss: 5589.45, base loss: 15964.93
[INFO 2017-06-30 07:06:56,650 main.py:52] epoch 6784, training loss: 5153.12, average training loss: 5588.70, base loss: 15964.71
[INFO 2017-06-30 07:06:59,778 main.py:52] epoch 6785, training loss: 5541.48, average training loss: 5588.53, base loss: 15965.00
[INFO 2017-06-30 07:07:02,920 main.py:52] epoch 6786, training loss: 5394.33, average training loss: 5588.69, base loss: 15965.36
[INFO 2017-06-30 07:07:06,099 main.py:52] epoch 6787, training loss: 5387.52, average training loss: 5588.49, base loss: 15965.68
[INFO 2017-06-30 07:07:09,228 main.py:52] epoch 6788, training loss: 5517.88, average training loss: 5588.17, base loss: 15965.67
[INFO 2017-06-30 07:07:12,359 main.py:52] epoch 6789, training loss: 5521.67, average training loss: 5587.79, base loss: 15965.43
[INFO 2017-06-30 07:07:15,510 main.py:52] epoch 6790, training loss: 5479.42, average training loss: 5588.02, base loss: 15965.24
[INFO 2017-06-30 07:07:18,632 main.py:52] epoch 6791, training loss: 5419.64, average training loss: 5588.08, base loss: 15965.14
[INFO 2017-06-30 07:07:21,804 main.py:52] epoch 6792, training loss: 5452.23, average training loss: 5587.76, base loss: 15965.16
[INFO 2017-06-30 07:07:24,967 main.py:52] epoch 6793, training loss: 5671.97, average training loss: 5587.94, base loss: 15965.25
[INFO 2017-06-30 07:07:28,078 main.py:52] epoch 6794, training loss: 5123.27, average training loss: 5587.15, base loss: 15964.65
[INFO 2017-06-30 07:07:31,278 main.py:52] epoch 6795, training loss: 5383.39, average training loss: 5586.45, base loss: 15964.37
[INFO 2017-06-30 07:07:34,410 main.py:52] epoch 6796, training loss: 4982.54, average training loss: 5585.94, base loss: 15963.73
[INFO 2017-06-30 07:07:37,541 main.py:52] epoch 6797, training loss: 5768.13, average training loss: 5585.60, base loss: 15963.93
[INFO 2017-06-30 07:07:40,660 main.py:52] epoch 6798, training loss: 5391.50, average training loss: 5585.35, base loss: 15963.70
[INFO 2017-06-30 07:07:43,786 main.py:52] epoch 6799, training loss: 5668.37, average training loss: 5585.63, base loss: 15964.11
[INFO 2017-06-30 07:07:43,787 main.py:54] epoch 6799, testing
[INFO 2017-06-30 07:07:56,919 main.py:97] average testing loss: 5471.91, base loss: 15732.21
[INFO 2017-06-30 07:07:56,919 main.py:98] improve_loss: 10260.31, improve_percent: 0.65
[INFO 2017-06-30 07:07:56,920 main.py:66] current best improved percent: 0.66
[INFO 2017-06-30 07:08:00,065 main.py:52] epoch 6800, training loss: 5576.55, average training loss: 5585.45, base loss: 15964.36
[INFO 2017-06-30 07:08:03,200 main.py:52] epoch 6801, training loss: 5035.69, average training loss: 5585.15, base loss: 15964.05
[INFO 2017-06-30 07:08:06,342 main.py:52] epoch 6802, training loss: 5834.57, average training loss: 5585.22, base loss: 15964.12
[INFO 2017-06-30 07:08:09,456 main.py:52] epoch 6803, training loss: 5477.73, average training loss: 5584.99, base loss: 15963.97
[INFO 2017-06-30 07:08:12,589 main.py:52] epoch 6804, training loss: 5405.30, average training loss: 5584.92, base loss: 15963.97
[INFO 2017-06-30 07:08:15,674 main.py:52] epoch 6805, training loss: 5595.86, average training loss: 5585.21, base loss: 15963.83
[INFO 2017-06-30 07:08:18,840 main.py:52] epoch 6806, training loss: 5285.25, average training loss: 5585.03, base loss: 15963.27
[INFO 2017-06-30 07:08:21,973 main.py:52] epoch 6807, training loss: 5447.46, average training loss: 5584.98, base loss: 15962.86
[INFO 2017-06-30 07:08:25,091 main.py:52] epoch 6808, training loss: 6030.48, average training loss: 5585.40, base loss: 15963.39
[INFO 2017-06-30 07:08:28,272 main.py:52] epoch 6809, training loss: 5531.81, average training loss: 5585.34, base loss: 15963.12
[INFO 2017-06-30 07:08:31,463 main.py:52] epoch 6810, training loss: 5439.74, average training loss: 5585.42, base loss: 15962.70
[INFO 2017-06-30 07:08:34,606 main.py:52] epoch 6811, training loss: 5551.45, average training loss: 5585.32, base loss: 15962.65
[INFO 2017-06-30 07:08:37,713 main.py:52] epoch 6812, training loss: 5296.36, average training loss: 5584.61, base loss: 15962.58
[INFO 2017-06-30 07:08:40,853 main.py:52] epoch 6813, training loss: 5344.88, average training loss: 5584.45, base loss: 15962.65
[INFO 2017-06-30 07:08:43,988 main.py:52] epoch 6814, training loss: 5837.56, average training loss: 5584.68, base loss: 15962.69
[INFO 2017-06-30 07:08:47,122 main.py:52] epoch 6815, training loss: 5261.52, average training loss: 5584.48, base loss: 15962.38
[INFO 2017-06-30 07:08:50,257 main.py:52] epoch 6816, training loss: 5599.99, average training loss: 5584.46, base loss: 15962.51
[INFO 2017-06-30 07:08:53,401 main.py:52] epoch 6817, training loss: 5293.97, average training loss: 5584.59, base loss: 15961.90
[INFO 2017-06-30 07:08:56,567 main.py:52] epoch 6818, training loss: 5551.37, average training loss: 5584.75, base loss: 15962.11
[INFO 2017-06-30 07:08:59,720 main.py:52] epoch 6819, training loss: 5282.50, average training loss: 5584.37, base loss: 15962.03
[INFO 2017-06-30 07:09:02,891 main.py:52] epoch 6820, training loss: 5407.07, average training loss: 5584.00, base loss: 15961.90
[INFO 2017-06-30 07:09:06,003 main.py:52] epoch 6821, training loss: 5487.01, average training loss: 5583.92, base loss: 15961.85
[INFO 2017-06-30 07:09:09,148 main.py:52] epoch 6822, training loss: 5763.91, average training loss: 5583.80, base loss: 15962.29
[INFO 2017-06-30 07:09:12,304 main.py:52] epoch 6823, training loss: 5481.23, average training loss: 5583.65, base loss: 15962.18
[INFO 2017-06-30 07:09:15,440 main.py:52] epoch 6824, training loss: 5811.49, average training loss: 5583.82, base loss: 15962.10
[INFO 2017-06-30 07:09:18,645 main.py:52] epoch 6825, training loss: 5905.70, average training loss: 5583.61, base loss: 15962.59
[INFO 2017-06-30 07:09:21,774 main.py:52] epoch 6826, training loss: 5165.15, average training loss: 5583.06, base loss: 15962.24
[INFO 2017-06-30 07:09:24,875 main.py:52] epoch 6827, training loss: 5474.02, average training loss: 5582.63, base loss: 15961.94
[INFO 2017-06-30 07:09:28,008 main.py:52] epoch 6828, training loss: 5850.57, average training loss: 5582.57, base loss: 15962.41
[INFO 2017-06-30 07:09:31,137 main.py:52] epoch 6829, training loss: 5579.31, average training loss: 5582.23, base loss: 15962.41
[INFO 2017-06-30 07:09:34,301 main.py:52] epoch 6830, training loss: 5164.94, average training loss: 5581.26, base loss: 15961.98
[INFO 2017-06-30 07:09:37,424 main.py:52] epoch 6831, training loss: 5621.29, average training loss: 5581.23, base loss: 15962.23
[INFO 2017-06-30 07:09:40,570 main.py:52] epoch 6832, training loss: 5487.30, average training loss: 5581.05, base loss: 15962.28
[INFO 2017-06-30 07:09:43,728 main.py:52] epoch 6833, training loss: 5070.78, average training loss: 5580.40, base loss: 15962.10
[INFO 2017-06-30 07:09:46,911 main.py:52] epoch 6834, training loss: 5691.52, average training loss: 5580.63, base loss: 15962.25
[INFO 2017-06-30 07:09:50,079 main.py:52] epoch 6835, training loss: 5561.25, average training loss: 5580.89, base loss: 15962.24
[INFO 2017-06-30 07:09:53,279 main.py:52] epoch 6836, training loss: 5517.09, average training loss: 5580.83, base loss: 15962.85
[INFO 2017-06-30 07:09:56,412 main.py:52] epoch 6837, training loss: 5637.25, average training loss: 5581.53, base loss: 15963.05
[INFO 2017-06-30 07:09:59,599 main.py:52] epoch 6838, training loss: 5394.34, average training loss: 5581.49, base loss: 15962.92
[INFO 2017-06-30 07:10:02,718 main.py:52] epoch 6839, training loss: 5313.26, average training loss: 5581.09, base loss: 15962.60
[INFO 2017-06-30 07:10:05,856 main.py:52] epoch 6840, training loss: 5241.05, average training loss: 5580.52, base loss: 15962.18
[INFO 2017-06-30 07:10:09,048 main.py:52] epoch 6841, training loss: 5621.51, average training loss: 5580.41, base loss: 15962.46
[INFO 2017-06-30 07:10:12,152 main.py:52] epoch 6842, training loss: 5456.42, average training loss: 5580.33, base loss: 15962.75
[INFO 2017-06-30 07:10:15,272 main.py:52] epoch 6843, training loss: 5244.45, average training loss: 5580.17, base loss: 15962.55
[INFO 2017-06-30 07:10:18,425 main.py:52] epoch 6844, training loss: 5303.34, average training loss: 5579.61, base loss: 15961.84
[INFO 2017-06-30 07:10:21,567 main.py:52] epoch 6845, training loss: 6074.68, average training loss: 5580.16, base loss: 15962.45
[INFO 2017-06-30 07:10:24,693 main.py:52] epoch 6846, training loss: 5798.82, average training loss: 5579.91, base loss: 15962.66
[INFO 2017-06-30 07:10:27,802 main.py:52] epoch 6847, training loss: 5605.72, average training loss: 5580.00, base loss: 15962.87
[INFO 2017-06-30 07:10:30,985 main.py:52] epoch 6848, training loss: 5981.59, average training loss: 5580.53, base loss: 15963.29
[INFO 2017-06-30 07:10:34,163 main.py:52] epoch 6849, training loss: 5266.87, average training loss: 5580.00, base loss: 15962.84
[INFO 2017-06-30 07:10:37,331 main.py:52] epoch 6850, training loss: 5236.48, average training loss: 5579.03, base loss: 15962.86
[INFO 2017-06-30 07:10:40,518 main.py:52] epoch 6851, training loss: 5537.85, average training loss: 5578.95, base loss: 15962.72
[INFO 2017-06-30 07:10:43,633 main.py:52] epoch 6852, training loss: 5741.15, average training loss: 5578.94, base loss: 15962.65
[INFO 2017-06-30 07:10:46,785 main.py:52] epoch 6853, training loss: 5701.05, average training loss: 5578.91, base loss: 15962.95
[INFO 2017-06-30 07:10:49,900 main.py:52] epoch 6854, training loss: 5472.38, average training loss: 5578.53, base loss: 15963.36
[INFO 2017-06-30 07:10:53,055 main.py:52] epoch 6855, training loss: 5565.87, average training loss: 5578.37, base loss: 15963.61
[INFO 2017-06-30 07:10:56,185 main.py:52] epoch 6856, training loss: 5714.52, average training loss: 5578.81, base loss: 15964.12
[INFO 2017-06-30 07:10:59,340 main.py:52] epoch 6857, training loss: 5346.81, average training loss: 5578.89, base loss: 15964.28
[INFO 2017-06-30 07:11:02,500 main.py:52] epoch 6858, training loss: 5668.54, average training loss: 5578.86, base loss: 15964.54
[INFO 2017-06-30 07:11:05,672 main.py:52] epoch 6859, training loss: 5613.58, average training loss: 5579.00, base loss: 15964.48
[INFO 2017-06-30 07:11:08,825 main.py:52] epoch 6860, training loss: 5437.24, average training loss: 5578.82, base loss: 15964.17
[INFO 2017-06-30 07:11:11,979 main.py:52] epoch 6861, training loss: 5497.81, average training loss: 5578.64, base loss: 15964.14
[INFO 2017-06-30 07:11:15,157 main.py:52] epoch 6862, training loss: 5786.72, average training loss: 5578.62, base loss: 15964.63
[INFO 2017-06-30 07:11:18,303 main.py:52] epoch 6863, training loss: 5735.67, average training loss: 5578.74, base loss: 15964.45
[INFO 2017-06-30 07:11:21,464 main.py:52] epoch 6864, training loss: 5802.05, average training loss: 5579.24, base loss: 15964.62
[INFO 2017-06-30 07:11:24,648 main.py:52] epoch 6865, training loss: 5505.12, average training loss: 5578.76, base loss: 15964.62
[INFO 2017-06-30 07:11:27,773 main.py:52] epoch 6866, training loss: 5377.36, average training loss: 5578.38, base loss: 15964.31
[INFO 2017-06-30 07:11:30,934 main.py:52] epoch 6867, training loss: 5897.06, average training loss: 5578.93, base loss: 15964.87
[INFO 2017-06-30 07:11:34,090 main.py:52] epoch 6868, training loss: 5387.09, average training loss: 5578.83, base loss: 15964.55
[INFO 2017-06-30 07:11:37,208 main.py:52] epoch 6869, training loss: 5694.89, average training loss: 5578.53, base loss: 15965.07
[INFO 2017-06-30 07:11:40,336 main.py:52] epoch 6870, training loss: 5893.61, average training loss: 5578.75, base loss: 15965.43
[INFO 2017-06-30 07:11:43,471 main.py:52] epoch 6871, training loss: 5556.27, average training loss: 5578.84, base loss: 15965.57
[INFO 2017-06-30 07:11:46,619 main.py:52] epoch 6872, training loss: 5680.72, average training loss: 5578.42, base loss: 15966.02
[INFO 2017-06-30 07:11:49,747 main.py:52] epoch 6873, training loss: 5514.91, average training loss: 5578.45, base loss: 15965.70
[INFO 2017-06-30 07:11:52,877 main.py:52] epoch 6874, training loss: 5089.65, average training loss: 5577.96, base loss: 15965.32
[INFO 2017-06-30 07:11:55,967 main.py:52] epoch 6875, training loss: 5522.67, average training loss: 5578.10, base loss: 15965.16
[INFO 2017-06-30 07:11:59,108 main.py:52] epoch 6876, training loss: 5835.25, average training loss: 5577.91, base loss: 15965.60
[INFO 2017-06-30 07:12:02,275 main.py:52] epoch 6877, training loss: 5354.40, average training loss: 5577.61, base loss: 15965.48
[INFO 2017-06-30 07:12:05,408 main.py:52] epoch 6878, training loss: 5728.48, average training loss: 5577.99, base loss: 15965.22
[INFO 2017-06-30 07:12:08,551 main.py:52] epoch 6879, training loss: 5355.97, average training loss: 5578.01, base loss: 15964.60
[INFO 2017-06-30 07:12:11,698 main.py:52] epoch 6880, training loss: 5677.55, average training loss: 5577.63, base loss: 15964.96
[INFO 2017-06-30 07:12:14,894 main.py:52] epoch 6881, training loss: 5643.37, average training loss: 5577.58, base loss: 15965.46
[INFO 2017-06-30 07:12:18,039 main.py:52] epoch 6882, training loss: 5765.88, average training loss: 5577.73, base loss: 15965.40
[INFO 2017-06-30 07:12:21,170 main.py:52] epoch 6883, training loss: 5250.46, average training loss: 5577.47, base loss: 15965.17
[INFO 2017-06-30 07:12:24,332 main.py:52] epoch 6884, training loss: 5824.68, average training loss: 5577.53, base loss: 15965.29
[INFO 2017-06-30 07:12:27,433 main.py:52] epoch 6885, training loss: 5462.89, average training loss: 5577.11, base loss: 15965.23
[INFO 2017-06-30 07:12:30,611 main.py:52] epoch 6886, training loss: 5620.44, average training loss: 5577.20, base loss: 15964.93
[INFO 2017-06-30 07:12:33,793 main.py:52] epoch 6887, training loss: 5385.40, average training loss: 5576.94, base loss: 15964.65
[INFO 2017-06-30 07:12:36,956 main.py:52] epoch 6888, training loss: 5703.24, average training loss: 5577.18, base loss: 15964.63
[INFO 2017-06-30 07:12:40,090 main.py:52] epoch 6889, training loss: 5580.03, average training loss: 5577.05, base loss: 15964.86
[INFO 2017-06-30 07:12:43,312 main.py:52] epoch 6890, training loss: 5403.08, average training loss: 5576.96, base loss: 15964.81
[INFO 2017-06-30 07:12:46,434 main.py:52] epoch 6891, training loss: 5600.25, average training loss: 5576.87, base loss: 15965.27
[INFO 2017-06-30 07:12:49,575 main.py:52] epoch 6892, training loss: 5795.09, average training loss: 5576.93, base loss: 15965.49
[INFO 2017-06-30 07:12:52,668 main.py:52] epoch 6893, training loss: 5640.10, average training loss: 5577.38, base loss: 15965.41
[INFO 2017-06-30 07:12:55,827 main.py:52] epoch 6894, training loss: 5369.34, average training loss: 5577.25, base loss: 15965.41
[INFO 2017-06-30 07:12:58,997 main.py:52] epoch 6895, training loss: 5015.81, average training loss: 5576.65, base loss: 15964.99
[INFO 2017-06-30 07:13:02,162 main.py:52] epoch 6896, training loss: 5908.09, average training loss: 5576.95, base loss: 15965.12
[INFO 2017-06-30 07:13:05,309 main.py:52] epoch 6897, training loss: 5641.24, average training loss: 5576.99, base loss: 15964.63
[INFO 2017-06-30 07:13:08,429 main.py:52] epoch 6898, training loss: 5119.49, average training loss: 5576.39, base loss: 15964.55
[INFO 2017-06-30 07:13:11,579 main.py:52] epoch 6899, training loss: 5486.09, average training loss: 5575.93, base loss: 15964.29
[INFO 2017-06-30 07:13:11,579 main.py:54] epoch 6899, testing
[INFO 2017-06-30 07:13:24,627 main.py:97] average testing loss: 5603.37, base loss: 16156.54
[INFO 2017-06-30 07:13:24,628 main.py:98] improve_loss: 10553.17, improve_percent: 0.65
[INFO 2017-06-30 07:13:24,629 main.py:66] current best improved percent: 0.66
[INFO 2017-06-30 07:13:27,788 main.py:52] epoch 6900, training loss: 5126.22, average training loss: 5575.60, base loss: 15963.74
[INFO 2017-06-30 07:13:30,941 main.py:52] epoch 6901, training loss: 5897.25, average training loss: 5575.84, base loss: 15964.12
[INFO 2017-06-30 07:13:34,072 main.py:52] epoch 6902, training loss: 5650.21, average training loss: 5576.00, base loss: 15964.13
[INFO 2017-06-30 07:13:37,247 main.py:52] epoch 6903, training loss: 5420.87, average training loss: 5575.52, base loss: 15964.35
[INFO 2017-06-30 07:13:40,427 main.py:52] epoch 6904, training loss: 5511.73, average training loss: 5575.13, base loss: 15964.25
[INFO 2017-06-30 07:13:43,612 main.py:52] epoch 6905, training loss: 5214.19, average training loss: 5574.93, base loss: 15964.26
[INFO 2017-06-30 07:13:46,733 main.py:52] epoch 6906, training loss: 5823.33, average training loss: 5575.08, base loss: 15964.45
[INFO 2017-06-30 07:13:49,894 main.py:52] epoch 6907, training loss: 5276.23, average training loss: 5575.15, base loss: 15963.91
[INFO 2017-06-30 07:13:53,035 main.py:52] epoch 6908, training loss: 5599.10, average training loss: 5575.19, base loss: 15963.89
[INFO 2017-06-30 07:13:56,169 main.py:52] epoch 6909, training loss: 5399.51, average training loss: 5574.81, base loss: 15964.04
[INFO 2017-06-30 07:13:59,297 main.py:52] epoch 6910, training loss: 5708.75, average training loss: 5574.39, base loss: 15963.86
[INFO 2017-06-30 07:14:02,414 main.py:52] epoch 6911, training loss: 5603.63, average training loss: 5574.12, base loss: 15963.57
[INFO 2017-06-30 07:14:05,577 main.py:52] epoch 6912, training loss: 5357.10, average training loss: 5574.22, base loss: 15963.46
[INFO 2017-06-30 07:14:08,742 main.py:52] epoch 6913, training loss: 5346.54, average training loss: 5573.63, base loss: 15963.24
[INFO 2017-06-30 07:14:11,876 main.py:52] epoch 6914, training loss: 5554.92, average training loss: 5573.45, base loss: 15963.45
[INFO 2017-06-30 07:14:15,011 main.py:52] epoch 6915, training loss: 5326.10, average training loss: 5573.35, base loss: 15963.16
[INFO 2017-06-30 07:14:18,147 main.py:52] epoch 6916, training loss: 5559.84, average training loss: 5573.77, base loss: 15963.29
[INFO 2017-06-30 07:14:21,273 main.py:52] epoch 6917, training loss: 5319.30, average training loss: 5573.28, base loss: 15963.00
[INFO 2017-06-30 07:14:24,404 main.py:52] epoch 6918, training loss: 5553.18, average training loss: 5573.60, base loss: 15962.69
[INFO 2017-06-30 07:14:27,540 main.py:52] epoch 6919, training loss: 5687.65, average training loss: 5574.16, base loss: 15962.44
[INFO 2017-06-30 07:14:30,682 main.py:52] epoch 6920, training loss: 5827.30, average training loss: 5574.55, base loss: 15962.24
[INFO 2017-06-30 07:14:33,871 main.py:52] epoch 6921, training loss: 5303.64, average training loss: 5574.14, base loss: 15961.75
[INFO 2017-06-30 07:14:37,021 main.py:52] epoch 6922, training loss: 5643.41, average training loss: 5574.17, base loss: 15961.94
[INFO 2017-06-30 07:14:40,193 main.py:52] epoch 6923, training loss: 5712.08, average training loss: 5574.20, base loss: 15962.21
[INFO 2017-06-30 07:14:43,355 main.py:52] epoch 6924, training loss: 5283.03, average training loss: 5573.88, base loss: 15961.91
[INFO 2017-06-30 07:14:46,485 main.py:52] epoch 6925, training loss: 5808.10, average training loss: 5573.99, base loss: 15962.20
[INFO 2017-06-30 07:14:49,619 main.py:52] epoch 6926, training loss: 5607.08, average training loss: 5573.58, base loss: 15962.52
[INFO 2017-06-30 07:14:52,730 main.py:52] epoch 6927, training loss: 5071.18, average training loss: 5573.41, base loss: 15962.54
[INFO 2017-06-30 07:14:55,859 main.py:52] epoch 6928, training loss: 5447.79, average training loss: 5573.10, base loss: 15962.52
[INFO 2017-06-30 07:14:59,029 main.py:52] epoch 6929, training loss: 5708.11, average training loss: 5573.16, base loss: 15962.97
[INFO 2017-06-30 07:15:02,151 main.py:52] epoch 6930, training loss: 5217.80, average training loss: 5573.08, base loss: 15962.51
[INFO 2017-06-30 07:15:05,316 main.py:52] epoch 6931, training loss: 5517.65, average training loss: 5572.98, base loss: 15962.23
[INFO 2017-06-30 07:15:08,493 main.py:52] epoch 6932, training loss: 5444.84, average training loss: 5572.60, base loss: 15962.15
[INFO 2017-06-30 07:15:11,629 main.py:52] epoch 6933, training loss: 5465.94, average training loss: 5572.21, base loss: 15962.19
[INFO 2017-06-30 07:15:14,790 main.py:52] epoch 6934, training loss: 5228.62, average training loss: 5572.13, base loss: 15962.00
[INFO 2017-06-30 07:15:17,989 main.py:52] epoch 6935, training loss: 5593.40, average training loss: 5572.65, base loss: 15962.13
[INFO 2017-06-30 07:15:21,124 main.py:52] epoch 6936, training loss: 5676.05, average training loss: 5572.69, base loss: 15962.38
[INFO 2017-06-30 07:15:24,287 main.py:52] epoch 6937, training loss: 5362.12, average training loss: 5572.42, base loss: 15962.16
[INFO 2017-06-30 07:15:27,446 main.py:52] epoch 6938, training loss: 5360.05, average training loss: 5572.29, base loss: 15961.87
[INFO 2017-06-30 07:15:30,568 main.py:52] epoch 6939, training loss: 5610.25, average training loss: 5572.23, base loss: 15961.69
[INFO 2017-06-30 07:15:33,713 main.py:52] epoch 6940, training loss: 5538.11, average training loss: 5572.67, base loss: 15962.06
[INFO 2017-06-30 07:15:36,844 main.py:52] epoch 6941, training loss: 5235.93, average training loss: 5572.08, base loss: 15961.62
[INFO 2017-06-30 07:15:40,006 main.py:52] epoch 6942, training loss: 5516.61, average training loss: 5572.03, base loss: 15961.70
[INFO 2017-06-30 07:15:43,169 main.py:52] epoch 6943, training loss: 5610.08, average training loss: 5571.80, base loss: 15961.81
[INFO 2017-06-30 07:15:46,357 main.py:52] epoch 6944, training loss: 5714.41, average training loss: 5572.24, base loss: 15962.02
[INFO 2017-06-30 07:15:49,484 main.py:52] epoch 6945, training loss: 5517.75, average training loss: 5571.94, base loss: 15962.02
[INFO 2017-06-30 07:15:52,643 main.py:52] epoch 6946, training loss: 5384.15, average training loss: 5571.99, base loss: 15962.15
[INFO 2017-06-30 07:15:55,797 main.py:52] epoch 6947, training loss: 5751.65, average training loss: 5572.02, base loss: 15962.63
[INFO 2017-06-30 07:15:58,940 main.py:52] epoch 6948, training loss: 5802.18, average training loss: 5572.34, base loss: 15962.82
[INFO 2017-06-30 07:16:02,117 main.py:52] epoch 6949, training loss: 5265.50, average training loss: 5572.15, base loss: 15962.22
[INFO 2017-06-30 07:16:05,239 main.py:52] epoch 6950, training loss: 5478.40, average training loss: 5572.65, base loss: 15962.25
[INFO 2017-06-30 07:16:08,382 main.py:52] epoch 6951, training loss: 5188.30, average training loss: 5572.39, base loss: 15961.96
[INFO 2017-06-30 07:16:11,499 main.py:52] epoch 6952, training loss: 5524.36, average training loss: 5572.43, base loss: 15962.06
[INFO 2017-06-30 07:16:14,647 main.py:52] epoch 6953, training loss: 5667.41, average training loss: 5571.89, base loss: 15962.23
[INFO 2017-06-30 07:16:17,770 main.py:52] epoch 6954, training loss: 5513.87, average training loss: 5571.81, base loss: 15962.69
[INFO 2017-06-30 07:16:20,951 main.py:52] epoch 6955, training loss: 5347.54, average training loss: 5571.81, base loss: 15962.49
[INFO 2017-06-30 07:16:24,117 main.py:52] epoch 6956, training loss: 5332.00, average training loss: 5571.19, base loss: 15962.43
[INFO 2017-06-30 07:16:27,279 main.py:52] epoch 6957, training loss: 5736.68, average training loss: 5571.29, base loss: 15962.70
[INFO 2017-06-30 07:16:30,437 main.py:52] epoch 6958, training loss: 5414.54, average training loss: 5571.08, base loss: 15962.50
[INFO 2017-06-30 07:16:33,568 main.py:52] epoch 6959, training loss: 5799.06, average training loss: 5571.11, base loss: 15962.68
[INFO 2017-06-30 07:16:36,662 main.py:52] epoch 6960, training loss: 5438.03, average training loss: 5570.71, base loss: 15962.72
[INFO 2017-06-30 07:16:39,836 main.py:52] epoch 6961, training loss: 5076.76, average training loss: 5569.96, base loss: 15962.22
[INFO 2017-06-30 07:16:42,985 main.py:52] epoch 6962, training loss: 5280.60, average training loss: 5569.65, base loss: 15962.31
[INFO 2017-06-30 07:16:46,120 main.py:52] epoch 6963, training loss: 5358.67, average training loss: 5569.45, base loss: 15962.09
[INFO 2017-06-30 07:16:49,260 main.py:52] epoch 6964, training loss: 5358.28, average training loss: 5569.39, base loss: 15961.97
[INFO 2017-06-30 07:16:52,405 main.py:52] epoch 6965, training loss: 5228.09, average training loss: 5569.21, base loss: 15961.59
[INFO 2017-06-30 07:16:55,538 main.py:52] epoch 6966, training loss: 5738.45, average training loss: 5569.11, base loss: 15961.80
[INFO 2017-06-30 07:16:58,709 main.py:52] epoch 6967, training loss: 5534.65, average training loss: 5569.01, base loss: 15961.73
[INFO 2017-06-30 07:17:01,857 main.py:52] epoch 6968, training loss: 5392.25, average training loss: 5568.52, base loss: 15961.31
[INFO 2017-06-30 07:17:04,955 main.py:52] epoch 6969, training loss: 5485.90, average training loss: 5567.95, base loss: 15961.48
[INFO 2017-06-30 07:17:08,146 main.py:52] epoch 6970, training loss: 5448.44, average training loss: 5568.20, base loss: 15961.94
[INFO 2017-06-30 07:17:11,298 main.py:52] epoch 6971, training loss: 5737.71, average training loss: 5568.47, base loss: 15962.78
[INFO 2017-06-30 07:17:14,435 main.py:52] epoch 6972, training loss: 5467.68, average training loss: 5568.39, base loss: 15963.05
[INFO 2017-06-30 07:17:17,553 main.py:52] epoch 6973, training loss: 5673.91, average training loss: 5568.00, base loss: 15963.23
[INFO 2017-06-30 07:17:20,699 main.py:52] epoch 6974, training loss: 5287.71, average training loss: 5567.48, base loss: 15963.19
[INFO 2017-06-30 07:17:23,849 main.py:52] epoch 6975, training loss: 5467.95, average training loss: 5567.36, base loss: 15963.30
[INFO 2017-06-30 07:17:27,010 main.py:52] epoch 6976, training loss: 5615.48, average training loss: 5567.30, base loss: 15963.34
[INFO 2017-06-30 07:17:30,159 main.py:52] epoch 6977, training loss: 5581.82, average training loss: 5567.21, base loss: 15963.48
[INFO 2017-06-30 07:17:33,317 main.py:52] epoch 6978, training loss: 5544.37, average training loss: 5566.79, base loss: 15963.77
[INFO 2017-06-30 07:17:36,478 main.py:52] epoch 6979, training loss: 5518.37, average training loss: 5566.81, base loss: 15963.61
[INFO 2017-06-30 07:17:39,645 main.py:52] epoch 6980, training loss: 5492.81, average training loss: 5566.84, base loss: 15963.34
[INFO 2017-06-30 07:17:42,789 main.py:52] epoch 6981, training loss: 5559.18, average training loss: 5566.71, base loss: 15963.22
[INFO 2017-06-30 07:17:45,893 main.py:52] epoch 6982, training loss: 5350.65, average training loss: 5566.19, base loss: 15962.74
[INFO 2017-06-30 07:17:49,020 main.py:52] epoch 6983, training loss: 5350.76, average training loss: 5565.93, base loss: 15962.90
[INFO 2017-06-30 07:17:52,157 main.py:52] epoch 6984, training loss: 5286.14, average training loss: 5565.32, base loss: 15962.59
[INFO 2017-06-30 07:17:55,297 main.py:52] epoch 6985, training loss: 5311.50, average training loss: 5565.56, base loss: 15962.08
[INFO 2017-06-30 07:17:58,442 main.py:52] epoch 6986, training loss: 5260.54, average training loss: 5565.20, base loss: 15962.03
[INFO 2017-06-30 07:18:01,591 main.py:52] epoch 6987, training loss: 5311.59, average training loss: 5565.15, base loss: 15962.13
[INFO 2017-06-30 07:18:04,694 main.py:52] epoch 6988, training loss: 5125.96, average training loss: 5564.28, base loss: 15961.71
[INFO 2017-06-30 07:18:07,811 main.py:52] epoch 6989, training loss: 5576.26, average training loss: 5564.46, base loss: 15961.69
[INFO 2017-06-30 07:18:10,976 main.py:52] epoch 6990, training loss: 5398.83, average training loss: 5564.16, base loss: 15961.49
[INFO 2017-06-30 07:18:14,098 main.py:52] epoch 6991, training loss: 5249.68, average training loss: 5563.97, base loss: 15961.28
[INFO 2017-06-30 07:18:17,215 main.py:52] epoch 6992, training loss: 5433.75, average training loss: 5563.63, base loss: 15961.27
[INFO 2017-06-30 07:18:20,368 main.py:52] epoch 6993, training loss: 5457.64, average training loss: 5563.48, base loss: 15961.18
[INFO 2017-06-30 07:18:23,496 main.py:52] epoch 6994, training loss: 5727.34, average training loss: 5563.28, base loss: 15961.49
[INFO 2017-06-30 07:18:26,615 main.py:52] epoch 6995, training loss: 5767.08, average training loss: 5563.46, base loss: 15961.72
[INFO 2017-06-30 07:18:29,770 main.py:52] epoch 6996, training loss: 5222.53, average training loss: 5563.17, base loss: 15961.46
[INFO 2017-06-30 07:18:32,937 main.py:52] epoch 6997, training loss: 5516.96, average training loss: 5562.66, base loss: 15961.37
[INFO 2017-06-30 07:18:36,026 main.py:52] epoch 6998, training loss: 5941.12, average training loss: 5563.19, base loss: 15962.29
[INFO 2017-06-30 07:18:39,226 main.py:52] epoch 6999, training loss: 5280.48, average training loss: 5562.57, base loss: 15962.44
[INFO 2017-06-30 07:18:39,226 main.py:54] epoch 6999, testing
[INFO 2017-06-30 07:18:52,155 main.py:97] average testing loss: 5625.97, base loss: 15811.95
[INFO 2017-06-30 07:18:52,155 main.py:98] improve_loss: 10185.98, improve_percent: 0.64
[INFO 2017-06-30 07:18:52,157 main.py:66] current best improved percent: 0.66
[INFO 2017-06-30 07:18:55,291 main.py:52] epoch 7000, training loss: 5260.56, average training loss: 5562.33, base loss: 15962.38
[INFO 2017-06-30 07:18:58,420 main.py:52] epoch 7001, training loss: 5722.77, average training loss: 5562.54, base loss: 15962.50
[INFO 2017-06-30 07:19:01,551 main.py:52] epoch 7002, training loss: 5672.83, average training loss: 5562.52, base loss: 15962.98
[INFO 2017-06-30 07:19:04,698 main.py:52] epoch 7003, training loss: 5602.41, average training loss: 5562.77, base loss: 15963.55
[INFO 2017-06-30 07:19:07,833 main.py:52] epoch 7004, training loss: 6195.17, average training loss: 5563.40, base loss: 15964.59
[INFO 2017-06-30 07:19:10,993 main.py:52] epoch 7005, training loss: 5750.63, average training loss: 5563.79, base loss: 15965.06
[INFO 2017-06-30 07:19:14,162 main.py:52] epoch 7006, training loss: 5799.67, average training loss: 5563.61, base loss: 15965.65
[INFO 2017-06-30 07:19:17,345 main.py:52] epoch 7007, training loss: 5483.98, average training loss: 5563.16, base loss: 15965.44
[INFO 2017-06-30 07:19:20,478 main.py:52] epoch 7008, training loss: 5522.62, average training loss: 5563.24, base loss: 15965.51
[INFO 2017-06-30 07:19:23,589 main.py:52] epoch 7009, training loss: 5908.64, average training loss: 5563.62, base loss: 15966.34
[INFO 2017-06-30 07:19:26,767 main.py:52] epoch 7010, training loss: 5283.62, average training loss: 5563.17, base loss: 15966.22
[INFO 2017-06-30 07:19:29,871 main.py:52] epoch 7011, training loss: 5486.24, average training loss: 5563.19, base loss: 15966.16
[INFO 2017-06-30 07:19:33,012 main.py:52] epoch 7012, training loss: 5216.25, average training loss: 5562.83, base loss: 15966.25
[INFO 2017-06-30 07:19:36,191 main.py:52] epoch 7013, training loss: 5293.39, average training loss: 5562.18, base loss: 15966.21
[INFO 2017-06-30 07:19:39,310 main.py:52] epoch 7014, training loss: 5601.70, average training loss: 5562.23, base loss: 15966.05
[INFO 2017-06-30 07:19:42,477 main.py:52] epoch 7015, training loss: 5539.62, average training loss: 5562.10, base loss: 15966.53
[INFO 2017-06-30 07:19:45,605 main.py:52] epoch 7016, training loss: 5804.92, average training loss: 5562.52, base loss: 15966.89
[INFO 2017-06-30 07:19:48,767 main.py:52] epoch 7017, training loss: 5336.53, average training loss: 5562.30, base loss: 15966.76
[INFO 2017-06-30 07:19:51,907 main.py:52] epoch 7018, training loss: 5223.51, average training loss: 5561.73, base loss: 15966.57
[INFO 2017-06-30 07:19:55,072 main.py:52] epoch 7019, training loss: 5646.38, average training loss: 5561.73, base loss: 15966.66
[INFO 2017-06-30 07:19:58,206 main.py:52] epoch 7020, training loss: 5862.53, average training loss: 5562.24, base loss: 15967.15
[INFO 2017-06-30 07:20:01,355 main.py:52] epoch 7021, training loss: 5805.04, average training loss: 5562.30, base loss: 15967.49
[INFO 2017-06-30 07:20:04,533 main.py:52] epoch 7022, training loss: 5758.28, average training loss: 5562.43, base loss: 15967.51
[INFO 2017-06-30 07:20:07,657 main.py:52] epoch 7023, training loss: 5286.46, average training loss: 5562.25, base loss: 15967.48
[INFO 2017-06-30 07:20:10,779 main.py:52] epoch 7024, training loss: 5512.87, average training loss: 5562.37, base loss: 15967.59
[INFO 2017-06-30 07:20:13,899 main.py:52] epoch 7025, training loss: 5592.10, average training loss: 5562.55, base loss: 15967.95
[INFO 2017-06-30 07:20:17,044 main.py:52] epoch 7026, training loss: 5644.07, average training loss: 5562.12, base loss: 15967.99
[INFO 2017-06-30 07:20:20,220 main.py:52] epoch 7027, training loss: 5690.36, average training loss: 5562.19, base loss: 15968.04
[INFO 2017-06-30 07:20:23,326 main.py:52] epoch 7028, training loss: 5508.43, average training loss: 5562.25, base loss: 15967.51
[INFO 2017-06-30 07:20:26,454 main.py:52] epoch 7029, training loss: 5430.48, average training loss: 5561.97, base loss: 15967.35
[INFO 2017-06-30 07:20:29,653 main.py:52] epoch 7030, training loss: 5117.66, average training loss: 5561.90, base loss: 15966.86
[INFO 2017-06-30 07:20:32,793 main.py:52] epoch 7031, training loss: 5710.90, average training loss: 5562.30, base loss: 15966.70
[INFO 2017-06-30 07:20:35,959 main.py:52] epoch 7032, training loss: 5303.34, average training loss: 5562.15, base loss: 15966.42
[INFO 2017-06-30 07:20:39,045 main.py:52] epoch 7033, training loss: 5620.33, average training loss: 5562.01, base loss: 15966.50
[INFO 2017-06-30 07:20:42,196 main.py:52] epoch 7034, training loss: 5594.87, average training loss: 5562.16, base loss: 15966.35
[INFO 2017-06-30 07:20:45,316 main.py:52] epoch 7035, training loss: 5201.02, average training loss: 5561.73, base loss: 15966.00
[INFO 2017-06-30 07:20:48,471 main.py:52] epoch 7036, training loss: 5447.91, average training loss: 5561.67, base loss: 15965.74
[INFO 2017-06-30 07:20:51,617 main.py:52] epoch 7037, training loss: 5545.44, average training loss: 5561.19, base loss: 15965.62
[INFO 2017-06-30 07:20:54,736 main.py:52] epoch 7038, training loss: 5324.88, average training loss: 5560.90, base loss: 15965.34
[INFO 2017-06-30 07:20:57,943 main.py:52] epoch 7039, training loss: 5610.99, average training loss: 5560.87, base loss: 15965.88
[INFO 2017-06-30 07:21:01,074 main.py:52] epoch 7040, training loss: 5748.91, average training loss: 5560.95, base loss: 15966.27
[INFO 2017-06-30 07:21:04,211 main.py:52] epoch 7041, training loss: 5395.65, average training loss: 5560.55, base loss: 15966.30
[INFO 2017-06-30 07:21:07,325 main.py:52] epoch 7042, training loss: 5365.17, average training loss: 5560.75, base loss: 15966.41
[INFO 2017-06-30 07:21:10,430 main.py:52] epoch 7043, training loss: 5518.45, average training loss: 5560.78, base loss: 15966.57
[INFO 2017-06-30 07:21:13,561 main.py:52] epoch 7044, training loss: 5793.98, average training loss: 5560.66, base loss: 15966.80
[INFO 2017-06-30 07:21:16,686 main.py:52] epoch 7045, training loss: 5705.49, average training loss: 5560.99, base loss: 15967.11
[INFO 2017-06-30 07:21:19,831 main.py:52] epoch 7046, training loss: 5402.35, average training loss: 5560.46, base loss: 15966.98
[INFO 2017-06-30 07:21:22,979 main.py:52] epoch 7047, training loss: 5835.18, average training loss: 5560.91, base loss: 15967.32
[INFO 2017-06-30 07:21:26,188 main.py:52] epoch 7048, training loss: 6081.69, average training loss: 5561.45, base loss: 15967.81
[INFO 2017-06-30 07:21:29,358 main.py:52] epoch 7049, training loss: 5139.48, average training loss: 5560.43, base loss: 15967.69
[INFO 2017-06-30 07:21:32,489 main.py:52] epoch 7050, training loss: 5609.84, average training loss: 5560.48, base loss: 15967.43
[INFO 2017-06-30 07:21:35,617 main.py:52] epoch 7051, training loss: 5444.44, average training loss: 5560.51, base loss: 15967.20
[INFO 2017-06-30 07:21:38,727 main.py:52] epoch 7052, training loss: 5363.31, average training loss: 5560.22, base loss: 15967.12
[INFO 2017-06-30 07:21:41,870 main.py:52] epoch 7053, training loss: 5690.87, average training loss: 5560.38, base loss: 15967.28
[INFO 2017-06-30 07:21:44,997 main.py:52] epoch 7054, training loss: 5446.89, average training loss: 5560.06, base loss: 15967.36
[INFO 2017-06-30 07:21:48,115 main.py:52] epoch 7055, training loss: 5290.08, average training loss: 5559.97, base loss: 15967.37
[INFO 2017-06-30 07:21:51,255 main.py:52] epoch 7056, training loss: 5689.05, average training loss: 5560.36, base loss: 15967.53
[INFO 2017-06-30 07:21:54,375 main.py:52] epoch 7057, training loss: 5696.53, average training loss: 5560.56, base loss: 15967.35
[INFO 2017-06-30 07:21:57,520 main.py:52] epoch 7058, training loss: 5508.11, average training loss: 5560.59, base loss: 15967.47
[INFO 2017-06-30 07:22:00,633 main.py:52] epoch 7059, training loss: 5801.86, average training loss: 5560.75, base loss: 15967.72
[INFO 2017-06-30 07:22:03,750 main.py:52] epoch 7060, training loss: 4992.77, average training loss: 5559.72, base loss: 15966.95
[INFO 2017-06-30 07:22:06,874 main.py:52] epoch 7061, training loss: 6042.46, average training loss: 5560.32, base loss: 15966.83
[INFO 2017-06-30 07:22:10,007 main.py:52] epoch 7062, training loss: 5909.80, average training loss: 5560.60, base loss: 15966.92
[INFO 2017-06-30 07:22:13,171 main.py:52] epoch 7063, training loss: 5288.29, average training loss: 5560.03, base loss: 15966.82
[INFO 2017-06-30 07:22:16,298 main.py:52] epoch 7064, training loss: 5419.92, average training loss: 5559.79, base loss: 15966.96
[INFO 2017-06-30 07:22:19,471 main.py:52] epoch 7065, training loss: 5447.92, average training loss: 5559.70, base loss: 15967.30
[INFO 2017-06-30 07:22:22,638 main.py:52] epoch 7066, training loss: 5129.98, average training loss: 5559.48, base loss: 15967.22
[INFO 2017-06-30 07:22:25,834 main.py:52] epoch 7067, training loss: 5728.05, average training loss: 5559.89, base loss: 15967.49
[INFO 2017-06-30 07:22:29,021 main.py:52] epoch 7068, training loss: 5360.96, average training loss: 5559.59, base loss: 15967.09
[INFO 2017-06-30 07:22:32,129 main.py:52] epoch 7069, training loss: 5658.33, average training loss: 5559.87, base loss: 15966.98
[INFO 2017-06-30 07:22:35,324 main.py:52] epoch 7070, training loss: 5422.60, average training loss: 5559.93, base loss: 15966.62
[INFO 2017-06-30 07:22:38,490 main.py:52] epoch 7071, training loss: 5543.80, average training loss: 5560.10, base loss: 15966.06
[INFO 2017-06-30 07:22:41,658 main.py:52] epoch 7072, training loss: 5542.70, average training loss: 5559.94, base loss: 15966.26
[INFO 2017-06-30 07:22:44,810 main.py:52] epoch 7073, training loss: 5941.63, average training loss: 5560.28, base loss: 15966.88
[INFO 2017-06-30 07:22:47,943 main.py:52] epoch 7074, training loss: 5582.33, average training loss: 5560.33, base loss: 15967.15
[INFO 2017-06-30 07:22:51,101 main.py:52] epoch 7075, training loss: 5404.17, average training loss: 5560.39, base loss: 15967.24
[INFO 2017-06-30 07:22:54,215 main.py:52] epoch 7076, training loss: 5828.40, average training loss: 5560.67, base loss: 15967.55
[INFO 2017-06-30 07:22:57,350 main.py:52] epoch 7077, training loss: 5421.69, average training loss: 5560.48, base loss: 15967.41
[INFO 2017-06-30 07:23:00,482 main.py:52] epoch 7078, training loss: 5393.86, average training loss: 5560.26, base loss: 15967.70
[INFO 2017-06-30 07:23:03,632 main.py:52] epoch 7079, training loss: 5465.73, average training loss: 5560.01, base loss: 15967.98
[INFO 2017-06-30 07:23:06,739 main.py:52] epoch 7080, training loss: 5340.37, average training loss: 5559.35, base loss: 15968.05
[INFO 2017-06-30 07:23:09,899 main.py:52] epoch 7081, training loss: 5098.81, average training loss: 5559.31, base loss: 15967.48
[INFO 2017-06-30 07:23:13,030 main.py:52] epoch 7082, training loss: 5743.12, average training loss: 5559.63, base loss: 15967.77
[INFO 2017-06-30 07:23:16,235 main.py:52] epoch 7083, training loss: 5480.91, average training loss: 5559.60, base loss: 15967.65
[INFO 2017-06-30 07:23:19,428 main.py:52] epoch 7084, training loss: 5869.89, average training loss: 5559.52, base loss: 15967.83
[INFO 2017-06-30 07:23:22,616 main.py:52] epoch 7085, training loss: 5443.50, average training loss: 5559.43, base loss: 15968.00
[INFO 2017-06-30 07:23:25,741 main.py:52] epoch 7086, training loss: 5657.88, average training loss: 5559.77, base loss: 15967.93
[INFO 2017-06-30 07:23:28,884 main.py:52] epoch 7087, training loss: 5340.73, average training loss: 5559.33, base loss: 15967.76
[INFO 2017-06-30 07:23:32,033 main.py:52] epoch 7088, training loss: 5463.82, average training loss: 5559.43, base loss: 15967.49
[INFO 2017-06-30 07:23:35,178 main.py:52] epoch 7089, training loss: 5783.26, average training loss: 5559.86, base loss: 15967.51
[INFO 2017-06-30 07:23:38,366 main.py:52] epoch 7090, training loss: 5894.06, average training loss: 5560.29, base loss: 15968.06
[INFO 2017-06-30 07:23:41,510 main.py:52] epoch 7091, training loss: 5072.06, average training loss: 5559.86, base loss: 15967.57
[INFO 2017-06-30 07:23:44,626 main.py:52] epoch 7092, training loss: 5369.26, average training loss: 5559.61, base loss: 15966.98
[INFO 2017-06-30 07:23:47,819 main.py:52] epoch 7093, training loss: 5991.92, average training loss: 5560.16, base loss: 15967.41
[INFO 2017-06-30 07:23:50,968 main.py:52] epoch 7094, training loss: 5506.21, average training loss: 5560.02, base loss: 15967.83
[INFO 2017-06-30 07:23:54,092 main.py:52] epoch 7095, training loss: 5318.47, average training loss: 5559.56, base loss: 15967.84
[INFO 2017-06-30 07:23:57,230 main.py:52] epoch 7096, training loss: 5748.63, average training loss: 5559.73, base loss: 15968.45
[INFO 2017-06-30 07:24:00,360 main.py:52] epoch 7097, training loss: 6160.94, average training loss: 5560.51, base loss: 15969.02
[INFO 2017-06-30 07:24:03,477 main.py:52] epoch 7098, training loss: 5844.53, average training loss: 5560.59, base loss: 15969.31
[INFO 2017-06-30 07:24:06,680 main.py:52] epoch 7099, training loss: 5712.71, average training loss: 5561.00, base loss: 15969.37
[INFO 2017-06-30 07:24:06,681 main.py:54] epoch 7099, testing
[INFO 2017-06-30 07:24:19,702 main.py:97] average testing loss: 5450.66, base loss: 16077.46
[INFO 2017-06-30 07:24:19,702 main.py:98] improve_loss: 10626.80, improve_percent: 0.66
[INFO 2017-06-30 07:24:19,704 main.py:62] model save to ./model/final.pth
[INFO 2017-06-30 07:24:19,738 main.py:66] current best improved percent: 0.66
[INFO 2017-06-30 07:24:22,840 main.py:52] epoch 7100, training loss: 5416.62, average training loss: 5560.86, base loss: 15968.96
[INFO 2017-06-30 07:24:25,956 main.py:52] epoch 7101, training loss: 5689.58, average training loss: 5561.31, base loss: 15969.18
[INFO 2017-06-30 07:24:29,094 main.py:52] epoch 7102, training loss: 5606.24, average training loss: 5561.03, base loss: 15969.45
[INFO 2017-06-30 07:24:32,293 main.py:52] epoch 7103, training loss: 5182.98, average training loss: 5560.32, base loss: 15969.01
[INFO 2017-06-30 07:24:35,389 main.py:52] epoch 7104, training loss: 5278.23, average training loss: 5560.11, base loss: 15968.37
[INFO 2017-06-30 07:24:38,486 main.py:52] epoch 7105, training loss: 5125.21, average training loss: 5559.31, base loss: 15968.03
[INFO 2017-06-30 07:24:41,598 main.py:52] epoch 7106, training loss: 5701.39, average training loss: 5559.80, base loss: 15968.04
[INFO 2017-06-30 07:24:44,746 main.py:52] epoch 7107, training loss: 5603.35, average training loss: 5559.91, base loss: 15968.34
[INFO 2017-06-30 07:24:47,871 main.py:52] epoch 7108, training loss: 5477.29, average training loss: 5559.80, base loss: 15968.66
[INFO 2017-06-30 07:24:50,998 main.py:52] epoch 7109, training loss: 5171.79, average training loss: 5559.45, base loss: 15968.32
[INFO 2017-06-30 07:24:54,149 main.py:52] epoch 7110, training loss: 5471.69, average training loss: 5559.08, base loss: 15968.14
[INFO 2017-06-30 07:24:57,285 main.py:52] epoch 7111, training loss: 5495.55, average training loss: 5559.03, base loss: 15968.34
[INFO 2017-06-30 07:25:00,456 main.py:52] epoch 7112, training loss: 5640.48, average training loss: 5558.58, base loss: 15968.63
[INFO 2017-06-30 07:25:03,587 main.py:52] epoch 7113, training loss: 5282.18, average training loss: 5558.61, base loss: 15968.20
[INFO 2017-06-30 07:25:06,707 main.py:52] epoch 7114, training loss: 5712.21, average training loss: 5558.73, base loss: 15968.20
[INFO 2017-06-30 07:25:09,851 main.py:52] epoch 7115, training loss: 5584.16, average training loss: 5558.64, base loss: 15968.34
[INFO 2017-06-30 07:25:13,031 main.py:52] epoch 7116, training loss: 5592.86, average training loss: 5558.78, base loss: 15968.28
[INFO 2017-06-30 07:25:16,180 main.py:52] epoch 7117, training loss: 5631.71, average training loss: 5558.97, base loss: 15968.18
[INFO 2017-06-30 07:25:19,329 main.py:52] epoch 7118, training loss: 5763.38, average training loss: 5558.94, base loss: 15968.45
[INFO 2017-06-30 07:25:22,472 main.py:52] epoch 7119, training loss: 5522.02, average training loss: 5559.01, base loss: 15968.40
[INFO 2017-06-30 07:25:25,592 main.py:52] epoch 7120, training loss: 5330.56, average training loss: 5559.00, base loss: 15967.87
[INFO 2017-06-30 07:25:28,700 main.py:52] epoch 7121, training loss: 5818.65, average training loss: 5559.15, base loss: 15967.95
[INFO 2017-06-30 07:25:31,866 main.py:52] epoch 7122, training loss: 5472.18, average training loss: 5558.90, base loss: 15967.85
[INFO 2017-06-30 07:25:35,036 main.py:52] epoch 7123, training loss: 5623.94, average training loss: 5558.79, base loss: 15967.90
[INFO 2017-06-30 07:25:38,161 main.py:52] epoch 7124, training loss: 5378.81, average training loss: 5558.56, base loss: 15967.36
[INFO 2017-06-30 07:25:41,303 main.py:52] epoch 7125, training loss: 5915.57, average training loss: 5558.66, base loss: 15967.56
[INFO 2017-06-30 07:25:44,419 main.py:52] epoch 7126, training loss: 5304.62, average training loss: 5558.66, base loss: 15967.32
[INFO 2017-06-30 07:25:47,620 main.py:52] epoch 7127, training loss: 5725.94, average training loss: 5558.60, base loss: 15967.43
[INFO 2017-06-30 07:25:50,778 main.py:52] epoch 7128, training loss: 5434.63, average training loss: 5558.70, base loss: 15967.41
[INFO 2017-06-30 07:25:53,869 main.py:52] epoch 7129, training loss: 5474.87, average training loss: 5558.51, base loss: 15967.39
[INFO 2017-06-30 07:25:57,050 main.py:52] epoch 7130, training loss: 6014.85, average training loss: 5559.18, base loss: 15968.16
[INFO 2017-06-30 07:26:00,189 main.py:52] epoch 7131, training loss: 5358.76, average training loss: 5559.09, base loss: 15968.42
[INFO 2017-06-30 07:26:03,348 main.py:52] epoch 7132, training loss: 5585.10, average training loss: 5559.29, base loss: 15968.40
[INFO 2017-06-30 07:26:06,459 main.py:52] epoch 7133, training loss: 5314.46, average training loss: 5558.59, base loss: 15968.10
[INFO 2017-06-30 07:26:09,647 main.py:52] epoch 7134, training loss: 5771.34, average training loss: 5558.80, base loss: 15968.34
[INFO 2017-06-30 07:26:12,779 main.py:52] epoch 7135, training loss: 5702.01, average training loss: 5558.83, base loss: 15968.52
[INFO 2017-06-30 07:26:15,899 main.py:52] epoch 7136, training loss: 5467.87, average training loss: 5558.77, base loss: 15968.56
[INFO 2017-06-30 07:26:19,049 main.py:52] epoch 7137, training loss: 5273.11, average training loss: 5558.55, base loss: 15968.68
[INFO 2017-06-30 07:26:22,191 main.py:52] epoch 7138, training loss: 5579.50, average training loss: 5558.66, base loss: 15968.68
[INFO 2017-06-30 07:26:25,313 main.py:52] epoch 7139, training loss: 5368.40, average training loss: 5558.58, base loss: 15968.29
[INFO 2017-06-30 07:26:28,465 main.py:52] epoch 7140, training loss: 5678.99, average training loss: 5558.17, base loss: 15968.41
[INFO 2017-06-30 07:26:31,583 main.py:52] epoch 7141, training loss: 5756.78, average training loss: 5558.41, base loss: 15968.93
[INFO 2017-06-30 07:26:34,760 main.py:52] epoch 7142, training loss: 5503.77, average training loss: 5558.46, base loss: 15969.00
[INFO 2017-06-30 07:26:37,900 main.py:52] epoch 7143, training loss: 4968.27, average training loss: 5557.49, base loss: 15968.64
[INFO 2017-06-30 07:26:41,020 main.py:52] epoch 7144, training loss: 5523.88, average training loss: 5557.61, base loss: 15968.56
[INFO 2017-06-30 07:26:44,192 main.py:52] epoch 7145, training loss: 5403.43, average training loss: 5557.58, base loss: 15967.87
[INFO 2017-06-30 07:26:47,282 main.py:52] epoch 7146, training loss: 5511.54, average training loss: 5557.80, base loss: 15968.12
[INFO 2017-06-30 07:26:50,458 main.py:52] epoch 7147, training loss: 5795.40, average training loss: 5558.02, base loss: 15968.33
[INFO 2017-06-30 07:26:53,568 main.py:52] epoch 7148, training loss: 5592.90, average training loss: 5558.05, base loss: 15968.40
[INFO 2017-06-30 07:26:56,707 main.py:52] epoch 7149, training loss: 5831.12, average training loss: 5558.57, base loss: 15968.48
[INFO 2017-06-30 07:26:59,824 main.py:52] epoch 7150, training loss: 5631.10, average training loss: 5558.48, base loss: 15968.43
[INFO 2017-06-30 07:27:02,948 main.py:52] epoch 7151, training loss: 5507.33, average training loss: 5558.03, base loss: 15968.69
[INFO 2017-06-30 07:27:06,055 main.py:52] epoch 7152, training loss: 5469.13, average training loss: 5557.84, base loss: 15968.72
[INFO 2017-06-30 07:27:09,169 main.py:52] epoch 7153, training loss: 5033.84, average training loss: 5557.10, base loss: 15968.37
[INFO 2017-06-30 07:27:12,347 main.py:52] epoch 7154, training loss: 5062.08, average training loss: 5556.65, base loss: 15968.13
[INFO 2017-06-30 07:27:15,481 main.py:52] epoch 7155, training loss: 5876.86, average training loss: 5556.98, base loss: 15968.57
[INFO 2017-06-30 07:27:18,609 main.py:52] epoch 7156, training loss: 5385.91, average training loss: 5556.89, base loss: 15969.03
[INFO 2017-06-30 07:27:21,774 main.py:52] epoch 7157, training loss: 5821.80, average training loss: 5557.09, base loss: 15969.02
[INFO 2017-06-30 07:27:24,898 main.py:52] epoch 7158, training loss: 5409.37, average training loss: 5556.89, base loss: 15968.89
[INFO 2017-06-30 07:27:28,042 main.py:52] epoch 7159, training loss: 5584.15, average training loss: 5556.68, base loss: 15968.30
[INFO 2017-06-30 07:27:31,228 main.py:52] epoch 7160, training loss: 5564.45, average training loss: 5556.77, base loss: 15968.61
[INFO 2017-06-30 07:27:34,398 main.py:52] epoch 7161, training loss: 5399.69, average training loss: 5556.26, base loss: 15968.21
[INFO 2017-06-30 07:27:37,588 main.py:52] epoch 7162, training loss: 5308.54, average training loss: 5555.81, base loss: 15967.63
[INFO 2017-06-30 07:27:40,762 main.py:52] epoch 7163, training loss: 5623.42, average training loss: 5555.86, base loss: 15967.48
[INFO 2017-06-30 07:27:43,968 main.py:52] epoch 7164, training loss: 5092.26, average training loss: 5555.36, base loss: 15966.93
[INFO 2017-06-30 07:27:47,098 main.py:52] epoch 7165, training loss: 5469.83, average training loss: 5555.06, base loss: 15966.61
[INFO 2017-06-30 07:27:50,241 main.py:52] epoch 7166, training loss: 5098.44, average training loss: 5554.25, base loss: 15965.83
[INFO 2017-06-30 07:27:53,348 main.py:52] epoch 7167, training loss: 6130.74, average training loss: 5554.64, base loss: 15966.05
[INFO 2017-06-30 07:27:56,475 main.py:52] epoch 7168, training loss: 4973.77, average training loss: 5554.03, base loss: 15965.78
[INFO 2017-06-30 07:27:59,626 main.py:52] epoch 7169, training loss: 5517.39, average training loss: 5553.62, base loss: 15965.99
[INFO 2017-06-30 07:28:02,791 main.py:52] epoch 7170, training loss: 5552.44, average training loss: 5553.69, base loss: 15966.04
[INFO 2017-06-30 07:28:05,916 main.py:52] epoch 7171, training loss: 5251.24, average training loss: 5553.41, base loss: 15965.57
[INFO 2017-06-30 07:28:09,060 main.py:52] epoch 7172, training loss: 5477.45, average training loss: 5553.14, base loss: 15965.37
[INFO 2017-06-30 07:28:12,218 main.py:52] epoch 7173, training loss: 5569.85, average training loss: 5552.85, base loss: 15965.43
[INFO 2017-06-30 07:28:15,336 main.py:52] epoch 7174, training loss: 5247.51, average training loss: 5552.63, base loss: 15965.49
[INFO 2017-06-30 07:28:18,482 main.py:52] epoch 7175, training loss: 5545.63, average training loss: 5552.54, base loss: 15965.58
[INFO 2017-06-30 07:28:21,631 main.py:52] epoch 7176, training loss: 5682.42, average training loss: 5552.49, base loss: 15965.53
[INFO 2017-06-30 07:28:24,819 main.py:52] epoch 7177, training loss: 6036.92, average training loss: 5552.57, base loss: 15966.12
[INFO 2017-06-30 07:28:27,966 main.py:52] epoch 7178, training loss: 5425.77, average training loss: 5552.18, base loss: 15965.99
[INFO 2017-06-30 07:28:31,053 main.py:52] epoch 7179, training loss: 5556.25, average training loss: 5552.67, base loss: 15965.90
[INFO 2017-06-30 07:28:34,188 main.py:52] epoch 7180, training loss: 5198.27, average training loss: 5552.71, base loss: 15965.64
[INFO 2017-06-30 07:28:37,298 main.py:52] epoch 7181, training loss: 5646.12, average training loss: 5552.55, base loss: 15965.60
[INFO 2017-06-30 07:28:40,445 main.py:52] epoch 7182, training loss: 5498.66, average training loss: 5552.45, base loss: 15965.14
[INFO 2017-06-30 07:28:43,577 main.py:52] epoch 7183, training loss: 5522.18, average training loss: 5552.19, base loss: 15965.25
[INFO 2017-06-30 07:28:46,731 main.py:52] epoch 7184, training loss: 5642.70, average training loss: 5551.99, base loss: 15965.63
[INFO 2017-06-30 07:28:49,877 main.py:52] epoch 7185, training loss: 6008.34, average training loss: 5552.44, base loss: 15966.08
[INFO 2017-06-30 07:28:53,008 main.py:52] epoch 7186, training loss: 5261.88, average training loss: 5552.01, base loss: 15966.14
[INFO 2017-06-30 07:28:56,101 main.py:52] epoch 7187, training loss: 5635.04, average training loss: 5552.17, base loss: 15966.39
[INFO 2017-06-30 07:28:59,272 main.py:52] epoch 7188, training loss: 5678.81, average training loss: 5552.68, base loss: 15966.14
[INFO 2017-06-30 07:29:02,400 main.py:52] epoch 7189, training loss: 5807.72, average training loss: 5552.91, base loss: 15966.22
[INFO 2017-06-30 07:29:05,554 main.py:52] epoch 7190, training loss: 5537.83, average training loss: 5552.33, base loss: 15966.09
[INFO 2017-06-30 07:29:08,696 main.py:52] epoch 7191, training loss: 5544.66, average training loss: 5552.22, base loss: 15965.91
[INFO 2017-06-30 07:29:11,856 main.py:52] epoch 7192, training loss: 5600.51, average training loss: 5552.25, base loss: 15965.63
[INFO 2017-06-30 07:29:14,986 main.py:52] epoch 7193, training loss: 5400.71, average training loss: 5552.15, base loss: 15965.56
[INFO 2017-06-30 07:29:18,115 main.py:52] epoch 7194, training loss: 4937.82, average training loss: 5551.50, base loss: 15965.21
[INFO 2017-06-30 07:29:21,258 main.py:52] epoch 7195, training loss: 6100.34, average training loss: 5551.85, base loss: 15965.90
[INFO 2017-06-30 07:29:24,427 main.py:52] epoch 7196, training loss: 5401.27, average training loss: 5551.63, base loss: 15965.71
[INFO 2017-06-30 07:29:27,537 main.py:52] epoch 7197, training loss: 6066.56, average training loss: 5552.43, base loss: 15966.02
[INFO 2017-06-30 07:29:30,666 main.py:52] epoch 7198, training loss: 5738.63, average training loss: 5552.75, base loss: 15966.37
[INFO 2017-06-30 07:29:33,763 main.py:52] epoch 7199, training loss: 5578.27, average training loss: 5552.65, base loss: 15966.29
[INFO 2017-06-30 07:29:33,764 main.py:54] epoch 7199, testing
[INFO 2017-06-30 07:29:46,798 main.py:97] average testing loss: 5600.18, base loss: 16391.26
[INFO 2017-06-30 07:29:46,798 main.py:98] improve_loss: 10791.07, improve_percent: 0.66
[INFO 2017-06-30 07:29:46,799 main.py:66] current best improved percent: 0.66
[INFO 2017-06-30 07:29:49,950 main.py:52] epoch 7200, training loss: 5671.26, average training loss: 5552.19, base loss: 15966.49
[INFO 2017-06-30 07:29:53,052 main.py:52] epoch 7201, training loss: 5372.06, average training loss: 5551.84, base loss: 15966.53
[INFO 2017-06-30 07:29:56,224 main.py:52] epoch 7202, training loss: 5627.24, average training loss: 5551.89, base loss: 15966.96
[INFO 2017-06-30 07:29:59,396 main.py:52] epoch 7203, training loss: 5388.16, average training loss: 5551.84, base loss: 15967.07
[INFO 2017-06-30 07:30:02,579 main.py:52] epoch 7204, training loss: 5710.65, average training loss: 5552.17, base loss: 15967.77
[INFO 2017-06-30 07:30:05,705 main.py:52] epoch 7205, training loss: 5445.70, average training loss: 5551.99, base loss: 15967.95
[INFO 2017-06-30 07:30:08,849 main.py:52] epoch 7206, training loss: 5932.19, average training loss: 5552.48, base loss: 15968.42
[INFO 2017-06-30 07:30:11,989 main.py:52] epoch 7207, training loss: 5588.49, average training loss: 5552.34, base loss: 15968.46
[INFO 2017-06-30 07:30:15,104 main.py:52] epoch 7208, training loss: 5600.64, average training loss: 5552.45, base loss: 15968.39
[INFO 2017-06-30 07:30:18,266 main.py:52] epoch 7209, training loss: 5566.42, average training loss: 5552.20, base loss: 15968.94
[INFO 2017-06-30 07:30:21,403 main.py:52] epoch 7210, training loss: 5460.93, average training loss: 5552.36, base loss: 15968.42
[INFO 2017-06-30 07:30:24,517 main.py:52] epoch 7211, training loss: 5691.25, average training loss: 5552.50, base loss: 15968.72
[INFO 2017-06-30 07:30:27,637 main.py:52] epoch 7212, training loss: 5484.88, average training loss: 5552.14, base loss: 15968.59
[INFO 2017-06-30 07:30:30,766 main.py:52] epoch 7213, training loss: 5707.33, average training loss: 5552.00, base loss: 15968.48
[INFO 2017-06-30 07:30:33,920 main.py:52] epoch 7214, training loss: 5176.53, average training loss: 5552.07, base loss: 15968.44
[INFO 2017-06-30 07:30:37,058 main.py:52] epoch 7215, training loss: 5487.92, average training loss: 5552.35, base loss: 15968.57
[INFO 2017-06-30 07:30:40,220 main.py:52] epoch 7216, training loss: 5560.57, average training loss: 5552.29, base loss: 15968.70
[INFO 2017-06-30 07:30:43,366 main.py:52] epoch 7217, training loss: 5759.20, average training loss: 5552.30, base loss: 15969.05
[INFO 2017-06-30 07:30:46,571 main.py:52] epoch 7218, training loss: 5562.46, average training loss: 5552.20, base loss: 15969.23
[INFO 2017-06-30 07:30:49,719 main.py:52] epoch 7219, training loss: 5586.62, average training loss: 5551.96, base loss: 15969.12
[INFO 2017-06-30 07:30:52,940 main.py:52] epoch 7220, training loss: 5313.49, average training loss: 5551.72, base loss: 15968.61
[INFO 2017-06-30 07:30:56,070 main.py:52] epoch 7221, training loss: 5368.70, average training loss: 5551.37, base loss: 15968.82
[INFO 2017-06-30 07:30:59,248 main.py:52] epoch 7222, training loss: 5246.00, average training loss: 5550.64, base loss: 15968.74
[INFO 2017-06-30 07:31:02,417 main.py:52] epoch 7223, training loss: 5306.27, average training loss: 5549.74, base loss: 15968.91
[INFO 2017-06-30 07:31:05,550 main.py:52] epoch 7224, training loss: 5621.60, average training loss: 5549.61, base loss: 15968.83
[INFO 2017-06-30 07:31:08,673 main.py:52] epoch 7225, training loss: 6408.72, average training loss: 5550.44, base loss: 15969.87
[INFO 2017-06-30 07:31:11,844 main.py:52] epoch 7226, training loss: 5535.57, average training loss: 5550.52, base loss: 15969.71
[INFO 2017-06-30 07:31:14,993 main.py:52] epoch 7227, training loss: 5513.95, average training loss: 5550.08, base loss: 15969.26
[INFO 2017-06-30 07:31:18,115 main.py:52] epoch 7228, training loss: 4989.01, average training loss: 5549.68, base loss: 15968.71
[INFO 2017-06-30 07:31:21,273 main.py:52] epoch 7229, training loss: 5561.23, average training loss: 5549.67, base loss: 15968.59
[INFO 2017-06-30 07:31:24,460 main.py:52] epoch 7230, training loss: 5611.41, average training loss: 5549.62, base loss: 15968.77
[INFO 2017-06-30 07:31:27,665 main.py:52] epoch 7231, training loss: 5497.73, average training loss: 5549.87, base loss: 15968.52
[INFO 2017-06-30 07:31:30,830 main.py:52] epoch 7232, training loss: 5410.23, average training loss: 5550.18, base loss: 15968.22
[INFO 2017-06-30 07:31:33,939 main.py:52] epoch 7233, training loss: 5564.49, average training loss: 5550.26, base loss: 15968.18
[INFO 2017-06-30 07:31:37,088 main.py:52] epoch 7234, training loss: 5052.16, average training loss: 5549.92, base loss: 15967.75
[INFO 2017-06-30 07:31:40,244 main.py:52] epoch 7235, training loss: 5702.26, average training loss: 5550.40, base loss: 15967.81
[INFO 2017-06-30 07:31:43,392 main.py:52] epoch 7236, training loss: 5506.40, average training loss: 5550.50, base loss: 15967.78
[INFO 2017-06-30 07:31:46,547 main.py:52] epoch 7237, training loss: 5354.75, average training loss: 5550.73, base loss: 15967.78
[INFO 2017-06-30 07:31:49,710 main.py:52] epoch 7238, training loss: 5736.88, average training loss: 5550.64, base loss: 15967.57
[INFO 2017-06-30 07:31:52,822 main.py:52] epoch 7239, training loss: 5158.32, average training loss: 5549.93, base loss: 15966.94
[INFO 2017-06-30 07:31:55,957 main.py:52] epoch 7240, training loss: 5279.03, average training loss: 5549.69, base loss: 15966.37
[INFO 2017-06-30 07:31:59,103 main.py:52] epoch 7241, training loss: 5632.41, average training loss: 5549.66, base loss: 15966.07
[INFO 2017-06-30 07:32:02,205 main.py:52] epoch 7242, training loss: 5437.67, average training loss: 5549.49, base loss: 15965.99
[INFO 2017-06-30 07:32:05,340 main.py:52] epoch 7243, training loss: 5051.15, average training loss: 5548.99, base loss: 15965.91
[INFO 2017-06-30 07:32:08,491 main.py:52] epoch 7244, training loss: 5774.25, average training loss: 5548.83, base loss: 15965.81
[INFO 2017-06-30 07:32:11,626 main.py:52] epoch 7245, training loss: 6023.89, average training loss: 5549.24, base loss: 15965.94
[INFO 2017-06-30 07:32:14,816 main.py:52] epoch 7246, training loss: 5730.31, average training loss: 5549.88, base loss: 15966.00
[INFO 2017-06-30 07:32:17,960 main.py:52] epoch 7247, training loss: 6156.28, average training loss: 5550.46, base loss: 15966.64
[INFO 2017-06-30 07:32:21,097 main.py:52] epoch 7248, training loss: 5555.26, average training loss: 5550.80, base loss: 15966.63
[INFO 2017-06-30 07:32:24,267 main.py:52] epoch 7249, training loss: 5674.22, average training loss: 5550.91, base loss: 15966.64
[INFO 2017-06-30 07:32:27,426 main.py:52] epoch 7250, training loss: 5446.93, average training loss: 5550.68, base loss: 15966.47
[INFO 2017-06-30 07:32:30,548 main.py:52] epoch 7251, training loss: 5339.20, average training loss: 5550.36, base loss: 15966.33
[INFO 2017-06-30 07:32:33,687 main.py:52] epoch 7252, training loss: 5096.50, average training loss: 5549.51, base loss: 15966.44
[INFO 2017-06-30 07:32:36,827 main.py:52] epoch 7253, training loss: 5573.80, average training loss: 5549.82, base loss: 15966.33
[INFO 2017-06-30 07:32:39,989 main.py:52] epoch 7254, training loss: 5459.16, average training loss: 5550.13, base loss: 15966.19
[INFO 2017-06-30 07:32:43,131 main.py:52] epoch 7255, training loss: 5398.81, average training loss: 5550.19, base loss: 15966.21
[INFO 2017-06-30 07:32:46,267 main.py:52] epoch 7256, training loss: 5803.79, average training loss: 5550.60, base loss: 15967.06
[INFO 2017-06-30 07:32:49,404 main.py:52] epoch 7257, training loss: 5663.55, average training loss: 5550.47, base loss: 15967.41
[INFO 2017-06-30 07:32:52,536 main.py:52] epoch 7258, training loss: 5236.46, average training loss: 5550.06, base loss: 15967.36
[INFO 2017-06-30 07:32:55,665 main.py:52] epoch 7259, training loss: 5697.32, average training loss: 5550.08, base loss: 15967.46
[INFO 2017-06-30 07:32:58,824 main.py:52] epoch 7260, training loss: 5320.82, average training loss: 5550.16, base loss: 15967.37
[INFO 2017-06-30 07:33:01,955 main.py:52] epoch 7261, training loss: 5372.55, average training loss: 5550.17, base loss: 15967.19
[INFO 2017-06-30 07:33:05,067 main.py:52] epoch 7262, training loss: 5817.82, average training loss: 5550.46, base loss: 15967.71
[INFO 2017-06-30 07:33:08,224 main.py:52] epoch 7263, training loss: 5315.06, average training loss: 5549.96, base loss: 15967.36
[INFO 2017-06-30 07:33:11,341 main.py:52] epoch 7264, training loss: 5500.73, average training loss: 5549.80, base loss: 15967.21
[INFO 2017-06-30 07:33:14,467 main.py:52] epoch 7265, training loss: 5803.04, average training loss: 5549.53, base loss: 15967.63
[INFO 2017-06-30 07:33:17,601 main.py:52] epoch 7266, training loss: 5909.36, average training loss: 5549.76, base loss: 15967.74
[INFO 2017-06-30 07:33:20,777 main.py:52] epoch 7267, training loss: 5621.56, average training loss: 5549.67, base loss: 15967.90
[INFO 2017-06-30 07:33:23,882 main.py:52] epoch 7268, training loss: 5379.87, average training loss: 5549.34, base loss: 15967.59
[INFO 2017-06-30 07:33:27,038 main.py:52] epoch 7269, training loss: 5461.94, average training loss: 5548.97, base loss: 15967.30
[INFO 2017-06-30 07:33:30,191 main.py:52] epoch 7270, training loss: 5428.87, average training loss: 5548.67, base loss: 15967.20
[INFO 2017-06-30 07:33:33,370 main.py:52] epoch 7271, training loss: 5725.80, average training loss: 5548.54, base loss: 15967.57
[INFO 2017-06-30 07:33:36,502 main.py:52] epoch 7272, training loss: 5547.06, average training loss: 5548.42, base loss: 15967.73
[INFO 2017-06-30 07:33:39,668 main.py:52] epoch 7273, training loss: 5426.05, average training loss: 5548.47, base loss: 15967.73
[INFO 2017-06-30 07:33:42,808 main.py:52] epoch 7274, training loss: 5504.07, average training loss: 5548.57, base loss: 15967.28
[INFO 2017-06-30 07:33:45,946 main.py:52] epoch 7275, training loss: 5148.41, average training loss: 5548.26, base loss: 15967.04
[INFO 2017-06-30 07:33:49,104 main.py:52] epoch 7276, training loss: 5556.36, average training loss: 5548.43, base loss: 15967.14
[INFO 2017-06-30 07:33:52,243 main.py:52] epoch 7277, training loss: 5424.49, average training loss: 5548.32, base loss: 15967.37
[INFO 2017-06-30 07:33:55,396 main.py:52] epoch 7278, training loss: 5791.08, average training loss: 5548.49, base loss: 15967.61
[INFO 2017-06-30 07:33:58,569 main.py:52] epoch 7279, training loss: 6179.39, average training loss: 5548.78, base loss: 15968.12
[INFO 2017-06-30 07:34:01,691 main.py:52] epoch 7280, training loss: 5435.75, average training loss: 5548.60, base loss: 15968.02
[INFO 2017-06-30 07:34:04,823 main.py:52] epoch 7281, training loss: 5405.49, average training loss: 5548.63, base loss: 15967.82
[INFO 2017-06-30 07:34:07,974 main.py:52] epoch 7282, training loss: 5475.46, average training loss: 5548.54, base loss: 15967.39
[INFO 2017-06-30 07:34:11,129 main.py:52] epoch 7283, training loss: 5560.59, average training loss: 5548.74, base loss: 15967.47
[INFO 2017-06-30 07:34:14,291 main.py:52] epoch 7284, training loss: 5320.76, average training loss: 5548.59, base loss: 15967.03
[INFO 2017-06-30 07:34:17,472 main.py:52] epoch 7285, training loss: 6075.15, average training loss: 5549.33, base loss: 15967.18
[INFO 2017-06-30 07:34:20,673 main.py:52] epoch 7286, training loss: 5305.23, average training loss: 5549.23, base loss: 15967.03
[INFO 2017-06-30 07:34:23,794 main.py:52] epoch 7287, training loss: 5813.30, average training loss: 5549.29, base loss: 15967.24
[INFO 2017-06-30 07:34:26,973 main.py:52] epoch 7288, training loss: 5586.67, average training loss: 5549.76, base loss: 15967.05
[INFO 2017-06-30 07:34:30,138 main.py:52] epoch 7289, training loss: 5352.59, average training loss: 5549.60, base loss: 15966.88
[INFO 2017-06-30 07:34:33,271 main.py:52] epoch 7290, training loss: 5525.83, average training loss: 5549.63, base loss: 15966.73
[INFO 2017-06-30 07:34:36,432 main.py:52] epoch 7291, training loss: 5556.04, average training loss: 5549.90, base loss: 15966.38
[INFO 2017-06-30 07:34:39,588 main.py:52] epoch 7292, training loss: 5257.88, average training loss: 5549.85, base loss: 15966.20
[INFO 2017-06-30 07:34:42,711 main.py:52] epoch 7293, training loss: 5570.59, average training loss: 5549.98, base loss: 15966.23
[INFO 2017-06-30 07:34:45,860 main.py:52] epoch 7294, training loss: 5343.73, average training loss: 5549.47, base loss: 15966.10
[INFO 2017-06-30 07:34:49,020 main.py:52] epoch 7295, training loss: 5486.67, average training loss: 5549.81, base loss: 15966.19
[INFO 2017-06-30 07:34:52,143 main.py:52] epoch 7296, training loss: 5760.12, average training loss: 5550.01, base loss: 15966.74
[INFO 2017-06-30 07:34:55,331 main.py:52] epoch 7297, training loss: 5503.92, average training loss: 5549.87, base loss: 15967.00
[INFO 2017-06-30 07:34:58,481 main.py:52] epoch 7298, training loss: 5363.72, average training loss: 5549.56, base loss: 15967.11
[INFO 2017-06-30 07:35:01,658 main.py:52] epoch 7299, training loss: 5317.98, average training loss: 5549.20, base loss: 15966.83
[INFO 2017-06-30 07:35:01,658 main.py:54] epoch 7299, testing
[INFO 2017-06-30 07:35:14,718 main.py:97] average testing loss: 5466.65, base loss: 15744.84
[INFO 2017-06-30 07:35:14,719 main.py:98] improve_loss: 10278.19, improve_percent: 0.65
[INFO 2017-06-30 07:35:14,720 main.py:66] current best improved percent: 0.66
[INFO 2017-06-30 07:35:17,855 main.py:52] epoch 7300, training loss: 6275.18, average training loss: 5550.00, base loss: 15967.63
[INFO 2017-06-30 07:35:21,007 main.py:52] epoch 7301, training loss: 5430.88, average training loss: 5549.89, base loss: 15967.80
[INFO 2017-06-30 07:35:24,175 main.py:52] epoch 7302, training loss: 5574.02, average training loss: 5550.28, base loss: 15967.76
[INFO 2017-06-30 07:35:27,345 main.py:52] epoch 7303, training loss: 5669.04, average training loss: 5550.14, base loss: 15967.94
[INFO 2017-06-30 07:35:30,475 main.py:52] epoch 7304, training loss: 5994.62, average training loss: 5550.59, base loss: 15968.45
[INFO 2017-06-30 07:35:33,635 main.py:52] epoch 7305, training loss: 5664.23, average training loss: 5550.94, base loss: 15968.69
[INFO 2017-06-30 07:35:36,737 main.py:52] epoch 7306, training loss: 5660.92, average training loss: 5550.97, base loss: 15968.99
[INFO 2017-06-30 07:35:39,884 main.py:52] epoch 7307, training loss: 5898.25, average training loss: 5551.29, base loss: 15969.51
[INFO 2017-06-30 07:35:43,110 main.py:52] epoch 7308, training loss: 5590.83, average training loss: 5551.25, base loss: 15969.83
[INFO 2017-06-30 07:35:46,289 main.py:52] epoch 7309, training loss: 4962.52, average training loss: 5550.39, base loss: 15969.54
[INFO 2017-06-30 07:35:49,453 main.py:52] epoch 7310, training loss: 5158.66, average training loss: 5549.86, base loss: 15969.07
[INFO 2017-06-30 07:35:52,553 main.py:52] epoch 7311, training loss: 5107.33, average training loss: 5549.66, base loss: 15968.62
[INFO 2017-06-30 07:35:55,656 main.py:52] epoch 7312, training loss: 5499.74, average training loss: 5549.53, base loss: 15968.80
[INFO 2017-06-30 07:35:58,841 main.py:52] epoch 7313, training loss: 5514.83, average training loss: 5549.15, base loss: 15968.98
[INFO 2017-06-30 07:36:01,989 main.py:52] epoch 7314, training loss: 5054.86, average training loss: 5548.53, base loss: 15968.42
[INFO 2017-06-30 07:36:05,160 main.py:52] epoch 7315, training loss: 5200.42, average training loss: 5548.15, base loss: 15967.94
[INFO 2017-06-30 07:36:08,291 main.py:52] epoch 7316, training loss: 5422.11, average training loss: 5548.23, base loss: 15967.57
[INFO 2017-06-30 07:36:11,435 main.py:52] epoch 7317, training loss: 5322.26, average training loss: 5547.82, base loss: 15967.41
[INFO 2017-06-30 07:36:14,548 main.py:52] epoch 7318, training loss: 5279.86, average training loss: 5547.81, base loss: 15967.51
[INFO 2017-06-30 07:36:17,692 main.py:52] epoch 7319, training loss: 5700.67, average training loss: 5547.45, base loss: 15968.14
[INFO 2017-06-30 07:36:20,812 main.py:52] epoch 7320, training loss: 5506.01, average training loss: 5547.08, base loss: 15968.20
[INFO 2017-06-30 07:36:23,963 main.py:52] epoch 7321, training loss: 5406.49, average training loss: 5547.03, base loss: 15967.63
[INFO 2017-06-30 07:36:27,109 main.py:52] epoch 7322, training loss: 5235.77, average training loss: 5546.75, base loss: 15967.09
[INFO 2017-06-30 07:36:30,253 main.py:52] epoch 7323, training loss: 5255.89, average training loss: 5546.53, base loss: 15966.46
[INFO 2017-06-30 07:36:33,381 main.py:52] epoch 7324, training loss: 5600.58, average training loss: 5546.73, base loss: 15966.43
[INFO 2017-06-30 07:36:36,507 main.py:52] epoch 7325, training loss: 5379.86, average training loss: 5546.58, base loss: 15966.62
[INFO 2017-06-30 07:36:39,662 main.py:52] epoch 7326, training loss: 5814.08, average training loss: 5547.10, base loss: 15966.42
[INFO 2017-06-30 07:36:42,806 main.py:52] epoch 7327, training loss: 5495.76, average training loss: 5547.39, base loss: 15966.65
[INFO 2017-06-30 07:36:45,931 main.py:52] epoch 7328, training loss: 5786.20, average training loss: 5547.90, base loss: 15966.83
[INFO 2017-06-30 07:36:49,089 main.py:52] epoch 7329, training loss: 5331.32, average training loss: 5547.63, base loss: 15966.56
[INFO 2017-06-30 07:36:52,171 main.py:52] epoch 7330, training loss: 5320.49, average training loss: 5547.36, base loss: 15966.69
[INFO 2017-06-30 07:36:55,291 main.py:52] epoch 7331, training loss: 5743.43, average training loss: 5547.37, base loss: 15967.05
[INFO 2017-06-30 07:36:58,421 main.py:52] epoch 7332, training loss: 5814.79, average training loss: 5547.10, base loss: 15967.65
[INFO 2017-06-30 07:37:01,571 main.py:52] epoch 7333, training loss: 5109.20, average training loss: 5546.59, base loss: 15967.30
[INFO 2017-06-30 07:37:04,717 main.py:52] epoch 7334, training loss: 5242.57, average training loss: 5546.16, base loss: 15966.61
[INFO 2017-06-30 07:37:07,839 main.py:52] epoch 7335, training loss: 5660.58, average training loss: 5546.33, base loss: 15966.82
[INFO 2017-06-30 07:37:10,984 main.py:52] epoch 7336, training loss: 5461.38, average training loss: 5546.33, base loss: 15967.04
[INFO 2017-06-30 07:37:14,108 main.py:52] epoch 7337, training loss: 5428.94, average training loss: 5546.21, base loss: 15967.49
[INFO 2017-06-30 07:37:17,260 main.py:52] epoch 7338, training loss: 5374.69, average training loss: 5545.91, base loss: 15967.42
[INFO 2017-06-30 07:37:20,383 main.py:52] epoch 7339, training loss: 5196.41, average training loss: 5545.81, base loss: 15967.27
[INFO 2017-06-30 07:37:23,544 main.py:52] epoch 7340, training loss: 5670.40, average training loss: 5545.96, base loss: 15967.54
[INFO 2017-06-30 07:37:26,654 main.py:52] epoch 7341, training loss: 5495.24, average training loss: 5545.95, base loss: 15967.73
[INFO 2017-06-30 07:37:29,793 main.py:52] epoch 7342, training loss: 5670.55, average training loss: 5546.20, base loss: 15968.10
[INFO 2017-06-30 07:37:32,951 main.py:52] epoch 7343, training loss: 5800.95, average training loss: 5546.53, base loss: 15967.85
[INFO 2017-06-30 07:37:36,043 main.py:52] epoch 7344, training loss: 5159.20, average training loss: 5546.04, base loss: 15967.73
[INFO 2017-06-30 07:37:39,241 main.py:52] epoch 7345, training loss: 5439.15, average training loss: 5546.10, base loss: 15967.70
[INFO 2017-06-30 07:37:42,408 main.py:52] epoch 7346, training loss: 5978.92, average training loss: 5546.40, base loss: 15967.72
[INFO 2017-06-30 07:37:45,547 main.py:52] epoch 7347, training loss: 5838.58, average training loss: 5546.81, base loss: 15967.62
[INFO 2017-06-30 07:37:48,680 main.py:52] epoch 7348, training loss: 5418.32, average training loss: 5546.66, base loss: 15967.85
[INFO 2017-06-30 07:37:51,866 main.py:52] epoch 7349, training loss: 5315.37, average training loss: 5546.39, base loss: 15967.72
[INFO 2017-06-30 07:37:55,002 main.py:52] epoch 7350, training loss: 5615.28, average training loss: 5546.40, base loss: 15968.11
[INFO 2017-06-30 07:37:58,171 main.py:52] epoch 7351, training loss: 5413.83, average training loss: 5546.02, base loss: 15968.13
[INFO 2017-06-30 07:38:01,317 main.py:52] epoch 7352, training loss: 5515.68, average training loss: 5546.12, base loss: 15967.65
[INFO 2017-06-30 07:38:04,431 main.py:52] epoch 7353, training loss: 5468.33, average training loss: 5545.93, base loss: 15967.42
[INFO 2017-06-30 07:38:07,553 main.py:52] epoch 7354, training loss: 5521.41, average training loss: 5545.38, base loss: 15967.06
[INFO 2017-06-30 07:38:10,662 main.py:52] epoch 7355, training loss: 5231.02, average training loss: 5545.02, base loss: 15966.35
[INFO 2017-06-30 07:38:13,806 main.py:52] epoch 7356, training loss: 5712.60, average training loss: 5545.17, base loss: 15966.89
[INFO 2017-06-30 07:38:16,929 main.py:52] epoch 7357, training loss: 5259.47, average training loss: 5544.90, base loss: 15966.71
[INFO 2017-06-30 07:38:20,074 main.py:52] epoch 7358, training loss: 5464.94, average training loss: 5544.49, base loss: 15966.83
[INFO 2017-06-30 07:38:23,220 main.py:52] epoch 7359, training loss: 5327.74, average training loss: 5544.11, base loss: 15967.20
[INFO 2017-06-30 07:38:26,341 main.py:52] epoch 7360, training loss: 5542.42, average training loss: 5544.25, base loss: 15967.30
[INFO 2017-06-30 07:38:29,490 main.py:52] epoch 7361, training loss: 5356.97, average training loss: 5543.87, base loss: 15967.34
[INFO 2017-06-30 07:38:32,659 main.py:52] epoch 7362, training loss: 5879.21, average training loss: 5544.12, base loss: 15967.83
[INFO 2017-06-30 07:38:35,829 main.py:52] epoch 7363, training loss: 5429.85, average training loss: 5544.27, base loss: 15967.18
[INFO 2017-06-30 07:38:38,952 main.py:52] epoch 7364, training loss: 5417.47, average training loss: 5544.15, base loss: 15966.78
[INFO 2017-06-30 07:38:42,061 main.py:52] epoch 7365, training loss: 5205.00, average training loss: 5543.78, base loss: 15966.23
[INFO 2017-06-30 07:38:45,255 main.py:52] epoch 7366, training loss: 5331.52, average training loss: 5543.70, base loss: 15966.23
[INFO 2017-06-30 07:38:48,444 main.py:52] epoch 7367, training loss: 6210.77, average training loss: 5544.49, base loss: 15967.05
[INFO 2017-06-30 07:38:51,596 main.py:52] epoch 7368, training loss: 5238.95, average training loss: 5543.74, base loss: 15967.27
[INFO 2017-06-30 07:38:54,723 main.py:52] epoch 7369, training loss: 5538.86, average training loss: 5543.31, base loss: 15967.56
[INFO 2017-06-30 07:38:57,892 main.py:52] epoch 7370, training loss: 5796.47, average training loss: 5543.57, base loss: 15967.40
[INFO 2017-06-30 07:39:01,118 main.py:52] epoch 7371, training loss: 5848.34, average training loss: 5543.73, base loss: 15967.55
[INFO 2017-06-30 07:39:04,290 main.py:52] epoch 7372, training loss: 5331.84, average training loss: 5543.48, base loss: 15967.00
[INFO 2017-06-30 07:39:07,395 main.py:52] epoch 7373, training loss: 5473.91, average training loss: 5543.25, base loss: 15966.78
[INFO 2017-06-30 07:39:10,541 main.py:52] epoch 7374, training loss: 5524.76, average training loss: 5543.17, base loss: 15966.54
[INFO 2017-06-30 07:39:13,714 main.py:52] epoch 7375, training loss: 5274.39, average training loss: 5542.77, base loss: 15966.20
[INFO 2017-06-30 07:39:16,869 main.py:52] epoch 7376, training loss: 5687.84, average training loss: 5543.13, base loss: 15966.34
[INFO 2017-06-30 07:39:20,016 main.py:52] epoch 7377, training loss: 5464.12, average training loss: 5542.70, base loss: 15966.30
[INFO 2017-06-30 07:39:23,147 main.py:52] epoch 7378, training loss: 5499.16, average training loss: 5542.76, base loss: 15966.04
[INFO 2017-06-30 07:39:26,272 main.py:52] epoch 7379, training loss: 5240.70, average training loss: 5542.70, base loss: 15965.73
[INFO 2017-06-30 07:39:29,406 main.py:52] epoch 7380, training loss: 5664.10, average training loss: 5542.76, base loss: 15965.92
[INFO 2017-06-30 07:39:32,573 main.py:52] epoch 7381, training loss: 5375.09, average training loss: 5542.41, base loss: 15965.87
[INFO 2017-06-30 07:39:35,710 main.py:52] epoch 7382, training loss: 5714.03, average training loss: 5542.42, base loss: 15965.54
[INFO 2017-06-30 07:39:38,835 main.py:52] epoch 7383, training loss: 5757.92, average training loss: 5542.44, base loss: 15965.62
[INFO 2017-06-30 07:39:42,046 main.py:52] epoch 7384, training loss: 5719.01, average training loss: 5542.83, base loss: 15965.89
[INFO 2017-06-30 07:39:45,148 main.py:52] epoch 7385, training loss: 5436.37, average training loss: 5542.60, base loss: 15966.10
[INFO 2017-06-30 07:39:48,247 main.py:52] epoch 7386, training loss: 5761.74, average training loss: 5542.80, base loss: 15966.03
[INFO 2017-06-30 07:39:51,354 main.py:52] epoch 7387, training loss: 5751.34, average training loss: 5543.23, base loss: 15966.47
[INFO 2017-06-30 07:39:54,507 main.py:52] epoch 7388, training loss: 5314.13, average training loss: 5542.85, base loss: 15966.33
[INFO 2017-06-30 07:39:57,630 main.py:52] epoch 7389, training loss: 5187.43, average training loss: 5542.62, base loss: 15966.18
[INFO 2017-06-30 07:40:00,793 main.py:52] epoch 7390, training loss: 5689.25, average training loss: 5543.18, base loss: 15966.12
[INFO 2017-06-30 07:40:03,946 main.py:52] epoch 7391, training loss: 5326.09, average training loss: 5542.91, base loss: 15965.95
[INFO 2017-06-30 07:40:07,105 main.py:52] epoch 7392, training loss: 5431.78, average training loss: 5543.20, base loss: 15966.08
[INFO 2017-06-30 07:40:10,241 main.py:52] epoch 7393, training loss: 5224.48, average training loss: 5542.88, base loss: 15965.62
[INFO 2017-06-30 07:40:13,340 main.py:52] epoch 7394, training loss: 5765.87, average training loss: 5542.95, base loss: 15965.52
[INFO 2017-06-30 07:40:16,500 main.py:52] epoch 7395, training loss: 5415.58, average training loss: 5542.89, base loss: 15965.58
[INFO 2017-06-30 07:40:19,680 main.py:52] epoch 7396, training loss: 5778.78, average training loss: 5543.00, base loss: 15966.01
[INFO 2017-06-30 07:40:22,833 main.py:52] epoch 7397, training loss: 5171.60, average training loss: 5542.58, base loss: 15965.87
[INFO 2017-06-30 07:40:26,040 main.py:52] epoch 7398, training loss: 5329.51, average training loss: 5542.43, base loss: 15965.83
[INFO 2017-06-30 07:40:29,212 main.py:52] epoch 7399, training loss: 5717.02, average training loss: 5543.07, base loss: 15965.88
[INFO 2017-06-30 07:40:29,212 main.py:54] epoch 7399, testing
[INFO 2017-06-30 07:40:42,242 main.py:97] average testing loss: 5656.66, base loss: 16625.74
[INFO 2017-06-30 07:40:42,242 main.py:98] improve_loss: 10969.08, improve_percent: 0.66
[INFO 2017-06-30 07:40:42,243 main.py:66] current best improved percent: 0.66
[INFO 2017-06-30 07:40:45,329 main.py:52] epoch 7400, training loss: 5518.09, average training loss: 5543.02, base loss: 15965.71
[INFO 2017-06-30 07:40:48,511 main.py:52] epoch 7401, training loss: 5385.36, average training loss: 5542.77, base loss: 15965.55
[INFO 2017-06-30 07:40:51,632 main.py:52] epoch 7402, training loss: 5681.45, average training loss: 5542.28, base loss: 15965.88
[INFO 2017-06-30 07:40:54,750 main.py:52] epoch 7403, training loss: 5390.06, average training loss: 5541.79, base loss: 15965.76
[INFO 2017-06-30 07:40:57,903 main.py:52] epoch 7404, training loss: 5349.25, average training loss: 5541.52, base loss: 15965.76
[INFO 2017-06-30 07:41:01,019 main.py:52] epoch 7405, training loss: 5221.53, average training loss: 5540.99, base loss: 15965.61
[INFO 2017-06-30 07:41:04,155 main.py:52] epoch 7406, training loss: 5267.87, average training loss: 5540.23, base loss: 15965.59
[INFO 2017-06-30 07:41:07,289 main.py:52] epoch 7407, training loss: 5438.61, average training loss: 5539.98, base loss: 15965.85
[INFO 2017-06-30 07:41:10,424 main.py:52] epoch 7408, training loss: 5488.11, average training loss: 5539.48, base loss: 15965.58
[INFO 2017-06-30 07:41:13,572 main.py:52] epoch 7409, training loss: 5679.07, average training loss: 5539.70, base loss: 15965.90
[INFO 2017-06-30 07:41:16,697 main.py:52] epoch 7410, training loss: 5572.71, average training loss: 5539.77, base loss: 15965.83
[INFO 2017-06-30 07:41:19,823 main.py:52] epoch 7411, training loss: 5872.82, average training loss: 5540.16, base loss: 15966.47
[INFO 2017-06-30 07:41:22,957 main.py:52] epoch 7412, training loss: 5637.10, average training loss: 5540.28, base loss: 15966.42
[INFO 2017-06-30 07:41:26,109 main.py:52] epoch 7413, training loss: 6050.63, average training loss: 5540.79, base loss: 15966.73
[INFO 2017-06-30 07:41:29,263 main.py:52] epoch 7414, training loss: 5310.45, average training loss: 5539.99, base loss: 15966.95
[INFO 2017-06-30 07:41:32,437 main.py:52] epoch 7415, training loss: 5726.76, average training loss: 5540.17, base loss: 15967.60
[INFO 2017-06-30 07:41:35,621 main.py:52] epoch 7416, training loss: 5685.45, average training loss: 5540.10, base loss: 15967.65
[INFO 2017-06-30 07:41:38,786 main.py:52] epoch 7417, training loss: 5801.01, average training loss: 5540.41, base loss: 15967.34
[INFO 2017-06-30 07:41:41,937 main.py:52] epoch 7418, training loss: 5894.79, average training loss: 5540.73, base loss: 15967.27
[INFO 2017-06-30 07:41:45,107 main.py:52] epoch 7419, training loss: 5476.95, average training loss: 5540.52, base loss: 15967.01
[INFO 2017-06-30 07:41:48,259 main.py:52] epoch 7420, training loss: 5733.97, average training loss: 5540.80, base loss: 15967.19
[INFO 2017-06-30 07:41:51,383 main.py:52] epoch 7421, training loss: 5733.19, average training loss: 5541.28, base loss: 15967.71
[INFO 2017-06-30 07:41:54,553 main.py:52] epoch 7422, training loss: 5316.09, average training loss: 5541.02, base loss: 15967.80
[INFO 2017-06-30 07:41:57,691 main.py:52] epoch 7423, training loss: 5798.16, average training loss: 5541.40, base loss: 15967.72
[INFO 2017-06-30 07:42:00,852 main.py:52] epoch 7424, training loss: 5448.16, average training loss: 5541.39, base loss: 15967.62
[INFO 2017-06-30 07:42:04,025 main.py:52] epoch 7425, training loss: 5341.33, average training loss: 5540.92, base loss: 15967.68
[INFO 2017-06-30 07:42:07,168 main.py:52] epoch 7426, training loss: 5302.03, average training loss: 5540.68, base loss: 15967.22
[INFO 2017-06-30 07:42:10,311 main.py:52] epoch 7427, training loss: 5094.57, average training loss: 5540.40, base loss: 15966.98
[INFO 2017-06-30 07:42:13,443 main.py:52] epoch 7428, training loss: 5092.43, average training loss: 5539.46, base loss: 15967.08
[INFO 2017-06-30 07:42:16,576 main.py:52] epoch 7429, training loss: 5258.10, average training loss: 5539.35, base loss: 15967.29
[INFO 2017-06-30 07:42:19,722 main.py:52] epoch 7430, training loss: 5890.99, average training loss: 5539.88, base loss: 15967.44
[INFO 2017-06-30 07:42:22,890 main.py:52] epoch 7431, training loss: 5617.99, average training loss: 5540.24, base loss: 15967.40
[INFO 2017-06-30 07:42:26,000 main.py:52] epoch 7432, training loss: 5590.90, average training loss: 5540.21, base loss: 15966.49
[INFO 2017-06-30 07:42:29,207 main.py:52] epoch 7433, training loss: 5269.71, average training loss: 5540.19, base loss: 15966.28
[INFO 2017-06-30 07:42:32,358 main.py:52] epoch 7434, training loss: 5695.41, average training loss: 5540.11, base loss: 15966.50
[INFO 2017-06-30 07:42:35,510 main.py:52] epoch 7435, training loss: 5591.75, average training loss: 5540.07, base loss: 15966.74
[INFO 2017-06-30 07:42:38,662 main.py:52] epoch 7436, training loss: 5565.70, average training loss: 5539.80, base loss: 15967.38
[INFO 2017-06-30 07:42:41,816 main.py:52] epoch 7437, training loss: 5416.26, average training loss: 5539.57, base loss: 15967.34
[INFO 2017-06-30 07:42:44,948 main.py:52] epoch 7438, training loss: 5240.81, average training loss: 5539.23, base loss: 15967.79
[INFO 2017-06-30 07:42:48,075 main.py:52] epoch 7439, training loss: 5118.55, average training loss: 5538.79, base loss: 15967.55
[INFO 2017-06-30 07:42:51,242 main.py:52] epoch 7440, training loss: 5398.66, average training loss: 5538.47, base loss: 15967.41
[INFO 2017-06-30 07:42:54,382 main.py:52] epoch 7441, training loss: 5543.61, average training loss: 5538.50, base loss: 15967.35
[INFO 2017-06-30 07:42:57,497 main.py:52] epoch 7442, training loss: 5651.32, average training loss: 5538.79, base loss: 15967.70
[INFO 2017-06-30 07:43:00,644 main.py:52] epoch 7443, training loss: 5704.85, average training loss: 5539.15, base loss: 15967.79
[INFO 2017-06-30 07:43:03,778 main.py:52] epoch 7444, training loss: 5273.85, average training loss: 5538.88, base loss: 15967.87
[INFO 2017-06-30 07:43:06,924 main.py:52] epoch 7445, training loss: 5190.51, average training loss: 5538.61, base loss: 15967.96
[INFO 2017-06-30 07:43:10,055 main.py:52] epoch 7446, training loss: 5607.27, average training loss: 5538.97, base loss: 15967.94
[INFO 2017-06-30 07:43:13,219 main.py:52] epoch 7447, training loss: 5262.93, average training loss: 5538.65, base loss: 15967.96
[INFO 2017-06-30 07:43:16,365 main.py:52] epoch 7448, training loss: 5191.22, average training loss: 5538.10, base loss: 15967.39
[INFO 2017-06-30 07:43:19,490 main.py:52] epoch 7449, training loss: 5526.95, average training loss: 5538.20, base loss: 15967.25
[INFO 2017-06-30 07:43:22,662 main.py:52] epoch 7450, training loss: 5413.95, average training loss: 5537.91, base loss: 15967.30
[INFO 2017-06-30 07:43:25,779 main.py:52] epoch 7451, training loss: 5662.24, average training loss: 5538.15, base loss: 15967.60
[INFO 2017-06-30 07:43:28,917 main.py:52] epoch 7452, training loss: 5621.94, average training loss: 5538.49, base loss: 15967.56
[INFO 2017-06-30 07:43:32,057 main.py:52] epoch 7453, training loss: 5586.88, average training loss: 5538.82, base loss: 15967.46
[INFO 2017-06-30 07:43:35,189 main.py:52] epoch 7454, training loss: 5688.44, average training loss: 5539.20, base loss: 15967.67
[INFO 2017-06-30 07:43:38,309 main.py:52] epoch 7455, training loss: 5496.09, average training loss: 5539.21, base loss: 15967.95
[INFO 2017-06-30 07:43:41,465 main.py:52] epoch 7456, training loss: 5350.40, average training loss: 5539.40, base loss: 15968.03
[INFO 2017-06-30 07:43:44,616 main.py:52] epoch 7457, training loss: 5240.09, average training loss: 5539.15, base loss: 15967.87
[INFO 2017-06-30 07:43:47,738 main.py:52] epoch 7458, training loss: 5411.31, average training loss: 5538.59, base loss: 15967.65
[INFO 2017-06-30 07:43:50,896 main.py:52] epoch 7459, training loss: 5323.53, average training loss: 5538.54, base loss: 15967.23
[INFO 2017-06-30 07:43:54,018 main.py:52] epoch 7460, training loss: 5409.43, average training loss: 5538.59, base loss: 15966.96
[INFO 2017-06-30 07:43:57,156 main.py:52] epoch 7461, training loss: 5097.23, average training loss: 5538.12, base loss: 15966.53
[INFO 2017-06-30 07:44:00,275 main.py:52] epoch 7462, training loss: 5373.57, average training loss: 5538.20, base loss: 15966.15
[INFO 2017-06-30 07:44:03,445 main.py:52] epoch 7463, training loss: 5525.95, average training loss: 5537.96, base loss: 15965.74
[INFO 2017-06-30 07:44:06,611 main.py:52] epoch 7464, training loss: 5664.28, average training loss: 5537.89, base loss: 15965.54
[INFO 2017-06-30 07:44:09,760 main.py:52] epoch 7465, training loss: 5493.95, average training loss: 5537.88, base loss: 15965.28
[INFO 2017-06-30 07:44:12,882 main.py:52] epoch 7466, training loss: 5436.17, average training loss: 5537.93, base loss: 15965.69
[INFO 2017-06-30 07:44:16,108 main.py:52] epoch 7467, training loss: 5434.29, average training loss: 5538.02, base loss: 15965.78
[INFO 2017-06-30 07:44:19,239 main.py:52] epoch 7468, training loss: 5495.07, average training loss: 5537.77, base loss: 15965.58
[INFO 2017-06-30 07:44:22,394 main.py:52] epoch 7469, training loss: 5672.89, average training loss: 5538.10, base loss: 15965.68
[INFO 2017-06-30 07:44:25,517 main.py:52] epoch 7470, training loss: 5639.39, average training loss: 5538.13, base loss: 15966.01
[INFO 2017-06-30 07:44:28,623 main.py:52] epoch 7471, training loss: 5116.22, average training loss: 5537.17, base loss: 15965.99
[INFO 2017-06-30 07:44:31,774 main.py:52] epoch 7472, training loss: 5377.78, average training loss: 5536.85, base loss: 15965.81
[INFO 2017-06-30 07:44:34,922 main.py:52] epoch 7473, training loss: 5426.07, average training loss: 5536.98, base loss: 15966.00
[INFO 2017-06-30 07:44:38,092 main.py:52] epoch 7474, training loss: 5295.70, average training loss: 5536.82, base loss: 15965.67
[INFO 2017-06-30 07:44:41,228 main.py:52] epoch 7475, training loss: 5728.52, average training loss: 5537.11, base loss: 15965.67
[INFO 2017-06-30 07:44:44,391 main.py:52] epoch 7476, training loss: 5476.93, average training loss: 5536.75, base loss: 15965.60
[INFO 2017-06-30 07:44:47,599 main.py:52] epoch 7477, training loss: 5787.72, average training loss: 5536.95, base loss: 15966.15
[INFO 2017-06-30 07:44:50,738 main.py:52] epoch 7478, training loss: 5179.32, average training loss: 5536.71, base loss: 15966.13
[INFO 2017-06-30 07:44:53,907 main.py:52] epoch 7479, training loss: 5446.89, average training loss: 5536.38, base loss: 15966.47
[INFO 2017-06-30 07:44:57,008 main.py:52] epoch 7480, training loss: 5586.04, average training loss: 5536.07, base loss: 15966.84
[INFO 2017-06-30 07:45:00,155 main.py:52] epoch 7481, training loss: 5654.98, average training loss: 5536.20, base loss: 15967.17
[INFO 2017-06-30 07:45:03,290 main.py:52] epoch 7482, training loss: 5452.46, average training loss: 5536.20, base loss: 15966.99
[INFO 2017-06-30 07:45:06,495 main.py:52] epoch 7483, training loss: 5669.26, average training loss: 5536.35, base loss: 15966.97
[INFO 2017-06-30 07:45:09,604 main.py:52] epoch 7484, training loss: 6397.44, average training loss: 5536.87, base loss: 15967.83
[INFO 2017-06-30 07:45:12,729 main.py:52] epoch 7485, training loss: 5666.94, average training loss: 5536.87, base loss: 15968.13
[INFO 2017-06-30 07:45:15,840 main.py:52] epoch 7486, training loss: 5478.97, average training loss: 5536.53, base loss: 15968.26
[INFO 2017-06-30 07:45:18,980 main.py:52] epoch 7487, training loss: 5508.67, average training loss: 5536.43, base loss: 15968.21
[INFO 2017-06-30 07:45:22,145 main.py:52] epoch 7488, training loss: 5195.95, average training loss: 5536.18, base loss: 15968.27
[INFO 2017-06-30 07:45:25,300 main.py:52] epoch 7489, training loss: 5381.47, average training loss: 5535.55, base loss: 15968.54
[INFO 2017-06-30 07:45:28,397 main.py:52] epoch 7490, training loss: 5332.58, average training loss: 5535.30, base loss: 15968.44
[INFO 2017-06-30 07:45:31,556 main.py:52] epoch 7491, training loss: 5435.13, average training loss: 5534.81, base loss: 15968.12
[INFO 2017-06-30 07:45:34,689 main.py:52] epoch 7492, training loss: 5706.67, average training loss: 5534.66, base loss: 15968.24
[INFO 2017-06-30 07:45:37,839 main.py:52] epoch 7493, training loss: 5644.31, average training loss: 5534.89, base loss: 15968.47
[INFO 2017-06-30 07:45:40,967 main.py:52] epoch 7494, training loss: 5583.54, average training loss: 5535.26, base loss: 15968.99
[INFO 2017-06-30 07:45:44,135 main.py:52] epoch 7495, training loss: 5487.91, average training loss: 5535.06, base loss: 15969.44
[INFO 2017-06-30 07:45:47,289 main.py:52] epoch 7496, training loss: 5716.50, average training loss: 5534.61, base loss: 15969.47
[INFO 2017-06-30 07:45:50,372 main.py:52] epoch 7497, training loss: 5192.45, average training loss: 5534.10, base loss: 15969.37
[INFO 2017-06-30 07:45:53,553 main.py:52] epoch 7498, training loss: 5138.79, average training loss: 5533.68, base loss: 15969.23
[INFO 2017-06-30 07:45:56,726 main.py:52] epoch 7499, training loss: 5638.46, average training loss: 5533.49, base loss: 15969.06
[INFO 2017-06-30 07:45:56,727 main.py:54] epoch 7499, testing
[INFO 2017-06-30 07:46:09,873 main.py:97] average testing loss: 5435.24, base loss: 15630.09
[INFO 2017-06-30 07:46:09,873 main.py:98] improve_loss: 10194.86, improve_percent: 0.65
[INFO 2017-06-30 07:46:09,876 main.py:66] current best improved percent: 0.66
[INFO 2017-06-30 07:46:12,995 main.py:52] epoch 7500, training loss: 5405.06, average training loss: 5533.20, base loss: 15969.01
[INFO 2017-06-30 07:46:16,115 main.py:52] epoch 7501, training loss: 5787.11, average training loss: 5533.23, base loss: 15969.73
[INFO 2017-06-30 07:46:19,225 main.py:52] epoch 7502, training loss: 5778.32, average training loss: 5532.98, base loss: 15970.06
[INFO 2017-06-30 07:46:22,355 main.py:52] epoch 7503, training loss: 5743.30, average training loss: 5532.97, base loss: 15970.75
[INFO 2017-06-30 07:46:25,458 main.py:52] epoch 7504, training loss: 5458.27, average training loss: 5532.68, base loss: 15970.43
[INFO 2017-06-30 07:46:28,589 main.py:52] epoch 7505, training loss: 5450.15, average training loss: 5532.01, base loss: 15970.30
[INFO 2017-06-30 07:46:31,769 main.py:52] epoch 7506, training loss: 6080.41, average training loss: 5532.67, base loss: 15970.96
[INFO 2017-06-30 07:46:34,910 main.py:52] epoch 7507, training loss: 5574.16, average training loss: 5533.04, base loss: 15971.02
[INFO 2017-06-30 07:46:38,058 main.py:52] epoch 7508, training loss: 5338.22, average training loss: 5532.91, base loss: 15970.71
[INFO 2017-06-30 07:46:41,163 main.py:52] epoch 7509, training loss: 5228.29, average training loss: 5532.64, base loss: 15970.60
[INFO 2017-06-30 07:46:44,324 main.py:52] epoch 7510, training loss: 5430.99, average training loss: 5532.40, base loss: 15970.92
[INFO 2017-06-30 07:46:47,500 main.py:52] epoch 7511, training loss: 5487.74, average training loss: 5532.05, base loss: 15971.13
[INFO 2017-06-30 07:46:50,639 main.py:52] epoch 7512, training loss: 5580.09, average training loss: 5532.31, base loss: 15971.19
[INFO 2017-06-30 07:46:53,788 main.py:52] epoch 7513, training loss: 5633.37, average training loss: 5532.09, base loss: 15971.29
[INFO 2017-06-30 07:46:56,932 main.py:52] epoch 7514, training loss: 5640.01, average training loss: 5532.46, base loss: 15971.31
[INFO 2017-06-30 07:47:00,063 main.py:52] epoch 7515, training loss: 5831.97, average training loss: 5532.63, base loss: 15971.20
[INFO 2017-06-30 07:47:03,248 main.py:52] epoch 7516, training loss: 5539.58, average training loss: 5532.59, base loss: 15971.01
[INFO 2017-06-30 07:47:06,349 main.py:52] epoch 7517, training loss: 5291.80, average training loss: 5531.89, base loss: 15970.83
[INFO 2017-06-30 07:47:09,478 main.py:52] epoch 7518, training loss: 5354.34, average training loss: 5531.58, base loss: 15970.96
[INFO 2017-06-30 07:47:12,609 main.py:52] epoch 7519, training loss: 5525.03, average training loss: 5531.83, base loss: 15971.20
[INFO 2017-06-30 07:47:15,761 main.py:52] epoch 7520, training loss: 5634.56, average training loss: 5532.03, base loss: 15971.08
[INFO 2017-06-30 07:47:18,898 main.py:52] epoch 7521, training loss: 5319.24, average training loss: 5531.76, base loss: 15970.94
[INFO 2017-06-30 07:47:21,991 main.py:52] epoch 7522, training loss: 5459.51, average training loss: 5531.58, base loss: 15971.23
[INFO 2017-06-30 07:47:25,141 main.py:52] epoch 7523, training loss: 5667.70, average training loss: 5531.78, base loss: 15971.75
[INFO 2017-06-30 07:47:28,284 main.py:52] epoch 7524, training loss: 5172.54, average training loss: 5531.54, base loss: 15971.56
[INFO 2017-06-30 07:47:31,419 main.py:52] epoch 7525, training loss: 5607.34, average training loss: 5531.83, base loss: 15971.85
[INFO 2017-06-30 07:47:34,555 main.py:52] epoch 7526, training loss: 5373.34, average training loss: 5532.10, base loss: 15971.53
[INFO 2017-06-30 07:47:37,669 main.py:52] epoch 7527, training loss: 5445.12, average training loss: 5532.07, base loss: 15971.35
[INFO 2017-06-30 07:47:40,851 main.py:52] epoch 7528, training loss: 5157.79, average training loss: 5532.26, base loss: 15970.90
[INFO 2017-06-30 07:47:43,981 main.py:52] epoch 7529, training loss: 5347.36, average training loss: 5532.24, base loss: 15970.57
[INFO 2017-06-30 07:47:47,124 main.py:52] epoch 7530, training loss: 5869.37, average training loss: 5532.97, base loss: 15970.67
[INFO 2017-06-30 07:47:50,263 main.py:52] epoch 7531, training loss: 5353.45, average training loss: 5532.81, base loss: 15970.47
[INFO 2017-06-30 07:47:53,422 main.py:52] epoch 7532, training loss: 5695.98, average training loss: 5532.45, base loss: 15970.31
[INFO 2017-06-30 07:47:56,575 main.py:52] epoch 7533, training loss: 5363.88, average training loss: 5532.13, base loss: 15969.71
[INFO 2017-06-30 07:47:59,745 main.py:52] epoch 7534, training loss: 5523.99, average training loss: 5531.77, base loss: 15969.44
[INFO 2017-06-30 07:48:02,881 main.py:52] epoch 7535, training loss: 5240.82, average training loss: 5531.66, base loss: 15969.02
[INFO 2017-06-30 07:48:06,035 main.py:52] epoch 7536, training loss: 5339.01, average training loss: 5531.24, base loss: 15969.12
[INFO 2017-06-30 07:48:09,174 main.py:52] epoch 7537, training loss: 5357.57, average training loss: 5530.62, base loss: 15969.33
[INFO 2017-06-30 07:48:12,316 main.py:52] epoch 7538, training loss: 5771.63, average training loss: 5530.89, base loss: 15969.55
[INFO 2017-06-30 07:48:15,469 main.py:52] epoch 7539, training loss: 5625.67, average training loss: 5530.76, base loss: 15969.69
[INFO 2017-06-30 07:48:18,651 main.py:52] epoch 7540, training loss: 4975.01, average training loss: 5530.20, base loss: 15969.18
[INFO 2017-06-30 07:48:21,800 main.py:52] epoch 7541, training loss: 5390.71, average training loss: 5529.30, base loss: 15968.97
[INFO 2017-06-30 07:48:24,940 main.py:52] epoch 7542, training loss: 5226.90, average training loss: 5528.75, base loss: 15968.58
[INFO 2017-06-30 07:48:28,071 main.py:52] epoch 7543, training loss: 5658.55, average training loss: 5528.88, base loss: 15968.61
[INFO 2017-06-30 07:48:31,215 main.py:52] epoch 7544, training loss: 4868.89, average training loss: 5528.07, base loss: 15967.94
[INFO 2017-06-30 07:48:34,370 main.py:52] epoch 7545, training loss: 5047.58, average training loss: 5527.85, base loss: 15967.86
[INFO 2017-06-30 07:48:37,494 main.py:52] epoch 7546, training loss: 5339.89, average training loss: 5527.64, base loss: 15967.73
[INFO 2017-06-30 07:48:40,646 main.py:52] epoch 7547, training loss: 5269.90, average training loss: 5527.22, base loss: 15967.64
[INFO 2017-06-30 07:48:43,759 main.py:52] epoch 7548, training loss: 5467.30, average training loss: 5526.95, base loss: 15967.85
[INFO 2017-06-30 07:48:46,845 main.py:52] epoch 7549, training loss: 5172.63, average training loss: 5526.48, base loss: 15967.65
[INFO 2017-06-30 07:48:49,978 main.py:52] epoch 7550, training loss: 5245.83, average training loss: 5526.08, base loss: 15967.20
[INFO 2017-06-30 07:48:53,114 main.py:52] epoch 7551, training loss: 5317.71, average training loss: 5525.96, base loss: 15966.79
[INFO 2017-06-30 07:48:56,287 main.py:52] epoch 7552, training loss: 5562.06, average training loss: 5526.02, base loss: 15966.81
[INFO 2017-06-30 07:48:59,411 main.py:52] epoch 7553, training loss: 5598.01, average training loss: 5525.44, base loss: 15966.92
[INFO 2017-06-30 07:49:02,540 main.py:52] epoch 7554, training loss: 5308.25, average training loss: 5525.28, base loss: 15966.83
[INFO 2017-06-30 07:49:05,727 main.py:52] epoch 7555, training loss: 5462.53, average training loss: 5524.94, base loss: 15966.93
[INFO 2017-06-30 07:49:08,829 main.py:52] epoch 7556, training loss: 5244.60, average training loss: 5524.19, base loss: 15966.71
[INFO 2017-06-30 07:49:11,980 main.py:52] epoch 7557, training loss: 5311.18, average training loss: 5523.64, base loss: 15966.26
[INFO 2017-06-30 07:49:15,136 main.py:52] epoch 7558, training loss: 5342.92, average training loss: 5523.77, base loss: 15966.09
[INFO 2017-06-30 07:49:18,235 main.py:52] epoch 7559, training loss: 5307.37, average training loss: 5523.60, base loss: 15966.11
[INFO 2017-06-30 07:49:21,361 main.py:52] epoch 7560, training loss: 5189.67, average training loss: 5523.21, base loss: 15965.78
[INFO 2017-06-30 07:49:24,515 main.py:52] epoch 7561, training loss: 5454.37, average training loss: 5522.56, base loss: 15965.75
[INFO 2017-06-30 07:49:27,635 main.py:52] epoch 7562, training loss: 5444.00, average training loss: 5522.98, base loss: 15965.73
[INFO 2017-06-30 07:49:30,802 main.py:52] epoch 7563, training loss: 5606.21, average training loss: 5522.83, base loss: 15966.04
[INFO 2017-06-30 07:49:33,986 main.py:52] epoch 7564, training loss: 5301.73, average training loss: 5522.79, base loss: 15966.04
[INFO 2017-06-30 07:49:37,176 main.py:52] epoch 7565, training loss: 5388.75, average training loss: 5522.59, base loss: 15965.64
[INFO 2017-06-30 07:49:40,341 main.py:52] epoch 7566, training loss: 5467.24, average training loss: 5522.33, base loss: 15965.52
[INFO 2017-06-30 07:49:43,497 main.py:52] epoch 7567, training loss: 5584.98, average training loss: 5522.32, base loss: 15965.45
[INFO 2017-06-30 07:49:46,623 main.py:52] epoch 7568, training loss: 5444.20, average training loss: 5522.38, base loss: 15965.37
[INFO 2017-06-30 07:49:49,795 main.py:52] epoch 7569, training loss: 5421.33, average training loss: 5521.91, base loss: 15965.37
[INFO 2017-06-30 07:49:52,904 main.py:52] epoch 7570, training loss: 5520.36, average training loss: 5521.87, base loss: 15965.28
[INFO 2017-06-30 07:49:56,031 main.py:52] epoch 7571, training loss: 5430.25, average training loss: 5521.87, base loss: 15965.52
[INFO 2017-06-30 07:49:59,141 main.py:52] epoch 7572, training loss: 5581.79, average training loss: 5522.19, base loss: 15965.51
[INFO 2017-06-30 07:50:02,274 main.py:52] epoch 7573, training loss: 5648.23, average training loss: 5522.12, base loss: 15966.08
[INFO 2017-06-30 07:50:05,457 main.py:52] epoch 7574, training loss: 5362.07, average training loss: 5522.18, base loss: 15966.03
[INFO 2017-06-30 07:50:08,605 main.py:52] epoch 7575, training loss: 5338.94, average training loss: 5521.65, base loss: 15965.84
[INFO 2017-06-30 07:50:11,730 main.py:52] epoch 7576, training loss: 5375.45, average training loss: 5521.38, base loss: 15965.78
[INFO 2017-06-30 07:50:14,871 main.py:52] epoch 7577, training loss: 5430.39, average training loss: 5521.53, base loss: 15965.54
[INFO 2017-06-30 07:50:17,986 main.py:52] epoch 7578, training loss: 5359.79, average training loss: 5521.41, base loss: 15965.29
[INFO 2017-06-30 07:50:21,142 main.py:52] epoch 7579, training loss: 5627.61, average training loss: 5521.88, base loss: 15965.15
[INFO 2017-06-30 07:50:24,237 main.py:52] epoch 7580, training loss: 5765.53, average training loss: 5521.95, base loss: 15965.02
[INFO 2017-06-30 07:50:27,390 main.py:52] epoch 7581, training loss: 5413.48, average training loss: 5522.14, base loss: 15964.94
[INFO 2017-06-30 07:50:30,533 main.py:52] epoch 7582, training loss: 5262.54, average training loss: 5521.26, base loss: 15964.60
[INFO 2017-06-30 07:50:33,657 main.py:52] epoch 7583, training loss: 5368.89, average training loss: 5520.54, base loss: 15964.61
[INFO 2017-06-30 07:50:36,806 main.py:52] epoch 7584, training loss: 5757.11, average training loss: 5520.93, base loss: 15964.93
[INFO 2017-06-30 07:50:39,951 main.py:52] epoch 7585, training loss: 5818.32, average training loss: 5521.30, base loss: 15965.60
[INFO 2017-06-30 07:50:43,109 main.py:52] epoch 7586, training loss: 4990.55, average training loss: 5520.63, base loss: 15965.24
[INFO 2017-06-30 07:50:46,258 main.py:52] epoch 7587, training loss: 5505.51, average training loss: 5520.54, base loss: 15965.18
[INFO 2017-06-30 07:50:49,365 main.py:52] epoch 7588, training loss: 5637.27, average training loss: 5520.66, base loss: 15965.68
[INFO 2017-06-30 07:50:52,520 main.py:52] epoch 7589, training loss: 5438.19, average training loss: 5520.64, base loss: 15966.01
[INFO 2017-06-30 07:50:55,628 main.py:52] epoch 7590, training loss: 5844.57, average training loss: 5520.68, base loss: 15966.28
[INFO 2017-06-30 07:50:58,819 main.py:52] epoch 7591, training loss: 5453.64, average training loss: 5520.51, base loss: 15966.29
[INFO 2017-06-30 07:51:01,981 main.py:52] epoch 7592, training loss: 5172.87, average training loss: 5520.41, base loss: 15965.92
[INFO 2017-06-30 07:51:05,119 main.py:52] epoch 7593, training loss: 5669.86, average training loss: 5520.58, base loss: 15966.21
[INFO 2017-06-30 07:51:08,206 main.py:52] epoch 7594, training loss: 5471.27, average training loss: 5520.43, base loss: 15965.99
[INFO 2017-06-30 07:51:11,382 main.py:52] epoch 7595, training loss: 5674.61, average training loss: 5520.52, base loss: 15965.65
[INFO 2017-06-30 07:51:14,482 main.py:52] epoch 7596, training loss: 5219.34, average training loss: 5520.62, base loss: 15965.27
[INFO 2017-06-30 07:51:17,641 main.py:52] epoch 7597, training loss: 5597.54, average training loss: 5520.41, base loss: 15965.20
[INFO 2017-06-30 07:51:20,765 main.py:52] epoch 7598, training loss: 5099.99, average training loss: 5519.92, base loss: 15964.92
[INFO 2017-06-30 07:51:23,904 main.py:52] epoch 7599, training loss: 5195.06, average training loss: 5519.28, base loss: 15964.64
[INFO 2017-06-30 07:51:23,905 main.py:54] epoch 7599, testing
[INFO 2017-06-30 07:51:36,861 main.py:97] average testing loss: 5481.39, base loss: 15787.21
[INFO 2017-06-30 07:51:36,861 main.py:98] improve_loss: 10305.82, improve_percent: 0.65
[INFO 2017-06-30 07:51:36,862 main.py:66] current best improved percent: 0.66
[INFO 2017-06-30 07:51:40,024 main.py:52] epoch 7600, training loss: 5583.98, average training loss: 5519.38, base loss: 15964.65
[INFO 2017-06-30 07:51:43,166 main.py:52] epoch 7601, training loss: 5609.31, average training loss: 5519.42, base loss: 15964.78
[INFO 2017-06-30 07:51:46,349 main.py:52] epoch 7602, training loss: 5504.21, average training loss: 5519.89, base loss: 15964.85
[INFO 2017-06-30 07:51:49,530 main.py:52] epoch 7603, training loss: 4995.51, average training loss: 5519.11, base loss: 15964.39
[INFO 2017-06-30 07:51:52,656 main.py:52] epoch 7604, training loss: 5551.92, average training loss: 5518.88, base loss: 15964.20
[INFO 2017-06-30 07:51:55,786 main.py:52] epoch 7605, training loss: 5731.85, average training loss: 5519.12, base loss: 15964.11
[INFO 2017-06-30 07:51:58,964 main.py:52] epoch 7606, training loss: 5431.25, average training loss: 5518.68, base loss: 15963.98
[INFO 2017-06-30 07:52:02,119 main.py:52] epoch 7607, training loss: 5350.01, average training loss: 5518.81, base loss: 15963.86
[INFO 2017-06-30 07:52:05,308 main.py:52] epoch 7608, training loss: 5294.89, average training loss: 5518.51, base loss: 15963.84
[INFO 2017-06-30 07:52:08,531 main.py:52] epoch 7609, training loss: 5498.83, average training loss: 5518.25, base loss: 15964.11
[INFO 2017-06-30 07:52:11,715 main.py:52] epoch 7610, training loss: 5249.08, average training loss: 5517.93, base loss: 15963.84
[INFO 2017-06-30 07:52:14,848 main.py:52] epoch 7611, training loss: 5422.74, average training loss: 5517.60, base loss: 15964.11
[INFO 2017-06-30 07:52:18,008 main.py:52] epoch 7612, training loss: 5724.09, average training loss: 5517.84, base loss: 15964.46
[INFO 2017-06-30 07:52:21,154 main.py:52] epoch 7613, training loss: 5275.27, average training loss: 5517.38, base loss: 15964.36
[INFO 2017-06-30 07:52:24,299 main.py:52] epoch 7614, training loss: 5351.17, average training loss: 5516.49, base loss: 15964.25
[INFO 2017-06-30 07:52:27,431 main.py:52] epoch 7615, training loss: 5400.00, average training loss: 5516.60, base loss: 15964.14
[INFO 2017-06-30 07:52:30,567 main.py:52] epoch 7616, training loss: 4955.87, average training loss: 5515.71, base loss: 15963.88
[INFO 2017-06-30 07:52:33,712 main.py:52] epoch 7617, training loss: 5207.30, average training loss: 5515.63, base loss: 15963.66
[INFO 2017-06-30 07:52:36,838 main.py:52] epoch 7618, training loss: 5496.31, average training loss: 5515.76, base loss: 15963.82
[INFO 2017-06-30 07:52:39,996 main.py:52] epoch 7619, training loss: 5039.02, average training loss: 5515.20, base loss: 15963.66
[INFO 2017-06-30 07:52:43,188 main.py:52] epoch 7620, training loss: 5579.83, average training loss: 5515.21, base loss: 15963.59
[INFO 2017-06-30 07:52:46,344 main.py:52] epoch 7621, training loss: 5696.82, average training loss: 5515.52, base loss: 15963.79
[INFO 2017-06-30 07:52:49,475 main.py:52] epoch 7622, training loss: 5103.29, average training loss: 5515.04, base loss: 15963.38
[INFO 2017-06-30 07:52:52,626 main.py:52] epoch 7623, training loss: 5514.90, average training loss: 5515.19, base loss: 15963.02
[INFO 2017-06-30 07:52:55,805 main.py:52] epoch 7624, training loss: 5498.69, average training loss: 5515.03, base loss: 15963.18
[INFO 2017-06-30 07:52:58,921 main.py:52] epoch 7625, training loss: 5586.65, average training loss: 5515.33, base loss: 15963.25
[INFO 2017-06-30 07:53:02,062 main.py:52] epoch 7626, training loss: 6062.18, average training loss: 5516.06, base loss: 15963.32
[INFO 2017-06-30 07:53:05,188 main.py:52] epoch 7627, training loss: 5428.78, average training loss: 5516.20, base loss: 15962.80
[INFO 2017-06-30 07:53:08,348 main.py:52] epoch 7628, training loss: 5655.53, average training loss: 5516.21, base loss: 15962.86
[INFO 2017-06-30 07:53:11,465 main.py:52] epoch 7629, training loss: 5374.67, average training loss: 5516.29, base loss: 15962.71
[INFO 2017-06-30 07:53:14,648 main.py:52] epoch 7630, training loss: 5550.84, average training loss: 5516.46, base loss: 15962.53
[INFO 2017-06-30 07:53:17,776 main.py:52] epoch 7631, training loss: 5754.38, average training loss: 5516.53, base loss: 15962.69
[INFO 2017-06-30 07:53:20,907 main.py:52] epoch 7632, training loss: 5460.82, average training loss: 5516.33, base loss: 15962.87
[INFO 2017-06-30 07:53:24,071 main.py:52] epoch 7633, training loss: 5310.65, average training loss: 5515.85, base loss: 15962.91
[INFO 2017-06-30 07:53:27,178 main.py:52] epoch 7634, training loss: 5517.36, average training loss: 5516.32, base loss: 15963.11
[INFO 2017-06-30 07:53:30,343 main.py:52] epoch 7635, training loss: 5744.78, average training loss: 5516.53, base loss: 15963.19
[INFO 2017-06-30 07:53:33,486 main.py:52] epoch 7636, training loss: 5159.76, average training loss: 5516.22, base loss: 15962.37
[INFO 2017-06-30 07:53:36,681 main.py:52] epoch 7637, training loss: 5252.93, average training loss: 5515.59, base loss: 15962.11
[INFO 2017-06-30 07:53:39,787 main.py:52] epoch 7638, training loss: 5326.21, average training loss: 5515.27, base loss: 15961.94
[INFO 2017-06-30 07:53:42,914 main.py:52] epoch 7639, training loss: 5469.65, average training loss: 5514.83, base loss: 15961.79
[INFO 2017-06-30 07:53:46,086 main.py:52] epoch 7640, training loss: 5535.46, average training loss: 5514.57, base loss: 15962.07
[INFO 2017-06-30 07:53:49,242 main.py:52] epoch 7641, training loss: 6038.89, average training loss: 5514.90, base loss: 15962.73
[INFO 2017-06-30 07:53:52,388 main.py:52] epoch 7642, training loss: 5550.68, average training loss: 5514.78, base loss: 15962.90
[INFO 2017-06-30 07:53:55,585 main.py:52] epoch 7643, training loss: 5243.78, average training loss: 5514.32, base loss: 15962.51
[INFO 2017-06-30 07:53:58,772 main.py:52] epoch 7644, training loss: 5375.76, average training loss: 5514.07, base loss: 15962.44
[INFO 2017-06-30 07:54:01,910 main.py:52] epoch 7645, training loss: 5140.48, average training loss: 5513.43, base loss: 15962.10
[INFO 2017-06-30 07:54:05,062 main.py:52] epoch 7646, training loss: 5400.79, average training loss: 5512.90, base loss: 15962.18
[INFO 2017-06-30 07:54:08,221 main.py:52] epoch 7647, training loss: 5499.40, average training loss: 5513.13, base loss: 15962.53
[INFO 2017-06-30 07:54:11,350 main.py:52] epoch 7648, training loss: 5417.91, average training loss: 5512.98, base loss: 15962.43
[INFO 2017-06-30 07:54:14,483 main.py:52] epoch 7649, training loss: 5725.97, average training loss: 5512.99, base loss: 15962.86
[INFO 2017-06-30 07:54:17,623 main.py:52] epoch 7650, training loss: 5566.89, average training loss: 5512.45, base loss: 15962.79
[INFO 2017-06-30 07:54:20,729 main.py:52] epoch 7651, training loss: 5367.60, average training loss: 5512.24, base loss: 15963.01
[INFO 2017-06-30 07:54:23,863 main.py:52] epoch 7652, training loss: 5644.97, average training loss: 5512.24, base loss: 15963.39
[INFO 2017-06-30 07:54:27,012 main.py:52] epoch 7653, training loss: 5509.89, average training loss: 5512.14, base loss: 15963.65
[INFO 2017-06-30 07:54:30,123 main.py:52] epoch 7654, training loss: 5773.70, average training loss: 5512.49, base loss: 15964.16
[INFO 2017-06-30 07:54:33,236 main.py:52] epoch 7655, training loss: 5501.43, average training loss: 5512.45, base loss: 15964.07
[INFO 2017-06-30 07:54:36,433 main.py:52] epoch 7656, training loss: 5586.81, average training loss: 5512.35, base loss: 15964.05
[INFO 2017-06-30 07:54:39,628 main.py:52] epoch 7657, training loss: 5504.29, average training loss: 5512.04, base loss: 15964.22
[INFO 2017-06-30 07:54:42,826 main.py:52] epoch 7658, training loss: 5311.94, average training loss: 5511.56, base loss: 15963.98
[INFO 2017-06-30 07:54:45,965 main.py:52] epoch 7659, training loss: 5367.67, average training loss: 5511.35, base loss: 15964.11
[INFO 2017-06-30 07:54:49,140 main.py:52] epoch 7660, training loss: 5085.48, average training loss: 5511.12, base loss: 15963.87
[INFO 2017-06-30 07:54:52,266 main.py:52] epoch 7661, training loss: 5486.12, average training loss: 5510.65, base loss: 15963.56
[INFO 2017-06-30 07:54:55,408 main.py:52] epoch 7662, training loss: 5425.34, average training loss: 5510.59, base loss: 15963.05
[INFO 2017-06-30 07:54:58,534 main.py:52] epoch 7663, training loss: 5287.89, average training loss: 5510.38, base loss: 15962.65
[INFO 2017-06-30 07:55:01,697 main.py:52] epoch 7664, training loss: 5322.68, average training loss: 5510.05, base loss: 15962.16
[INFO 2017-06-30 07:55:04,872 main.py:52] epoch 7665, training loss: 5492.96, average training loss: 5510.18, base loss: 15962.04
[INFO 2017-06-30 07:55:08,010 main.py:52] epoch 7666, training loss: 4694.24, average training loss: 5509.44, base loss: 15961.46
[INFO 2017-06-30 07:55:11,152 main.py:52] epoch 7667, training loss: 5437.31, average training loss: 5509.86, base loss: 15961.74
[INFO 2017-06-30 07:55:14,298 main.py:52] epoch 7668, training loss: 5212.08, average training loss: 5509.41, base loss: 15961.55
[INFO 2017-06-30 07:55:17,422 main.py:52] epoch 7669, training loss: 5711.01, average training loss: 5509.28, base loss: 15961.93
[INFO 2017-06-30 07:55:20,571 main.py:52] epoch 7670, training loss: 5521.98, average training loss: 5509.51, base loss: 15962.06
[INFO 2017-06-30 07:55:23,759 main.py:52] epoch 7671, training loss: 5264.06, average training loss: 5509.03, base loss: 15961.95
[INFO 2017-06-30 07:55:26,908 main.py:52] epoch 7672, training loss: 5342.12, average training loss: 5508.69, base loss: 15962.36
[INFO 2017-06-30 07:55:30,029 main.py:52] epoch 7673, training loss: 5760.03, average training loss: 5509.15, base loss: 15962.72
[INFO 2017-06-30 07:55:33,210 main.py:52] epoch 7674, training loss: 5484.22, average training loss: 5509.52, base loss: 15962.52
[INFO 2017-06-30 07:55:36,327 main.py:52] epoch 7675, training loss: 5337.32, average training loss: 5509.36, base loss: 15962.62
[INFO 2017-06-30 07:55:39,465 main.py:52] epoch 7676, training loss: 5325.86, average training loss: 5508.80, base loss: 15962.58
[INFO 2017-06-30 07:55:42,593 main.py:52] epoch 7677, training loss: 5465.51, average training loss: 5508.91, base loss: 15962.15
[INFO 2017-06-30 07:55:45,734 main.py:52] epoch 7678, training loss: 5455.32, average training loss: 5508.49, base loss: 15962.23
[INFO 2017-06-30 07:55:48,872 main.py:52] epoch 7679, training loss: 5210.04, average training loss: 5507.72, base loss: 15961.88
[INFO 2017-06-30 07:55:52,036 main.py:52] epoch 7680, training loss: 4928.61, average training loss: 5507.26, base loss: 15961.58
[INFO 2017-06-30 07:55:55,192 main.py:52] epoch 7681, training loss: 5463.64, average training loss: 5507.46, base loss: 15961.69
[INFO 2017-06-30 07:55:58,351 main.py:52] epoch 7682, training loss: 5452.47, average training loss: 5507.51, base loss: 15962.09
[INFO 2017-06-30 07:56:01,457 main.py:52] epoch 7683, training loss: 5201.44, average training loss: 5507.24, base loss: 15962.03
[INFO 2017-06-30 07:56:04,621 main.py:52] epoch 7684, training loss: 5791.09, average training loss: 5507.91, base loss: 15962.19
[INFO 2017-06-30 07:56:07,756 main.py:52] epoch 7685, training loss: 5416.49, average training loss: 5507.58, base loss: 15962.15
[INFO 2017-06-30 07:56:10,887 main.py:52] epoch 7686, training loss: 5016.79, average training loss: 5507.29, base loss: 15961.96
[INFO 2017-06-30 07:56:14,018 main.py:52] epoch 7687, training loss: 5665.60, average training loss: 5507.40, base loss: 15962.25
[INFO 2017-06-30 07:56:17,149 main.py:52] epoch 7688, training loss: 5579.73, average training loss: 5507.17, base loss: 15961.83
[INFO 2017-06-30 07:56:20,300 main.py:52] epoch 7689, training loss: 5325.92, average training loss: 5506.97, base loss: 15961.41
[INFO 2017-06-30 07:56:23,475 main.py:52] epoch 7690, training loss: 5610.16, average training loss: 5507.22, base loss: 15961.52
[INFO 2017-06-30 07:56:26,597 main.py:52] epoch 7691, training loss: 5191.41, average training loss: 5506.62, base loss: 15961.79
[INFO 2017-06-30 07:56:29,734 main.py:52] epoch 7692, training loss: 5456.13, average training loss: 5506.66, base loss: 15961.92
[INFO 2017-06-30 07:56:32,883 main.py:52] epoch 7693, training loss: 5344.37, average training loss: 5506.56, base loss: 15961.97
[INFO 2017-06-30 07:56:35,967 main.py:52] epoch 7694, training loss: 5417.45, average training loss: 5506.59, base loss: 15961.55
[INFO 2017-06-30 07:56:39,133 main.py:52] epoch 7695, training loss: 5305.45, average training loss: 5505.99, base loss: 15961.28
[INFO 2017-06-30 07:56:42,307 main.py:52] epoch 7696, training loss: 5330.64, average training loss: 5506.03, base loss: 15960.83
[INFO 2017-06-30 07:56:45,457 main.py:52] epoch 7697, training loss: 5342.44, average training loss: 5506.11, base loss: 15960.55
[INFO 2017-06-30 07:56:48,629 main.py:52] epoch 7698, training loss: 5447.45, average training loss: 5505.56, base loss: 15960.42
[INFO 2017-06-30 07:56:51,814 main.py:52] epoch 7699, training loss: 5777.98, average training loss: 5505.23, base loss: 15960.64
[INFO 2017-06-30 07:56:51,814 main.py:54] epoch 7699, testing
[INFO 2017-06-30 07:57:04,792 main.py:97] average testing loss: 5339.80, base loss: 15499.36
[INFO 2017-06-30 07:57:04,792 main.py:98] improve_loss: 10159.56, improve_percent: 0.66
[INFO 2017-06-30 07:57:04,794 main.py:66] current best improved percent: 0.66
[INFO 2017-06-30 07:57:07,920 main.py:52] epoch 7700, training loss: 5753.38, average training loss: 5505.44, base loss: 15960.82
[INFO 2017-06-30 07:57:11,078 main.py:52] epoch 7701, training loss: 5232.84, average training loss: 5504.82, base loss: 15960.65
[INFO 2017-06-30 07:57:14,244 main.py:52] epoch 7702, training loss: 5411.14, average training loss: 5504.49, base loss: 15960.78
[INFO 2017-06-30 07:57:17,392 main.py:52] epoch 7703, training loss: 5408.31, average training loss: 5504.64, base loss: 15960.70
[INFO 2017-06-30 07:57:20,553 main.py:52] epoch 7704, training loss: 5183.67, average training loss: 5504.23, base loss: 15960.10
[INFO 2017-06-30 07:57:23,699 main.py:52] epoch 7705, training loss: 5400.75, average training loss: 5503.76, base loss: 15959.60
[INFO 2017-06-30 07:57:26,865 main.py:52] epoch 7706, training loss: 5863.68, average training loss: 5503.81, base loss: 15960.03
[INFO 2017-06-30 07:57:30,019 main.py:52] epoch 7707, training loss: 5075.29, average training loss: 5503.10, base loss: 15959.54
[INFO 2017-06-30 07:57:33,122 main.py:52] epoch 7708, training loss: 5211.31, average training loss: 5502.44, base loss: 15959.63
[INFO 2017-06-30 07:57:36,279 main.py:52] epoch 7709, training loss: 5188.01, average training loss: 5502.17, base loss: 15959.35
[INFO 2017-06-30 07:57:39,447 main.py:52] epoch 7710, training loss: 5256.61, average training loss: 5501.92, base loss: 15959.33
[INFO 2017-06-30 07:57:42,617 main.py:52] epoch 7711, training loss: 5436.15, average training loss: 5501.98, base loss: 15959.73
[INFO 2017-06-30 07:57:45,749 main.py:52] epoch 7712, training loss: 5445.06, average training loss: 5501.92, base loss: 15960.02
[INFO 2017-06-30 07:57:48,886 main.py:52] epoch 7713, training loss: 5357.06, average training loss: 5501.68, base loss: 15960.16
[INFO 2017-06-30 07:57:51,985 main.py:52] epoch 7714, training loss: 5126.22, average training loss: 5501.56, base loss: 15959.93
[INFO 2017-06-30 07:57:55,119 main.py:52] epoch 7715, training loss: 5147.58, average training loss: 5501.32, base loss: 15959.98
[INFO 2017-06-30 07:57:58,248 main.py:52] epoch 7716, training loss: 5305.93, average training loss: 5501.48, base loss: 15959.86
[INFO 2017-06-30 07:58:01,456 main.py:52] epoch 7717, training loss: 5435.53, average training loss: 5501.51, base loss: 15960.04
[INFO 2017-06-30 07:58:04,609 main.py:52] epoch 7718, training loss: 5141.08, average training loss: 5501.22, base loss: 15959.84
[INFO 2017-06-30 07:58:07,753 main.py:52] epoch 7719, training loss: 5163.35, average training loss: 5501.21, base loss: 15959.73
[INFO 2017-06-30 07:58:10,910 main.py:52] epoch 7720, training loss: 5876.91, average training loss: 5501.45, base loss: 15960.18
[INFO 2017-06-30 07:58:14,016 main.py:52] epoch 7721, training loss: 5607.04, average training loss: 5501.04, base loss: 15960.55
[INFO 2017-06-30 07:58:17,204 main.py:52] epoch 7722, training loss: 5196.57, average training loss: 5500.24, base loss: 15959.99
[INFO 2017-06-30 07:58:20,292 main.py:52] epoch 7723, training loss: 5462.09, average training loss: 5500.05, base loss: 15959.70
[INFO 2017-06-30 07:58:23,433 main.py:52] epoch 7724, training loss: 5984.71, average training loss: 5500.71, base loss: 15960.31
[INFO 2017-06-30 07:58:26,523 main.py:52] epoch 7725, training loss: 5281.05, average training loss: 5500.88, base loss: 15960.02
[INFO 2017-06-30 07:58:29,650 main.py:52] epoch 7726, training loss: 5194.19, average training loss: 5499.86, base loss: 15959.85
[INFO 2017-06-30 07:58:32,827 main.py:52] epoch 7727, training loss: 5726.38, average training loss: 5500.50, base loss: 15959.63
[INFO 2017-06-30 07:58:35,923 main.py:52] epoch 7728, training loss: 5401.59, average training loss: 5500.41, base loss: 15959.67
[INFO 2017-06-30 07:58:39,032 main.py:52] epoch 7729, training loss: 5604.61, average training loss: 5500.38, base loss: 15959.48
[INFO 2017-06-30 07:58:42,174 main.py:52] epoch 7730, training loss: 5352.86, average training loss: 5500.22, base loss: 15959.15
[INFO 2017-06-30 07:58:45,314 main.py:52] epoch 7731, training loss: 5403.93, average training loss: 5500.20, base loss: 15959.04
[INFO 2017-06-30 07:58:48,439 main.py:52] epoch 7732, training loss: 5506.44, average training loss: 5500.05, base loss: 15959.23
[INFO 2017-06-30 07:58:51,565 main.py:52] epoch 7733, training loss: 5004.24, average training loss: 5499.19, base loss: 15959.01
[INFO 2017-06-30 07:58:54,697 main.py:52] epoch 7734, training loss: 5108.93, average training loss: 5498.55, base loss: 15959.16
[INFO 2017-06-30 07:58:57,884 main.py:52] epoch 7735, training loss: 5262.52, average training loss: 5498.05, base loss: 15959.00
[INFO 2017-06-30 07:59:01,063 main.py:52] epoch 7736, training loss: 5223.92, average training loss: 5497.67, base loss: 15958.92
[INFO 2017-06-30 07:59:04,185 main.py:52] epoch 7737, training loss: 5321.30, average training loss: 5497.76, base loss: 15958.90
[INFO 2017-06-30 07:59:07,325 main.py:52] epoch 7738, training loss: 5102.60, average training loss: 5497.19, base loss: 15958.53
[INFO 2017-06-30 07:59:10,497 main.py:52] epoch 7739, training loss: 5498.56, average training loss: 5497.15, base loss: 15958.56
[INFO 2017-06-30 07:59:13,658 main.py:52] epoch 7740, training loss: 5342.91, average training loss: 5496.90, base loss: 15958.23
[INFO 2017-06-30 07:59:16,828 main.py:52] epoch 7741, training loss: 5201.81, average training loss: 5496.56, base loss: 15958.06
[INFO 2017-06-30 07:59:19,969 main.py:52] epoch 7742, training loss: 5930.30, average training loss: 5496.99, base loss: 15958.38
[INFO 2017-06-30 07:59:23,093 main.py:52] epoch 7743, training loss: 5047.64, average training loss: 5496.48, base loss: 15957.73
[INFO 2017-06-30 07:59:26,221 main.py:52] epoch 7744, training loss: 5817.78, average training loss: 5496.79, base loss: 15958.33
[INFO 2017-06-30 07:59:29,323 main.py:52] epoch 7745, training loss: 5325.76, average training loss: 5496.94, base loss: 15958.28
[INFO 2017-06-30 07:59:32,472 main.py:52] epoch 7746, training loss: 5475.18, average training loss: 5496.42, base loss: 15957.83
[INFO 2017-06-30 07:59:35,612 main.py:52] epoch 7747, training loss: 5172.47, average training loss: 5496.00, base loss: 15957.68
[INFO 2017-06-30 07:59:38,758 main.py:52] epoch 7748, training loss: 5292.39, average training loss: 5496.08, base loss: 15957.89
[INFO 2017-06-30 07:59:41,902 main.py:52] epoch 7749, training loss: 5326.81, average training loss: 5495.46, base loss: 15957.66
[INFO 2017-06-30 07:59:45,086 main.py:52] epoch 7750, training loss: 5582.15, average training loss: 5495.79, base loss: 15957.62
[INFO 2017-06-30 07:59:48,223 main.py:52] epoch 7751, training loss: 5149.53, average training loss: 5495.52, base loss: 15957.31
[INFO 2017-06-30 07:59:51,356 main.py:52] epoch 7752, training loss: 5565.38, average training loss: 5494.90, base loss: 15957.13
[INFO 2017-06-30 07:59:54,519 main.py:52] epoch 7753, training loss: 5592.21, average training loss: 5494.88, base loss: 15957.02
[INFO 2017-06-30 07:59:57,626 main.py:52] epoch 7754, training loss: 5717.55, average training loss: 5495.02, base loss: 15956.76
[INFO 2017-06-30 08:00:00,821 main.py:52] epoch 7755, training loss: 5242.81, average training loss: 5494.69, base loss: 15956.32
[INFO 2017-06-30 08:00:03,948 main.py:52] epoch 7756, training loss: 5894.25, average training loss: 5495.07, base loss: 15956.64
[INFO 2017-06-30 08:00:07,050 main.py:52] epoch 7757, training loss: 5664.22, average training loss: 5494.90, base loss: 15956.87
[INFO 2017-06-30 08:00:10,140 main.py:52] epoch 7758, training loss: 5648.63, average training loss: 5495.02, base loss: 15956.96
[INFO 2017-06-30 08:00:13,249 main.py:52] epoch 7759, training loss: 5844.81, average training loss: 5495.33, base loss: 15957.08
[INFO 2017-06-30 08:00:16,429 main.py:52] epoch 7760, training loss: 5477.44, average training loss: 5495.05, base loss: 15957.14
[INFO 2017-06-30 08:00:19,585 main.py:52] epoch 7761, training loss: 5714.14, average training loss: 5495.06, base loss: 15957.14
[INFO 2017-06-30 08:00:22,719 main.py:52] epoch 7762, training loss: 4993.38, average training loss: 5494.48, base loss: 15956.90
[INFO 2017-06-30 08:00:25,850 main.py:52] epoch 7763, training loss: 5440.14, average training loss: 5494.24, base loss: 15957.12
[INFO 2017-06-30 08:00:28,945 main.py:52] epoch 7764, training loss: 5405.12, average training loss: 5494.29, base loss: 15957.04
[INFO 2017-06-30 08:00:32,067 main.py:52] epoch 7765, training loss: 5675.31, average training loss: 5494.45, base loss: 15956.62
[INFO 2017-06-30 08:00:35,233 main.py:52] epoch 7766, training loss: 5965.84, average training loss: 5495.29, base loss: 15957.31
[INFO 2017-06-30 08:00:38,356 main.py:52] epoch 7767, training loss: 5475.92, average training loss: 5495.11, base loss: 15957.43
[INFO 2017-06-30 08:00:41,505 main.py:52] epoch 7768, training loss: 5377.67, average training loss: 5494.73, base loss: 15957.43
[INFO 2017-06-30 08:00:44,638 main.py:52] epoch 7769, training loss: 5485.33, average training loss: 5494.94, base loss: 15957.67
[INFO 2017-06-30 08:00:47,772 main.py:52] epoch 7770, training loss: 5671.32, average training loss: 5494.50, base loss: 15957.83
[INFO 2017-06-30 08:00:50,952 main.py:52] epoch 7771, training loss: 5421.50, average training loss: 5494.44, base loss: 15957.64
[INFO 2017-06-30 08:00:54,113 main.py:52] epoch 7772, training loss: 5497.10, average training loss: 5494.72, base loss: 15957.84
[INFO 2017-06-30 08:00:57,276 main.py:52] epoch 7773, training loss: 5545.99, average training loss: 5494.58, base loss: 15957.90
[INFO 2017-06-30 08:01:00,413 main.py:52] epoch 7774, training loss: 5610.27, average training loss: 5494.64, base loss: 15957.44
[INFO 2017-06-30 08:01:03,567 main.py:52] epoch 7775, training loss: 5526.01, average training loss: 5494.54, base loss: 15957.08
[INFO 2017-06-30 08:01:06,700 main.py:52] epoch 7776, training loss: 5181.57, average training loss: 5494.17, base loss: 15956.87
[INFO 2017-06-30 08:01:09,858 main.py:52] epoch 7777, training loss: 5466.93, average training loss: 5494.47, base loss: 15957.15
[INFO 2017-06-30 08:01:13,005 main.py:52] epoch 7778, training loss: 5808.50, average training loss: 5494.64, base loss: 15957.55
[INFO 2017-06-30 08:01:16,118 main.py:52] epoch 7779, training loss: 5891.29, average training loss: 5495.17, base loss: 15957.89
[INFO 2017-06-30 08:01:19,266 main.py:52] epoch 7780, training loss: 5308.15, average training loss: 5495.11, base loss: 15957.56
[INFO 2017-06-30 08:01:22,429 main.py:52] epoch 7781, training loss: 5478.13, average training loss: 5495.09, base loss: 15957.74
[INFO 2017-06-30 08:01:25,590 main.py:52] epoch 7782, training loss: 4870.86, average training loss: 5494.58, base loss: 15957.36
[INFO 2017-06-30 08:01:28,737 main.py:52] epoch 7783, training loss: 5356.88, average training loss: 5494.35, base loss: 15957.29
[INFO 2017-06-30 08:01:31,869 main.py:52] epoch 7784, training loss: 5469.87, average training loss: 5494.66, base loss: 15957.29
[INFO 2017-06-30 08:01:35,016 main.py:52] epoch 7785, training loss: 5655.28, average training loss: 5494.78, base loss: 15957.37
[INFO 2017-06-30 08:01:38,147 main.py:52] epoch 7786, training loss: 5012.33, average training loss: 5494.40, base loss: 15957.16
[INFO 2017-06-30 08:01:41,315 main.py:52] epoch 7787, training loss: 5268.74, average training loss: 5494.28, base loss: 15957.34
[INFO 2017-06-30 08:01:44,467 main.py:52] epoch 7788, training loss: 5314.25, average training loss: 5494.07, base loss: 15957.26
[INFO 2017-06-30 08:01:47,615 main.py:52] epoch 7789, training loss: 5084.44, average training loss: 5493.64, base loss: 15956.92
[INFO 2017-06-30 08:01:50,713 main.py:52] epoch 7790, training loss: 5533.97, average training loss: 5493.69, base loss: 15956.55
[INFO 2017-06-30 08:01:53,841 main.py:52] epoch 7791, training loss: 6190.31, average training loss: 5494.46, base loss: 15956.97
[INFO 2017-06-30 08:01:56,980 main.py:52] epoch 7792, training loss: 5514.39, average training loss: 5494.52, base loss: 15957.07
[INFO 2017-06-30 08:02:00,131 main.py:52] epoch 7793, training loss: 5563.76, average training loss: 5494.42, base loss: 15957.13
[INFO 2017-06-30 08:02:03,254 main.py:52] epoch 7794, training loss: 5381.19, average training loss: 5494.67, base loss: 15956.89
[INFO 2017-06-30 08:02:06,392 main.py:52] epoch 7795, training loss: 5449.49, average training loss: 5494.74, base loss: 15956.72
[INFO 2017-06-30 08:02:09,574 main.py:52] epoch 7796, training loss: 5280.35, average training loss: 5495.04, base loss: 15956.33
[INFO 2017-06-30 08:02:12,728 main.py:52] epoch 7797, training loss: 4998.65, average training loss: 5494.27, base loss: 15956.05
[INFO 2017-06-30 08:02:15,890 main.py:52] epoch 7798, training loss: 5185.58, average training loss: 5494.06, base loss: 15955.86
[INFO 2017-06-30 08:02:19,012 main.py:52] epoch 7799, training loss: 5414.12, average training loss: 5493.81, base loss: 15955.73
[INFO 2017-06-30 08:02:19,012 main.py:54] epoch 7799, testing
[INFO 2017-06-30 08:02:31,978 main.py:97] average testing loss: 5559.97, base loss: 16027.90
[INFO 2017-06-30 08:02:31,978 main.py:98] improve_loss: 10467.93, improve_percent: 0.65
[INFO 2017-06-30 08:02:31,980 main.py:66] current best improved percent: 0.66
[INFO 2017-06-30 08:02:35,079 main.py:52] epoch 7800, training loss: 5410.91, average training loss: 5493.64, base loss: 15955.72
[INFO 2017-06-30 08:02:38,189 main.py:52] epoch 7801, training loss: 5303.32, average training loss: 5493.91, base loss: 15955.69
[INFO 2017-06-30 08:02:41,335 main.py:52] epoch 7802, training loss: 5671.48, average training loss: 5493.75, base loss: 15955.81
[INFO 2017-06-30 08:02:44,443 main.py:52] epoch 7803, training loss: 5651.72, average training loss: 5493.92, base loss: 15955.67
[INFO 2017-06-30 08:02:47,561 main.py:52] epoch 7804, training loss: 5563.98, average training loss: 5494.08, base loss: 15955.71
[INFO 2017-06-30 08:02:50,694 main.py:52] epoch 7805, training loss: 5504.50, average training loss: 5493.99, base loss: 15955.69
[INFO 2017-06-30 08:02:53,832 main.py:52] epoch 7806, training loss: 5511.53, average training loss: 5494.21, base loss: 15955.40
[INFO 2017-06-30 08:02:56,990 main.py:52] epoch 7807, training loss: 5620.72, average training loss: 5494.39, base loss: 15955.83
[INFO 2017-06-30 08:03:00,133 main.py:52] epoch 7808, training loss: 5296.17, average training loss: 5493.65, base loss: 15956.10
[INFO 2017-06-30 08:03:03,263 main.py:52] epoch 7809, training loss: 5651.68, average training loss: 5493.77, base loss: 15956.47
[INFO 2017-06-30 08:03:06,409 main.py:52] epoch 7810, training loss: 5770.06, average training loss: 5494.10, base loss: 15956.64
[INFO 2017-06-30 08:03:09,566 main.py:52] epoch 7811, training loss: 5727.71, average training loss: 5494.28, base loss: 15956.94
[INFO 2017-06-30 08:03:12,667 main.py:52] epoch 7812, training loss: 5491.92, average training loss: 5494.48, base loss: 15956.79
[INFO 2017-06-30 08:03:15,789 main.py:52] epoch 7813, training loss: 5745.48, average training loss: 5494.88, base loss: 15957.10
[INFO 2017-06-30 08:03:18,940 main.py:52] epoch 7814, training loss: 5485.44, average training loss: 5494.52, base loss: 15957.10
[INFO 2017-06-30 08:03:22,034 main.py:52] epoch 7815, training loss: 5503.91, average training loss: 5494.77, base loss: 15957.50
[INFO 2017-06-30 08:03:25,182 main.py:52] epoch 7816, training loss: 5547.61, average training loss: 5494.71, base loss: 15957.79
[INFO 2017-06-30 08:03:28,315 main.py:52] epoch 7817, training loss: 4949.46, average training loss: 5494.37, base loss: 15957.57
[INFO 2017-06-30 08:03:31,518 main.py:52] epoch 7818, training loss: 5753.59, average training loss: 5494.57, base loss: 15957.81
[INFO 2017-06-30 08:03:34,687 main.py:52] epoch 7819, training loss: 5297.86, average training loss: 5494.59, base loss: 15957.71
[INFO 2017-06-30 08:03:37,882 main.py:52] epoch 7820, training loss: 5842.72, average training loss: 5495.02, base loss: 15958.07
[INFO 2017-06-30 08:03:41,012 main.py:52] epoch 7821, training loss: 5621.07, average training loss: 5495.16, base loss: 15957.99
[INFO 2017-06-30 08:03:44,156 main.py:52] epoch 7822, training loss: 5095.33, average training loss: 5494.49, base loss: 15957.75
[INFO 2017-06-30 08:03:47,277 main.py:52] epoch 7823, training loss: 5931.34, average training loss: 5494.94, base loss: 15957.80
[INFO 2017-06-30 08:03:50,405 main.py:52] epoch 7824, training loss: 5165.81, average training loss: 5494.29, base loss: 15957.65
[INFO 2017-06-30 08:03:53,519 main.py:52] epoch 7825, training loss: 5111.76, average training loss: 5493.50, base loss: 15957.59
[INFO 2017-06-30 08:03:56,665 main.py:52] epoch 7826, training loss: 5634.81, average training loss: 5493.97, base loss: 15958.14
[INFO 2017-06-30 08:03:59,793 main.py:52] epoch 7827, training loss: 5880.41, average training loss: 5494.37, base loss: 15958.59
[INFO 2017-06-30 08:04:02,965 main.py:52] epoch 7828, training loss: 5201.41, average training loss: 5493.73, base loss: 15958.45
[INFO 2017-06-30 08:04:06,103 main.py:52] epoch 7829, training loss: 5504.36, average training loss: 5493.65, base loss: 15958.66
[INFO 2017-06-30 08:04:09,187 main.py:52] epoch 7830, training loss: 5436.81, average training loss: 5493.92, base loss: 15958.67
[INFO 2017-06-30 08:04:12,320 main.py:52] epoch 7831, training loss: 5670.49, average training loss: 5493.97, base loss: 15958.99
[INFO 2017-06-30 08:04:15,484 main.py:52] epoch 7832, training loss: 5400.14, average training loss: 5493.88, base loss: 15958.92
[INFO 2017-06-30 08:04:18,636 main.py:52] epoch 7833, training loss: 5466.79, average training loss: 5494.28, base loss: 15958.91
[INFO 2017-06-30 08:04:21,835 main.py:52] epoch 7834, training loss: 5346.72, average training loss: 5493.94, base loss: 15958.78
[INFO 2017-06-30 08:04:24,951 main.py:52] epoch 7835, training loss: 5869.98, average training loss: 5494.24, base loss: 15959.38
[INFO 2017-06-30 08:04:28,126 main.py:52] epoch 7836, training loss: 5832.11, average training loss: 5494.56, base loss: 15959.78
[INFO 2017-06-30 08:04:31,291 main.py:52] epoch 7837, training loss: 5047.20, average training loss: 5493.97, base loss: 15959.25
[INFO 2017-06-30 08:04:34,409 main.py:52] epoch 7838, training loss: 5467.55, average training loss: 5494.04, base loss: 15959.15
[INFO 2017-06-30 08:04:37,545 main.py:52] epoch 7839, training loss: 5718.57, average training loss: 5494.45, base loss: 15959.47
[INFO 2017-06-30 08:04:40,699 main.py:52] epoch 7840, training loss: 5535.95, average training loss: 5494.74, base loss: 15959.31
[INFO 2017-06-30 08:04:43,839 main.py:52] epoch 7841, training loss: 6056.77, average training loss: 5495.18, base loss: 15959.80
[INFO 2017-06-30 08:04:46,996 main.py:52] epoch 7842, training loss: 5206.08, average training loss: 5494.93, base loss: 15959.64
[INFO 2017-06-30 08:04:50,135 main.py:52] epoch 7843, training loss: 5764.93, average training loss: 5495.45, base loss: 15959.87
[INFO 2017-06-30 08:04:53,279 main.py:52] epoch 7844, training loss: 5119.29, average training loss: 5495.26, base loss: 15959.67
[INFO 2017-06-30 08:04:56,429 main.py:52] epoch 7845, training loss: 5537.89, average training loss: 5494.73, base loss: 15960.10
[INFO 2017-06-30 08:04:59,562 main.py:52] epoch 7846, training loss: 5237.71, average training loss: 5494.17, base loss: 15960.01
[INFO 2017-06-30 08:05:02,734 main.py:52] epoch 7847, training loss: 5173.63, average training loss: 5493.73, base loss: 15959.63
[INFO 2017-06-30 08:05:05,828 main.py:52] epoch 7848, training loss: 5248.28, average training loss: 5493.00, base loss: 15959.70
[INFO 2017-06-30 08:05:08,967 main.py:52] epoch 7849, training loss: 5187.87, average training loss: 5492.92, base loss: 15959.45
[INFO 2017-06-30 08:05:12,119 main.py:52] epoch 7850, training loss: 5210.22, average training loss: 5492.90, base loss: 15959.50
[INFO 2017-06-30 08:05:15,258 main.py:52] epoch 7851, training loss: 5073.08, average training loss: 5492.43, base loss: 15959.79
[INFO 2017-06-30 08:05:18,359 main.py:52] epoch 7852, training loss: 5642.19, average training loss: 5492.33, base loss: 15960.02
[INFO 2017-06-30 08:05:21,477 main.py:52] epoch 7853, training loss: 5225.24, average training loss: 5491.86, base loss: 15960.14
[INFO 2017-06-30 08:05:24,611 main.py:52] epoch 7854, training loss: 5796.44, average training loss: 5492.18, base loss: 15960.30
[INFO 2017-06-30 08:05:27,743 main.py:52] epoch 7855, training loss: 5514.99, average training loss: 5492.13, base loss: 15960.48
[INFO 2017-06-30 08:05:30,865 main.py:52] epoch 7856, training loss: 5468.87, average training loss: 5491.88, base loss: 15960.82
[INFO 2017-06-30 08:05:34,027 main.py:52] epoch 7857, training loss: 5491.96, average training loss: 5492.03, base loss: 15960.79
[INFO 2017-06-30 08:05:37,184 main.py:52] epoch 7858, training loss: 5544.23, average training loss: 5491.90, base loss: 15960.97
[INFO 2017-06-30 08:05:40,371 main.py:52] epoch 7859, training loss: 5622.15, average training loss: 5491.91, base loss: 15961.15
[INFO 2017-06-30 08:05:43,536 main.py:52] epoch 7860, training loss: 5013.51, average training loss: 5491.49, base loss: 15960.69
[INFO 2017-06-30 08:05:46,653 main.py:52] epoch 7861, training loss: 5484.65, average training loss: 5491.48, base loss: 15960.65
[INFO 2017-06-30 08:05:49,850 main.py:52] epoch 7862, training loss: 5211.67, average training loss: 5490.90, base loss: 15960.47
[INFO 2017-06-30 08:05:52,965 main.py:52] epoch 7863, training loss: 5494.15, average training loss: 5490.66, base loss: 15960.16
[INFO 2017-06-30 08:05:56,104 main.py:52] epoch 7864, training loss: 5230.88, average training loss: 5490.09, base loss: 15959.65
[INFO 2017-06-30 08:05:59,260 main.py:52] epoch 7865, training loss: 5232.95, average training loss: 5489.82, base loss: 15959.52
[INFO 2017-06-30 08:06:02,423 main.py:52] epoch 7866, training loss: 5554.79, average training loss: 5489.99, base loss: 15959.58
[INFO 2017-06-30 08:06:05,566 main.py:52] epoch 7867, training loss: 5775.54, average training loss: 5489.87, base loss: 15959.68
[INFO 2017-06-30 08:06:08,720 main.py:52] epoch 7868, training loss: 5017.31, average training loss: 5489.50, base loss: 15959.16
[INFO 2017-06-30 08:06:11,850 main.py:52] epoch 7869, training loss: 5597.27, average training loss: 5489.40, base loss: 15959.12
[INFO 2017-06-30 08:06:14,960 main.py:52] epoch 7870, training loss: 5520.36, average training loss: 5489.03, base loss: 15959.01
[INFO 2017-06-30 08:06:18,097 main.py:52] epoch 7871, training loss: 5462.31, average training loss: 5488.94, base loss: 15958.80
[INFO 2017-06-30 08:06:21,233 main.py:52] epoch 7872, training loss: 5529.59, average training loss: 5488.79, base loss: 15958.45
[INFO 2017-06-30 08:06:24,377 main.py:52] epoch 7873, training loss: 5045.52, average training loss: 5488.32, base loss: 15958.03
[INFO 2017-06-30 08:06:27,525 main.py:52] epoch 7874, training loss: 5485.60, average training loss: 5488.71, base loss: 15958.28
[INFO 2017-06-30 08:06:30,651 main.py:52] epoch 7875, training loss: 5773.31, average training loss: 5488.96, base loss: 15958.79
[INFO 2017-06-30 08:06:33,798 main.py:52] epoch 7876, training loss: 5538.77, average training loss: 5488.67, base loss: 15959.25
[INFO 2017-06-30 08:06:36,919 main.py:52] epoch 7877, training loss: 5506.65, average training loss: 5488.82, base loss: 15959.53
[INFO 2017-06-30 08:06:40,089 main.py:52] epoch 7878, training loss: 5489.59, average training loss: 5488.58, base loss: 15959.98
[INFO 2017-06-30 08:06:43,236 main.py:52] epoch 7879, training loss: 5055.63, average training loss: 5488.28, base loss: 15959.65
[INFO 2017-06-30 08:06:46,365 main.py:52] epoch 7880, training loss: 5535.16, average training loss: 5488.14, base loss: 15959.80
[INFO 2017-06-30 08:06:49,490 main.py:52] epoch 7881, training loss: 5703.26, average training loss: 5488.20, base loss: 15959.72
[INFO 2017-06-30 08:06:52,609 main.py:52] epoch 7882, training loss: 5585.43, average training loss: 5488.02, base loss: 15959.92
[INFO 2017-06-30 08:06:55,717 main.py:52] epoch 7883, training loss: 5234.09, average training loss: 5488.00, base loss: 15959.82
[INFO 2017-06-30 08:06:58,904 main.py:52] epoch 7884, training loss: 5935.05, average training loss: 5488.11, base loss: 15960.61
[INFO 2017-06-30 08:07:02,076 main.py:52] epoch 7885, training loss: 5953.36, average training loss: 5488.60, base loss: 15961.42
[INFO 2017-06-30 08:07:05,220 main.py:52] epoch 7886, training loss: 5240.79, average training loss: 5488.22, base loss: 15961.28
[INFO 2017-06-30 08:07:08,370 main.py:52] epoch 7887, training loss: 5137.24, average training loss: 5487.97, base loss: 15960.98
[INFO 2017-06-30 08:07:11,480 main.py:52] epoch 7888, training loss: 5942.19, average training loss: 5488.21, base loss: 15961.77
[INFO 2017-06-30 08:07:14,588 main.py:52] epoch 7889, training loss: 5548.04, average training loss: 5488.18, base loss: 15961.49
[INFO 2017-06-30 08:07:17,723 main.py:52] epoch 7890, training loss: 5237.60, average training loss: 5488.02, base loss: 15961.45
[INFO 2017-06-30 08:07:20,861 main.py:52] epoch 7891, training loss: 5392.21, average training loss: 5487.81, base loss: 15961.52
[INFO 2017-06-30 08:07:24,003 main.py:52] epoch 7892, training loss: 5317.64, average training loss: 5487.33, base loss: 15961.59
[INFO 2017-06-30 08:07:27,149 main.py:52] epoch 7893, training loss: 5614.97, average training loss: 5487.30, base loss: 15961.92
[INFO 2017-06-30 08:07:30,304 main.py:52] epoch 7894, training loss: 5392.65, average training loss: 5487.33, base loss: 15961.93
[INFO 2017-06-30 08:07:33,449 main.py:52] epoch 7895, training loss: 5345.19, average training loss: 5487.66, base loss: 15961.78
[INFO 2017-06-30 08:07:36,622 main.py:52] epoch 7896, training loss: 5388.72, average training loss: 5487.14, base loss: 15961.34
[INFO 2017-06-30 08:07:39,807 main.py:52] epoch 7897, training loss: 5578.14, average training loss: 5487.07, base loss: 15961.09
[INFO 2017-06-30 08:07:42,986 main.py:52] epoch 7898, training loss: 5186.23, average training loss: 5487.14, base loss: 15960.81
[INFO 2017-06-30 08:07:46,130 main.py:52] epoch 7899, training loss: 5140.39, average training loss: 5486.80, base loss: 15960.92
[INFO 2017-06-30 08:07:46,131 main.py:54] epoch 7899, testing
[INFO 2017-06-30 08:07:59,215 main.py:97] average testing loss: 5320.01, base loss: 15588.77
[INFO 2017-06-30 08:07:59,215 main.py:98] improve_loss: 10268.75, improve_percent: 0.66
[INFO 2017-06-30 08:07:59,216 main.py:66] current best improved percent: 0.66
[INFO 2017-06-30 08:08:02,389 main.py:52] epoch 7900, training loss: 5120.50, average training loss: 5486.79, base loss: 15960.51
[INFO 2017-06-30 08:08:05,566 main.py:52] epoch 7901, training loss: 5587.09, average training loss: 5486.48, base loss: 15960.51
[INFO 2017-06-30 08:08:08,698 main.py:52] epoch 7902, training loss: 5082.17, average training loss: 5485.91, base loss: 15960.44
[INFO 2017-06-30 08:08:11,821 main.py:52] epoch 7903, training loss: 5964.10, average training loss: 5486.45, base loss: 15960.73
[INFO 2017-06-30 08:08:15,014 main.py:52] epoch 7904, training loss: 5637.44, average training loss: 5486.58, base loss: 15960.84
[INFO 2017-06-30 08:08:18,154 main.py:52] epoch 7905, training loss: 5217.04, average training loss: 5486.58, base loss: 15960.61
[INFO 2017-06-30 08:08:21,287 main.py:52] epoch 7906, training loss: 5379.04, average training loss: 5486.14, base loss: 15960.93
[INFO 2017-06-30 08:08:24,480 main.py:52] epoch 7907, training loss: 5493.06, average training loss: 5486.36, base loss: 15961.43
[INFO 2017-06-30 08:08:27,684 main.py:52] epoch 7908, training loss: 5616.64, average training loss: 5486.37, base loss: 15961.46
[INFO 2017-06-30 08:08:30,815 main.py:52] epoch 7909, training loss: 5689.10, average training loss: 5486.66, base loss: 15961.60
[INFO 2017-06-30 08:08:33,941 main.py:52] epoch 7910, training loss: 5219.79, average training loss: 5486.17, base loss: 15961.49
[INFO 2017-06-30 08:08:37,077 main.py:52] epoch 7911, training loss: 5214.73, average training loss: 5485.79, base loss: 15961.03
[INFO 2017-06-30 08:08:40,192 main.py:52] epoch 7912, training loss: 5635.82, average training loss: 5486.06, base loss: 15960.97
[INFO 2017-06-30 08:08:43,352 main.py:52] epoch 7913, training loss: 5282.62, average training loss: 5486.00, base loss: 15960.49
[INFO 2017-06-30 08:08:46,507 main.py:52] epoch 7914, training loss: 5570.66, average training loss: 5486.02, base loss: 15960.75
[INFO 2017-06-30 08:08:49,698 main.py:52] epoch 7915, training loss: 5367.35, average training loss: 5486.06, base loss: 15960.94
[INFO 2017-06-30 08:08:52,840 main.py:52] epoch 7916, training loss: 5528.39, average training loss: 5486.03, base loss: 15960.87
[INFO 2017-06-30 08:08:56,006 main.py:52] epoch 7917, training loss: 5777.63, average training loss: 5486.48, base loss: 15961.35
[INFO 2017-06-30 08:08:59,135 main.py:52] epoch 7918, training loss: 5698.33, average training loss: 5486.63, base loss: 15961.40
[INFO 2017-06-30 08:09:02,263 main.py:52] epoch 7919, training loss: 5577.74, average training loss: 5486.52, base loss: 15961.35
[INFO 2017-06-30 08:09:05,461 main.py:52] epoch 7920, training loss: 5523.37, average training loss: 5486.22, base loss: 15961.28
[INFO 2017-06-30 08:09:08,609 main.py:52] epoch 7921, training loss: 5693.08, average training loss: 5486.60, base loss: 15961.46
[INFO 2017-06-30 08:09:11,706 main.py:52] epoch 7922, training loss: 5324.56, average training loss: 5486.29, base loss: 15961.55
[INFO 2017-06-30 08:09:14,901 main.py:52] epoch 7923, training loss: 5355.41, average training loss: 5485.93, base loss: 15961.91
[INFO 2017-06-30 08:09:18,040 main.py:52] epoch 7924, training loss: 5710.58, average training loss: 5486.36, base loss: 15961.61
[INFO 2017-06-30 08:09:21,209 main.py:52] epoch 7925, training loss: 5209.99, average training loss: 5485.76, base loss: 15961.23
[INFO 2017-06-30 08:09:24,358 main.py:52] epoch 7926, training loss: 5431.64, average training loss: 5485.58, base loss: 15961.25
[INFO 2017-06-30 08:09:27,504 main.py:52] epoch 7927, training loss: 5426.98, average training loss: 5485.94, base loss: 15961.36
[INFO 2017-06-30 08:09:30,668 main.py:52] epoch 7928, training loss: 5426.62, average training loss: 5485.92, base loss: 15961.39
[INFO 2017-06-30 08:09:33,790 main.py:52] epoch 7929, training loss: 5458.99, average training loss: 5485.67, base loss: 15961.16
[INFO 2017-06-30 08:09:36,918 main.py:52] epoch 7930, training loss: 5387.10, average training loss: 5485.84, base loss: 15961.31
[INFO 2017-06-30 08:09:40,093 main.py:52] epoch 7931, training loss: 5205.39, average training loss: 5485.53, base loss: 15961.47
[INFO 2017-06-30 08:09:43,250 main.py:52] epoch 7932, training loss: 5386.62, average training loss: 5485.47, base loss: 15961.08
[INFO 2017-06-30 08:09:46,383 main.py:52] epoch 7933, training loss: 5534.12, average training loss: 5485.54, base loss: 15960.67
[INFO 2017-06-30 08:09:49,530 main.py:52] epoch 7934, training loss: 5799.55, average training loss: 5486.11, base loss: 15960.89
[INFO 2017-06-30 08:09:52,660 main.py:52] epoch 7935, training loss: 5539.79, average training loss: 5486.05, base loss: 15961.25
[INFO 2017-06-30 08:09:55,794 main.py:52] epoch 7936, training loss: 5027.45, average training loss: 5485.40, base loss: 15961.13
[INFO 2017-06-30 08:09:58,943 main.py:52] epoch 7937, training loss: 5408.65, average training loss: 5485.45, base loss: 15961.28
[INFO 2017-06-30 08:10:02,093 main.py:52] epoch 7938, training loss: 5327.95, average training loss: 5485.42, base loss: 15961.17
[INFO 2017-06-30 08:10:05,208 main.py:52] epoch 7939, training loss: 5611.16, average training loss: 5485.42, base loss: 15961.24
[INFO 2017-06-30 08:10:08,327 main.py:52] epoch 7940, training loss: 5307.00, average training loss: 5485.19, base loss: 15961.37
[INFO 2017-06-30 08:10:11,451 main.py:52] epoch 7941, training loss: 5497.93, average training loss: 5485.45, base loss: 15961.69
[INFO 2017-06-30 08:10:14,597 main.py:52] epoch 7942, training loss: 5425.22, average training loss: 5485.36, base loss: 15961.92
[INFO 2017-06-30 08:10:17,735 main.py:52] epoch 7943, training loss: 5619.00, average training loss: 5485.37, base loss: 15961.97
[INFO 2017-06-30 08:10:20,853 main.py:52] epoch 7944, training loss: 5296.34, average training loss: 5484.95, base loss: 15961.78
[INFO 2017-06-30 08:10:23,996 main.py:52] epoch 7945, training loss: 5028.69, average training loss: 5484.46, base loss: 15961.45
[INFO 2017-06-30 08:10:27,175 main.py:52] epoch 7946, training loss: 5262.75, average training loss: 5484.34, base loss: 15961.47
[INFO 2017-06-30 08:10:30,311 main.py:52] epoch 7947, training loss: 5262.40, average training loss: 5483.85, base loss: 15961.16
[INFO 2017-06-30 08:10:33,480 main.py:52] epoch 7948, training loss: 5076.73, average training loss: 5483.12, base loss: 15960.96
[INFO 2017-06-30 08:10:36,621 main.py:52] epoch 7949, training loss: 5934.60, average training loss: 5483.79, base loss: 15961.23
[INFO 2017-06-30 08:10:39,786 main.py:52] epoch 7950, training loss: 5275.78, average training loss: 5483.59, base loss: 15960.92
[INFO 2017-06-30 08:10:42,937 main.py:52] epoch 7951, training loss: 5202.17, average training loss: 5483.61, base loss: 15960.90
[INFO 2017-06-30 08:10:46,083 main.py:52] epoch 7952, training loss: 5665.10, average training loss: 5483.75, base loss: 15961.49
[INFO 2017-06-30 08:10:49,222 main.py:52] epoch 7953, training loss: 5700.98, average training loss: 5483.78, base loss: 15961.52
[INFO 2017-06-30 08:10:52,356 main.py:52] epoch 7954, training loss: 5433.22, average training loss: 5483.70, base loss: 15961.94
[INFO 2017-06-30 08:10:55,456 main.py:52] epoch 7955, training loss: 5178.09, average training loss: 5483.53, base loss: 15961.86
[INFO 2017-06-30 08:10:58,643 main.py:52] epoch 7956, training loss: 5787.76, average training loss: 5483.99, base loss: 15962.25
[INFO 2017-06-30 08:11:01,791 main.py:52] epoch 7957, training loss: 5274.07, average training loss: 5483.52, base loss: 15962.44
[INFO 2017-06-30 08:11:04,922 main.py:52] epoch 7958, training loss: 5375.33, average training loss: 5483.48, base loss: 15962.40
[INFO 2017-06-30 08:11:08,068 main.py:52] epoch 7959, training loss: 5383.12, average training loss: 5483.07, base loss: 15962.25
[INFO 2017-06-30 08:11:11,208 main.py:52] epoch 7960, training loss: 5577.53, average training loss: 5483.21, base loss: 15962.18
[INFO 2017-06-30 08:11:14,395 main.py:52] epoch 7961, training loss: 5253.47, average training loss: 5483.38, base loss: 15962.04
[INFO 2017-06-30 08:11:17,512 main.py:52] epoch 7962, training loss: 5469.94, average training loss: 5483.57, base loss: 15962.27
[INFO 2017-06-30 08:11:20,610 main.py:52] epoch 7963, training loss: 5375.00, average training loss: 5483.59, base loss: 15962.16
[INFO 2017-06-30 08:11:23,765 main.py:52] epoch 7964, training loss: 5559.38, average training loss: 5483.79, base loss: 15962.31
[INFO 2017-06-30 08:11:26,940 main.py:52] epoch 7965, training loss: 5606.54, average training loss: 5484.17, base loss: 15962.57
[INFO 2017-06-30 08:11:30,080 main.py:52] epoch 7966, training loss: 5322.39, average training loss: 5483.75, base loss: 15962.37
[INFO 2017-06-30 08:11:33,240 main.py:52] epoch 7967, training loss: 5151.99, average training loss: 5483.37, base loss: 15962.35
[INFO 2017-06-30 08:11:36,454 main.py:52] epoch 7968, training loss: 5555.95, average training loss: 5483.53, base loss: 15962.09
[INFO 2017-06-30 08:11:39,579 main.py:52] epoch 7969, training loss: 5808.29, average training loss: 5483.86, base loss: 15962.03
[INFO 2017-06-30 08:11:42,770 main.py:52] epoch 7970, training loss: 5148.19, average training loss: 5483.56, base loss: 15961.82
[INFO 2017-06-30 08:11:45,914 main.py:52] epoch 7971, training loss: 5355.26, average training loss: 5483.17, base loss: 15962.07
[INFO 2017-06-30 08:11:49,064 main.py:52] epoch 7972, training loss: 5185.25, average training loss: 5482.89, base loss: 15962.12
[INFO 2017-06-30 08:11:52,212 main.py:52] epoch 7973, training loss: 5571.02, average training loss: 5482.79, base loss: 15962.73
[INFO 2017-06-30 08:11:55,337 main.py:52] epoch 7974, training loss: 5456.24, average training loss: 5482.96, base loss: 15962.95
[INFO 2017-06-30 08:11:58,457 main.py:52] epoch 7975, training loss: 5394.60, average training loss: 5482.88, base loss: 15963.00
[INFO 2017-06-30 08:12:01,584 main.py:52] epoch 7976, training loss: 5631.20, average training loss: 5482.90, base loss: 15963.01
[INFO 2017-06-30 08:12:04,719 main.py:52] epoch 7977, training loss: 5230.73, average training loss: 5482.55, base loss: 15962.92
[INFO 2017-06-30 08:12:07,853 main.py:52] epoch 7978, training loss: 5217.01, average training loss: 5482.22, base loss: 15962.36
[INFO 2017-06-30 08:12:10,974 main.py:52] epoch 7979, training loss: 5471.43, average training loss: 5482.17, base loss: 15962.41
[INFO 2017-06-30 08:12:14,074 main.py:52] epoch 7980, training loss: 5327.56, average training loss: 5482.01, base loss: 15961.97
[INFO 2017-06-30 08:12:17,194 main.py:52] epoch 7981, training loss: 5818.04, average training loss: 5482.27, base loss: 15962.13
[INFO 2017-06-30 08:12:20,319 main.py:52] epoch 7982, training loss: 5310.85, average training loss: 5482.23, base loss: 15962.42
[INFO 2017-06-30 08:12:23,479 main.py:52] epoch 7983, training loss: 5218.80, average training loss: 5482.10, base loss: 15962.25
[INFO 2017-06-30 08:12:26,637 main.py:52] epoch 7984, training loss: 5511.52, average training loss: 5482.32, base loss: 15962.06
[INFO 2017-06-30 08:12:29,789 main.py:52] epoch 7985, training loss: 5157.75, average training loss: 5482.17, base loss: 15961.61
[INFO 2017-06-30 08:12:32,921 main.py:52] epoch 7986, training loss: 5281.25, average training loss: 5482.19, base loss: 15961.34
[INFO 2017-06-30 08:12:36,059 main.py:52] epoch 7987, training loss: 5421.48, average training loss: 5482.30, base loss: 15961.19
[INFO 2017-06-30 08:12:39,252 main.py:52] epoch 7988, training loss: 5391.78, average training loss: 5482.56, base loss: 15960.91
[INFO 2017-06-30 08:12:42,333 main.py:52] epoch 7989, training loss: 5675.58, average training loss: 5482.66, base loss: 15960.94
[INFO 2017-06-30 08:12:45,451 main.py:52] epoch 7990, training loss: 5629.29, average training loss: 5482.89, base loss: 15961.22
[INFO 2017-06-30 08:12:48,640 main.py:52] epoch 7991, training loss: 5692.61, average training loss: 5483.34, base loss: 15961.09
[INFO 2017-06-30 08:12:51,795 main.py:52] epoch 7992, training loss: 5458.05, average training loss: 5483.36, base loss: 15961.28
[INFO 2017-06-30 08:12:54,912 main.py:52] epoch 7993, training loss: 5329.60, average training loss: 5483.23, base loss: 15960.98
[INFO 2017-06-30 08:12:58,017 main.py:52] epoch 7994, training loss: 5386.82, average training loss: 5482.89, base loss: 15961.05
[INFO 2017-06-30 08:13:01,197 main.py:52] epoch 7995, training loss: 5334.49, average training loss: 5482.46, base loss: 15961.52
[INFO 2017-06-30 08:13:04,354 main.py:52] epoch 7996, training loss: 5240.15, average training loss: 5482.48, base loss: 15961.16
[INFO 2017-06-30 08:13:07,467 main.py:52] epoch 7997, training loss: 5631.26, average training loss: 5482.59, base loss: 15961.25
[INFO 2017-06-30 08:13:10,630 main.py:52] epoch 7998, training loss: 5265.93, average training loss: 5481.92, base loss: 15961.22
[INFO 2017-06-30 08:13:13,790 main.py:52] epoch 7999, training loss: 5234.59, average training loss: 5481.87, base loss: 15961.16
[INFO 2017-06-30 08:13:13,791 main.py:54] epoch 7999, testing
[INFO 2017-06-30 08:13:26,791 main.py:97] average testing loss: 5455.57, base loss: 16034.01
[INFO 2017-06-30 08:13:26,791 main.py:98] improve_loss: 10578.45, improve_percent: 0.66
[INFO 2017-06-30 08:13:26,793 main.py:66] current best improved percent: 0.66
[INFO 2017-06-30 08:13:29,944 main.py:52] epoch 8000, training loss: 5331.43, average training loss: 5481.94, base loss: 15961.38
[INFO 2017-06-30 08:13:33,073 main.py:52] epoch 8001, training loss: 5402.12, average training loss: 5481.62, base loss: 15961.37
[INFO 2017-06-30 08:13:36,226 main.py:52] epoch 8002, training loss: 5485.98, average training loss: 5481.43, base loss: 15961.56
[INFO 2017-06-30 08:13:39,350 main.py:52] epoch 8003, training loss: 5498.19, average training loss: 5481.33, base loss: 15962.07
[INFO 2017-06-30 08:13:42,505 main.py:52] epoch 8004, training loss: 5491.18, average training loss: 5480.63, base loss: 15961.95
[INFO 2017-06-30 08:13:45,629 main.py:52] epoch 8005, training loss: 5246.05, average training loss: 5480.12, base loss: 15961.98
[INFO 2017-06-30 08:13:48,788 main.py:52] epoch 8006, training loss: 5408.36, average training loss: 5479.73, base loss: 15962.31
[INFO 2017-06-30 08:13:51,931 main.py:52] epoch 8007, training loss: 5473.34, average training loss: 5479.72, base loss: 15962.04
[INFO 2017-06-30 08:13:55,038 main.py:52] epoch 8008, training loss: 5151.47, average training loss: 5479.35, base loss: 15961.49
[INFO 2017-06-30 08:13:58,196 main.py:52] epoch 8009, training loss: 5268.85, average training loss: 5478.71, base loss: 15961.20
[INFO 2017-06-30 08:14:01,336 main.py:52] epoch 8010, training loss: 5474.95, average training loss: 5478.90, base loss: 15961.05
[INFO 2017-06-30 08:14:04,494 main.py:52] epoch 8011, training loss: 4947.44, average training loss: 5478.36, base loss: 15960.69
[INFO 2017-06-30 08:14:07,624 main.py:52] epoch 8012, training loss: 5140.31, average training loss: 5478.28, base loss: 15960.50
[INFO 2017-06-30 08:14:10,776 main.py:52] epoch 8013, training loss: 5368.46, average training loss: 5478.36, base loss: 15960.38
[INFO 2017-06-30 08:14:13,946 main.py:52] epoch 8014, training loss: 5441.12, average training loss: 5478.20, base loss: 15960.25
[INFO 2017-06-30 08:14:17,057 main.py:52] epoch 8015, training loss: 5677.84, average training loss: 5478.34, base loss: 15960.13
[INFO 2017-06-30 08:14:20,187 main.py:52] epoch 8016, training loss: 5516.25, average training loss: 5478.05, base loss: 15960.11
[INFO 2017-06-30 08:14:23,351 main.py:52] epoch 8017, training loss: 5163.20, average training loss: 5477.88, base loss: 15959.44
[INFO 2017-06-30 08:14:26,502 main.py:52] epoch 8018, training loss: 5353.47, average training loss: 5478.01, base loss: 15959.26
[INFO 2017-06-30 08:14:29,660 main.py:52] epoch 8019, training loss: 5831.97, average training loss: 5478.19, base loss: 15959.50
[INFO 2017-06-30 08:14:32,818 main.py:52] epoch 8020, training loss: 5242.98, average training loss: 5477.57, base loss: 15959.31
[INFO 2017-06-30 08:14:35,966 main.py:52] epoch 8021, training loss: 5013.91, average training loss: 5476.78, base loss: 15958.72
[INFO 2017-06-30 08:14:39,147 main.py:52] epoch 8022, training loss: 4996.37, average training loss: 5476.02, base loss: 15958.41
[INFO 2017-06-30 08:14:42,341 main.py:52] epoch 8023, training loss: 5683.39, average training loss: 5476.42, base loss: 15958.62
[INFO 2017-06-30 08:14:45,498 main.py:52] epoch 8024, training loss: 4967.08, average training loss: 5475.87, base loss: 15958.27
[INFO 2017-06-30 08:14:48,630 main.py:52] epoch 8025, training loss: 6186.02, average training loss: 5476.46, base loss: 15959.10
[INFO 2017-06-30 08:14:51,764 main.py:52] epoch 8026, training loss: 5516.44, average training loss: 5476.34, base loss: 15959.52
[INFO 2017-06-30 08:14:54,891 main.py:52] epoch 8027, training loss: 4854.35, average training loss: 5475.50, base loss: 15958.97
[INFO 2017-06-30 08:14:58,030 main.py:52] epoch 8028, training loss: 5565.31, average training loss: 5475.56, base loss: 15958.86
[INFO 2017-06-30 08:15:01,192 main.py:52] epoch 8029, training loss: 5517.04, average training loss: 5475.64, base loss: 15959.13
[INFO 2017-06-30 08:15:04,324 main.py:52] epoch 8030, training loss: 5225.54, average training loss: 5475.75, base loss: 15958.94
[INFO 2017-06-30 08:15:07,488 main.py:52] epoch 8031, training loss: 4958.75, average training loss: 5475.00, base loss: 15958.55
[INFO 2017-06-30 08:15:10,618 main.py:52] epoch 8032, training loss: 5551.96, average training loss: 5475.25, base loss: 15958.91
[INFO 2017-06-30 08:15:13,772 main.py:52] epoch 8033, training loss: 5184.70, average training loss: 5474.81, base loss: 15959.19
[INFO 2017-06-30 08:15:16,971 main.py:52] epoch 8034, training loss: 5657.10, average training loss: 5474.87, base loss: 15959.69
[INFO 2017-06-30 08:15:20,151 main.py:52] epoch 8035, training loss: 5269.73, average training loss: 5474.94, base loss: 15959.57
[INFO 2017-06-30 08:15:23,308 main.py:52] epoch 8036, training loss: 5105.99, average training loss: 5474.60, base loss: 15959.37
[INFO 2017-06-30 08:15:26,442 main.py:52] epoch 8037, training loss: 5769.85, average training loss: 5474.83, base loss: 15959.67
[INFO 2017-06-30 08:15:29,588 main.py:52] epoch 8038, training loss: 5665.76, average training loss: 5475.17, base loss: 15960.09
[INFO 2017-06-30 08:15:32,682 main.py:52] epoch 8039, training loss: 5569.49, average training loss: 5475.12, base loss: 15960.34
[INFO 2017-06-30 08:15:35,820 main.py:52] epoch 8040, training loss: 5427.73, average training loss: 5474.80, base loss: 15960.26
[INFO 2017-06-30 08:15:38,980 main.py:52] epoch 8041, training loss: 5393.50, average training loss: 5474.80, base loss: 15960.50
[INFO 2017-06-30 08:15:42,166 main.py:52] epoch 8042, training loss: 5487.65, average training loss: 5474.92, base loss: 15960.62
[INFO 2017-06-30 08:15:45,310 main.py:52] epoch 8043, training loss: 5112.15, average training loss: 5474.52, base loss: 15960.58
[INFO 2017-06-30 08:15:48,487 main.py:52] epoch 8044, training loss: 5290.95, average training loss: 5474.01, base loss: 15960.00
[INFO 2017-06-30 08:15:51,627 main.py:52] epoch 8045, training loss: 5390.18, average training loss: 5473.70, base loss: 15959.84
[INFO 2017-06-30 08:15:54,767 main.py:52] epoch 8046, training loss: 5561.11, average training loss: 5473.86, base loss: 15959.71
[INFO 2017-06-30 08:15:57,904 main.py:52] epoch 8047, training loss: 5150.30, average training loss: 5473.17, base loss: 15959.08
[INFO 2017-06-30 08:16:01,038 main.py:52] epoch 8048, training loss: 5487.98, average training loss: 5472.58, base loss: 15959.23
[INFO 2017-06-30 08:16:04,173 main.py:52] epoch 8049, training loss: 5539.67, average training loss: 5472.98, base loss: 15959.45
[INFO 2017-06-30 08:16:07,284 main.py:52] epoch 8050, training loss: 5413.10, average training loss: 5472.78, base loss: 15959.29
[INFO 2017-06-30 08:16:10,410 main.py:52] epoch 8051, training loss: 4979.96, average training loss: 5472.32, base loss: 15958.62
[INFO 2017-06-30 08:16:13,557 main.py:52] epoch 8052, training loss: 5237.10, average training loss: 5472.19, base loss: 15958.40
[INFO 2017-06-30 08:16:16,736 main.py:52] epoch 8053, training loss: 5559.25, average training loss: 5472.06, base loss: 15958.12
[INFO 2017-06-30 08:16:19,889 main.py:52] epoch 8054, training loss: 5407.45, average training loss: 5472.02, base loss: 15957.95
[INFO 2017-06-30 08:16:23,058 main.py:52] epoch 8055, training loss: 5314.71, average training loss: 5472.05, base loss: 15957.71
[INFO 2017-06-30 08:16:26,244 main.py:52] epoch 8056, training loss: 5627.12, average training loss: 5471.98, base loss: 15957.61
[INFO 2017-06-30 08:16:29,375 main.py:52] epoch 8057, training loss: 5750.60, average training loss: 5472.04, base loss: 15957.85
[INFO 2017-06-30 08:16:32,475 main.py:52] epoch 8058, training loss: 5399.61, average training loss: 5471.93, base loss: 15958.03
[INFO 2017-06-30 08:16:35,608 main.py:52] epoch 8059, training loss: 5661.11, average training loss: 5471.79, base loss: 15958.38
[INFO 2017-06-30 08:16:38,823 main.py:52] epoch 8060, training loss: 5270.43, average training loss: 5472.07, base loss: 15958.50
[INFO 2017-06-30 08:16:41,958 main.py:52] epoch 8061, training loss: 5661.58, average training loss: 5471.69, base loss: 15958.84
[INFO 2017-06-30 08:16:45,100 main.py:52] epoch 8062, training loss: 5344.91, average training loss: 5471.12, base loss: 15958.95
[INFO 2017-06-30 08:16:48,256 main.py:52] epoch 8063, training loss: 5461.48, average training loss: 5471.29, base loss: 15959.17
[INFO 2017-06-30 08:16:51,415 main.py:52] epoch 8064, training loss: 5463.11, average training loss: 5471.34, base loss: 15958.95
[INFO 2017-06-30 08:16:54,562 main.py:52] epoch 8065, training loss: 5545.12, average training loss: 5471.43, base loss: 15958.91
[INFO 2017-06-30 08:16:57,745 main.py:52] epoch 8066, training loss: 4922.70, average training loss: 5471.23, base loss: 15958.75
[INFO 2017-06-30 08:17:00,889 main.py:52] epoch 8067, training loss: 5677.18, average training loss: 5471.18, base loss: 15958.97
[INFO 2017-06-30 08:17:04,035 main.py:52] epoch 8068, training loss: 5570.48, average training loss: 5471.39, base loss: 15959.58
[INFO 2017-06-30 08:17:07,172 main.py:52] epoch 8069, training loss: 5419.08, average training loss: 5471.15, base loss: 15959.59
[INFO 2017-06-30 08:17:10,268 main.py:52] epoch 8070, training loss: 5643.11, average training loss: 5471.37, base loss: 15959.55
[INFO 2017-06-30 08:17:13,392 main.py:52] epoch 8071, training loss: 5137.94, average training loss: 5470.96, base loss: 15958.99
[INFO 2017-06-30 08:17:16,519 main.py:52] epoch 8072, training loss: 5395.05, average training loss: 5470.81, base loss: 15958.91
[INFO 2017-06-30 08:17:19,644 main.py:52] epoch 8073, training loss: 5953.49, average training loss: 5470.83, base loss: 15959.04
[INFO 2017-06-30 08:17:22,814 main.py:52] epoch 8074, training loss: 5822.78, average training loss: 5471.07, base loss: 15959.35
[INFO 2017-06-30 08:17:25,962 main.py:52] epoch 8075, training loss: 5167.13, average training loss: 5470.83, base loss: 15959.07
[INFO 2017-06-30 08:17:29,112 main.py:52] epoch 8076, training loss: 5254.93, average training loss: 5470.26, base loss: 15958.65
[INFO 2017-06-30 08:17:32,243 main.py:52] epoch 8077, training loss: 6011.91, average training loss: 5470.85, base loss: 15958.89
[INFO 2017-06-30 08:17:35,394 main.py:52] epoch 8078, training loss: 5403.18, average training loss: 5470.85, base loss: 15959.20
[INFO 2017-06-30 08:17:38,535 main.py:52] epoch 8079, training loss: 5374.22, average training loss: 5470.76, base loss: 15959.47
[INFO 2017-06-30 08:17:41,658 main.py:52] epoch 8080, training loss: 5584.63, average training loss: 5471.01, base loss: 15959.86
[INFO 2017-06-30 08:17:44,801 main.py:52] epoch 8081, training loss: 5518.68, average training loss: 5471.43, base loss: 15959.99
[INFO 2017-06-30 08:17:47,933 main.py:52] epoch 8082, training loss: 5826.03, average training loss: 5471.51, base loss: 15960.26
[INFO 2017-06-30 08:17:51,083 main.py:52] epoch 8083, training loss: 5586.40, average training loss: 5471.62, base loss: 15960.74
[INFO 2017-06-30 08:17:54,213 main.py:52] epoch 8084, training loss: 5347.12, average training loss: 5471.09, base loss: 15960.99
[INFO 2017-06-30 08:17:57,324 main.py:52] epoch 8085, training loss: 5161.65, average training loss: 5470.81, base loss: 15961.08
[INFO 2017-06-30 08:18:00,478 main.py:52] epoch 8086, training loss: 5466.79, average training loss: 5470.62, base loss: 15961.15
[INFO 2017-06-30 08:18:03,609 main.py:52] epoch 8087, training loss: 5663.94, average training loss: 5470.94, base loss: 15961.11
[INFO 2017-06-30 08:18:06,734 main.py:52] epoch 8088, training loss: 5810.97, average training loss: 5471.29, base loss: 15961.44
[INFO 2017-06-30 08:18:09,858 main.py:52] epoch 8089, training loss: 5558.41, average training loss: 5471.07, base loss: 15961.39
[INFO 2017-06-30 08:18:13,032 main.py:52] epoch 8090, training loss: 5446.34, average training loss: 5470.62, base loss: 15961.55
[INFO 2017-06-30 08:18:16,127 main.py:52] epoch 8091, training loss: 5238.98, average training loss: 5470.78, base loss: 15961.63
[INFO 2017-06-30 08:18:19,277 main.py:52] epoch 8092, training loss: 5454.54, average training loss: 5470.87, base loss: 15961.21
[INFO 2017-06-30 08:18:22,384 main.py:52] epoch 8093, training loss: 5400.94, average training loss: 5470.28, base loss: 15961.20
[INFO 2017-06-30 08:18:25,525 main.py:52] epoch 8094, training loss: 5376.19, average training loss: 5470.15, base loss: 15961.16
[INFO 2017-06-30 08:18:28,651 main.py:52] epoch 8095, training loss: 5260.52, average training loss: 5470.09, base loss: 15961.24
[INFO 2017-06-30 08:18:31,796 main.py:52] epoch 8096, training loss: 5682.71, average training loss: 5470.03, base loss: 15960.95
[INFO 2017-06-30 08:18:34,940 main.py:52] epoch 8097, training loss: 5192.50, average training loss: 5469.06, base loss: 15960.65
[INFO 2017-06-30 08:18:38,050 main.py:52] epoch 8098, training loss: 5403.26, average training loss: 5468.62, base loss: 15960.55
[INFO 2017-06-30 08:18:41,211 main.py:52] epoch 8099, training loss: 5430.47, average training loss: 5468.33, base loss: 15959.88
[INFO 2017-06-30 08:18:41,212 main.py:54] epoch 8099, testing
[INFO 2017-06-30 08:18:54,436 main.py:97] average testing loss: 5444.43, base loss: 16059.26
[INFO 2017-06-30 08:18:54,436 main.py:98] improve_loss: 10614.83, improve_percent: 0.66
[INFO 2017-06-30 08:18:54,437 main.py:62] model save to ./model/final.pth
[INFO 2017-06-30 08:18:54,472 main.py:66] current best improved percent: 0.66
[INFO 2017-06-30 08:18:57,611 main.py:52] epoch 8100, training loss: 5334.44, average training loss: 5468.25, base loss: 15960.13
[INFO 2017-06-30 08:19:00,733 main.py:52] epoch 8101, training loss: 5020.08, average training loss: 5467.58, base loss: 15959.60
[INFO 2017-06-30 08:19:03,841 main.py:52] epoch 8102, training loss: 5165.74, average training loss: 5467.14, base loss: 15958.95
[INFO 2017-06-30 08:19:07,050 main.py:52] epoch 8103, training loss: 5431.17, average training loss: 5467.39, base loss: 15958.74
[INFO 2017-06-30 08:19:10,177 main.py:52] epoch 8104, training loss: 5613.36, average training loss: 5467.72, base loss: 15958.48
[INFO 2017-06-30 08:19:13,333 main.py:52] epoch 8105, training loss: 5421.25, average training loss: 5468.02, base loss: 15958.46
[INFO 2017-06-30 08:19:16,456 main.py:52] epoch 8106, training loss: 5366.59, average training loss: 5467.69, base loss: 15958.11
[INFO 2017-06-30 08:19:19,625 main.py:52] epoch 8107, training loss: 5378.39, average training loss: 5467.46, base loss: 15957.78
[INFO 2017-06-30 08:19:22,753 main.py:52] epoch 8108, training loss: 5458.15, average training loss: 5467.44, base loss: 15957.63
[INFO 2017-06-30 08:19:25,903 main.py:52] epoch 8109, training loss: 5391.30, average training loss: 5467.66, base loss: 15958.01
[INFO 2017-06-30 08:19:29,093 main.py:52] epoch 8110, training loss: 5239.55, average training loss: 5467.43, base loss: 15958.04
[INFO 2017-06-30 08:19:32,237 main.py:52] epoch 8111, training loss: 5201.54, average training loss: 5467.13, base loss: 15958.02
[INFO 2017-06-30 08:19:35,415 main.py:52] epoch 8112, training loss: 5571.76, average training loss: 5467.07, base loss: 15958.16
[INFO 2017-06-30 08:19:38,539 main.py:52] epoch 8113, training loss: 5526.53, average training loss: 5467.31, base loss: 15958.25
[INFO 2017-06-30 08:19:41,636 main.py:52] epoch 8114, training loss: 5567.70, average training loss: 5467.17, base loss: 15958.17
[INFO 2017-06-30 08:19:44,756 main.py:52] epoch 8115, training loss: 5424.30, average training loss: 5467.01, base loss: 15958.27
[INFO 2017-06-30 08:19:47,883 main.py:52] epoch 8116, training loss: 5401.62, average training loss: 5466.81, base loss: 15958.25
[INFO 2017-06-30 08:19:50,997 main.py:52] epoch 8117, training loss: 5517.45, average training loss: 5466.70, base loss: 15958.29
[INFO 2017-06-30 08:19:54,117 main.py:52] epoch 8118, training loss: 5505.82, average training loss: 5466.44, base loss: 15958.10
[INFO 2017-06-30 08:19:57,218 main.py:52] epoch 8119, training loss: 4985.18, average training loss: 5465.91, base loss: 15957.73
[INFO 2017-06-30 08:20:00,342 main.py:52] epoch 8120, training loss: 5110.15, average training loss: 5465.69, base loss: 15957.19
[INFO 2017-06-30 08:20:03,507 main.py:52] epoch 8121, training loss: 5733.59, average training loss: 5465.60, base loss: 15957.15
[INFO 2017-06-30 08:20:06,654 main.py:52] epoch 8122, training loss: 5632.54, average training loss: 5465.76, base loss: 15957.57
[INFO 2017-06-30 08:20:09,843 main.py:52] epoch 8123, training loss: 5309.98, average training loss: 5465.45, base loss: 15957.32
[INFO 2017-06-30 08:20:12,983 main.py:52] epoch 8124, training loss: 5395.63, average training loss: 5465.46, base loss: 15957.27
[INFO 2017-06-30 08:20:16,127 main.py:52] epoch 8125, training loss: 5457.05, average training loss: 5465.01, base loss: 15957.43
[INFO 2017-06-30 08:20:19,287 main.py:52] epoch 8126, training loss: 5079.94, average training loss: 5464.78, base loss: 15957.19
[INFO 2017-06-30 08:20:22,467 main.py:52] epoch 8127, training loss: 5052.37, average training loss: 5464.11, base loss: 15957.10
[INFO 2017-06-30 08:20:25,628 main.py:52] epoch 8128, training loss: 5777.01, average training loss: 5464.45, base loss: 15957.35
[INFO 2017-06-30 08:20:28,766 main.py:52] epoch 8129, training loss: 5363.77, average training loss: 5464.34, base loss: 15957.18
[INFO 2017-06-30 08:20:31,887 main.py:52] epoch 8130, training loss: 5424.32, average training loss: 5463.75, base loss: 15957.18
[INFO 2017-06-30 08:20:35,013 main.py:52] epoch 8131, training loss: 5777.42, average training loss: 5464.17, base loss: 15957.43
[INFO 2017-06-30 08:20:38,211 main.py:52] epoch 8132, training loss: 5125.47, average training loss: 5463.71, base loss: 15957.02
[INFO 2017-06-30 08:20:41,342 main.py:52] epoch 8133, training loss: 5080.28, average training loss: 5463.47, base loss: 15956.64
[INFO 2017-06-30 08:20:44,496 main.py:52] epoch 8134, training loss: 5399.83, average training loss: 5463.10, base loss: 15956.59
[INFO 2017-06-30 08:20:47,646 main.py:52] epoch 8135, training loss: 5282.19, average training loss: 5462.68, base loss: 15956.18
[INFO 2017-06-30 08:20:50,813 main.py:52] epoch 8136, training loss: 5730.64, average training loss: 5462.94, base loss: 15956.83
[INFO 2017-06-30 08:20:53,938 main.py:52] epoch 8137, training loss: 5703.91, average training loss: 5463.37, base loss: 15957.02
[INFO 2017-06-30 08:20:57,100 main.py:52] epoch 8138, training loss: 5732.10, average training loss: 5463.53, base loss: 15957.54
[INFO 2017-06-30 08:21:00,228 main.py:52] epoch 8139, training loss: 5660.85, average training loss: 5463.82, base loss: 15957.98
[INFO 2017-06-30 08:21:03,390 main.py:52] epoch 8140, training loss: 5338.11, average training loss: 5463.48, base loss: 15957.78
[INFO 2017-06-30 08:21:06,506 main.py:52] epoch 8141, training loss: 5337.13, average training loss: 5463.06, base loss: 15957.68
[INFO 2017-06-30 08:21:09,667 main.py:52] epoch 8142, training loss: 5376.83, average training loss: 5462.93, base loss: 15957.80
[INFO 2017-06-30 08:21:12,822 main.py:52] epoch 8143, training loss: 5145.89, average training loss: 5463.11, base loss: 15957.66
[INFO 2017-06-30 08:21:15,953 main.py:52] epoch 8144, training loss: 5326.76, average training loss: 5462.91, base loss: 15957.40
[INFO 2017-06-30 08:21:19,115 main.py:52] epoch 8145, training loss: 5201.83, average training loss: 5462.71, base loss: 15957.02
[INFO 2017-06-30 08:21:22,259 main.py:52] epoch 8146, training loss: 5088.06, average training loss: 5462.29, base loss: 15956.95
[INFO 2017-06-30 08:21:25,423 main.py:52] epoch 8147, training loss: 5255.10, average training loss: 5461.75, base loss: 15957.48
[INFO 2017-06-30 08:21:28,582 main.py:52] epoch 8148, training loss: 5520.11, average training loss: 5461.67, base loss: 15957.52
[INFO 2017-06-30 08:21:31,720 main.py:52] epoch 8149, training loss: 5439.67, average training loss: 5461.28, base loss: 15957.44
[INFO 2017-06-30 08:21:34,857 main.py:52] epoch 8150, training loss: 5565.22, average training loss: 5461.22, base loss: 15957.82
[INFO 2017-06-30 08:21:38,002 main.py:52] epoch 8151, training loss: 5474.08, average training loss: 5461.18, base loss: 15958.04
[INFO 2017-06-30 08:21:41,152 main.py:52] epoch 8152, training loss: 5176.70, average training loss: 5460.89, base loss: 15957.78
[INFO 2017-06-30 08:21:44,280 main.py:52] epoch 8153, training loss: 5434.32, average training loss: 5461.29, base loss: 15957.77
[INFO 2017-06-30 08:21:47,396 main.py:52] epoch 8154, training loss: 5387.29, average training loss: 5461.62, base loss: 15957.68
[INFO 2017-06-30 08:21:50,538 main.py:52] epoch 8155, training loss: 5638.17, average training loss: 5461.38, base loss: 15957.86
[INFO 2017-06-30 08:21:53,704 main.py:52] epoch 8156, training loss: 5636.55, average training loss: 5461.63, base loss: 15957.74
[INFO 2017-06-30 08:21:56,830 main.py:52] epoch 8157, training loss: 5194.14, average training loss: 5461.00, base loss: 15957.39
[INFO 2017-06-30 08:21:59,956 main.py:52] epoch 8158, training loss: 5329.66, average training loss: 5460.92, base loss: 15956.98
[INFO 2017-06-30 08:22:03,137 main.py:52] epoch 8159, training loss: 5316.33, average training loss: 5460.65, base loss: 15956.80
[INFO 2017-06-30 08:22:06,293 main.py:52] epoch 8160, training loss: 5506.39, average training loss: 5460.60, base loss: 15956.30
[INFO 2017-06-30 08:22:09,436 main.py:52] epoch 8161, training loss: 5242.49, average training loss: 5460.44, base loss: 15955.95
[INFO 2017-06-30 08:22:12,581 main.py:52] epoch 8162, training loss: 5437.60, average training loss: 5460.57, base loss: 15955.31
[INFO 2017-06-30 08:22:15,735 main.py:52] epoch 8163, training loss: 5486.83, average training loss: 5460.43, base loss: 15954.79
[INFO 2017-06-30 08:22:18,830 main.py:52] epoch 8164, training loss: 5532.84, average training loss: 5460.87, base loss: 15955.14
[INFO 2017-06-30 08:22:21,986 main.py:52] epoch 8165, training loss: 5458.13, average training loss: 5460.86, base loss: 15955.37
[INFO 2017-06-30 08:22:25,139 main.py:52] epoch 8166, training loss: 5224.10, average training loss: 5460.99, base loss: 15955.13
[INFO 2017-06-30 08:22:28,260 main.py:52] epoch 8167, training loss: 5202.71, average training loss: 5460.06, base loss: 15955.14
[INFO 2017-06-30 08:22:31,506 main.py:52] epoch 8168, training loss: 5551.93, average training loss: 5460.64, base loss: 15955.18
[INFO 2017-06-30 08:22:34,614 main.py:52] epoch 8169, training loss: 5515.85, average training loss: 5460.63, base loss: 15954.97
[INFO 2017-06-30 08:22:37,759 main.py:52] epoch 8170, training loss: 5298.66, average training loss: 5460.38, base loss: 15954.77
[INFO 2017-06-30 08:22:40,916 main.py:52] epoch 8171, training loss: 6164.48, average training loss: 5461.29, base loss: 15955.04
[INFO 2017-06-30 08:22:44,046 main.py:52] epoch 8172, training loss: 5313.17, average training loss: 5461.13, base loss: 15954.87
[INFO 2017-06-30 08:22:47,182 main.py:52] epoch 8173, training loss: 5404.02, average training loss: 5460.96, base loss: 15954.95
[INFO 2017-06-30 08:22:50,357 main.py:52] epoch 8174, training loss: 5089.98, average training loss: 5460.81, base loss: 15954.76
[INFO 2017-06-30 08:22:53,512 main.py:52] epoch 8175, training loss: 5201.91, average training loss: 5460.46, base loss: 15954.49
[INFO 2017-06-30 08:22:56,620 main.py:52] epoch 8176, training loss: 5532.53, average training loss: 5460.31, base loss: 15954.60
[INFO 2017-06-30 08:22:59,785 main.py:52] epoch 8177, training loss: 4950.20, average training loss: 5459.23, base loss: 15954.20
[INFO 2017-06-30 08:23:02,918 main.py:52] epoch 8178, training loss: 5501.84, average training loss: 5459.30, base loss: 15954.36
[INFO 2017-06-30 08:23:06,073 main.py:52] epoch 8179, training loss: 5803.17, average training loss: 5459.55, base loss: 15954.98
[INFO 2017-06-30 08:23:09,221 main.py:52] epoch 8180, training loss: 5352.44, average training loss: 5459.70, base loss: 15955.04
[INFO 2017-06-30 08:23:12,381 main.py:52] epoch 8181, training loss: 5510.31, average training loss: 5459.57, base loss: 15955.14
[INFO 2017-06-30 08:23:15,532 main.py:52] epoch 8182, training loss: 5366.32, average training loss: 5459.44, base loss: 15955.29
[INFO 2017-06-30 08:23:18,664 main.py:52] epoch 8183, training loss: 5707.36, average training loss: 5459.62, base loss: 15955.97
[INFO 2017-06-30 08:23:21,780 main.py:52] epoch 8184, training loss: 5548.13, average training loss: 5459.53, base loss: 15955.86
[INFO 2017-06-30 08:23:24,895 main.py:52] epoch 8185, training loss: 5669.96, average training loss: 5459.19, base loss: 15956.09
[INFO 2017-06-30 08:23:28,008 main.py:52] epoch 8186, training loss: 5506.42, average training loss: 5459.43, base loss: 15956.15
[INFO 2017-06-30 08:23:31,175 main.py:52] epoch 8187, training loss: 5442.44, average training loss: 5459.24, base loss: 15956.36
[INFO 2017-06-30 08:23:34,332 main.py:52] epoch 8188, training loss: 5715.94, average training loss: 5459.28, base loss: 15956.82
[INFO 2017-06-30 08:23:37,461 main.py:52] epoch 8189, training loss: 5288.39, average training loss: 5458.76, base loss: 15957.13
[INFO 2017-06-30 08:23:40,604 main.py:52] epoch 8190, training loss: 5469.13, average training loss: 5458.69, base loss: 15956.73
[INFO 2017-06-30 08:23:43,734 main.py:52] epoch 8191, training loss: 5326.34, average training loss: 5458.47, base loss: 15956.44
[INFO 2017-06-30 08:23:46,948 main.py:52] epoch 8192, training loss: 5061.58, average training loss: 5457.93, base loss: 15956.11
[INFO 2017-06-30 08:23:50,178 main.py:52] epoch 8193, training loss: 5275.01, average training loss: 5457.81, base loss: 15955.99
[INFO 2017-06-30 08:23:53,340 main.py:52] epoch 8194, training loss: 5435.88, average training loss: 5458.30, base loss: 15956.13
[INFO 2017-06-30 08:23:56,505 main.py:52] epoch 8195, training loss: 5236.17, average training loss: 5457.44, base loss: 15955.89
[INFO 2017-06-30 08:23:59,709 main.py:52] epoch 8196, training loss: 5364.94, average training loss: 5457.40, base loss: 15955.92
[INFO 2017-06-30 08:24:02,845 main.py:52] epoch 8197, training loss: 5057.72, average training loss: 5456.39, base loss: 15955.94
[INFO 2017-06-30 08:24:05,996 main.py:52] epoch 8198, training loss: 5384.35, average training loss: 5456.04, base loss: 15956.01
[INFO 2017-06-30 08:24:09,142 main.py:52] epoch 8199, training loss: 5519.72, average training loss: 5455.98, base loss: 15956.21
[INFO 2017-06-30 08:24:09,142 main.py:54] epoch 8199, testing
[INFO 2017-06-30 08:24:22,564 main.py:97] average testing loss: 5468.59, base loss: 15991.13
[INFO 2017-06-30 08:24:22,564 main.py:98] improve_loss: 10522.54, improve_percent: 0.66
[INFO 2017-06-30 08:24:22,566 main.py:66] current best improved percent: 0.66
[INFO 2017-06-30 08:24:25,699 main.py:52] epoch 8200, training loss: 5157.00, average training loss: 5455.47, base loss: 15956.17
[INFO 2017-06-30 08:24:28,871 main.py:52] epoch 8201, training loss: 5017.32, average training loss: 5455.11, base loss: 15956.07
[INFO 2017-06-30 08:24:32,032 main.py:52] epoch 8202, training loss: 5264.75, average training loss: 5454.75, base loss: 15955.98
[INFO 2017-06-30 08:24:35,180 main.py:52] epoch 8203, training loss: 5140.02, average training loss: 5454.50, base loss: 15955.64
[INFO 2017-06-30 08:24:38,342 main.py:52] epoch 8204, training loss: 5350.64, average training loss: 5454.14, base loss: 15955.91
[INFO 2017-06-30 08:24:41,457 main.py:52] epoch 8205, training loss: 5558.93, average training loss: 5454.25, base loss: 15956.21
[INFO 2017-06-30 08:24:44,571 main.py:52] epoch 8206, training loss: 5509.86, average training loss: 5453.83, base loss: 15956.28
[INFO 2017-06-30 08:24:47,703 main.py:52] epoch 8207, training loss: 5789.18, average training loss: 5454.03, base loss: 15956.18
[INFO 2017-06-30 08:24:50,831 main.py:52] epoch 8208, training loss: 5575.20, average training loss: 5454.01, base loss: 15956.04
[INFO 2017-06-30 08:24:54,009 main.py:52] epoch 8209, training loss: 5518.64, average training loss: 5453.96, base loss: 15955.76
[INFO 2017-06-30 08:24:57,129 main.py:52] epoch 8210, training loss: 5546.73, average training loss: 5454.05, base loss: 15955.98
[INFO 2017-06-30 08:25:00,261 main.py:52] epoch 8211, training loss: 5447.83, average training loss: 5453.80, base loss: 15955.58
[INFO 2017-06-30 08:25:03,404 main.py:52] epoch 8212, training loss: 5205.10, average training loss: 5453.52, base loss: 15955.16
[INFO 2017-06-30 08:25:06,565 main.py:52] epoch 8213, training loss: 5615.23, average training loss: 5453.43, base loss: 15955.22
[INFO 2017-06-30 08:25:09,684 main.py:52] epoch 8214, training loss: 5312.74, average training loss: 5453.57, base loss: 15955.06
[INFO 2017-06-30 08:25:12,793 main.py:52] epoch 8215, training loss: 5453.94, average training loss: 5453.53, base loss: 15954.72
[INFO 2017-06-30 08:25:15,976 main.py:52] epoch 8216, training loss: 5592.98, average training loss: 5453.57, base loss: 15954.79
[INFO 2017-06-30 08:25:19,113 main.py:52] epoch 8217, training loss: 5895.52, average training loss: 5453.70, base loss: 15954.95
[INFO 2017-06-30 08:25:22,254 main.py:52] epoch 8218, training loss: 5447.74, average training loss: 5453.59, base loss: 15955.38
[INFO 2017-06-30 08:25:25,400 main.py:52] epoch 8219, training loss: 5073.80, average training loss: 5453.07, base loss: 15955.24
[INFO 2017-06-30 08:25:28,544 main.py:52] epoch 8220, training loss: 5826.63, average training loss: 5453.59, base loss: 15955.63
[INFO 2017-06-30 08:25:31,707 main.py:52] epoch 8221, training loss: 5380.61, average training loss: 5453.60, base loss: 15955.92
[INFO 2017-06-30 08:25:34,804 main.py:52] epoch 8222, training loss: 5095.48, average training loss: 5453.45, base loss: 15955.68
[INFO 2017-06-30 08:25:37,970 main.py:52] epoch 8223, training loss: 5536.27, average training loss: 5453.68, base loss: 15955.65
[INFO 2017-06-30 08:25:41,161 main.py:52] epoch 8224, training loss: 5475.31, average training loss: 5453.53, base loss: 15955.36
[INFO 2017-06-30 08:25:44,338 main.py:52] epoch 8225, training loss: 5613.36, average training loss: 5452.74, base loss: 15955.22
[INFO 2017-06-30 08:25:47,464 main.py:52] epoch 8226, training loss: 5661.27, average training loss: 5452.86, base loss: 15955.20
[INFO 2017-06-30 08:25:50,579 main.py:52] epoch 8227, training loss: 5387.33, average training loss: 5452.74, base loss: 15955.15
[INFO 2017-06-30 08:25:53,756 main.py:52] epoch 8228, training loss: 5427.02, average training loss: 5453.17, base loss: 15955.48
[INFO 2017-06-30 08:25:56,882 main.py:52] epoch 8229, training loss: 5305.49, average training loss: 5452.92, base loss: 15955.55
[INFO 2017-06-30 08:26:00,033 main.py:52] epoch 8230, training loss: 5367.44, average training loss: 5452.67, base loss: 15955.13
[INFO 2017-06-30 08:26:03,182 main.py:52] epoch 8231, training loss: 5257.75, average training loss: 5452.43, base loss: 15955.15
[INFO 2017-06-30 08:26:06,317 main.py:52] epoch 8232, training loss: 5486.71, average training loss: 5452.51, base loss: 15955.11
[INFO 2017-06-30 08:26:09,465 main.py:52] epoch 8233, training loss: 5566.55, average training loss: 5452.51, base loss: 15955.24
[INFO 2017-06-30 08:26:12,610 main.py:52] epoch 8234, training loss: 5242.68, average training loss: 5452.70, base loss: 15955.01
[INFO 2017-06-30 08:26:15,806 main.py:52] epoch 8235, training loss: 5586.01, average training loss: 5452.59, base loss: 15955.07
[INFO 2017-06-30 08:26:18,947 main.py:52] epoch 8236, training loss: 5494.86, average training loss: 5452.58, base loss: 15954.94
[INFO 2017-06-30 08:26:22,047 main.py:52] epoch 8237, training loss: 5524.23, average training loss: 5452.74, base loss: 15955.05
[INFO 2017-06-30 08:26:25,195 main.py:52] epoch 8238, training loss: 5224.78, average training loss: 5452.23, base loss: 15954.95
[INFO 2017-06-30 08:26:28,324 main.py:52] epoch 8239, training loss: 5484.14, average training loss: 5452.56, base loss: 15955.11
[INFO 2017-06-30 08:26:31,427 main.py:52] epoch 8240, training loss: 5490.39, average training loss: 5452.77, base loss: 15955.42
[INFO 2017-06-30 08:26:34,585 main.py:52] epoch 8241, training loss: 5558.74, average training loss: 5452.70, base loss: 15955.49
[INFO 2017-06-30 08:26:37,756 main.py:52] epoch 8242, training loss: 5448.35, average training loss: 5452.71, base loss: 15955.80
[INFO 2017-06-30 08:26:40,906 main.py:52] epoch 8243, training loss: 5375.91, average training loss: 5453.03, base loss: 15955.88
[INFO 2017-06-30 08:26:44,030 main.py:52] epoch 8244, training loss: 5540.95, average training loss: 5452.80, base loss: 15956.06
[INFO 2017-06-30 08:26:47,172 main.py:52] epoch 8245, training loss: 5357.06, average training loss: 5452.13, base loss: 15955.60
[INFO 2017-06-30 08:26:50,284 main.py:52] epoch 8246, training loss: 5327.30, average training loss: 5451.73, base loss: 15954.88
[INFO 2017-06-30 08:26:53,411 main.py:52] epoch 8247, training loss: 5133.87, average training loss: 5450.71, base loss: 15954.98
[INFO 2017-06-30 08:26:56,597 main.py:52] epoch 8248, training loss: 5724.13, average training loss: 5450.88, base loss: 15955.27
[INFO 2017-06-30 08:26:59,769 main.py:52] epoch 8249, training loss: 5597.73, average training loss: 5450.80, base loss: 15955.64
[INFO 2017-06-30 08:27:02,893 main.py:52] epoch 8250, training loss: 5650.84, average training loss: 5451.00, base loss: 15955.88
[INFO 2017-06-30 08:27:06,029 main.py:52] epoch 8251, training loss: 5304.81, average training loss: 5450.97, base loss: 15955.69
[INFO 2017-06-30 08:27:09,181 main.py:52] epoch 8252, training loss: 5709.06, average training loss: 5451.58, base loss: 15956.00
[INFO 2017-06-30 08:27:12,341 main.py:52] epoch 8253, training loss: 5489.84, average training loss: 5451.50, base loss: 15956.18
[INFO 2017-06-30 08:27:15,476 main.py:52] epoch 8254, training loss: 5620.01, average training loss: 5451.66, base loss: 15956.53
[INFO 2017-06-30 08:27:18,632 main.py:52] epoch 8255, training loss: 5339.26, average training loss: 5451.60, base loss: 15956.29
[INFO 2017-06-30 08:27:21,787 main.py:52] epoch 8256, training loss: 5356.68, average training loss: 5451.15, base loss: 15956.67
[INFO 2017-06-30 08:27:24,936 main.py:52] epoch 8257, training loss: 5538.21, average training loss: 5451.03, base loss: 15957.02
[INFO 2017-06-30 08:27:28,083 main.py:52] epoch 8258, training loss: 5325.37, average training loss: 5451.11, base loss: 15957.17
[INFO 2017-06-30 08:27:31,236 main.py:52] epoch 8259, training loss: 5520.92, average training loss: 5450.94, base loss: 15957.06
[INFO 2017-06-30 08:27:34,381 main.py:52] epoch 8260, training loss: 5273.55, average training loss: 5450.89, base loss: 15956.79
[INFO 2017-06-30 08:27:37,489 main.py:52] epoch 8261, training loss: 5876.04, average training loss: 5451.39, base loss: 15956.80
[INFO 2017-06-30 08:27:40,605 main.py:52] epoch 8262, training loss: 5206.10, average training loss: 5450.78, base loss: 15957.04
[INFO 2017-06-30 08:27:43,750 main.py:52] epoch 8263, training loss: 5413.36, average training loss: 5450.88, base loss: 15957.08
[INFO 2017-06-30 08:27:46,890 main.py:52] epoch 8264, training loss: 5142.20, average training loss: 5450.52, base loss: 15956.45
[INFO 2017-06-30 08:27:50,056 main.py:52] epoch 8265, training loss: 5142.82, average training loss: 5449.86, base loss: 15956.02
[INFO 2017-06-30 08:27:53,148 main.py:52] epoch 8266, training loss: 5352.38, average training loss: 5449.31, base loss: 15955.76
[INFO 2017-06-30 08:27:56,288 main.py:52] epoch 8267, training loss: 5480.91, average training loss: 5449.16, base loss: 15955.42
[INFO 2017-06-30 08:27:59,422 main.py:52] epoch 8268, training loss: 5656.01, average training loss: 5449.44, base loss: 15955.71
[INFO 2017-06-30 08:28:02,601 main.py:52] epoch 8269, training loss: 5392.02, average training loss: 5449.37, base loss: 15955.64
[INFO 2017-06-30 08:28:05,749 main.py:52] epoch 8270, training loss: 5631.16, average training loss: 5449.57, base loss: 15955.83
[INFO 2017-06-30 08:28:08,871 main.py:52] epoch 8271, training loss: 5849.79, average training loss: 5449.70, base loss: 15956.09
[INFO 2017-06-30 08:28:12,023 main.py:52] epoch 8272, training loss: 5277.99, average training loss: 5449.43, base loss: 15956.47
[INFO 2017-06-30 08:28:15,205 main.py:52] epoch 8273, training loss: 5518.70, average training loss: 5449.52, base loss: 15956.72
[INFO 2017-06-30 08:28:18,393 main.py:52] epoch 8274, training loss: 5411.76, average training loss: 5449.43, base loss: 15956.64
[INFO 2017-06-30 08:28:21,491 main.py:52] epoch 8275, training loss: 5428.18, average training loss: 5449.71, base loss: 15956.53
[INFO 2017-06-30 08:28:24,655 main.py:52] epoch 8276, training loss: 5184.99, average training loss: 5449.34, base loss: 15956.48
[INFO 2017-06-30 08:28:27,776 main.py:52] epoch 8277, training loss: 5061.40, average training loss: 5448.97, base loss: 15956.20
[INFO 2017-06-30 08:28:30,915 main.py:52] epoch 8278, training loss: 5531.01, average training loss: 5448.71, base loss: 15956.26
[INFO 2017-06-30 08:28:34,071 main.py:52] epoch 8279, training loss: 5272.04, average training loss: 5447.81, base loss: 15956.38
[INFO 2017-06-30 08:28:37,271 main.py:52] epoch 8280, training loss: 5176.05, average training loss: 5447.55, base loss: 15956.34
[INFO 2017-06-30 08:28:40,424 main.py:52] epoch 8281, training loss: 5983.72, average training loss: 5448.12, base loss: 15956.97
[INFO 2017-06-30 08:28:43,559 main.py:52] epoch 8282, training loss: 5408.69, average training loss: 5448.06, base loss: 15957.13
[INFO 2017-06-30 08:28:46,667 main.py:52] epoch 8283, training loss: 5364.44, average training loss: 5447.86, base loss: 15957.25
[INFO 2017-06-30 08:28:49,823 main.py:52] epoch 8284, training loss: 5459.51, average training loss: 5448.00, base loss: 15957.36
[INFO 2017-06-30 08:28:52,942 main.py:52] epoch 8285, training loss: 5662.22, average training loss: 5447.59, base loss: 15957.59
[INFO 2017-06-30 08:28:56,072 main.py:52] epoch 8286, training loss: 5379.12, average training loss: 5447.66, base loss: 15957.62
[INFO 2017-06-30 08:28:59,179 main.py:52] epoch 8287, training loss: 5439.70, average training loss: 5447.29, base loss: 15957.92
[INFO 2017-06-30 08:29:02,356 main.py:52] epoch 8288, training loss: 5407.61, average training loss: 5447.11, base loss: 15957.43
[INFO 2017-06-30 08:29:05,497 main.py:52] epoch 8289, training loss: 5410.10, average training loss: 5447.17, base loss: 15957.28
[INFO 2017-06-30 08:29:08,623 main.py:52] epoch 8290, training loss: 5400.68, average training loss: 5447.04, base loss: 15957.55
[INFO 2017-06-30 08:29:11,721 main.py:52] epoch 8291, training loss: 5066.35, average training loss: 5446.55, base loss: 15957.46
[INFO 2017-06-30 08:29:14,846 main.py:52] epoch 8292, training loss: 5694.93, average training loss: 5446.99, base loss: 15957.49
[INFO 2017-06-30 08:29:17,999 main.py:52] epoch 8293, training loss: 5344.43, average training loss: 5446.76, base loss: 15957.11
[INFO 2017-06-30 08:29:21,148 main.py:52] epoch 8294, training loss: 5440.33, average training loss: 5446.86, base loss: 15956.99
[INFO 2017-06-30 08:29:24,282 main.py:52] epoch 8295, training loss: 5280.29, average training loss: 5446.65, base loss: 15956.57
[INFO 2017-06-30 08:29:27,458 main.py:52] epoch 8296, training loss: 5333.32, average training loss: 5446.23, base loss: 15956.19
[INFO 2017-06-30 08:29:30,605 main.py:52] epoch 8297, training loss: 5351.33, average training loss: 5446.07, base loss: 15956.02
[INFO 2017-06-30 08:29:33,749 main.py:52] epoch 8298, training loss: 5514.10, average training loss: 5446.22, base loss: 15956.11
[INFO 2017-06-30 08:29:36,865 main.py:52] epoch 8299, training loss: 5642.94, average training loss: 5446.55, base loss: 15956.10
[INFO 2017-06-30 08:29:36,866 main.py:54] epoch 8299, testing
[INFO 2017-06-30 08:29:49,956 main.py:97] average testing loss: 5289.75, base loss: 16050.04
[INFO 2017-06-30 08:29:49,956 main.py:98] improve_loss: 10760.28, improve_percent: 0.67
[INFO 2017-06-30 08:29:49,958 main.py:62] model save to ./model/final.pth
[INFO 2017-06-30 08:29:49,993 main.py:66] current best improved percent: 0.67
[INFO 2017-06-30 08:29:53,183 main.py:52] epoch 8300, training loss: 5129.52, average training loss: 5445.40, base loss: 15956.13
[INFO 2017-06-30 08:29:56,338 main.py:52] epoch 8301, training loss: 5701.50, average training loss: 5445.67, base loss: 15956.47
[INFO 2017-06-30 08:29:59,481 main.py:52] epoch 8302, training loss: 5410.75, average training loss: 5445.51, base loss: 15956.49
[INFO 2017-06-30 08:30:02,647 main.py:52] epoch 8303, training loss: 5777.33, average training loss: 5445.62, base loss: 15956.67
[INFO 2017-06-30 08:30:05,785 main.py:52] epoch 8304, training loss: 5382.02, average training loss: 5445.01, base loss: 15956.68
[INFO 2017-06-30 08:30:08,943 main.py:52] epoch 8305, training loss: 5732.27, average training loss: 5445.07, base loss: 15957.09
[INFO 2017-06-30 08:30:12,113 main.py:52] epoch 8306, training loss: 5802.67, average training loss: 5445.22, base loss: 15957.69
[INFO 2017-06-30 08:30:15,227 main.py:52] epoch 8307, training loss: 5781.06, average training loss: 5445.10, base loss: 15958.31
[INFO 2017-06-30 08:30:18,401 main.py:52] epoch 8308, training loss: 5858.35, average training loss: 5445.37, base loss: 15958.57
[INFO 2017-06-30 08:30:21,557 main.py:52] epoch 8309, training loss: 5768.27, average training loss: 5446.17, base loss: 15958.68
[INFO 2017-06-30 08:30:24,683 main.py:52] epoch 8310, training loss: 5361.82, average training loss: 5446.38, base loss: 15958.63
[INFO 2017-06-30 08:30:27,837 main.py:52] epoch 8311, training loss: 5343.41, average training loss: 5446.61, base loss: 15958.75
[INFO 2017-06-30 08:30:30,962 main.py:52] epoch 8312, training loss: 5084.58, average training loss: 5446.20, base loss: 15958.43
[INFO 2017-06-30 08:30:34,097 main.py:52] epoch 8313, training loss: 5722.79, average training loss: 5446.40, base loss: 15958.44
[INFO 2017-06-30 08:30:37,300 main.py:52] epoch 8314, training loss: 5581.87, average training loss: 5446.93, base loss: 15958.45
[INFO 2017-06-30 08:30:40,414 main.py:52] epoch 8315, training loss: 5470.63, average training loss: 5447.20, base loss: 15958.21
[INFO 2017-06-30 08:30:43,588 main.py:52] epoch 8316, training loss: 5482.04, average training loss: 5447.26, base loss: 15957.87
[INFO 2017-06-30 08:30:46,716 main.py:52] epoch 8317, training loss: 5570.59, average training loss: 5447.51, base loss: 15957.47
[INFO 2017-06-30 08:30:49,889 main.py:52] epoch 8318, training loss: 5479.65, average training loss: 5447.71, base loss: 15957.77
[INFO 2017-06-30 08:30:53,053 main.py:52] epoch 8319, training loss: 5687.98, average training loss: 5447.70, base loss: 15958.09
[INFO 2017-06-30 08:30:56,167 main.py:52] epoch 8320, training loss: 5569.66, average training loss: 5447.76, base loss: 15958.34
[INFO 2017-06-30 08:30:59,340 main.py:52] epoch 8321, training loss: 5466.79, average training loss: 5447.82, base loss: 15958.43
[INFO 2017-06-30 08:31:02,477 main.py:52] epoch 8322, training loss: 5547.98, average training loss: 5448.13, base loss: 15958.34
[INFO 2017-06-30 08:31:05,589 main.py:52] epoch 8323, training loss: 5534.00, average training loss: 5448.41, base loss: 15958.30
[INFO 2017-06-30 08:31:08,758 main.py:52] epoch 8324, training loss: 5158.66, average training loss: 5447.97, base loss: 15958.42
[INFO 2017-06-30 08:31:11,888 main.py:52] epoch 8325, training loss: 5633.61, average training loss: 5448.22, base loss: 15958.74
[INFO 2017-06-30 08:31:15,048 main.py:52] epoch 8326, training loss: 5731.95, average training loss: 5448.14, base loss: 15959.01
[INFO 2017-06-30 08:31:18,208 main.py:52] epoch 8327, training loss: 5285.96, average training loss: 5447.93, base loss: 15958.94
[INFO 2017-06-30 08:31:21,402 main.py:52] epoch 8328, training loss: 5237.79, average training loss: 5447.38, base loss: 15958.69
[INFO 2017-06-30 08:31:24,546 main.py:52] epoch 8329, training loss: 5093.59, average training loss: 5447.14, base loss: 15958.33
[INFO 2017-06-30 08:31:27,667 main.py:52] epoch 8330, training loss: 5285.28, average training loss: 5447.11, base loss: 15958.23
[INFO 2017-06-30 08:31:30,822 main.py:52] epoch 8331, training loss: 5584.85, average training loss: 5446.95, base loss: 15958.67
[INFO 2017-06-30 08:31:33,944 main.py:52] epoch 8332, training loss: 5556.21, average training loss: 5446.69, base loss: 15958.54
[INFO 2017-06-30 08:31:37,054 main.py:52] epoch 8333, training loss: 5735.65, average training loss: 5447.32, base loss: 15958.56
[INFO 2017-06-30 08:31:40,198 main.py:52] epoch 8334, training loss: 5856.21, average training loss: 5447.93, base loss: 15958.53
[INFO 2017-06-30 08:31:43,345 main.py:52] epoch 8335, training loss: 5414.50, average training loss: 5447.69, base loss: 15958.36
[INFO 2017-06-30 08:31:46,437 main.py:52] epoch 8336, training loss: 5614.15, average training loss: 5447.84, base loss: 15958.81
[INFO 2017-06-30 08:31:49,585 main.py:52] epoch 8337, training loss: 5383.10, average training loss: 5447.79, base loss: 15959.13
[INFO 2017-06-30 08:31:52,793 main.py:52] epoch 8338, training loss: 5526.18, average training loss: 5447.94, base loss: 15959.42
[INFO 2017-06-30 08:31:55,980 main.py:52] epoch 8339, training loss: 5534.61, average training loss: 5448.28, base loss: 15959.65
[INFO 2017-06-30 08:31:59,130 main.py:52] epoch 8340, training loss: 5622.94, average training loss: 5448.24, base loss: 15960.13
[INFO 2017-06-30 08:32:02,275 main.py:52] epoch 8341, training loss: 5219.90, average training loss: 5447.96, base loss: 15960.26
[INFO 2017-06-30 08:32:05,441 main.py:52] epoch 8342, training loss: 5266.97, average training loss: 5447.56, base loss: 15960.04
[INFO 2017-06-30 08:32:08,569 main.py:52] epoch 8343, training loss: 6009.39, average training loss: 5447.76, base loss: 15960.81
[INFO 2017-06-30 08:32:11,732 main.py:52] epoch 8344, training loss: 5180.48, average training loss: 5447.79, base loss: 15960.71
[INFO 2017-06-30 08:32:14,878 main.py:52] epoch 8345, training loss: 5641.66, average training loss: 5447.99, base loss: 15960.82
[INFO 2017-06-30 08:32:17,991 main.py:52] epoch 8346, training loss: 5878.10, average training loss: 5447.89, base loss: 15961.23
[INFO 2017-06-30 08:32:21,130 main.py:52] epoch 8347, training loss: 5553.83, average training loss: 5447.60, base loss: 15961.59
[INFO 2017-06-30 08:32:24,287 main.py:52] epoch 8348, training loss: 5269.27, average training loss: 5447.45, base loss: 15961.46
[INFO 2017-06-30 08:32:27,469 main.py:52] epoch 8349, training loss: 5565.37, average training loss: 5447.70, base loss: 15961.75
[INFO 2017-06-30 08:32:30,629 main.py:52] epoch 8350, training loss: 5318.49, average training loss: 5447.41, base loss: 15961.91
[INFO 2017-06-30 08:32:33,759 main.py:52] epoch 8351, training loss: 5415.22, average training loss: 5447.41, base loss: 15962.19
[INFO 2017-06-30 08:32:36,933 main.py:52] epoch 8352, training loss: 5280.91, average training loss: 5447.17, base loss: 15962.17
[INFO 2017-06-30 08:32:40,066 main.py:52] epoch 8353, training loss: 5473.46, average training loss: 5447.18, base loss: 15962.06
[INFO 2017-06-30 08:32:43,174 main.py:52] epoch 8354, training loss: 5142.41, average training loss: 5446.80, base loss: 15961.62
[INFO 2017-06-30 08:32:46,331 main.py:52] epoch 8355, training loss: 5166.56, average training loss: 5446.74, base loss: 15961.38
[INFO 2017-06-30 08:32:49,490 main.py:52] epoch 8356, training loss: 5206.04, average training loss: 5446.23, base loss: 15961.30
[INFO 2017-06-30 08:32:52,632 main.py:52] epoch 8357, training loss: 5266.38, average training loss: 5446.24, base loss: 15960.97
[INFO 2017-06-30 08:32:55,771 main.py:52] epoch 8358, training loss: 5352.23, average training loss: 5446.12, base loss: 15961.23
[INFO 2017-06-30 08:32:58,945 main.py:52] epoch 8359, training loss: 5641.20, average training loss: 5446.44, base loss: 15961.49
[INFO 2017-06-30 08:33:02,068 main.py:52] epoch 8360, training loss: 5010.49, average training loss: 5445.90, base loss: 15961.26
[INFO 2017-06-30 08:33:05,216 main.py:52] epoch 8361, training loss: 5257.14, average training loss: 5445.80, base loss: 15961.17
[INFO 2017-06-30 08:33:08,369 main.py:52] epoch 8362, training loss: 5477.44, average training loss: 5445.40, base loss: 15961.52
[INFO 2017-06-30 08:33:11,505 main.py:52] epoch 8363, training loss: 5760.49, average training loss: 5445.73, base loss: 15962.13
[INFO 2017-06-30 08:33:14,618 main.py:52] epoch 8364, training loss: 5402.34, average training loss: 5445.72, base loss: 15962.00
[INFO 2017-06-30 08:33:17,746 main.py:52] epoch 8365, training loss: 5900.32, average training loss: 5446.41, base loss: 15962.60
[INFO 2017-06-30 08:33:20,878 main.py:52] epoch 8366, training loss: 6238.51, average training loss: 5447.32, base loss: 15963.70
[INFO 2017-06-30 08:33:24,022 main.py:52] epoch 8367, training loss: 5505.91, average training loss: 5446.62, base loss: 15964.16
[INFO 2017-06-30 08:33:27,188 main.py:52] epoch 8368, training loss: 5442.02, average training loss: 5446.82, base loss: 15964.24
[INFO 2017-06-30 08:33:30,321 main.py:52] epoch 8369, training loss: 5497.46, average training loss: 5446.78, base loss: 15964.51
[INFO 2017-06-30 08:33:33,478 main.py:52] epoch 8370, training loss: 5690.64, average training loss: 5446.67, base loss: 15964.86
[INFO 2017-06-30 08:33:36,613 main.py:52] epoch 8371, training loss: 5812.63, average training loss: 5446.64, base loss: 15965.24
[INFO 2017-06-30 08:33:39,776 main.py:52] epoch 8372, training loss: 5084.13, average training loss: 5446.39, base loss: 15965.13
[INFO 2017-06-30 08:33:42,983 main.py:52] epoch 8373, training loss: 5207.51, average training loss: 5446.12, base loss: 15964.61
[INFO 2017-06-30 08:33:46,098 main.py:52] epoch 8374, training loss: 5237.63, average training loss: 5445.83, base loss: 15964.02
[INFO 2017-06-30 08:33:49,204 main.py:52] epoch 8375, training loss: 5271.99, average training loss: 5445.83, base loss: 15963.66
[INFO 2017-06-30 08:33:52,342 main.py:52] epoch 8376, training loss: 5465.70, average training loss: 5445.61, base loss: 15963.64
[INFO 2017-06-30 08:33:55,453 main.py:52] epoch 8377, training loss: 5675.91, average training loss: 5445.82, base loss: 15964.12
[INFO 2017-06-30 08:33:58,606 main.py:52] epoch 8378, training loss: 5532.15, average training loss: 5445.86, base loss: 15964.24
[INFO 2017-06-30 08:34:01,714 main.py:52] epoch 8379, training loss: 5403.52, average training loss: 5446.02, base loss: 15963.82
[INFO 2017-06-30 08:34:04,849 main.py:52] epoch 8380, training loss: 5328.66, average training loss: 5445.68, base loss: 15963.80
[INFO 2017-06-30 08:34:07,972 main.py:52] epoch 8381, training loss: 5326.30, average training loss: 5445.63, base loss: 15963.96
[INFO 2017-06-30 08:34:11,114 main.py:52] epoch 8382, training loss: 5260.50, average training loss: 5445.18, base loss: 15964.06
[INFO 2017-06-30 08:34:14,256 main.py:52] epoch 8383, training loss: 5828.64, average training loss: 5445.25, base loss: 15964.27
[INFO 2017-06-30 08:34:17,380 main.py:52] epoch 8384, training loss: 5114.59, average training loss: 5444.65, base loss: 15963.83
[INFO 2017-06-30 08:34:20,531 main.py:52] epoch 8385, training loss: 5414.89, average training loss: 5444.63, base loss: 15963.92
[INFO 2017-06-30 08:34:23,695 main.py:52] epoch 8386, training loss: 5377.88, average training loss: 5444.24, base loss: 15963.86
[INFO 2017-06-30 08:34:26,864 main.py:52] epoch 8387, training loss: 5494.81, average training loss: 5443.98, base loss: 15964.31
[INFO 2017-06-30 08:34:29,999 main.py:52] epoch 8388, training loss: 5707.27, average training loss: 5444.38, base loss: 15964.41
[INFO 2017-06-30 08:34:33,210 main.py:52] epoch 8389, training loss: 5661.34, average training loss: 5444.85, base loss: 15964.75
[INFO 2017-06-30 08:34:36,337 main.py:52] epoch 8390, training loss: 5562.04, average training loss: 5444.72, base loss: 15964.58
[INFO 2017-06-30 08:34:39,483 main.py:52] epoch 8391, training loss: 5528.86, average training loss: 5444.93, base loss: 15964.32
[INFO 2017-06-30 08:34:42,634 main.py:52] epoch 8392, training loss: 5598.23, average training loss: 5445.09, base loss: 15964.27
[INFO 2017-06-30 08:34:45,771 main.py:52] epoch 8393, training loss: 5221.36, average training loss: 5445.09, base loss: 15964.04
[INFO 2017-06-30 08:34:48,914 main.py:52] epoch 8394, training loss: 5495.95, average training loss: 5444.82, base loss: 15964.26
[INFO 2017-06-30 08:34:52,082 main.py:52] epoch 8395, training loss: 5748.50, average training loss: 5445.15, base loss: 15964.62
[INFO 2017-06-30 08:34:55,221 main.py:52] epoch 8396, training loss: 5231.03, average training loss: 5444.61, base loss: 15964.64
[INFO 2017-06-30 08:34:58,351 main.py:52] epoch 8397, training loss: 5402.33, average training loss: 5444.84, base loss: 15964.69
[INFO 2017-06-30 08:35:01,478 main.py:52] epoch 8398, training loss: 5714.21, average training loss: 5445.22, base loss: 15964.94
[INFO 2017-06-30 08:35:04,633 main.py:52] epoch 8399, training loss: 5328.34, average training loss: 5444.83, base loss: 15964.98
[INFO 2017-06-30 08:35:04,634 main.py:54] epoch 8399, testing
[INFO 2017-06-30 08:35:17,591 main.py:97] average testing loss: 5432.09, base loss: 16031.26
[INFO 2017-06-30 08:35:17,591 main.py:98] improve_loss: 10599.17, improve_percent: 0.66
[INFO 2017-06-30 08:35:17,594 main.py:66] current best improved percent: 0.67
[INFO 2017-06-30 08:35:20,789 main.py:52] epoch 8400, training loss: 5684.38, average training loss: 5445.00, base loss: 15965.21
[INFO 2017-06-30 08:35:24,007 main.py:52] epoch 8401, training loss: 5447.18, average training loss: 5445.06, base loss: 15964.72
[INFO 2017-06-30 08:35:27,147 main.py:52] epoch 8402, training loss: 5359.58, average training loss: 5444.74, base loss: 15964.75
[INFO 2017-06-30 08:35:30,290 main.py:52] epoch 8403, training loss: 5441.27, average training loss: 5444.79, base loss: 15964.82
[INFO 2017-06-30 08:35:33,448 main.py:52] epoch 8404, training loss: 5816.39, average training loss: 5445.26, base loss: 15965.25
[INFO 2017-06-30 08:35:36,609 main.py:52] epoch 8405, training loss: 5411.38, average training loss: 5445.45, base loss: 15965.01
[INFO 2017-06-30 08:35:39,762 main.py:52] epoch 8406, training loss: 5456.79, average training loss: 5445.64, base loss: 15964.83
[INFO 2017-06-30 08:35:42,895 main.py:52] epoch 8407, training loss: 5568.50, average training loss: 5445.77, base loss: 15964.50
[INFO 2017-06-30 08:35:46,053 main.py:52] epoch 8408, training loss: 5754.55, average training loss: 5446.03, base loss: 15964.78
[INFO 2017-06-30 08:35:49,187 main.py:52] epoch 8409, training loss: 5368.60, average training loss: 5445.72, base loss: 15964.73
[INFO 2017-06-30 08:35:52,293 main.py:52] epoch 8410, training loss: 4993.72, average training loss: 5445.14, base loss: 15964.47
[INFO 2017-06-30 08:35:55,492 main.py:52] epoch 8411, training loss: 5488.89, average training loss: 5444.76, base loss: 15964.99
[INFO 2017-06-30 08:35:58,638 main.py:52] epoch 8412, training loss: 5316.75, average training loss: 5444.44, base loss: 15965.20
[INFO 2017-06-30 08:36:01,776 main.py:52] epoch 8413, training loss: 5446.11, average training loss: 5443.83, base loss: 15965.45
[INFO 2017-06-30 08:36:04,870 main.py:52] epoch 8414, training loss: 5587.19, average training loss: 5444.11, base loss: 15965.86
[INFO 2017-06-30 08:36:08,019 main.py:52] epoch 8415, training loss: 5419.22, average training loss: 5443.80, base loss: 15965.80
[INFO 2017-06-30 08:36:11,158 main.py:52] epoch 8416, training loss: 5103.36, average training loss: 5443.22, base loss: 15965.57
[INFO 2017-06-30 08:36:14,333 main.py:52] epoch 8417, training loss: 5076.47, average training loss: 5442.50, base loss: 15965.45
[INFO 2017-06-30 08:36:17,466 main.py:52] epoch 8418, training loss: 5299.05, average training loss: 5441.90, base loss: 15965.41
[INFO 2017-06-30 08:36:20,598 main.py:52] epoch 8419, training loss: 5139.29, average training loss: 5441.56, base loss: 15965.16
[INFO 2017-06-30 08:36:23,772 main.py:52] epoch 8420, training loss: 5543.72, average training loss: 5441.37, base loss: 15965.18
[INFO 2017-06-30 08:36:26,927 main.py:52] epoch 8421, training loss: 5392.27, average training loss: 5441.03, base loss: 15965.18
[INFO 2017-06-30 08:36:30,052 main.py:52] epoch 8422, training loss: 6123.96, average training loss: 5441.84, base loss: 15965.91
[INFO 2017-06-30 08:36:33,224 main.py:52] epoch 8423, training loss: 5141.41, average training loss: 5441.18, base loss: 15965.97
[INFO 2017-06-30 08:36:36,390 main.py:52] epoch 8424, training loss: 5276.87, average training loss: 5441.01, base loss: 15965.69
[INFO 2017-06-30 08:36:39,519 main.py:52] epoch 8425, training loss: 5724.42, average training loss: 5441.39, base loss: 15965.68
[INFO 2017-06-30 08:36:42,634 main.py:52] epoch 8426, training loss: 5164.87, average training loss: 5441.26, base loss: 15965.47
[INFO 2017-06-30 08:36:45,759 main.py:52] epoch 8427, training loss: 5380.29, average training loss: 5441.54, base loss: 15965.32
[INFO 2017-06-30 08:36:48,895 main.py:52] epoch 8428, training loss: 5716.00, average training loss: 5442.17, base loss: 15965.51
[INFO 2017-06-30 08:36:51,993 main.py:52] epoch 8429, training loss: 5324.99, average training loss: 5442.23, base loss: 15965.07
[INFO 2017-06-30 08:36:55,118 main.py:52] epoch 8430, training loss: 5762.39, average training loss: 5442.11, base loss: 15965.39
[INFO 2017-06-30 08:36:58,231 main.py:52] epoch 8431, training loss: 5281.79, average training loss: 5441.77, base loss: 15965.38
[INFO 2017-06-30 08:37:01,359 main.py:52] epoch 8432, training loss: 5746.01, average training loss: 5441.92, base loss: 15965.90
[INFO 2017-06-30 08:37:04,465 main.py:52] epoch 8433, training loss: 5458.68, average training loss: 5442.11, base loss: 15965.94
[INFO 2017-06-30 08:37:07,602 main.py:52] epoch 8434, training loss: 5434.35, average training loss: 5441.85, base loss: 15966.10
[INFO 2017-06-30 08:37:10,746 main.py:52] epoch 8435, training loss: 5433.85, average training loss: 5441.69, base loss: 15965.55
[INFO 2017-06-30 08:37:13,907 main.py:52] epoch 8436, training loss: 5747.67, average training loss: 5441.88, base loss: 15966.01
[INFO 2017-06-30 08:37:17,018 main.py:52] epoch 8437, training loss: 5356.23, average training loss: 5441.82, base loss: 15966.29
[INFO 2017-06-30 08:37:20,142 main.py:52] epoch 8438, training loss: 5399.00, average training loss: 5441.97, base loss: 15966.19
[INFO 2017-06-30 08:37:23,271 main.py:52] epoch 8439, training loss: 5612.83, average training loss: 5442.47, base loss: 15966.27
[INFO 2017-06-30 08:37:26,428 main.py:52] epoch 8440, training loss: 5274.93, average training loss: 5442.35, base loss: 15965.99
[INFO 2017-06-30 08:37:29,575 main.py:52] epoch 8441, training loss: 5477.81, average training loss: 5442.28, base loss: 15965.80
[INFO 2017-06-30 08:37:32,731 main.py:52] epoch 8442, training loss: 5164.91, average training loss: 5441.79, base loss: 15965.64
[INFO 2017-06-30 08:37:35,859 main.py:52] epoch 8443, training loss: 5185.19, average training loss: 5441.27, base loss: 15965.65
[INFO 2017-06-30 08:37:39,041 main.py:52] epoch 8444, training loss: 5723.43, average training loss: 5441.72, base loss: 15965.90
[INFO 2017-06-30 08:37:42,189 main.py:52] epoch 8445, training loss: 5469.16, average training loss: 5442.00, base loss: 15966.04
[INFO 2017-06-30 08:37:45,300 main.py:52] epoch 8446, training loss: 5234.18, average training loss: 5441.63, base loss: 15965.67
[INFO 2017-06-30 08:37:48,431 main.py:52] epoch 8447, training loss: 5485.91, average training loss: 5441.85, base loss: 15965.76
[INFO 2017-06-30 08:37:51,552 main.py:52] epoch 8448, training loss: 5998.39, average training loss: 5442.66, base loss: 15966.12
[INFO 2017-06-30 08:37:54,734 main.py:52] epoch 8449, training loss: 5379.12, average training loss: 5442.51, base loss: 15966.33
[INFO 2017-06-30 08:37:57,887 main.py:52] epoch 8450, training loss: 5656.08, average training loss: 5442.75, base loss: 15966.46
[INFO 2017-06-30 08:38:01,085 main.py:52] epoch 8451, training loss: 5221.88, average training loss: 5442.31, base loss: 15965.88
[INFO 2017-06-30 08:38:04,221 main.py:52] epoch 8452, training loss: 5438.62, average training loss: 5442.13, base loss: 15965.82
[INFO 2017-06-30 08:38:07,413 main.py:52] epoch 8453, training loss: 6167.05, average training loss: 5442.71, base loss: 15966.62
[INFO 2017-06-30 08:38:10,521 main.py:52] epoch 8454, training loss: 5428.67, average training loss: 5442.45, base loss: 15966.53
[INFO 2017-06-30 08:38:13,699 main.py:52] epoch 8455, training loss: 5176.68, average training loss: 5442.13, base loss: 15965.98
[INFO 2017-06-30 08:38:16,845 main.py:52] epoch 8456, training loss: 5207.48, average training loss: 5441.99, base loss: 15965.83
[INFO 2017-06-30 08:38:19,947 main.py:52] epoch 8457, training loss: 5228.84, average training loss: 5441.98, base loss: 15965.68
[INFO 2017-06-30 08:38:23,087 main.py:52] epoch 8458, training loss: 5548.36, average training loss: 5442.11, base loss: 15965.75
[INFO 2017-06-30 08:38:26,248 main.py:52] epoch 8459, training loss: 5194.98, average training loss: 5441.98, base loss: 15965.62
[INFO 2017-06-30 08:38:29,344 main.py:52] epoch 8460, training loss: 5122.65, average training loss: 5441.70, base loss: 15965.75
[INFO 2017-06-30 08:38:32,483 main.py:52] epoch 8461, training loss: 5665.42, average training loss: 5442.27, base loss: 15965.86
[INFO 2017-06-30 08:38:35,653 main.py:52] epoch 8462, training loss: 5465.67, average training loss: 5442.36, base loss: 15965.74
[INFO 2017-06-30 08:38:38,819 main.py:52] epoch 8463, training loss: 5232.91, average training loss: 5442.06, base loss: 15965.47
[INFO 2017-06-30 08:38:41,954 main.py:52] epoch 8464, training loss: 5410.16, average training loss: 5441.81, base loss: 15965.58
[INFO 2017-06-30 08:38:45,101 main.py:52] epoch 8465, training loss: 5210.62, average training loss: 5441.53, base loss: 15965.30
[INFO 2017-06-30 08:38:48,277 main.py:52] epoch 8466, training loss: 5446.38, average training loss: 5441.54, base loss: 15964.99
[INFO 2017-06-30 08:38:51,452 main.py:52] epoch 8467, training loss: 5799.24, average training loss: 5441.90, base loss: 15965.30
[INFO 2017-06-30 08:38:54,569 main.py:52] epoch 8468, training loss: 6056.22, average training loss: 5442.46, base loss: 15965.75
[INFO 2017-06-30 08:38:57,681 main.py:52] epoch 8469, training loss: 5702.15, average training loss: 5442.49, base loss: 15966.02
[INFO 2017-06-30 08:39:00,827 main.py:52] epoch 8470, training loss: 5180.34, average training loss: 5442.03, base loss: 15965.99
[INFO 2017-06-30 08:39:03,929 main.py:52] epoch 8471, training loss: 5396.49, average training loss: 5442.31, base loss: 15965.50
[INFO 2017-06-30 08:39:07,104 main.py:52] epoch 8472, training loss: 5349.32, average training loss: 5442.29, base loss: 15965.36
[INFO 2017-06-30 08:39:10,204 main.py:52] epoch 8473, training loss: 5125.60, average training loss: 5441.99, base loss: 15965.24
[INFO 2017-06-30 08:39:13,296 main.py:52] epoch 8474, training loss: 5700.85, average training loss: 5442.39, base loss: 15965.60
[INFO 2017-06-30 08:39:16,456 main.py:52] epoch 8475, training loss: 5534.49, average training loss: 5442.20, base loss: 15965.66
[INFO 2017-06-30 08:39:19,554 main.py:52] epoch 8476, training loss: 5319.02, average training loss: 5442.04, base loss: 15965.93
[INFO 2017-06-30 08:39:22,660 main.py:52] epoch 8477, training loss: 5431.46, average training loss: 5441.68, base loss: 15966.28
[INFO 2017-06-30 08:39:25,796 main.py:52] epoch 8478, training loss: 5260.76, average training loss: 5441.76, base loss: 15966.32
[INFO 2017-06-30 08:39:28,959 main.py:52] epoch 8479, training loss: 5038.32, average training loss: 5441.36, base loss: 15965.90
[INFO 2017-06-30 08:39:32,124 main.py:52] epoch 8480, training loss: 5737.16, average training loss: 5441.51, base loss: 15965.88
[INFO 2017-06-30 08:39:35,229 main.py:52] epoch 8481, training loss: 5361.81, average training loss: 5441.21, base loss: 15965.66
[INFO 2017-06-30 08:39:38,363 main.py:52] epoch 8482, training loss: 5209.24, average training loss: 5440.97, base loss: 15965.39
[INFO 2017-06-30 08:39:41,466 main.py:52] epoch 8483, training loss: 5195.74, average training loss: 5440.50, base loss: 15965.27
[INFO 2017-06-30 08:39:44,577 main.py:52] epoch 8484, training loss: 5767.82, average training loss: 5439.87, base loss: 15965.35
[INFO 2017-06-30 08:39:47,744 main.py:52] epoch 8485, training loss: 5863.25, average training loss: 5440.06, base loss: 15965.46
[INFO 2017-06-30 08:39:50,903 main.py:52] epoch 8486, training loss: 5581.60, average training loss: 5440.17, base loss: 15965.89
[INFO 2017-06-30 08:39:54,028 main.py:52] epoch 8487, training loss: 5349.73, average training loss: 5440.01, base loss: 15965.95
[INFO 2017-06-30 08:39:57,207 main.py:52] epoch 8488, training loss: 5531.49, average training loss: 5440.34, base loss: 15966.27
[INFO 2017-06-30 08:40:00,331 main.py:52] epoch 8489, training loss: 5008.08, average training loss: 5439.97, base loss: 15965.94
[INFO 2017-06-30 08:40:03,474 main.py:52] epoch 8490, training loss: 5460.67, average training loss: 5440.10, base loss: 15965.72
[INFO 2017-06-30 08:40:06,649 main.py:52] epoch 8491, training loss: 5306.95, average training loss: 5439.97, base loss: 15965.53
[INFO 2017-06-30 08:40:09,804 main.py:52] epoch 8492, training loss: 5404.73, average training loss: 5439.67, base loss: 15965.81
[INFO 2017-06-30 08:40:12,980 main.py:52] epoch 8493, training loss: 5357.58, average training loss: 5439.38, base loss: 15965.95
[INFO 2017-06-30 08:40:16,127 main.py:52] epoch 8494, training loss: 5506.23, average training loss: 5439.30, base loss: 15966.28
[INFO 2017-06-30 08:40:19,327 main.py:52] epoch 8495, training loss: 5382.71, average training loss: 5439.20, base loss: 15966.37
[INFO 2017-06-30 08:40:22,510 main.py:52] epoch 8496, training loss: 5478.20, average training loss: 5438.96, base loss: 15966.40
[INFO 2017-06-30 08:40:25,666 main.py:52] epoch 8497, training loss: 5799.99, average training loss: 5439.57, base loss: 15966.92
[INFO 2017-06-30 08:40:28,794 main.py:52] epoch 8498, training loss: 5384.93, average training loss: 5439.81, base loss: 15966.66
[INFO 2017-06-30 08:40:31,945 main.py:52] epoch 8499, training loss: 5459.05, average training loss: 5439.63, base loss: 15966.42
[INFO 2017-06-30 08:40:31,945 main.py:54] epoch 8499, testing
[INFO 2017-06-30 08:40:45,070 main.py:97] average testing loss: 5476.87, base loss: 16245.26
[INFO 2017-06-30 08:40:45,070 main.py:98] improve_loss: 10768.38, improve_percent: 0.66
[INFO 2017-06-30 08:40:45,072 main.py:66] current best improved percent: 0.67
[INFO 2017-06-30 08:40:48,206 main.py:52] epoch 8500, training loss: 5737.13, average training loss: 5439.97, base loss: 15966.88
[INFO 2017-06-30 08:40:51,332 main.py:52] epoch 8501, training loss: 5172.23, average training loss: 5439.35, base loss: 15966.66
[INFO 2017-06-30 08:40:54,468 main.py:52] epoch 8502, training loss: 5634.81, average training loss: 5439.21, base loss: 15966.96
[INFO 2017-06-30 08:40:57,638 main.py:52] epoch 8503, training loss: 5368.90, average training loss: 5438.83, base loss: 15966.99
[INFO 2017-06-30 08:41:00,798 main.py:52] epoch 8504, training loss: 5544.42, average training loss: 5438.92, base loss: 15967.09
[INFO 2017-06-30 08:41:03,946 main.py:52] epoch 8505, training loss: 5760.40, average training loss: 5439.23, base loss: 15967.74
[INFO 2017-06-30 08:41:07,130 main.py:52] epoch 8506, training loss: 5274.96, average training loss: 5438.42, base loss: 15968.00
[INFO 2017-06-30 08:41:10,282 main.py:52] epoch 8507, training loss: 5447.55, average training loss: 5438.30, base loss: 15968.49
[INFO 2017-06-30 08:41:13,453 main.py:52] epoch 8508, training loss: 5390.97, average training loss: 5438.35, base loss: 15968.47
[INFO 2017-06-30 08:41:16,569 main.py:52] epoch 8509, training loss: 5612.70, average training loss: 5438.73, base loss: 15968.50
[INFO 2017-06-30 08:41:19,748 main.py:52] epoch 8510, training loss: 5434.14, average training loss: 5438.74, base loss: 15968.37
[INFO 2017-06-30 08:41:22,919 main.py:52] epoch 8511, training loss: 4905.78, average training loss: 5438.16, base loss: 15967.94
[INFO 2017-06-30 08:41:26,088 main.py:52] epoch 8512, training loss: 5429.95, average training loss: 5438.01, base loss: 15968.29
[INFO 2017-06-30 08:41:29,222 main.py:52] epoch 8513, training loss: 5338.86, average training loss: 5437.71, base loss: 15968.62
[INFO 2017-06-30 08:41:32,383 main.py:52] epoch 8514, training loss: 5341.65, average training loss: 5437.41, base loss: 15968.74
[INFO 2017-06-30 08:41:35,553 main.py:52] epoch 8515, training loss: 5913.92, average training loss: 5437.49, base loss: 15969.22
[INFO 2017-06-30 08:41:38,697 main.py:52] epoch 8516, training loss: 5425.45, average training loss: 5437.38, base loss: 15969.15
[INFO 2017-06-30 08:41:41,874 main.py:52] epoch 8517, training loss: 5239.32, average training loss: 5437.33, base loss: 15969.09
[INFO 2017-06-30 08:41:45,035 main.py:52] epoch 8518, training loss: 5551.37, average training loss: 5437.52, base loss: 15969.12
[INFO 2017-06-30 08:41:48,142 main.py:52] epoch 8519, training loss: 5674.91, average training loss: 5437.67, base loss: 15969.61
[INFO 2017-06-30 08:41:51,293 main.py:52] epoch 8520, training loss: 5392.70, average training loss: 5437.43, base loss: 15969.70
[INFO 2017-06-30 08:41:54,442 main.py:52] epoch 8521, training loss: 5155.67, average training loss: 5437.27, base loss: 15969.56
[INFO 2017-06-30 08:41:57,580 main.py:52] epoch 8522, training loss: 5468.87, average training loss: 5437.28, base loss: 15969.79
[INFO 2017-06-30 08:42:00,709 main.py:52] epoch 8523, training loss: 5342.45, average training loss: 5436.95, base loss: 15969.77
[INFO 2017-06-30 08:42:03,902 main.py:52] epoch 8524, training loss: 5264.96, average training loss: 5437.05, base loss: 15969.86
[INFO 2017-06-30 08:42:07,076 main.py:52] epoch 8525, training loss: 5771.84, average training loss: 5437.21, base loss: 15970.06
[INFO 2017-06-30 08:42:10,233 main.py:52] epoch 8526, training loss: 5426.07, average training loss: 5437.26, base loss: 15970.19
[INFO 2017-06-30 08:42:13,391 main.py:52] epoch 8527, training loss: 5268.45, average training loss: 5437.09, base loss: 15970.35
[INFO 2017-06-30 08:42:16,530 main.py:52] epoch 8528, training loss: 5442.21, average training loss: 5437.37, base loss: 15970.55
[INFO 2017-06-30 08:42:19,672 main.py:52] epoch 8529, training loss: 5462.10, average training loss: 5437.49, base loss: 15970.64
[INFO 2017-06-30 08:42:22,799 main.py:52] epoch 8530, training loss: 4984.93, average training loss: 5436.60, base loss: 15970.45
[INFO 2017-06-30 08:42:25,946 main.py:52] epoch 8531, training loss: 5247.06, average training loss: 5436.49, base loss: 15970.18
[INFO 2017-06-30 08:42:29,100 main.py:52] epoch 8532, training loss: 5559.13, average training loss: 5436.36, base loss: 15970.30
[INFO 2017-06-30 08:42:32,278 main.py:52] epoch 8533, training loss: 5448.11, average training loss: 5436.44, base loss: 15970.43
[INFO 2017-06-30 08:42:35,452 main.py:52] epoch 8534, training loss: 5302.60, average training loss: 5436.22, base loss: 15970.39
[INFO 2017-06-30 08:42:38,614 main.py:52] epoch 8535, training loss: 5680.06, average training loss: 5436.66, base loss: 15970.49
[INFO 2017-06-30 08:42:41,810 main.py:52] epoch 8536, training loss: 5378.05, average training loss: 5436.70, base loss: 15970.36
[INFO 2017-06-30 08:42:44,938 main.py:52] epoch 8537, training loss: 5226.87, average training loss: 5436.57, base loss: 15970.14
[INFO 2017-06-30 08:42:48,044 main.py:52] epoch 8538, training loss: 5774.40, average training loss: 5436.57, base loss: 15970.45
[INFO 2017-06-30 08:42:51,168 main.py:52] epoch 8539, training loss: 5350.26, average training loss: 5436.30, base loss: 15970.33
[INFO 2017-06-30 08:42:54,355 main.py:52] epoch 8540, training loss: 5601.17, average training loss: 5436.92, base loss: 15970.09
[INFO 2017-06-30 08:42:57,547 main.py:52] epoch 8541, training loss: 5202.96, average training loss: 5436.73, base loss: 15969.81
[INFO 2017-06-30 08:43:00,721 main.py:52] epoch 8542, training loss: 5772.65, average training loss: 5437.28, base loss: 15969.95
[INFO 2017-06-30 08:43:03,815 main.py:52] epoch 8543, training loss: 5209.23, average training loss: 5436.83, base loss: 15969.66
[INFO 2017-06-30 08:43:06,961 main.py:52] epoch 8544, training loss: 5750.01, average training loss: 5437.71, base loss: 15969.82
[INFO 2017-06-30 08:43:10,090 main.py:52] epoch 8545, training loss: 5421.92, average training loss: 5438.09, base loss: 15969.79
[INFO 2017-06-30 08:43:13,255 main.py:52] epoch 8546, training loss: 5428.17, average training loss: 5438.17, base loss: 15969.84
[INFO 2017-06-30 08:43:16,398 main.py:52] epoch 8547, training loss: 5370.14, average training loss: 5438.27, base loss: 15970.15
[INFO 2017-06-30 08:43:19,549 main.py:52] epoch 8548, training loss: 5411.39, average training loss: 5438.22, base loss: 15970.33
[INFO 2017-06-30 08:43:22,705 main.py:52] epoch 8549, training loss: 4995.20, average training loss: 5438.04, base loss: 15970.17
[INFO 2017-06-30 08:43:25,856 main.py:52] epoch 8550, training loss: 5548.00, average training loss: 5438.34, base loss: 15970.21
[INFO 2017-06-30 08:43:28,970 main.py:52] epoch 8551, training loss: 5686.14, average training loss: 5438.71, base loss: 15970.40
[INFO 2017-06-30 08:43:32,083 main.py:52] epoch 8552, training loss: 5809.76, average training loss: 5438.96, base loss: 15970.64
[INFO 2017-06-30 08:43:35,233 main.py:52] epoch 8553, training loss: 5436.34, average training loss: 5438.80, base loss: 15970.41
[INFO 2017-06-30 08:43:38,376 main.py:52] epoch 8554, training loss: 5371.47, average training loss: 5438.86, base loss: 15970.35
[INFO 2017-06-30 08:43:41,481 main.py:52] epoch 8555, training loss: 5233.00, average training loss: 5438.63, base loss: 15970.17
[INFO 2017-06-30 08:43:44,623 main.py:52] epoch 8556, training loss: 5553.86, average training loss: 5438.94, base loss: 15970.52
[INFO 2017-06-30 08:43:47,762 main.py:52] epoch 8557, training loss: 5137.34, average training loss: 5438.77, base loss: 15970.42
[INFO 2017-06-30 08:43:50,878 main.py:52] epoch 8558, training loss: 4989.51, average training loss: 5438.41, base loss: 15970.09
[INFO 2017-06-30 08:43:54,027 main.py:52] epoch 8559, training loss: 5418.84, average training loss: 5438.52, base loss: 15969.95
[INFO 2017-06-30 08:43:57,200 main.py:52] epoch 8560, training loss: 5701.80, average training loss: 5439.04, base loss: 15969.88
[INFO 2017-06-30 08:44:00,350 main.py:52] epoch 8561, training loss: 5617.97, average training loss: 5439.20, base loss: 15969.88
[INFO 2017-06-30 08:44:03,494 main.py:52] epoch 8562, training loss: 5219.38, average training loss: 5438.98, base loss: 15969.86
[INFO 2017-06-30 08:44:06,654 main.py:52] epoch 8563, training loss: 5148.93, average training loss: 5438.52, base loss: 15969.57
[INFO 2017-06-30 08:44:09,845 main.py:52] epoch 8564, training loss: 5836.12, average training loss: 5439.05, base loss: 15969.88
[INFO 2017-06-30 08:44:13,006 main.py:52] epoch 8565, training loss: 5260.02, average training loss: 5438.92, base loss: 15970.09
[INFO 2017-06-30 08:44:16,174 main.py:52] epoch 8566, training loss: 5487.72, average training loss: 5438.94, base loss: 15969.96
[INFO 2017-06-30 08:44:19,301 main.py:52] epoch 8567, training loss: 5558.23, average training loss: 5438.92, base loss: 15970.31
[INFO 2017-06-30 08:44:22,429 main.py:52] epoch 8568, training loss: 5388.62, average training loss: 5438.86, base loss: 15970.60
[INFO 2017-06-30 08:44:25,593 main.py:52] epoch 8569, training loss: 5536.85, average training loss: 5438.98, base loss: 15971.08
[INFO 2017-06-30 08:44:28,744 main.py:52] epoch 8570, training loss: 5654.33, average training loss: 5439.11, base loss: 15971.34
[INFO 2017-06-30 08:44:31,853 main.py:52] epoch 8571, training loss: 5067.60, average training loss: 5438.75, base loss: 15971.21
[INFO 2017-06-30 08:44:35,003 main.py:52] epoch 8572, training loss: 5300.40, average training loss: 5438.47, base loss: 15970.57
[INFO 2017-06-30 08:44:38,172 main.py:52] epoch 8573, training loss: 5585.71, average training loss: 5438.41, base loss: 15970.62
[INFO 2017-06-30 08:44:41,336 main.py:52] epoch 8574, training loss: 5121.80, average training loss: 5438.17, base loss: 15969.94
[INFO 2017-06-30 08:44:44,456 main.py:52] epoch 8575, training loss: 4965.54, average training loss: 5437.79, base loss: 15969.60
[INFO 2017-06-30 08:44:47,610 main.py:52] epoch 8576, training loss: 5290.79, average training loss: 5437.71, base loss: 15969.44
[INFO 2017-06-30 08:44:50,751 main.py:52] epoch 8577, training loss: 5395.68, average training loss: 5437.67, base loss: 15969.81
[INFO 2017-06-30 08:44:53,870 main.py:52] epoch 8578, training loss: 5278.10, average training loss: 5437.59, base loss: 15970.06
[INFO 2017-06-30 08:44:57,030 main.py:52] epoch 8579, training loss: 5833.32, average training loss: 5437.80, base loss: 15970.34
[INFO 2017-06-30 08:45:00,197 main.py:52] epoch 8580, training loss: 5061.31, average training loss: 5437.09, base loss: 15970.03
[INFO 2017-06-30 08:45:03,331 main.py:52] epoch 8581, training loss: 5527.38, average training loss: 5437.21, base loss: 15970.25
[INFO 2017-06-30 08:45:06,467 main.py:52] epoch 8582, training loss: 5127.52, average training loss: 5437.07, base loss: 15970.20
[INFO 2017-06-30 08:45:09,653 main.py:52] epoch 8583, training loss: 5669.25, average training loss: 5437.37, base loss: 15970.36
[INFO 2017-06-30 08:45:12,849 main.py:52] epoch 8584, training loss: 4910.53, average training loss: 5436.52, base loss: 15969.97
[INFO 2017-06-30 08:45:15,975 main.py:52] epoch 8585, training loss: 5144.25, average training loss: 5435.85, base loss: 15969.61
[INFO 2017-06-30 08:45:19,175 main.py:52] epoch 8586, training loss: 4874.45, average training loss: 5435.73, base loss: 15969.12
[INFO 2017-06-30 08:45:22,332 main.py:52] epoch 8587, training loss: 5624.31, average training loss: 5435.85, base loss: 15969.08
[INFO 2017-06-30 08:45:25,488 main.py:52] epoch 8588, training loss: 5274.03, average training loss: 5435.49, base loss: 15969.12
[INFO 2017-06-30 08:45:28,626 main.py:52] epoch 8589, training loss: 5819.05, average training loss: 5435.87, base loss: 15969.57
[INFO 2017-06-30 08:45:31,775 main.py:52] epoch 8590, training loss: 5548.68, average training loss: 5435.58, base loss: 15969.31
[INFO 2017-06-30 08:45:34,940 main.py:52] epoch 8591, training loss: 5403.37, average training loss: 5435.53, base loss: 15968.91
[INFO 2017-06-30 08:45:38,069 main.py:52] epoch 8592, training loss: 4863.72, average training loss: 5435.22, base loss: 15968.38
[INFO 2017-06-30 08:45:41,229 main.py:52] epoch 8593, training loss: 5506.94, average training loss: 5435.05, base loss: 15968.55
[INFO 2017-06-30 08:45:44,395 main.py:52] epoch 8594, training loss: 5371.41, average training loss: 5434.95, base loss: 15968.61
[INFO 2017-06-30 08:45:47,524 main.py:52] epoch 8595, training loss: 5205.64, average training loss: 5434.48, base loss: 15968.35
[INFO 2017-06-30 08:45:50,728 main.py:52] epoch 8596, training loss: 5545.29, average training loss: 5434.81, base loss: 15968.71
[INFO 2017-06-30 08:45:53,862 main.py:52] epoch 8597, training loss: 5350.64, average training loss: 5434.56, base loss: 15968.96
[INFO 2017-06-30 08:45:57,029 main.py:52] epoch 8598, training loss: 5648.69, average training loss: 5435.11, base loss: 15969.15
[INFO 2017-06-30 08:46:00,205 main.py:52] epoch 8599, training loss: 5343.91, average training loss: 5435.26, base loss: 15969.05
[INFO 2017-06-30 08:46:00,206 main.py:54] epoch 8599, testing
[INFO 2017-06-30 08:46:13,266 main.py:97] average testing loss: 5570.49, base loss: 16279.49
[INFO 2017-06-30 08:46:13,266 main.py:98] improve_loss: 10709.00, improve_percent: 0.66
[INFO 2017-06-30 08:46:13,268 main.py:66] current best improved percent: 0.67
[INFO 2017-06-30 08:46:16,436 main.py:52] epoch 8600, training loss: 5274.15, average training loss: 5434.95, base loss: 15969.43
[INFO 2017-06-30 08:46:19,587 main.py:52] epoch 8601, training loss: 5191.30, average training loss: 5434.53, base loss: 15969.27
[INFO 2017-06-30 08:46:22,740 main.py:52] epoch 8602, training loss: 5237.57, average training loss: 5434.27, base loss: 15968.97
[INFO 2017-06-30 08:46:25,947 main.py:52] epoch 8603, training loss: 5604.01, average training loss: 5434.87, base loss: 15968.82
[INFO 2017-06-30 08:46:29,073 main.py:52] epoch 8604, training loss: 5608.41, average training loss: 5434.93, base loss: 15969.03
[INFO 2017-06-30 08:46:32,273 main.py:52] epoch 8605, training loss: 5071.07, average training loss: 5434.27, base loss: 15969.12
[INFO 2017-06-30 08:46:35,425 main.py:52] epoch 8606, training loss: 5378.03, average training loss: 5434.22, base loss: 15969.11
[INFO 2017-06-30 08:46:38,576 main.py:52] epoch 8607, training loss: 5656.74, average training loss: 5434.52, base loss: 15969.45
[INFO 2017-06-30 08:46:41,727 main.py:52] epoch 8608, training loss: 4950.67, average training loss: 5434.18, base loss: 15969.26
[INFO 2017-06-30 08:46:44,854 main.py:52] epoch 8609, training loss: 5383.10, average training loss: 5434.06, base loss: 15969.18
[INFO 2017-06-30 08:46:47,974 main.py:52] epoch 8610, training loss: 5276.93, average training loss: 5434.09, base loss: 15969.35
[INFO 2017-06-30 08:46:51,114 main.py:52] epoch 8611, training loss: 5579.69, average training loss: 5434.25, base loss: 15969.54
[INFO 2017-06-30 08:46:54,247 main.py:52] epoch 8612, training loss: 5315.43, average training loss: 5433.84, base loss: 15969.38
[INFO 2017-06-30 08:46:57,395 main.py:52] epoch 8613, training loss: 5206.26, average training loss: 5433.77, base loss: 15969.08
[INFO 2017-06-30 08:47:00,516 main.py:52] epoch 8614, training loss: 5589.92, average training loss: 5434.01, base loss: 15969.17
[INFO 2017-06-30 08:47:03,655 main.py:52] epoch 8615, training loss: 5408.40, average training loss: 5434.02, base loss: 15969.17
[INFO 2017-06-30 08:47:06,830 main.py:52] epoch 8616, training loss: 5598.05, average training loss: 5434.66, base loss: 15969.58
[INFO 2017-06-30 08:47:10,012 main.py:52] epoch 8617, training loss: 5483.19, average training loss: 5434.94, base loss: 15970.15
[INFO 2017-06-30 08:47:13,174 main.py:52] epoch 8618, training loss: 5500.55, average training loss: 5434.94, base loss: 15970.48
[INFO 2017-06-30 08:47:16,309 main.py:52] epoch 8619, training loss: 5264.36, average training loss: 5435.17, base loss: 15970.39
[INFO 2017-06-30 08:47:19,436 main.py:52] epoch 8620, training loss: 5448.46, average training loss: 5435.03, base loss: 15970.70
[INFO 2017-06-30 08:47:22,586 main.py:52] epoch 8621, training loss: 5403.47, average training loss: 5434.74, base loss: 15970.69
[INFO 2017-06-30 08:47:25,684 main.py:52] epoch 8622, training loss: 5183.47, average training loss: 5434.82, base loss: 15970.53
[INFO 2017-06-30 08:47:28,841 main.py:52] epoch 8623, training loss: 5500.39, average training loss: 5434.81, base loss: 15970.38
[INFO 2017-06-30 08:47:31,996 main.py:52] epoch 8624, training loss: 5274.65, average training loss: 5434.58, base loss: 15970.33
[INFO 2017-06-30 08:47:35,161 main.py:52] epoch 8625, training loss: 5500.03, average training loss: 5434.50, base loss: 15970.38
[INFO 2017-06-30 08:47:38,275 main.py:52] epoch 8626, training loss: 5423.30, average training loss: 5433.86, base loss: 15970.20
[INFO 2017-06-30 08:47:41,416 main.py:52] epoch 8627, training loss: 5288.74, average training loss: 5433.72, base loss: 15969.80
[INFO 2017-06-30 08:47:44,558 main.py:52] epoch 8628, training loss: 5454.87, average training loss: 5433.52, base loss: 15970.21
[INFO 2017-06-30 08:47:47,694 main.py:52] epoch 8629, training loss: 5658.15, average training loss: 5433.80, base loss: 15970.86
[INFO 2017-06-30 08:47:50,817 main.py:52] epoch 8630, training loss: 5164.56, average training loss: 5433.41, base loss: 15970.46
[INFO 2017-06-30 08:47:53,978 main.py:52] epoch 8631, training loss: 5603.84, average training loss: 5433.26, base loss: 15970.57
[INFO 2017-06-30 08:47:57,118 main.py:52] epoch 8632, training loss: 5195.90, average training loss: 5433.00, base loss: 15970.66
[INFO 2017-06-30 08:48:00,266 main.py:52] epoch 8633, training loss: 5056.79, average training loss: 5432.74, base loss: 15970.62
[INFO 2017-06-30 08:48:03,420 main.py:52] epoch 8634, training loss: 5121.44, average training loss: 5432.35, base loss: 15970.17
[INFO 2017-06-30 08:48:06,589 main.py:52] epoch 8635, training loss: 5923.86, average training loss: 5432.53, base loss: 15970.60
[INFO 2017-06-30 08:48:09,730 main.py:52] epoch 8636, training loss: 5534.63, average training loss: 5432.90, base loss: 15970.67
[INFO 2017-06-30 08:48:12,863 main.py:52] epoch 8637, training loss: 5134.03, average training loss: 5432.78, base loss: 15970.55
[INFO 2017-06-30 08:48:16,021 main.py:52] epoch 8638, training loss: 5583.63, average training loss: 5433.04, base loss: 15970.76
[INFO 2017-06-30 08:48:19,145 main.py:52] epoch 8639, training loss: 5877.29, average training loss: 5433.45, base loss: 15971.15
[INFO 2017-06-30 08:48:22,271 main.py:52] epoch 8640, training loss: 5551.37, average training loss: 5433.46, base loss: 15971.16
[INFO 2017-06-30 08:48:25,403 main.py:52] epoch 8641, training loss: 4965.21, average training loss: 5432.39, base loss: 15970.80
[INFO 2017-06-30 08:48:28,506 main.py:52] epoch 8642, training loss: 5685.47, average training loss: 5432.53, base loss: 15971.10
[INFO 2017-06-30 08:48:31,693 main.py:52] epoch 8643, training loss: 5647.00, average training loss: 5432.93, base loss: 15971.41
[INFO 2017-06-30 08:48:34,828 main.py:52] epoch 8644, training loss: 5676.81, average training loss: 5433.23, base loss: 15972.26
[INFO 2017-06-30 08:48:37,977 main.py:52] epoch 8645, training loss: 5284.18, average training loss: 5433.37, base loss: 15972.47
[INFO 2017-06-30 08:48:41,103 main.py:52] epoch 8646, training loss: 5485.77, average training loss: 5433.46, base loss: 15972.85
[INFO 2017-06-30 08:48:44,264 main.py:52] epoch 8647, training loss: 5396.97, average training loss: 5433.36, base loss: 15972.84
[INFO 2017-06-30 08:48:47,414 main.py:52] epoch 8648, training loss: 5422.91, average training loss: 5433.36, base loss: 15972.85
[INFO 2017-06-30 08:48:50,551 main.py:52] epoch 8649, training loss: 5230.98, average training loss: 5432.87, base loss: 15972.44
[INFO 2017-06-30 08:48:53,808 main.py:52] epoch 8650, training loss: 5559.81, average training loss: 5432.86, base loss: 15972.22
[INFO 2017-06-30 08:48:56,946 main.py:52] epoch 8651, training loss: 5518.55, average training loss: 5433.01, base loss: 15972.38
[INFO 2017-06-30 08:49:00,077 main.py:52] epoch 8652, training loss: 5393.01, average training loss: 5432.76, base loss: 15972.64
[INFO 2017-06-30 08:49:03,221 main.py:52] epoch 8653, training loss: 5453.15, average training loss: 5432.70, base loss: 15973.10
[INFO 2017-06-30 08:49:06,379 main.py:52] epoch 8654, training loss: 5127.74, average training loss: 5432.06, base loss: 15973.12
[INFO 2017-06-30 08:49:09,561 main.py:52] epoch 8655, training loss: 5575.61, average training loss: 5432.13, base loss: 15973.34
[INFO 2017-06-30 08:49:12,711 main.py:52] epoch 8656, training loss: 5258.66, average training loss: 5431.80, base loss: 15973.38
[INFO 2017-06-30 08:49:15,818 main.py:52] epoch 8657, training loss: 5211.08, average training loss: 5431.51, base loss: 15973.60
[INFO 2017-06-30 08:49:18,997 main.py:52] epoch 8658, training loss: 5457.56, average training loss: 5431.65, base loss: 15973.56
[INFO 2017-06-30 08:49:22,170 main.py:52] epoch 8659, training loss: 5440.12, average training loss: 5431.73, base loss: 15973.44
[INFO 2017-06-30 08:49:25,342 main.py:52] epoch 8660, training loss: 5532.04, average training loss: 5432.17, base loss: 15973.60
[INFO 2017-06-30 08:49:28,499 main.py:52] epoch 8661, training loss: 5579.40, average training loss: 5432.27, base loss: 15973.99
[INFO 2017-06-30 08:49:31,655 main.py:52] epoch 8662, training loss: 5715.36, average training loss: 5432.56, base loss: 15974.47
[INFO 2017-06-30 08:49:34,809 main.py:52] epoch 8663, training loss: 5627.52, average training loss: 5432.90, base loss: 15974.80
[INFO 2017-06-30 08:49:37,955 main.py:52] epoch 8664, training loss: 5575.90, average training loss: 5433.15, base loss: 15974.95
[INFO 2017-06-30 08:49:41,157 main.py:52] epoch 8665, training loss: 5427.72, average training loss: 5433.08, base loss: 15975.10
[INFO 2017-06-30 08:49:44,290 main.py:52] epoch 8666, training loss: 4933.76, average training loss: 5433.32, base loss: 15974.71
[INFO 2017-06-30 08:49:47,450 main.py:52] epoch 8667, training loss: 5865.68, average training loss: 5433.75, base loss: 15975.31
[INFO 2017-06-30 08:49:50,610 main.py:52] epoch 8668, training loss: 5543.93, average training loss: 5434.08, base loss: 15975.56
[INFO 2017-06-30 08:49:53,728 main.py:52] epoch 8669, training loss: 4911.23, average training loss: 5433.28, base loss: 15975.03
[INFO 2017-06-30 08:49:56,870 main.py:52] epoch 8670, training loss: 5663.07, average training loss: 5433.42, base loss: 15975.35
[INFO 2017-06-30 08:50:00,016 main.py:52] epoch 8671, training loss: 5168.59, average training loss: 5433.33, base loss: 15975.26
[INFO 2017-06-30 08:50:03,170 main.py:52] epoch 8672, training loss: 4772.67, average training loss: 5432.76, base loss: 15974.87
[INFO 2017-06-30 08:50:06,320 main.py:52] epoch 8673, training loss: 5437.67, average training loss: 5432.44, base loss: 15974.94
[INFO 2017-06-30 08:50:09,485 main.py:52] epoch 8674, training loss: 5236.27, average training loss: 5432.19, base loss: 15974.90
[INFO 2017-06-30 08:50:12,652 main.py:52] epoch 8675, training loss: 5149.99, average training loss: 5432.00, base loss: 15974.58
[INFO 2017-06-30 08:50:15,768 main.py:52] epoch 8676, training loss: 5149.88, average training loss: 5431.83, base loss: 15974.21
[INFO 2017-06-30 08:50:18,859 main.py:52] epoch 8677, training loss: 5739.15, average training loss: 5432.10, base loss: 15974.58
[INFO 2017-06-30 08:50:22,012 main.py:52] epoch 8678, training loss: 5366.94, average training loss: 5432.01, base loss: 15974.34
[INFO 2017-06-30 08:50:25,126 main.py:52] epoch 8679, training loss: 5155.12, average training loss: 5431.96, base loss: 15973.95
[INFO 2017-06-30 08:50:28,237 main.py:52] epoch 8680, training loss: 5451.32, average training loss: 5432.48, base loss: 15974.07
[INFO 2017-06-30 08:50:31,380 main.py:52] epoch 8681, training loss: 5597.56, average training loss: 5432.61, base loss: 15974.61
[INFO 2017-06-30 08:50:34,518 main.py:52] epoch 8682, training loss: 5227.98, average training loss: 5432.39, base loss: 15974.39
[INFO 2017-06-30 08:50:37,649 main.py:52] epoch 8683, training loss: 5744.14, average training loss: 5432.93, base loss: 15974.79
[INFO 2017-06-30 08:50:40,788 main.py:52] epoch 8684, training loss: 5669.25, average training loss: 5432.81, base loss: 15975.29
[INFO 2017-06-30 08:50:43,935 main.py:52] epoch 8685, training loss: 5230.79, average training loss: 5432.62, base loss: 15975.47
[INFO 2017-06-30 08:50:47,060 main.py:52] epoch 8686, training loss: 5257.95, average training loss: 5432.87, base loss: 15975.64
[INFO 2017-06-30 08:50:50,223 main.py:52] epoch 8687, training loss: 5430.54, average training loss: 5432.63, base loss: 15975.69
[INFO 2017-06-30 08:50:53,355 main.py:52] epoch 8688, training loss: 5328.00, average training loss: 5432.38, base loss: 15975.85
[INFO 2017-06-30 08:50:56,451 main.py:52] epoch 8689, training loss: 5582.87, average training loss: 5432.64, base loss: 15976.17
[INFO 2017-06-30 08:50:59,608 main.py:52] epoch 8690, training loss: 5647.76, average training loss: 5432.67, base loss: 15976.37
[INFO 2017-06-30 08:51:02,691 main.py:52] epoch 8691, training loss: 5467.00, average training loss: 5432.95, base loss: 15976.33
[INFO 2017-06-30 08:51:05,849 main.py:52] epoch 8692, training loss: 5366.51, average training loss: 5432.86, base loss: 15976.55
[INFO 2017-06-30 08:51:08,995 main.py:52] epoch 8693, training loss: 5790.81, average training loss: 5433.31, base loss: 15976.82
[INFO 2017-06-30 08:51:12,138 main.py:52] epoch 8694, training loss: 5797.14, average training loss: 5433.69, base loss: 15977.35
[INFO 2017-06-30 08:51:15,288 main.py:52] epoch 8695, training loss: 5220.15, average training loss: 5433.60, base loss: 15977.13
[INFO 2017-06-30 08:51:18,445 main.py:52] epoch 8696, training loss: 5358.02, average training loss: 5433.63, base loss: 15977.05
[INFO 2017-06-30 08:51:21,550 main.py:52] epoch 8697, training loss: 5467.21, average training loss: 5433.75, base loss: 15977.08
[INFO 2017-06-30 08:51:24,694 main.py:52] epoch 8698, training loss: 5597.02, average training loss: 5433.90, base loss: 15976.94
[INFO 2017-06-30 08:51:27,849 main.py:52] epoch 8699, training loss: 5589.02, average training loss: 5433.71, base loss: 15977.09
[INFO 2017-06-30 08:51:27,849 main.py:54] epoch 8699, testing
[INFO 2017-06-30 08:51:40,886 main.py:97] average testing loss: 5397.79, base loss: 15556.92
[INFO 2017-06-30 08:51:40,886 main.py:98] improve_loss: 10159.13, improve_percent: 0.65
[INFO 2017-06-30 08:51:40,889 main.py:66] current best improved percent: 0.67
[INFO 2017-06-30 08:51:44,022 main.py:52] epoch 8700, training loss: 5374.50, average training loss: 5433.33, base loss: 15977.08
[INFO 2017-06-30 08:51:47,159 main.py:52] epoch 8701, training loss: 5484.90, average training loss: 5433.59, base loss: 15977.39
[INFO 2017-06-30 08:51:50,290 main.py:52] epoch 8702, training loss: 5887.37, average training loss: 5434.06, base loss: 15978.00
[INFO 2017-06-30 08:51:53,437 main.py:52] epoch 8703, training loss: 5573.72, average training loss: 5434.23, base loss: 15977.94
[INFO 2017-06-30 08:51:56,555 main.py:52] epoch 8704, training loss: 5250.78, average training loss: 5434.29, base loss: 15977.56
[INFO 2017-06-30 08:51:59,717 main.py:52] epoch 8705, training loss: 5429.57, average training loss: 5434.32, base loss: 15977.15
[INFO 2017-06-30 08:52:02,851 main.py:52] epoch 8706, training loss: 5306.42, average training loss: 5433.77, base loss: 15977.05
[INFO 2017-06-30 08:52:05,984 main.py:52] epoch 8707, training loss: 5763.56, average training loss: 5434.45, base loss: 15977.25
[INFO 2017-06-30 08:52:09,143 main.py:52] epoch 8708, training loss: 5486.21, average training loss: 5434.73, base loss: 15977.30
[INFO 2017-06-30 08:52:12,293 main.py:52] epoch 8709, training loss: 5836.33, average training loss: 5435.38, base loss: 15977.97
[INFO 2017-06-30 08:52:15,440 main.py:52] epoch 8710, training loss: 5608.30, average training loss: 5435.73, base loss: 15978.01
[INFO 2017-06-30 08:52:18,602 main.py:52] epoch 8711, training loss: 5247.59, average training loss: 5435.54, base loss: 15977.86
[INFO 2017-06-30 08:52:21,713 main.py:52] epoch 8712, training loss: 5730.18, average training loss: 5435.83, base loss: 15978.10
[INFO 2017-06-30 08:52:24,859 main.py:52] epoch 8713, training loss: 5193.49, average training loss: 5435.66, base loss: 15977.77
[INFO 2017-06-30 08:52:27,977 main.py:52] epoch 8714, training loss: 5492.38, average training loss: 5436.03, base loss: 15978.03
[INFO 2017-06-30 08:52:31,119 main.py:52] epoch 8715, training loss: 5039.89, average training loss: 5435.92, base loss: 15977.74
[INFO 2017-06-30 08:52:34,247 main.py:52] epoch 8716, training loss: 5193.92, average training loss: 5435.81, base loss: 15977.56
[INFO 2017-06-30 08:52:37,409 main.py:52] epoch 8717, training loss: 5427.88, average training loss: 5435.80, base loss: 15977.47
[INFO 2017-06-30 08:52:40,567 main.py:52] epoch 8718, training loss: 5730.08, average training loss: 5436.39, base loss: 15977.76
[INFO 2017-06-30 08:52:43,730 main.py:52] epoch 8719, training loss: 5582.55, average training loss: 5436.81, base loss: 15977.54
[INFO 2017-06-30 08:52:46,848 main.py:52] epoch 8720, training loss: 5572.68, average training loss: 5436.50, base loss: 15977.69
[INFO 2017-06-30 08:52:49,982 main.py:52] epoch 8721, training loss: 5699.20, average training loss: 5436.60, base loss: 15978.05
[INFO 2017-06-30 08:52:53,100 main.py:52] epoch 8722, training loss: 5672.26, average training loss: 5437.07, base loss: 15978.14
[INFO 2017-06-30 08:52:56,290 main.py:52] epoch 8723, training loss: 5213.55, average training loss: 5436.82, base loss: 15978.03
[INFO 2017-06-30 08:52:59,456 main.py:52] epoch 8724, training loss: 5588.04, average training loss: 5436.43, base loss: 15978.22
[INFO 2017-06-30 08:53:02,582 main.py:52] epoch 8725, training loss: 5259.41, average training loss: 5436.41, base loss: 15978.48
[INFO 2017-06-30 08:53:05,729 main.py:52] epoch 8726, training loss: 5417.84, average training loss: 5436.63, base loss: 15978.66
[INFO 2017-06-30 08:53:08,860 main.py:52] epoch 8727, training loss: 5732.02, average training loss: 5436.64, base loss: 15979.06
[INFO 2017-06-30 08:53:12,016 main.py:52] epoch 8728, training loss: 5644.20, average training loss: 5436.88, base loss: 15979.66
[INFO 2017-06-30 08:53:15,137 main.py:52] epoch 8729, training loss: 5336.14, average training loss: 5436.61, base loss: 15979.71
[INFO 2017-06-30 08:53:18,239 main.py:52] epoch 8730, training loss: 5247.49, average training loss: 5436.50, base loss: 15979.77
[INFO 2017-06-30 08:53:21,347 main.py:52] epoch 8731, training loss: 5362.67, average training loss: 5436.46, base loss: 15979.56
[INFO 2017-06-30 08:53:24,460 main.py:52] epoch 8732, training loss: 5375.23, average training loss: 5436.33, base loss: 15979.82
[INFO 2017-06-30 08:53:27,566 main.py:52] epoch 8733, training loss: 5240.60, average training loss: 5436.57, base loss: 15979.86
[INFO 2017-06-30 08:53:30,711 main.py:52] epoch 8734, training loss: 5897.96, average training loss: 5437.36, base loss: 15980.37
[INFO 2017-06-30 08:53:33,868 main.py:52] epoch 8735, training loss: 5440.93, average training loss: 5437.54, base loss: 15980.34
[INFO 2017-06-30 08:53:36,998 main.py:52] epoch 8736, training loss: 5550.90, average training loss: 5437.86, base loss: 15980.06
[INFO 2017-06-30 08:53:40,129 main.py:52] epoch 8737, training loss: 5568.71, average training loss: 5438.11, base loss: 15980.36
[INFO 2017-06-30 08:53:43,276 main.py:52] epoch 8738, training loss: 5235.87, average training loss: 5438.24, base loss: 15980.42
[INFO 2017-06-30 08:53:46,449 main.py:52] epoch 8739, training loss: 5737.29, average training loss: 5438.48, base loss: 15980.77
[INFO 2017-06-30 08:53:49,614 main.py:52] epoch 8740, training loss: 5271.02, average training loss: 5438.41, base loss: 15980.56
[INFO 2017-06-30 08:53:52,743 main.py:52] epoch 8741, training loss: 5452.50, average training loss: 5438.66, base loss: 15980.71
[INFO 2017-06-30 08:53:55,925 main.py:52] epoch 8742, training loss: 5443.11, average training loss: 5438.17, base loss: 15980.97
[INFO 2017-06-30 08:53:59,052 main.py:52] epoch 8743, training loss: 5711.51, average training loss: 5438.84, base loss: 15981.22
[INFO 2017-06-30 08:54:02,170 main.py:52] epoch 8744, training loss: 5053.50, average training loss: 5438.07, base loss: 15981.19
[INFO 2017-06-30 08:54:05,324 main.py:52] epoch 8745, training loss: 5403.49, average training loss: 5438.15, base loss: 15981.40
[INFO 2017-06-30 08:54:08,455 main.py:52] epoch 8746, training loss: 5292.22, average training loss: 5437.97, base loss: 15981.27
[INFO 2017-06-30 08:54:11,594 main.py:52] epoch 8747, training loss: 5312.13, average training loss: 5438.11, base loss: 15980.90
[INFO 2017-06-30 08:54:14,718 main.py:52] epoch 8748, training loss: 5289.04, average training loss: 5438.10, base loss: 15980.94
[INFO 2017-06-30 08:54:17,877 main.py:52] epoch 8749, training loss: 5239.28, average training loss: 5438.02, base loss: 15980.74
[INFO 2017-06-30 08:54:21,040 main.py:52] epoch 8750, training loss: 5444.86, average training loss: 5437.88, base loss: 15980.77
[INFO 2017-06-30 08:54:24,159 main.py:52] epoch 8751, training loss: 5624.74, average training loss: 5438.35, base loss: 15981.14
[INFO 2017-06-30 08:54:27,263 main.py:52] epoch 8752, training loss: 5333.17, average training loss: 5438.12, base loss: 15980.86
[INFO 2017-06-30 08:54:30,455 main.py:52] epoch 8753, training loss: 5288.06, average training loss: 5437.82, base loss: 15980.54
[INFO 2017-06-30 08:54:33,610 main.py:52] epoch 8754, training loss: 5332.83, average training loss: 5437.43, base loss: 15980.33
[INFO 2017-06-30 08:54:36,787 main.py:52] epoch 8755, training loss: 5063.91, average training loss: 5437.25, base loss: 15980.21
[INFO 2017-06-30 08:54:39,894 main.py:52] epoch 8756, training loss: 5296.40, average training loss: 5436.66, base loss: 15980.04
[INFO 2017-06-30 08:54:43,002 main.py:52] epoch 8757, training loss: 5232.32, average training loss: 5436.22, base loss: 15980.28
[INFO 2017-06-30 08:54:46,113 main.py:52] epoch 8758, training loss: 5172.14, average training loss: 5435.75, base loss: 15980.09
[INFO 2017-06-30 08:54:49,262 main.py:52] epoch 8759, training loss: 5200.20, average training loss: 5435.10, base loss: 15980.22
[INFO 2017-06-30 08:54:52,429 main.py:52] epoch 8760, training loss: 5642.95, average training loss: 5435.27, base loss: 15980.32
[INFO 2017-06-30 08:54:55,610 main.py:52] epoch 8761, training loss: 5144.92, average training loss: 5434.70, base loss: 15980.15
[INFO 2017-06-30 08:54:58,777 main.py:52] epoch 8762, training loss: 5285.67, average training loss: 5434.99, base loss: 15980.00
[INFO 2017-06-30 08:55:01,897 main.py:52] epoch 8763, training loss: 5662.59, average training loss: 5435.21, base loss: 15980.28
[INFO 2017-06-30 08:55:05,061 main.py:52] epoch 8764, training loss: 5349.74, average training loss: 5435.16, base loss: 15980.20
[INFO 2017-06-30 08:55:08,194 main.py:52] epoch 8765, training loss: 4942.48, average training loss: 5434.43, base loss: 15979.72
[INFO 2017-06-30 08:55:11,382 main.py:52] epoch 8766, training loss: 5423.32, average training loss: 5433.88, base loss: 15979.81
[INFO 2017-06-30 08:55:14,513 main.py:52] epoch 8767, training loss: 5327.79, average training loss: 5433.74, base loss: 15979.53
[INFO 2017-06-30 08:55:17,634 main.py:52] epoch 8768, training loss: 5454.39, average training loss: 5433.81, base loss: 15979.39
[INFO 2017-06-30 08:55:20,826 main.py:52] epoch 8769, training loss: 5548.55, average training loss: 5433.88, base loss: 15979.11
[INFO 2017-06-30 08:55:23,986 main.py:52] epoch 8770, training loss: 5489.50, average training loss: 5433.69, base loss: 15979.04
[INFO 2017-06-30 08:55:27,135 main.py:52] epoch 8771, training loss: 5138.23, average training loss: 5433.41, base loss: 15978.72
[INFO 2017-06-30 08:55:30,289 main.py:52] epoch 8772, training loss: 5264.24, average training loss: 5433.18, base loss: 15978.56
[INFO 2017-06-30 08:55:33,445 main.py:52] epoch 8773, training loss: 5552.13, average training loss: 5433.18, base loss: 15978.68
[INFO 2017-06-30 08:55:36,563 main.py:52] epoch 8774, training loss: 5281.68, average training loss: 5432.86, base loss: 15978.74
[INFO 2017-06-30 08:55:39,728 main.py:52] epoch 8775, training loss: 5425.90, average training loss: 5432.76, base loss: 15978.74
[INFO 2017-06-30 08:55:42,891 main.py:52] epoch 8776, training loss: 5003.42, average training loss: 5432.58, base loss: 15978.77
[INFO 2017-06-30 08:55:46,034 main.py:52] epoch 8777, training loss: 5267.42, average training loss: 5432.38, base loss: 15978.75
[INFO 2017-06-30 08:55:49,157 main.py:52] epoch 8778, training loss: 5408.01, average training loss: 5431.98, base loss: 15978.93
[INFO 2017-06-30 08:55:52,283 main.py:52] epoch 8779, training loss: 5445.45, average training loss: 5431.53, base loss: 15978.94
[INFO 2017-06-30 08:55:55,421 main.py:52] epoch 8780, training loss: 5460.29, average training loss: 5431.68, base loss: 15979.22
[INFO 2017-06-30 08:55:58,557 main.py:52] epoch 8781, training loss: 5611.23, average training loss: 5431.82, base loss: 15979.35
[INFO 2017-06-30 08:56:01,698 main.py:52] epoch 8782, training loss: 5528.20, average training loss: 5432.47, base loss: 15979.40
[INFO 2017-06-30 08:56:04,870 main.py:52] epoch 8783, training loss: 5176.07, average training loss: 5432.29, base loss: 15979.25
[INFO 2017-06-30 08:56:08,012 main.py:52] epoch 8784, training loss: 5301.10, average training loss: 5432.12, base loss: 15979.20
[INFO 2017-06-30 08:56:11,190 main.py:52] epoch 8785, training loss: 5301.28, average training loss: 5431.77, base loss: 15979.16
[INFO 2017-06-30 08:56:14,362 main.py:52] epoch 8786, training loss: 5213.32, average training loss: 5431.97, base loss: 15979.12
[INFO 2017-06-30 08:56:17,502 main.py:52] epoch 8787, training loss: 5129.74, average training loss: 5431.83, base loss: 15978.84
[INFO 2017-06-30 08:56:20,641 main.py:52] epoch 8788, training loss: 5413.87, average training loss: 5431.93, base loss: 15978.42
[INFO 2017-06-30 08:56:23,744 main.py:52] epoch 8789, training loss: 4960.00, average training loss: 5431.81, base loss: 15978.23
[INFO 2017-06-30 08:56:26,872 main.py:52] epoch 8790, training loss: 5127.74, average training loss: 5431.40, base loss: 15978.17
[INFO 2017-06-30 08:56:29,991 main.py:52] epoch 8791, training loss: 5744.68, average training loss: 5430.96, base loss: 15978.35
[INFO 2017-06-30 08:56:33,117 main.py:52] epoch 8792, training loss: 5382.34, average training loss: 5430.82, base loss: 15978.42
[INFO 2017-06-30 08:56:36,288 main.py:52] epoch 8793, training loss: 5585.06, average training loss: 5430.84, base loss: 15978.54
[INFO 2017-06-30 08:56:39,412 main.py:52] epoch 8794, training loss: 5074.37, average training loss: 5430.54, base loss: 15978.30
[INFO 2017-06-30 08:56:42,550 main.py:52] epoch 8795, training loss: 5425.74, average training loss: 5430.51, base loss: 15978.27
[INFO 2017-06-30 08:56:45,703 main.py:52] epoch 8796, training loss: 5130.64, average training loss: 5430.36, base loss: 15978.29
[INFO 2017-06-30 08:56:48,836 main.py:52] epoch 8797, training loss: 5665.09, average training loss: 5431.03, base loss: 15978.53
[INFO 2017-06-30 08:56:51,950 main.py:52] epoch 8798, training loss: 5186.38, average training loss: 5431.03, base loss: 15978.24
[INFO 2017-06-30 08:56:55,083 main.py:52] epoch 8799, training loss: 5142.22, average training loss: 5430.76, base loss: 15978.33
[INFO 2017-06-30 08:56:55,083 main.py:54] epoch 8799, testing
[INFO 2017-06-30 08:57:08,160 main.py:97] average testing loss: 5370.36, base loss: 15722.42
[INFO 2017-06-30 08:57:08,161 main.py:98] improve_loss: 10352.06, improve_percent: 0.66
[INFO 2017-06-30 08:57:08,163 main.py:66] current best improved percent: 0.67
[INFO 2017-06-30 08:57:11,289 main.py:52] epoch 8800, training loss: 5428.77, average training loss: 5430.78, base loss: 15978.36
[INFO 2017-06-30 08:57:14,425 main.py:52] epoch 8801, training loss: 5425.75, average training loss: 5430.90, base loss: 15978.49
[INFO 2017-06-30 08:57:17,549 main.py:52] epoch 8802, training loss: 5315.92, average training loss: 5430.54, base loss: 15978.07
[INFO 2017-06-30 08:57:20,668 main.py:52] epoch 8803, training loss: 5704.66, average training loss: 5430.60, base loss: 15978.07
[INFO 2017-06-30 08:57:23,841 main.py:52] epoch 8804, training loss: 5399.71, average training loss: 5430.43, base loss: 15978.26
[INFO 2017-06-30 08:57:26,972 main.py:52] epoch 8805, training loss: 5253.45, average training loss: 5430.18, base loss: 15978.01
[INFO 2017-06-30 08:57:30,096 main.py:52] epoch 8806, training loss: 5049.44, average training loss: 5429.72, base loss: 15978.09
[INFO 2017-06-30 08:57:33,311 main.py:52] epoch 8807, training loss: 5374.08, average training loss: 5429.47, base loss: 15978.44
[INFO 2017-06-30 08:57:36,417 main.py:52] epoch 8808, training loss: 5484.01, average training loss: 5429.66, base loss: 15978.48
[INFO 2017-06-30 08:57:39,526 main.py:52] epoch 8809, training loss: 5023.06, average training loss: 5429.03, base loss: 15978.32
[INFO 2017-06-30 08:57:42,700 main.py:52] epoch 8810, training loss: 5640.21, average training loss: 5428.90, base loss: 15978.42
[INFO 2017-06-30 08:57:45,823 main.py:52] epoch 8811, training loss: 5262.47, average training loss: 5428.44, base loss: 15978.32
[INFO 2017-06-30 08:57:48,942 main.py:52] epoch 8812, training loss: 5174.37, average training loss: 5428.12, base loss: 15978.03
[INFO 2017-06-30 08:57:52,116 main.py:52] epoch 8813, training loss: 5319.81, average training loss: 5427.69, base loss: 15977.98
[INFO 2017-06-30 08:57:55,265 main.py:52] epoch 8814, training loss: 5400.82, average training loss: 5427.61, base loss: 15978.14
[INFO 2017-06-30 08:57:58,391 main.py:52] epoch 8815, training loss: 5533.38, average training loss: 5427.64, base loss: 15978.49
[INFO 2017-06-30 08:58:01,517 main.py:52] epoch 8816, training loss: 5311.23, average training loss: 5427.40, base loss: 15978.59
[INFO 2017-06-30 08:58:04,666 main.py:52] epoch 8817, training loss: 5319.28, average training loss: 5427.77, base loss: 15978.26
[INFO 2017-06-30 08:58:07,781 main.py:52] epoch 8818, training loss: 5421.15, average training loss: 5427.44, base loss: 15978.52
[INFO 2017-06-30 08:58:10,922 main.py:52] epoch 8819, training loss: 5142.28, average training loss: 5427.28, base loss: 15978.50
[INFO 2017-06-30 08:58:14,062 main.py:52] epoch 8820, training loss: 4977.67, average training loss: 5426.42, base loss: 15978.38
[INFO 2017-06-30 08:58:17,247 main.py:52] epoch 8821, training loss: 5071.82, average training loss: 5425.87, base loss: 15978.62
[INFO 2017-06-30 08:58:20,377 main.py:52] epoch 8822, training loss: 5171.17, average training loss: 5425.95, base loss: 15978.30
[INFO 2017-06-30 08:58:23,535 main.py:52] epoch 8823, training loss: 4992.88, average training loss: 5425.01, base loss: 15978.13
[INFO 2017-06-30 08:58:26,686 main.py:52] epoch 8824, training loss: 5039.55, average training loss: 5424.88, base loss: 15977.67
[INFO 2017-06-30 08:58:29,803 main.py:52] epoch 8825, training loss: 5099.56, average training loss: 5424.87, base loss: 15977.61
[INFO 2017-06-30 08:58:32,971 main.py:52] epoch 8826, training loss: 5757.65, average training loss: 5424.99, base loss: 15977.97
[INFO 2017-06-30 08:58:36,131 main.py:52] epoch 8827, training loss: 5335.19, average training loss: 5424.45, base loss: 15977.84
[INFO 2017-06-30 08:58:39,287 main.py:52] epoch 8828, training loss: 5062.66, average training loss: 5424.31, base loss: 15977.32
[INFO 2017-06-30 08:58:42,406 main.py:52] epoch 8829, training loss: 5885.48, average training loss: 5424.69, base loss: 15977.72
[INFO 2017-06-30 08:58:45,559 main.py:52] epoch 8830, training loss: 5156.60, average training loss: 5424.41, base loss: 15977.90
[INFO 2017-06-30 08:58:48,686 main.py:52] epoch 8831, training loss: 5080.91, average training loss: 5423.82, base loss: 15978.13
[INFO 2017-06-30 08:58:51,797 main.py:52] epoch 8832, training loss: 5666.59, average training loss: 5424.09, base loss: 15978.47
[INFO 2017-06-30 08:58:54,962 main.py:52] epoch 8833, training loss: 5139.76, average training loss: 5423.76, base loss: 15978.47
[INFO 2017-06-30 08:58:58,085 main.py:52] epoch 8834, training loss: 5627.05, average training loss: 5424.04, base loss: 15979.16
[INFO 2017-06-30 08:59:01,223 main.py:52] epoch 8835, training loss: 5456.11, average training loss: 5423.63, base loss: 15979.57
[INFO 2017-06-30 08:59:04,387 main.py:52] epoch 8836, training loss: 5457.48, average training loss: 5423.25, base loss: 15979.63
[INFO 2017-06-30 08:59:07,510 main.py:52] epoch 8837, training loss: 5222.48, average training loss: 5423.43, base loss: 15979.49
[INFO 2017-06-30 08:59:10,657 main.py:52] epoch 8838, training loss: 5203.52, average training loss: 5423.16, base loss: 15979.43
[INFO 2017-06-30 08:59:13,801 main.py:52] epoch 8839, training loss: 5348.56, average training loss: 5422.79, base loss: 15979.52
[INFO 2017-06-30 08:59:16,977 main.py:52] epoch 8840, training loss: 5652.12, average training loss: 5422.91, base loss: 15979.99
[INFO 2017-06-30 08:59:20,112 main.py:52] epoch 8841, training loss: 5381.84, average training loss: 5422.23, base loss: 15979.98
[INFO 2017-06-30 08:59:23,239 main.py:52] epoch 8842, training loss: 5547.93, average training loss: 5422.58, base loss: 15980.14
[INFO 2017-06-30 08:59:26,416 main.py:52] epoch 8843, training loss: 5930.20, average training loss: 5422.74, base loss: 15980.47
[INFO 2017-06-30 08:59:29,563 main.py:52] epoch 8844, training loss: 5122.24, average training loss: 5422.74, base loss: 15980.26
[INFO 2017-06-30 08:59:32,679 main.py:52] epoch 8845, training loss: 5343.54, average training loss: 5422.55, base loss: 15980.21
[INFO 2017-06-30 08:59:35,850 main.py:52] epoch 8846, training loss: 5146.62, average training loss: 5422.46, base loss: 15979.73
[INFO 2017-06-30 08:59:39,016 main.py:52] epoch 8847, training loss: 5335.10, average training loss: 5422.62, base loss: 15979.95
[INFO 2017-06-30 08:59:42,138 main.py:52] epoch 8848, training loss: 5188.77, average training loss: 5422.56, base loss: 15979.58
[INFO 2017-06-30 08:59:45,272 main.py:52] epoch 8849, training loss: 5674.77, average training loss: 5423.05, base loss: 15979.91
[INFO 2017-06-30 08:59:48,382 main.py:52] epoch 8850, training loss: 5526.77, average training loss: 5423.36, base loss: 15980.44
[INFO 2017-06-30 08:59:51,511 main.py:52] epoch 8851, training loss: 5441.18, average training loss: 5423.73, base loss: 15980.54
[INFO 2017-06-30 08:59:54,624 main.py:52] epoch 8852, training loss: 5191.34, average training loss: 5423.28, base loss: 15980.74
[INFO 2017-06-30 08:59:57,773 main.py:52] epoch 8853, training loss: 5745.75, average training loss: 5423.80, base loss: 15981.03
[INFO 2017-06-30 09:00:00,879 main.py:52] epoch 8854, training loss: 5254.95, average training loss: 5423.26, base loss: 15981.31
[INFO 2017-06-30 09:00:04,042 main.py:52] epoch 8855, training loss: 5578.52, average training loss: 5423.32, base loss: 15981.78
[INFO 2017-06-30 09:00:07,189 main.py:52] epoch 8856, training loss: 5511.67, average training loss: 5423.37, base loss: 15982.06
[INFO 2017-06-30 09:00:10,373 main.py:52] epoch 8857, training loss: 5961.47, average training loss: 5423.84, base loss: 15982.55
[INFO 2017-06-30 09:00:13,528 main.py:52] epoch 8858, training loss: 5350.42, average training loss: 5423.64, base loss: 15982.82
[INFO 2017-06-30 09:00:16,656 main.py:52] epoch 8859, training loss: 5640.08, average training loss: 5423.66, base loss: 15983.36
[INFO 2017-06-30 09:00:19,817 main.py:52] epoch 8860, training loss: 5285.10, average training loss: 5423.93, base loss: 15983.41
[INFO 2017-06-30 09:00:22,961 main.py:52] epoch 8861, training loss: 5331.99, average training loss: 5423.78, base loss: 15983.44
[INFO 2017-06-30 09:00:26,073 main.py:52] epoch 8862, training loss: 5389.55, average training loss: 5423.96, base loss: 15983.29
[INFO 2017-06-30 09:00:29,262 main.py:52] epoch 8863, training loss: 5383.41, average training loss: 5423.85, base loss: 15983.15
[INFO 2017-06-30 09:00:32,399 main.py:52] epoch 8864, training loss: 5457.31, average training loss: 5424.07, base loss: 15983.27
[INFO 2017-06-30 09:00:35,498 main.py:52] epoch 8865, training loss: 5310.68, average training loss: 5424.15, base loss: 15983.43
[INFO 2017-06-30 09:00:38,677 main.py:52] epoch 8866, training loss: 5343.34, average training loss: 5423.94, base loss: 15983.33
[INFO 2017-06-30 09:00:41,792 main.py:52] epoch 8867, training loss: 5961.12, average training loss: 5424.12, base loss: 15983.65
[INFO 2017-06-30 09:00:44,944 main.py:52] epoch 8868, training loss: 5205.36, average training loss: 5424.31, base loss: 15983.45
[INFO 2017-06-30 09:00:48,082 main.py:52] epoch 8869, training loss: 5210.72, average training loss: 5423.93, base loss: 15983.51
[INFO 2017-06-30 09:00:51,221 main.py:52] epoch 8870, training loss: 5339.70, average training loss: 5423.74, base loss: 15983.11
[INFO 2017-06-30 09:00:54,358 main.py:52] epoch 8871, training loss: 5803.71, average training loss: 5424.09, base loss: 15983.42
[INFO 2017-06-30 09:00:57,541 main.py:52] epoch 8872, training loss: 5183.11, average training loss: 5423.74, base loss: 15983.25
[INFO 2017-06-30 09:01:00,697 main.py:52] epoch 8873, training loss: 4849.34, average training loss: 5423.54, base loss: 15982.82
[INFO 2017-06-30 09:01:03,891 main.py:52] epoch 8874, training loss: 5515.23, average training loss: 5423.57, base loss: 15982.94
[INFO 2017-06-30 09:01:07,014 main.py:52] epoch 8875, training loss: 5559.37, average training loss: 5423.36, base loss: 15983.38
[INFO 2017-06-30 09:01:10,139 main.py:52] epoch 8876, training loss: 5062.78, average training loss: 5422.88, base loss: 15983.05
[INFO 2017-06-30 09:01:13,258 main.py:52] epoch 8877, training loss: 5221.25, average training loss: 5422.60, base loss: 15982.99
[INFO 2017-06-30 09:01:16,467 main.py:52] epoch 8878, training loss: 5481.24, average training loss: 5422.59, base loss: 15982.81
[INFO 2017-06-30 09:01:19,617 main.py:52] epoch 8879, training loss: 5429.16, average training loss: 5422.96, base loss: 15982.52
[INFO 2017-06-30 09:01:22,759 main.py:52] epoch 8880, training loss: 5171.97, average training loss: 5422.60, base loss: 15982.43
[INFO 2017-06-30 09:01:25,948 main.py:52] epoch 8881, training loss: 5362.37, average training loss: 5422.26, base loss: 15982.38
[INFO 2017-06-30 09:01:29,107 main.py:52] epoch 8882, training loss: 5310.83, average training loss: 5421.98, base loss: 15982.44
[INFO 2017-06-30 09:01:32,265 main.py:52] epoch 8883, training loss: 5313.09, average training loss: 5422.06, base loss: 15982.33
[INFO 2017-06-30 09:01:35,421 main.py:52] epoch 8884, training loss: 5249.60, average training loss: 5421.38, base loss: 15982.51
[INFO 2017-06-30 09:01:38,572 main.py:52] epoch 8885, training loss: 5677.87, average training loss: 5421.10, base loss: 15982.61
[INFO 2017-06-30 09:01:41,754 main.py:52] epoch 8886, training loss: 5246.04, average training loss: 5421.11, base loss: 15982.27
[INFO 2017-06-30 09:01:44,886 main.py:52] epoch 8887, training loss: 5311.21, average training loss: 5421.28, base loss: 15982.31
[INFO 2017-06-30 09:01:48,019 main.py:52] epoch 8888, training loss: 5602.16, average training loss: 5420.94, base loss: 15982.27
[INFO 2017-06-30 09:01:51,161 main.py:52] epoch 8889, training loss: 5815.45, average training loss: 5421.21, base loss: 15982.35
[INFO 2017-06-30 09:01:54,291 main.py:52] epoch 8890, training loss: 5350.47, average training loss: 5421.32, base loss: 15982.16
[INFO 2017-06-30 09:01:57,431 main.py:52] epoch 8891, training loss: 5579.76, average training loss: 5421.51, base loss: 15981.98
[INFO 2017-06-30 09:02:00,572 main.py:52] epoch 8892, training loss: 5156.55, average training loss: 5421.35, base loss: 15981.50
[INFO 2017-06-30 09:02:03,715 main.py:52] epoch 8893, training loss: 4865.49, average training loss: 5420.60, base loss: 15981.34
[INFO 2017-06-30 09:02:06,852 main.py:52] epoch 8894, training loss: 5411.88, average training loss: 5420.62, base loss: 15981.44
[INFO 2017-06-30 09:02:09,990 main.py:52] epoch 8895, training loss: 5316.98, average training loss: 5420.59, base loss: 15981.74
[INFO 2017-06-30 09:02:13,130 main.py:52] epoch 8896, training loss: 5206.71, average training loss: 5420.41, base loss: 15981.48
[INFO 2017-06-30 09:02:16,324 main.py:52] epoch 8897, training loss: 5407.00, average training loss: 5420.24, base loss: 15981.49
[INFO 2017-06-30 09:02:19,470 main.py:52] epoch 8898, training loss: 5318.77, average training loss: 5420.37, base loss: 15981.35
[INFO 2017-06-30 09:02:22,628 main.py:52] epoch 8899, training loss: 5558.58, average training loss: 5420.79, base loss: 15981.16
[INFO 2017-06-30 09:02:22,629 main.py:54] epoch 8899, testing
[INFO 2017-06-30 09:02:35,802 main.py:97] average testing loss: 5350.23, base loss: 15607.88
[INFO 2017-06-30 09:02:35,802 main.py:98] improve_loss: 10257.64, improve_percent: 0.66
[INFO 2017-06-30 09:02:35,803 main.py:66] current best improved percent: 0.67
[INFO 2017-06-30 09:02:38,944 main.py:52] epoch 8900, training loss: 5503.10, average training loss: 5421.17, base loss: 15981.01
[INFO 2017-06-30 09:02:42,095 main.py:52] epoch 8901, training loss: 5364.56, average training loss: 5420.95, base loss: 15981.25
[INFO 2017-06-30 09:02:45,250 main.py:52] epoch 8902, training loss: 5459.98, average training loss: 5421.33, base loss: 15981.33
[INFO 2017-06-30 09:02:48,391 main.py:52] epoch 8903, training loss: 5654.33, average training loss: 5421.02, base loss: 15981.48
[INFO 2017-06-30 09:02:51,501 main.py:52] epoch 8904, training loss: 5524.55, average training loss: 5420.90, base loss: 15981.41
[INFO 2017-06-30 09:02:54,648 main.py:52] epoch 8905, training loss: 5148.88, average training loss: 5420.83, base loss: 15981.12
[INFO 2017-06-30 09:02:57,810 main.py:52] epoch 8906, training loss: 5476.40, average training loss: 5420.93, base loss: 15981.48
[INFO 2017-06-30 09:03:00,915 main.py:52] epoch 8907, training loss: 5638.78, average training loss: 5421.08, base loss: 15981.78
[INFO 2017-06-30 09:03:04,088 main.py:52] epoch 8908, training loss: 5748.43, average training loss: 5421.21, base loss: 15981.50
[INFO 2017-06-30 09:03:07,246 main.py:52] epoch 8909, training loss: 5036.72, average training loss: 5420.56, base loss: 15980.91
[INFO 2017-06-30 09:03:10,395 main.py:52] epoch 8910, training loss: 5559.74, average training loss: 5420.90, base loss: 15981.19
[INFO 2017-06-30 09:03:13,550 main.py:52] epoch 8911, training loss: 5210.85, average training loss: 5420.89, base loss: 15981.14
[INFO 2017-06-30 09:03:16,700 main.py:52] epoch 8912, training loss: 5114.93, average training loss: 5420.37, base loss: 15981.09
[INFO 2017-06-30 09:03:19,860 main.py:52] epoch 8913, training loss: 5676.96, average training loss: 5420.77, base loss: 15981.32
[INFO 2017-06-30 09:03:23,016 main.py:52] epoch 8914, training loss: 5200.75, average training loss: 5420.40, base loss: 15981.27
[INFO 2017-06-30 09:03:26,187 main.py:52] epoch 8915, training loss: 5412.80, average training loss: 5420.44, base loss: 15981.21
[INFO 2017-06-30 09:03:29,344 main.py:52] epoch 8916, training loss: 5160.92, average training loss: 5420.07, base loss: 15980.97
[INFO 2017-06-30 09:03:32,500 main.py:52] epoch 8917, training loss: 4891.67, average training loss: 5419.19, base loss: 15980.75
[INFO 2017-06-30 09:03:35,660 main.py:52] epoch 8918, training loss: 5198.68, average training loss: 5418.69, base loss: 15980.39
[INFO 2017-06-30 09:03:38,831 main.py:52] epoch 8919, training loss: 5711.97, average training loss: 5418.82, base loss: 15980.43
[INFO 2017-06-30 09:03:42,011 main.py:52] epoch 8920, training loss: 5344.93, average training loss: 5418.64, base loss: 15980.57
[INFO 2017-06-30 09:03:45,147 main.py:52] epoch 8921, training loss: 5562.06, average training loss: 5418.51, base loss: 15980.87
[INFO 2017-06-30 09:03:48,308 main.py:52] epoch 8922, training loss: 5216.84, average training loss: 5418.41, base loss: 15980.79
[INFO 2017-06-30 09:03:51,433 main.py:52] epoch 8923, training loss: 5024.67, average training loss: 5418.08, base loss: 15980.38
[INFO 2017-06-30 09:03:54,607 main.py:52] epoch 8924, training loss: 5195.04, average training loss: 5417.56, base loss: 15980.39
[INFO 2017-06-30 09:03:57,738 main.py:52] epoch 8925, training loss: 5527.74, average training loss: 5417.88, base loss: 15980.60
[INFO 2017-06-30 09:04:00,938 main.py:52] epoch 8926, training loss: 5200.07, average training loss: 5417.65, base loss: 15980.51
[INFO 2017-06-30 09:04:04,081 main.py:52] epoch 8927, training loss: 5048.52, average training loss: 5417.27, base loss: 15980.39
[INFO 2017-06-30 09:04:07,212 main.py:52] epoch 8928, training loss: 5098.83, average training loss: 5416.94, base loss: 15980.17
[INFO 2017-06-30 09:04:10,367 main.py:52] epoch 8929, training loss: 5492.48, average training loss: 5416.97, base loss: 15980.13
[INFO 2017-06-30 09:04:13,485 main.py:52] epoch 8930, training loss: 5147.50, average training loss: 5416.73, base loss: 15979.88
[INFO 2017-06-30 09:04:16,691 main.py:52] epoch 8931, training loss: 5287.44, average training loss: 5416.82, base loss: 15979.65
[INFO 2017-06-30 09:04:19,852 main.py:52] epoch 8932, training loss: 5063.45, average training loss: 5416.49, base loss: 15979.82
[INFO 2017-06-30 09:04:22,982 main.py:52] epoch 8933, training loss: 5215.09, average training loss: 5416.17, base loss: 15979.55
[INFO 2017-06-30 09:04:26,148 main.py:52] epoch 8934, training loss: 5527.41, average training loss: 5415.90, base loss: 15979.68
[INFO 2017-06-30 09:04:29,274 main.py:52] epoch 8935, training loss: 5273.19, average training loss: 5415.63, base loss: 15979.60
[INFO 2017-06-30 09:04:32,434 main.py:52] epoch 8936, training loss: 5015.86, average training loss: 5415.62, base loss: 15979.19
[INFO 2017-06-30 09:04:35,547 main.py:52] epoch 8937, training loss: 5263.92, average training loss: 5415.48, base loss: 15979.22
[INFO 2017-06-30 09:04:38,712 main.py:52] epoch 8938, training loss: 5164.75, average training loss: 5415.31, base loss: 15979.04
[INFO 2017-06-30 09:04:41,918 main.py:52] epoch 8939, training loss: 5339.47, average training loss: 5415.04, base loss: 15978.94
[INFO 2017-06-30 09:04:45,053 main.py:52] epoch 8940, training loss: 5441.54, average training loss: 5415.18, base loss: 15979.14
[INFO 2017-06-30 09:04:48,206 main.py:52] epoch 8941, training loss: 5969.12, average training loss: 5415.65, base loss: 15979.81
[INFO 2017-06-30 09:04:51,342 main.py:52] epoch 8942, training loss: 5052.34, average training loss: 5415.28, base loss: 15979.62
[INFO 2017-06-30 09:04:54,506 main.py:52] epoch 8943, training loss: 5293.29, average training loss: 5414.95, base loss: 15979.79
[INFO 2017-06-30 09:04:57,634 main.py:52] epoch 8944, training loss: 5224.17, average training loss: 5414.88, base loss: 15979.91
[INFO 2017-06-30 09:05:00,819 main.py:52] epoch 8945, training loss: 5081.14, average training loss: 5414.93, base loss: 15979.67
[INFO 2017-06-30 09:05:03,982 main.py:52] epoch 8946, training loss: 5308.19, average training loss: 5414.98, base loss: 15979.74
[INFO 2017-06-30 09:05:07,089 main.py:52] epoch 8947, training loss: 5298.83, average training loss: 5415.01, base loss: 15979.53
[INFO 2017-06-30 09:05:10,273 main.py:52] epoch 8948, training loss: 5466.87, average training loss: 5415.40, base loss: 15979.70
[INFO 2017-06-30 09:05:13,423 main.py:52] epoch 8949, training loss: 5275.18, average training loss: 5414.74, base loss: 15979.74
[INFO 2017-06-30 09:05:16,577 main.py:52] epoch 8950, training loss: 5197.20, average training loss: 5414.66, base loss: 15979.59
[INFO 2017-06-30 09:05:19,754 main.py:52] epoch 8951, training loss: 5097.60, average training loss: 5414.56, base loss: 15979.79
[INFO 2017-06-30 09:05:22,896 main.py:52] epoch 8952, training loss: 5645.41, average training loss: 5414.54, base loss: 15980.24
[INFO 2017-06-30 09:05:26,020 main.py:52] epoch 8953, training loss: 5177.94, average training loss: 5414.02, base loss: 15980.40
[INFO 2017-06-30 09:05:29,146 main.py:52] epoch 8954, training loss: 5036.10, average training loss: 5413.62, base loss: 15980.39
[INFO 2017-06-30 09:05:32,275 main.py:52] epoch 8955, training loss: 5323.87, average training loss: 5413.77, base loss: 15980.41
[INFO 2017-06-30 09:05:35,386 main.py:52] epoch 8956, training loss: 5185.74, average training loss: 5413.16, base loss: 15980.23
[INFO 2017-06-30 09:05:38,497 main.py:52] epoch 8957, training loss: 5084.23, average training loss: 5412.97, base loss: 15979.94
[INFO 2017-06-30 09:05:41,613 main.py:52] epoch 8958, training loss: 5080.51, average training loss: 5412.68, base loss: 15979.77
[INFO 2017-06-30 09:05:44,789 main.py:52] epoch 8959, training loss: 5385.95, average training loss: 5412.68, base loss: 15979.63
[INFO 2017-06-30 09:05:47,953 main.py:52] epoch 8960, training loss: 5266.35, average training loss: 5412.37, base loss: 15979.50
[INFO 2017-06-30 09:05:51,088 main.py:52] epoch 8961, training loss: 5730.38, average training loss: 5412.85, base loss: 15979.50
[INFO 2017-06-30 09:05:54,278 main.py:52] epoch 8962, training loss: 5066.58, average training loss: 5412.44, base loss: 15979.49
[INFO 2017-06-30 09:05:57,429 main.py:52] epoch 8963, training loss: 5284.29, average training loss: 5412.35, base loss: 15979.62
[INFO 2017-06-30 09:06:00,572 main.py:52] epoch 8964, training loss: 5389.09, average training loss: 5412.18, base loss: 15979.73
[INFO 2017-06-30 09:06:03,692 main.py:52] epoch 8965, training loss: 5424.33, average training loss: 5412.00, base loss: 15979.76
[INFO 2017-06-30 09:06:06,802 main.py:52] epoch 8966, training loss: 5193.22, average training loss: 5411.87, base loss: 15979.53
[INFO 2017-06-30 09:06:10,009 main.py:52] epoch 8967, training loss: 5470.47, average training loss: 5412.19, base loss: 15979.76
[INFO 2017-06-30 09:06:13,148 main.py:52] epoch 8968, training loss: 5680.32, average training loss: 5412.32, base loss: 15980.35
[INFO 2017-06-30 09:06:16,307 main.py:52] epoch 8969, training loss: 5218.65, average training loss: 5411.73, base loss: 15980.46
[INFO 2017-06-30 09:06:19,426 main.py:52] epoch 8970, training loss: 5533.17, average training loss: 5412.11, base loss: 15980.52
[INFO 2017-06-30 09:06:22,535 main.py:52] epoch 8971, training loss: 5492.49, average training loss: 5412.25, base loss: 15980.55
[INFO 2017-06-30 09:06:25,691 main.py:52] epoch 8972, training loss: 5003.57, average training loss: 5412.07, base loss: 15979.94
[INFO 2017-06-30 09:06:28,871 main.py:52] epoch 8973, training loss: 5671.94, average training loss: 5412.17, base loss: 15979.96
[INFO 2017-06-30 09:06:32,014 main.py:52] epoch 8974, training loss: 5403.23, average training loss: 5412.11, base loss: 15979.53
[INFO 2017-06-30 09:06:35,154 main.py:52] epoch 8975, training loss: 5286.64, average training loss: 5412.01, base loss: 15979.53
[INFO 2017-06-30 09:06:38,288 main.py:52] epoch 8976, training loss: 5552.90, average training loss: 5411.93, base loss: 15979.55
[INFO 2017-06-30 09:06:41,452 main.py:52] epoch 8977, training loss: 4990.12, average training loss: 5411.69, base loss: 15979.32
[INFO 2017-06-30 09:06:44,600 main.py:52] epoch 8978, training loss: 5245.55, average training loss: 5411.72, base loss: 15979.43
[INFO 2017-06-30 09:06:47,731 main.py:52] epoch 8979, training loss: 5538.28, average training loss: 5411.78, base loss: 15979.65
[INFO 2017-06-30 09:06:50,848 main.py:52] epoch 8980, training loss: 5112.00, average training loss: 5411.57, base loss: 15979.53
[INFO 2017-06-30 09:06:53,994 main.py:52] epoch 8981, training loss: 5001.10, average training loss: 5410.75, base loss: 15979.23
[INFO 2017-06-30 09:06:57,160 main.py:52] epoch 8982, training loss: 5181.48, average training loss: 5410.62, base loss: 15979.27
[INFO 2017-06-30 09:07:00,333 main.py:52] epoch 8983, training loss: 5938.32, average training loss: 5411.34, base loss: 15979.41
[INFO 2017-06-30 09:07:03,463 main.py:52] epoch 8984, training loss: 5098.30, average training loss: 5410.93, base loss: 15978.94
[INFO 2017-06-30 09:07:06,640 main.py:52] epoch 8985, training loss: 4923.33, average training loss: 5410.69, base loss: 15978.71
[INFO 2017-06-30 09:07:09,796 main.py:52] epoch 8986, training loss: 5204.44, average training loss: 5410.62, base loss: 15978.58
[INFO 2017-06-30 09:07:12,965 main.py:52] epoch 8987, training loss: 4958.82, average training loss: 5410.15, base loss: 15978.18
[INFO 2017-06-30 09:07:16,094 main.py:52] epoch 8988, training loss: 5135.38, average training loss: 5409.90, base loss: 15978.10
[INFO 2017-06-30 09:07:19,207 main.py:52] epoch 8989, training loss: 4973.24, average training loss: 5409.19, base loss: 15977.83
[INFO 2017-06-30 09:07:22,324 main.py:52] epoch 8990, training loss: 5046.74, average training loss: 5408.61, base loss: 15977.75
[INFO 2017-06-30 09:07:25,490 main.py:52] epoch 8991, training loss: 5114.21, average training loss: 5408.03, base loss: 15977.59
[INFO 2017-06-30 09:07:28,611 main.py:52] epoch 8992, training loss: 5362.56, average training loss: 5407.94, base loss: 15977.22
[INFO 2017-06-30 09:07:31,811 main.py:52] epoch 8993, training loss: 5348.97, average training loss: 5407.96, base loss: 15977.30
[INFO 2017-06-30 09:07:34,934 main.py:52] epoch 8994, training loss: 5133.35, average training loss: 5407.70, base loss: 15977.39
[INFO 2017-06-30 09:07:38,133 main.py:52] epoch 8995, training loss: 5541.82, average training loss: 5407.91, base loss: 15977.85
[INFO 2017-06-30 09:07:41,297 main.py:52] epoch 8996, training loss: 5109.61, average training loss: 5407.78, base loss: 15977.81
[INFO 2017-06-30 09:07:44,429 main.py:52] epoch 8997, training loss: 5663.19, average training loss: 5407.81, base loss: 15978.04
[INFO 2017-06-30 09:07:47,593 main.py:52] epoch 8998, training loss: 5422.62, average training loss: 5407.97, base loss: 15977.90
[INFO 2017-06-30 09:07:50,734 main.py:52] epoch 8999, training loss: 5110.91, average training loss: 5407.85, base loss: 15977.77
[INFO 2017-06-30 09:07:50,735 main.py:54] epoch 8999, testing
[INFO 2017-06-30 09:08:03,722 main.py:97] average testing loss: 5386.00, base loss: 15771.17
[INFO 2017-06-30 09:08:03,722 main.py:98] improve_loss: 10385.17, improve_percent: 0.66
[INFO 2017-06-30 09:08:03,724 main.py:66] current best improved percent: 0.67
[INFO 2017-06-30 09:08:06,851 main.py:52] epoch 9000, training loss: 5457.11, average training loss: 5407.97, base loss: 15977.75
[INFO 2017-06-30 09:08:09,998 main.py:52] epoch 9001, training loss: 5512.05, average training loss: 5408.08, base loss: 15978.03
[INFO 2017-06-30 09:08:13,135 main.py:52] epoch 9002, training loss: 5155.27, average training loss: 5407.75, base loss: 15978.08
[INFO 2017-06-30 09:08:16,345 main.py:52] epoch 9003, training loss: 5248.17, average training loss: 5407.50, base loss: 15977.99
[INFO 2017-06-30 09:08:19,535 main.py:52] epoch 9004, training loss: 5177.41, average training loss: 5407.19, base loss: 15977.67
[INFO 2017-06-30 09:08:22,646 main.py:52] epoch 9005, training loss: 5160.74, average training loss: 5407.10, base loss: 15977.49
[INFO 2017-06-30 09:08:25,745 main.py:52] epoch 9006, training loss: 5415.46, average training loss: 5407.11, base loss: 15976.94
[INFO 2017-06-30 09:08:28,916 main.py:52] epoch 9007, training loss: 5618.42, average training loss: 5407.25, base loss: 15976.91
[INFO 2017-06-30 09:08:32,054 main.py:52] epoch 9008, training loss: 5409.44, average training loss: 5407.51, base loss: 15977.03
[INFO 2017-06-30 09:08:35,219 main.py:52] epoch 9009, training loss: 5902.77, average training loss: 5408.15, base loss: 15977.59
[INFO 2017-06-30 09:08:38,391 main.py:52] epoch 9010, training loss: 5263.14, average training loss: 5407.93, base loss: 15977.37
[INFO 2017-06-30 09:08:41,511 main.py:52] epoch 9011, training loss: 5314.56, average training loss: 5408.30, base loss: 15977.04
[INFO 2017-06-30 09:08:44,632 main.py:52] epoch 9012, training loss: 5158.15, average training loss: 5408.32, base loss: 15977.11
[INFO 2017-06-30 09:08:47,822 main.py:52] epoch 9013, training loss: 5326.22, average training loss: 5408.28, base loss: 15976.97
[INFO 2017-06-30 09:08:50,994 main.py:52] epoch 9014, training loss: 5331.26, average training loss: 5408.17, base loss: 15977.02
[INFO 2017-06-30 09:08:54,104 main.py:52] epoch 9015, training loss: 5258.91, average training loss: 5407.75, base loss: 15976.88
[INFO 2017-06-30 09:08:57,199 main.py:52] epoch 9016, training loss: 5478.38, average training loss: 5407.71, base loss: 15977.17
[INFO 2017-06-30 09:09:00,331 main.py:52] epoch 9017, training loss: 5199.04, average training loss: 5407.75, base loss: 15976.90
[INFO 2017-06-30 09:09:03,505 main.py:52] epoch 9018, training loss: 5357.20, average training loss: 5407.75, base loss: 15976.40
[INFO 2017-06-30 09:09:06,663 main.py:52] epoch 9019, training loss: 5614.01, average training loss: 5407.53, base loss: 15976.16
[INFO 2017-06-30 09:09:09,851 main.py:52] epoch 9020, training loss: 5605.97, average training loss: 5407.89, base loss: 15976.56
[INFO 2017-06-30 09:09:12,989 main.py:52] epoch 9021, training loss: 5376.31, average training loss: 5408.26, base loss: 15976.65
[INFO 2017-06-30 09:09:16,145 main.py:52] epoch 9022, training loss: 5205.12, average training loss: 5408.47, base loss: 15976.18
[INFO 2017-06-30 09:09:19,293 main.py:52] epoch 9023, training loss: 5326.35, average training loss: 5408.11, base loss: 15976.28
[INFO 2017-06-30 09:09:22,448 main.py:52] epoch 9024, training loss: 5501.44, average training loss: 5408.64, base loss: 15976.31
[INFO 2017-06-30 09:09:25,626 main.py:52] epoch 9025, training loss: 5243.57, average training loss: 5407.70, base loss: 15976.20
[INFO 2017-06-30 09:09:28,728 main.py:52] epoch 9026, training loss: 5175.18, average training loss: 5407.36, base loss: 15976.02
[INFO 2017-06-30 09:09:31,842 main.py:52] epoch 9027, training loss: 4929.47, average training loss: 5407.43, base loss: 15975.66
[INFO 2017-06-30 09:09:34,962 main.py:52] epoch 9028, training loss: 5421.64, average training loss: 5407.29, base loss: 15975.70
[INFO 2017-06-30 09:09:38,115 main.py:52] epoch 9029, training loss: 5648.94, average training loss: 5407.42, base loss: 15975.96
[INFO 2017-06-30 09:09:41,283 main.py:52] epoch 9030, training loss: 5147.01, average training loss: 5407.34, base loss: 15975.97
[INFO 2017-06-30 09:09:44,405 main.py:52] epoch 9031, training loss: 5722.83, average training loss: 5408.11, base loss: 15976.55
[INFO 2017-06-30 09:09:47,525 main.py:52] epoch 9032, training loss: 5287.61, average training loss: 5407.84, base loss: 15976.41
[INFO 2017-06-30 09:09:50,661 main.py:52] epoch 9033, training loss: 5347.20, average training loss: 5408.01, base loss: 15976.30
[INFO 2017-06-30 09:09:53,800 main.py:52] epoch 9034, training loss: 5365.36, average training loss: 5407.71, base loss: 15976.10
[INFO 2017-06-30 09:09:56,931 main.py:52] epoch 9035, training loss: 4940.70, average training loss: 5407.39, base loss: 15975.83
[INFO 2017-06-30 09:10:00,056 main.py:52] epoch 9036, training loss: 5078.85, average training loss: 5407.36, base loss: 15975.40
[INFO 2017-06-30 09:10:03,206 main.py:52] epoch 9037, training loss: 5863.74, average training loss: 5407.45, base loss: 15975.48
[INFO 2017-06-30 09:10:06,333 main.py:52] epoch 9038, training loss: 5373.31, average training loss: 5407.16, base loss: 15975.41
[INFO 2017-06-30 09:10:09,482 main.py:52] epoch 9039, training loss: 5305.97, average training loss: 5406.90, base loss: 15975.72
[INFO 2017-06-30 09:10:12,639 main.py:52] epoch 9040, training loss: 5483.72, average training loss: 5406.95, base loss: 15975.93
[INFO 2017-06-30 09:10:15,767 main.py:52] epoch 9041, training loss: 5332.98, average training loss: 5406.89, base loss: 15976.33
[INFO 2017-06-30 09:10:18,916 main.py:52] epoch 9042, training loss: 5535.99, average training loss: 5406.94, base loss: 15976.37
[INFO 2017-06-30 09:10:22,036 main.py:52] epoch 9043, training loss: 5513.79, average training loss: 5407.34, base loss: 15976.15
[INFO 2017-06-30 09:10:25,210 main.py:52] epoch 9044, training loss: 5429.03, average training loss: 5407.48, base loss: 15975.85
[INFO 2017-06-30 09:10:28,360 main.py:52] epoch 9045, training loss: 5154.42, average training loss: 5407.24, base loss: 15975.68
[INFO 2017-06-30 09:10:31,531 main.py:52] epoch 9046, training loss: 5572.75, average training loss: 5407.26, base loss: 15975.67
[INFO 2017-06-30 09:10:34,698 main.py:52] epoch 9047, training loss: 5501.28, average training loss: 5407.61, base loss: 15975.59
[INFO 2017-06-30 09:10:37,871 main.py:52] epoch 9048, training loss: 5158.50, average training loss: 5407.28, base loss: 15975.42
[INFO 2017-06-30 09:10:41,047 main.py:52] epoch 9049, training loss: 5057.06, average training loss: 5406.79, base loss: 15975.39
[INFO 2017-06-30 09:10:44,225 main.py:52] epoch 9050, training loss: 5256.07, average training loss: 5406.64, base loss: 15975.37
[INFO 2017-06-30 09:10:47,430 main.py:52] epoch 9051, training loss: 5376.99, average training loss: 5407.03, base loss: 15975.35
[INFO 2017-06-30 09:10:50,537 main.py:52] epoch 9052, training loss: 5573.51, average training loss: 5407.37, base loss: 15975.44
[INFO 2017-06-30 09:10:53,684 main.py:52] epoch 9053, training loss: 5864.74, average training loss: 5407.68, base loss: 15976.03
[INFO 2017-06-30 09:10:56,782 main.py:52] epoch 9054, training loss: 5368.81, average training loss: 5407.64, base loss: 15976.18
[INFO 2017-06-30 09:10:59,947 main.py:52] epoch 9055, training loss: 5553.17, average training loss: 5407.88, base loss: 15976.28
[INFO 2017-06-30 09:11:03,077 main.py:52] epoch 9056, training loss: 5478.36, average training loss: 5407.73, base loss: 15976.09
[INFO 2017-06-30 09:11:06,236 main.py:52] epoch 9057, training loss: 5236.94, average training loss: 5407.21, base loss: 15975.86
[INFO 2017-06-30 09:11:09,452 main.py:52] epoch 9058, training loss: 5430.72, average training loss: 5407.24, base loss: 15975.89
[INFO 2017-06-30 09:11:12,639 main.py:52] epoch 9059, training loss: 5455.85, average training loss: 5407.04, base loss: 15975.88
[INFO 2017-06-30 09:11:15,802 main.py:52] epoch 9060, training loss: 5530.27, average training loss: 5407.30, base loss: 15976.37
[INFO 2017-06-30 09:11:18,967 main.py:52] epoch 9061, training loss: 5210.47, average training loss: 5406.85, base loss: 15976.27
[INFO 2017-06-30 09:11:22,122 main.py:52] epoch 9062, training loss: 5758.55, average training loss: 5407.26, base loss: 15976.27
[INFO 2017-06-30 09:11:25,280 main.py:52] epoch 9063, training loss: 5415.46, average training loss: 5407.22, base loss: 15975.89
[INFO 2017-06-30 09:11:28,465 main.py:52] epoch 9064, training loss: 5557.77, average training loss: 5407.31, base loss: 15976.19
[INFO 2017-06-30 09:11:31,657 main.py:52] epoch 9065, training loss: 5275.79, average training loss: 5407.04, base loss: 15975.97
[INFO 2017-06-30 09:11:34,824 main.py:52] epoch 9066, training loss: 5517.72, average training loss: 5407.64, base loss: 15975.75
[INFO 2017-06-30 09:11:37,951 main.py:52] epoch 9067, training loss: 5073.96, average training loss: 5407.03, base loss: 15975.30
[INFO 2017-06-30 09:11:41,079 main.py:52] epoch 9068, training loss: 5358.53, average training loss: 5406.82, base loss: 15975.28
[INFO 2017-06-30 09:11:44,182 main.py:52] epoch 9069, training loss: 5515.72, average training loss: 5406.92, base loss: 15975.11
[INFO 2017-06-30 09:11:47,324 main.py:52] epoch 9070, training loss: 5095.87, average training loss: 5406.37, base loss: 15974.79
[INFO 2017-06-30 09:11:50,454 main.py:52] epoch 9071, training loss: 5565.56, average training loss: 5406.80, base loss: 15974.77
[INFO 2017-06-30 09:11:53,568 main.py:52] epoch 9072, training loss: 5561.87, average training loss: 5406.96, base loss: 15974.96
[INFO 2017-06-30 09:11:56,727 main.py:52] epoch 9073, training loss: 5338.29, average training loss: 5406.35, base loss: 15974.87
[INFO 2017-06-30 09:11:59,880 main.py:52] epoch 9074, training loss: 5076.95, average training loss: 5405.60, base loss: 15974.91
[INFO 2017-06-30 09:12:03,056 main.py:52] epoch 9075, training loss: 5535.27, average training loss: 5405.97, base loss: 15975.39
[INFO 2017-06-30 09:12:06,187 main.py:52] epoch 9076, training loss: 5082.09, average training loss: 5405.80, base loss: 15975.03
[INFO 2017-06-30 09:12:09,303 main.py:52] epoch 9077, training loss: 5696.36, average training loss: 5405.48, base loss: 15974.99
[INFO 2017-06-30 09:12:12,447 main.py:52] epoch 9078, training loss: 5252.14, average training loss: 5405.33, base loss: 15974.81
[INFO 2017-06-30 09:12:15,581 main.py:52] epoch 9079, training loss: 5238.54, average training loss: 5405.20, base loss: 15974.71
[INFO 2017-06-30 09:12:18,760 main.py:52] epoch 9080, training loss: 5548.35, average training loss: 5405.16, base loss: 15974.75
[INFO 2017-06-30 09:12:21,861 main.py:52] epoch 9081, training loss: 5298.14, average training loss: 5404.94, base loss: 15974.40
[INFO 2017-06-30 09:12:25,000 main.py:52] epoch 9082, training loss: 5273.87, average training loss: 5404.39, base loss: 15974.22
[INFO 2017-06-30 09:12:28,141 main.py:52] epoch 9083, training loss: 5308.19, average training loss: 5404.11, base loss: 15974.36
[INFO 2017-06-30 09:12:31,300 main.py:52] epoch 9084, training loss: 5195.13, average training loss: 5403.96, base loss: 15974.13
[INFO 2017-06-30 09:12:34,510 main.py:52] epoch 9085, training loss: 5102.80, average training loss: 5403.90, base loss: 15973.93
[INFO 2017-06-30 09:12:37,643 main.py:52] epoch 9086, training loss: 5679.15, average training loss: 5404.11, base loss: 15974.18
[INFO 2017-06-30 09:12:40,760 main.py:52] epoch 9087, training loss: 5607.33, average training loss: 5404.05, base loss: 15974.32
[INFO 2017-06-30 09:12:43,910 main.py:52] epoch 9088, training loss: 5648.00, average training loss: 5403.89, base loss: 15974.67
[INFO 2017-06-30 09:12:47,026 main.py:52] epoch 9089, training loss: 5012.21, average training loss: 5403.35, base loss: 15974.51
[INFO 2017-06-30 09:12:50,191 main.py:52] epoch 9090, training loss: 5360.53, average training loss: 5403.26, base loss: 15974.50
[INFO 2017-06-30 09:12:53,333 main.py:52] epoch 9091, training loss: 5591.08, average training loss: 5403.61, base loss: 15974.71
[INFO 2017-06-30 09:12:56,483 main.py:52] epoch 9092, training loss: 5493.01, average training loss: 5403.65, base loss: 15974.26
[INFO 2017-06-30 09:12:59,643 main.py:52] epoch 9093, training loss: 5557.98, average training loss: 5403.81, base loss: 15974.32
[INFO 2017-06-30 09:13:02,789 main.py:52] epoch 9094, training loss: 5524.10, average training loss: 5403.95, base loss: 15974.02
[INFO 2017-06-30 09:13:05,936 main.py:52] epoch 9095, training loss: 5556.04, average training loss: 5404.25, base loss: 15974.14
[INFO 2017-06-30 09:13:09,055 main.py:52] epoch 9096, training loss: 5276.45, average training loss: 5403.84, base loss: 15974.51
[INFO 2017-06-30 09:13:12,198 main.py:52] epoch 9097, training loss: 5624.56, average training loss: 5404.28, base loss: 15974.76
[INFO 2017-06-30 09:13:15,322 main.py:52] epoch 9098, training loss: 5902.32, average training loss: 5404.78, base loss: 15975.56
[INFO 2017-06-30 09:13:18,522 main.py:52] epoch 9099, training loss: 5384.55, average training loss: 5404.73, base loss: 15975.81
[INFO 2017-06-30 09:13:18,522 main.py:54] epoch 9099, testing
[INFO 2017-06-30 09:13:31,603 main.py:97] average testing loss: 5384.95, base loss: 16085.24
[INFO 2017-06-30 09:13:31,603 main.py:98] improve_loss: 10700.29, improve_percent: 0.67
[INFO 2017-06-30 09:13:31,605 main.py:66] current best improved percent: 0.67
[INFO 2017-06-30 09:13:34,781 main.py:52] epoch 9100, training loss: 5344.94, average training loss: 5404.74, base loss: 15975.89
[INFO 2017-06-30 09:13:37,932 main.py:52] epoch 9101, training loss: 5265.99, average training loss: 5404.99, base loss: 15976.28
[INFO 2017-06-30 09:13:41,028 main.py:52] epoch 9102, training loss: 5133.94, average training loss: 5404.95, base loss: 15976.11
[INFO 2017-06-30 09:13:44,135 main.py:52] epoch 9103, training loss: 5733.31, average training loss: 5405.26, base loss: 15976.29
[INFO 2017-06-30 09:13:47,280 main.py:52] epoch 9104, training loss: 5561.24, average training loss: 5405.20, base loss: 15976.60
[INFO 2017-06-30 09:13:50,419 main.py:52] epoch 9105, training loss: 5150.77, average training loss: 5404.93, base loss: 15976.61
[INFO 2017-06-30 09:13:53,589 main.py:52] epoch 9106, training loss: 5562.91, average training loss: 5405.13, base loss: 15976.81
[INFO 2017-06-30 09:13:56,907 main.py:52] epoch 9107, training loss: 5180.30, average training loss: 5404.93, base loss: 15976.66
[INFO 2017-06-30 09:14:00,080 main.py:52] epoch 9108, training loss: 5285.79, average training loss: 5404.76, base loss: 15976.75
[INFO 2017-06-30 09:14:03,188 main.py:52] epoch 9109, training loss: 5351.48, average training loss: 5404.72, base loss: 15976.82
[INFO 2017-06-30 09:14:06,318 main.py:52] epoch 9110, training loss: 5349.56, average training loss: 5404.83, base loss: 15976.92
[INFO 2017-06-30 09:14:09,488 main.py:52] epoch 9111, training loss: 6015.01, average training loss: 5405.64, base loss: 15977.48
[INFO 2017-06-30 09:14:12,699 main.py:52] epoch 9112, training loss: 5546.13, average training loss: 5405.62, base loss: 15977.53
[INFO 2017-06-30 09:14:15,843 main.py:52] epoch 9113, training loss: 4866.27, average training loss: 5404.96, base loss: 15976.95
[INFO 2017-06-30 09:14:19,013 main.py:52] epoch 9114, training loss: 5798.71, average training loss: 5405.19, base loss: 15977.27
[INFO 2017-06-30 09:14:22,131 main.py:52] epoch 9115, training loss: 5160.14, average training loss: 5404.92, base loss: 15977.23
[INFO 2017-06-30 09:14:25,268 main.py:52] epoch 9116, training loss: 5602.02, average training loss: 5405.12, base loss: 15977.22
[INFO 2017-06-30 09:14:28,458 main.py:52] epoch 9117, training loss: 5509.63, average training loss: 5405.12, base loss: 15976.69
[INFO 2017-06-30 09:14:31,586 main.py:52] epoch 9118, training loss: 5591.06, average training loss: 5405.20, base loss: 15976.89
[INFO 2017-06-30 09:14:34,692 main.py:52] epoch 9119, training loss: 5215.02, average training loss: 5405.43, base loss: 15976.92
[INFO 2017-06-30 09:14:37,903 main.py:52] epoch 9120, training loss: 4917.57, average training loss: 5405.24, base loss: 15976.88
[INFO 2017-06-30 09:14:41,063 main.py:52] epoch 9121, training loss: 5179.56, average training loss: 5404.69, base loss: 15976.71
[INFO 2017-06-30 09:14:44,194 main.py:52] epoch 9122, training loss: 5388.89, average training loss: 5404.44, base loss: 15976.55
[INFO 2017-06-30 09:14:47,320 main.py:52] epoch 9123, training loss: 5579.11, average training loss: 5404.71, base loss: 15976.71
[INFO 2017-06-30 09:14:50,459 main.py:52] epoch 9124, training loss: 5442.25, average training loss: 5404.76, base loss: 15976.68
[INFO 2017-06-30 09:14:53,643 main.py:52] epoch 9125, training loss: 5334.50, average training loss: 5404.63, base loss: 15976.41
[INFO 2017-06-30 09:14:56,792 main.py:52] epoch 9126, training loss: 5075.90, average training loss: 5404.63, base loss: 15976.21
[INFO 2017-06-30 09:14:59,930 main.py:52] epoch 9127, training loss: 5246.40, average training loss: 5404.82, base loss: 15975.89
[INFO 2017-06-30 09:15:03,098 main.py:52] epoch 9128, training loss: 5472.53, average training loss: 5404.52, base loss: 15975.92
[INFO 2017-06-30 09:15:06,263 main.py:52] epoch 9129, training loss: 5263.61, average training loss: 5404.42, base loss: 15975.92
[INFO 2017-06-30 09:15:09,393 main.py:52] epoch 9130, training loss: 5543.30, average training loss: 5404.54, base loss: 15976.19
[INFO 2017-06-30 09:15:12,582 main.py:52] epoch 9131, training loss: 5271.01, average training loss: 5404.03, base loss: 15975.98
[INFO 2017-06-30 09:15:15,773 main.py:52] epoch 9132, training loss: 5941.68, average training loss: 5404.85, base loss: 15976.28
[INFO 2017-06-30 09:15:18,933 main.py:52] epoch 9133, training loss: 5539.06, average training loss: 5405.31, base loss: 15976.64
[INFO 2017-06-30 09:15:22,078 main.py:52] epoch 9134, training loss: 5321.40, average training loss: 5405.23, base loss: 15976.47
[INFO 2017-06-30 09:15:25,219 main.py:52] epoch 9135, training loss: 5815.93, average training loss: 5405.76, base loss: 15976.82
[INFO 2017-06-30 09:15:28,417 main.py:52] epoch 9136, training loss: 5337.95, average training loss: 5405.37, base loss: 15976.93
[INFO 2017-06-30 09:15:31,586 main.py:52] epoch 9137, training loss: 5689.18, average training loss: 5405.36, base loss: 15977.35
[INFO 2017-06-30 09:15:34,727 main.py:52] epoch 9138, training loss: 5107.97, average training loss: 5404.73, base loss: 15977.22
[INFO 2017-06-30 09:15:37,867 main.py:52] epoch 9139, training loss: 5479.87, average training loss: 5404.55, base loss: 15977.30
[INFO 2017-06-30 09:15:40,996 main.py:52] epoch 9140, training loss: 5221.81, average training loss: 5404.43, base loss: 15977.10
[INFO 2017-06-30 09:15:44,156 main.py:52] epoch 9141, training loss: 5110.54, average training loss: 5404.21, base loss: 15977.14
[INFO 2017-06-30 09:15:47,308 main.py:52] epoch 9142, training loss: 5392.88, average training loss: 5404.22, base loss: 15977.18
[INFO 2017-06-30 09:15:50,425 main.py:52] epoch 9143, training loss: 5448.45, average training loss: 5404.53, base loss: 15977.20
[INFO 2017-06-30 09:15:53,536 main.py:52] epoch 9144, training loss: 5111.69, average training loss: 5404.31, base loss: 15976.92
[INFO 2017-06-30 09:15:56,648 main.py:52] epoch 9145, training loss: 5546.88, average training loss: 5404.66, base loss: 15977.24
[INFO 2017-06-30 09:15:59,789 main.py:52] epoch 9146, training loss: 5820.92, average training loss: 5405.39, base loss: 15977.38
[INFO 2017-06-30 09:16:02,930 main.py:52] epoch 9147, training loss: 5507.55, average training loss: 5405.64, base loss: 15977.45
[INFO 2017-06-30 09:16:06,134 main.py:52] epoch 9148, training loss: 5167.14, average training loss: 5405.29, base loss: 15977.01
[INFO 2017-06-30 09:16:09,282 main.py:52] epoch 9149, training loss: 5273.16, average training loss: 5405.12, base loss: 15976.73
[INFO 2017-06-30 09:16:12,420 main.py:52] epoch 9150, training loss: 5529.52, average training loss: 5405.09, base loss: 15976.48
[INFO 2017-06-30 09:16:15,560 main.py:52] epoch 9151, training loss: 5329.01, average training loss: 5404.94, base loss: 15976.33
[INFO 2017-06-30 09:16:18,679 main.py:52] epoch 9152, training loss: 5292.26, average training loss: 5405.06, base loss: 15975.96
[INFO 2017-06-30 09:16:21,787 main.py:52] epoch 9153, training loss: 5437.96, average training loss: 5405.06, base loss: 15975.70
[INFO 2017-06-30 09:16:24,898 main.py:52] epoch 9154, training loss: 5597.34, average training loss: 5405.27, base loss: 15975.85
[INFO 2017-06-30 09:16:28,072 main.py:52] epoch 9155, training loss: 5480.85, average training loss: 5405.11, base loss: 15975.94
[INFO 2017-06-30 09:16:31,212 main.py:52] epoch 9156, training loss: 5447.66, average training loss: 5404.92, base loss: 15975.82
[INFO 2017-06-30 09:16:34,393 main.py:52] epoch 9157, training loss: 5578.09, average training loss: 5405.31, base loss: 15976.28
[INFO 2017-06-30 09:16:37,532 main.py:52] epoch 9158, training loss: 5338.19, average training loss: 5405.32, base loss: 15976.04
[INFO 2017-06-30 09:16:40,697 main.py:52] epoch 9159, training loss: 5546.23, average training loss: 5405.55, base loss: 15976.26
[INFO 2017-06-30 09:16:43,813 main.py:52] epoch 9160, training loss: 5027.58, average training loss: 5405.07, base loss: 15975.97
[INFO 2017-06-30 09:16:46,923 main.py:52] epoch 9161, training loss: 5505.98, average training loss: 5405.33, base loss: 15976.16
[INFO 2017-06-30 09:16:50,022 main.py:52] epoch 9162, training loss: 5236.80, average training loss: 5405.13, base loss: 15975.90
[INFO 2017-06-30 09:16:53,196 main.py:52] epoch 9163, training loss: 5070.88, average training loss: 5404.71, base loss: 15975.68
[INFO 2017-06-30 09:16:56,293 main.py:52] epoch 9164, training loss: 5231.67, average training loss: 5404.41, base loss: 15975.31
[INFO 2017-06-30 09:16:59,448 main.py:52] epoch 9165, training loss: 5540.91, average training loss: 5404.50, base loss: 15975.37
[INFO 2017-06-30 09:17:02,590 main.py:52] epoch 9166, training loss: 5065.46, average training loss: 5404.34, base loss: 15975.06
[INFO 2017-06-30 09:17:05,751 main.py:52] epoch 9167, training loss: 5434.86, average training loss: 5404.57, base loss: 15974.88
[INFO 2017-06-30 09:17:08,888 main.py:52] epoch 9168, training loss: 5284.91, average training loss: 5404.30, base loss: 15974.83
[INFO 2017-06-30 09:17:12,034 main.py:52] epoch 9169, training loss: 5138.94, average training loss: 5403.93, base loss: 15974.23
[INFO 2017-06-30 09:17:15,193 main.py:52] epoch 9170, training loss: 5836.79, average training loss: 5404.46, base loss: 15974.35
[INFO 2017-06-30 09:17:18,376 main.py:52] epoch 9171, training loss: 5493.80, average training loss: 5403.79, base loss: 15974.00
[INFO 2017-06-30 09:17:21,534 main.py:52] epoch 9172, training loss: 5070.88, average training loss: 5403.55, base loss: 15974.00
[INFO 2017-06-30 09:17:24,675 main.py:52] epoch 9173, training loss: 5271.10, average training loss: 5403.42, base loss: 15973.64
[INFO 2017-06-30 09:17:27,846 main.py:52] epoch 9174, training loss: 5237.81, average training loss: 5403.57, base loss: 15973.41
[INFO 2017-06-30 09:17:30,984 main.py:52] epoch 9175, training loss: 5340.00, average training loss: 5403.70, base loss: 15973.59
[INFO 2017-06-30 09:17:34,127 main.py:52] epoch 9176, training loss: 5753.92, average training loss: 5403.93, base loss: 15973.89
[INFO 2017-06-30 09:17:37,268 main.py:52] epoch 9177, training loss: 5583.00, average training loss: 5404.56, base loss: 15974.16
[INFO 2017-06-30 09:17:40,445 main.py:52] epoch 9178, training loss: 5513.25, average training loss: 5404.57, base loss: 15974.31
[INFO 2017-06-30 09:17:43,602 main.py:52] epoch 9179, training loss: 5518.17, average training loss: 5404.28, base loss: 15974.24
[INFO 2017-06-30 09:17:46,785 main.py:52] epoch 9180, training loss: 5194.29, average training loss: 5404.13, base loss: 15974.19
[INFO 2017-06-30 09:17:49,918 main.py:52] epoch 9181, training loss: 5523.88, average training loss: 5404.14, base loss: 15974.17
[INFO 2017-06-30 09:17:53,095 main.py:52] epoch 9182, training loss: 5143.74, average training loss: 5403.92, base loss: 15974.17
[INFO 2017-06-30 09:17:56,266 main.py:52] epoch 9183, training loss: 5269.46, average training loss: 5403.48, base loss: 15974.22
[INFO 2017-06-30 09:17:59,397 main.py:52] epoch 9184, training loss: 5441.08, average training loss: 5403.37, base loss: 15974.49
[INFO 2017-06-30 09:18:02,554 main.py:52] epoch 9185, training loss: 5418.18, average training loss: 5403.12, base loss: 15974.68
[INFO 2017-06-30 09:18:05,688 main.py:52] epoch 9186, training loss: 5499.61, average training loss: 5403.11, base loss: 15974.72
[INFO 2017-06-30 09:18:08,859 main.py:52] epoch 9187, training loss: 5235.48, average training loss: 5402.91, base loss: 15974.50
[INFO 2017-06-30 09:18:12,046 main.py:52] epoch 9188, training loss: 5533.87, average training loss: 5402.72, base loss: 15975.02
[INFO 2017-06-30 09:18:15,226 main.py:52] epoch 9189, training loss: 5083.38, average training loss: 5402.52, base loss: 15975.23
[INFO 2017-06-30 09:18:18,413 main.py:52] epoch 9190, training loss: 5485.12, average training loss: 5402.54, base loss: 15975.35
[INFO 2017-06-30 09:18:21,583 main.py:52] epoch 9191, training loss: 5484.70, average training loss: 5402.69, base loss: 15975.44
[INFO 2017-06-30 09:18:24,722 main.py:52] epoch 9192, training loss: 5717.82, average training loss: 5403.35, base loss: 15975.52
[INFO 2017-06-30 09:18:27,895 main.py:52] epoch 9193, training loss: 5605.07, average training loss: 5403.68, base loss: 15975.65
[INFO 2017-06-30 09:18:31,029 main.py:52] epoch 9194, training loss: 5090.09, average training loss: 5403.33, base loss: 15975.61
[INFO 2017-06-30 09:18:34,156 main.py:52] epoch 9195, training loss: 4591.61, average training loss: 5402.69, base loss: 15975.19
[INFO 2017-06-30 09:18:37,330 main.py:52] epoch 9196, training loss: 5507.76, average training loss: 5402.83, base loss: 15975.24
[INFO 2017-06-30 09:18:40,519 main.py:52] epoch 9197, training loss: 5367.89, average training loss: 5403.14, base loss: 15975.00
[INFO 2017-06-30 09:18:43,676 main.py:52] epoch 9198, training loss: 5139.03, average training loss: 5402.90, base loss: 15974.97
[INFO 2017-06-30 09:18:46,828 main.py:52] epoch 9199, training loss: 5522.28, average training loss: 5402.90, base loss: 15974.93
[INFO 2017-06-30 09:18:46,828 main.py:54] epoch 9199, testing
[INFO 2017-06-30 09:18:59,933 main.py:97] average testing loss: 5530.95, base loss: 16227.93
[INFO 2017-06-30 09:18:59,934 main.py:98] improve_loss: 10696.98, improve_percent: 0.66
[INFO 2017-06-30 09:18:59,936 main.py:66] current best improved percent: 0.67
[INFO 2017-06-30 09:19:03,064 main.py:52] epoch 9200, training loss: 5293.41, average training loss: 5403.04, base loss: 15974.72
[INFO 2017-06-30 09:19:06,236 main.py:52] epoch 9201, training loss: 5108.28, average training loss: 5403.13, base loss: 15974.27
[INFO 2017-06-30 09:19:09,353 main.py:52] epoch 9202, training loss: 5405.47, average training loss: 5403.27, base loss: 15973.94
[INFO 2017-06-30 09:19:12,520 main.py:52] epoch 9203, training loss: 5436.15, average training loss: 5403.56, base loss: 15973.53
[INFO 2017-06-30 09:19:15,689 main.py:52] epoch 9204, training loss: 5534.25, average training loss: 5403.75, base loss: 15973.75
[INFO 2017-06-30 09:19:18,836 main.py:52] epoch 9205, training loss: 5532.45, average training loss: 5403.72, base loss: 15973.87
[INFO 2017-06-30 09:19:21,997 main.py:52] epoch 9206, training loss: 5367.83, average training loss: 5403.58, base loss: 15973.98
[INFO 2017-06-30 09:19:25,121 main.py:52] epoch 9207, training loss: 5284.38, average training loss: 5403.07, base loss: 15973.95
[INFO 2017-06-30 09:19:28,252 main.py:52] epoch 9208, training loss: 5375.28, average training loss: 5402.87, base loss: 15973.72
[INFO 2017-06-30 09:19:31,405 main.py:52] epoch 9209, training loss: 5490.74, average training loss: 5402.85, base loss: 15973.64
[INFO 2017-06-30 09:19:34,573 main.py:52] epoch 9210, training loss: 5653.64, average training loss: 5402.95, base loss: 15974.07
[INFO 2017-06-30 09:19:37,720 main.py:52] epoch 9211, training loss: 5065.90, average training loss: 5402.57, base loss: 15973.63
[INFO 2017-06-30 09:19:40,879 main.py:52] epoch 9212, training loss: 5599.06, average training loss: 5402.97, base loss: 15973.76
[INFO 2017-06-30 09:19:44,017 main.py:52] epoch 9213, training loss: 5689.57, average training loss: 5403.04, base loss: 15974.03
[INFO 2017-06-30 09:19:47,131 main.py:52] epoch 9214, training loss: 5570.74, average training loss: 5403.30, base loss: 15973.95
[INFO 2017-06-30 09:19:50,255 main.py:52] epoch 9215, training loss: 4964.87, average training loss: 5402.81, base loss: 15973.75
[INFO 2017-06-30 09:19:53,415 main.py:52] epoch 9216, training loss: 5397.38, average training loss: 5402.61, base loss: 15974.04
[INFO 2017-06-30 09:19:56,522 main.py:52] epoch 9217, training loss: 5478.79, average training loss: 5402.20, base loss: 15974.13
[INFO 2017-06-30 09:19:59,684 main.py:52] epoch 9218, training loss: 5181.34, average training loss: 5401.93, base loss: 15974.07
[INFO 2017-06-30 09:20:02,830 main.py:52] epoch 9219, training loss: 5459.51, average training loss: 5402.32, base loss: 15974.37
[INFO 2017-06-30 09:20:05,976 main.py:52] epoch 9220, training loss: 5630.14, average training loss: 5402.12, base loss: 15974.67
[INFO 2017-06-30 09:20:09,075 main.py:52] epoch 9221, training loss: 5437.25, average training loss: 5402.18, base loss: 15974.54
[INFO 2017-06-30 09:20:12,253 main.py:52] epoch 9222, training loss: 5551.29, average training loss: 5402.63, base loss: 15974.61
[INFO 2017-06-30 09:20:15,391 main.py:52] epoch 9223, training loss: 5289.95, average training loss: 5402.39, base loss: 15974.61
[INFO 2017-06-30 09:20:18,500 main.py:52] epoch 9224, training loss: 6156.01, average training loss: 5403.07, base loss: 15975.30
[INFO 2017-06-30 09:20:21,594 main.py:52] epoch 9225, training loss: 5363.71, average training loss: 5402.82, base loss: 15975.34
[INFO 2017-06-30 09:20:24,704 main.py:52] epoch 9226, training loss: 5538.66, average training loss: 5402.69, base loss: 15975.03
[INFO 2017-06-30 09:20:27,851 main.py:52] epoch 9227, training loss: 5217.78, average training loss: 5402.52, base loss: 15974.98
[INFO 2017-06-30 09:20:30,981 main.py:52] epoch 9228, training loss: 5132.59, average training loss: 5402.23, base loss: 15974.81
[INFO 2017-06-30 09:20:34,105 main.py:52] epoch 9229, training loss: 5504.67, average training loss: 5402.43, base loss: 15975.00
[INFO 2017-06-30 09:20:37,240 main.py:52] epoch 9230, training loss: 5260.78, average training loss: 5402.32, base loss: 15974.75
[INFO 2017-06-30 09:20:40,383 main.py:52] epoch 9231, training loss: 5328.39, average training loss: 5402.39, base loss: 15974.51
[INFO 2017-06-30 09:20:43,494 main.py:52] epoch 9232, training loss: 5317.77, average training loss: 5402.22, base loss: 15974.39
[INFO 2017-06-30 09:20:46,660 main.py:52] epoch 9233, training loss: 5503.99, average training loss: 5402.16, base loss: 15974.74
[INFO 2017-06-30 09:20:49,779 main.py:52] epoch 9234, training loss: 5778.99, average training loss: 5402.70, base loss: 15975.06
[INFO 2017-06-30 09:20:52,909 main.py:52] epoch 9235, training loss: 5365.42, average training loss: 5402.48, base loss: 15974.93
[INFO 2017-06-30 09:20:56,043 main.py:52] epoch 9236, training loss: 5506.06, average training loss: 5402.49, base loss: 15974.94
[INFO 2017-06-30 09:20:59,192 main.py:52] epoch 9237, training loss: 5245.81, average training loss: 5402.21, base loss: 15974.85
[INFO 2017-06-30 09:21:02,337 main.py:52] epoch 9238, training loss: 5326.96, average training loss: 5402.31, base loss: 15975.06
[INFO 2017-06-30 09:21:05,470 main.py:52] epoch 9239, training loss: 5446.07, average training loss: 5402.27, base loss: 15975.43
[INFO 2017-06-30 09:21:08,602 main.py:52] epoch 9240, training loss: 5675.91, average training loss: 5402.46, base loss: 15975.33
[INFO 2017-06-30 09:21:11,744 main.py:52] epoch 9241, training loss: 5258.58, average training loss: 5402.16, base loss: 15975.34
[INFO 2017-06-30 09:21:14,895 main.py:52] epoch 9242, training loss: 5181.08, average training loss: 5401.89, base loss: 15975.44
[INFO 2017-06-30 09:21:18,041 main.py:52] epoch 9243, training loss: 5955.35, average training loss: 5402.47, base loss: 15976.05
[INFO 2017-06-30 09:21:21,207 main.py:52] epoch 9244, training loss: 5031.40, average training loss: 5401.96, base loss: 15975.74
[INFO 2017-06-30 09:21:24,339 main.py:52] epoch 9245, training loss: 5352.04, average training loss: 5401.96, base loss: 15975.92
[INFO 2017-06-30 09:21:27,504 main.py:52] epoch 9246, training loss: 5555.50, average training loss: 5402.19, base loss: 15976.18
[INFO 2017-06-30 09:21:30,653 main.py:52] epoch 9247, training loss: 4882.20, average training loss: 5401.93, base loss: 15975.94
[INFO 2017-06-30 09:21:33,786 main.py:52] epoch 9248, training loss: 5089.90, average training loss: 5401.30, base loss: 15975.41
[INFO 2017-06-30 09:21:36,947 main.py:52] epoch 9249, training loss: 5296.54, average training loss: 5401.00, base loss: 15975.60
[INFO 2017-06-30 09:21:40,088 main.py:52] epoch 9250, training loss: 5516.76, average training loss: 5400.86, base loss: 15975.64
[INFO 2017-06-30 09:21:43,265 main.py:52] epoch 9251, training loss: 5240.49, average training loss: 5400.80, base loss: 15975.54
[INFO 2017-06-30 09:21:46,447 main.py:52] epoch 9252, training loss: 5350.28, average training loss: 5400.44, base loss: 15976.01
[INFO 2017-06-30 09:21:49,580 main.py:52] epoch 9253, training loss: 5271.23, average training loss: 5400.22, base loss: 15976.50
[INFO 2017-06-30 09:21:52,720 main.py:52] epoch 9254, training loss: 5310.39, average training loss: 5399.91, base loss: 15976.73
[INFO 2017-06-30 09:21:55,863 main.py:52] epoch 9255, training loss: 5378.28, average training loss: 5399.95, base loss: 15977.10
[INFO 2017-06-30 09:21:58,984 main.py:52] epoch 9256, training loss: 5239.47, average training loss: 5399.83, base loss: 15976.92
[INFO 2017-06-30 09:22:02,109 main.py:52] epoch 9257, training loss: 5183.87, average training loss: 5399.48, base loss: 15976.89
[INFO 2017-06-30 09:22:05,310 main.py:52] epoch 9258, training loss: 5738.65, average training loss: 5399.89, base loss: 15977.15
[INFO 2017-06-30 09:22:08,468 main.py:52] epoch 9259, training loss: 5396.52, average training loss: 5399.77, base loss: 15977.20
[INFO 2017-06-30 09:22:11,607 main.py:52] epoch 9260, training loss: 5549.76, average training loss: 5400.05, base loss: 15977.46
[INFO 2017-06-30 09:22:14,792 main.py:52] epoch 9261, training loss: 5287.25, average training loss: 5399.46, base loss: 15977.79
[INFO 2017-06-30 09:22:17,978 main.py:52] epoch 9262, training loss: 5411.11, average training loss: 5399.66, base loss: 15977.38
[INFO 2017-06-30 09:22:21,142 main.py:52] epoch 9263, training loss: 5171.46, average training loss: 5399.42, base loss: 15977.33
[INFO 2017-06-30 09:22:24,255 main.py:52] epoch 9264, training loss: 5294.79, average training loss: 5399.57, base loss: 15977.25
[INFO 2017-06-30 09:22:27,394 main.py:52] epoch 9265, training loss: 5433.57, average training loss: 5399.86, base loss: 15977.18
[INFO 2017-06-30 09:22:30,542 main.py:52] epoch 9266, training loss: 5507.24, average training loss: 5400.02, base loss: 15976.83
[INFO 2017-06-30 09:22:33,669 main.py:52] epoch 9267, training loss: 4981.77, average training loss: 5399.52, base loss: 15976.51
[INFO 2017-06-30 09:22:36,801 main.py:52] epoch 9268, training loss: 4942.03, average training loss: 5398.81, base loss: 15976.29
[INFO 2017-06-30 09:22:39,922 main.py:52] epoch 9269, training loss: 5226.52, average training loss: 5398.64, base loss: 15976.42
[INFO 2017-06-30 09:22:43,096 main.py:52] epoch 9270, training loss: 5186.23, average training loss: 5398.19, base loss: 15976.31
[INFO 2017-06-30 09:22:46,248 main.py:52] epoch 9271, training loss: 5473.10, average training loss: 5397.82, base loss: 15976.52
[INFO 2017-06-30 09:22:49,413 main.py:52] epoch 9272, training loss: 5504.34, average training loss: 5398.04, base loss: 15976.36
[INFO 2017-06-30 09:22:52,557 main.py:52] epoch 9273, training loss: 5122.58, average training loss: 5397.65, base loss: 15975.93
[INFO 2017-06-30 09:22:55,714 main.py:52] epoch 9274, training loss: 5551.61, average training loss: 5397.79, base loss: 15975.98
[INFO 2017-06-30 09:22:58,876 main.py:52] epoch 9275, training loss: 5201.49, average training loss: 5397.56, base loss: 15975.91
[INFO 2017-06-30 09:23:02,022 main.py:52] epoch 9276, training loss: 5661.79, average training loss: 5398.04, base loss: 15975.49
[INFO 2017-06-30 09:23:05,193 main.py:52] epoch 9277, training loss: 5520.45, average training loss: 5398.50, base loss: 15975.45
[INFO 2017-06-30 09:23:08,386 main.py:52] epoch 9278, training loss: 5308.11, average training loss: 5398.27, base loss: 15975.60
[INFO 2017-06-30 09:23:11,528 main.py:52] epoch 9279, training loss: 5485.57, average training loss: 5398.49, base loss: 15975.95
[INFO 2017-06-30 09:23:14,661 main.py:52] epoch 9280, training loss: 5248.72, average training loss: 5398.56, base loss: 15975.90
[INFO 2017-06-30 09:23:17,806 main.py:52] epoch 9281, training loss: 4855.98, average training loss: 5397.43, base loss: 15975.75
[INFO 2017-06-30 09:23:20,938 main.py:52] epoch 9282, training loss: 5488.94, average training loss: 5397.51, base loss: 15975.74
[INFO 2017-06-30 09:23:24,060 main.py:52] epoch 9283, training loss: 5368.07, average training loss: 5397.52, base loss: 15975.90
[INFO 2017-06-30 09:23:27,220 main.py:52] epoch 9284, training loss: 5367.00, average training loss: 5397.42, base loss: 15975.57
[INFO 2017-06-30 09:23:30,360 main.py:52] epoch 9285, training loss: 5595.75, average training loss: 5397.36, base loss: 15975.63
[INFO 2017-06-30 09:23:33,526 main.py:52] epoch 9286, training loss: 5209.77, average training loss: 5397.19, base loss: 15975.22
[INFO 2017-06-30 09:23:36,658 main.py:52] epoch 9287, training loss: 5047.71, average training loss: 5396.80, base loss: 15974.72
[INFO 2017-06-30 09:23:39,786 main.py:52] epoch 9288, training loss: 5617.67, average training loss: 5397.01, base loss: 15974.97
[INFO 2017-06-30 09:23:42,924 main.py:52] epoch 9289, training loss: 5707.69, average training loss: 5397.30, base loss: 15975.30
[INFO 2017-06-30 09:23:46,092 main.py:52] epoch 9290, training loss: 5689.60, average training loss: 5397.59, base loss: 15975.68
[INFO 2017-06-30 09:23:49,252 main.py:52] epoch 9291, training loss: 5642.98, average training loss: 5398.17, base loss: 15975.85
[INFO 2017-06-30 09:23:52,392 main.py:52] epoch 9292, training loss: 5703.09, average training loss: 5398.18, base loss: 15976.01
[INFO 2017-06-30 09:23:55,508 main.py:52] epoch 9293, training loss: 5525.53, average training loss: 5398.36, base loss: 15975.97
[INFO 2017-06-30 09:23:58,652 main.py:52] epoch 9294, training loss: 5403.44, average training loss: 5398.32, base loss: 15975.71
[INFO 2017-06-30 09:24:01,776 main.py:52] epoch 9295, training loss: 5282.79, average training loss: 5398.32, base loss: 15975.65
[INFO 2017-06-30 09:24:04,925 main.py:52] epoch 9296, training loss: 5529.97, average training loss: 5398.52, base loss: 15975.93
[INFO 2017-06-30 09:24:08,098 main.py:52] epoch 9297, training loss: 5208.66, average training loss: 5398.38, base loss: 15975.99
[INFO 2017-06-30 09:24:11,268 main.py:52] epoch 9298, training loss: 5430.95, average training loss: 5398.30, base loss: 15976.12
[INFO 2017-06-30 09:24:14,415 main.py:52] epoch 9299, training loss: 5181.55, average training loss: 5397.83, base loss: 15976.16
[INFO 2017-06-30 09:24:14,415 main.py:54] epoch 9299, testing
[INFO 2017-06-30 09:24:27,600 main.py:97] average testing loss: 5345.58, base loss: 15636.07
[INFO 2017-06-30 09:24:27,600 main.py:98] improve_loss: 10290.49, improve_percent: 0.66
[INFO 2017-06-30 09:24:27,602 main.py:66] current best improved percent: 0.67
[INFO 2017-06-30 09:24:30,774 main.py:52] epoch 9300, training loss: 5720.09, average training loss: 5398.42, base loss: 15976.51
[INFO 2017-06-30 09:24:33,954 main.py:52] epoch 9301, training loss: 5112.68, average training loss: 5397.84, base loss: 15976.57
[INFO 2017-06-30 09:24:37,085 main.py:52] epoch 9302, training loss: 5527.31, average training loss: 5397.95, base loss: 15976.84
[INFO 2017-06-30 09:24:40,224 main.py:52] epoch 9303, training loss: 5341.27, average training loss: 5397.52, base loss: 15976.65
[INFO 2017-06-30 09:24:43,354 main.py:52] epoch 9304, training loss: 5222.32, average training loss: 5397.36, base loss: 15976.45
[INFO 2017-06-30 09:24:46,512 main.py:52] epoch 9305, training loss: 4875.52, average training loss: 5396.50, base loss: 15976.26
[INFO 2017-06-30 09:24:49,647 main.py:52] epoch 9306, training loss: 5461.30, average training loss: 5396.16, base loss: 15976.22
[INFO 2017-06-30 09:24:52,789 main.py:52] epoch 9307, training loss: 5099.21, average training loss: 5395.48, base loss: 15976.04
[INFO 2017-06-30 09:24:55,919 main.py:52] epoch 9308, training loss: 4994.84, average training loss: 5394.61, base loss: 15975.38
[INFO 2017-06-30 09:24:59,053 main.py:52] epoch 9309, training loss: 5718.23, average training loss: 5394.56, base loss: 15975.42
[INFO 2017-06-30 09:25:02,186 main.py:52] epoch 9310, training loss: 5218.20, average training loss: 5394.42, base loss: 15975.49
[INFO 2017-06-30 09:25:05,333 main.py:52] epoch 9311, training loss: 5143.17, average training loss: 5394.22, base loss: 15975.25
[INFO 2017-06-30 09:25:08,470 main.py:52] epoch 9312, training loss: 5490.79, average training loss: 5394.63, base loss: 15975.37
[INFO 2017-06-30 09:25:11,612 main.py:52] epoch 9313, training loss: 5495.31, average training loss: 5394.40, base loss: 15975.48
[INFO 2017-06-30 09:25:14,770 main.py:52] epoch 9314, training loss: 5508.65, average training loss: 5394.32, base loss: 15975.48
[INFO 2017-06-30 09:25:17,882 main.py:52] epoch 9315, training loss: 5681.80, average training loss: 5394.54, base loss: 15975.79
[INFO 2017-06-30 09:25:21,053 main.py:52] epoch 9316, training loss: 5815.53, average training loss: 5394.87, base loss: 15976.13
[INFO 2017-06-30 09:25:24,226 main.py:52] epoch 9317, training loss: 5156.80, average training loss: 5394.46, base loss: 15975.89
[INFO 2017-06-30 09:25:27,351 main.py:52] epoch 9318, training loss: 5274.64, average training loss: 5394.25, base loss: 15975.90
[INFO 2017-06-30 09:25:30,464 main.py:52] epoch 9319, training loss: 5329.30, average training loss: 5393.89, base loss: 15975.73
[INFO 2017-06-30 09:25:33,578 main.py:52] epoch 9320, training loss: 5644.57, average training loss: 5393.97, base loss: 15975.92
[INFO 2017-06-30 09:25:36,742 main.py:52] epoch 9321, training loss: 5471.01, average training loss: 5393.97, base loss: 15976.24
[INFO 2017-06-30 09:25:39,915 main.py:52] epoch 9322, training loss: 5210.94, average training loss: 5393.63, base loss: 15976.15
[INFO 2017-06-30 09:25:43,079 main.py:52] epoch 9323, training loss: 5114.99, average training loss: 5393.21, base loss: 15976.10
[INFO 2017-06-30 09:25:46,406 main.py:52] epoch 9324, training loss: 5224.37, average training loss: 5393.28, base loss: 15975.67
[INFO 2017-06-30 09:25:49,562 main.py:52] epoch 9325, training loss: 5394.60, average training loss: 5393.04, base loss: 15975.56
[INFO 2017-06-30 09:25:52,748 main.py:52] epoch 9326, training loss: 5441.66, average training loss: 5392.75, base loss: 15975.76
[INFO 2017-06-30 09:25:55,910 main.py:52] epoch 9327, training loss: 5122.63, average training loss: 5392.59, base loss: 15975.44
[INFO 2017-06-30 09:25:59,044 main.py:52] epoch 9328, training loss: 5388.68, average training loss: 5392.74, base loss: 15975.42
[INFO 2017-06-30 09:26:02,207 main.py:52] epoch 9329, training loss: 5340.70, average training loss: 5392.99, base loss: 15975.33
[INFO 2017-06-30 09:26:05,381 main.py:52] epoch 9330, training loss: 5366.40, average training loss: 5393.07, base loss: 15975.51
[INFO 2017-06-30 09:26:08,537 main.py:52] epoch 9331, training loss: 5121.20, average training loss: 5392.60, base loss: 15975.45
[INFO 2017-06-30 09:26:11,665 main.py:52] epoch 9332, training loss: 5319.77, average training loss: 5392.37, base loss: 15975.35
[INFO 2017-06-30 09:26:14,783 main.py:52] epoch 9333, training loss: 5430.02, average training loss: 5392.06, base loss: 15975.52
[INFO 2017-06-30 09:26:17,927 main.py:52] epoch 9334, training loss: 5586.12, average training loss: 5391.79, base loss: 15975.56
[INFO 2017-06-30 09:26:21,115 main.py:52] epoch 9335, training loss: 5240.99, average training loss: 5391.62, base loss: 15975.46
[INFO 2017-06-30 09:26:24,247 main.py:52] epoch 9336, training loss: 5286.51, average training loss: 5391.29, base loss: 15975.42
[INFO 2017-06-30 09:26:27,398 main.py:52] epoch 9337, training loss: 5481.84, average training loss: 5391.39, base loss: 15975.53
[INFO 2017-06-30 09:26:30,541 main.py:52] epoch 9338, training loss: 5366.04, average training loss: 5391.23, base loss: 15976.02
[INFO 2017-06-30 09:26:33,682 main.py:52] epoch 9339, training loss: 4928.55, average training loss: 5390.62, base loss: 15975.87
[INFO 2017-06-30 09:26:36,808 main.py:52] epoch 9340, training loss: 5142.84, average training loss: 5390.14, base loss: 15975.66
[INFO 2017-06-30 09:26:39,953 main.py:52] epoch 9341, training loss: 5068.93, average training loss: 5389.99, base loss: 15975.54
[INFO 2017-06-30 09:26:43,139 main.py:52] epoch 9342, training loss: 5676.42, average training loss: 5390.40, base loss: 15975.94
[INFO 2017-06-30 09:26:46,365 main.py:52] epoch 9343, training loss: 5647.48, average training loss: 5390.04, base loss: 15976.53
[INFO 2017-06-30 09:26:49,559 main.py:52] epoch 9344, training loss: 5734.50, average training loss: 5390.59, base loss: 15977.00
[INFO 2017-06-30 09:26:52,689 main.py:52] epoch 9345, training loss: 5469.15, average training loss: 5390.42, base loss: 15976.99
[INFO 2017-06-30 09:26:55,850 main.py:52] epoch 9346, training loss: 5204.74, average training loss: 5389.75, base loss: 15977.09
[INFO 2017-06-30 09:26:58,969 main.py:52] epoch 9347, training loss: 5120.73, average training loss: 5389.31, base loss: 15976.87
[INFO 2017-06-30 09:27:02,106 main.py:52] epoch 9348, training loss: 5509.41, average training loss: 5389.55, base loss: 15976.68
[INFO 2017-06-30 09:27:05,250 main.py:52] epoch 9349, training loss: 5243.31, average training loss: 5389.23, base loss: 15976.57
[INFO 2017-06-30 09:27:08,411 main.py:52] epoch 9350, training loss: 5747.00, average training loss: 5389.66, base loss: 15977.08
[INFO 2017-06-30 09:27:11,564 main.py:52] epoch 9351, training loss: 5596.52, average training loss: 5389.84, base loss: 15977.46
[INFO 2017-06-30 09:27:14,706 main.py:52] epoch 9352, training loss: 5434.74, average training loss: 5390.00, base loss: 15977.56
[INFO 2017-06-30 09:27:17,866 main.py:52] epoch 9353, training loss: 5117.78, average training loss: 5389.64, base loss: 15977.37
[INFO 2017-06-30 09:27:21,015 main.py:52] epoch 9354, training loss: 5432.35, average training loss: 5389.93, base loss: 15977.18
[INFO 2017-06-30 09:27:24,136 main.py:52] epoch 9355, training loss: 5310.38, average training loss: 5390.07, base loss: 15976.85
[INFO 2017-06-30 09:27:27,242 main.py:52] epoch 9356, training loss: 5385.97, average training loss: 5390.25, base loss: 15976.80
[INFO 2017-06-30 09:27:30,371 main.py:52] epoch 9357, training loss: 5311.83, average training loss: 5390.30, base loss: 15976.72
[INFO 2017-06-30 09:27:33,494 main.py:52] epoch 9358, training loss: 5267.14, average training loss: 5390.21, base loss: 15976.77
[INFO 2017-06-30 09:27:36,609 main.py:52] epoch 9359, training loss: 5663.53, average training loss: 5390.24, base loss: 15976.78
[INFO 2017-06-30 09:27:39,800 main.py:52] epoch 9360, training loss: 5528.35, average training loss: 5390.75, base loss: 15976.92
[INFO 2017-06-30 09:27:42,962 main.py:52] epoch 9361, training loss: 5045.56, average training loss: 5390.54, base loss: 15976.97
[INFO 2017-06-30 09:27:46,090 main.py:52] epoch 9362, training loss: 5043.45, average training loss: 5390.11, base loss: 15976.79
[INFO 2017-06-30 09:27:49,236 main.py:52] epoch 9363, training loss: 5400.06, average training loss: 5389.75, base loss: 15976.86
[INFO 2017-06-30 09:27:52,410 main.py:52] epoch 9364, training loss: 5245.06, average training loss: 5389.59, base loss: 15976.46
[INFO 2017-06-30 09:27:55,564 main.py:52] epoch 9365, training loss: 5468.99, average training loss: 5389.16, base loss: 15976.65
[INFO 2017-06-30 09:27:58,716 main.py:52] epoch 9366, training loss: 5212.51, average training loss: 5388.13, base loss: 15976.73
[INFO 2017-06-30 09:28:01,850 main.py:52] epoch 9367, training loss: 5256.58, average training loss: 5387.88, base loss: 15976.86
[INFO 2017-06-30 09:28:04,997 main.py:52] epoch 9368, training loss: 5562.91, average training loss: 5388.01, base loss: 15977.16
[INFO 2017-06-30 09:28:08,149 main.py:52] epoch 9369, training loss: 4962.12, average training loss: 5387.47, base loss: 15976.82
[INFO 2017-06-30 09:28:11,311 main.py:52] epoch 9370, training loss: 5967.95, average training loss: 5387.75, base loss: 15977.00
[INFO 2017-06-30 09:28:14,442 main.py:52] epoch 9371, training loss: 5209.07, average training loss: 5387.14, base loss: 15976.74
[INFO 2017-06-30 09:28:17,617 main.py:52] epoch 9372, training loss: 5243.38, average training loss: 5387.30, base loss: 15976.91
[INFO 2017-06-30 09:28:20,735 main.py:52] epoch 9373, training loss: 5272.07, average training loss: 5387.37, base loss: 15977.04
[INFO 2017-06-30 09:28:23,841 main.py:52] epoch 9374, training loss: 5280.44, average training loss: 5387.41, base loss: 15976.83
[INFO 2017-06-30 09:28:26,957 main.py:52] epoch 9375, training loss: 5508.58, average training loss: 5387.65, base loss: 15976.63
[INFO 2017-06-30 09:28:30,099 main.py:52] epoch 9376, training loss: 5388.98, average training loss: 5387.57, base loss: 15976.39
[INFO 2017-06-30 09:28:33,220 main.py:52] epoch 9377, training loss: 5099.52, average training loss: 5386.99, base loss: 15975.98
[INFO 2017-06-30 09:28:36,379 main.py:52] epoch 9378, training loss: 5176.38, average training loss: 5386.64, base loss: 15975.93
[INFO 2017-06-30 09:28:39,536 main.py:52] epoch 9379, training loss: 5315.73, average training loss: 5386.55, base loss: 15976.24
[INFO 2017-06-30 09:28:42,684 main.py:52] epoch 9380, training loss: 5197.95, average training loss: 5386.42, base loss: 15976.03
[INFO 2017-06-30 09:28:45,773 main.py:52] epoch 9381, training loss: 5634.70, average training loss: 5386.73, base loss: 15976.42
[INFO 2017-06-30 09:28:48,874 main.py:52] epoch 9382, training loss: 5138.45, average training loss: 5386.61, base loss: 15975.95
[INFO 2017-06-30 09:28:52,001 main.py:52] epoch 9383, training loss: 5497.74, average training loss: 5386.28, base loss: 15975.88
[INFO 2017-06-30 09:28:55,134 main.py:52] epoch 9384, training loss: 5021.73, average training loss: 5386.18, base loss: 15975.55
[INFO 2017-06-30 09:28:58,320 main.py:52] epoch 9385, training loss: 5405.68, average training loss: 5386.17, base loss: 15975.54
[INFO 2017-06-30 09:29:01,427 main.py:52] epoch 9386, training loss: 5369.31, average training loss: 5386.16, base loss: 15975.57
[INFO 2017-06-30 09:29:04,565 main.py:52] epoch 9387, training loss: 5614.05, average training loss: 5386.28, base loss: 15975.96
[INFO 2017-06-30 09:29:07,654 main.py:52] epoch 9388, training loss: 5044.46, average training loss: 5385.62, base loss: 15975.60
[INFO 2017-06-30 09:29:10,795 main.py:52] epoch 9389, training loss: 4978.88, average training loss: 5384.94, base loss: 15975.61
[INFO 2017-06-30 09:29:13,925 main.py:52] epoch 9390, training loss: 5376.35, average training loss: 5384.75, base loss: 15975.66
[INFO 2017-06-30 09:29:17,073 main.py:52] epoch 9391, training loss: 5503.68, average training loss: 5384.73, base loss: 15975.86
[INFO 2017-06-30 09:29:20,190 main.py:52] epoch 9392, training loss: 5532.24, average training loss: 5384.66, base loss: 15976.07
[INFO 2017-06-30 09:29:23,336 main.py:52] epoch 9393, training loss: 5108.28, average training loss: 5384.55, base loss: 15975.89
[INFO 2017-06-30 09:29:26,496 main.py:52] epoch 9394, training loss: 5381.74, average training loss: 5384.43, base loss: 15975.70
[INFO 2017-06-30 09:29:29,643 main.py:52] epoch 9395, training loss: 5322.04, average training loss: 5384.01, base loss: 15975.42
[INFO 2017-06-30 09:29:32,782 main.py:52] epoch 9396, training loss: 5676.53, average training loss: 5384.45, base loss: 15975.79
[INFO 2017-06-30 09:29:35,922 main.py:52] epoch 9397, training loss: 5339.09, average training loss: 5384.39, base loss: 15976.15
[INFO 2017-06-30 09:29:39,080 main.py:52] epoch 9398, training loss: 5037.81, average training loss: 5383.71, base loss: 15975.72
[INFO 2017-06-30 09:29:42,236 main.py:52] epoch 9399, training loss: 5241.64, average training loss: 5383.63, base loss: 15975.52
[INFO 2017-06-30 09:29:42,236 main.py:54] epoch 9399, testing
[INFO 2017-06-30 09:29:55,188 main.py:97] average testing loss: 5400.85, base loss: 16350.39
[INFO 2017-06-30 09:29:55,188 main.py:98] improve_loss: 10949.54, improve_percent: 0.67
[INFO 2017-06-30 09:29:55,190 main.py:66] current best improved percent: 0.67
[INFO 2017-06-30 09:29:58,335 main.py:52] epoch 9400, training loss: 5685.72, average training loss: 5383.63, base loss: 15975.49
[INFO 2017-06-30 09:30:01,463 main.py:52] epoch 9401, training loss: 5261.56, average training loss: 5383.44, base loss: 15975.19
[INFO 2017-06-30 09:30:04,608 main.py:52] epoch 9402, training loss: 5138.49, average training loss: 5383.22, base loss: 15974.96
[INFO 2017-06-30 09:30:07,749 main.py:52] epoch 9403, training loss: 5464.36, average training loss: 5383.24, base loss: 15975.07
[INFO 2017-06-30 09:30:10,846 main.py:52] epoch 9404, training loss: 5004.30, average training loss: 5382.43, base loss: 15974.84
[INFO 2017-06-30 09:30:14,007 main.py:52] epoch 9405, training loss: 5169.49, average training loss: 5382.19, base loss: 15974.80
[INFO 2017-06-30 09:30:17,112 main.py:52] epoch 9406, training loss: 5336.86, average training loss: 5382.07, base loss: 15974.75
[INFO 2017-06-30 09:30:20,254 main.py:52] epoch 9407, training loss: 5058.87, average training loss: 5381.56, base loss: 15974.67
[INFO 2017-06-30 09:30:23,414 main.py:52] epoch 9408, training loss: 5065.29, average training loss: 5380.87, base loss: 15974.49
[INFO 2017-06-30 09:30:26,541 main.py:52] epoch 9409, training loss: 5350.41, average training loss: 5380.85, base loss: 15974.62
[INFO 2017-06-30 09:30:29,689 main.py:52] epoch 9410, training loss: 4792.45, average training loss: 5380.65, base loss: 15974.28
[INFO 2017-06-30 09:30:32,829 main.py:52] epoch 9411, training loss: 5366.24, average training loss: 5380.53, base loss: 15973.85
[INFO 2017-06-30 09:30:35,987 main.py:52] epoch 9412, training loss: 5251.23, average training loss: 5380.46, base loss: 15973.85
[INFO 2017-06-30 09:30:39,154 main.py:52] epoch 9413, training loss: 5300.69, average training loss: 5380.32, base loss: 15973.81
[INFO 2017-06-30 09:30:42,290 main.py:52] epoch 9414, training loss: 5490.60, average training loss: 5380.22, base loss: 15973.76
[INFO 2017-06-30 09:30:45,440 main.py:52] epoch 9415, training loss: 5668.35, average training loss: 5380.47, base loss: 15973.71
[INFO 2017-06-30 09:30:48,559 main.py:52] epoch 9416, training loss: 5211.55, average training loss: 5380.58, base loss: 15973.84
[INFO 2017-06-30 09:30:51,693 main.py:52] epoch 9417, training loss: 5202.37, average training loss: 5380.71, base loss: 15973.81
[INFO 2017-06-30 09:30:54,846 main.py:52] epoch 9418, training loss: 5411.80, average training loss: 5380.82, base loss: 15973.82
[INFO 2017-06-30 09:30:57,975 main.py:52] epoch 9419, training loss: 5793.91, average training loss: 5381.47, base loss: 15974.01
[INFO 2017-06-30 09:31:01,116 main.py:52] epoch 9420, training loss: 5447.55, average training loss: 5381.38, base loss: 15973.76
[INFO 2017-06-30 09:31:04,287 main.py:52] epoch 9421, training loss: 5114.88, average training loss: 5381.10, base loss: 15973.08
[INFO 2017-06-30 09:31:07,448 main.py:52] epoch 9422, training loss: 5650.77, average training loss: 5380.63, base loss: 15973.09
[INFO 2017-06-30 09:31:10,642 main.py:52] epoch 9423, training loss: 5339.25, average training loss: 5380.82, base loss: 15972.84
[INFO 2017-06-30 09:31:13,794 main.py:52] epoch 9424, training loss: 5601.08, average training loss: 5381.15, base loss: 15973.09
[INFO 2017-06-30 09:31:16,915 main.py:52] epoch 9425, training loss: 5259.17, average training loss: 5380.68, base loss: 15973.03
[INFO 2017-06-30 09:31:20,088 main.py:52] epoch 9426, training loss: 5632.98, average training loss: 5381.15, base loss: 15973.46
[INFO 2017-06-30 09:31:23,207 main.py:52] epoch 9427, training loss: 5251.73, average training loss: 5381.02, base loss: 15973.49
[INFO 2017-06-30 09:31:26,365 main.py:52] epoch 9428, training loss: 5195.61, average training loss: 5380.50, base loss: 15973.14
[INFO 2017-06-30 09:31:29,498 main.py:52] epoch 9429, training loss: 5889.83, average training loss: 5381.07, base loss: 15973.06
[INFO 2017-06-30 09:31:32,627 main.py:52] epoch 9430, training loss: 5607.23, average training loss: 5380.91, base loss: 15973.07
[INFO 2017-06-30 09:31:35,773 main.py:52] epoch 9431, training loss: 5526.33, average training loss: 5381.16, base loss: 15973.17
[INFO 2017-06-30 09:31:38,959 main.py:52] epoch 9432, training loss: 5275.47, average training loss: 5380.69, base loss: 15973.14
[INFO 2017-06-30 09:31:42,091 main.py:52] epoch 9433, training loss: 5417.93, average training loss: 5380.64, base loss: 15973.16
[INFO 2017-06-30 09:31:45,240 main.py:52] epoch 9434, training loss: 5440.01, average training loss: 5380.65, base loss: 15973.47
[INFO 2017-06-30 09:31:48,414 main.py:52] epoch 9435, training loss: 5822.81, average training loss: 5381.04, base loss: 15974.06
[INFO 2017-06-30 09:31:51,588 main.py:52] epoch 9436, training loss: 5595.71, average training loss: 5380.89, base loss: 15974.27
[INFO 2017-06-30 09:31:54,687 main.py:52] epoch 9437, training loss: 5226.32, average training loss: 5380.76, base loss: 15974.14
[INFO 2017-06-30 09:31:57,825 main.py:52] epoch 9438, training loss: 5125.78, average training loss: 5380.48, base loss: 15973.91
[INFO 2017-06-30 09:32:00,981 main.py:52] epoch 9439, training loss: 4973.31, average training loss: 5379.84, base loss: 15973.58
[INFO 2017-06-30 09:32:04,134 main.py:52] epoch 9440, training loss: 5636.37, average training loss: 5380.21, base loss: 15973.54
[INFO 2017-06-30 09:32:07,261 main.py:52] epoch 9441, training loss: 5254.71, average training loss: 5379.98, base loss: 15973.35
[INFO 2017-06-30 09:32:10,403 main.py:52] epoch 9442, training loss: 5653.61, average training loss: 5380.47, base loss: 15973.59
[INFO 2017-06-30 09:32:13,539 main.py:52] epoch 9443, training loss: 5030.02, average training loss: 5380.32, base loss: 15973.48
[INFO 2017-06-30 09:32:16,701 main.py:52] epoch 9444, training loss: 5259.68, average training loss: 5379.85, base loss: 15973.13
[INFO 2017-06-30 09:32:19,850 main.py:52] epoch 9445, training loss: 5477.61, average training loss: 5379.86, base loss: 15973.04
[INFO 2017-06-30 09:32:23,039 main.py:52] epoch 9446, training loss: 5416.49, average training loss: 5380.04, base loss: 15973.30
[INFO 2017-06-30 09:32:26,169 main.py:52] epoch 9447, training loss: 5130.47, average training loss: 5379.69, base loss: 15973.31
[INFO 2017-06-30 09:32:29,319 main.py:52] epoch 9448, training loss: 5635.64, average training loss: 5379.33, base loss: 15973.21
[INFO 2017-06-30 09:32:32,415 main.py:52] epoch 9449, training loss: 5481.60, average training loss: 5379.43, base loss: 15972.97
[INFO 2017-06-30 09:32:35,553 main.py:52] epoch 9450, training loss: 5273.18, average training loss: 5379.05, base loss: 15972.84
[INFO 2017-06-30 09:32:38,672 main.py:52] epoch 9451, training loss: 5238.65, average training loss: 5379.06, base loss: 15972.70
[INFO 2017-06-30 09:32:41,854 main.py:52] epoch 9452, training loss: 5562.68, average training loss: 5379.19, base loss: 15972.97
[INFO 2017-06-30 09:32:45,004 main.py:52] epoch 9453, training loss: 5055.75, average training loss: 5378.07, base loss: 15972.70
[INFO 2017-06-30 09:32:48,180 main.py:52] epoch 9454, training loss: 5175.75, average training loss: 5377.82, base loss: 15972.55
[INFO 2017-06-30 09:32:51,320 main.py:52] epoch 9455, training loss: 5077.59, average training loss: 5377.72, base loss: 15972.47
[INFO 2017-06-30 09:32:54,482 main.py:52] epoch 9456, training loss: 4958.90, average training loss: 5377.47, base loss: 15972.02
[INFO 2017-06-30 09:32:57,607 main.py:52] epoch 9457, training loss: 5426.28, average training loss: 5377.67, base loss: 15972.17
[INFO 2017-06-30 09:33:00,717 main.py:52] epoch 9458, training loss: 5211.96, average training loss: 5377.34, base loss: 15972.39
[INFO 2017-06-30 09:33:03,888 main.py:52] epoch 9459, training loss: 5186.91, average training loss: 5377.33, base loss: 15972.34
[INFO 2017-06-30 09:33:07,020 main.py:52] epoch 9460, training loss: 5482.87, average training loss: 5377.69, base loss: 15972.41
[INFO 2017-06-30 09:33:10,143 main.py:52] epoch 9461, training loss: 5599.04, average training loss: 5377.62, base loss: 15972.77
[INFO 2017-06-30 09:33:13,307 main.py:52] epoch 9462, training loss: 5113.38, average training loss: 5377.27, base loss: 15972.75
[INFO 2017-06-30 09:33:16,463 main.py:52] epoch 9463, training loss: 5231.03, average training loss: 5377.27, base loss: 15972.81
[INFO 2017-06-30 09:33:19,606 main.py:52] epoch 9464, training loss: 5367.91, average training loss: 5377.22, base loss: 15972.67
[INFO 2017-06-30 09:33:22,766 main.py:52] epoch 9465, training loss: 5645.74, average training loss: 5377.66, base loss: 15973.01
[INFO 2017-06-30 09:33:25,888 main.py:52] epoch 9466, training loss: 5192.76, average training loss: 5377.41, base loss: 15972.79
[INFO 2017-06-30 09:33:29,031 main.py:52] epoch 9467, training loss: 5607.87, average training loss: 5377.21, base loss: 15973.04
[INFO 2017-06-30 09:33:32,153 main.py:52] epoch 9468, training loss: 5392.26, average training loss: 5376.55, base loss: 15972.82
[INFO 2017-06-30 09:33:35,317 main.py:52] epoch 9469, training loss: 5212.70, average training loss: 5376.06, base loss: 15972.85
[INFO 2017-06-30 09:33:38,476 main.py:52] epoch 9470, training loss: 5263.87, average training loss: 5376.14, base loss: 15972.57
[INFO 2017-06-30 09:33:41,580 main.py:52] epoch 9471, training loss: 5408.67, average training loss: 5376.16, base loss: 15972.49
[INFO 2017-06-30 09:33:44,720 main.py:52] epoch 9472, training loss: 5297.55, average training loss: 5376.11, base loss: 15972.34
[INFO 2017-06-30 09:33:47,812 main.py:52] epoch 9473, training loss: 5695.25, average training loss: 5376.67, base loss: 15972.35
[INFO 2017-06-30 09:33:50,979 main.py:52] epoch 9474, training loss: 5603.15, average training loss: 5376.58, base loss: 15972.54
[INFO 2017-06-30 09:33:54,093 main.py:52] epoch 9475, training loss: 5543.05, average training loss: 5376.59, base loss: 15972.71
[INFO 2017-06-30 09:33:57,200 main.py:52] epoch 9476, training loss: 5247.80, average training loss: 5376.51, base loss: 15972.59
[INFO 2017-06-30 09:34:00,377 main.py:52] epoch 9477, training loss: 5317.62, average training loss: 5376.40, base loss: 15972.38
[INFO 2017-06-30 09:34:03,561 main.py:52] epoch 9478, training loss: 5579.84, average training loss: 5376.72, base loss: 15972.63
[INFO 2017-06-30 09:34:06,689 main.py:52] epoch 9479, training loss: 5267.71, average training loss: 5376.95, base loss: 15972.73
[INFO 2017-06-30 09:34:09,840 main.py:52] epoch 9480, training loss: 5645.56, average training loss: 5376.86, base loss: 15972.57
[INFO 2017-06-30 09:34:12,965 main.py:52] epoch 9481, training loss: 5229.18, average training loss: 5376.72, base loss: 15972.04
[INFO 2017-06-30 09:34:16,149 main.py:52] epoch 9482, training loss: 5219.21, average training loss: 5376.73, base loss: 15971.73
[INFO 2017-06-30 09:34:19,290 main.py:52] epoch 9483, training loss: 5637.67, average training loss: 5377.18, base loss: 15972.01
[INFO 2017-06-30 09:34:22,407 main.py:52] epoch 9484, training loss: 5112.64, average training loss: 5376.52, base loss: 15971.89
[INFO 2017-06-30 09:34:25,574 main.py:52] epoch 9485, training loss: 5437.62, average training loss: 5376.10, base loss: 15971.66
[INFO 2017-06-30 09:34:28,735 main.py:52] epoch 9486, training loss: 5175.43, average training loss: 5375.69, base loss: 15971.39
[INFO 2017-06-30 09:34:31,861 main.py:52] epoch 9487, training loss: 5727.01, average training loss: 5376.07, base loss: 15971.63
[INFO 2017-06-30 09:34:34,996 main.py:52] epoch 9488, training loss: 5450.09, average training loss: 5375.99, base loss: 15971.42
[INFO 2017-06-30 09:34:38,202 main.py:52] epoch 9489, training loss: 5367.61, average training loss: 5376.35, base loss: 15971.28
[INFO 2017-06-30 09:34:41,337 main.py:52] epoch 9490, training loss: 5263.62, average training loss: 5376.15, base loss: 15971.35
[INFO 2017-06-30 09:34:44,506 main.py:52] epoch 9491, training loss: 5108.95, average training loss: 5375.95, base loss: 15971.32
[INFO 2017-06-30 09:34:47,659 main.py:52] epoch 9492, training loss: 5293.57, average training loss: 5375.84, base loss: 15970.80
[INFO 2017-06-30 09:34:50,807 main.py:52] epoch 9493, training loss: 5093.51, average training loss: 5375.57, base loss: 15970.46
[INFO 2017-06-30 09:34:53,968 main.py:52] epoch 9494, training loss: 5214.43, average training loss: 5375.28, base loss: 15970.39
[INFO 2017-06-30 09:34:57,114 main.py:52] epoch 9495, training loss: 5466.40, average training loss: 5375.37, base loss: 15970.53
[INFO 2017-06-30 09:35:00,243 main.py:52] epoch 9496, training loss: 5680.23, average training loss: 5375.57, base loss: 15970.95
[INFO 2017-06-30 09:35:03,382 main.py:52] epoch 9497, training loss: 5113.21, average training loss: 5374.88, base loss: 15970.87
[INFO 2017-06-30 09:35:06,506 main.py:52] epoch 9498, training loss: 5645.82, average training loss: 5375.14, base loss: 15971.26
[INFO 2017-06-30 09:35:09,631 main.py:52] epoch 9499, training loss: 5382.59, average training loss: 5375.07, base loss: 15971.12
[INFO 2017-06-30 09:35:09,632 main.py:54] epoch 9499, testing
[INFO 2017-06-30 09:35:22,711 main.py:97] average testing loss: 5382.18, base loss: 16119.06
[INFO 2017-06-30 09:35:22,711 main.py:98] improve_loss: 10736.88, improve_percent: 0.67
[INFO 2017-06-30 09:35:22,713 main.py:66] current best improved percent: 0.67
[INFO 2017-06-30 09:35:25,849 main.py:52] epoch 9500, training loss: 4981.19, average training loss: 5374.31, base loss: 15970.76
[INFO 2017-06-30 09:35:29,022 main.py:52] epoch 9501, training loss: 5078.67, average training loss: 5374.22, base loss: 15970.61
[INFO 2017-06-30 09:35:32,190 main.py:52] epoch 9502, training loss: 5499.42, average training loss: 5374.08, base loss: 15970.57
[INFO 2017-06-30 09:35:35,325 main.py:52] epoch 9503, training loss: 5424.95, average training loss: 5374.14, base loss: 15970.81
[INFO 2017-06-30 09:35:38,470 main.py:52] epoch 9504, training loss: 5714.82, average training loss: 5374.31, base loss: 15971.38
[INFO 2017-06-30 09:35:41,644 main.py:52] epoch 9505, training loss: 5998.94, average training loss: 5374.55, base loss: 15972.21
[INFO 2017-06-30 09:35:44,779 main.py:52] epoch 9506, training loss: 4967.93, average training loss: 5374.24, base loss: 15971.69
[INFO 2017-06-30 09:35:47,919 main.py:52] epoch 9507, training loss: 5567.25, average training loss: 5374.36, base loss: 15971.39
[INFO 2017-06-30 09:35:51,054 main.py:52] epoch 9508, training loss: 5112.72, average training loss: 5374.08, base loss: 15971.38
[INFO 2017-06-30 09:35:54,206 main.py:52] epoch 9509, training loss: 5546.72, average training loss: 5374.02, base loss: 15971.54
[INFO 2017-06-30 09:35:57,321 main.py:52] epoch 9510, training loss: 5354.77, average training loss: 5373.94, base loss: 15971.11
[INFO 2017-06-30 09:36:00,433 main.py:52] epoch 9511, training loss: 5128.83, average training loss: 5374.16, base loss: 15970.97
[INFO 2017-06-30 09:36:03,581 main.py:52] epoch 9512, training loss: 5647.96, average training loss: 5374.38, base loss: 15970.68
[INFO 2017-06-30 09:36:06,745 main.py:52] epoch 9513, training loss: 5284.61, average training loss: 5374.32, base loss: 15970.70
[INFO 2017-06-30 09:36:09,886 main.py:52] epoch 9514, training loss: 5382.80, average training loss: 5374.36, base loss: 15970.71
[INFO 2017-06-30 09:36:12,987 main.py:52] epoch 9515, training loss: 5184.28, average training loss: 5373.63, base loss: 15970.65
[INFO 2017-06-30 09:36:16,146 main.py:52] epoch 9516, training loss: 5245.97, average training loss: 5373.45, base loss: 15970.82
[INFO 2017-06-30 09:36:19,264 main.py:52] epoch 9517, training loss: 5589.61, average training loss: 5373.80, base loss: 15970.88
[INFO 2017-06-30 09:36:22,379 main.py:52] epoch 9518, training loss: 5353.40, average training loss: 5373.61, base loss: 15971.30
[INFO 2017-06-30 09:36:25,504 main.py:52] epoch 9519, training loss: 5325.77, average training loss: 5373.26, base loss: 15971.48
[INFO 2017-06-30 09:36:28,668 main.py:52] epoch 9520, training loss: 4962.46, average training loss: 5372.83, base loss: 15971.23
[INFO 2017-06-30 09:36:31,807 main.py:52] epoch 9521, training loss: 5668.75, average training loss: 5373.34, base loss: 15971.30
[INFO 2017-06-30 09:36:34,924 main.py:52] epoch 9522, training loss: 5341.71, average training loss: 5373.21, base loss: 15971.41
[INFO 2017-06-30 09:36:38,123 main.py:52] epoch 9523, training loss: 4922.16, average training loss: 5372.79, base loss: 15971.22
[INFO 2017-06-30 09:36:41,285 main.py:52] epoch 9524, training loss: 5135.56, average training loss: 5372.66, base loss: 15971.03
[INFO 2017-06-30 09:36:44,434 main.py:52] epoch 9525, training loss: 5413.95, average training loss: 5372.31, base loss: 15971.27
[INFO 2017-06-30 09:36:47,577 main.py:52] epoch 9526, training loss: 5162.88, average training loss: 5372.04, base loss: 15971.37
[INFO 2017-06-30 09:36:50,736 main.py:52] epoch 9527, training loss: 5303.73, average training loss: 5372.08, base loss: 15970.88
[INFO 2017-06-30 09:36:53,890 main.py:52] epoch 9528, training loss: 5546.50, average training loss: 5372.18, base loss: 15971.30
[INFO 2017-06-30 09:36:57,080 main.py:52] epoch 9529, training loss: 5120.78, average training loss: 5371.84, base loss: 15970.94
[INFO 2017-06-30 09:37:00,230 main.py:52] epoch 9530, training loss: 5509.32, average training loss: 5372.37, base loss: 15970.80
[INFO 2017-06-30 09:37:03,329 main.py:52] epoch 9531, training loss: 5400.95, average training loss: 5372.52, base loss: 15970.73
[INFO 2017-06-30 09:37:06,494 main.py:52] epoch 9532, training loss: 5003.56, average training loss: 5371.96, base loss: 15970.69
[INFO 2017-06-30 09:37:09,627 main.py:52] epoch 9533, training loss: 5952.57, average training loss: 5372.47, base loss: 15970.86
[INFO 2017-06-30 09:37:12,796 main.py:52] epoch 9534, training loss: 5372.19, average training loss: 5372.54, base loss: 15971.08
[INFO 2017-06-30 09:37:15,977 main.py:52] epoch 9535, training loss: 5314.88, average training loss: 5372.17, base loss: 15970.87
[INFO 2017-06-30 09:37:19,155 main.py:52] epoch 9536, training loss: 5539.07, average training loss: 5372.33, base loss: 15971.03
[INFO 2017-06-30 09:37:22,335 main.py:52] epoch 9537, training loss: 5440.64, average training loss: 5372.55, base loss: 15971.35
[INFO 2017-06-30 09:37:25,463 main.py:52] epoch 9538, training loss: 5371.60, average training loss: 5372.14, base loss: 15971.01
[INFO 2017-06-30 09:37:28,615 main.py:52] epoch 9539, training loss: 5750.89, average training loss: 5372.55, base loss: 15971.13
[INFO 2017-06-30 09:37:31,737 main.py:52] epoch 9540, training loss: 5426.76, average training loss: 5372.37, base loss: 15971.15
[INFO 2017-06-30 09:37:34,873 main.py:52] epoch 9541, training loss: 5277.80, average training loss: 5372.45, base loss: 15970.93
[INFO 2017-06-30 09:37:38,006 main.py:52] epoch 9542, training loss: 5161.85, average training loss: 5371.83, base loss: 15970.79
[INFO 2017-06-30 09:37:41,173 main.py:52] epoch 9543, training loss: 5237.82, average training loss: 5371.86, base loss: 15970.52
[INFO 2017-06-30 09:37:44,327 main.py:52] epoch 9544, training loss: 5354.19, average training loss: 5371.47, base loss: 15970.50
[INFO 2017-06-30 09:37:47,499 main.py:52] epoch 9545, training loss: 5323.63, average training loss: 5371.37, base loss: 15970.68
[INFO 2017-06-30 09:37:50,607 main.py:52] epoch 9546, training loss: 5569.84, average training loss: 5371.51, base loss: 15970.94
[INFO 2017-06-30 09:37:53,784 main.py:52] epoch 9547, training loss: 5559.21, average training loss: 5371.70, base loss: 15970.99
[INFO 2017-06-30 09:37:56,885 main.py:52] epoch 9548, training loss: 5504.78, average training loss: 5371.79, base loss: 15971.06
[INFO 2017-06-30 09:38:00,030 main.py:52] epoch 9549, training loss: 5350.20, average training loss: 5372.15, base loss: 15971.04
[INFO 2017-06-30 09:38:03,173 main.py:52] epoch 9550, training loss: 5221.62, average training loss: 5371.82, base loss: 15971.00
[INFO 2017-06-30 09:38:06,348 main.py:52] epoch 9551, training loss: 5496.85, average training loss: 5371.63, base loss: 15971.30
[INFO 2017-06-30 09:38:09,546 main.py:52] epoch 9552, training loss: 5399.01, average training loss: 5371.22, base loss: 15971.27
[INFO 2017-06-30 09:38:12,663 main.py:52] epoch 9553, training loss: 5076.32, average training loss: 5370.86, base loss: 15971.18
[INFO 2017-06-30 09:38:15,779 main.py:52] epoch 9554, training loss: 5739.12, average training loss: 5371.23, base loss: 15971.71
[INFO 2017-06-30 09:38:18,985 main.py:52] epoch 9555, training loss: 5449.18, average training loss: 5371.45, base loss: 15972.01
[INFO 2017-06-30 09:38:22,146 main.py:52] epoch 9556, training loss: 4922.23, average training loss: 5370.81, base loss: 15971.40
[INFO 2017-06-30 09:38:25,278 main.py:52] epoch 9557, training loss: 5344.01, average training loss: 5371.02, base loss: 15971.24
[INFO 2017-06-30 09:38:28,437 main.py:52] epoch 9558, training loss: 5234.51, average training loss: 5371.27, base loss: 15971.04
[INFO 2017-06-30 09:38:31,595 main.py:52] epoch 9559, training loss: 5170.34, average training loss: 5371.02, base loss: 15970.82
[INFO 2017-06-30 09:38:34,739 main.py:52] epoch 9560, training loss: 5675.37, average training loss: 5370.99, base loss: 15971.28
[INFO 2017-06-30 09:38:37,874 main.py:52] epoch 9561, training loss: 5353.77, average training loss: 5370.73, base loss: 15971.55
[INFO 2017-06-30 09:38:41,027 main.py:52] epoch 9562, training loss: 5083.96, average training loss: 5370.59, base loss: 15971.42
[INFO 2017-06-30 09:38:44,161 main.py:52] epoch 9563, training loss: 4625.03, average training loss: 5370.07, base loss: 15971.02
[INFO 2017-06-30 09:38:47,293 main.py:52] epoch 9564, training loss: 5365.14, average training loss: 5369.60, base loss: 15970.96
[INFO 2017-06-30 09:38:50,416 main.py:52] epoch 9565, training loss: 5429.95, average training loss: 5369.77, base loss: 15971.04
[INFO 2017-06-30 09:38:53,563 main.py:52] epoch 9566, training loss: 5253.59, average training loss: 5369.53, base loss: 15971.12
[INFO 2017-06-30 09:38:56,729 main.py:52] epoch 9567, training loss: 5147.21, average training loss: 5369.12, base loss: 15971.10
[INFO 2017-06-30 09:38:59,880 main.py:52] epoch 9568, training loss: 5408.04, average training loss: 5369.14, base loss: 15971.07
[INFO 2017-06-30 09:39:03,044 main.py:52] epoch 9569, training loss: 5244.59, average training loss: 5368.85, base loss: 15971.06
[INFO 2017-06-30 09:39:06,167 main.py:52] epoch 9570, training loss: 5293.41, average training loss: 5368.49, base loss: 15971.09
[INFO 2017-06-30 09:39:09,302 main.py:52] epoch 9571, training loss: 5391.31, average training loss: 5368.81, base loss: 15970.88
[INFO 2017-06-30 09:39:12,471 main.py:52] epoch 9572, training loss: 5477.12, average training loss: 5368.99, base loss: 15971.01
[INFO 2017-06-30 09:39:15,620 main.py:52] epoch 9573, training loss: 5350.34, average training loss: 5368.75, base loss: 15970.89
[INFO 2017-06-30 09:39:18,801 main.py:52] epoch 9574, training loss: 5052.89, average training loss: 5368.68, base loss: 15970.69
[INFO 2017-06-30 09:39:21,940 main.py:52] epoch 9575, training loss: 5569.85, average training loss: 5369.29, base loss: 15970.77
[INFO 2017-06-30 09:39:25,117 main.py:52] epoch 9576, training loss: 5168.59, average training loss: 5369.17, base loss: 15970.37
[INFO 2017-06-30 09:39:28,287 main.py:52] epoch 9577, training loss: 5155.95, average training loss: 5368.93, base loss: 15970.39
[INFO 2017-06-30 09:39:31,391 main.py:52] epoch 9578, training loss: 5073.87, average training loss: 5368.72, base loss: 15970.11
[INFO 2017-06-30 09:39:34,553 main.py:52] epoch 9579, training loss: 5388.25, average training loss: 5368.28, base loss: 15970.01
[INFO 2017-06-30 09:39:37,707 main.py:52] epoch 9580, training loss: 5372.54, average training loss: 5368.59, base loss: 15970.40
[INFO 2017-06-30 09:39:40,840 main.py:52] epoch 9581, training loss: 4976.02, average training loss: 5368.04, base loss: 15970.16
[INFO 2017-06-30 09:39:44,006 main.py:52] epoch 9582, training loss: 5712.26, average training loss: 5368.62, base loss: 15970.47
[INFO 2017-06-30 09:39:47,182 main.py:52] epoch 9583, training loss: 5396.33, average training loss: 5368.35, base loss: 15970.49
[INFO 2017-06-30 09:39:50,296 main.py:52] epoch 9584, training loss: 5500.28, average training loss: 5368.94, base loss: 15970.41
[INFO 2017-06-30 09:39:53,433 main.py:52] epoch 9585, training loss: 5396.24, average training loss: 5369.19, base loss: 15970.28
[INFO 2017-06-30 09:39:56,582 main.py:52] epoch 9586, training loss: 5194.35, average training loss: 5369.51, base loss: 15970.09
[INFO 2017-06-30 09:39:59,725 main.py:52] epoch 9587, training loss: 4918.40, average training loss: 5368.80, base loss: 15969.83
[INFO 2017-06-30 09:40:02,895 main.py:52] epoch 9588, training loss: 5723.45, average training loss: 5369.25, base loss: 15970.06
[INFO 2017-06-30 09:40:06,015 main.py:52] epoch 9589, training loss: 5885.34, average training loss: 5369.32, base loss: 15970.28
[INFO 2017-06-30 09:40:09,207 main.py:52] epoch 9590, training loss: 5425.39, average training loss: 5369.20, base loss: 15970.40
[INFO 2017-06-30 09:40:12,404 main.py:52] epoch 9591, training loss: 5093.51, average training loss: 5368.89, base loss: 15970.33
[INFO 2017-06-30 09:40:15,595 main.py:52] epoch 9592, training loss: 5083.10, average training loss: 5369.11, base loss: 15970.08
[INFO 2017-06-30 09:40:18,759 main.py:52] epoch 9593, training loss: 5087.99, average training loss: 5368.69, base loss: 15970.05
[INFO 2017-06-30 09:40:21,921 main.py:52] epoch 9594, training loss: 5190.54, average training loss: 5368.51, base loss: 15970.02
[INFO 2017-06-30 09:40:25,036 main.py:52] epoch 9595, training loss: 5411.16, average training loss: 5368.71, base loss: 15970.06
[INFO 2017-06-30 09:40:28,168 main.py:52] epoch 9596, training loss: 5511.53, average training loss: 5368.68, base loss: 15970.10
[INFO 2017-06-30 09:40:31,331 main.py:52] epoch 9597, training loss: 5571.99, average training loss: 5368.90, base loss: 15970.20
[INFO 2017-06-30 09:40:34,481 main.py:52] epoch 9598, training loss: 5523.28, average training loss: 5368.77, base loss: 15970.03
[INFO 2017-06-30 09:40:37,645 main.py:52] epoch 9599, training loss: 5630.86, average training loss: 5369.06, base loss: 15969.99
[INFO 2017-06-30 09:40:37,645 main.py:54] epoch 9599, testing
[INFO 2017-06-30 09:40:50,734 main.py:97] average testing loss: 5280.43, base loss: 15677.71
[INFO 2017-06-30 09:40:50,734 main.py:98] improve_loss: 10397.28, improve_percent: 0.66
[INFO 2017-06-30 09:40:50,735 main.py:66] current best improved percent: 0.67
[INFO 2017-06-30 09:40:53,875 main.py:52] epoch 9600, training loss: 5080.03, average training loss: 5368.87, base loss: 15969.44
[INFO 2017-06-30 09:40:57,021 main.py:52] epoch 9601, training loss: 5453.40, average training loss: 5369.13, base loss: 15969.12
[INFO 2017-06-30 09:41:00,149 main.py:52] epoch 9602, training loss: 5305.72, average training loss: 5369.20, base loss: 15968.85
[INFO 2017-06-30 09:41:03,291 main.py:52] epoch 9603, training loss: 5209.49, average training loss: 5368.80, base loss: 15968.59
[INFO 2017-06-30 09:41:06,373 main.py:52] epoch 9604, training loss: 4865.82, average training loss: 5368.06, base loss: 15968.06
[INFO 2017-06-30 09:41:09,546 main.py:52] epoch 9605, training loss: 5265.53, average training loss: 5368.25, base loss: 15968.07
[INFO 2017-06-30 09:41:12,689 main.py:52] epoch 9606, training loss: 5212.75, average training loss: 5368.09, base loss: 15968.41
[INFO 2017-06-30 09:41:15,868 main.py:52] epoch 9607, training loss: 5175.28, average training loss: 5367.61, base loss: 15968.35
[INFO 2017-06-30 09:41:19,001 main.py:52] epoch 9608, training loss: 4919.34, average training loss: 5367.58, base loss: 15967.86
[INFO 2017-06-30 09:41:22,132 main.py:52] epoch 9609, training loss: 4851.80, average training loss: 5367.04, base loss: 15967.50
[INFO 2017-06-30 09:41:25,261 main.py:52] epoch 9610, training loss: 5705.10, average training loss: 5367.47, base loss: 15967.99
[INFO 2017-06-30 09:41:28,459 main.py:52] epoch 9611, training loss: 5250.49, average training loss: 5367.14, base loss: 15968.03
[INFO 2017-06-30 09:41:31,577 main.py:52] epoch 9612, training loss: 4664.98, average training loss: 5366.49, base loss: 15967.57
[INFO 2017-06-30 09:41:34,728 main.py:52] epoch 9613, training loss: 5502.82, average training loss: 5366.79, base loss: 15967.68
[INFO 2017-06-30 09:41:37,871 main.py:52] epoch 9614, training loss: 5248.36, average training loss: 5366.45, base loss: 15967.59
[INFO 2017-06-30 09:41:41,039 main.py:52] epoch 9615, training loss: 5263.35, average training loss: 5366.30, base loss: 15967.43
[INFO 2017-06-30 09:41:44,221 main.py:52] epoch 9616, training loss: 5030.04, average training loss: 5365.74, base loss: 15967.36
[INFO 2017-06-30 09:41:47,339 main.py:52] epoch 9617, training loss: 5308.55, average training loss: 5365.56, base loss: 15967.18
[INFO 2017-06-30 09:41:50,448 main.py:52] epoch 9618, training loss: 5589.30, average training loss: 5365.65, base loss: 15967.12
[INFO 2017-06-30 09:41:53,612 main.py:52] epoch 9619, training loss: 5165.80, average training loss: 5365.55, base loss: 15966.94
[INFO 2017-06-30 09:41:56,745 main.py:52] epoch 9620, training loss: 6008.60, average training loss: 5366.11, base loss: 15967.22
[INFO 2017-06-30 09:41:59,904 main.py:52] epoch 9621, training loss: 5490.67, average training loss: 5366.20, base loss: 15967.41
[INFO 2017-06-30 09:42:03,049 main.py:52] epoch 9622, training loss: 5429.49, average training loss: 5366.44, base loss: 15967.28
[INFO 2017-06-30 09:42:06,226 main.py:52] epoch 9623, training loss: 5442.50, average training loss: 5366.39, base loss: 15967.47
[INFO 2017-06-30 09:42:09,387 main.py:52] epoch 9624, training loss: 5655.84, average training loss: 5366.77, base loss: 15967.88
[INFO 2017-06-30 09:42:12,511 main.py:52] epoch 9625, training loss: 5321.97, average training loss: 5366.59, base loss: 15968.00
[INFO 2017-06-30 09:42:15,694 main.py:52] epoch 9626, training loss: 5123.98, average training loss: 5366.29, base loss: 15968.07
[INFO 2017-06-30 09:42:18,794 main.py:52] epoch 9627, training loss: 5051.00, average training loss: 5366.05, base loss: 15967.68
[INFO 2017-06-30 09:42:21,940 main.py:52] epoch 9628, training loss: 5299.92, average training loss: 5365.90, base loss: 15967.73
[INFO 2017-06-30 09:42:25,083 main.py:52] epoch 9629, training loss: 5699.14, average training loss: 5365.94, base loss: 15968.11
[INFO 2017-06-30 09:42:28,233 main.py:52] epoch 9630, training loss: 5220.54, average training loss: 5365.99, base loss: 15968.08
[INFO 2017-06-30 09:42:31,347 main.py:52] epoch 9631, training loss: 5063.65, average training loss: 5365.45, base loss: 15968.01
[INFO 2017-06-30 09:42:34,498 main.py:52] epoch 9632, training loss: 5098.10, average training loss: 5365.36, base loss: 15967.76
[INFO 2017-06-30 09:42:37,648 main.py:52] epoch 9633, training loss: 5129.14, average training loss: 5365.43, base loss: 15967.29
[INFO 2017-06-30 09:42:40,805 main.py:52] epoch 9634, training loss: 5208.79, average training loss: 5365.52, base loss: 15967.00
[INFO 2017-06-30 09:42:43,951 main.py:52] epoch 9635, training loss: 5542.78, average training loss: 5365.13, base loss: 15967.06
[INFO 2017-06-30 09:42:47,093 main.py:52] epoch 9636, training loss: 5194.16, average training loss: 5364.79, base loss: 15966.69
[INFO 2017-06-30 09:42:50,235 main.py:52] epoch 9637, training loss: 5387.92, average training loss: 5365.05, base loss: 15966.71
[INFO 2017-06-30 09:42:53,336 main.py:52] epoch 9638, training loss: 5661.50, average training loss: 5365.13, base loss: 15967.05
[INFO 2017-06-30 09:42:56,479 main.py:52] epoch 9639, training loss: 5713.11, average training loss: 5364.96, base loss: 15967.51
[INFO 2017-06-30 09:42:59,642 main.py:52] epoch 9640, training loss: 5309.66, average training loss: 5364.72, base loss: 15967.09
[INFO 2017-06-30 09:43:02,787 main.py:52] epoch 9641, training loss: 5440.33, average training loss: 5365.20, base loss: 15967.54
[INFO 2017-06-30 09:43:05,935 main.py:52] epoch 9642, training loss: 5295.94, average training loss: 5364.81, base loss: 15967.45
[INFO 2017-06-30 09:43:09,082 main.py:52] epoch 9643, training loss: 5288.40, average training loss: 5364.45, base loss: 15967.36
[INFO 2017-06-30 09:43:12,256 main.py:52] epoch 9644, training loss: 5387.50, average training loss: 5364.16, base loss: 15967.36
[INFO 2017-06-30 09:43:15,394 main.py:52] epoch 9645, training loss: 5158.81, average training loss: 5364.03, base loss: 15967.04
[INFO 2017-06-30 09:43:18,498 main.py:52] epoch 9646, training loss: 5461.03, average training loss: 5364.01, base loss: 15966.93
[INFO 2017-06-30 09:43:21,619 main.py:52] epoch 9647, training loss: 5392.01, average training loss: 5364.00, base loss: 15967.00
[INFO 2017-06-30 09:43:24,763 main.py:52] epoch 9648, training loss: 5000.26, average training loss: 5363.58, base loss: 15966.67
[INFO 2017-06-30 09:43:27,926 main.py:52] epoch 9649, training loss: 5467.90, average training loss: 5363.82, base loss: 15966.61
[INFO 2017-06-30 09:43:31,107 main.py:52] epoch 9650, training loss: 5180.86, average training loss: 5363.44, base loss: 15966.49
[INFO 2017-06-30 09:43:34,196 main.py:52] epoch 9651, training loss: 5166.79, average training loss: 5363.09, base loss: 15966.36
[INFO 2017-06-30 09:43:37,394 main.py:52] epoch 9652, training loss: 5577.50, average training loss: 5363.27, base loss: 15966.54
[INFO 2017-06-30 09:43:40,510 main.py:52] epoch 9653, training loss: 5362.39, average training loss: 5363.18, base loss: 15966.28
[INFO 2017-06-30 09:43:43,624 main.py:52] epoch 9654, training loss: 5316.58, average training loss: 5363.37, base loss: 15966.23
[INFO 2017-06-30 09:43:46,718 main.py:52] epoch 9655, training loss: 5259.00, average training loss: 5363.05, base loss: 15965.92
[INFO 2017-06-30 09:43:49,852 main.py:52] epoch 9656, training loss: 5655.20, average training loss: 5363.45, base loss: 15965.80
[INFO 2017-06-30 09:43:53,012 main.py:52] epoch 9657, training loss: 4900.74, average training loss: 5363.14, base loss: 15965.60
[INFO 2017-06-30 09:43:56,153 main.py:52] epoch 9658, training loss: 5628.81, average training loss: 5363.31, base loss: 15965.43
[INFO 2017-06-30 09:43:59,244 main.py:52] epoch 9659, training loss: 5527.00, average training loss: 5363.40, base loss: 15965.39
[INFO 2017-06-30 09:44:02,394 main.py:52] epoch 9660, training loss: 5311.06, average training loss: 5363.18, base loss: 15965.50
[INFO 2017-06-30 09:44:05,582 main.py:52] epoch 9661, training loss: 4609.61, average training loss: 5362.21, base loss: 15965.22
[INFO 2017-06-30 09:44:08,779 main.py:52] epoch 9662, training loss: 5395.67, average training loss: 5361.89, base loss: 15965.42
[INFO 2017-06-30 09:44:11,967 main.py:52] epoch 9663, training loss: 5350.81, average training loss: 5361.61, base loss: 15965.52
[INFO 2017-06-30 09:44:15,091 main.py:52] epoch 9664, training loss: 5445.41, average training loss: 5361.48, base loss: 15965.86
[INFO 2017-06-30 09:44:18,221 main.py:52] epoch 9665, training loss: 5436.68, average training loss: 5361.49, base loss: 15965.87
[INFO 2017-06-30 09:44:21,352 main.py:52] epoch 9666, training loss: 5090.67, average training loss: 5361.65, base loss: 15965.79
[INFO 2017-06-30 09:44:24,488 main.py:52] epoch 9667, training loss: 5267.39, average training loss: 5361.05, base loss: 15965.39
[INFO 2017-06-30 09:44:27,622 main.py:52] epoch 9668, training loss: 5127.73, average training loss: 5360.63, base loss: 15964.94
[INFO 2017-06-30 09:44:30,804 main.py:52] epoch 9669, training loss: 5205.26, average training loss: 5360.92, base loss: 15964.42
[INFO 2017-06-30 09:44:33,957 main.py:52] epoch 9670, training loss: 5585.58, average training loss: 5360.85, base loss: 15964.44
[INFO 2017-06-30 09:44:37,110 main.py:52] epoch 9671, training loss: 4959.75, average training loss: 5360.64, base loss: 15964.14
[INFO 2017-06-30 09:44:40,230 main.py:52] epoch 9672, training loss: 5080.56, average training loss: 5360.95, base loss: 15964.10
[INFO 2017-06-30 09:44:43,393 main.py:52] epoch 9673, training loss: 5396.58, average training loss: 5360.91, base loss: 15963.91
[INFO 2017-06-30 09:44:46,540 main.py:52] epoch 9674, training loss: 5087.12, average training loss: 5360.76, base loss: 15963.84
[INFO 2017-06-30 09:44:49,692 main.py:52] epoch 9675, training loss: 5294.67, average training loss: 5360.90, base loss: 15964.04
[INFO 2017-06-30 09:44:52,816 main.py:52] epoch 9676, training loss: 5108.78, average training loss: 5360.86, base loss: 15963.82
[INFO 2017-06-30 09:44:55,976 main.py:52] epoch 9677, training loss: 5719.51, average training loss: 5360.84, base loss: 15964.36
[INFO 2017-06-30 09:44:59,102 main.py:52] epoch 9678, training loss: 4871.43, average training loss: 5360.34, base loss: 15964.00
[INFO 2017-06-30 09:45:02,259 main.py:52] epoch 9679, training loss: 5358.84, average training loss: 5360.55, base loss: 15963.74
[INFO 2017-06-30 09:45:05,431 main.py:52] epoch 9680, training loss: 5430.99, average training loss: 5360.53, base loss: 15963.67
[INFO 2017-06-30 09:45:08,534 main.py:52] epoch 9681, training loss: 5294.54, average training loss: 5360.22, base loss: 15963.46
[INFO 2017-06-30 09:45:11,662 main.py:52] epoch 9682, training loss: 5223.65, average training loss: 5360.22, base loss: 15963.46
[INFO 2017-06-30 09:45:14,787 main.py:52] epoch 9683, training loss: 5024.88, average training loss: 5359.50, base loss: 15963.32
[INFO 2017-06-30 09:45:17,908 main.py:52] epoch 9684, training loss: 5259.45, average training loss: 5359.09, base loss: 15963.07
[INFO 2017-06-30 09:45:21,094 main.py:52] epoch 9685, training loss: 5348.15, average training loss: 5359.21, base loss: 15962.92
[INFO 2017-06-30 09:45:24,214 main.py:52] epoch 9686, training loss: 5116.66, average training loss: 5359.07, base loss: 15962.72
[INFO 2017-06-30 09:45:27,428 main.py:52] epoch 9687, training loss: 5207.31, average training loss: 5358.84, base loss: 15962.71
[INFO 2017-06-30 09:45:30,562 main.py:52] epoch 9688, training loss: 5169.74, average training loss: 5358.69, base loss: 15962.39
[INFO 2017-06-30 09:45:33,682 main.py:52] epoch 9689, training loss: 5165.40, average training loss: 5358.27, base loss: 15962.15
[INFO 2017-06-30 09:45:36,827 main.py:52] epoch 9690, training loss: 5077.02, average training loss: 5357.70, base loss: 15961.98
[INFO 2017-06-30 09:45:39,982 main.py:52] epoch 9691, training loss: 4939.67, average training loss: 5357.17, base loss: 15961.57
[INFO 2017-06-30 09:45:43,136 main.py:52] epoch 9692, training loss: 5350.66, average training loss: 5357.15, base loss: 15961.50
[INFO 2017-06-30 09:45:46,261 main.py:52] epoch 9693, training loss: 5107.16, average training loss: 5356.47, base loss: 15961.23
[INFO 2017-06-30 09:45:49,397 main.py:52] epoch 9694, training loss: 4749.83, average training loss: 5355.42, base loss: 15961.16
[INFO 2017-06-30 09:45:52,550 main.py:52] epoch 9695, training loss: 5197.88, average training loss: 5355.40, base loss: 15961.22
[INFO 2017-06-30 09:45:55,683 main.py:52] epoch 9696, training loss: 5470.64, average training loss: 5355.51, base loss: 15961.49
[INFO 2017-06-30 09:45:58,831 main.py:52] epoch 9697, training loss: 5357.23, average training loss: 5355.40, base loss: 15961.54
[INFO 2017-06-30 09:46:01,989 main.py:52] epoch 9698, training loss: 5089.71, average training loss: 5354.90, base loss: 15961.63
[INFO 2017-06-30 09:46:05,151 main.py:52] epoch 9699, training loss: 5379.76, average training loss: 5354.69, base loss: 15961.43
[INFO 2017-06-30 09:46:05,151 main.py:54] epoch 9699, testing
[INFO 2017-06-30 09:46:18,228 main.py:97] average testing loss: 5361.89, base loss: 16214.26
[INFO 2017-06-30 09:46:18,229 main.py:98] improve_loss: 10852.38, improve_percent: 0.67
[INFO 2017-06-30 09:46:18,230 main.py:66] current best improved percent: 0.67
[INFO 2017-06-30 09:46:21,348 main.py:52] epoch 9700, training loss: 5365.48, average training loss: 5354.68, base loss: 15961.41
[INFO 2017-06-30 09:46:24,488 main.py:52] epoch 9701, training loss: 5162.76, average training loss: 5354.36, base loss: 15961.34
[INFO 2017-06-30 09:46:27,605 main.py:52] epoch 9702, training loss: 5228.35, average training loss: 5353.70, base loss: 15961.55
[INFO 2017-06-30 09:46:30,765 main.py:52] epoch 9703, training loss: 5754.98, average training loss: 5353.88, base loss: 15961.91
[INFO 2017-06-30 09:46:33,908 main.py:52] epoch 9704, training loss: 5076.77, average training loss: 5353.70, base loss: 15961.92
[INFO 2017-06-30 09:46:37,046 main.py:52] epoch 9705, training loss: 5473.76, average training loss: 5353.75, base loss: 15962.08
[INFO 2017-06-30 09:46:40,216 main.py:52] epoch 9706, training loss: 5278.86, average training loss: 5353.72, base loss: 15961.64
[INFO 2017-06-30 09:46:43,357 main.py:52] epoch 9707, training loss: 5547.37, average training loss: 5353.50, base loss: 15961.74
[INFO 2017-06-30 09:46:46,522 main.py:52] epoch 9708, training loss: 5536.26, average training loss: 5353.55, base loss: 15961.98
[INFO 2017-06-30 09:46:49,704 main.py:52] epoch 9709, training loss: 5074.24, average training loss: 5352.79, base loss: 15961.90
[INFO 2017-06-30 09:46:52,821 main.py:52] epoch 9710, training loss: 5231.03, average training loss: 5352.42, base loss: 15961.72
[INFO 2017-06-30 09:46:55,986 main.py:52] epoch 9711, training loss: 5235.82, average training loss: 5352.40, base loss: 15961.70
[INFO 2017-06-30 09:46:59,093 main.py:52] epoch 9712, training loss: 5339.41, average training loss: 5352.01, base loss: 15961.70
[INFO 2017-06-30 09:47:02,262 main.py:52] epoch 9713, training loss: 5286.79, average training loss: 5352.11, base loss: 15961.82
[INFO 2017-06-30 09:47:05,358 main.py:52] epoch 9714, training loss: 5463.36, average training loss: 5352.08, base loss: 15961.84
[INFO 2017-06-30 09:47:08,571 main.py:52] epoch 9715, training loss: 5104.30, average training loss: 5352.14, base loss: 15961.58
[INFO 2017-06-30 09:47:11,674 main.py:52] epoch 9716, training loss: 5294.55, average training loss: 5352.24, base loss: 15961.66
[INFO 2017-06-30 09:47:14,859 main.py:52] epoch 9717, training loss: 5131.72, average training loss: 5351.95, base loss: 15961.66
[INFO 2017-06-30 09:47:17,999 main.py:52] epoch 9718, training loss: 5681.22, average training loss: 5351.90, base loss: 15961.80
[INFO 2017-06-30 09:47:21,139 main.py:52] epoch 9719, training loss: 5400.29, average training loss: 5351.72, base loss: 15961.85
[INFO 2017-06-30 09:47:24,301 main.py:52] epoch 9720, training loss: 5516.99, average training loss: 5351.66, base loss: 15962.21
[INFO 2017-06-30 09:47:27,429 main.py:52] epoch 9721, training loss: 5491.16, average training loss: 5351.45, base loss: 15962.52
[INFO 2017-06-30 09:47:30,593 main.py:52] epoch 9722, training loss: 5182.04, average training loss: 5350.96, base loss: 15962.44
[INFO 2017-06-30 09:47:33,726 main.py:52] epoch 9723, training loss: 5307.54, average training loss: 5351.06, base loss: 15962.44
[INFO 2017-06-30 09:47:36,884 main.py:52] epoch 9724, training loss: 5195.91, average training loss: 5350.66, base loss: 15962.30
[INFO 2017-06-30 09:47:40,021 main.py:52] epoch 9725, training loss: 5483.23, average training loss: 5350.89, base loss: 15962.53
[INFO 2017-06-30 09:47:43,175 main.py:52] epoch 9726, training loss: 5568.90, average training loss: 5351.04, base loss: 15963.01
[INFO 2017-06-30 09:47:46,343 main.py:52] epoch 9727, training loss: 4776.06, average training loss: 5350.08, base loss: 15963.05
[INFO 2017-06-30 09:47:49,476 main.py:52] epoch 9728, training loss: 5328.46, average training loss: 5349.77, base loss: 15963.13
[INFO 2017-06-30 09:47:52,568 main.py:52] epoch 9729, training loss: 5599.03, average training loss: 5350.03, base loss: 15963.28
[INFO 2017-06-30 09:47:55,686 main.py:52] epoch 9730, training loss: 5193.49, average training loss: 5349.97, base loss: 15963.36
[INFO 2017-06-30 09:47:58,827 main.py:52] epoch 9731, training loss: 5186.20, average training loss: 5349.80, base loss: 15963.09
[INFO 2017-06-30 09:48:02,054 main.py:52] epoch 9732, training loss: 5681.16, average training loss: 5350.10, base loss: 15963.11
[INFO 2017-06-30 09:48:05,268 main.py:52] epoch 9733, training loss: 5271.74, average training loss: 5350.14, base loss: 15963.22
[INFO 2017-06-30 09:48:08,369 main.py:52] epoch 9734, training loss: 5451.98, average training loss: 5349.69, base loss: 15963.47
[INFO 2017-06-30 09:48:11,490 main.py:52] epoch 9735, training loss: 5260.04, average training loss: 5349.51, base loss: 15963.07
[INFO 2017-06-30 09:48:14,646 main.py:52] epoch 9736, training loss: 5118.28, average training loss: 5349.08, base loss: 15963.16
[INFO 2017-06-30 09:48:17,749 main.py:52] epoch 9737, training loss: 5343.79, average training loss: 5348.85, base loss: 15963.27
[INFO 2017-06-30 09:48:20,843 main.py:52] epoch 9738, training loss: 5592.49, average training loss: 5349.21, base loss: 15963.12
[INFO 2017-06-30 09:48:23,983 main.py:52] epoch 9739, training loss: 5385.60, average training loss: 5348.86, base loss: 15962.80
[INFO 2017-06-30 09:48:27,125 main.py:52] epoch 9740, training loss: 5328.18, average training loss: 5348.91, base loss: 15962.69
[INFO 2017-06-30 09:48:30,255 main.py:52] epoch 9741, training loss: 5464.58, average training loss: 5348.93, base loss: 15962.92
[INFO 2017-06-30 09:48:33,352 main.py:52] epoch 9742, training loss: 5216.01, average training loss: 5348.70, base loss: 15962.58
[INFO 2017-06-30 09:48:36,482 main.py:52] epoch 9743, training loss: 5175.39, average training loss: 5348.16, base loss: 15962.28
[INFO 2017-06-30 09:48:39,593 main.py:52] epoch 9744, training loss: 5035.33, average training loss: 5348.14, base loss: 15962.04
[INFO 2017-06-30 09:48:42,705 main.py:52] epoch 9745, training loss: 5632.01, average training loss: 5348.37, base loss: 15962.08
[INFO 2017-06-30 09:48:45,852 main.py:52] epoch 9746, training loss: 5116.83, average training loss: 5348.20, base loss: 15962.05
[INFO 2017-06-30 09:48:49,003 main.py:52] epoch 9747, training loss: 5494.32, average training loss: 5348.38, base loss: 15962.19
[INFO 2017-06-30 09:48:52,140 main.py:52] epoch 9748, training loss: 6085.41, average training loss: 5349.18, base loss: 15962.57
[INFO 2017-06-30 09:48:55,327 main.py:52] epoch 9749, training loss: 5029.83, average training loss: 5348.97, base loss: 15962.13
[INFO 2017-06-30 09:48:58,485 main.py:52] epoch 9750, training loss: 5324.81, average training loss: 5348.85, base loss: 15962.25
[INFO 2017-06-30 09:49:01,622 main.py:52] epoch 9751, training loss: 5073.37, average training loss: 5348.29, base loss: 15962.02
[INFO 2017-06-30 09:49:04,754 main.py:52] epoch 9752, training loss: 5480.00, average training loss: 5348.44, base loss: 15961.89
[INFO 2017-06-30 09:49:07,864 main.py:52] epoch 9753, training loss: 5082.50, average training loss: 5348.24, base loss: 15961.71
[INFO 2017-06-30 09:49:10,991 main.py:52] epoch 9754, training loss: 5119.96, average training loss: 5348.02, base loss: 15961.62
[INFO 2017-06-30 09:49:14,128 main.py:52] epoch 9755, training loss: 5035.02, average training loss: 5347.99, base loss: 15961.35
[INFO 2017-06-30 09:49:17,301 main.py:52] epoch 9756, training loss: 4776.25, average training loss: 5347.47, base loss: 15961.05
[INFO 2017-06-30 09:49:20,465 main.py:52] epoch 9757, training loss: 5377.18, average training loss: 5347.62, base loss: 15960.79
[INFO 2017-06-30 09:49:23,686 main.py:52] epoch 9758, training loss: 5256.23, average training loss: 5347.70, base loss: 15960.88
[INFO 2017-06-30 09:49:26,809 main.py:52] epoch 9759, training loss: 5215.74, average training loss: 5347.72, base loss: 15960.71
[INFO 2017-06-30 09:49:29,960 main.py:52] epoch 9760, training loss: 5262.29, average training loss: 5347.34, base loss: 15960.76
[INFO 2017-06-30 09:49:33,082 main.py:52] epoch 9761, training loss: 5388.79, average training loss: 5347.58, base loss: 15961.13
[INFO 2017-06-30 09:49:36,281 main.py:52] epoch 9762, training loss: 5290.31, average training loss: 5347.59, base loss: 15961.03
[INFO 2017-06-30 09:49:39,390 main.py:52] epoch 9763, training loss: 5382.26, average training loss: 5347.31, base loss: 15960.95
[INFO 2017-06-30 09:49:42,527 main.py:52] epoch 9764, training loss: 5460.43, average training loss: 5347.42, base loss: 15960.86
[INFO 2017-06-30 09:49:45,641 main.py:52] epoch 9765, training loss: 4777.60, average training loss: 5347.25, base loss: 15960.71
[INFO 2017-06-30 09:49:48,792 main.py:52] epoch 9766, training loss: 5281.20, average training loss: 5347.11, base loss: 15960.54
[INFO 2017-06-30 09:49:51,954 main.py:52] epoch 9767, training loss: 5246.72, average training loss: 5347.03, base loss: 15960.70
[INFO 2017-06-30 09:49:55,053 main.py:52] epoch 9768, training loss: 5453.12, average training loss: 5347.03, base loss: 15960.78
[INFO 2017-06-30 09:49:58,196 main.py:52] epoch 9769, training loss: 5249.25, average training loss: 5346.73, base loss: 15960.60
[INFO 2017-06-30 09:50:01,356 main.py:52] epoch 9770, training loss: 5197.79, average training loss: 5346.44, base loss: 15960.51
[INFO 2017-06-30 09:50:04,497 main.py:52] epoch 9771, training loss: 5346.03, average training loss: 5346.64, base loss: 15960.41
[INFO 2017-06-30 09:50:07,615 main.py:52] epoch 9772, training loss: 5416.78, average training loss: 5346.80, base loss: 15960.45
[INFO 2017-06-30 09:50:10,757 main.py:52] epoch 9773, training loss: 5403.01, average training loss: 5346.65, base loss: 15960.53
[INFO 2017-06-30 09:50:13,895 main.py:52] epoch 9774, training loss: 5212.34, average training loss: 5346.58, base loss: 15960.71
[INFO 2017-06-30 09:50:17,030 main.py:52] epoch 9775, training loss: 4934.16, average training loss: 5346.09, base loss: 15960.35
[INFO 2017-06-30 09:50:20,165 main.py:52] epoch 9776, training loss: 5611.75, average training loss: 5346.69, base loss: 15960.63
[INFO 2017-06-30 09:50:23,278 main.py:52] epoch 9777, training loss: 5089.12, average training loss: 5346.52, base loss: 15960.60
[INFO 2017-06-30 09:50:26,441 main.py:52] epoch 9778, training loss: 5115.18, average training loss: 5346.22, base loss: 15960.76
[INFO 2017-06-30 09:50:29,570 main.py:52] epoch 9779, training loss: 5161.12, average training loss: 5345.94, base loss: 15960.68
[INFO 2017-06-30 09:50:32,720 main.py:52] epoch 9780, training loss: 5321.01, average training loss: 5345.80, base loss: 15960.86
[INFO 2017-06-30 09:50:35,873 main.py:52] epoch 9781, training loss: 5593.56, average training loss: 5345.78, base loss: 15960.71
[INFO 2017-06-30 09:50:38,986 main.py:52] epoch 9782, training loss: 5578.86, average training loss: 5345.83, base loss: 15960.78
[INFO 2017-06-30 09:50:42,136 main.py:52] epoch 9783, training loss: 5499.28, average training loss: 5346.16, base loss: 15960.89
[INFO 2017-06-30 09:50:45,295 main.py:52] epoch 9784, training loss: 5093.35, average training loss: 5345.95, base loss: 15960.73
[INFO 2017-06-30 09:50:48,483 main.py:52] epoch 9785, training loss: 5119.33, average training loss: 5345.77, base loss: 15960.69
[INFO 2017-06-30 09:50:51,653 main.py:52] epoch 9786, training loss: 5464.09, average training loss: 5346.02, base loss: 15960.72
[INFO 2017-06-30 09:50:54,821 main.py:52] epoch 9787, training loss: 5437.58, average training loss: 5346.33, base loss: 15960.81
[INFO 2017-06-30 09:50:57,950 main.py:52] epoch 9788, training loss: 5113.35, average training loss: 5346.02, base loss: 15960.81
[INFO 2017-06-30 09:51:01,113 main.py:52] epoch 9789, training loss: 5417.15, average training loss: 5346.48, base loss: 15961.02
[INFO 2017-06-30 09:51:04,228 main.py:52] epoch 9790, training loss: 5246.73, average training loss: 5346.60, base loss: 15960.75
[INFO 2017-06-30 09:51:07,383 main.py:52] epoch 9791, training loss: 5828.34, average training loss: 5346.68, base loss: 15961.09
[INFO 2017-06-30 09:51:10,565 main.py:52] epoch 9792, training loss: 5475.89, average training loss: 5346.78, base loss: 15961.17
[INFO 2017-06-30 09:51:13,647 main.py:52] epoch 9793, training loss: 5338.53, average training loss: 5346.53, base loss: 15961.31
[INFO 2017-06-30 09:51:16,816 main.py:52] epoch 9794, training loss: 5809.57, average training loss: 5347.27, base loss: 15961.80
[INFO 2017-06-30 09:51:19,977 main.py:52] epoch 9795, training loss: 5195.54, average training loss: 5347.04, base loss: 15962.02
[INFO 2017-06-30 09:51:23,104 main.py:52] epoch 9796, training loss: 5277.99, average training loss: 5347.18, base loss: 15961.82
[INFO 2017-06-30 09:51:26,224 main.py:52] epoch 9797, training loss: 5793.82, average training loss: 5347.31, base loss: 15962.36
[INFO 2017-06-30 09:51:29,378 main.py:52] epoch 9798, training loss: 5763.19, average training loss: 5347.89, base loss: 15962.63
[INFO 2017-06-30 09:51:32,515 main.py:52] epoch 9799, training loss: 5474.98, average training loss: 5348.22, base loss: 15962.32
[INFO 2017-06-30 09:51:32,515 main.py:54] epoch 9799, testing
[INFO 2017-06-30 09:51:45,561 main.py:97] average testing loss: 5337.02, base loss: 15829.14
[INFO 2017-06-30 09:51:45,562 main.py:98] improve_loss: 10492.13, improve_percent: 0.66
[INFO 2017-06-30 09:51:45,563 main.py:66] current best improved percent: 0.67
[INFO 2017-06-30 09:51:48,673 main.py:52] epoch 9800, training loss: 5065.20, average training loss: 5347.86, base loss: 15962.15
[INFO 2017-06-30 09:51:51,785 main.py:52] epoch 9801, training loss: 5353.77, average training loss: 5347.79, base loss: 15962.16
[INFO 2017-06-30 09:51:54,923 main.py:52] epoch 9802, training loss: 5267.02, average training loss: 5347.74, base loss: 15962.08
[INFO 2017-06-30 09:51:58,036 main.py:52] epoch 9803, training loss: 5248.56, average training loss: 5347.28, base loss: 15962.22
[INFO 2017-06-30 09:52:01,161 main.py:52] epoch 9804, training loss: 5759.49, average training loss: 5347.64, base loss: 15962.49
[INFO 2017-06-30 09:52:04,289 main.py:52] epoch 9805, training loss: 5137.87, average training loss: 5347.53, base loss: 15962.21
[INFO 2017-06-30 09:52:07,466 main.py:52] epoch 9806, training loss: 5360.47, average training loss: 5347.84, base loss: 15962.08
[INFO 2017-06-30 09:52:10,561 main.py:52] epoch 9807, training loss: 5494.08, average training loss: 5347.96, base loss: 15962.51
[INFO 2017-06-30 09:52:13,656 main.py:52] epoch 9808, training loss: 5060.68, average training loss: 5347.53, base loss: 15962.36
[INFO 2017-06-30 09:52:16,778 main.py:52] epoch 9809, training loss: 5547.86, average training loss: 5348.06, base loss: 15962.62
[INFO 2017-06-30 09:52:19,902 main.py:52] epoch 9810, training loss: 5273.40, average training loss: 5347.69, base loss: 15962.72
[INFO 2017-06-30 09:52:23,058 main.py:52] epoch 9811, training loss: 5098.09, average training loss: 5347.53, base loss: 15962.88
[INFO 2017-06-30 09:52:26,199 main.py:52] epoch 9812, training loss: 5391.50, average training loss: 5347.74, base loss: 15962.79
[INFO 2017-06-30 09:52:29,356 main.py:52] epoch 9813, training loss: 5484.11, average training loss: 5347.91, base loss: 15963.17
[INFO 2017-06-30 09:52:32,474 main.py:52] epoch 9814, training loss: 5148.64, average training loss: 5347.66, base loss: 15963.25
[INFO 2017-06-30 09:52:35,648 main.py:52] epoch 9815, training loss: 5158.41, average training loss: 5347.28, base loss: 15963.24
[INFO 2017-06-30 09:52:38,797 main.py:52] epoch 9816, training loss: 5279.87, average training loss: 5347.25, base loss: 15963.10
[INFO 2017-06-30 09:52:41,953 main.py:52] epoch 9817, training loss: 5267.60, average training loss: 5347.20, base loss: 15963.10
[INFO 2017-06-30 09:52:45,090 main.py:52] epoch 9818, training loss: 5327.09, average training loss: 5347.10, base loss: 15962.95
[INFO 2017-06-30 09:52:48,240 main.py:52] epoch 9819, training loss: 4998.35, average training loss: 5346.96, base loss: 15962.74
[INFO 2017-06-30 09:52:51,415 main.py:52] epoch 9820, training loss: 5544.22, average training loss: 5347.53, base loss: 15962.99
[INFO 2017-06-30 09:52:54,579 main.py:52] epoch 9821, training loss: 5580.00, average training loss: 5348.04, base loss: 15963.46
[INFO 2017-06-30 09:52:57,698 main.py:52] epoch 9822, training loss: 5388.62, average training loss: 5348.25, base loss: 15963.44
[INFO 2017-06-30 09:53:00,847 main.py:52] epoch 9823, training loss: 5999.02, average training loss: 5349.26, base loss: 15964.03
[INFO 2017-06-30 09:53:03,978 main.py:52] epoch 9824, training loss: 4914.57, average training loss: 5349.13, base loss: 15964.03
[INFO 2017-06-30 09:53:07,101 main.py:52] epoch 9825, training loss: 5185.03, average training loss: 5349.22, base loss: 15964.17
[INFO 2017-06-30 09:53:10,278 main.py:52] epoch 9826, training loss: 5245.28, average training loss: 5348.71, base loss: 15963.96
[INFO 2017-06-30 09:53:13,412 main.py:52] epoch 9827, training loss: 5572.33, average training loss: 5348.94, base loss: 15964.13
[INFO 2017-06-30 09:53:16,538 main.py:52] epoch 9828, training loss: 5277.44, average training loss: 5349.16, base loss: 15963.93
[INFO 2017-06-30 09:53:19,700 main.py:52] epoch 9829, training loss: 5145.38, average training loss: 5348.42, base loss: 15963.69
[INFO 2017-06-30 09:53:22,838 main.py:52] epoch 9830, training loss: 5225.66, average training loss: 5348.49, base loss: 15963.62
[INFO 2017-06-30 09:53:25,969 main.py:52] epoch 9831, training loss: 5074.99, average training loss: 5348.48, base loss: 15963.36
[INFO 2017-06-30 09:53:29,076 main.py:52] epoch 9832, training loss: 5361.50, average training loss: 5348.18, base loss: 15963.50
[INFO 2017-06-30 09:53:32,249 main.py:52] epoch 9833, training loss: 5740.69, average training loss: 5348.78, base loss: 15964.05
[INFO 2017-06-30 09:53:35,375 main.py:52] epoch 9834, training loss: 5164.90, average training loss: 5348.32, base loss: 15964.22
[INFO 2017-06-30 09:53:38,507 main.py:52] epoch 9835, training loss: 5115.89, average training loss: 5347.98, base loss: 15964.05
[INFO 2017-06-30 09:53:41,646 main.py:52] epoch 9836, training loss: 5497.52, average training loss: 5348.02, base loss: 15964.09
[INFO 2017-06-30 09:53:44,841 main.py:52] epoch 9837, training loss: 5429.50, average training loss: 5348.22, base loss: 15964.06
[INFO 2017-06-30 09:53:48,009 main.py:52] epoch 9838, training loss: 5525.49, average training loss: 5348.54, base loss: 15964.12
[INFO 2017-06-30 09:53:51,186 main.py:52] epoch 9839, training loss: 5699.15, average training loss: 5348.89, base loss: 15964.36
[INFO 2017-06-30 09:53:54,333 main.py:52] epoch 9840, training loss: 5254.97, average training loss: 5348.50, base loss: 15964.31
[INFO 2017-06-30 09:53:57,465 main.py:52] epoch 9841, training loss: 5272.74, average training loss: 5348.39, base loss: 15964.16
[INFO 2017-06-30 09:54:00,618 main.py:52] epoch 9842, training loss: 5258.60, average training loss: 5348.10, base loss: 15964.19
[INFO 2017-06-30 09:54:03,770 main.py:52] epoch 9843, training loss: 5332.34, average training loss: 5347.50, base loss: 15964.04
[INFO 2017-06-30 09:54:06,887 main.py:52] epoch 9844, training loss: 5272.40, average training loss: 5347.65, base loss: 15964.00
[INFO 2017-06-30 09:54:10,083 main.py:52] epoch 9845, training loss: 5657.74, average training loss: 5347.97, base loss: 15964.05
[INFO 2017-06-30 09:54:13,175 main.py:52] epoch 9846, training loss: 5564.22, average training loss: 5348.38, base loss: 15964.11
[INFO 2017-06-30 09:54:16,341 main.py:52] epoch 9847, training loss: 5510.65, average training loss: 5348.56, base loss: 15964.28
[INFO 2017-06-30 09:54:19,482 main.py:52] epoch 9848, training loss: 5519.23, average training loss: 5348.89, base loss: 15964.47
[INFO 2017-06-30 09:54:22,634 main.py:52] epoch 9849, training loss: 5388.51, average training loss: 5348.60, base loss: 15964.33
[INFO 2017-06-30 09:54:25,766 main.py:52] epoch 9850, training loss: 5548.75, average training loss: 5348.63, base loss: 15964.32
[INFO 2017-06-30 09:54:28,922 main.py:52] epoch 9851, training loss: 5268.62, average training loss: 5348.45, base loss: 15964.39
[INFO 2017-06-30 09:54:32,056 main.py:52] epoch 9852, training loss: 5243.80, average training loss: 5348.51, base loss: 15964.72
[INFO 2017-06-30 09:54:35,176 main.py:52] epoch 9853, training loss: 5618.99, average training loss: 5348.38, base loss: 15964.83
[INFO 2017-06-30 09:54:38,319 main.py:52] epoch 9854, training loss: 5244.02, average training loss: 5348.37, base loss: 15964.76
[INFO 2017-06-30 09:54:41,457 main.py:52] epoch 9855, training loss: 5480.05, average training loss: 5348.27, base loss: 15964.43
[INFO 2017-06-30 09:54:44,604 main.py:52] epoch 9856, training loss: 5381.38, average training loss: 5348.14, base loss: 15964.43
[INFO 2017-06-30 09:54:47,769 main.py:52] epoch 9857, training loss: 5149.44, average training loss: 5347.33, base loss: 15964.11
[INFO 2017-06-30 09:54:50,904 main.py:52] epoch 9858, training loss: 4981.61, average training loss: 5346.96, base loss: 15963.65
[INFO 2017-06-30 09:54:54,017 main.py:52] epoch 9859, training loss: 5251.90, average training loss: 5346.57, base loss: 15963.36
[INFO 2017-06-30 09:54:57,195 main.py:52] epoch 9860, training loss: 5794.03, average training loss: 5347.08, base loss: 15963.74
[INFO 2017-06-30 09:55:00,325 main.py:52] epoch 9861, training loss: 5234.16, average training loss: 5346.98, base loss: 15963.78
[INFO 2017-06-30 09:55:03,463 main.py:52] epoch 9862, training loss: 5505.70, average training loss: 5347.10, base loss: 15963.86
[INFO 2017-06-30 09:55:06,609 main.py:52] epoch 9863, training loss: 5044.83, average training loss: 5346.76, base loss: 15963.37
[INFO 2017-06-30 09:55:09,808 main.py:52] epoch 9864, training loss: 5500.15, average training loss: 5346.80, base loss: 15963.55
[INFO 2017-06-30 09:55:12,933 main.py:52] epoch 9865, training loss: 5464.11, average training loss: 5346.95, base loss: 15963.48
[INFO 2017-06-30 09:55:16,071 main.py:52] epoch 9866, training loss: 5104.99, average training loss: 5346.72, base loss: 15963.41
[INFO 2017-06-30 09:55:19,194 main.py:52] epoch 9867, training loss: 5611.32, average training loss: 5346.37, base loss: 15963.54
[INFO 2017-06-30 09:55:22,312 main.py:52] epoch 9868, training loss: 5086.83, average training loss: 5346.25, base loss: 15963.30
[INFO 2017-06-30 09:55:25,483 main.py:52] epoch 9869, training loss: 5553.88, average training loss: 5346.59, base loss: 15963.47
[INFO 2017-06-30 09:55:28,611 main.py:52] epoch 9870, training loss: 5159.10, average training loss: 5346.41, base loss: 15963.31
[INFO 2017-06-30 09:55:31,786 main.py:52] epoch 9871, training loss: 5369.87, average training loss: 5345.98, base loss: 15963.46
[INFO 2017-06-30 09:55:34,925 main.py:52] epoch 9872, training loss: 5567.11, average training loss: 5346.36, base loss: 15963.21
[INFO 2017-06-30 09:55:38,082 main.py:52] epoch 9873, training loss: 5197.34, average training loss: 5346.71, base loss: 15963.03
[INFO 2017-06-30 09:55:41,207 main.py:52] epoch 9874, training loss: 5323.47, average training loss: 5346.52, base loss: 15962.89
[INFO 2017-06-30 09:55:44,339 main.py:52] epoch 9875, training loss: 5032.59, average training loss: 5345.99, base loss: 15962.45
[INFO 2017-06-30 09:55:47,472 main.py:52] epoch 9876, training loss: 5310.71, average training loss: 5346.24, base loss: 15962.49
[INFO 2017-06-30 09:55:50,587 main.py:52] epoch 9877, training loss: 5409.84, average training loss: 5346.43, base loss: 15962.39
[INFO 2017-06-30 09:55:53,767 main.py:52] epoch 9878, training loss: 5126.14, average training loss: 5346.07, base loss: 15962.00
[INFO 2017-06-30 09:55:56,909 main.py:52] epoch 9879, training loss: 5194.36, average training loss: 5345.84, base loss: 15961.78
[INFO 2017-06-30 09:56:00,042 main.py:52] epoch 9880, training loss: 5636.96, average training loss: 5346.30, base loss: 15962.15
[INFO 2017-06-30 09:56:03,161 main.py:52] epoch 9881, training loss: 5009.72, average training loss: 5345.95, base loss: 15961.98
[INFO 2017-06-30 09:56:06,290 main.py:52] epoch 9882, training loss: 5519.90, average training loss: 5346.16, base loss: 15962.25
[INFO 2017-06-30 09:56:09,454 main.py:52] epoch 9883, training loss: 5360.37, average training loss: 5346.21, base loss: 15962.31
[INFO 2017-06-30 09:56:12,584 main.py:52] epoch 9884, training loss: 5351.67, average training loss: 5346.31, base loss: 15962.30
[INFO 2017-06-30 09:56:15,723 main.py:52] epoch 9885, training loss: 5093.71, average training loss: 5345.72, base loss: 15962.00
[INFO 2017-06-30 09:56:18,842 main.py:52] epoch 9886, training loss: 5835.10, average training loss: 5346.31, base loss: 15962.48
[INFO 2017-06-30 09:56:22,014 main.py:52] epoch 9887, training loss: 5570.20, average training loss: 5346.57, base loss: 15962.61
[INFO 2017-06-30 09:56:25,171 main.py:52] epoch 9888, training loss: 5195.11, average training loss: 5346.16, base loss: 15962.87
[INFO 2017-06-30 09:56:28,323 main.py:52] epoch 9889, training loss: 5137.36, average training loss: 5345.49, base loss: 15963.02
[INFO 2017-06-30 09:56:31,466 main.py:52] epoch 9890, training loss: 5630.84, average training loss: 5345.77, base loss: 15963.52
[INFO 2017-06-30 09:56:34,626 main.py:52] epoch 9891, training loss: 5846.03, average training loss: 5346.03, base loss: 15964.08
[INFO 2017-06-30 09:56:37,749 main.py:52] epoch 9892, training loss: 4947.04, average training loss: 5345.82, base loss: 15963.97
[INFO 2017-06-30 09:56:40,889 main.py:52] epoch 9893, training loss: 5234.34, average training loss: 5346.19, base loss: 15963.66
[INFO 2017-06-30 09:56:44,017 main.py:52] epoch 9894, training loss: 5656.90, average training loss: 5346.44, base loss: 15964.05
[INFO 2017-06-30 09:56:47,166 main.py:52] epoch 9895, training loss: 5227.95, average training loss: 5346.35, base loss: 15964.05
[INFO 2017-06-30 09:56:50,288 main.py:52] epoch 9896, training loss: 5570.34, average training loss: 5346.71, base loss: 15963.83
[INFO 2017-06-30 09:56:53,426 main.py:52] epoch 9897, training loss: 5422.23, average training loss: 5346.73, base loss: 15964.03
[INFO 2017-06-30 09:56:56,559 main.py:52] epoch 9898, training loss: 5238.59, average training loss: 5346.65, base loss: 15964.05
[INFO 2017-06-30 09:56:59,728 main.py:52] epoch 9899, training loss: 5648.18, average training loss: 5346.74, base loss: 15964.26
[INFO 2017-06-30 09:56:59,728 main.py:54] epoch 9899, testing
[INFO 2017-06-30 09:57:12,752 main.py:97] average testing loss: 5242.15, base loss: 15490.17
[INFO 2017-06-30 09:57:12,752 main.py:98] improve_loss: 10248.03, improve_percent: 0.66
[INFO 2017-06-30 09:57:12,754 main.py:66] current best improved percent: 0.67
[INFO 2017-06-30 09:57:15,914 main.py:52] epoch 9900, training loss: 5123.15, average training loss: 5346.36, base loss: 15964.05
[INFO 2017-06-30 09:57:19,067 main.py:52] epoch 9901, training loss: 5455.34, average training loss: 5346.45, base loss: 15964.02
[INFO 2017-06-30 09:57:22,177 main.py:52] epoch 9902, training loss: 5033.50, average training loss: 5346.02, base loss: 15964.21
[INFO 2017-06-30 09:57:25,314 main.py:52] epoch 9903, training loss: 4960.23, average training loss: 5345.33, base loss: 15964.20
[INFO 2017-06-30 09:57:28,452 main.py:52] epoch 9904, training loss: 5204.18, average training loss: 5345.01, base loss: 15963.92
[INFO 2017-06-30 09:57:31,764 main.py:52] epoch 9905, training loss: 5493.04, average training loss: 5345.35, base loss: 15964.12
[INFO 2017-06-30 09:57:34,917 main.py:52] epoch 9906, training loss: 5171.29, average training loss: 5345.05, base loss: 15964.02
[INFO 2017-06-30 09:57:38,101 main.py:52] epoch 9907, training loss: 5199.50, average training loss: 5344.61, base loss: 15964.03
[INFO 2017-06-30 09:57:41,257 main.py:52] epoch 9908, training loss: 5459.68, average training loss: 5344.32, base loss: 15964.18
[INFO 2017-06-30 09:57:44,382 main.py:52] epoch 9909, training loss: 5131.82, average training loss: 5344.41, base loss: 15963.87
[INFO 2017-06-30 09:57:47,505 main.py:52] epoch 9910, training loss: 5267.34, average training loss: 5344.12, base loss: 15963.93
[INFO 2017-06-30 09:57:50,658 main.py:52] epoch 9911, training loss: 5333.63, average training loss: 5344.24, base loss: 15964.13
[INFO 2017-06-30 09:57:53,762 main.py:52] epoch 9912, training loss: 5293.32, average training loss: 5344.42, base loss: 15964.09
[INFO 2017-06-30 09:57:56,900 main.py:52] epoch 9913, training loss: 5542.99, average training loss: 5344.29, base loss: 15964.23
[INFO 2017-06-30 09:58:00,088 main.py:52] epoch 9914, training loss: 5298.70, average training loss: 5344.39, base loss: 15964.30
[INFO 2017-06-30 09:58:03,282 main.py:52] epoch 9915, training loss: 5539.29, average training loss: 5344.51, base loss: 15964.28
[INFO 2017-06-30 09:58:06,447 main.py:52] epoch 9916, training loss: 5311.37, average training loss: 5344.66, base loss: 15964.22
[INFO 2017-06-30 09:58:09,571 main.py:52] epoch 9917, training loss: 5177.93, average training loss: 5344.95, base loss: 15964.20
[INFO 2017-06-30 09:58:12,701 main.py:52] epoch 9918, training loss: 5629.31, average training loss: 5345.38, base loss: 15964.09
[INFO 2017-06-30 09:58:15,846 main.py:52] epoch 9919, training loss: 5094.29, average training loss: 5344.76, base loss: 15963.98
[INFO 2017-06-30 09:58:19,002 main.py:52] epoch 9920, training loss: 5401.78, average training loss: 5344.82, base loss: 15964.18
[INFO 2017-06-30 09:58:22,169 main.py:52] epoch 9921, training loss: 5271.66, average training loss: 5344.53, base loss: 15964.10
[INFO 2017-06-30 09:58:25,311 main.py:52] epoch 9922, training loss: 5070.44, average training loss: 5344.38, base loss: 15963.96
[INFO 2017-06-30 09:58:28,433 main.py:52] epoch 9923, training loss: 5416.44, average training loss: 5344.77, base loss: 15964.28
[INFO 2017-06-30 09:58:31,587 main.py:52] epoch 9924, training loss: 5470.42, average training loss: 5345.05, base loss: 15964.48
[INFO 2017-06-30 09:58:34,701 main.py:52] epoch 9925, training loss: 5452.53, average training loss: 5344.97, base loss: 15964.70
[INFO 2017-06-30 09:58:37,826 main.py:52] epoch 9926, training loss: 5632.57, average training loss: 5345.41, base loss: 15964.84
[INFO 2017-06-30 09:58:40,979 main.py:52] epoch 9927, training loss: 5327.96, average training loss: 5345.69, base loss: 15964.79
[INFO 2017-06-30 09:58:44,088 main.py:52] epoch 9928, training loss: 4946.61, average training loss: 5345.53, base loss: 15964.56
[INFO 2017-06-30 09:58:47,195 main.py:52] epoch 9929, training loss: 4987.10, average training loss: 5345.03, base loss: 15964.50
[INFO 2017-06-30 09:58:50,379 main.py:52] epoch 9930, training loss: 5181.98, average training loss: 5345.06, base loss: 15964.28
[INFO 2017-06-30 09:58:53,522 main.py:52] epoch 9931, training loss: 5047.80, average training loss: 5344.82, base loss: 15964.14
[INFO 2017-06-30 09:58:56,704 main.py:52] epoch 9932, training loss: 5122.14, average training loss: 5344.88, base loss: 15964.21
[INFO 2017-06-30 09:58:59,826 main.py:52] epoch 9933, training loss: 5428.34, average training loss: 5345.09, base loss: 15964.39
[INFO 2017-06-30 09:59:02,966 main.py:52] epoch 9934, training loss: 5514.46, average training loss: 5345.08, base loss: 15964.59
[INFO 2017-06-30 09:59:06,104 main.py:52] epoch 9935, training loss: 5451.12, average training loss: 5345.26, base loss: 15964.62
[INFO 2017-06-30 09:59:09,226 main.py:52] epoch 9936, training loss: 5205.92, average training loss: 5345.45, base loss: 15964.52
[INFO 2017-06-30 09:59:12,372 main.py:52] epoch 9937, training loss: 5250.85, average training loss: 5345.44, base loss: 15964.58
[INFO 2017-06-30 09:59:15,531 main.py:52] epoch 9938, training loss: 5213.83, average training loss: 5345.49, base loss: 15964.60
[INFO 2017-06-30 09:59:18,675 main.py:52] epoch 9939, training loss: 5191.45, average training loss: 5345.34, base loss: 15964.59
[INFO 2017-06-30 09:59:21,808 main.py:52] epoch 9940, training loss: 5299.58, average training loss: 5345.20, base loss: 15964.76
[INFO 2017-06-30 09:59:24,933 main.py:52] epoch 9941, training loss: 5305.49, average training loss: 5344.53, base loss: 15964.66
[INFO 2017-06-30 09:59:28,065 main.py:52] epoch 9942, training loss: 4938.97, average training loss: 5344.42, base loss: 15964.28
[INFO 2017-06-30 09:59:31,236 main.py:52] epoch 9943, training loss: 5338.03, average training loss: 5344.46, base loss: 15963.98
[INFO 2017-06-30 09:59:34,390 main.py:52] epoch 9944, training loss: 5394.79, average training loss: 5344.63, base loss: 15964.21
[INFO 2017-06-30 09:59:37,575 main.py:52] epoch 9945, training loss: 5630.70, average training loss: 5345.18, base loss: 15964.78
[INFO 2017-06-30 09:59:40,739 main.py:52] epoch 9946, training loss: 5100.56, average training loss: 5344.98, base loss: 15964.57
[INFO 2017-06-30 09:59:43,912 main.py:52] epoch 9947, training loss: 5187.39, average training loss: 5344.86, base loss: 15964.29
[INFO 2017-06-30 09:59:47,044 main.py:52] epoch 9948, training loss: 5251.19, average training loss: 5344.65, base loss: 15964.41
[INFO 2017-06-30 09:59:50,160 main.py:52] epoch 9949, training loss: 5589.29, average training loss: 5344.96, base loss: 15964.76
[INFO 2017-06-30 09:59:53,343 main.py:52] epoch 9950, training loss: 5025.74, average training loss: 5344.79, base loss: 15964.62
[INFO 2017-06-30 09:59:56,463 main.py:52] epoch 9951, training loss: 5232.24, average training loss: 5344.93, base loss: 15964.56
[INFO 2017-06-30 09:59:59,591 main.py:52] epoch 9952, training loss: 5298.76, average training loss: 5344.58, base loss: 15964.46
[INFO 2017-06-30 10:00:02,726 main.py:52] epoch 9953, training loss: 5747.38, average training loss: 5345.15, base loss: 15964.90
[INFO 2017-06-30 10:00:05,831 main.py:52] epoch 9954, training loss: 5404.29, average training loss: 5345.52, base loss: 15964.95
[INFO 2017-06-30 10:00:08,965 main.py:52] epoch 9955, training loss: 5492.86, average training loss: 5345.69, base loss: 15965.01
[INFO 2017-06-30 10:00:12,095 main.py:52] epoch 9956, training loss: 5373.23, average training loss: 5345.87, base loss: 15965.07
[INFO 2017-06-30 10:00:15,197 main.py:52] epoch 9957, training loss: 5556.48, average training loss: 5346.35, base loss: 15965.30
[INFO 2017-06-30 10:00:18,340 main.py:52] epoch 9958, training loss: 5494.76, average training loss: 5346.76, base loss: 15965.46
[INFO 2017-06-30 10:00:21,490 main.py:52] epoch 9959, training loss: 5722.40, average training loss: 5347.10, base loss: 15965.85
[INFO 2017-06-30 10:00:24,666 main.py:52] epoch 9960, training loss: 5286.56, average training loss: 5347.12, base loss: 15965.97
[INFO 2017-06-30 10:00:27,828 main.py:52] epoch 9961, training loss: 5524.07, average training loss: 5346.91, base loss: 15965.96
[INFO 2017-06-30 10:00:30,978 main.py:52] epoch 9962, training loss: 5399.91, average training loss: 5347.24, base loss: 15966.04
[INFO 2017-06-30 10:00:34,113 main.py:52] epoch 9963, training loss: 5359.81, average training loss: 5347.32, base loss: 15966.27
[INFO 2017-06-30 10:00:37,233 main.py:52] epoch 9964, training loss: 5244.00, average training loss: 5347.17, base loss: 15966.43
[INFO 2017-06-30 10:00:40,369 main.py:52] epoch 9965, training loss: 5316.35, average training loss: 5347.07, base loss: 15966.74
[INFO 2017-06-30 10:00:43,536 main.py:52] epoch 9966, training loss: 5234.66, average training loss: 5347.11, base loss: 15966.72
[INFO 2017-06-30 10:00:46,659 main.py:52] epoch 9967, training loss: 5061.26, average training loss: 5346.70, base loss: 15966.52
[INFO 2017-06-30 10:00:49,787 main.py:52] epoch 9968, training loss: 5101.85, average training loss: 5346.12, base loss: 15966.56
[INFO 2017-06-30 10:00:52,923 main.py:52] epoch 9969, training loss: 5472.22, average training loss: 5346.37, base loss: 15966.85
[INFO 2017-06-30 10:00:56,059 main.py:52] epoch 9970, training loss: 5189.15, average training loss: 5346.03, base loss: 15966.82
[INFO 2017-06-30 10:00:59,236 main.py:52] epoch 9971, training loss: 5337.63, average training loss: 5345.87, base loss: 15966.74
[INFO 2017-06-30 10:01:02,352 main.py:52] epoch 9972, training loss: 5271.53, average training loss: 5346.14, base loss: 15966.54
[INFO 2017-06-30 10:01:05,468 main.py:52] epoch 9973, training loss: 5145.71, average training loss: 5345.62, base loss: 15966.15
[INFO 2017-06-30 10:01:08,628 main.py:52] epoch 9974, training loss: 5209.44, average training loss: 5345.42, base loss: 15966.02
[INFO 2017-06-30 10:01:11,727 main.py:52] epoch 9975, training loss: 5646.24, average training loss: 5345.78, base loss: 15966.10
[INFO 2017-06-30 10:01:14,857 main.py:52] epoch 9976, training loss: 5386.78, average training loss: 5345.62, base loss: 15966.24
[INFO 2017-06-30 10:01:17,936 main.py:52] epoch 9977, training loss: 5065.71, average training loss: 5345.69, base loss: 15966.48
[INFO 2017-06-30 10:01:21,111 main.py:52] epoch 9978, training loss: 5130.24, average training loss: 5345.58, base loss: 15966.47
[INFO 2017-06-30 10:01:24,260 main.py:52] epoch 9979, training loss: 5266.39, average training loss: 5345.30, base loss: 15966.47
[INFO 2017-06-30 10:01:27,431 main.py:52] epoch 9980, training loss: 5323.56, average training loss: 5345.52, base loss: 15966.30
[INFO 2017-06-30 10:01:30,558 main.py:52] epoch 9981, training loss: 5269.24, average training loss: 5345.78, base loss: 15966.32
[INFO 2017-06-30 10:01:33,716 main.py:52] epoch 9982, training loss: 5036.63, average training loss: 5345.64, base loss: 15966.29
[INFO 2017-06-30 10:01:36,875 main.py:52] epoch 9983, training loss: 5560.69, average training loss: 5345.26, base loss: 15966.77
[INFO 2017-06-30 10:01:40,015 main.py:52] epoch 9984, training loss: 5565.59, average training loss: 5345.73, base loss: 15967.16
[INFO 2017-06-30 10:01:43,133 main.py:52] epoch 9985, training loss: 5607.46, average training loss: 5346.41, base loss: 15967.29
[INFO 2017-06-30 10:01:46,261 main.py:52] epoch 9986, training loss: 5479.15, average training loss: 5346.69, base loss: 15967.35
[INFO 2017-06-30 10:01:49,407 main.py:52] epoch 9987, training loss: 5638.08, average training loss: 5347.37, base loss: 15967.63
[INFO 2017-06-30 10:01:52,550 main.py:52] epoch 9988, training loss: 5615.22, average training loss: 5347.85, base loss: 15967.89
[INFO 2017-06-30 10:01:55,657 main.py:52] epoch 9989, training loss: 5522.06, average training loss: 5348.40, base loss: 15968.20
[INFO 2017-06-30 10:01:58,797 main.py:52] epoch 9990, training loss: 5203.02, average training loss: 5348.55, base loss: 15968.19
[INFO 2017-06-30 10:02:01,934 main.py:52] epoch 9991, training loss: 4904.61, average training loss: 5348.34, base loss: 15968.10
[INFO 2017-06-30 10:02:05,111 main.py:52] epoch 9992, training loss: 4731.26, average training loss: 5347.71, base loss: 15967.78
[INFO 2017-06-30 10:02:08,283 main.py:52] epoch 9993, training loss: 5023.85, average training loss: 5347.39, base loss: 15967.78
[INFO 2017-06-30 10:02:11,428 main.py:52] epoch 9994, training loss: 6014.39, average training loss: 5348.27, base loss: 15968.14
[INFO 2017-06-30 10:02:14,542 main.py:52] epoch 9995, training loss: 5513.90, average training loss: 5348.24, base loss: 15968.61
[INFO 2017-06-30 10:02:17,662 main.py:52] epoch 9996, training loss: 5493.28, average training loss: 5348.62, base loss: 15968.72
[INFO 2017-06-30 10:02:20,811 main.py:52] epoch 9997, training loss: 5624.93, average training loss: 5348.58, base loss: 15968.60
[INFO 2017-06-30 10:02:23,995 main.py:52] epoch 9998, training loss: 5224.74, average training loss: 5348.39, base loss: 15968.38
[INFO 2017-06-30 10:02:27,118 main.py:52] epoch 9999, training loss: 5301.87, average training loss: 5348.58, base loss: 15968.31
[INFO 2017-06-30 10:02:27,118 main.py:54] epoch 9999, testing
[INFO 2017-06-30 10:02:40,170 main.py:97] average testing loss: 5421.20, base loss: 16311.69
[INFO 2017-06-30 10:02:40,170 main.py:98] improve_loss: 10890.49, improve_percent: 0.67
[INFO 2017-06-30 10:02:40,172 main.py:66] current best improved percent: 0.67
[INFO 2017-06-30 10:02:43,309 main.py:52] epoch 10000, training loss: 5293.82, average training loss: 5348.41, base loss: 15968.29
[INFO 2017-06-30 10:02:46,428 main.py:52] epoch 10001, training loss: 5391.47, average training loss: 5348.29, base loss: 15968.34
[INFO 2017-06-30 10:02:49,530 main.py:52] epoch 10002, training loss: 5045.68, average training loss: 5348.18, base loss: 15968.23
[INFO 2017-06-30 10:02:52,669 main.py:52] epoch 10003, training loss: 5491.02, average training loss: 5348.43, base loss: 15968.10
[INFO 2017-06-30 10:02:55,802 main.py:52] epoch 10004, training loss: 5694.04, average training loss: 5348.94, base loss: 15968.57
[INFO 2017-06-30 10:02:58,945 main.py:52] epoch 10005, training loss: 5350.73, average training loss: 5349.13, base loss: 15968.70
[INFO 2017-06-30 10:03:02,086 main.py:52] epoch 10006, training loss: 5294.92, average training loss: 5349.01, base loss: 15968.54
[INFO 2017-06-30 10:03:05,193 main.py:52] epoch 10007, training loss: 5186.70, average training loss: 5348.58, base loss: 15968.10
[INFO 2017-06-30 10:03:08,337 main.py:52] epoch 10008, training loss: 5779.35, average training loss: 5348.95, base loss: 15968.61
[INFO 2017-06-30 10:03:11,503 main.py:52] epoch 10009, training loss: 5378.39, average training loss: 5348.43, base loss: 15969.00
[INFO 2017-06-30 10:03:14,630 main.py:52] epoch 10010, training loss: 5741.28, average training loss: 5348.90, base loss: 15969.34
[INFO 2017-06-30 10:03:17,738 main.py:52] epoch 10011, training loss: 5439.02, average training loss: 5349.03, base loss: 15969.61
[INFO 2017-06-30 10:03:20,882 main.py:52] epoch 10012, training loss: 5102.50, average training loss: 5348.97, base loss: 15969.44
[INFO 2017-06-30 10:03:24,049 main.py:52] epoch 10013, training loss: 5362.54, average training loss: 5349.01, base loss: 15969.58
[INFO 2017-06-30 10:03:27,240 main.py:52] epoch 10014, training loss: 4923.26, average training loss: 5348.60, base loss: 15969.33
[INFO 2017-06-30 10:03:30,469 main.py:52] epoch 10015, training loss: 5493.28, average training loss: 5348.84, base loss: 15969.52
[INFO 2017-06-30 10:03:33,637 main.py:52] epoch 10016, training loss: 5246.91, average training loss: 5348.60, base loss: 15969.44
[INFO 2017-06-30 10:03:36,817 main.py:52] epoch 10017, training loss: 5501.55, average training loss: 5348.91, base loss: 15969.52
[INFO 2017-06-30 10:03:39,940 main.py:52] epoch 10018, training loss: 5454.84, average training loss: 5349.01, base loss: 15969.29
[INFO 2017-06-30 10:03:43,081 main.py:52] epoch 10019, training loss: 5318.53, average training loss: 5348.71, base loss: 15968.83
[INFO 2017-06-30 10:03:46,208 main.py:52] epoch 10020, training loss: 5662.41, average training loss: 5348.77, base loss: 15969.12
[INFO 2017-06-30 10:03:49,393 main.py:52] epoch 10021, training loss: 5756.14, average training loss: 5349.15, base loss: 15969.37
[INFO 2017-06-30 10:03:52,562 main.py:52] epoch 10022, training loss: 5419.57, average training loss: 5349.36, base loss: 15969.57
[INFO 2017-06-30 10:03:55,720 main.py:52] epoch 10023, training loss: 4992.83, average training loss: 5349.03, base loss: 15969.53
[INFO 2017-06-30 10:03:58,880 main.py:52] epoch 10024, training loss: 5330.72, average training loss: 5348.86, base loss: 15969.49
[INFO 2017-06-30 10:04:02,017 main.py:52] epoch 10025, training loss: 5440.38, average training loss: 5349.05, base loss: 15969.67
[INFO 2017-06-30 10:04:05,163 main.py:52] epoch 10026, training loss: 5362.38, average training loss: 5349.24, base loss: 15970.00
[INFO 2017-06-30 10:04:08,317 main.py:52] epoch 10027, training loss: 5360.34, average training loss: 5349.67, base loss: 15969.88
[INFO 2017-06-30 10:04:11,470 main.py:52] epoch 10028, training loss: 5386.28, average training loss: 5349.64, base loss: 15969.88
[INFO 2017-06-30 10:04:14,643 main.py:52] epoch 10029, training loss: 5215.41, average training loss: 5349.20, base loss: 15969.88
[INFO 2017-06-30 10:04:17,831 main.py:52] epoch 10030, training loss: 4974.61, average training loss: 5349.03, base loss: 15969.73
[INFO 2017-06-30 10:04:20,990 main.py:52] epoch 10031, training loss: 5241.75, average training loss: 5348.55, base loss: 15969.81
[INFO 2017-06-30 10:04:24,120 main.py:52] epoch 10032, training loss: 5270.45, average training loss: 5348.53, base loss: 15969.82
[INFO 2017-06-30 10:04:27,270 main.py:52] epoch 10033, training loss: 5260.30, average training loss: 5348.44, base loss: 15969.72
[INFO 2017-06-30 10:04:30,416 main.py:52] epoch 10034, training loss: 5027.04, average training loss: 5348.11, base loss: 15969.51
[INFO 2017-06-30 10:04:33,608 main.py:52] epoch 10035, training loss: 5267.02, average training loss: 5348.43, base loss: 15969.46
[INFO 2017-06-30 10:04:36,768 main.py:52] epoch 10036, training loss: 5023.17, average training loss: 5348.38, base loss: 15969.26
[INFO 2017-06-30 10:04:39,872 main.py:52] epoch 10037, training loss: 5262.97, average training loss: 5347.78, base loss: 15969.44
[INFO 2017-06-30 10:04:42,997 main.py:52] epoch 10038, training loss: 5336.71, average training loss: 5347.74, base loss: 15969.66
[INFO 2017-06-30 10:04:46,149 main.py:52] epoch 10039, training loss: 5570.79, average training loss: 5348.00, base loss: 15969.61
[INFO 2017-06-30 10:04:49,349 main.py:52] epoch 10040, training loss: 5452.40, average training loss: 5347.97, base loss: 15970.10
[INFO 2017-06-30 10:04:52,463 main.py:52] epoch 10041, training loss: 5650.41, average training loss: 5348.29, base loss: 15970.18
[INFO 2017-06-30 10:04:55,646 main.py:52] epoch 10042, training loss: 4946.47, average training loss: 5347.70, base loss: 15969.76
[INFO 2017-06-30 10:04:58,793 main.py:52] epoch 10043, training loss: 5282.76, average training loss: 5347.47, base loss: 15969.56
[INFO 2017-06-30 10:05:01,921 main.py:52] epoch 10044, training loss: 5295.88, average training loss: 5347.34, base loss: 15969.54
[INFO 2017-06-30 10:05:05,071 main.py:52] epoch 10045, training loss: 5280.86, average training loss: 5347.46, base loss: 15969.51
[INFO 2017-06-30 10:05:08,239 main.py:52] epoch 10046, training loss: 5062.55, average training loss: 5346.95, base loss: 15969.48
[INFO 2017-06-30 10:05:11,383 main.py:52] epoch 10047, training loss: 5185.99, average training loss: 5346.64, base loss: 15969.30
[INFO 2017-06-30 10:05:14,512 main.py:52] epoch 10048, training loss: 5385.72, average training loss: 5346.86, base loss: 15969.24
[INFO 2017-06-30 10:05:17,659 main.py:52] epoch 10049, training loss: 5171.38, average training loss: 5346.98, base loss: 15969.16
[INFO 2017-06-30 10:05:20,819 main.py:52] epoch 10050, training loss: 5376.35, average training loss: 5347.10, base loss: 15969.49
[INFO 2017-06-30 10:05:23,948 main.py:52] epoch 10051, training loss: 4975.48, average training loss: 5346.70, base loss: 15969.41
[INFO 2017-06-30 10:05:27,059 main.py:52] epoch 10052, training loss: 5343.89, average training loss: 5346.47, base loss: 15969.31
[INFO 2017-06-30 10:05:30,299 main.py:52] epoch 10053, training loss: 5062.76, average training loss: 5345.67, base loss: 15969.21
[INFO 2017-06-30 10:05:33,418 main.py:52] epoch 10054, training loss: 5516.45, average training loss: 5345.81, base loss: 15969.19
[INFO 2017-06-30 10:05:36,605 main.py:52] epoch 10055, training loss: 5326.54, average training loss: 5345.59, base loss: 15969.50
[INFO 2017-06-30 10:05:39,748 main.py:52] epoch 10056, training loss: 5563.47, average training loss: 5345.67, base loss: 15969.50
[INFO 2017-06-30 10:05:42,866 main.py:52] epoch 10057, training loss: 5197.17, average training loss: 5345.63, base loss: 15969.52
[INFO 2017-06-30 10:05:45,981 main.py:52] epoch 10058, training loss: 5562.89, average training loss: 5345.76, base loss: 15969.62
[INFO 2017-06-30 10:05:49,113 main.py:52] epoch 10059, training loss: 4954.13, average training loss: 5345.26, base loss: 15969.52
[INFO 2017-06-30 10:05:52,284 main.py:52] epoch 10060, training loss: 5951.15, average training loss: 5345.68, base loss: 15969.91
[INFO 2017-06-30 10:05:55,447 main.py:52] epoch 10061, training loss: 5754.75, average training loss: 5346.23, base loss: 15970.53
[INFO 2017-06-30 10:05:58,657 main.py:52] epoch 10062, training loss: 5420.17, average training loss: 5345.89, base loss: 15970.73
[INFO 2017-06-30 10:06:01,789 main.py:52] epoch 10063, training loss: 5699.71, average training loss: 5346.17, base loss: 15970.84
[INFO 2017-06-30 10:06:04,926 main.py:52] epoch 10064, training loss: 5451.94, average training loss: 5346.07, base loss: 15970.92
[INFO 2017-06-30 10:06:08,043 main.py:52] epoch 10065, training loss: 5351.39, average training loss: 5346.14, base loss: 15971.03
[INFO 2017-06-30 10:06:11,198 main.py:52] epoch 10066, training loss: 4966.76, average training loss: 5345.59, base loss: 15970.75
[INFO 2017-06-30 10:06:14,331 main.py:52] epoch 10067, training loss: 5170.40, average training loss: 5345.69, base loss: 15970.58
[INFO 2017-06-30 10:06:17,443 main.py:52] epoch 10068, training loss: 5174.26, average training loss: 5345.51, base loss: 15970.29
[INFO 2017-06-30 10:06:20,594 main.py:52] epoch 10069, training loss: 5164.92, average training loss: 5345.15, base loss: 15970.08
[INFO 2017-06-30 10:06:23,719 main.py:52] epoch 10070, training loss: 5464.47, average training loss: 5345.52, base loss: 15970.20
[INFO 2017-06-30 10:06:26,885 main.py:52] epoch 10071, training loss: 5154.41, average training loss: 5345.11, base loss: 15970.12
[INFO 2017-06-30 10:06:30,101 main.py:52] epoch 10072, training loss: 5506.62, average training loss: 5345.06, base loss: 15970.51
[INFO 2017-06-30 10:06:33,239 main.py:52] epoch 10073, training loss: 5316.67, average training loss: 5345.04, base loss: 15970.51
[INFO 2017-06-30 10:06:36,415 main.py:52] epoch 10074, training loss: 5419.31, average training loss: 5345.38, base loss: 15970.71
[INFO 2017-06-30 10:06:39,553 main.py:52] epoch 10075, training loss: 5674.82, average training loss: 5345.52, base loss: 15970.97
[INFO 2017-06-30 10:06:42,675 main.py:52] epoch 10076, training loss: 5590.36, average training loss: 5346.03, base loss: 15970.76
[INFO 2017-06-30 10:06:45,832 main.py:52] epoch 10077, training loss: 5078.44, average training loss: 5345.41, base loss: 15970.42
[INFO 2017-06-30 10:06:48,989 main.py:52] epoch 10078, training loss: 5417.91, average training loss: 5345.57, base loss: 15970.53
[INFO 2017-06-30 10:06:52,147 main.py:52] epoch 10079, training loss: 5390.29, average training loss: 5345.72, base loss: 15970.63
[INFO 2017-06-30 10:06:55,283 main.py:52] epoch 10080, training loss: 5460.46, average training loss: 5345.64, base loss: 15970.73
[INFO 2017-06-30 10:06:58,386 main.py:52] epoch 10081, training loss: 5429.76, average training loss: 5345.77, base loss: 15970.94
[INFO 2017-06-30 10:07:01,505 main.py:52] epoch 10082, training loss: 5436.78, average training loss: 5345.93, base loss: 15971.08
[INFO 2017-06-30 10:07:04,629 main.py:52] epoch 10083, training loss: 5129.09, average training loss: 5345.75, base loss: 15970.89
[INFO 2017-06-30 10:07:07,743 main.py:52] epoch 10084, training loss: 5240.99, average training loss: 5345.80, base loss: 15970.88
[INFO 2017-06-30 10:07:10,917 main.py:52] epoch 10085, training loss: 5213.72, average training loss: 5345.91, base loss: 15970.75
[INFO 2017-06-30 10:07:14,059 main.py:52] epoch 10086, training loss: 5575.47, average training loss: 5345.81, base loss: 15971.21
[INFO 2017-06-30 10:07:17,205 main.py:52] epoch 10087, training loss: 5520.52, average training loss: 5345.72, base loss: 15971.22
[INFO 2017-06-30 10:07:20,346 main.py:52] epoch 10088, training loss: 5417.73, average training loss: 5345.49, base loss: 15971.46
[INFO 2017-06-30 10:07:23,471 main.py:52] epoch 10089, training loss: 5442.47, average training loss: 5345.92, base loss: 15971.77
[INFO 2017-06-30 10:07:26,555 main.py:52] epoch 10090, training loss: 5263.00, average training loss: 5345.82, base loss: 15971.58
[INFO 2017-06-30 10:07:29,682 main.py:52] epoch 10091, training loss: 5329.09, average training loss: 5345.56, base loss: 15971.67
[INFO 2017-06-30 10:07:32,838 main.py:52] epoch 10092, training loss: 5604.56, average training loss: 5345.67, base loss: 15972.09
[INFO 2017-06-30 10:07:35,953 main.py:52] epoch 10093, training loss: 5357.00, average training loss: 5345.47, base loss: 15972.14
[INFO 2017-06-30 10:07:39,111 main.py:52] epoch 10094, training loss: 5545.42, average training loss: 5345.49, base loss: 15972.22
[INFO 2017-06-30 10:07:42,263 main.py:52] epoch 10095, training loss: 4919.64, average training loss: 5344.85, base loss: 15971.85
[INFO 2017-06-30 10:07:45,390 main.py:52] epoch 10096, training loss: 5302.04, average training loss: 5344.88, base loss: 15971.85
[INFO 2017-06-30 10:07:48,561 main.py:52] epoch 10097, training loss: 5194.35, average training loss: 5344.45, base loss: 15971.95
[INFO 2017-06-30 10:07:51,651 main.py:52] epoch 10098, training loss: 5312.08, average training loss: 5343.86, base loss: 15972.02
[INFO 2017-06-30 10:07:54,799 main.py:52] epoch 10099, training loss: 5282.13, average training loss: 5343.76, base loss: 15972.13
[INFO 2017-06-30 10:07:54,799 main.py:54] epoch 10099, testing
[INFO 2017-06-30 10:08:07,871 main.py:97] average testing loss: 5498.40, base loss: 16818.17
[INFO 2017-06-30 10:08:07,871 main.py:98] improve_loss: 11319.76, improve_percent: 0.67
[INFO 2017-06-30 10:08:07,874 main.py:62] model save to ./model/final.pth
[INFO 2017-06-30 10:08:07,909 main.py:66] current best improved percent: 0.67
[INFO 2017-06-30 10:08:11,061 main.py:52] epoch 10100, training loss: 5349.92, average training loss: 5343.76, base loss: 15971.98
[INFO 2017-06-30 10:08:14,211 main.py:52] epoch 10101, training loss: 5877.46, average training loss: 5344.37, base loss: 15972.35
[INFO 2017-06-30 10:08:17,350 main.py:52] epoch 10102, training loss: 5346.94, average training loss: 5344.59, base loss: 15972.10
[INFO 2017-06-30 10:08:20,501 main.py:52] epoch 10103, training loss: 5239.56, average training loss: 5344.09, base loss: 15971.90
[INFO 2017-06-30 10:08:23,627 main.py:52] epoch 10104, training loss: 5221.65, average training loss: 5343.75, base loss: 15971.67
[INFO 2017-06-30 10:08:26,768 main.py:52] epoch 10105, training loss: 5165.34, average training loss: 5343.77, base loss: 15971.34
[INFO 2017-06-30 10:08:29,923 main.py:52] epoch 10106, training loss: 5605.35, average training loss: 5343.81, base loss: 15971.58
[INFO 2017-06-30 10:08:33,070 main.py:52] epoch 10107, training loss: 5276.13, average training loss: 5343.91, base loss: 15971.51
[INFO 2017-06-30 10:08:36,234 main.py:52] epoch 10108, training loss: 5256.06, average training loss: 5343.88, base loss: 15971.32
[INFO 2017-06-30 10:08:39,387 main.py:52] epoch 10109, training loss: 5297.77, average training loss: 5343.82, base loss: 15971.11
[INFO 2017-06-30 10:08:42,540 main.py:52] epoch 10110, training loss: 5439.84, average training loss: 5343.91, base loss: 15970.98
[INFO 2017-06-30 10:08:45,652 main.py:52] epoch 10111, training loss: 5312.49, average training loss: 5343.21, base loss: 15970.60
[INFO 2017-06-30 10:08:48,794 main.py:52] epoch 10112, training loss: 5446.17, average training loss: 5343.11, base loss: 15970.67
[INFO 2017-06-30 10:08:51,918 main.py:52] epoch 10113, training loss: 5377.82, average training loss: 5343.62, base loss: 15970.82
[INFO 2017-06-30 10:08:55,053 main.py:52] epoch 10114, training loss: 5634.45, average training loss: 5343.46, base loss: 15971.18
[INFO 2017-06-30 10:08:58,195 main.py:52] epoch 10115, training loss: 5085.50, average training loss: 5343.38, base loss: 15971.35
[INFO 2017-06-30 10:09:01,344 main.py:52] epoch 10116, training loss: 5162.95, average training loss: 5342.94, base loss: 15971.53
[INFO 2017-06-30 10:09:04,514 main.py:52] epoch 10117, training loss: 5758.88, average training loss: 5343.19, base loss: 15971.80
[INFO 2017-06-30 10:09:07,648 main.py:52] epoch 10118, training loss: 5170.49, average training loss: 5342.77, base loss: 15971.56
[INFO 2017-06-30 10:09:10,800 main.py:52] epoch 10119, training loss: 5326.91, average training loss: 5342.88, base loss: 15971.38
[INFO 2017-06-30 10:09:13,923 main.py:52] epoch 10120, training loss: 5189.32, average training loss: 5343.16, base loss: 15971.27
[INFO 2017-06-30 10:09:17,103 main.py:52] epoch 10121, training loss: 5780.43, average training loss: 5343.76, base loss: 15971.62
[INFO 2017-06-30 10:09:20,245 main.py:52] epoch 10122, training loss: 4903.93, average training loss: 5343.27, base loss: 15971.24
[INFO 2017-06-30 10:09:23,392 main.py:52] epoch 10123, training loss: 5125.87, average training loss: 5342.82, base loss: 15971.17
[INFO 2017-06-30 10:09:26,535 main.py:52] epoch 10124, training loss: 5173.34, average training loss: 5342.55, base loss: 15970.90
[INFO 2017-06-30 10:09:29,675 main.py:52] epoch 10125, training loss: 5522.04, average training loss: 5342.74, base loss: 15971.18
[INFO 2017-06-30 10:09:32,771 main.py:52] epoch 10126, training loss: 5406.86, average training loss: 5343.07, base loss: 15971.09
[INFO 2017-06-30 10:09:35,911 main.py:52] epoch 10127, training loss: 5522.44, average training loss: 5343.34, base loss: 15971.37
[INFO 2017-06-30 10:09:39,035 main.py:52] epoch 10128, training loss: 5454.62, average training loss: 5343.33, base loss: 15971.57
[INFO 2017-06-30 10:09:42,181 main.py:52] epoch 10129, training loss: 5126.90, average training loss: 5343.19, base loss: 15971.63
[INFO 2017-06-30 10:09:45,332 main.py:52] epoch 10130, training loss: 5325.38, average training loss: 5342.97, base loss: 15971.88
[INFO 2017-06-30 10:09:48,489 main.py:52] epoch 10131, training loss: 5007.79, average training loss: 5342.71, base loss: 15971.69
[INFO 2017-06-30 10:09:51,645 main.py:52] epoch 10132, training loss: 5301.80, average training loss: 5342.07, base loss: 15971.40
[INFO 2017-06-30 10:09:54,769 main.py:52] epoch 10133, training loss: 5223.37, average training loss: 5341.75, base loss: 15971.11
[INFO 2017-06-30 10:09:57,867 main.py:52] epoch 10134, training loss: 5875.33, average training loss: 5342.31, base loss: 15971.33
[INFO 2017-06-30 10:10:00,994 main.py:52] epoch 10135, training loss: 5153.33, average training loss: 5341.64, base loss: 15971.03
[INFO 2017-06-30 10:10:04,109 main.py:52] epoch 10136, training loss: 5383.95, average training loss: 5341.69, base loss: 15971.25
[INFO 2017-06-30 10:10:07,222 main.py:52] epoch 10137, training loss: 5160.73, average training loss: 5341.16, base loss: 15971.14
[INFO 2017-06-30 10:10:10,369 main.py:52] epoch 10138, training loss: 5661.22, average training loss: 5341.72, base loss: 15971.19
[INFO 2017-06-30 10:10:13,487 main.py:52] epoch 10139, training loss: 5084.06, average training loss: 5341.32, base loss: 15970.92
[INFO 2017-06-30 10:10:16,633 main.py:52] epoch 10140, training loss: 5112.92, average training loss: 5341.21, base loss: 15970.97
[INFO 2017-06-30 10:10:19,758 main.py:52] epoch 10141, training loss: 5203.62, average training loss: 5341.30, base loss: 15971.00
[INFO 2017-06-30 10:10:22,897 main.py:52] epoch 10142, training loss: 5167.94, average training loss: 5341.08, base loss: 15970.89
[INFO 2017-06-30 10:10:26,107 main.py:52] epoch 10143, training loss: 5395.70, average training loss: 5341.03, base loss: 15970.95
[INFO 2017-06-30 10:10:29,255 main.py:52] epoch 10144, training loss: 5310.60, average training loss: 5341.23, base loss: 15971.12
[INFO 2017-06-30 10:10:32,400 main.py:52] epoch 10145, training loss: 5484.60, average training loss: 5341.16, base loss: 15971.07
[INFO 2017-06-30 10:10:35,507 main.py:52] epoch 10146, training loss: 5298.91, average training loss: 5340.64, base loss: 15971.09
[INFO 2017-06-30 10:10:38,658 main.py:52] epoch 10147, training loss: 5052.79, average training loss: 5340.19, base loss: 15970.98
[INFO 2017-06-30 10:10:41,749 main.py:52] epoch 10148, training loss: 4939.44, average training loss: 5339.96, base loss: 15970.58
[INFO 2017-06-30 10:10:44,834 main.py:52] epoch 10149, training loss: 5121.49, average training loss: 5339.81, base loss: 15970.54
[INFO 2017-06-30 10:10:47,993 main.py:52] epoch 10150, training loss: 5335.58, average training loss: 5339.61, base loss: 15970.28
[INFO 2017-06-30 10:10:51,167 main.py:52] epoch 10151, training loss: 4870.12, average training loss: 5339.15, base loss: 15969.69
[INFO 2017-06-30 10:10:54,284 main.py:52] epoch 10152, training loss: 5102.68, average training loss: 5338.96, base loss: 15969.27
[INFO 2017-06-30 10:10:57,419 main.py:52] epoch 10153, training loss: 5302.11, average training loss: 5338.83, base loss: 15969.05
[INFO 2017-06-30 10:11:00,546 main.py:52] epoch 10154, training loss: 5278.33, average training loss: 5338.51, base loss: 15968.98
[INFO 2017-06-30 10:11:03,664 main.py:52] epoch 10155, training loss: 5267.24, average training loss: 5338.30, base loss: 15968.70
[INFO 2017-06-30 10:11:06,789 main.py:52] epoch 10156, training loss: 5284.41, average training loss: 5338.13, base loss: 15968.68
[INFO 2017-06-30 10:11:09,961 main.py:52] epoch 10157, training loss: 5456.52, average training loss: 5338.01, base loss: 15968.88
[INFO 2017-06-30 10:11:13,124 main.py:52] epoch 10158, training loss: 5407.59, average training loss: 5338.08, base loss: 15968.71
[INFO 2017-06-30 10:11:16,343 main.py:52] epoch 10159, training loss: 5502.29, average training loss: 5338.04, base loss: 15968.43
[INFO 2017-06-30 10:11:19,484 main.py:52] epoch 10160, training loss: 5435.92, average training loss: 5338.44, base loss: 15968.58
[INFO 2017-06-30 10:11:22,631 main.py:52] epoch 10161, training loss: 5258.20, average training loss: 5338.20, base loss: 15968.56
[INFO 2017-06-30 10:11:25,729 main.py:52] epoch 10162, training loss: 5446.79, average training loss: 5338.41, base loss: 15968.49
[INFO 2017-06-30 10:11:28,856 main.py:52] epoch 10163, training loss: 5165.77, average training loss: 5338.50, base loss: 15968.09
[INFO 2017-06-30 10:11:32,018 main.py:52] epoch 10164, training loss: 5171.35, average training loss: 5338.44, base loss: 15967.87
[INFO 2017-06-30 10:11:35,147 main.py:52] epoch 10165, training loss: 5477.84, average training loss: 5338.38, base loss: 15968.09
[INFO 2017-06-30 10:11:38,299 main.py:52] epoch 10166, training loss: 5213.06, average training loss: 5338.53, base loss: 15967.82
[INFO 2017-06-30 10:11:41,423 main.py:52] epoch 10167, training loss: 5232.64, average training loss: 5338.32, base loss: 15967.50
[INFO 2017-06-30 10:11:44,534 main.py:52] epoch 10168, training loss: 5181.74, average training loss: 5338.22, base loss: 15967.34
[INFO 2017-06-30 10:11:47,696 main.py:52] epoch 10169, training loss: 5416.66, average training loss: 5338.50, base loss: 15967.26
[INFO 2017-06-30 10:11:50,826 main.py:52] epoch 10170, training loss: 5558.27, average training loss: 5338.22, base loss: 15967.46
[INFO 2017-06-30 10:11:53,950 main.py:52] epoch 10171, training loss: 5242.31, average training loss: 5337.97, base loss: 15967.50
[INFO 2017-06-30 10:11:57,069 main.py:52] epoch 10172, training loss: 5048.78, average training loss: 5337.95, base loss: 15967.22
[INFO 2017-06-30 10:12:00,218 main.py:52] epoch 10173, training loss: 5412.11, average training loss: 5338.09, base loss: 15967.30
[INFO 2017-06-30 10:12:03,385 main.py:52] epoch 10174, training loss: 5549.90, average training loss: 5338.40, base loss: 15967.25
[INFO 2017-06-30 10:12:06,492 main.py:52] epoch 10175, training loss: 5640.70, average training loss: 5338.70, base loss: 15967.48
[INFO 2017-06-30 10:12:09,635 main.py:52] epoch 10176, training loss: 5360.50, average training loss: 5338.31, base loss: 15967.36
[INFO 2017-06-30 10:12:12,766 main.py:52] epoch 10177, training loss: 5003.17, average training loss: 5337.73, base loss: 15966.83
[INFO 2017-06-30 10:12:15,901 main.py:52] epoch 10178, training loss: 5559.59, average training loss: 5337.77, base loss: 15967.40
[INFO 2017-06-30 10:12:19,067 main.py:52] epoch 10179, training loss: 4896.98, average training loss: 5337.15, base loss: 15967.34
[INFO 2017-06-30 10:12:22,188 main.py:52] epoch 10180, training loss: 5051.10, average training loss: 5337.01, base loss: 15967.15
[INFO 2017-06-30 10:12:25,338 main.py:52] epoch 10181, training loss: 5361.60, average training loss: 5336.85, base loss: 15967.06
[INFO 2017-06-30 10:12:28,475 main.py:52] epoch 10182, training loss: 5180.18, average training loss: 5336.88, base loss: 15966.85
[INFO 2017-06-30 10:12:31,577 main.py:52] epoch 10183, training loss: 5260.91, average training loss: 5336.87, base loss: 15966.66
[INFO 2017-06-30 10:12:34,696 main.py:52] epoch 10184, training loss: 5142.92, average training loss: 5336.58, base loss: 15966.71
[INFO 2017-06-30 10:12:37,827 main.py:52] epoch 10185, training loss: 5026.74, average training loss: 5336.19, base loss: 15966.69
[INFO 2017-06-30 10:12:40,960 main.py:52] epoch 10186, training loss: 5641.50, average training loss: 5336.33, base loss: 15967.16
[INFO 2017-06-30 10:12:44,119 main.py:52] epoch 10187, training loss: 5437.41, average training loss: 5336.53, base loss: 15967.36
[INFO 2017-06-30 10:12:47,283 main.py:52] epoch 10188, training loss: 5365.26, average training loss: 5336.36, base loss: 15967.35
[INFO 2017-06-30 10:12:50,431 main.py:52] epoch 10189, training loss: 5651.79, average training loss: 5336.93, base loss: 15967.17
[INFO 2017-06-30 10:12:53,568 main.py:52] epoch 10190, training loss: 5516.91, average training loss: 5336.96, base loss: 15967.51
[INFO 2017-06-30 10:12:56,709 main.py:52] epoch 10191, training loss: 5177.66, average training loss: 5336.65, base loss: 15967.36
[INFO 2017-06-30 10:12:59,837 main.py:52] epoch 10192, training loss: 5088.56, average training loss: 5336.02, base loss: 15967.32
[INFO 2017-06-30 10:13:02,990 main.py:52] epoch 10193, training loss: 5621.57, average training loss: 5336.04, base loss: 15967.71
[INFO 2017-06-30 10:13:06,079 main.py:52] epoch 10194, training loss: 5734.71, average training loss: 5336.69, base loss: 15968.52
[INFO 2017-06-30 10:13:09,233 main.py:52] epoch 10195, training loss: 5346.86, average training loss: 5337.44, base loss: 15968.86
[INFO 2017-06-30 10:13:12,372 main.py:52] epoch 10196, training loss: 5705.23, average training loss: 5337.64, base loss: 15969.07
[INFO 2017-06-30 10:13:15,493 main.py:52] epoch 10197, training loss: 5394.33, average training loss: 5337.66, base loss: 15969.13
[INFO 2017-06-30 10:13:18,620 main.py:52] epoch 10198, training loss: 5673.57, average training loss: 5338.20, base loss: 15969.30
[INFO 2017-06-30 10:13:21,761 main.py:52] epoch 10199, training loss: 5424.40, average training loss: 5338.10, base loss: 15969.58
[INFO 2017-06-30 10:13:21,762 main.py:54] epoch 10199, testing
[INFO 2017-06-30 10:13:34,907 main.py:97] average testing loss: 5416.72, base loss: 15748.39
[INFO 2017-06-30 10:13:34,907 main.py:98] improve_loss: 10331.67, improve_percent: 0.66
[INFO 2017-06-30 10:13:34,909 main.py:66] current best improved percent: 0.67
[INFO 2017-06-30 10:13:38,024 main.py:52] epoch 10200, training loss: 5374.44, average training loss: 5338.18, base loss: 15969.55
[INFO 2017-06-30 10:13:41,183 main.py:52] epoch 10201, training loss: 5282.42, average training loss: 5338.36, base loss: 15969.68
[INFO 2017-06-30 10:13:44,292 main.py:52] epoch 10202, training loss: 5137.99, average training loss: 5338.09, base loss: 15969.34
[INFO 2017-06-30 10:13:47,440 main.py:52] epoch 10203, training loss: 5217.18, average training loss: 5337.87, base loss: 15968.94
[INFO 2017-06-30 10:13:50,602 main.py:52] epoch 10204, training loss: 5534.49, average training loss: 5337.87, base loss: 15968.79
[INFO 2017-06-30 10:13:53,773 main.py:52] epoch 10205, training loss: 5657.47, average training loss: 5338.00, base loss: 15968.97
[INFO 2017-06-30 10:13:56,925 main.py:52] epoch 10206, training loss: 5596.47, average training loss: 5338.22, base loss: 15969.00
[INFO 2017-06-30 10:14:00,060 main.py:52] epoch 10207, training loss: 5488.50, average training loss: 5338.43, base loss: 15969.07
[INFO 2017-06-30 10:14:03,196 main.py:52] epoch 10208, training loss: 5431.58, average training loss: 5338.48, base loss: 15969.27
[INFO 2017-06-30 10:14:06,305 main.py:52] epoch 10209, training loss: 5591.08, average training loss: 5338.58, base loss: 15969.52
[INFO 2017-06-30 10:14:09,438 main.py:52] epoch 10210, training loss: 5524.14, average training loss: 5338.45, base loss: 15969.41
[INFO 2017-06-30 10:14:12,519 main.py:52] epoch 10211, training loss: 5213.59, average training loss: 5338.60, base loss: 15969.39
[INFO 2017-06-30 10:14:15,622 main.py:52] epoch 10212, training loss: 5209.92, average training loss: 5338.21, base loss: 15969.10
[INFO 2017-06-30 10:14:18,751 main.py:52] epoch 10213, training loss: 5261.22, average training loss: 5337.79, base loss: 15968.98
[INFO 2017-06-30 10:14:21,867 main.py:52] epoch 10214, training loss: 5181.95, average training loss: 5337.40, base loss: 15968.88
[INFO 2017-06-30 10:14:24,989 main.py:52] epoch 10215, training loss: 5364.51, average training loss: 5337.80, base loss: 15969.04
[INFO 2017-06-30 10:14:28,116 main.py:52] epoch 10216, training loss: 5569.05, average training loss: 5337.97, base loss: 15969.06
[INFO 2017-06-30 10:14:31,233 main.py:52] epoch 10217, training loss: 5514.94, average training loss: 5338.00, base loss: 15969.18
[INFO 2017-06-30 10:14:34,368 main.py:52] epoch 10218, training loss: 5638.08, average training loss: 5338.46, base loss: 15969.19
[INFO 2017-06-30 10:14:37,574 main.py:52] epoch 10219, training loss: 5486.33, average training loss: 5338.49, base loss: 15969.12
[INFO 2017-06-30 10:14:40,774 main.py:52] epoch 10220, training loss: 5442.12, average training loss: 5338.30, base loss: 15969.25
[INFO 2017-06-30 10:14:43,914 main.py:52] epoch 10221, training loss: 5585.44, average training loss: 5338.45, base loss: 15969.65
[INFO 2017-06-30 10:14:47,063 main.py:52] epoch 10222, training loss: 5295.46, average training loss: 5338.19, base loss: 15969.72
[INFO 2017-06-30 10:14:50,199 main.py:52] epoch 10223, training loss: 5491.37, average training loss: 5338.39, base loss: 15969.98
[INFO 2017-06-30 10:14:53,350 main.py:52] epoch 10224, training loss: 5379.57, average training loss: 5337.62, base loss: 15969.84
[INFO 2017-06-30 10:14:56,509 main.py:52] epoch 10225, training loss: 4962.38, average training loss: 5337.22, base loss: 15969.84
[INFO 2017-06-30 10:14:59,650 main.py:52] epoch 10226, training loss: 5484.94, average training loss: 5337.16, base loss: 15969.98
[INFO 2017-06-30 10:15:02,799 main.py:52] epoch 10227, training loss: 5838.37, average training loss: 5337.78, base loss: 15970.31
[INFO 2017-06-30 10:15:05,908 main.py:52] epoch 10228, training loss: 5078.46, average training loss: 5337.73, base loss: 15970.15
[INFO 2017-06-30 10:15:09,052 main.py:52] epoch 10229, training loss: 5053.33, average training loss: 5337.28, base loss: 15970.17
[INFO 2017-06-30 10:15:12,214 main.py:52] epoch 10230, training loss: 5385.80, average training loss: 5337.40, base loss: 15970.32
[INFO 2017-06-30 10:15:15,365 main.py:52] epoch 10231, training loss: 5513.31, average training loss: 5337.59, base loss: 15970.78
[INFO 2017-06-30 10:15:18,506 main.py:52] epoch 10232, training loss: 5329.35, average training loss: 5337.60, base loss: 15970.86
[INFO 2017-06-30 10:15:21,682 main.py:52] epoch 10233, training loss: 5480.45, average training loss: 5337.57, base loss: 15971.41
[INFO 2017-06-30 10:15:24,843 main.py:52] epoch 10234, training loss: 5110.64, average training loss: 5336.91, base loss: 15971.18
[INFO 2017-06-30 10:15:27,994 main.py:52] epoch 10235, training loss: 5116.95, average training loss: 5336.66, base loss: 15971.31
[INFO 2017-06-30 10:15:31,173 main.py:52] epoch 10236, training loss: 5333.81, average training loss: 5336.49, base loss: 15971.22
[INFO 2017-06-30 10:15:34,282 main.py:52] epoch 10237, training loss: 5372.86, average training loss: 5336.61, base loss: 15971.39
[INFO 2017-06-30 10:15:37,470 main.py:52] epoch 10238, training loss: 5191.14, average training loss: 5336.48, base loss: 15971.45
[INFO 2017-06-30 10:15:40,638 main.py:52] epoch 10239, training loss: 5092.65, average training loss: 5336.12, base loss: 15971.24
[INFO 2017-06-30 10:15:43,804 main.py:52] epoch 10240, training loss: 5225.78, average training loss: 5335.67, base loss: 15970.96
[INFO 2017-06-30 10:15:46,928 main.py:52] epoch 10241, training loss: 5499.73, average training loss: 5335.91, base loss: 15971.16
[INFO 2017-06-30 10:15:50,027 main.py:52] epoch 10242, training loss: 5363.18, average training loss: 5336.10, base loss: 15971.36
[INFO 2017-06-30 10:15:53,130 main.py:52] epoch 10243, training loss: 4947.37, average training loss: 5335.09, base loss: 15971.23
[INFO 2017-06-30 10:15:56,282 main.py:52] epoch 10244, training loss: 5055.59, average training loss: 5335.11, base loss: 15971.05
[INFO 2017-06-30 10:15:59,429 main.py:52] epoch 10245, training loss: 4940.55, average training loss: 5334.70, base loss: 15970.89
[INFO 2017-06-30 10:16:02,597 main.py:52] epoch 10246, training loss: 5417.09, average training loss: 5334.56, base loss: 15970.85
[INFO 2017-06-30 10:16:05,750 main.py:52] epoch 10247, training loss: 5083.04, average training loss: 5334.76, base loss: 15970.65
[INFO 2017-06-30 10:16:08,891 main.py:52] epoch 10248, training loss: 5105.55, average training loss: 5334.78, base loss: 15970.55
[INFO 2017-06-30 10:16:12,017 main.py:52] epoch 10249, training loss: 5135.92, average training loss: 5334.62, base loss: 15970.42
[INFO 2017-06-30 10:16:15,180 main.py:52] epoch 10250, training loss: 5458.52, average training loss: 5334.56, base loss: 15970.52
[INFO 2017-06-30 10:16:18,261 main.py:52] epoch 10251, training loss: 5414.20, average training loss: 5334.73, base loss: 15970.49
[INFO 2017-06-30 10:16:21,406 main.py:52] epoch 10252, training loss: 5194.61, average training loss: 5334.58, base loss: 15970.52
[INFO 2017-06-30 10:16:24,561 main.py:52] epoch 10253, training loss: 5337.05, average training loss: 5334.64, base loss: 15970.53
[INFO 2017-06-30 10:16:27,680 main.py:52] epoch 10254, training loss: 5030.33, average training loss: 5334.36, base loss: 15970.16
[INFO 2017-06-30 10:16:30,824 main.py:52] epoch 10255, training loss: 5233.95, average training loss: 5334.22, base loss: 15969.95
[INFO 2017-06-30 10:16:33,959 main.py:52] epoch 10256, training loss: 5662.51, average training loss: 5334.64, base loss: 15970.19
[INFO 2017-06-30 10:16:37,112 main.py:52] epoch 10257, training loss: 5356.87, average training loss: 5334.82, base loss: 15970.41
[INFO 2017-06-30 10:16:40,248 main.py:52] epoch 10258, training loss: 5400.31, average training loss: 5334.48, base loss: 15970.36
[INFO 2017-06-30 10:16:43,391 main.py:52] epoch 10259, training loss: 5288.18, average training loss: 5334.37, base loss: 15970.59
[INFO 2017-06-30 10:16:46,576 main.py:52] epoch 10260, training loss: 5467.09, average training loss: 5334.29, base loss: 15970.93
[INFO 2017-06-30 10:16:49,728 main.py:52] epoch 10261, training loss: 5562.79, average training loss: 5334.56, base loss: 15971.13
[INFO 2017-06-30 10:16:52,889 main.py:52] epoch 10262, training loss: 5449.86, average training loss: 5334.60, base loss: 15971.35
[INFO 2017-06-30 10:16:56,028 main.py:52] epoch 10263, training loss: 5369.31, average training loss: 5334.80, base loss: 15971.30
[INFO 2017-06-30 10:16:59,127 main.py:52] epoch 10264, training loss: 5545.30, average training loss: 5335.05, base loss: 15971.42
[INFO 2017-06-30 10:17:02,343 main.py:52] epoch 10265, training loss: 5328.91, average training loss: 5334.94, base loss: 15971.13
[INFO 2017-06-30 10:17:05,459 main.py:52] epoch 10266, training loss: 5160.85, average training loss: 5334.60, base loss: 15970.74
[INFO 2017-06-30 10:17:08,587 main.py:52] epoch 10267, training loss: 5534.01, average training loss: 5335.15, base loss: 15970.56
[INFO 2017-06-30 10:17:11,700 main.py:52] epoch 10268, training loss: 5382.38, average training loss: 5335.59, base loss: 15970.69
[INFO 2017-06-30 10:17:14,881 main.py:52] epoch 10269, training loss: 5513.47, average training loss: 5335.88, base loss: 15970.79
[INFO 2017-06-30 10:17:18,018 main.py:52] epoch 10270, training loss: 5326.39, average training loss: 5336.02, base loss: 15970.52
[INFO 2017-06-30 10:17:21,210 main.py:52] epoch 10271, training loss: 5125.79, average training loss: 5335.67, base loss: 15970.62
[INFO 2017-06-30 10:17:24,343 main.py:52] epoch 10272, training loss: 4974.49, average training loss: 5335.14, base loss: 15970.28
[INFO 2017-06-30 10:17:27,490 main.py:52] epoch 10273, training loss: 5192.28, average training loss: 5335.21, base loss: 15970.22
[INFO 2017-06-30 10:17:30,639 main.py:52] epoch 10274, training loss: 5731.41, average training loss: 5335.39, base loss: 15970.61
[INFO 2017-06-30 10:17:33,758 main.py:52] epoch 10275, training loss: 5222.55, average training loss: 5335.41, base loss: 15970.73
[INFO 2017-06-30 10:17:36,915 main.py:52] epoch 10276, training loss: 5670.78, average training loss: 5335.42, base loss: 15971.09
[INFO 2017-06-30 10:17:40,049 main.py:52] epoch 10277, training loss: 4912.14, average training loss: 5334.81, base loss: 15970.92
[INFO 2017-06-30 10:17:43,256 main.py:52] epoch 10278, training loss: 5012.41, average training loss: 5334.52, base loss: 15970.72
[INFO 2017-06-30 10:17:46,351 main.py:52] epoch 10279, training loss: 5400.93, average training loss: 5334.43, base loss: 15970.72
[INFO 2017-06-30 10:17:49,473 main.py:52] epoch 10280, training loss: 5529.99, average training loss: 5334.71, base loss: 15971.11
[INFO 2017-06-30 10:17:52,577 main.py:52] epoch 10281, training loss: 5177.29, average training loss: 5335.03, base loss: 15971.04
[INFO 2017-06-30 10:17:55,702 main.py:52] epoch 10282, training loss: 5327.78, average training loss: 5334.87, base loss: 15970.87
[INFO 2017-06-30 10:17:58,831 main.py:52] epoch 10283, training loss: 5423.35, average training loss: 5334.93, base loss: 15970.39
[INFO 2017-06-30 10:18:02,015 main.py:52] epoch 10284, training loss: 5123.27, average training loss: 5334.68, base loss: 15970.13
[INFO 2017-06-30 10:18:05,180 main.py:52] epoch 10285, training loss: 5451.28, average training loss: 5334.54, base loss: 15970.21
[INFO 2017-06-30 10:18:08,344 main.py:52] epoch 10286, training loss: 4896.64, average training loss: 5334.23, base loss: 15969.74
[INFO 2017-06-30 10:18:11,485 main.py:52] epoch 10287, training loss: 5212.59, average training loss: 5334.39, base loss: 15969.63
[INFO 2017-06-30 10:18:14,661 main.py:52] epoch 10288, training loss: 5464.71, average training loss: 5334.24, base loss: 15969.79
[INFO 2017-06-30 10:18:17,835 main.py:52] epoch 10289, training loss: 5471.02, average training loss: 5334.00, base loss: 15970.06
[INFO 2017-06-30 10:18:20,957 main.py:52] epoch 10290, training loss: 5309.52, average training loss: 5333.62, base loss: 15969.93
[INFO 2017-06-30 10:18:24,166 main.py:52] epoch 10291, training loss: 4982.65, average training loss: 5332.96, base loss: 15969.62
[INFO 2017-06-30 10:18:27,335 main.py:52] epoch 10292, training loss: 5650.98, average training loss: 5332.91, base loss: 15969.95
[INFO 2017-06-30 10:18:30,467 main.py:52] epoch 10293, training loss: 5045.99, average training loss: 5332.43, base loss: 15969.77
[INFO 2017-06-30 10:18:33,592 main.py:52] epoch 10294, training loss: 5060.56, average training loss: 5332.09, base loss: 15969.73
[INFO 2017-06-30 10:18:36,704 main.py:52] epoch 10295, training loss: 5406.89, average training loss: 5332.21, base loss: 15969.91
[INFO 2017-06-30 10:18:39,860 main.py:52] epoch 10296, training loss: 5103.76, average training loss: 5331.79, base loss: 15969.56
[INFO 2017-06-30 10:18:43,046 main.py:52] epoch 10297, training loss: 5550.68, average training loss: 5332.13, base loss: 15969.78
[INFO 2017-06-30 10:18:46,203 main.py:52] epoch 10298, training loss: 5087.88, average training loss: 5331.78, base loss: 15969.84
[INFO 2017-06-30 10:18:49,378 main.py:52] epoch 10299, training loss: 5409.41, average training loss: 5332.01, base loss: 15970.06
[INFO 2017-06-30 10:18:49,378 main.py:54] epoch 10299, testing
[INFO 2017-06-30 10:19:02,547 main.py:97] average testing loss: 5265.04, base loss: 15668.17
[INFO 2017-06-30 10:19:02,547 main.py:98] improve_loss: 10403.14, improve_percent: 0.66
[INFO 2017-06-30 10:19:02,549 main.py:66] current best improved percent: 0.67
[INFO 2017-06-30 10:19:05,685 main.py:52] epoch 10300, training loss: 5210.57, average training loss: 5331.50, base loss: 15970.12
[INFO 2017-06-30 10:19:08,802 main.py:52] epoch 10301, training loss: 5129.96, average training loss: 5331.52, base loss: 15970.09
[INFO 2017-06-30 10:19:11,969 main.py:52] epoch 10302, training loss: 5252.27, average training loss: 5331.24, base loss: 15970.12
[INFO 2017-06-30 10:19:15,106 main.py:52] epoch 10303, training loss: 5296.76, average training loss: 5331.20, base loss: 15970.32
[INFO 2017-06-30 10:19:18,229 main.py:52] epoch 10304, training loss: 5571.03, average training loss: 5331.55, base loss: 15970.82
[INFO 2017-06-30 10:19:21,368 main.py:52] epoch 10305, training loss: 5195.00, average training loss: 5331.87, base loss: 15970.61
[INFO 2017-06-30 10:19:24,510 main.py:52] epoch 10306, training loss: 5099.78, average training loss: 5331.51, base loss: 15970.46
[INFO 2017-06-30 10:19:27,659 main.py:52] epoch 10307, training loss: 5292.16, average training loss: 5331.70, base loss: 15970.84
[INFO 2017-06-30 10:19:30,789 main.py:52] epoch 10308, training loss: 5535.25, average training loss: 5332.24, base loss: 15971.22
[INFO 2017-06-30 10:19:33,936 main.py:52] epoch 10309, training loss: 5316.89, average training loss: 5331.84, base loss: 15971.40
[INFO 2017-06-30 10:19:37,082 main.py:52] epoch 10310, training loss: 5174.51, average training loss: 5331.80, base loss: 15971.75
[INFO 2017-06-30 10:19:40,166 main.py:52] epoch 10311, training loss: 5275.58, average training loss: 5331.93, base loss: 15971.71
[INFO 2017-06-30 10:19:43,297 main.py:52] epoch 10312, training loss: 5436.70, average training loss: 5331.87, base loss: 15971.76
[INFO 2017-06-30 10:19:46,439 main.py:52] epoch 10313, training loss: 5158.02, average training loss: 5331.54, base loss: 15971.70
[INFO 2017-06-30 10:19:49,586 main.py:52] epoch 10314, training loss: 5124.92, average training loss: 5331.15, base loss: 15971.86
[INFO 2017-06-30 10:19:52,677 main.py:52] epoch 10315, training loss: 5253.63, average training loss: 5330.72, base loss: 15972.03
[INFO 2017-06-30 10:19:55,772 main.py:52] epoch 10316, training loss: 5126.61, average training loss: 5330.04, base loss: 15972.04
[INFO 2017-06-30 10:19:58,930 main.py:52] epoch 10317, training loss: 5226.25, average training loss: 5330.10, base loss: 15972.18
[INFO 2017-06-30 10:20:02,069 main.py:52] epoch 10318, training loss: 5178.13, average training loss: 5330.01, base loss: 15971.96
[INFO 2017-06-30 10:20:05,198 main.py:52] epoch 10319, training loss: 5274.56, average training loss: 5329.95, base loss: 15971.92
[INFO 2017-06-30 10:20:08,346 main.py:52] epoch 10320, training loss: 5355.27, average training loss: 5329.66, base loss: 15972.18
[INFO 2017-06-30 10:20:11,549 main.py:52] epoch 10321, training loss: 5290.52, average training loss: 5329.48, base loss: 15972.41
[INFO 2017-06-30 10:20:14,713 main.py:52] epoch 10322, training loss: 5168.33, average training loss: 5329.44, base loss: 15972.27
[INFO 2017-06-30 10:20:17,857 main.py:52] epoch 10323, training loss: 4987.83, average training loss: 5329.31, base loss: 15972.05
[INFO 2017-06-30 10:20:20,967 main.py:52] epoch 10324, training loss: 5496.58, average training loss: 5329.59, base loss: 15972.06
[INFO 2017-06-30 10:20:24,137 main.py:52] epoch 10325, training loss: 4801.41, average training loss: 5328.99, base loss: 15971.89
[INFO 2017-06-30 10:20:27,310 main.py:52] epoch 10326, training loss: 5178.85, average training loss: 5328.73, base loss: 15972.05
[INFO 2017-06-30 10:20:30,469 main.py:52] epoch 10327, training loss: 5168.88, average training loss: 5328.78, base loss: 15972.01
[INFO 2017-06-30 10:20:33,608 main.py:52] epoch 10328, training loss: 5293.87, average training loss: 5328.68, base loss: 15971.86
[INFO 2017-06-30 10:20:36,792 main.py:52] epoch 10329, training loss: 5097.06, average training loss: 5328.44, base loss: 15971.73
[INFO 2017-06-30 10:20:39,910 main.py:52] epoch 10330, training loss: 5300.75, average training loss: 5328.37, base loss: 15971.73
[INFO 2017-06-30 10:20:43,094 main.py:52] epoch 10331, training loss: 4765.85, average training loss: 5328.02, base loss: 15971.49
[INFO 2017-06-30 10:20:46,203 main.py:52] epoch 10332, training loss: 4876.58, average training loss: 5327.57, base loss: 15971.30
[INFO 2017-06-30 10:20:49,380 main.py:52] epoch 10333, training loss: 5306.67, average training loss: 5327.45, base loss: 15971.60
[INFO 2017-06-30 10:20:52,523 main.py:52] epoch 10334, training loss: 5317.15, average training loss: 5327.18, base loss: 15971.86
[INFO 2017-06-30 10:20:55,683 main.py:52] epoch 10335, training loss: 5056.83, average training loss: 5327.00, base loss: 15971.92
[INFO 2017-06-30 10:20:58,800 main.py:52] epoch 10336, training loss: 5319.20, average training loss: 5327.03, base loss: 15971.89
[INFO 2017-06-30 10:21:01,949 main.py:52] epoch 10337, training loss: 5203.97, average training loss: 5326.75, base loss: 15971.85
[INFO 2017-06-30 10:21:05,079 main.py:52] epoch 10338, training loss: 5248.41, average training loss: 5326.63, base loss: 15971.97
[INFO 2017-06-30 10:21:08,232 main.py:52] epoch 10339, training loss: 5341.34, average training loss: 5327.05, base loss: 15972.01
[INFO 2017-06-30 10:21:11,402 main.py:52] epoch 10340, training loss: 5280.33, average training loss: 5327.18, base loss: 15972.20
[INFO 2017-06-30 10:21:14,543 main.py:52] epoch 10341, training loss: 5036.97, average training loss: 5327.15, base loss: 15972.07
[INFO 2017-06-30 10:21:17,769 main.py:52] epoch 10342, training loss: 5386.26, average training loss: 5326.86, base loss: 15971.85
[INFO 2017-06-30 10:21:20,931 main.py:52] epoch 10343, training loss: 4810.41, average training loss: 5326.03, base loss: 15971.32
[INFO 2017-06-30 10:21:24,039 main.py:52] epoch 10344, training loss: 5356.05, average training loss: 5325.65, base loss: 15971.50
[INFO 2017-06-30 10:21:27,174 main.py:52] epoch 10345, training loss: 5598.54, average training loss: 5325.78, base loss: 15972.02
[INFO 2017-06-30 10:21:30,298 main.py:52] epoch 10346, training loss: 5586.36, average training loss: 5326.16, base loss: 15971.83
[INFO 2017-06-30 10:21:33,427 main.py:52] epoch 10347, training loss: 5128.43, average training loss: 5326.17, base loss: 15971.62
[INFO 2017-06-30 10:21:36,571 main.py:52] epoch 10348, training loss: 5346.65, average training loss: 5326.00, base loss: 15971.43
[INFO 2017-06-30 10:21:39,769 main.py:52] epoch 10349, training loss: 5148.99, average training loss: 5325.91, base loss: 15971.06
[INFO 2017-06-30 10:21:42,935 main.py:52] epoch 10350, training loss: 4874.87, average training loss: 5325.04, base loss: 15971.05
[INFO 2017-06-30 10:21:46,067 main.py:52] epoch 10351, training loss: 5014.79, average training loss: 5324.46, base loss: 15971.09
[INFO 2017-06-30 10:21:49,196 main.py:52] epoch 10352, training loss: 5493.89, average training loss: 5324.51, base loss: 15971.32
[INFO 2017-06-30 10:21:52,326 main.py:52] epoch 10353, training loss: 5679.31, average training loss: 5325.08, base loss: 15971.75
[INFO 2017-06-30 10:21:55,468 main.py:52] epoch 10354, training loss: 5358.90, average training loss: 5325.00, base loss: 15971.67
[INFO 2017-06-30 10:21:58,603 main.py:52] epoch 10355, training loss: 5316.86, average training loss: 5325.01, base loss: 15971.53
[INFO 2017-06-30 10:22:01,724 main.py:52] epoch 10356, training loss: 5364.55, average training loss: 5324.99, base loss: 15971.92
[INFO 2017-06-30 10:22:04,894 main.py:52] epoch 10357, training loss: 5066.94, average training loss: 5324.74, base loss: 15971.99
[INFO 2017-06-30 10:22:08,028 main.py:52] epoch 10358, training loss: 5188.34, average training loss: 5324.66, base loss: 15971.81
[INFO 2017-06-30 10:22:11,163 main.py:52] epoch 10359, training loss: 5783.04, average training loss: 5324.78, base loss: 15972.24
[INFO 2017-06-30 10:22:14,348 main.py:52] epoch 10360, training loss: 5056.00, average training loss: 5324.31, base loss: 15971.99
[INFO 2017-06-30 10:22:17,540 main.py:52] epoch 10361, training loss: 5653.95, average training loss: 5324.92, base loss: 15972.10
[INFO 2017-06-30 10:22:20,695 main.py:52] epoch 10362, training loss: 5273.34, average training loss: 5325.15, base loss: 15971.81
[INFO 2017-06-30 10:22:23,854 main.py:52] epoch 10363, training loss: 5120.55, average training loss: 5324.87, base loss: 15971.37
[INFO 2017-06-30 10:22:26,995 main.py:52] epoch 10364, training loss: 5104.33, average training loss: 5324.73, base loss: 15971.67
[INFO 2017-06-30 10:22:30,173 main.py:52] epoch 10365, training loss: 5289.45, average training loss: 5324.55, base loss: 15971.76
[INFO 2017-06-30 10:22:33,328 main.py:52] epoch 10366, training loss: 5408.36, average training loss: 5324.75, base loss: 15971.52
[INFO 2017-06-30 10:22:36,473 main.py:52] epoch 10367, training loss: 5413.32, average training loss: 5324.90, base loss: 15971.43
[INFO 2017-06-30 10:22:39,640 main.py:52] epoch 10368, training loss: 5463.67, average training loss: 5324.80, base loss: 15971.26
[INFO 2017-06-30 10:22:42,824 main.py:52] epoch 10369, training loss: 5058.14, average training loss: 5324.90, base loss: 15970.92
[INFO 2017-06-30 10:22:45,938 main.py:52] epoch 10370, training loss: 5116.93, average training loss: 5324.05, base loss: 15971.05
[INFO 2017-06-30 10:22:49,095 main.py:52] epoch 10371, training loss: 5180.78, average training loss: 5324.02, base loss: 15970.86
[INFO 2017-06-30 10:22:52,254 main.py:52] epoch 10372, training loss: 5065.15, average training loss: 5323.84, base loss: 15970.66
[INFO 2017-06-30 10:22:55,431 main.py:52] epoch 10373, training loss: 5280.99, average training loss: 5323.85, base loss: 15970.60
[INFO 2017-06-30 10:22:58,538 main.py:52] epoch 10374, training loss: 5362.83, average training loss: 5323.93, base loss: 15970.71
[INFO 2017-06-30 10:23:01,673 main.py:52] epoch 10375, training loss: 5911.82, average training loss: 5324.34, base loss: 15971.35
[INFO 2017-06-30 10:23:04,792 main.py:52] epoch 10376, training loss: 5918.09, average training loss: 5324.86, base loss: 15971.65
[INFO 2017-06-30 10:23:07,983 main.py:52] epoch 10377, training loss: 5474.35, average training loss: 5325.24, base loss: 15971.67
[INFO 2017-06-30 10:23:11,154 main.py:52] epoch 10378, training loss: 5307.48, average training loss: 5325.37, base loss: 15971.96
[INFO 2017-06-30 10:23:14,258 main.py:52] epoch 10379, training loss: 5300.05, average training loss: 5325.36, base loss: 15971.94
[INFO 2017-06-30 10:23:17,406 main.py:52] epoch 10380, training loss: 5563.34, average training loss: 5325.72, base loss: 15972.29
[INFO 2017-06-30 10:23:20,561 main.py:52] epoch 10381, training loss: 5774.50, average training loss: 5325.86, base loss: 15972.68
[INFO 2017-06-30 10:23:23,707 main.py:52] epoch 10382, training loss: 5636.15, average training loss: 5326.36, base loss: 15972.73
[INFO 2017-06-30 10:23:26,865 main.py:52] epoch 10383, training loss: 5225.73, average training loss: 5326.09, base loss: 15972.82
[INFO 2017-06-30 10:23:30,011 main.py:52] epoch 10384, training loss: 5478.16, average training loss: 5326.54, base loss: 15972.98
[INFO 2017-06-30 10:23:33,143 main.py:52] epoch 10385, training loss: 5236.13, average training loss: 5326.37, base loss: 15972.97
[INFO 2017-06-30 10:23:36,376 main.py:52] epoch 10386, training loss: 5855.53, average training loss: 5326.86, base loss: 15973.54
[INFO 2017-06-30 10:23:39,492 main.py:52] epoch 10387, training loss: 5361.18, average training loss: 5326.61, base loss: 15973.69
[INFO 2017-06-30 10:23:42,679 main.py:52] epoch 10388, training loss: 5508.09, average training loss: 5327.07, base loss: 15973.34
[INFO 2017-06-30 10:23:45,843 main.py:52] epoch 10389, training loss: 5208.87, average training loss: 5327.30, base loss: 15972.90
[INFO 2017-06-30 10:23:49,009 main.py:52] epoch 10390, training loss: 5368.63, average training loss: 5327.29, base loss: 15972.92
[INFO 2017-06-30 10:23:52,135 main.py:52] epoch 10391, training loss: 4972.41, average training loss: 5326.76, base loss: 15972.67
[INFO 2017-06-30 10:23:55,248 main.py:52] epoch 10392, training loss: 5344.10, average training loss: 5326.57, base loss: 15972.95
[INFO 2017-06-30 10:23:58,440 main.py:52] epoch 10393, training loss: 5336.44, average training loss: 5326.80, base loss: 15973.18
[INFO 2017-06-30 10:24:01,577 main.py:52] epoch 10394, training loss: 5252.82, average training loss: 5326.67, base loss: 15973.01
[INFO 2017-06-30 10:24:04,699 main.py:52] epoch 10395, training loss: 5281.83, average training loss: 5326.63, base loss: 15973.12
[INFO 2017-06-30 10:24:07,815 main.py:52] epoch 10396, training loss: 5304.17, average training loss: 5326.26, base loss: 15972.79
[INFO 2017-06-30 10:24:10,954 main.py:52] epoch 10397, training loss: 5259.11, average training loss: 5326.18, base loss: 15972.78
[INFO 2017-06-30 10:24:14,096 main.py:52] epoch 10398, training loss: 5316.21, average training loss: 5326.46, base loss: 15972.87
[INFO 2017-06-30 10:24:17,240 main.py:52] epoch 10399, training loss: 4935.38, average training loss: 5326.15, base loss: 15972.61
[INFO 2017-06-30 10:24:17,240 main.py:54] epoch 10399, testing
[INFO 2017-06-30 10:24:30,280 main.py:97] average testing loss: 5188.89, base loss: 15480.66
[INFO 2017-06-30 10:24:30,280 main.py:98] improve_loss: 10291.76, improve_percent: 0.66
[INFO 2017-06-30 10:24:30,281 main.py:66] current best improved percent: 0.67
[INFO 2017-06-30 10:24:33,394 main.py:52] epoch 10400, training loss: 5412.17, average training loss: 5325.88, base loss: 15972.87
[INFO 2017-06-30 10:24:36,510 main.py:52] epoch 10401, training loss: 5102.81, average training loss: 5325.72, base loss: 15972.67
[INFO 2017-06-30 10:24:39,839 main.py:52] epoch 10402, training loss: 5380.54, average training loss: 5325.96, base loss: 15972.61
[INFO 2017-06-30 10:24:43,027 main.py:52] epoch 10403, training loss: 5411.85, average training loss: 5325.91, base loss: 15972.61
[INFO 2017-06-30 10:24:46,187 main.py:52] epoch 10404, training loss: 5202.85, average training loss: 5326.11, base loss: 15972.78
[INFO 2017-06-30 10:24:49,320 main.py:52] epoch 10405, training loss: 5075.71, average training loss: 5326.01, base loss: 15972.79
[INFO 2017-06-30 10:24:52,429 main.py:52] epoch 10406, training loss: 5703.10, average training loss: 5326.38, base loss: 15973.09
[INFO 2017-06-30 10:24:55,575 main.py:52] epoch 10407, training loss: 5193.89, average training loss: 5326.51, base loss: 15972.94
[INFO 2017-06-30 10:24:58,746 main.py:52] epoch 10408, training loss: 5242.97, average training loss: 5326.69, base loss: 15972.71
[INFO 2017-06-30 10:25:01,878 main.py:52] epoch 10409, training loss: 4967.65, average training loss: 5326.31, base loss: 15972.45
[INFO 2017-06-30 10:25:05,001 main.py:52] epoch 10410, training loss: 5230.83, average training loss: 5326.75, base loss: 15972.22
[INFO 2017-06-30 10:25:08,169 main.py:52] epoch 10411, training loss: 5622.18, average training loss: 5327.00, base loss: 15972.45
[INFO 2017-06-30 10:25:11,263 main.py:52] epoch 10412, training loss: 4985.08, average training loss: 5326.74, base loss: 15972.15
[INFO 2017-06-30 10:25:14,356 main.py:52] epoch 10413, training loss: 5486.46, average training loss: 5326.92, base loss: 15972.42
[INFO 2017-06-30 10:25:17,479 main.py:52] epoch 10414, training loss: 5290.81, average training loss: 5326.72, base loss: 15972.19
[INFO 2017-06-30 10:25:20,602 main.py:52] epoch 10415, training loss: 5172.61, average training loss: 5326.23, base loss: 15972.20
[INFO 2017-06-30 10:25:23,789 main.py:52] epoch 10416, training loss: 4984.41, average training loss: 5326.00, base loss: 15972.19
[INFO 2017-06-30 10:25:26,950 main.py:52] epoch 10417, training loss: 5402.66, average training loss: 5326.20, base loss: 15972.27
[INFO 2017-06-30 10:25:30,099 main.py:52] epoch 10418, training loss: 5264.98, average training loss: 5326.05, base loss: 15972.28
[INFO 2017-06-30 10:25:33,227 main.py:52] epoch 10419, training loss: 5316.24, average training loss: 5325.58, base loss: 15972.23
[INFO 2017-06-30 10:25:36,346 main.py:52] epoch 10420, training loss: 5235.96, average training loss: 5325.37, base loss: 15972.26
[INFO 2017-06-30 10:25:39,542 main.py:52] epoch 10421, training loss: 5331.05, average training loss: 5325.58, base loss: 15972.27
[INFO 2017-06-30 10:25:42,687 main.py:52] epoch 10422, training loss: 5109.29, average training loss: 5325.04, base loss: 15972.34
[INFO 2017-06-30 10:25:45,859 main.py:52] epoch 10423, training loss: 4999.38, average training loss: 5324.70, base loss: 15972.03
[INFO 2017-06-30 10:25:49,019 main.py:52] epoch 10424, training loss: 5698.52, average training loss: 5324.80, base loss: 15972.52
[INFO 2017-06-30 10:25:52,151 main.py:52] epoch 10425, training loss: 5156.30, average training loss: 5324.69, base loss: 15972.44
[INFO 2017-06-30 10:25:55,292 main.py:52] epoch 10426, training loss: 5004.80, average training loss: 5324.07, base loss: 15972.43
[INFO 2017-06-30 10:25:58,458 main.py:52] epoch 10427, training loss: 5483.02, average training loss: 5324.30, base loss: 15972.51
[INFO 2017-06-30 10:26:01,617 main.py:52] epoch 10428, training loss: 5303.62, average training loss: 5324.41, base loss: 15972.37
[INFO 2017-06-30 10:26:04,755 main.py:52] epoch 10429, training loss: 5439.36, average training loss: 5323.96, base loss: 15972.49
[INFO 2017-06-30 10:26:07,936 main.py:52] epoch 10430, training loss: 4850.72, average training loss: 5323.20, base loss: 15972.09
[INFO 2017-06-30 10:26:11,085 main.py:52] epoch 10431, training loss: 4951.28, average training loss: 5322.62, base loss: 15971.51
[INFO 2017-06-30 10:26:14,287 main.py:52] epoch 10432, training loss: 4948.57, average training loss: 5322.30, base loss: 15971.61
[INFO 2017-06-30 10:26:17,431 main.py:52] epoch 10433, training loss: 4860.51, average training loss: 5321.74, base loss: 15971.53
[INFO 2017-06-30 10:26:20,554 main.py:52] epoch 10434, training loss: 5465.13, average training loss: 5321.76, base loss: 15971.73
[INFO 2017-06-30 10:26:23,730 main.py:52] epoch 10435, training loss: 5483.91, average training loss: 5321.43, base loss: 15971.76
[INFO 2017-06-30 10:26:26,849 main.py:52] epoch 10436, training loss: 5254.10, average training loss: 5321.08, base loss: 15971.85
[INFO 2017-06-30 10:26:29,994 main.py:52] epoch 10437, training loss: 5271.08, average training loss: 5321.13, base loss: 15971.77
[INFO 2017-06-30 10:26:33,143 main.py:52] epoch 10438, training loss: 5420.30, average training loss: 5321.42, base loss: 15971.77
[INFO 2017-06-30 10:26:36,270 main.py:52] epoch 10439, training loss: 5258.40, average training loss: 5321.71, base loss: 15971.60
[INFO 2017-06-30 10:26:39,391 main.py:52] epoch 10440, training loss: 5849.34, average training loss: 5321.92, base loss: 15971.83
[INFO 2017-06-30 10:26:42,546 main.py:52] epoch 10441, training loss: 5102.75, average training loss: 5321.77, base loss: 15971.87
[INFO 2017-06-30 10:26:45,664 main.py:52] epoch 10442, training loss: 5209.73, average training loss: 5321.33, base loss: 15971.87
[INFO 2017-06-30 10:26:48,827 main.py:52] epoch 10443, training loss: 5786.09, average training loss: 5322.08, base loss: 15972.32
[INFO 2017-06-30 10:26:51,985 main.py:52] epoch 10444, training loss: 5534.46, average training loss: 5322.36, base loss: 15972.53
[INFO 2017-06-30 10:26:55,139 main.py:52] epoch 10445, training loss: 5363.29, average training loss: 5322.24, base loss: 15972.58
[INFO 2017-06-30 10:26:58,274 main.py:52] epoch 10446, training loss: 5337.55, average training loss: 5322.16, base loss: 15972.44
[INFO 2017-06-30 10:27:01,451 main.py:52] epoch 10447, training loss: 5047.13, average training loss: 5322.08, base loss: 15971.96
[INFO 2017-06-30 10:27:04,597 main.py:52] epoch 10448, training loss: 5088.57, average training loss: 5321.53, base loss: 15971.84
[INFO 2017-06-30 10:27:07,729 main.py:52] epoch 10449, training loss: 5373.63, average training loss: 5321.42, base loss: 15972.04
[INFO 2017-06-30 10:27:10,855 main.py:52] epoch 10450, training loss: 5721.56, average training loss: 5321.87, base loss: 15972.31
[INFO 2017-06-30 10:27:13,985 main.py:52] epoch 10451, training loss: 5330.18, average training loss: 5321.96, base loss: 15972.10
[INFO 2017-06-30 10:27:17,106 main.py:52] epoch 10452, training loss: 5019.28, average training loss: 5321.42, base loss: 15971.67
[INFO 2017-06-30 10:27:20,279 main.py:52] epoch 10453, training loss: 5255.60, average training loss: 5321.62, base loss: 15971.72
[INFO 2017-06-30 10:27:23,407 main.py:52] epoch 10454, training loss: 5124.24, average training loss: 5321.57, base loss: 15971.34
[INFO 2017-06-30 10:27:26,567 main.py:52] epoch 10455, training loss: 5253.06, average training loss: 5321.74, base loss: 15971.16
[INFO 2017-06-30 10:27:29,666 main.py:52] epoch 10456, training loss: 5205.57, average training loss: 5321.99, base loss: 15971.32
[INFO 2017-06-30 10:27:32,816 main.py:52] epoch 10457, training loss: 4969.99, average training loss: 5321.54, base loss: 15971.04
[INFO 2017-06-30 10:27:35,915 main.py:52] epoch 10458, training loss: 5151.73, average training loss: 5321.48, base loss: 15970.98
[INFO 2017-06-30 10:27:39,105 main.py:52] epoch 10459, training loss: 5362.95, average training loss: 5321.65, base loss: 15970.95
[INFO 2017-06-30 10:27:42,231 main.py:52] epoch 10460, training loss: 5095.14, average training loss: 5321.26, base loss: 15970.81
[INFO 2017-06-30 10:27:45,366 main.py:52] epoch 10461, training loss: 5310.30, average training loss: 5320.97, base loss: 15970.97
[INFO 2017-06-30 10:27:48,573 main.py:52] epoch 10462, training loss: 5356.58, average training loss: 5321.22, base loss: 15971.28
[INFO 2017-06-30 10:27:51,716 main.py:52] epoch 10463, training loss: 5420.97, average training loss: 5321.41, base loss: 15971.36
[INFO 2017-06-30 10:27:54,913 main.py:52] epoch 10464, training loss: 5137.46, average training loss: 5321.18, base loss: 15971.45
[INFO 2017-06-30 10:27:58,093 main.py:52] epoch 10465, training loss: 5517.66, average training loss: 5321.05, base loss: 15971.68
[INFO 2017-06-30 10:28:01,206 main.py:52] epoch 10466, training loss: 5405.85, average training loss: 5321.26, base loss: 15971.72
[INFO 2017-06-30 10:28:04,382 main.py:52] epoch 10467, training loss: 5359.44, average training loss: 5321.01, base loss: 15971.86
[INFO 2017-06-30 10:28:07,550 main.py:52] epoch 10468, training loss: 5352.60, average training loss: 5320.97, base loss: 15971.93
[INFO 2017-06-30 10:28:10,712 main.py:52] epoch 10469, training loss: 5289.54, average training loss: 5321.05, base loss: 15971.94
[INFO 2017-06-30 10:28:13,881 main.py:52] epoch 10470, training loss: 5331.39, average training loss: 5321.12, base loss: 15971.97
[INFO 2017-06-30 10:28:17,030 main.py:52] epoch 10471, training loss: 5330.74, average training loss: 5321.04, base loss: 15971.90
[INFO 2017-06-30 10:28:20,140 main.py:52] epoch 10472, training loss: 4944.15, average training loss: 5320.69, base loss: 15971.46
[INFO 2017-06-30 10:28:23,294 main.py:52] epoch 10473, training loss: 5328.85, average training loss: 5320.32, base loss: 15971.34
[INFO 2017-06-30 10:28:26,460 main.py:52] epoch 10474, training loss: 5510.33, average training loss: 5320.23, base loss: 15971.72
[INFO 2017-06-30 10:28:29,617 main.py:52] epoch 10475, training loss: 4930.86, average training loss: 5319.62, base loss: 15971.58
[INFO 2017-06-30 10:28:32,750 main.py:52] epoch 10476, training loss: 5077.68, average training loss: 5319.45, base loss: 15971.55
[INFO 2017-06-30 10:28:35,856 main.py:52] epoch 10477, training loss: 5709.87, average training loss: 5319.84, base loss: 15971.85
[INFO 2017-06-30 10:28:39,019 main.py:52] epoch 10478, training loss: 5141.52, average training loss: 5319.40, base loss: 15971.84
[INFO 2017-06-30 10:28:42,154 main.py:52] epoch 10479, training loss: 5429.13, average training loss: 5319.56, base loss: 15971.99
[INFO 2017-06-30 10:28:45,343 main.py:52] epoch 10480, training loss: 5172.22, average training loss: 5319.09, base loss: 15971.86
[INFO 2017-06-30 10:28:48,458 main.py:52] epoch 10481, training loss: 5382.62, average training loss: 5319.24, base loss: 15971.84
[INFO 2017-06-30 10:28:51,563 main.py:52] epoch 10482, training loss: 5099.70, average training loss: 5319.12, base loss: 15971.83
[INFO 2017-06-30 10:28:54,687 main.py:52] epoch 10483, training loss: 5318.86, average training loss: 5318.80, base loss: 15972.17
[INFO 2017-06-30 10:28:57,834 main.py:52] epoch 10484, training loss: 5269.16, average training loss: 5318.96, base loss: 15971.99
[INFO 2017-06-30 10:29:00,992 main.py:52] epoch 10485, training loss: 5407.98, average training loss: 5318.93, base loss: 15972.40
[INFO 2017-06-30 10:29:04,154 main.py:52] epoch 10486, training loss: 5234.17, average training loss: 5318.99, base loss: 15972.33
[INFO 2017-06-30 10:29:07,312 main.py:52] epoch 10487, training loss: 5449.66, average training loss: 5318.71, base loss: 15972.43
[INFO 2017-06-30 10:29:10,426 main.py:52] epoch 10488, training loss: 5126.64, average training loss: 5318.39, base loss: 15972.02
[INFO 2017-06-30 10:29:13,567 main.py:52] epoch 10489, training loss: 5319.81, average training loss: 5318.34, base loss: 15971.87
[INFO 2017-06-30 10:29:16,690 main.py:52] epoch 10490, training loss: 5122.23, average training loss: 5318.20, base loss: 15971.91
[INFO 2017-06-30 10:29:19,882 main.py:52] epoch 10491, training loss: 5177.70, average training loss: 5318.27, base loss: 15971.50
[INFO 2017-06-30 10:29:23,028 main.py:52] epoch 10492, training loss: 5430.52, average training loss: 5318.40, base loss: 15971.66
[INFO 2017-06-30 10:29:26,151 main.py:52] epoch 10493, training loss: 5394.25, average training loss: 5318.70, base loss: 15971.80
[INFO 2017-06-30 10:29:29,343 main.py:52] epoch 10494, training loss: 5165.80, average training loss: 5318.66, base loss: 15971.64
[INFO 2017-06-30 10:29:32,497 main.py:52] epoch 10495, training loss: 5184.08, average training loss: 5318.37, base loss: 15971.65
[INFO 2017-06-30 10:29:35,636 main.py:52] epoch 10496, training loss: 5355.46, average training loss: 5318.05, base loss: 15971.70
[INFO 2017-06-30 10:29:38,800 main.py:52] epoch 10497, training loss: 5496.51, average training loss: 5318.43, base loss: 15971.93
[INFO 2017-06-30 10:29:41,925 main.py:52] epoch 10498, training loss: 5306.94, average training loss: 5318.09, base loss: 15971.94
[INFO 2017-06-30 10:29:45,053 main.py:52] epoch 10499, training loss: 4824.90, average training loss: 5317.54, base loss: 15971.59
[INFO 2017-06-30 10:29:45,053 main.py:54] epoch 10499, testing
[INFO 2017-06-30 10:29:58,215 main.py:97] average testing loss: 5285.44, base loss: 15816.05
[INFO 2017-06-30 10:29:58,215 main.py:98] improve_loss: 10530.61, improve_percent: 0.67
[INFO 2017-06-30 10:29:58,218 main.py:66] current best improved percent: 0.67
[INFO 2017-06-30 10:30:01,372 main.py:52] epoch 10500, training loss: 5298.57, average training loss: 5317.85, base loss: 15971.50
[INFO 2017-06-30 10:30:04,500 main.py:52] epoch 10501, training loss: 5241.09, average training loss: 5318.02, base loss: 15971.33
[INFO 2017-06-30 10:30:07,623 main.py:52] epoch 10502, training loss: 5356.08, average training loss: 5317.87, base loss: 15971.51
[INFO 2017-06-30 10:30:10,796 main.py:52] epoch 10503, training loss: 5357.68, average training loss: 5317.81, base loss: 15971.41
[INFO 2017-06-30 10:30:13,965 main.py:52] epoch 10504, training loss: 5811.94, average training loss: 5317.90, base loss: 15971.95
[INFO 2017-06-30 10:30:17,139 main.py:52] epoch 10505, training loss: 5046.98, average training loss: 5316.95, base loss: 15972.09
[INFO 2017-06-30 10:30:20,249 main.py:52] epoch 10506, training loss: 5156.96, average training loss: 5317.14, base loss: 15971.86
[INFO 2017-06-30 10:30:23,377 main.py:52] epoch 10507, training loss: 5422.48, average training loss: 5316.99, base loss: 15971.71
[INFO 2017-06-30 10:30:26,496 main.py:52] epoch 10508, training loss: 5225.21, average training loss: 5317.11, base loss: 15971.68
[INFO 2017-06-30 10:30:29,623 main.py:52] epoch 10509, training loss: 5494.01, average training loss: 5317.05, base loss: 15971.74
[INFO 2017-06-30 10:30:32,797 main.py:52] epoch 10510, training loss: 5363.34, average training loss: 5317.06, base loss: 15971.88
[INFO 2017-06-30 10:30:35,912 main.py:52] epoch 10511, training loss: 5428.68, average training loss: 5317.36, base loss: 15972.03
[INFO 2017-06-30 10:30:39,058 main.py:52] epoch 10512, training loss: 5350.14, average training loss: 5317.06, base loss: 15972.09
[INFO 2017-06-30 10:30:42,208 main.py:52] epoch 10513, training loss: 5262.08, average training loss: 5317.04, base loss: 15972.16
[INFO 2017-06-30 10:30:45,334 main.py:52] epoch 10514, training loss: 5117.02, average training loss: 5316.78, base loss: 15971.82
[INFO 2017-06-30 10:30:48,459 main.py:52] epoch 10515, training loss: 5522.25, average training loss: 5317.11, base loss: 15971.89
[INFO 2017-06-30 10:30:51,622 main.py:52] epoch 10516, training loss: 5373.86, average training loss: 5317.24, base loss: 15972.17
[INFO 2017-06-30 10:30:54,730 main.py:52] epoch 10517, training loss: 4785.27, average training loss: 5316.44, base loss: 15972.13
[INFO 2017-06-30 10:30:57,869 main.py:52] epoch 10518, training loss: 5639.06, average training loss: 5316.72, base loss: 15972.46
[INFO 2017-06-30 10:31:01,017 main.py:52] epoch 10519, training loss: 4978.49, average training loss: 5316.38, base loss: 15972.38
[INFO 2017-06-30 10:31:04,167 main.py:52] epoch 10520, training loss: 5451.03, average training loss: 5316.87, base loss: 15972.50
[INFO 2017-06-30 10:31:07,310 main.py:52] epoch 10521, training loss: 5018.96, average training loss: 5316.22, base loss: 15972.26
[INFO 2017-06-30 10:31:10,458 main.py:52] epoch 10522, training loss: 5257.46, average training loss: 5316.13, base loss: 15972.05
[INFO 2017-06-30 10:31:13,628 main.py:52] epoch 10523, training loss: 5446.75, average training loss: 5316.66, base loss: 15972.08
[INFO 2017-06-30 10:31:16,776 main.py:52] epoch 10524, training loss: 5419.67, average training loss: 5316.94, base loss: 15972.16
[INFO 2017-06-30 10:31:19,983 main.py:52] epoch 10525, training loss: 5382.35, average training loss: 5316.91, base loss: 15972.22
[INFO 2017-06-30 10:31:23,155 main.py:52] epoch 10526, training loss: 5076.31, average training loss: 5316.82, base loss: 15972.31
[INFO 2017-06-30 10:31:26,273 main.py:52] epoch 10527, training loss: 5327.67, average training loss: 5316.85, base loss: 15972.46
[INFO 2017-06-30 10:31:29,452 main.py:52] epoch 10528, training loss: 5206.39, average training loss: 5316.51, base loss: 15972.86
[INFO 2017-06-30 10:31:32,605 main.py:52] epoch 10529, training loss: 5273.16, average training loss: 5316.66, base loss: 15973.01
[INFO 2017-06-30 10:31:35,733 main.py:52] epoch 10530, training loss: 5455.60, average training loss: 5316.60, base loss: 15972.92
[INFO 2017-06-30 10:31:38,897 main.py:52] epoch 10531, training loss: 5342.81, average training loss: 5316.55, base loss: 15972.99
[INFO 2017-06-30 10:31:42,005 main.py:52] epoch 10532, training loss: 5523.63, average training loss: 5317.07, base loss: 15973.18
[INFO 2017-06-30 10:31:45,135 main.py:52] epoch 10533, training loss: 5188.75, average training loss: 5316.30, base loss: 15973.19
[INFO 2017-06-30 10:31:48,259 main.py:52] epoch 10534, training loss: 5358.20, average training loss: 5316.29, base loss: 15973.17
[INFO 2017-06-30 10:31:51,379 main.py:52] epoch 10535, training loss: 5446.43, average training loss: 5316.42, base loss: 15973.26
[INFO 2017-06-30 10:31:54,533 main.py:52] epoch 10536, training loss: 5369.68, average training loss: 5316.25, base loss: 15973.74
[INFO 2017-06-30 10:31:57,693 main.py:52] epoch 10537, training loss: 5084.92, average training loss: 5315.89, base loss: 15973.83
[INFO 2017-06-30 10:32:00,833 main.py:52] epoch 10538, training loss: 4820.02, average training loss: 5315.34, base loss: 15973.39
[INFO 2017-06-30 10:32:03,978 main.py:52] epoch 10539, training loss: 5337.66, average training loss: 5314.93, base loss: 15973.44
[INFO 2017-06-30 10:32:07,118 main.py:52] epoch 10540, training loss: 5323.89, average training loss: 5314.83, base loss: 15973.28
[INFO 2017-06-30 10:32:10,275 main.py:52] epoch 10541, training loss: 5040.17, average training loss: 5314.59, base loss: 15973.23
[INFO 2017-06-30 10:32:13,402 main.py:52] epoch 10542, training loss: 5163.56, average training loss: 5314.59, base loss: 15972.89
[INFO 2017-06-30 10:32:16,589 main.py:52] epoch 10543, training loss: 5412.94, average training loss: 5314.77, base loss: 15972.93
[INFO 2017-06-30 10:32:19,775 main.py:52] epoch 10544, training loss: 5208.96, average training loss: 5314.62, base loss: 15972.84
[INFO 2017-06-30 10:32:22,929 main.py:52] epoch 10545, training loss: 5532.09, average training loss: 5314.83, base loss: 15972.69
[INFO 2017-06-30 10:32:26,041 main.py:52] epoch 10546, training loss: 5273.34, average training loss: 5314.53, base loss: 15972.64
[INFO 2017-06-30 10:32:29,169 main.py:52] epoch 10547, training loss: 5539.65, average training loss: 5314.51, base loss: 15972.63
[INFO 2017-06-30 10:32:32,344 main.py:52] epoch 10548, training loss: 5365.23, average training loss: 5314.37, base loss: 15972.77
[INFO 2017-06-30 10:32:35,456 main.py:52] epoch 10549, training loss: 5369.04, average training loss: 5314.39, base loss: 15973.16
[INFO 2017-06-30 10:32:38,576 main.py:52] epoch 10550, training loss: 4922.04, average training loss: 5314.09, base loss: 15972.91
[INFO 2017-06-30 10:32:41,709 main.py:52] epoch 10551, training loss: 5210.57, average training loss: 5313.81, base loss: 15972.81
[INFO 2017-06-30 10:32:44,849 main.py:52] epoch 10552, training loss: 5298.99, average training loss: 5313.71, base loss: 15972.85
[INFO 2017-06-30 10:32:47,973 main.py:52] epoch 10553, training loss: 4780.24, average training loss: 5313.41, base loss: 15972.70
[INFO 2017-06-30 10:32:51,116 main.py:52] epoch 10554, training loss: 5282.50, average training loss: 5312.95, base loss: 15972.40
[INFO 2017-06-30 10:32:54,254 main.py:52] epoch 10555, training loss: 5353.93, average training loss: 5312.86, base loss: 15972.51
[INFO 2017-06-30 10:32:57,387 main.py:52] epoch 10556, training loss: 5243.57, average training loss: 5313.18, base loss: 15972.54
[INFO 2017-06-30 10:33:00,563 main.py:52] epoch 10557, training loss: 5158.88, average training loss: 5312.99, base loss: 15972.36
[INFO 2017-06-30 10:33:03,689 main.py:52] epoch 10558, training loss: 5088.31, average training loss: 5312.85, base loss: 15972.46
[INFO 2017-06-30 10:33:06,849 main.py:52] epoch 10559, training loss: 5014.34, average training loss: 5312.69, base loss: 15972.52
[INFO 2017-06-30 10:33:09,987 main.py:52] epoch 10560, training loss: 5413.78, average training loss: 5312.43, base loss: 15972.71
[INFO 2017-06-30 10:33:13,116 main.py:52] epoch 10561, training loss: 5181.58, average training loss: 5312.26, base loss: 15972.88
[INFO 2017-06-30 10:33:16,314 main.py:52] epoch 10562, training loss: 5118.95, average training loss: 5312.29, base loss: 15972.68
[INFO 2017-06-30 10:33:19,441 main.py:52] epoch 10563, training loss: 5078.42, average training loss: 5312.75, base loss: 15972.69
[INFO 2017-06-30 10:33:22,594 main.py:52] epoch 10564, training loss: 5096.55, average training loss: 5312.48, base loss: 15972.85
[INFO 2017-06-30 10:33:25,712 main.py:52] epoch 10565, training loss: 5294.49, average training loss: 5312.34, base loss: 15972.85
[INFO 2017-06-30 10:33:28,913 main.py:52] epoch 10566, training loss: 5418.05, average training loss: 5312.51, base loss: 15972.91
[INFO 2017-06-30 10:33:32,093 main.py:52] epoch 10567, training loss: 5373.26, average training loss: 5312.73, base loss: 15973.08
[INFO 2017-06-30 10:33:35,300 main.py:52] epoch 10568, training loss: 5142.66, average training loss: 5312.47, base loss: 15972.75
[INFO 2017-06-30 10:33:38,471 main.py:52] epoch 10569, training loss: 5548.31, average training loss: 5312.77, base loss: 15972.92
[INFO 2017-06-30 10:33:41,600 main.py:52] epoch 10570, training loss: 5381.58, average training loss: 5312.86, base loss: 15972.98
[INFO 2017-06-30 10:33:44,746 main.py:52] epoch 10571, training loss: 5054.11, average training loss: 5312.52, base loss: 15972.71
[INFO 2017-06-30 10:33:47,933 main.py:52] epoch 10572, training loss: 5018.16, average training loss: 5312.06, base loss: 15972.69
[INFO 2017-06-30 10:33:51,024 main.py:52] epoch 10573, training loss: 5305.66, average training loss: 5312.02, base loss: 15973.01
[INFO 2017-06-30 10:33:54,121 main.py:52] epoch 10574, training loss: 4953.54, average training loss: 5311.92, base loss: 15972.94
[INFO 2017-06-30 10:33:57,253 main.py:52] epoch 10575, training loss: 5198.92, average training loss: 5311.55, base loss: 15972.76
[INFO 2017-06-30 10:34:00,373 main.py:52] epoch 10576, training loss: 5492.88, average training loss: 5311.87, base loss: 15973.19
[INFO 2017-06-30 10:34:03,497 main.py:52] epoch 10577, training loss: 5080.99, average training loss: 5311.80, base loss: 15973.06
[INFO 2017-06-30 10:34:06,663 main.py:52] epoch 10578, training loss: 5834.58, average training loss: 5312.56, base loss: 15973.54
[INFO 2017-06-30 10:34:09,805 main.py:52] epoch 10579, training loss: 5153.32, average training loss: 5312.32, base loss: 15973.43
[INFO 2017-06-30 10:34:12,965 main.py:52] epoch 10580, training loss: 5497.87, average training loss: 5312.45, base loss: 15973.65
[INFO 2017-06-30 10:34:16,112 main.py:52] epoch 10581, training loss: 5693.06, average training loss: 5313.17, base loss: 15973.91
[INFO 2017-06-30 10:34:19,258 main.py:52] epoch 10582, training loss: 4947.09, average training loss: 5312.40, base loss: 15973.80
[INFO 2017-06-30 10:34:22,423 main.py:52] epoch 10583, training loss: 5157.07, average training loss: 5312.16, base loss: 15973.40
[INFO 2017-06-30 10:34:25,612 main.py:52] epoch 10584, training loss: 5291.00, average training loss: 5311.95, base loss: 15973.43
[INFO 2017-06-30 10:34:28,759 main.py:52] epoch 10585, training loss: 5424.29, average training loss: 5311.98, base loss: 15973.49
[INFO 2017-06-30 10:34:31,913 main.py:52] epoch 10586, training loss: 5317.49, average training loss: 5312.10, base loss: 15973.45
[INFO 2017-06-30 10:34:35,094 main.py:52] epoch 10587, training loss: 5472.65, average training loss: 5312.66, base loss: 15973.63
[INFO 2017-06-30 10:34:38,229 main.py:52] epoch 10588, training loss: 5042.57, average training loss: 5311.98, base loss: 15973.64
[INFO 2017-06-30 10:34:41,397 main.py:52] epoch 10589, training loss: 5369.63, average training loss: 5311.46, base loss: 15973.60
[INFO 2017-06-30 10:34:44,559 main.py:52] epoch 10590, training loss: 5529.92, average training loss: 5311.57, base loss: 15973.64
[INFO 2017-06-30 10:34:47,677 main.py:52] epoch 10591, training loss: 5187.08, average training loss: 5311.66, base loss: 15973.51
[INFO 2017-06-30 10:34:50,800 main.py:52] epoch 10592, training loss: 5452.42, average training loss: 5312.03, base loss: 15973.55
[INFO 2017-06-30 10:34:53,937 main.py:52] epoch 10593, training loss: 5476.95, average training loss: 5312.42, base loss: 15973.85
[INFO 2017-06-30 10:34:57,097 main.py:52] epoch 10594, training loss: 5507.06, average training loss: 5312.73, base loss: 15973.87
[INFO 2017-06-30 10:35:00,290 main.py:52] epoch 10595, training loss: 5239.18, average training loss: 5312.56, base loss: 15973.76
[INFO 2017-06-30 10:35:03,439 main.py:52] epoch 10596, training loss: 5400.43, average training loss: 5312.45, base loss: 15973.84
[INFO 2017-06-30 10:35:06,618 main.py:52] epoch 10597, training loss: 5571.10, average training loss: 5312.45, base loss: 15974.03
[INFO 2017-06-30 10:35:09,807 main.py:52] epoch 10598, training loss: 5329.17, average training loss: 5312.26, base loss: 15973.69
[INFO 2017-06-30 10:35:12,925 main.py:52] epoch 10599, training loss: 5342.43, average training loss: 5311.97, base loss: 15973.54
[INFO 2017-06-30 10:35:12,926 main.py:54] epoch 10599, testing
[INFO 2017-06-30 10:35:26,099 main.py:97] average testing loss: 5264.24, base loss: 16151.22
[INFO 2017-06-30 10:35:26,099 main.py:98] improve_loss: 10886.97, improve_percent: 0.67
[INFO 2017-06-30 10:35:26,101 main.py:62] model save to ./model/final.pth
[INFO 2017-06-30 10:35:26,135 main.py:66] current best improved percent: 0.67
[INFO 2017-06-30 10:35:29,259 main.py:52] epoch 10600, training loss: 5102.70, average training loss: 5311.99, base loss: 15973.38
[INFO 2017-06-30 10:35:32,384 main.py:52] epoch 10601, training loss: 5112.19, average training loss: 5311.65, base loss: 15973.29
[INFO 2017-06-30 10:35:35,500 main.py:52] epoch 10602, training loss: 5511.19, average training loss: 5311.86, base loss: 15973.20
[INFO 2017-06-30 10:35:38,633 main.py:52] epoch 10603, training loss: 5443.35, average training loss: 5312.09, base loss: 15973.35
[INFO 2017-06-30 10:35:41,779 main.py:52] epoch 10604, training loss: 5659.39, average training loss: 5312.88, base loss: 15973.77
[INFO 2017-06-30 10:35:44,959 main.py:52] epoch 10605, training loss: 4930.43, average training loss: 5312.55, base loss: 15973.84
[INFO 2017-06-30 10:35:48,144 main.py:52] epoch 10606, training loss: 4953.71, average training loss: 5312.29, base loss: 15973.67
[INFO 2017-06-30 10:35:51,278 main.py:52] epoch 10607, training loss: 5003.01, average training loss: 5312.12, base loss: 15973.38
[INFO 2017-06-30 10:35:54,429 main.py:52] epoch 10608, training loss: 5506.61, average training loss: 5312.70, base loss: 15973.69
[INFO 2017-06-30 10:35:57,577 main.py:52] epoch 10609, training loss: 5264.58, average training loss: 5313.12, base loss: 15973.79
[INFO 2017-06-30 10:36:00,726 main.py:52] epoch 10610, training loss: 5543.89, average training loss: 5312.95, base loss: 15974.10
[INFO 2017-06-30 10:36:03,876 main.py:52] epoch 10611, training loss: 5306.30, average training loss: 5313.01, base loss: 15974.29
[INFO 2017-06-30 10:36:07,007 main.py:52] epoch 10612, training loss: 5180.34, average training loss: 5313.53, base loss: 15973.92
[INFO 2017-06-30 10:36:10,103 main.py:52] epoch 10613, training loss: 5350.23, average training loss: 5313.37, base loss: 15973.62
[INFO 2017-06-30 10:36:13,259 main.py:52] epoch 10614, training loss: 5526.91, average training loss: 5313.65, base loss: 15973.82
[INFO 2017-06-30 10:36:16,385 main.py:52] epoch 10615, training loss: 5557.67, average training loss: 5313.95, base loss: 15973.98
[INFO 2017-06-30 10:36:19,558 main.py:52] epoch 10616, training loss: 5082.00, average training loss: 5314.00, base loss: 15973.66
[INFO 2017-06-30 10:36:22,675 main.py:52] epoch 10617, training loss: 5416.08, average training loss: 5314.11, base loss: 15973.68
[INFO 2017-06-30 10:36:25,820 main.py:52] epoch 10618, training loss: 5343.28, average training loss: 5313.86, base loss: 15973.74
[INFO 2017-06-30 10:36:28,988 main.py:52] epoch 10619, training loss: 5559.70, average training loss: 5314.25, base loss: 15973.83
[INFO 2017-06-30 10:36:32,133 main.py:52] epoch 10620, training loss: 4953.45, average training loss: 5313.20, base loss: 15973.43
[INFO 2017-06-30 10:36:35,321 main.py:52] epoch 10621, training loss: 5098.24, average training loss: 5312.81, base loss: 15973.26
[INFO 2017-06-30 10:36:38,490 main.py:52] epoch 10622, training loss: 5542.05, average training loss: 5312.92, base loss: 15973.70
[INFO 2017-06-30 10:36:41,675 main.py:52] epoch 10623, training loss: 5343.59, average training loss: 5312.82, base loss: 15973.65
[INFO 2017-06-30 10:36:44,796 main.py:52] epoch 10624, training loss: 5084.08, average training loss: 5312.25, base loss: 15973.45
[INFO 2017-06-30 10:36:47,953 main.py:52] epoch 10625, training loss: 5114.84, average training loss: 5312.04, base loss: 15973.46
[INFO 2017-06-30 10:36:51,077 main.py:52] epoch 10626, training loss: 4879.60, average training loss: 5311.80, base loss: 15973.26
[INFO 2017-06-30 10:36:54,222 main.py:52] epoch 10627, training loss: 4967.49, average training loss: 5311.71, base loss: 15973.10
[INFO 2017-06-30 10:36:57,370 main.py:52] epoch 10628, training loss: 5019.26, average training loss: 5311.43, base loss: 15973.08
[INFO 2017-06-30 10:37:00,488 main.py:52] epoch 10629, training loss: 4897.09, average training loss: 5310.63, base loss: 15972.90
[INFO 2017-06-30 10:37:03,662 main.py:52] epoch 10630, training loss: 5471.46, average training loss: 5310.88, base loss: 15972.95
[INFO 2017-06-30 10:37:06,810 main.py:52] epoch 10631, training loss: 5196.88, average training loss: 5311.01, base loss: 15972.81
[INFO 2017-06-30 10:37:09,949 main.py:52] epoch 10632, training loss: 5142.93, average training loss: 5311.06, base loss: 15972.82
[INFO 2017-06-30 10:37:13,073 main.py:52] epoch 10633, training loss: 5288.66, average training loss: 5311.22, base loss: 15972.61
[INFO 2017-06-30 10:37:16,217 main.py:52] epoch 10634, training loss: 5307.34, average training loss: 5311.32, base loss: 15972.62
[INFO 2017-06-30 10:37:19,370 main.py:52] epoch 10635, training loss: 4771.46, average training loss: 5310.55, base loss: 15972.42
[INFO 2017-06-30 10:37:22,534 main.py:52] epoch 10636, training loss: 5136.46, average training loss: 5310.49, base loss: 15972.24
[INFO 2017-06-30 10:37:25,676 main.py:52] epoch 10637, training loss: 5056.48, average training loss: 5310.16, base loss: 15972.13
[INFO 2017-06-30 10:37:28,807 main.py:52] epoch 10638, training loss: 5180.73, average training loss: 5309.68, base loss: 15971.89
[INFO 2017-06-30 10:37:31,914 main.py:52] epoch 10639, training loss: 5339.61, average training loss: 5309.30, base loss: 15971.89
[INFO 2017-06-30 10:37:35,044 main.py:52] epoch 10640, training loss: 5432.37, average training loss: 5309.43, base loss: 15971.93
[INFO 2017-06-30 10:37:38,227 main.py:52] epoch 10641, training loss: 4981.35, average training loss: 5308.97, base loss: 15971.62
[INFO 2017-06-30 10:37:41,432 main.py:52] epoch 10642, training loss: 6011.24, average training loss: 5309.68, base loss: 15972.02
[INFO 2017-06-30 10:37:44,557 main.py:52] epoch 10643, training loss: 5754.18, average training loss: 5310.15, base loss: 15972.17
[INFO 2017-06-30 10:37:47,696 main.py:52] epoch 10644, training loss: 5669.46, average training loss: 5310.43, base loss: 15972.54
[INFO 2017-06-30 10:37:50,825 main.py:52] epoch 10645, training loss: 5478.36, average training loss: 5310.75, base loss: 15972.48
[INFO 2017-06-30 10:37:53,945 main.py:52] epoch 10646, training loss: 5159.17, average training loss: 5310.45, base loss: 15972.19
[INFO 2017-06-30 10:37:57,086 main.py:52] epoch 10647, training loss: 5773.80, average training loss: 5310.83, base loss: 15972.71
[INFO 2017-06-30 10:38:00,247 main.py:52] epoch 10648, training loss: 5220.75, average training loss: 5311.05, base loss: 15972.68
[INFO 2017-06-30 10:38:03,416 main.py:52] epoch 10649, training loss: 5352.56, average training loss: 5310.93, base loss: 15972.89
[INFO 2017-06-30 10:38:06,555 main.py:52] epoch 10650, training loss: 5834.41, average training loss: 5311.59, base loss: 15973.12
[INFO 2017-06-30 10:38:09,718 main.py:52] epoch 10651, training loss: 5273.85, average training loss: 5311.69, base loss: 15973.45
[INFO 2017-06-30 10:38:12,857 main.py:52] epoch 10652, training loss: 5108.82, average training loss: 5311.23, base loss: 15973.49
[INFO 2017-06-30 10:38:15,951 main.py:52] epoch 10653, training loss: 5249.26, average training loss: 5311.11, base loss: 15973.62
[INFO 2017-06-30 10:38:19,095 main.py:52] epoch 10654, training loss: 5199.99, average training loss: 5311.00, base loss: 15973.73
[INFO 2017-06-30 10:38:22,253 main.py:52] epoch 10655, training loss: 5591.08, average training loss: 5311.33, base loss: 15974.07
[INFO 2017-06-30 10:38:25,405 main.py:52] epoch 10656, training loss: 5579.85, average training loss: 5311.25, base loss: 15974.45
[INFO 2017-06-30 10:38:28,543 main.py:52] epoch 10657, training loss: 5435.51, average training loss: 5311.79, base loss: 15974.64
[INFO 2017-06-30 10:38:31,691 main.py:52] epoch 10658, training loss: 5233.09, average training loss: 5311.39, base loss: 15974.39
[INFO 2017-06-30 10:38:34,838 main.py:52] epoch 10659, training loss: 4964.30, average training loss: 5310.83, base loss: 15974.30
[INFO 2017-06-30 10:38:37,993 main.py:52] epoch 10660, training loss: 5119.19, average training loss: 5310.64, base loss: 15974.21
[INFO 2017-06-30 10:38:41,144 main.py:52] epoch 10661, training loss: 5242.48, average training loss: 5311.27, base loss: 15974.32
[INFO 2017-06-30 10:38:44,307 main.py:52] epoch 10662, training loss: 5444.80, average training loss: 5311.32, base loss: 15974.65
[INFO 2017-06-30 10:38:47,445 main.py:52] epoch 10663, training loss: 5325.69, average training loss: 5311.29, base loss: 15974.62
[INFO 2017-06-30 10:38:50,578 main.py:52] epoch 10664, training loss: 4978.11, average training loss: 5310.83, base loss: 15974.53
[INFO 2017-06-30 10:38:53,718 main.py:52] epoch 10665, training loss: 5450.44, average training loss: 5310.84, base loss: 15974.70
[INFO 2017-06-30 10:38:56,890 main.py:52] epoch 10666, training loss: 5272.28, average training loss: 5311.02, base loss: 15974.57
[INFO 2017-06-30 10:39:00,055 main.py:52] epoch 10667, training loss: 5295.28, average training loss: 5311.05, base loss: 15974.42
[INFO 2017-06-30 10:39:03,200 main.py:52] epoch 10668, training loss: 5628.29, average training loss: 5311.55, base loss: 15974.62
[INFO 2017-06-30 10:39:06,385 main.py:52] epoch 10669, training loss: 4983.42, average training loss: 5311.33, base loss: 15974.10
[INFO 2017-06-30 10:39:09,588 main.py:52] epoch 10670, training loss: 5261.96, average training loss: 5311.01, base loss: 15974.12
[INFO 2017-06-30 10:39:12,696 main.py:52] epoch 10671, training loss: 4890.38, average training loss: 5310.94, base loss: 15973.91
[INFO 2017-06-30 10:39:15,819 main.py:52] epoch 10672, training loss: 5281.38, average training loss: 5311.14, base loss: 15973.67
[INFO 2017-06-30 10:39:18,964 main.py:52] epoch 10673, training loss: 5190.00, average training loss: 5310.93, base loss: 15973.44
[INFO 2017-06-30 10:39:22,101 main.py:52] epoch 10674, training loss: 5263.08, average training loss: 5311.11, base loss: 15973.52
[INFO 2017-06-30 10:39:25,262 main.py:52] epoch 10675, training loss: 5570.95, average training loss: 5311.38, base loss: 15973.58
[INFO 2017-06-30 10:39:28,420 main.py:52] epoch 10676, training loss: 5588.52, average training loss: 5311.86, base loss: 15973.57
[INFO 2017-06-30 10:39:31,594 main.py:52] epoch 10677, training loss: 5311.42, average training loss: 5311.45, base loss: 15973.40
[INFO 2017-06-30 10:39:34,744 main.py:52] epoch 10678, training loss: 5404.39, average training loss: 5311.99, base loss: 15973.32
[INFO 2017-06-30 10:39:37,889 main.py:52] epoch 10679, training loss: 5474.09, average training loss: 5312.10, base loss: 15973.23
[INFO 2017-06-30 10:39:41,055 main.py:52] epoch 10680, training loss: 5547.05, average training loss: 5312.22, base loss: 15973.32
[INFO 2017-06-30 10:39:44,243 main.py:52] epoch 10681, training loss: 5214.91, average training loss: 5312.14, base loss: 15973.47
[INFO 2017-06-30 10:39:47,342 main.py:52] epoch 10682, training loss: 5426.32, average training loss: 5312.34, base loss: 15973.34
[INFO 2017-06-30 10:39:50,449 main.py:52] epoch 10683, training loss: 5081.27, average training loss: 5312.40, base loss: 15973.23
[INFO 2017-06-30 10:39:53,584 main.py:52] epoch 10684, training loss: 5107.11, average training loss: 5312.25, base loss: 15973.34
[INFO 2017-06-30 10:39:56,752 main.py:52] epoch 10685, training loss: 5485.57, average training loss: 5312.38, base loss: 15973.63
[INFO 2017-06-30 10:39:59,843 main.py:52] epoch 10686, training loss: 5024.19, average training loss: 5312.29, base loss: 15973.41
[INFO 2017-06-30 10:40:02,978 main.py:52] epoch 10687, training loss: 5704.31, average training loss: 5312.79, base loss: 15973.37
[INFO 2017-06-30 10:40:06,109 main.py:52] epoch 10688, training loss: 5111.08, average training loss: 5312.73, base loss: 15973.22
[INFO 2017-06-30 10:40:09,272 main.py:52] epoch 10689, training loss: 5135.63, average training loss: 5312.70, base loss: 15973.13
[INFO 2017-06-30 10:40:12,448 main.py:52] epoch 10690, training loss: 5405.74, average training loss: 5313.03, base loss: 15973.45
[INFO 2017-06-30 10:40:15,604 main.py:52] epoch 10691, training loss: 5292.81, average training loss: 5313.38, base loss: 15973.76
[INFO 2017-06-30 10:40:18,753 main.py:52] epoch 10692, training loss: 5373.04, average training loss: 5313.40, base loss: 15973.51
[INFO 2017-06-30 10:40:21,867 main.py:52] epoch 10693, training loss: 5704.67, average training loss: 5314.00, base loss: 15973.99
[INFO 2017-06-30 10:40:24,993 main.py:52] epoch 10694, training loss: 5262.92, average training loss: 5314.51, base loss: 15974.04
[INFO 2017-06-30 10:40:28,168 main.py:52] epoch 10695, training loss: 5666.09, average training loss: 5314.98, base loss: 15974.33
[INFO 2017-06-30 10:40:31,322 main.py:52] epoch 10696, training loss: 5209.46, average training loss: 5314.72, base loss: 15974.26
[INFO 2017-06-30 10:40:34,490 main.py:52] epoch 10697, training loss: 5477.70, average training loss: 5314.84, base loss: 15974.56
[INFO 2017-06-30 10:40:37,606 main.py:52] epoch 10698, training loss: 5124.31, average training loss: 5314.88, base loss: 15974.37
[INFO 2017-06-30 10:40:40,760 main.py:52] epoch 10699, training loss: 5453.17, average training loss: 5314.95, base loss: 15974.71
[INFO 2017-06-30 10:40:40,761 main.py:54] epoch 10699, testing
[INFO 2017-06-30 10:40:53,981 main.py:97] average testing loss: 5317.69, base loss: 16154.25
[INFO 2017-06-30 10:40:53,982 main.py:98] improve_loss: 10836.56, improve_percent: 0.67
[INFO 2017-06-30 10:40:53,983 main.py:66] current best improved percent: 0.67
[INFO 2017-06-30 10:40:57,145 main.py:52] epoch 10700, training loss: 5038.18, average training loss: 5314.62, base loss: 15974.90
[INFO 2017-06-30 10:41:00,276 main.py:52] epoch 10701, training loss: 4931.37, average training loss: 5314.39, base loss: 15974.64
[INFO 2017-06-30 10:41:03,358 main.py:52] epoch 10702, training loss: 4875.69, average training loss: 5314.04, base loss: 15974.35
[INFO 2017-06-30 10:41:06,482 main.py:52] epoch 10703, training loss: 5104.36, average training loss: 5313.39, base loss: 15974.15
[INFO 2017-06-30 10:41:09,624 main.py:52] epoch 10704, training loss: 4978.00, average training loss: 5313.29, base loss: 15973.87
[INFO 2017-06-30 10:41:12,999 main.py:52] epoch 10705, training loss: 5646.00, average training loss: 5313.46, base loss: 15974.29
[INFO 2017-06-30 10:41:16,155 main.py:52] epoch 10706, training loss: 5289.35, average training loss: 5313.47, base loss: 15974.32
[INFO 2017-06-30 10:41:19,277 main.py:52] epoch 10707, training loss: 4989.28, average training loss: 5312.91, base loss: 15973.88
[INFO 2017-06-30 10:41:22,422 main.py:52] epoch 10708, training loss: 5777.17, average training loss: 5313.15, base loss: 15974.32
[INFO 2017-06-30 10:41:25,561 main.py:52] epoch 10709, training loss: 5415.59, average training loss: 5313.50, base loss: 15974.20
[INFO 2017-06-30 10:41:28,702 main.py:52] epoch 10710, training loss: 5319.05, average training loss: 5313.58, base loss: 15974.30
[INFO 2017-06-30 10:41:31,832 main.py:52] epoch 10711, training loss: 5627.93, average training loss: 5313.98, base loss: 15974.62
[INFO 2017-06-30 10:41:35,001 main.py:52] epoch 10712, training loss: 5288.99, average training loss: 5313.93, base loss: 15974.81
[INFO 2017-06-30 10:41:38,122 main.py:52] epoch 10713, training loss: 5741.73, average training loss: 5314.38, base loss: 15975.04
[INFO 2017-06-30 10:41:41,254 main.py:52] epoch 10714, training loss: 5369.09, average training loss: 5314.29, base loss: 15974.97
[INFO 2017-06-30 10:41:44,370 main.py:52] epoch 10715, training loss: 5647.60, average training loss: 5314.83, base loss: 15975.04
[INFO 2017-06-30 10:41:47,534 main.py:52] epoch 10716, training loss: 5372.21, average training loss: 5314.91, base loss: 15974.66
[INFO 2017-06-30 10:41:50,644 main.py:52] epoch 10717, training loss: 5346.21, average training loss: 5315.12, base loss: 15974.42
[INFO 2017-06-30 10:41:53,817 main.py:52] epoch 10718, training loss: 5067.24, average training loss: 5314.51, base loss: 15974.38
[INFO 2017-06-30 10:41:56,940 main.py:52] epoch 10719, training loss: 5146.78, average training loss: 5314.25, base loss: 15974.54
[INFO 2017-06-30 10:42:00,084 main.py:52] epoch 10720, training loss: 5262.23, average training loss: 5314.00, base loss: 15974.64
[INFO 2017-06-30 10:42:03,195 main.py:52] epoch 10721, training loss: 5094.26, average training loss: 5313.60, base loss: 15974.38
[INFO 2017-06-30 10:42:06,343 main.py:52] epoch 10722, training loss: 4815.03, average training loss: 5313.24, base loss: 15974.06
[INFO 2017-06-30 10:42:09,484 main.py:52] epoch 10723, training loss: 5284.91, average training loss: 5313.21, base loss: 15974.11
[INFO 2017-06-30 10:42:12,575 main.py:52] epoch 10724, training loss: 5551.01, average training loss: 5313.57, base loss: 15974.08
[INFO 2017-06-30 10:42:15,695 main.py:52] epoch 10725, training loss: 5177.17, average training loss: 5313.26, base loss: 15974.05
[INFO 2017-06-30 10:42:18,835 main.py:52] epoch 10726, training loss: 5208.46, average training loss: 5312.90, base loss: 15973.90
[INFO 2017-06-30 10:42:21,992 main.py:52] epoch 10727, training loss: 5368.78, average training loss: 5313.49, base loss: 15973.70
[INFO 2017-06-30 10:42:25,126 main.py:52] epoch 10728, training loss: 5247.26, average training loss: 5313.41, base loss: 15973.57
[INFO 2017-06-30 10:42:28,321 main.py:52] epoch 10729, training loss: 5541.13, average training loss: 5313.35, base loss: 15973.60
[INFO 2017-06-30 10:42:31,468 main.py:52] epoch 10730, training loss: 5048.93, average training loss: 5313.21, base loss: 15973.24
[INFO 2017-06-30 10:42:34,596 main.py:52] epoch 10731, training loss: 5174.32, average training loss: 5313.20, base loss: 15973.06
[INFO 2017-06-30 10:42:37,765 main.py:52] epoch 10732, training loss: 4972.04, average training loss: 5312.49, base loss: 15972.90
[INFO 2017-06-30 10:42:40,932 main.py:52] epoch 10733, training loss: 5320.49, average training loss: 5312.54, base loss: 15972.70
[INFO 2017-06-30 10:42:44,047 main.py:52] epoch 10734, training loss: 5395.73, average training loss: 5312.48, base loss: 15972.73
[INFO 2017-06-30 10:42:47,233 main.py:52] epoch 10735, training loss: 5118.67, average training loss: 5312.34, base loss: 15972.56
[INFO 2017-06-30 10:42:50,423 main.py:52] epoch 10736, training loss: 5374.95, average training loss: 5312.60, base loss: 15972.80
[INFO 2017-06-30 10:42:53,608 main.py:52] epoch 10737, training loss: 5110.54, average training loss: 5312.36, base loss: 15972.70
[INFO 2017-06-30 10:42:56,725 main.py:52] epoch 10738, training loss: 5041.12, average training loss: 5311.81, base loss: 15972.67
[INFO 2017-06-30 10:42:59,874 main.py:52] epoch 10739, training loss: 5522.73, average training loss: 5311.95, base loss: 15972.71
[INFO 2017-06-30 10:43:03,026 main.py:52] epoch 10740, training loss: 5671.65, average training loss: 5312.29, base loss: 15972.80
[INFO 2017-06-30 10:43:06,207 main.py:52] epoch 10741, training loss: 5248.89, average training loss: 5312.08, base loss: 15972.70
[INFO 2017-06-30 10:43:09,309 main.py:52] epoch 10742, training loss: 5318.05, average training loss: 5312.18, base loss: 15972.69
[INFO 2017-06-30 10:43:12,480 main.py:52] epoch 10743, training loss: 5330.31, average training loss: 5312.33, base loss: 15972.72
[INFO 2017-06-30 10:43:15,657 main.py:52] epoch 10744, training loss: 5623.94, average training loss: 5312.92, base loss: 15972.98
[INFO 2017-06-30 10:43:18,790 main.py:52] epoch 10745, training loss: 5501.46, average training loss: 5312.79, base loss: 15973.17
[INFO 2017-06-30 10:43:21,948 main.py:52] epoch 10746, training loss: 5178.99, average training loss: 5312.85, base loss: 15973.08
[INFO 2017-06-30 10:43:25,102 main.py:52] epoch 10747, training loss: 5142.42, average training loss: 5312.50, base loss: 15973.07
[INFO 2017-06-30 10:43:28,283 main.py:52] epoch 10748, training loss: 5385.43, average training loss: 5311.80, base loss: 15973.07
[INFO 2017-06-30 10:43:31,450 main.py:52] epoch 10749, training loss: 5464.80, average training loss: 5312.24, base loss: 15973.22
[INFO 2017-06-30 10:43:34,569 main.py:52] epoch 10750, training loss: 4943.51, average training loss: 5311.86, base loss: 15972.95
[INFO 2017-06-30 10:43:37,697 main.py:52] epoch 10751, training loss: 5554.64, average training loss: 5312.34, base loss: 15973.05
[INFO 2017-06-30 10:43:40,851 main.py:52] epoch 10752, training loss: 5222.38, average training loss: 5312.08, base loss: 15972.89
[INFO 2017-06-30 10:43:43,967 main.py:52] epoch 10753, training loss: 5444.37, average training loss: 5312.44, base loss: 15972.96
[INFO 2017-06-30 10:43:47,122 main.py:52] epoch 10754, training loss: 5322.48, average training loss: 5312.64, base loss: 15973.10
[INFO 2017-06-30 10:43:50,253 main.py:52] epoch 10755, training loss: 5137.38, average training loss: 5312.75, base loss: 15973.05
[INFO 2017-06-30 10:43:53,414 main.py:52] epoch 10756, training loss: 5300.67, average training loss: 5313.27, base loss: 15973.11
[INFO 2017-06-30 10:43:56,549 main.py:52] epoch 10757, training loss: 5428.41, average training loss: 5313.32, base loss: 15973.12
[INFO 2017-06-30 10:43:59,668 main.py:52] epoch 10758, training loss: 5273.48, average training loss: 5313.34, base loss: 15973.20
[INFO 2017-06-30 10:44:02,781 main.py:52] epoch 10759, training loss: 5300.98, average training loss: 5313.42, base loss: 15973.19
[INFO 2017-06-30 10:44:05,909 main.py:52] epoch 10760, training loss: 5300.65, average training loss: 5313.46, base loss: 15973.25
[INFO 2017-06-30 10:44:09,116 main.py:52] epoch 10761, training loss: 5389.10, average training loss: 5313.46, base loss: 15973.25
[INFO 2017-06-30 10:44:12,213 main.py:52] epoch 10762, training loss: 4966.32, average training loss: 5313.14, base loss: 15972.97
[INFO 2017-06-30 10:44:15,359 main.py:52] epoch 10763, training loss: 4897.47, average training loss: 5312.65, base loss: 15972.73
[INFO 2017-06-30 10:44:18,522 main.py:52] epoch 10764, training loss: 5335.09, average training loss: 5312.53, base loss: 15972.85
[INFO 2017-06-30 10:44:21,710 main.py:52] epoch 10765, training loss: 5210.27, average training loss: 5312.96, base loss: 15972.97
[INFO 2017-06-30 10:44:24,838 main.py:52] epoch 10766, training loss: 5452.71, average training loss: 5313.13, base loss: 15973.07
[INFO 2017-06-30 10:44:27,931 main.py:52] epoch 10767, training loss: 4927.18, average training loss: 5312.81, base loss: 15972.90
[INFO 2017-06-30 10:44:31,057 main.py:52] epoch 10768, training loss: 5295.49, average training loss: 5312.66, base loss: 15972.76
[INFO 2017-06-30 10:44:34,195 main.py:52] epoch 10769, training loss: 4923.59, average training loss: 5312.33, base loss: 15972.33
[INFO 2017-06-30 10:44:37,372 main.py:52] epoch 10770, training loss: 5620.31, average training loss: 5312.75, base loss: 15972.35
[INFO 2017-06-30 10:44:40,522 main.py:52] epoch 10771, training loss: 5335.23, average training loss: 5312.74, base loss: 15972.37
[INFO 2017-06-30 10:44:43,662 main.py:52] epoch 10772, training loss: 5200.35, average training loss: 5312.53, base loss: 15972.61
[INFO 2017-06-30 10:44:46,778 main.py:52] epoch 10773, training loss: 5059.47, average training loss: 5312.18, base loss: 15972.54
[INFO 2017-06-30 10:44:49,944 main.py:52] epoch 10774, training loss: 5053.21, average training loss: 5312.02, base loss: 15972.55
[INFO 2017-06-30 10:44:53,098 main.py:52] epoch 10775, training loss: 5292.48, average training loss: 5312.38, base loss: 15972.96
[INFO 2017-06-30 10:44:56,199 main.py:52] epoch 10776, training loss: 5527.38, average training loss: 5312.30, base loss: 15973.25
[INFO 2017-06-30 10:44:59,367 main.py:52] epoch 10777, training loss: 5632.74, average training loss: 5312.84, base loss: 15973.26
[INFO 2017-06-30 10:45:02,525 main.py:52] epoch 10778, training loss: 5125.73, average training loss: 5312.85, base loss: 15973.18
[INFO 2017-06-30 10:45:05,649 main.py:52] epoch 10779, training loss: 5202.70, average training loss: 5312.89, base loss: 15973.20
[INFO 2017-06-30 10:45:08,765 main.py:52] epoch 10780, training loss: 5260.14, average training loss: 5312.83, base loss: 15973.09
[INFO 2017-06-30 10:45:11,910 main.py:52] epoch 10781, training loss: 5061.88, average training loss: 5312.30, base loss: 15973.14
[INFO 2017-06-30 10:45:15,041 main.py:52] epoch 10782, training loss: 5275.00, average training loss: 5312.00, base loss: 15973.28
[INFO 2017-06-30 10:45:18,169 main.py:52] epoch 10783, training loss: 4919.79, average training loss: 5311.42, base loss: 15973.10
[INFO 2017-06-30 10:45:21,332 main.py:52] epoch 10784, training loss: 5038.96, average training loss: 5311.36, base loss: 15973.14
[INFO 2017-06-30 10:45:24,457 main.py:52] epoch 10785, training loss: 5250.52, average training loss: 5311.49, base loss: 15973.30
[INFO 2017-06-30 10:45:27,594 main.py:52] epoch 10786, training loss: 5460.36, average training loss: 5311.49, base loss: 15973.47
[INFO 2017-06-30 10:45:30,693 main.py:52] epoch 10787, training loss: 5053.07, average training loss: 5311.11, base loss: 15973.35
[INFO 2017-06-30 10:45:33,873 main.py:52] epoch 10788, training loss: 5458.26, average training loss: 5311.45, base loss: 15973.38
[INFO 2017-06-30 10:45:37,025 main.py:52] epoch 10789, training loss: 5710.42, average training loss: 5311.74, base loss: 15973.70
[INFO 2017-06-30 10:45:40,136 main.py:52] epoch 10790, training loss: 5600.52, average training loss: 5312.10, base loss: 15974.13
[INFO 2017-06-30 10:45:43,274 main.py:52] epoch 10791, training loss: 5432.83, average training loss: 5311.70, base loss: 15974.34
[INFO 2017-06-30 10:45:46,427 main.py:52] epoch 10792, training loss: 5169.76, average training loss: 5311.40, base loss: 15974.23
[INFO 2017-06-30 10:45:49,579 main.py:52] epoch 10793, training loss: 5317.99, average training loss: 5311.38, base loss: 15974.36
[INFO 2017-06-30 10:45:52,728 main.py:52] epoch 10794, training loss: 5558.24, average training loss: 5311.12, base loss: 15974.47
[INFO 2017-06-30 10:45:55,898 main.py:52] epoch 10795, training loss: 5186.48, average training loss: 5311.12, base loss: 15974.27
[INFO 2017-06-30 10:45:59,023 main.py:52] epoch 10796, training loss: 5089.03, average training loss: 5310.93, base loss: 15974.39
[INFO 2017-06-30 10:46:02,158 main.py:52] epoch 10797, training loss: 5734.05, average training loss: 5310.87, base loss: 15975.03
[INFO 2017-06-30 10:46:05,270 main.py:52] epoch 10798, training loss: 5290.85, average training loss: 5310.39, base loss: 15974.96
[INFO 2017-06-30 10:46:08,402 main.py:52] epoch 10799, training loss: 5262.72, average training loss: 5310.18, base loss: 15974.89
[INFO 2017-06-30 10:46:08,402 main.py:54] epoch 10799, testing
[INFO 2017-06-30 10:46:21,609 main.py:97] average testing loss: 5184.19, base loss: 15544.41
[INFO 2017-06-30 10:46:21,609 main.py:98] improve_loss: 10360.22, improve_percent: 0.67
[INFO 2017-06-30 10:46:21,611 main.py:66] current best improved percent: 0.67
[INFO 2017-06-30 10:46:24,728 main.py:52] epoch 10800, training loss: 5348.31, average training loss: 5310.46, base loss: 15974.76
[INFO 2017-06-30 10:46:27,927 main.py:52] epoch 10801, training loss: 5631.97, average training loss: 5310.74, base loss: 15974.89
[INFO 2017-06-30 10:46:31,269 main.py:52] epoch 10802, training loss: 5183.41, average training loss: 5310.66, base loss: 15975.03
[INFO 2017-06-30 10:46:34,391 main.py:52] epoch 10803, training loss: 5240.66, average training loss: 5310.65, base loss: 15975.17
[INFO 2017-06-30 10:46:37,539 main.py:52] epoch 10804, training loss: 5130.06, average training loss: 5310.02, base loss: 15975.28
[INFO 2017-06-30 10:46:40,659 main.py:52] epoch 10805, training loss: 5296.29, average training loss: 5310.18, base loss: 15975.58
[INFO 2017-06-30 10:46:43,856 main.py:52] epoch 10806, training loss: 5414.27, average training loss: 5310.23, base loss: 15975.77
[INFO 2017-06-30 10:46:46,995 main.py:52] epoch 10807, training loss: 5167.38, average training loss: 5309.91, base loss: 15975.88
[INFO 2017-06-30 10:46:50,206 main.py:52] epoch 10808, training loss: 5449.63, average training loss: 5310.30, base loss: 15975.73
[INFO 2017-06-30 10:46:53,358 main.py:52] epoch 10809, training loss: 5051.90, average training loss: 5309.80, base loss: 15975.42
[INFO 2017-06-30 10:46:56,531 main.py:52] epoch 10810, training loss: 5202.17, average training loss: 5309.73, base loss: 15975.17
[INFO 2017-06-30 10:46:59,661 main.py:52] epoch 10811, training loss: 5473.28, average training loss: 5310.10, base loss: 15975.12
[INFO 2017-06-30 10:47:02,844 main.py:52] epoch 10812, training loss: 4914.66, average training loss: 5309.63, base loss: 15975.05
[INFO 2017-06-30 10:47:05,985 main.py:52] epoch 10813, training loss: 5515.47, average training loss: 5309.66, base loss: 15974.91
[INFO 2017-06-30 10:47:09,147 main.py:52] epoch 10814, training loss: 5409.65, average training loss: 5309.92, base loss: 15974.92
[INFO 2017-06-30 10:47:12,297 main.py:52] epoch 10815, training loss: 5516.66, average training loss: 5310.28, base loss: 15975.24
[INFO 2017-06-30 10:47:15,488 main.py:52] epoch 10816, training loss: 5375.85, average training loss: 5310.37, base loss: 15975.32
[INFO 2017-06-30 10:47:18,611 main.py:52] epoch 10817, training loss: 5198.71, average training loss: 5310.31, base loss: 15975.33
[INFO 2017-06-30 10:47:21,712 main.py:52] epoch 10818, training loss: 5285.14, average training loss: 5310.26, base loss: 15975.55
[INFO 2017-06-30 10:47:24,879 main.py:52] epoch 10819, training loss: 5570.77, average training loss: 5310.84, base loss: 15976.14
[INFO 2017-06-30 10:47:28,015 main.py:52] epoch 10820, training loss: 5271.17, average training loss: 5310.56, base loss: 15976.28
[INFO 2017-06-30 10:47:31,188 main.py:52] epoch 10821, training loss: 5793.96, average training loss: 5310.78, base loss: 15976.70
[INFO 2017-06-30 10:47:34,396 main.py:52] epoch 10822, training loss: 5442.24, average training loss: 5310.83, base loss: 15976.74
[INFO 2017-06-30 10:47:37,562 main.py:52] epoch 10823, training loss: 5098.67, average training loss: 5309.93, base loss: 15976.44
[INFO 2017-06-30 10:47:40,672 main.py:52] epoch 10824, training loss: 5509.76, average training loss: 5310.53, base loss: 15976.75
[INFO 2017-06-30 10:47:43,841 main.py:52] epoch 10825, training loss: 5121.37, average training loss: 5310.46, base loss: 15976.77
[INFO 2017-06-30 10:47:46,963 main.py:52] epoch 10826, training loss: 5214.40, average training loss: 5310.43, base loss: 15976.63
[INFO 2017-06-30 10:47:50,066 main.py:52] epoch 10827, training loss: 5156.60, average training loss: 5310.02, base loss: 15976.47
[INFO 2017-06-30 10:47:53,194 main.py:52] epoch 10828, training loss: 5328.21, average training loss: 5310.07, base loss: 15976.44
[INFO 2017-06-30 10:47:56,303 main.py:52] epoch 10829, training loss: 5349.45, average training loss: 5310.27, base loss: 15976.59
[INFO 2017-06-30 10:47:59,451 main.py:52] epoch 10830, training loss: 5358.95, average training loss: 5310.40, base loss: 15976.66
[INFO 2017-06-30 10:48:02,565 main.py:52] epoch 10831, training loss: 5089.99, average training loss: 5310.42, base loss: 15976.67
[INFO 2017-06-30 10:48:05,723 main.py:52] epoch 10832, training loss: 5087.07, average training loss: 5310.14, base loss: 15976.45
[INFO 2017-06-30 10:48:08,843 main.py:52] epoch 10833, training loss: 5197.72, average training loss: 5309.60, base loss: 15976.29
[INFO 2017-06-30 10:48:11,995 main.py:52] epoch 10834, training loss: 5456.82, average training loss: 5309.89, base loss: 15976.42
[INFO 2017-06-30 10:48:15,158 main.py:52] epoch 10835, training loss: 5142.98, average training loss: 5309.92, base loss: 15976.67
[INFO 2017-06-30 10:48:18,260 main.py:52] epoch 10836, training loss: 5460.28, average training loss: 5309.88, base loss: 15977.03
[INFO 2017-06-30 10:48:21,413 main.py:52] epoch 10837, training loss: 5130.08, average training loss: 5309.58, base loss: 15976.90
[INFO 2017-06-30 10:48:24,567 main.py:52] epoch 10838, training loss: 5181.76, average training loss: 5309.24, base loss: 15976.55
[INFO 2017-06-30 10:48:27,684 main.py:52] epoch 10839, training loss: 5334.08, average training loss: 5308.87, base loss: 15976.70
[INFO 2017-06-30 10:48:30,834 main.py:52] epoch 10840, training loss: 5014.21, average training loss: 5308.63, base loss: 15976.67
[INFO 2017-06-30 10:48:33,959 main.py:52] epoch 10841, training loss: 5248.00, average training loss: 5308.61, base loss: 15976.68
[INFO 2017-06-30 10:48:37,080 main.py:52] epoch 10842, training loss: 5240.80, average training loss: 5308.59, base loss: 15976.69
[INFO 2017-06-30 10:48:40,201 main.py:52] epoch 10843, training loss: 4798.75, average training loss: 5308.06, base loss: 15976.36
[INFO 2017-06-30 10:48:43,329 main.py:52] epoch 10844, training loss: 5441.48, average training loss: 5308.23, base loss: 15976.26
[INFO 2017-06-30 10:48:46,451 main.py:52] epoch 10845, training loss: 5515.89, average training loss: 5308.08, base loss: 15976.48
[INFO 2017-06-30 10:48:49,610 main.py:52] epoch 10846, training loss: 5432.57, average training loss: 5307.95, base loss: 15976.75
[INFO 2017-06-30 10:48:52,760 main.py:52] epoch 10847, training loss: 5054.93, average training loss: 5307.50, base loss: 15976.86
[INFO 2017-06-30 10:48:55,897 main.py:52] epoch 10848, training loss: 5382.24, average training loss: 5307.36, base loss: 15977.13
[INFO 2017-06-30 10:48:59,040 main.py:52] epoch 10849, training loss: 5192.77, average training loss: 5307.16, base loss: 15977.21
[INFO 2017-06-30 10:49:02,166 main.py:52] epoch 10850, training loss: 5595.77, average training loss: 5307.21, base loss: 15977.54
[INFO 2017-06-30 10:49:05,305 main.py:52] epoch 10851, training loss: 5300.13, average training loss: 5307.24, base loss: 15977.54
[INFO 2017-06-30 10:49:08,424 main.py:52] epoch 10852, training loss: 5396.56, average training loss: 5307.40, base loss: 15977.66
[INFO 2017-06-30 10:49:11,598 main.py:52] epoch 10853, training loss: 5092.95, average training loss: 5306.87, base loss: 15977.57
[INFO 2017-06-30 10:49:14,773 main.py:52] epoch 10854, training loss: 5152.36, average training loss: 5306.78, base loss: 15977.56
[INFO 2017-06-30 10:49:17,884 main.py:52] epoch 10855, training loss: 5235.25, average training loss: 5306.53, base loss: 15977.57
[INFO 2017-06-30 10:49:20,991 main.py:52] epoch 10856, training loss: 5131.62, average training loss: 5306.28, base loss: 15977.60
[INFO 2017-06-30 10:49:24,136 main.py:52] epoch 10857, training loss: 5386.63, average training loss: 5306.52, base loss: 15977.46
[INFO 2017-06-30 10:49:27,309 main.py:52] epoch 10858, training loss: 5441.82, average training loss: 5306.98, base loss: 15977.47
[INFO 2017-06-30 10:49:30,446 main.py:52] epoch 10859, training loss: 5511.39, average training loss: 5307.24, base loss: 15977.42
[INFO 2017-06-30 10:49:33,606 main.py:52] epoch 10860, training loss: 5415.35, average training loss: 5306.86, base loss: 15977.46
[INFO 2017-06-30 10:49:36,749 main.py:52] epoch 10861, training loss: 5348.15, average training loss: 5306.98, base loss: 15977.45
[INFO 2017-06-30 10:49:39,943 main.py:52] epoch 10862, training loss: 5670.39, average training loss: 5307.14, base loss: 15977.63
[INFO 2017-06-30 10:49:43,090 main.py:52] epoch 10863, training loss: 5108.45, average training loss: 5307.20, base loss: 15977.48
[INFO 2017-06-30 10:49:46,259 main.py:52] epoch 10864, training loss: 5049.12, average training loss: 5306.75, base loss: 15977.27
[INFO 2017-06-30 10:49:49,405 main.py:52] epoch 10865, training loss: 5570.85, average training loss: 5306.86, base loss: 15977.38
[INFO 2017-06-30 10:49:52,523 main.py:52] epoch 10866, training loss: 5074.76, average training loss: 5306.83, base loss: 15977.10
[INFO 2017-06-30 10:49:55,682 main.py:52] epoch 10867, training loss: 5354.12, average training loss: 5306.57, base loss: 15977.12
[INFO 2017-06-30 10:49:58,833 main.py:52] epoch 10868, training loss: 5526.33, average training loss: 5307.01, base loss: 15977.35
[INFO 2017-06-30 10:50:01,967 main.py:52] epoch 10869, training loss: 5460.04, average training loss: 5306.92, base loss: 15977.36
[INFO 2017-06-30 10:50:05,133 main.py:52] epoch 10870, training loss: 5136.92, average training loss: 5306.90, base loss: 15977.06
[INFO 2017-06-30 10:50:08,248 main.py:52] epoch 10871, training loss: 5548.60, average training loss: 5307.07, base loss: 15977.40
[INFO 2017-06-30 10:50:11,382 main.py:52] epoch 10872, training loss: 5099.59, average training loss: 5306.61, base loss: 15977.19
[INFO 2017-06-30 10:50:14,558 main.py:52] epoch 10873, training loss: 5172.05, average training loss: 5306.58, base loss: 15976.77
[INFO 2017-06-30 10:50:17,677 main.py:52] epoch 10874, training loss: 5279.43, average training loss: 5306.54, base loss: 15976.59
[INFO 2017-06-30 10:50:20,813 main.py:52] epoch 10875, training loss: 5135.98, average training loss: 5306.64, base loss: 15976.37
[INFO 2017-06-30 10:50:23,940 main.py:52] epoch 10876, training loss: 5399.75, average training loss: 5306.73, base loss: 15976.49
[INFO 2017-06-30 10:50:27,109 main.py:52] epoch 10877, training loss: 5255.45, average training loss: 5306.58, base loss: 15976.70
[INFO 2017-06-30 10:50:30,209 main.py:52] epoch 10878, training loss: 5460.31, average training loss: 5306.91, base loss: 15976.91
[INFO 2017-06-30 10:50:33,361 main.py:52] epoch 10879, training loss: 4991.96, average training loss: 5306.71, base loss: 15977.09
[INFO 2017-06-30 10:50:36,516 main.py:52] epoch 10880, training loss: 5680.11, average training loss: 5306.75, base loss: 15977.29
[INFO 2017-06-30 10:50:39,645 main.py:52] epoch 10881, training loss: 5482.23, average training loss: 5307.22, base loss: 15977.33
[INFO 2017-06-30 10:50:42,820 main.py:52] epoch 10882, training loss: 5117.08, average training loss: 5306.82, base loss: 15977.05
[INFO 2017-06-30 10:50:45,941 main.py:52] epoch 10883, training loss: 5128.17, average training loss: 5306.59, base loss: 15976.71
[INFO 2017-06-30 10:50:49,093 main.py:52] epoch 10884, training loss: 5182.34, average training loss: 5306.42, base loss: 15976.74
[INFO 2017-06-30 10:50:52,243 main.py:52] epoch 10885, training loss: 5280.58, average training loss: 5306.61, base loss: 15976.69
[INFO 2017-06-30 10:50:55,398 main.py:52] epoch 10886, training loss: 5307.59, average training loss: 5306.08, base loss: 15976.55
[INFO 2017-06-30 10:50:58,557 main.py:52] epoch 10887, training loss: 4871.30, average training loss: 5305.38, base loss: 15976.15
[INFO 2017-06-30 10:51:01,697 main.py:52] epoch 10888, training loss: 5462.49, average training loss: 5305.65, base loss: 15976.29
[INFO 2017-06-30 10:51:04,856 main.py:52] epoch 10889, training loss: 5172.77, average training loss: 5305.68, base loss: 15976.33
[INFO 2017-06-30 10:51:08,036 main.py:52] epoch 10890, training loss: 5229.94, average training loss: 5305.28, base loss: 15976.28
[INFO 2017-06-30 10:51:11,211 main.py:52] epoch 10891, training loss: 5146.79, average training loss: 5304.58, base loss: 15975.95
[INFO 2017-06-30 10:51:14,320 main.py:52] epoch 10892, training loss: 5330.87, average training loss: 5304.97, base loss: 15976.04
[INFO 2017-06-30 10:51:17,439 main.py:52] epoch 10893, training loss: 5238.12, average training loss: 5304.97, base loss: 15976.12
[INFO 2017-06-30 10:51:20,587 main.py:52] epoch 10894, training loss: 5238.71, average training loss: 5304.55, base loss: 15975.66
[INFO 2017-06-30 10:51:23,747 main.py:52] epoch 10895, training loss: 5207.52, average training loss: 5304.53, base loss: 15975.30
[INFO 2017-06-30 10:51:26,886 main.py:52] epoch 10896, training loss: 5255.76, average training loss: 5304.22, base loss: 15975.43
[INFO 2017-06-30 10:51:30,050 main.py:52] epoch 10897, training loss: 5382.43, average training loss: 5304.18, base loss: 15975.56
[INFO 2017-06-30 10:51:33,197 main.py:52] epoch 10898, training loss: 5443.06, average training loss: 5304.38, base loss: 15975.88
[INFO 2017-06-30 10:51:36,344 main.py:52] epoch 10899, training loss: 5688.01, average training loss: 5304.42, base loss: 15976.34
[INFO 2017-06-30 10:51:36,345 main.py:54] epoch 10899, testing
[INFO 2017-06-30 10:51:49,385 main.py:97] average testing loss: 5187.18, base loss: 15218.94
[INFO 2017-06-30 10:51:49,385 main.py:98] improve_loss: 10031.77, improve_percent: 0.66
[INFO 2017-06-30 10:51:49,388 main.py:66] current best improved percent: 0.67
[INFO 2017-06-30 10:51:52,720 main.py:52] epoch 10900, training loss: 5230.52, average training loss: 5304.53, base loss: 15976.29
[INFO 2017-06-30 10:51:55,845 main.py:52] epoch 10901, training loss: 5463.54, average training loss: 5304.54, base loss: 15976.57
[INFO 2017-06-30 10:51:59,013 main.py:52] epoch 10902, training loss: 5709.41, average training loss: 5305.21, base loss: 15976.81
[INFO 2017-06-30 10:52:02,176 main.py:52] epoch 10903, training loss: 5127.35, average training loss: 5305.38, base loss: 15976.86
[INFO 2017-06-30 10:52:05,285 main.py:52] epoch 10904, training loss: 5082.51, average training loss: 5305.26, base loss: 15976.76
[INFO 2017-06-30 10:52:08,442 main.py:52] epoch 10905, training loss: 5115.24, average training loss: 5304.88, base loss: 15976.55
[INFO 2017-06-30 10:52:11,560 main.py:52] epoch 10906, training loss: 5301.16, average training loss: 5305.01, base loss: 15976.54
[INFO 2017-06-30 10:52:14,707 main.py:52] epoch 10907, training loss: 5342.82, average training loss: 5305.15, base loss: 15976.54
[INFO 2017-06-30 10:52:17,880 main.py:52] epoch 10908, training loss: 5549.90, average training loss: 5305.24, base loss: 15976.63
[INFO 2017-06-30 10:52:21,047 main.py:52] epoch 10909, training loss: 5705.30, average training loss: 5305.82, base loss: 15977.05
[INFO 2017-06-30 10:52:24,219 main.py:52] epoch 10910, training loss: 5426.11, average training loss: 5305.98, base loss: 15977.34
[INFO 2017-06-30 10:52:27,387 main.py:52] epoch 10911, training loss: 5340.02, average training loss: 5305.98, base loss: 15977.50
[INFO 2017-06-30 10:52:30,505 main.py:52] epoch 10912, training loss: 5254.25, average training loss: 5305.94, base loss: 15977.79
[INFO 2017-06-30 10:52:33,604 main.py:52] epoch 10913, training loss: 5276.40, average training loss: 5305.68, base loss: 15977.98
[INFO 2017-06-30 10:52:36,729 main.py:52] epoch 10914, training loss: 5098.98, average training loss: 5305.48, base loss: 15977.83
[INFO 2017-06-30 10:52:39,904 main.py:52] epoch 10915, training loss: 5477.27, average training loss: 5305.41, base loss: 15978.07
[INFO 2017-06-30 10:52:43,050 main.py:52] epoch 10916, training loss: 5328.55, average training loss: 5305.43, base loss: 15978.20
[INFO 2017-06-30 10:52:46,161 main.py:52] epoch 10917, training loss: 5246.88, average training loss: 5305.50, base loss: 15978.25
[INFO 2017-06-30 10:52:49,312 main.py:52] epoch 10918, training loss: 4986.40, average training loss: 5304.86, base loss: 15978.33
[INFO 2017-06-30 10:52:52,480 main.py:52] epoch 10919, training loss: 5592.44, average training loss: 5305.36, base loss: 15978.53
[INFO 2017-06-30 10:52:55,648 main.py:52] epoch 10920, training loss: 5254.24, average training loss: 5305.21, base loss: 15978.35
[INFO 2017-06-30 10:52:58,786 main.py:52] epoch 10921, training loss: 5253.41, average training loss: 5305.19, base loss: 15978.18
[INFO 2017-06-30 10:53:01,925 main.py:52] epoch 10922, training loss: 5082.17, average training loss: 5305.20, base loss: 15977.96
[INFO 2017-06-30 10:53:05,071 main.py:52] epoch 10923, training loss: 5235.89, average training loss: 5305.02, base loss: 15977.89
[INFO 2017-06-30 10:53:08,251 main.py:52] epoch 10924, training loss: 5408.12, average training loss: 5304.96, base loss: 15978.04
[INFO 2017-06-30 10:53:11,372 main.py:52] epoch 10925, training loss: 5597.38, average training loss: 5305.10, base loss: 15978.19
[INFO 2017-06-30 10:53:14,522 main.py:52] epoch 10926, training loss: 5269.30, average training loss: 5304.74, base loss: 15978.17
[INFO 2017-06-30 10:53:17,664 main.py:52] epoch 10927, training loss: 5613.88, average training loss: 5305.03, base loss: 15978.34
[INFO 2017-06-30 10:53:20,799 main.py:52] epoch 10928, training loss: 6055.51, average training loss: 5306.14, base loss: 15978.95
[INFO 2017-06-30 10:53:23,931 main.py:52] epoch 10929, training loss: 5215.80, average training loss: 5306.36, base loss: 15978.78
[INFO 2017-06-30 10:53:27,031 main.py:52] epoch 10930, training loss: 5401.80, average training loss: 5306.58, base loss: 15978.61
[INFO 2017-06-30 10:53:30,199 main.py:52] epoch 10931, training loss: 5131.45, average training loss: 5306.67, base loss: 15978.42
[INFO 2017-06-30 10:53:33,389 main.py:52] epoch 10932, training loss: 5422.04, average training loss: 5306.97, base loss: 15978.58
[INFO 2017-06-30 10:53:36,553 main.py:52] epoch 10933, training loss: 5332.01, average training loss: 5306.87, base loss: 15978.80
[INFO 2017-06-30 10:53:39,702 main.py:52] epoch 10934, training loss: 5306.68, average training loss: 5306.66, base loss: 15978.89
[INFO 2017-06-30 10:53:42,869 main.py:52] epoch 10935, training loss: 5346.10, average training loss: 5306.56, base loss: 15978.99
[INFO 2017-06-30 10:53:45,977 main.py:52] epoch 10936, training loss: 5502.11, average training loss: 5306.85, base loss: 15979.13
[INFO 2017-06-30 10:53:49,157 main.py:52] epoch 10937, training loss: 5433.06, average training loss: 5307.04, base loss: 15979.14
[INFO 2017-06-30 10:53:52,342 main.py:52] epoch 10938, training loss: 5213.55, average training loss: 5307.04, base loss: 15979.18
[INFO 2017-06-30 10:53:55,461 main.py:52] epoch 10939, training loss: 5249.27, average training loss: 5307.09, base loss: 15978.85
[INFO 2017-06-30 10:53:58,586 main.py:52] epoch 10940, training loss: 5134.98, average training loss: 5306.93, base loss: 15978.28
[INFO 2017-06-30 10:54:01,724 main.py:52] epoch 10941, training loss: 5268.91, average training loss: 5306.89, base loss: 15978.14
[INFO 2017-06-30 10:54:04,868 main.py:52] epoch 10942, training loss: 5566.64, average training loss: 5307.52, base loss: 15978.19
[INFO 2017-06-30 10:54:07,983 main.py:52] epoch 10943, training loss: 5421.56, average training loss: 5307.60, base loss: 15978.10
[INFO 2017-06-30 10:54:11,090 main.py:52] epoch 10944, training loss: 5336.73, average training loss: 5307.55, base loss: 15978.09
[INFO 2017-06-30 10:54:14,246 main.py:52] epoch 10945, training loss: 5108.65, average training loss: 5307.02, base loss: 15978.17
[INFO 2017-06-30 10:54:17,353 main.py:52] epoch 10946, training loss: 5454.95, average training loss: 5307.38, base loss: 15978.40
[INFO 2017-06-30 10:54:20,499 main.py:52] epoch 10947, training loss: 5450.90, average training loss: 5307.64, base loss: 15978.59
[INFO 2017-06-30 10:54:23,637 main.py:52] epoch 10948, training loss: 5276.24, average training loss: 5307.67, base loss: 15978.52
[INFO 2017-06-30 10:54:26,802 main.py:52] epoch 10949, training loss: 5362.58, average training loss: 5307.44, base loss: 15978.71
[INFO 2017-06-30 10:54:29,938 main.py:52] epoch 10950, training loss: 5475.67, average training loss: 5307.89, base loss: 15978.71
[INFO 2017-06-30 10:54:33,098 main.py:52] epoch 10951, training loss: 5018.58, average training loss: 5307.68, base loss: 15978.40
[INFO 2017-06-30 10:54:36,261 main.py:52] epoch 10952, training loss: 5178.82, average training loss: 5307.56, base loss: 15978.14
[INFO 2017-06-30 10:54:39,415 main.py:52] epoch 10953, training loss: 5357.38, average training loss: 5307.17, base loss: 15978.17
[INFO 2017-06-30 10:54:42,603 main.py:52] epoch 10954, training loss: 5347.64, average training loss: 5307.11, base loss: 15978.31
[INFO 2017-06-30 10:54:45,806 main.py:52] epoch 10955, training loss: 5481.16, average training loss: 5307.10, base loss: 15978.34
[INFO 2017-06-30 10:54:48,939 main.py:52] epoch 10956, training loss: 5409.92, average training loss: 5307.14, base loss: 15978.78
[INFO 2017-06-30 10:54:52,056 main.py:52] epoch 10957, training loss: 5271.83, average training loss: 5306.85, base loss: 15979.02
[INFO 2017-06-30 10:54:55,210 main.py:52] epoch 10958, training loss: 5216.07, average training loss: 5306.57, base loss: 15978.98
[INFO 2017-06-30 10:54:58,341 main.py:52] epoch 10959, training loss: 5172.17, average training loss: 5306.02, base loss: 15978.89
[INFO 2017-06-30 10:55:01,458 main.py:52] epoch 10960, training loss: 5330.55, average training loss: 5306.07, base loss: 15978.90
[INFO 2017-06-30 10:55:04,661 main.py:52] epoch 10961, training loss: 4990.46, average training loss: 5305.53, base loss: 15979.07
[INFO 2017-06-30 10:55:07,828 main.py:52] epoch 10962, training loss: 5592.33, average training loss: 5305.72, base loss: 15979.28
[INFO 2017-06-30 10:55:10,957 main.py:52] epoch 10963, training loss: 5271.26, average training loss: 5305.64, base loss: 15979.26
[INFO 2017-06-30 10:55:14,143 main.py:52] epoch 10964, training loss: 5051.43, average training loss: 5305.44, base loss: 15979.22
[INFO 2017-06-30 10:55:17,281 main.py:52] epoch 10965, training loss: 5098.64, average training loss: 5305.23, base loss: 15979.35
[INFO 2017-06-30 10:55:20,410 main.py:52] epoch 10966, training loss: 5334.85, average training loss: 5305.33, base loss: 15979.35
[INFO 2017-06-30 10:55:23,596 main.py:52] epoch 10967, training loss: 5694.77, average training loss: 5305.96, base loss: 15979.82
[INFO 2017-06-30 10:55:26,764 main.py:52] epoch 10968, training loss: 5500.37, average training loss: 5306.36, base loss: 15980.11
[INFO 2017-06-30 10:55:29,938 main.py:52] epoch 10969, training loss: 5692.29, average training loss: 5306.58, base loss: 15980.23
[INFO 2017-06-30 10:55:33,056 main.py:52] epoch 10970, training loss: 5130.80, average training loss: 5306.52, base loss: 15980.04
[INFO 2017-06-30 10:55:36,258 main.py:52] epoch 10971, training loss: 5162.91, average training loss: 5306.34, base loss: 15979.87
[INFO 2017-06-30 10:55:39,396 main.py:52] epoch 10972, training loss: 5357.56, average training loss: 5306.43, base loss: 15979.64
[INFO 2017-06-30 10:55:42,542 main.py:52] epoch 10973, training loss: 5402.72, average training loss: 5306.69, base loss: 15979.56
[INFO 2017-06-30 10:55:45,666 main.py:52] epoch 10974, training loss: 5303.13, average training loss: 5306.78, base loss: 15979.27
[INFO 2017-06-30 10:55:48,814 main.py:52] epoch 10975, training loss: 5344.09, average training loss: 5306.48, base loss: 15979.25
[INFO 2017-06-30 10:55:51,911 main.py:52] epoch 10976, training loss: 5420.09, average training loss: 5306.51, base loss: 15979.33
[INFO 2017-06-30 10:55:55,074 main.py:52] epoch 10977, training loss: 5230.93, average training loss: 5306.68, base loss: 15979.57
[INFO 2017-06-30 10:55:58,202 main.py:52] epoch 10978, training loss: 5582.50, average training loss: 5307.13, base loss: 15979.85
[INFO 2017-06-30 10:56:01,284 main.py:52] epoch 10979, training loss: 5236.13, average training loss: 5307.10, base loss: 15980.00
[INFO 2017-06-30 10:56:04,420 main.py:52] epoch 10980, training loss: 5268.25, average training loss: 5307.04, base loss: 15979.96
[INFO 2017-06-30 10:56:07,572 main.py:52] epoch 10981, training loss: 5590.47, average training loss: 5307.37, base loss: 15980.24
[INFO 2017-06-30 10:56:10,710 main.py:52] epoch 10982, training loss: 5151.24, average training loss: 5307.48, base loss: 15979.97
[INFO 2017-06-30 10:56:13,811 main.py:52] epoch 10983, training loss: 5254.04, average training loss: 5307.17, base loss: 15979.94
[INFO 2017-06-30 10:56:16,963 main.py:52] epoch 10984, training loss: 5176.91, average training loss: 5306.79, base loss: 15979.81
[INFO 2017-06-30 10:56:20,095 main.py:52] epoch 10985, training loss: 5489.88, average training loss: 5306.67, base loss: 15980.07
[INFO 2017-06-30 10:56:23,214 main.py:52] epoch 10986, training loss: 5510.11, average training loss: 5306.70, base loss: 15980.50
[INFO 2017-06-30 10:56:26,326 main.py:52] epoch 10987, training loss: 5239.59, average training loss: 5306.30, base loss: 15980.50
[INFO 2017-06-30 10:56:29,463 main.py:52] epoch 10988, training loss: 5242.60, average training loss: 5305.93, base loss: 15980.28
[INFO 2017-06-30 10:56:32,576 main.py:52] epoch 10989, training loss: 5575.20, average training loss: 5305.98, base loss: 15980.34
[INFO 2017-06-30 10:56:35,694 main.py:52] epoch 10990, training loss: 5406.46, average training loss: 5306.18, base loss: 15980.37
[INFO 2017-06-30 10:56:38,827 main.py:52] epoch 10991, training loss: 5126.59, average training loss: 5306.41, base loss: 15980.36
[INFO 2017-06-30 10:56:41,983 main.py:52] epoch 10992, training loss: 5414.48, average training loss: 5307.09, base loss: 15980.31
[INFO 2017-06-30 10:56:45,150 main.py:52] epoch 10993, training loss: 5148.04, average training loss: 5307.21, base loss: 15979.97
[INFO 2017-06-30 10:56:48,345 main.py:52] epoch 10994, training loss: 5145.54, average training loss: 5306.34, base loss: 15980.21
[INFO 2017-06-30 10:56:51,485 main.py:52] epoch 10995, training loss: 4907.36, average training loss: 5305.74, base loss: 15980.04
[INFO 2017-06-30 10:56:54,666 main.py:52] epoch 10996, training loss: 5352.57, average training loss: 5305.60, base loss: 15979.96
[INFO 2017-06-30 10:56:57,802 main.py:52] epoch 10997, training loss: 5255.74, average training loss: 5305.23, base loss: 15979.85
[INFO 2017-06-30 10:57:00,993 main.py:52] epoch 10998, training loss: 5395.41, average training loss: 5305.40, base loss: 15980.04
[INFO 2017-06-30 10:57:04,184 main.py:52] epoch 10999, training loss: 5460.16, average training loss: 5305.56, base loss: 15979.97
[INFO 2017-06-30 10:57:04,185 main.py:54] epoch 10999, testing
[INFO 2017-06-30 10:57:17,410 main.py:97] average testing loss: 5190.93, base loss: 15460.02
[INFO 2017-06-30 10:57:17,410 main.py:98] improve_loss: 10269.09, improve_percent: 0.66
[INFO 2017-06-30 10:57:17,411 main.py:66] current best improved percent: 0.67
[INFO 2017-06-30 10:57:20,546 main.py:52] epoch 11000, training loss: 5072.68, average training loss: 5305.34, base loss: 15980.00
[INFO 2017-06-30 10:57:23,674 main.py:52] epoch 11001, training loss: 5007.50, average training loss: 5304.95, base loss: 15979.74
[INFO 2017-06-30 10:57:26,855 main.py:52] epoch 11002, training loss: 5091.80, average training loss: 5305.00, base loss: 15979.69
[INFO 2017-06-30 10:57:30,004 main.py:52] epoch 11003, training loss: 5441.78, average training loss: 5304.95, base loss: 15980.05
[INFO 2017-06-30 10:57:33,168 main.py:52] epoch 11004, training loss: 5269.56, average training loss: 5304.52, base loss: 15980.24
[INFO 2017-06-30 10:57:36,540 main.py:52] epoch 11005, training loss: 5281.99, average training loss: 5304.46, base loss: 15980.42
[INFO 2017-06-30 10:57:39,660 main.py:52] epoch 11006, training loss: 5516.94, average training loss: 5304.68, base loss: 15980.33
[INFO 2017-06-30 10:57:42,817 main.py:52] epoch 11007, training loss: 5135.11, average training loss: 5304.63, base loss: 15980.10
[INFO 2017-06-30 10:57:45,976 main.py:52] epoch 11008, training loss: 5092.86, average training loss: 5303.94, base loss: 15980.18
[INFO 2017-06-30 10:57:49,112 main.py:52] epoch 11009, training loss: 5251.99, average training loss: 5303.81, base loss: 15980.46
[INFO 2017-06-30 10:57:52,243 main.py:52] epoch 11010, training loss: 5322.89, average training loss: 5303.39, base loss: 15980.53
[INFO 2017-06-30 10:57:55,362 main.py:52] epoch 11011, training loss: 5025.76, average training loss: 5302.98, base loss: 15980.49
[INFO 2017-06-30 10:57:58,492 main.py:52] epoch 11012, training loss: 5028.42, average training loss: 5302.91, base loss: 15980.39
[INFO 2017-06-30 10:58:01,665 main.py:52] epoch 11013, training loss: 5618.33, average training loss: 5303.16, base loss: 15980.73
[INFO 2017-06-30 10:58:04,798 main.py:52] epoch 11014, training loss: 5293.69, average training loss: 5303.53, base loss: 15980.74
[INFO 2017-06-30 10:58:07,982 main.py:52] epoch 11015, training loss: 4751.20, average training loss: 5302.79, base loss: 15980.33
[INFO 2017-06-30 10:58:11,111 main.py:52] epoch 11016, training loss: 5112.92, average training loss: 5302.66, base loss: 15979.96
[INFO 2017-06-30 10:58:14,260 main.py:52] epoch 11017, training loss: 5133.42, average training loss: 5302.29, base loss: 15979.81
[INFO 2017-06-30 10:58:17,407 main.py:52] epoch 11018, training loss: 5297.19, average training loss: 5302.13, base loss: 15979.98
[INFO 2017-06-30 10:58:20,587 main.py:52] epoch 11019, training loss: 4977.36, average training loss: 5301.79, base loss: 15979.78
[INFO 2017-06-30 10:58:23,707 main.py:52] epoch 11020, training loss: 5469.68, average training loss: 5301.60, base loss: 15980.23
[INFO 2017-06-30 10:58:26,911 main.py:52] epoch 11021, training loss: 5478.07, average training loss: 5301.32, base loss: 15980.54
[INFO 2017-06-30 10:58:30,097 main.py:52] epoch 11022, training loss: 5130.46, average training loss: 5301.03, base loss: 15980.36
[INFO 2017-06-30 10:58:33,262 main.py:52] epoch 11023, training loss: 5388.34, average training loss: 5301.43, base loss: 15980.48
[INFO 2017-06-30 10:58:36,372 main.py:52] epoch 11024, training loss: 5251.22, average training loss: 5301.35, base loss: 15980.49
[INFO 2017-06-30 10:58:39,528 main.py:52] epoch 11025, training loss: 5523.39, average training loss: 5301.43, base loss: 15980.74
[INFO 2017-06-30 10:58:42,744 main.py:52] epoch 11026, training loss: 5118.49, average training loss: 5301.19, base loss: 15980.57
[INFO 2017-06-30 10:58:45,881 main.py:52] epoch 11027, training loss: 5337.35, average training loss: 5301.16, base loss: 15980.46
[INFO 2017-06-30 10:58:49,023 main.py:52] epoch 11028, training loss: 5092.03, average training loss: 5300.87, base loss: 15980.35
[INFO 2017-06-30 10:58:52,143 main.py:52] epoch 11029, training loss: 5137.87, average training loss: 5300.79, base loss: 15980.03
[INFO 2017-06-30 10:58:55,275 main.py:52] epoch 11030, training loss: 5014.06, average training loss: 5300.83, base loss: 15979.70
[INFO 2017-06-30 10:58:58,401 main.py:52] epoch 11031, training loss: 5545.58, average training loss: 5301.13, base loss: 15979.69
[INFO 2017-06-30 10:59:01,542 main.py:52] epoch 11032, training loss: 5323.00, average training loss: 5301.19, base loss: 15979.69
[INFO 2017-06-30 10:59:04,654 main.py:52] epoch 11033, training loss: 5276.83, average training loss: 5301.20, base loss: 15979.77
[INFO 2017-06-30 10:59:07,804 main.py:52] epoch 11034, training loss: 5399.83, average training loss: 5301.58, base loss: 15979.78
[INFO 2017-06-30 10:59:10,953 main.py:52] epoch 11035, training loss: 5515.72, average training loss: 5301.83, base loss: 15979.74
[INFO 2017-06-30 10:59:14,093 main.py:52] epoch 11036, training loss: 5317.09, average training loss: 5302.12, base loss: 15979.69
[INFO 2017-06-30 10:59:17,246 main.py:52] epoch 11037, training loss: 5150.86, average training loss: 5302.01, base loss: 15979.34
[INFO 2017-06-30 10:59:20,357 main.py:52] epoch 11038, training loss: 5032.43, average training loss: 5301.70, base loss: 15979.18
[INFO 2017-06-30 10:59:23,502 main.py:52] epoch 11039, training loss: 5262.67, average training loss: 5301.39, base loss: 15979.30
[INFO 2017-06-30 10:59:26,622 main.py:52] epoch 11040, training loss: 5156.34, average training loss: 5301.10, base loss: 15979.44
[INFO 2017-06-30 10:59:29,802 main.py:52] epoch 11041, training loss: 5688.40, average training loss: 5301.14, base loss: 15980.11
[INFO 2017-06-30 10:59:32,923 main.py:52] epoch 11042, training loss: 5456.21, average training loss: 5301.65, base loss: 15980.19
[INFO 2017-06-30 10:59:36,117 main.py:52] epoch 11043, training loss: 5602.54, average training loss: 5301.97, base loss: 15980.45
[INFO 2017-06-30 10:59:39,251 main.py:52] epoch 11044, training loss: 5027.31, average training loss: 5301.70, base loss: 15980.48
[INFO 2017-06-30 10:59:42,377 main.py:52] epoch 11045, training loss: 5134.84, average training loss: 5301.55, base loss: 15980.64
[INFO 2017-06-30 10:59:45,511 main.py:52] epoch 11046, training loss: 5251.19, average training loss: 5301.74, base loss: 15980.55
[INFO 2017-06-30 10:59:48,648 main.py:52] epoch 11047, training loss: 5197.54, average training loss: 5301.75, base loss: 15980.38
[INFO 2017-06-30 10:59:51,792 main.py:52] epoch 11048, training loss: 5236.75, average training loss: 5301.60, base loss: 15980.26
[INFO 2017-06-30 10:59:55,021 main.py:52] epoch 11049, training loss: 5251.05, average training loss: 5301.68, base loss: 15980.33
[INFO 2017-06-30 10:59:58,163 main.py:52] epoch 11050, training loss: 5332.55, average training loss: 5301.64, base loss: 15980.24
[INFO 2017-06-30 11:00:01,258 main.py:52] epoch 11051, training loss: 5379.30, average training loss: 5302.04, base loss: 15980.33
[INFO 2017-06-30 11:00:04,398 main.py:52] epoch 11052, training loss: 5339.65, average training loss: 5302.04, base loss: 15980.55
[INFO 2017-06-30 11:00:07,583 main.py:52] epoch 11053, training loss: 5219.53, average training loss: 5302.19, base loss: 15980.45
[INFO 2017-06-30 11:00:10,763 main.py:52] epoch 11054, training loss: 5347.90, average training loss: 5302.03, base loss: 15980.50
[INFO 2017-06-30 11:00:13,903 main.py:52] epoch 11055, training loss: 5249.67, average training loss: 5301.95, base loss: 15980.68
[INFO 2017-06-30 11:00:17,100 main.py:52] epoch 11056, training loss: 5254.09, average training loss: 5301.64, base loss: 15980.64
[INFO 2017-06-30 11:00:20,276 main.py:52] epoch 11057, training loss: 5471.44, average training loss: 5301.91, base loss: 15980.73
[INFO 2017-06-30 11:00:23,434 main.py:52] epoch 11058, training loss: 5452.80, average training loss: 5301.80, base loss: 15980.72
[INFO 2017-06-30 11:00:26,605 main.py:52] epoch 11059, training loss: 5385.15, average training loss: 5302.24, base loss: 15980.65
[INFO 2017-06-30 11:00:29,740 main.py:52] epoch 11060, training loss: 5286.41, average training loss: 5301.57, base loss: 15980.88
[INFO 2017-06-30 11:00:32,878 main.py:52] epoch 11061, training loss: 5122.83, average training loss: 5300.94, base loss: 15980.81
[INFO 2017-06-30 11:00:36,002 main.py:52] epoch 11062, training loss: 5367.45, average training loss: 5300.89, base loss: 15980.97
[INFO 2017-06-30 11:00:39,163 main.py:52] epoch 11063, training loss: 5518.83, average training loss: 5300.70, base loss: 15981.24
[INFO 2017-06-30 11:00:42,280 main.py:52] epoch 11064, training loss: 5560.35, average training loss: 5300.81, base loss: 15981.90
[INFO 2017-06-30 11:00:45,420 main.py:52] epoch 11065, training loss: 5576.37, average training loss: 5301.04, base loss: 15982.23
[INFO 2017-06-30 11:00:48,540 main.py:52] epoch 11066, training loss: 5389.57, average training loss: 5301.46, base loss: 15982.21
[INFO 2017-06-30 11:00:51,690 main.py:52] epoch 11067, training loss: 4761.72, average training loss: 5301.05, base loss: 15981.93
[INFO 2017-06-30 11:00:54,830 main.py:52] epoch 11068, training loss: 5738.19, average training loss: 5301.62, base loss: 15982.29
[INFO 2017-06-30 11:00:57,989 main.py:52] epoch 11069, training loss: 5403.58, average training loss: 5301.85, base loss: 15982.53
[INFO 2017-06-30 11:01:01,148 main.py:52] epoch 11070, training loss: 5027.47, average training loss: 5301.42, base loss: 15982.19
[INFO 2017-06-30 11:01:04,268 main.py:52] epoch 11071, training loss: 5470.84, average training loss: 5301.73, base loss: 15982.30
[INFO 2017-06-30 11:01:07,406 main.py:52] epoch 11072, training loss: 5755.85, average training loss: 5301.98, base loss: 15982.46
[INFO 2017-06-30 11:01:10,556 main.py:52] epoch 11073, training loss: 5220.61, average training loss: 5301.89, base loss: 15982.51
[INFO 2017-06-30 11:01:13,755 main.py:52] epoch 11074, training loss: 5144.31, average training loss: 5301.61, base loss: 15982.50
[INFO 2017-06-30 11:01:16,927 main.py:52] epoch 11075, training loss: 5490.80, average training loss: 5301.43, base loss: 15982.70
[INFO 2017-06-30 11:01:20,029 main.py:52] epoch 11076, training loss: 5562.03, average training loss: 5301.40, base loss: 15982.80
[INFO 2017-06-30 11:01:23,166 main.py:52] epoch 11077, training loss: 5223.44, average training loss: 5301.55, base loss: 15982.72
[INFO 2017-06-30 11:01:26,341 main.py:52] epoch 11078, training loss: 5214.60, average training loss: 5301.34, base loss: 15982.91
[INFO 2017-06-30 11:01:29,475 main.py:52] epoch 11079, training loss: 5348.75, average training loss: 5301.30, base loss: 15982.72
[INFO 2017-06-30 11:01:32,653 main.py:52] epoch 11080, training loss: 5452.98, average training loss: 5301.29, base loss: 15982.83
[INFO 2017-06-30 11:01:35,773 main.py:52] epoch 11081, training loss: 5267.68, average training loss: 5301.13, base loss: 15982.73
[INFO 2017-06-30 11:01:38,942 main.py:52] epoch 11082, training loss: 5122.79, average training loss: 5300.82, base loss: 15982.80
[INFO 2017-06-30 11:01:42,066 main.py:52] epoch 11083, training loss: 5992.05, average training loss: 5301.68, base loss: 15983.42
[INFO 2017-06-30 11:01:45,188 main.py:52] epoch 11084, training loss: 5624.83, average training loss: 5302.06, base loss: 15983.46
[INFO 2017-06-30 11:01:48,352 main.py:52] epoch 11085, training loss: 5154.88, average training loss: 5302.00, base loss: 15983.26
[INFO 2017-06-30 11:01:51,495 main.py:52] epoch 11086, training loss: 5337.55, average training loss: 5301.77, base loss: 15983.45
[INFO 2017-06-30 11:01:54,610 main.py:52] epoch 11087, training loss: 5536.61, average training loss: 5301.78, base loss: 15983.49
[INFO 2017-06-30 11:01:57,695 main.py:52] epoch 11088, training loss: 5380.23, average training loss: 5301.75, base loss: 15983.50
[INFO 2017-06-30 11:02:00,831 main.py:52] epoch 11089, training loss: 5690.21, average training loss: 5301.99, base loss: 15983.56
[INFO 2017-06-30 11:02:03,982 main.py:52] epoch 11090, training loss: 5182.90, average training loss: 5301.91, base loss: 15983.66
[INFO 2017-06-30 11:02:07,110 main.py:52] epoch 11091, training loss: 5310.66, average training loss: 5301.89, base loss: 15983.60
[INFO 2017-06-30 11:02:10,258 main.py:52] epoch 11092, training loss: 5229.53, average training loss: 5301.52, base loss: 15983.48
[INFO 2017-06-30 11:02:13,412 main.py:52] epoch 11093, training loss: 5432.85, average training loss: 5301.60, base loss: 15983.36
[INFO 2017-06-30 11:02:16,570 main.py:52] epoch 11094, training loss: 4902.50, average training loss: 5300.95, base loss: 15983.18
[INFO 2017-06-30 11:02:19,696 main.py:52] epoch 11095, training loss: 5076.16, average training loss: 5301.11, base loss: 15983.09
[INFO 2017-06-30 11:02:22,774 main.py:52] epoch 11096, training loss: 5131.42, average training loss: 5300.94, base loss: 15983.08
[INFO 2017-06-30 11:02:25,984 main.py:52] epoch 11097, training loss: 4831.79, average training loss: 5300.58, base loss: 15982.99
[INFO 2017-06-30 11:02:29,126 main.py:52] epoch 11098, training loss: 5277.52, average training loss: 5300.54, base loss: 15982.87
[INFO 2017-06-30 11:02:32,238 main.py:52] epoch 11099, training loss: 5117.58, average training loss: 5300.38, base loss: 15982.73
[INFO 2017-06-30 11:02:32,239 main.py:54] epoch 11099, testing
[INFO 2017-06-30 11:02:45,224 main.py:97] average testing loss: 5269.85, base loss: 15948.52
[INFO 2017-06-30 11:02:45,224 main.py:98] improve_loss: 10678.68, improve_percent: 0.67
[INFO 2017-06-30 11:02:45,227 main.py:66] current best improved percent: 0.67
[INFO 2017-06-30 11:02:48,531 main.py:52] epoch 11100, training loss: 5178.03, average training loss: 5300.20, base loss: 15982.81
[INFO 2017-06-30 11:02:51,681 main.py:52] epoch 11101, training loss: 5248.17, average training loss: 5299.58, base loss: 15983.04
[INFO 2017-06-30 11:02:54,863 main.py:52] epoch 11102, training loss: 5105.01, average training loss: 5299.33, base loss: 15982.77
[INFO 2017-06-30 11:02:57,992 main.py:52] epoch 11103, training loss: 5236.63, average training loss: 5299.33, base loss: 15982.42
[INFO 2017-06-30 11:03:01,109 main.py:52] epoch 11104, training loss: 5588.68, average training loss: 5299.70, base loss: 15982.39
[INFO 2017-06-30 11:03:04,277 main.py:52] epoch 11105, training loss: 5506.85, average training loss: 5300.04, base loss: 15982.57
[INFO 2017-06-30 11:03:07,429 main.py:52] epoch 11106, training loss: 5301.04, average training loss: 5299.74, base loss: 15982.50
[INFO 2017-06-30 11:03:10,619 main.py:52] epoch 11107, training loss: 5443.49, average training loss: 5299.90, base loss: 15982.63
[INFO 2017-06-30 11:03:13,762 main.py:52] epoch 11108, training loss: 5383.79, average training loss: 5300.03, base loss: 15982.57
[INFO 2017-06-30 11:03:16,896 main.py:52] epoch 11109, training loss: 5148.96, average training loss: 5299.88, base loss: 15982.38
[INFO 2017-06-30 11:03:20,034 main.py:52] epoch 11110, training loss: 5603.80, average training loss: 5300.05, base loss: 15982.40
[INFO 2017-06-30 11:03:23,161 main.py:52] epoch 11111, training loss: 5592.75, average training loss: 5300.33, base loss: 15982.79
[INFO 2017-06-30 11:03:26,320 main.py:52] epoch 11112, training loss: 5170.64, average training loss: 5300.05, base loss: 15982.75
[INFO 2017-06-30 11:03:29,511 main.py:52] epoch 11113, training loss: 5331.91, average training loss: 5300.00, base loss: 15982.72
[INFO 2017-06-30 11:03:32,660 main.py:52] epoch 11114, training loss: 5015.45, average training loss: 5299.39, base loss: 15982.43
[INFO 2017-06-30 11:03:35,780 main.py:52] epoch 11115, training loss: 5113.09, average training loss: 5299.41, base loss: 15982.01
[INFO 2017-06-30 11:03:38,941 main.py:52] epoch 11116, training loss: 5226.78, average training loss: 5299.48, base loss: 15982.23
[INFO 2017-06-30 11:03:42,088 main.py:52] epoch 11117, training loss: 5317.98, average training loss: 5299.04, base loss: 15982.42
[INFO 2017-06-30 11:03:45,240 main.py:52] epoch 11118, training loss: 4820.56, average training loss: 5298.69, base loss: 15982.24
[INFO 2017-06-30 11:03:48,337 main.py:52] epoch 11119, training loss: 5412.46, average training loss: 5298.77, base loss: 15982.43
[INFO 2017-06-30 11:03:51,480 main.py:52] epoch 11120, training loss: 5172.61, average training loss: 5298.75, base loss: 15982.25
[INFO 2017-06-30 11:03:54,619 main.py:52] epoch 11121, training loss: 5412.33, average training loss: 5298.39, base loss: 15982.22
[INFO 2017-06-30 11:03:57,772 main.py:52] epoch 11122, training loss: 5129.66, average training loss: 5298.61, base loss: 15982.17
[INFO 2017-06-30 11:04:00,940 main.py:52] epoch 11123, training loss: 5241.42, average training loss: 5298.73, base loss: 15982.06
[INFO 2017-06-30 11:04:04,084 main.py:52] epoch 11124, training loss: 5419.42, average training loss: 5298.97, base loss: 15981.83
[INFO 2017-06-30 11:04:07,259 main.py:52] epoch 11125, training loss: 5227.12, average training loss: 5298.68, base loss: 15981.31
[INFO 2017-06-30 11:04:10,387 main.py:52] epoch 11126, training loss: 5253.08, average training loss: 5298.53, base loss: 15981.27
[INFO 2017-06-30 11:04:13,515 main.py:52] epoch 11127, training loss: 5463.82, average training loss: 5298.47, base loss: 15981.43
[INFO 2017-06-30 11:04:16,674 main.py:52] epoch 11128, training loss: 5753.94, average training loss: 5298.77, base loss: 15981.91
[INFO 2017-06-30 11:04:19,785 main.py:52] epoch 11129, training loss: 5305.76, average training loss: 5298.94, base loss: 15981.63
[INFO 2017-06-30 11:04:22,947 main.py:52] epoch 11130, training loss: 5288.60, average training loss: 5298.91, base loss: 15981.68
[INFO 2017-06-30 11:04:26,061 main.py:52] epoch 11131, training loss: 5064.15, average training loss: 5298.96, base loss: 15981.45
[INFO 2017-06-30 11:04:29,215 main.py:52] epoch 11132, training loss: 5138.06, average training loss: 5298.80, base loss: 15981.45
[INFO 2017-06-30 11:04:32,365 main.py:52] epoch 11133, training loss: 5037.88, average training loss: 5298.61, base loss: 15981.34
[INFO 2017-06-30 11:04:35,471 main.py:52] epoch 11134, training loss: 5432.23, average training loss: 5298.17, base loss: 15981.44
[INFO 2017-06-30 11:04:38,606 main.py:52] epoch 11135, training loss: 5338.52, average training loss: 5298.36, base loss: 15981.66
[INFO 2017-06-30 11:04:41,779 main.py:52] epoch 11136, training loss: 4952.75, average training loss: 5297.93, base loss: 15981.60
[INFO 2017-06-30 11:04:44,895 main.py:52] epoch 11137, training loss: 5269.97, average training loss: 5298.04, base loss: 15981.72
[INFO 2017-06-30 11:04:48,068 main.py:52] epoch 11138, training loss: 5376.72, average training loss: 5297.75, base loss: 15981.81
[INFO 2017-06-30 11:04:51,198 main.py:52] epoch 11139, training loss: 4903.67, average training loss: 5297.57, base loss: 15981.77
[INFO 2017-06-30 11:04:54,334 main.py:52] epoch 11140, training loss: 5107.28, average training loss: 5297.56, base loss: 15981.74
[INFO 2017-06-30 11:04:57,451 main.py:52] epoch 11141, training loss: 5375.98, average training loss: 5297.74, base loss: 15981.70
[INFO 2017-06-30 11:05:00,606 main.py:52] epoch 11142, training loss: 4842.87, average training loss: 5297.41, base loss: 15981.38
[INFO 2017-06-30 11:05:03,753 main.py:52] epoch 11143, training loss: 5330.38, average training loss: 5297.35, base loss: 15981.56
[INFO 2017-06-30 11:05:06,895 main.py:52] epoch 11144, training loss: 5142.09, average training loss: 5297.18, base loss: 15981.44
[INFO 2017-06-30 11:05:10,036 main.py:52] epoch 11145, training loss: 5218.05, average training loss: 5296.91, base loss: 15981.57
[INFO 2017-06-30 11:05:13,160 main.py:52] epoch 11146, training loss: 5296.80, average training loss: 5296.91, base loss: 15981.80
[INFO 2017-06-30 11:05:16,286 main.py:52] epoch 11147, training loss: 4957.36, average training loss: 5296.81, base loss: 15981.29
[INFO 2017-06-30 11:05:19,427 main.py:52] epoch 11148, training loss: 5361.91, average training loss: 5297.24, base loss: 15981.34
[INFO 2017-06-30 11:05:22,564 main.py:52] epoch 11149, training loss: 5175.74, average training loss: 5297.29, base loss: 15981.15
[INFO 2017-06-30 11:05:25,693 main.py:52] epoch 11150, training loss: 4886.02, average training loss: 5296.84, base loss: 15981.04
[INFO 2017-06-30 11:05:28,823 main.py:52] epoch 11151, training loss: 5074.94, average training loss: 5297.05, base loss: 15980.86
[INFO 2017-06-30 11:05:31,967 main.py:52] epoch 11152, training loss: 5259.32, average training loss: 5297.20, base loss: 15981.19
[INFO 2017-06-30 11:05:35,089 main.py:52] epoch 11153, training loss: 5723.00, average training loss: 5297.62, base loss: 15981.69
[INFO 2017-06-30 11:05:38,238 main.py:52] epoch 11154, training loss: 5116.29, average training loss: 5297.46, base loss: 15981.76
[INFO 2017-06-30 11:05:41,391 main.py:52] epoch 11155, training loss: 5461.26, average training loss: 5297.66, base loss: 15981.82
[INFO 2017-06-30 11:05:44,536 main.py:52] epoch 11156, training loss: 5658.21, average training loss: 5298.03, base loss: 15981.93
[INFO 2017-06-30 11:05:47,637 main.py:52] epoch 11157, training loss: 5362.72, average training loss: 5297.94, base loss: 15982.09
[INFO 2017-06-30 11:05:50,814 main.py:52] epoch 11158, training loss: 5524.50, average training loss: 5298.05, base loss: 15982.19
[INFO 2017-06-30 11:05:53,941 main.py:52] epoch 11159, training loss: 5729.38, average training loss: 5298.28, base loss: 15982.49
[INFO 2017-06-30 11:05:57,067 main.py:52] epoch 11160, training loss: 5347.33, average training loss: 5298.19, base loss: 15982.53
[INFO 2017-06-30 11:06:00,206 main.py:52] epoch 11161, training loss: 5237.20, average training loss: 5298.17, base loss: 15982.38
[INFO 2017-06-30 11:06:03,365 main.py:52] epoch 11162, training loss: 5303.54, average training loss: 5298.03, base loss: 15982.35
[INFO 2017-06-30 11:06:06,506 main.py:52] epoch 11163, training loss: 5151.65, average training loss: 5298.01, base loss: 15982.32
[INFO 2017-06-30 11:06:09,653 main.py:52] epoch 11164, training loss: 5008.06, average training loss: 5297.85, base loss: 15982.07
[INFO 2017-06-30 11:06:12,810 main.py:52] epoch 11165, training loss: 5298.71, average training loss: 5297.67, base loss: 15982.23
[INFO 2017-06-30 11:06:15,949 main.py:52] epoch 11166, training loss: 5248.54, average training loss: 5297.71, base loss: 15982.35
[INFO 2017-06-30 11:06:19,115 main.py:52] epoch 11167, training loss: 5392.33, average training loss: 5297.87, base loss: 15982.51
[INFO 2017-06-30 11:06:22,238 main.py:52] epoch 11168, training loss: 5016.21, average training loss: 5297.70, base loss: 15982.39
[INFO 2017-06-30 11:06:25,384 main.py:52] epoch 11169, training loss: 5414.34, average training loss: 5297.70, base loss: 15982.50
[INFO 2017-06-30 11:06:28,533 main.py:52] epoch 11170, training loss: 5413.30, average training loss: 5297.55, base loss: 15982.47
[INFO 2017-06-30 11:06:31,704 main.py:52] epoch 11171, training loss: 5340.41, average training loss: 5297.65, base loss: 15982.35
[INFO 2017-06-30 11:06:34,867 main.py:52] epoch 11172, training loss: 5328.76, average training loss: 5297.93, base loss: 15982.26
[INFO 2017-06-30 11:06:38,034 main.py:52] epoch 11173, training loss: 5262.99, average training loss: 5297.78, base loss: 15982.32
[INFO 2017-06-30 11:06:41,154 main.py:52] epoch 11174, training loss: 4764.81, average training loss: 5297.00, base loss: 15981.81
[INFO 2017-06-30 11:06:44,305 main.py:52] epoch 11175, training loss: 5168.29, average training loss: 5296.52, base loss: 15981.50
[INFO 2017-06-30 11:06:47,455 main.py:52] epoch 11176, training loss: 5169.25, average training loss: 5296.33, base loss: 15981.61
[INFO 2017-06-30 11:06:50,580 main.py:52] epoch 11177, training loss: 5168.65, average training loss: 5296.50, base loss: 15981.45
[INFO 2017-06-30 11:06:53,713 main.py:52] epoch 11178, training loss: 5312.82, average training loss: 5296.25, base loss: 15981.39
[INFO 2017-06-30 11:06:56,849 main.py:52] epoch 11179, training loss: 5262.57, average training loss: 5296.62, base loss: 15981.38
[INFO 2017-06-30 11:06:59,995 main.py:52] epoch 11180, training loss: 5329.08, average training loss: 5296.89, base loss: 15981.46
[INFO 2017-06-30 11:07:03,123 main.py:52] epoch 11181, training loss: 4971.57, average training loss: 5296.50, base loss: 15981.40
[INFO 2017-06-30 11:07:06,206 main.py:52] epoch 11182, training loss: 5035.04, average training loss: 5296.36, base loss: 15981.11
[INFO 2017-06-30 11:07:09,330 main.py:52] epoch 11183, training loss: 5233.73, average training loss: 5296.33, base loss: 15980.86
[INFO 2017-06-30 11:07:12,430 main.py:52] epoch 11184, training loss: 5417.23, average training loss: 5296.61, base loss: 15981.09
[INFO 2017-06-30 11:07:15,553 main.py:52] epoch 11185, training loss: 5457.38, average training loss: 5297.04, base loss: 15981.27
[INFO 2017-06-30 11:07:18,685 main.py:52] epoch 11186, training loss: 5665.37, average training loss: 5297.06, base loss: 15981.72
[INFO 2017-06-30 11:07:21,807 main.py:52] epoch 11187, training loss: 5309.57, average training loss: 5296.93, base loss: 15981.77
[INFO 2017-06-30 11:07:24,919 main.py:52] epoch 11188, training loss: 5326.36, average training loss: 5296.89, base loss: 15981.90
[INFO 2017-06-30 11:07:28,073 main.py:52] epoch 11189, training loss: 5372.09, average training loss: 5296.61, base loss: 15982.03
[INFO 2017-06-30 11:07:31,217 main.py:52] epoch 11190, training loss: 5243.51, average training loss: 5296.34, base loss: 15981.86
[INFO 2017-06-30 11:07:34,325 main.py:52] epoch 11191, training loss: 5142.00, average training loss: 5296.31, base loss: 15981.60
[INFO 2017-06-30 11:07:37,430 main.py:52] epoch 11192, training loss: 4805.66, average training loss: 5296.02, base loss: 15981.49
[INFO 2017-06-30 11:07:40,583 main.py:52] epoch 11193, training loss: 5376.62, average training loss: 5295.78, base loss: 15981.62
[INFO 2017-06-30 11:07:43,710 main.py:52] epoch 11194, training loss: 5009.73, average training loss: 5295.05, base loss: 15981.53
[INFO 2017-06-30 11:07:46,824 main.py:52] epoch 11195, training loss: 5262.30, average training loss: 5294.97, base loss: 15981.46
[INFO 2017-06-30 11:07:49,981 main.py:52] epoch 11196, training loss: 5379.33, average training loss: 5294.64, base loss: 15981.63
[INFO 2017-06-30 11:07:53,144 main.py:52] epoch 11197, training loss: 5317.42, average training loss: 5294.57, base loss: 15981.95
[INFO 2017-06-30 11:07:56,277 main.py:52] epoch 11198, training loss: 5369.32, average training loss: 5294.26, base loss: 15982.12
[INFO 2017-06-30 11:07:59,402 main.py:52] epoch 11199, training loss: 5243.69, average training loss: 5294.08, base loss: 15982.09
[INFO 2017-06-30 11:07:59,402 main.py:54] epoch 11199, testing
[INFO 2017-06-30 11:08:12,556 main.py:97] average testing loss: 5293.36, base loss: 16513.99
[INFO 2017-06-30 11:08:12,556 main.py:98] improve_loss: 11220.63, improve_percent: 0.68
[INFO 2017-06-30 11:08:12,558 main.py:62] model save to ./model/final.pth
[INFO 2017-06-30 11:08:12,592 main.py:66] current best improved percent: 0.68
[INFO 2017-06-30 11:08:15,748 main.py:52] epoch 11200, training loss: 5032.83, average training loss: 5293.74, base loss: 15981.59
[INFO 2017-06-30 11:08:19,103 main.py:52] epoch 11201, training loss: 5303.01, average training loss: 5293.76, base loss: 15981.72
[INFO 2017-06-30 11:08:22,219 main.py:52] epoch 11202, training loss: 4936.16, average training loss: 5293.56, base loss: 15981.36
[INFO 2017-06-30 11:08:25,353 main.py:52] epoch 11203, training loss: 4817.30, average training loss: 5293.16, base loss: 15981.13
[INFO 2017-06-30 11:08:28,469 main.py:52] epoch 11204, training loss: 5398.39, average training loss: 5293.02, base loss: 15981.47
[INFO 2017-06-30 11:08:31,596 main.py:52] epoch 11205, training loss: 5860.61, average training loss: 5293.22, base loss: 15981.91
[INFO 2017-06-30 11:08:34,772 main.py:52] epoch 11206, training loss: 5853.77, average training loss: 5293.48, base loss: 15982.10
[INFO 2017-06-30 11:08:37,942 main.py:52] epoch 11207, training loss: 5032.37, average training loss: 5293.03, base loss: 15981.98
[INFO 2017-06-30 11:08:41,068 main.py:52] epoch 11208, training loss: 5050.35, average training loss: 5292.64, base loss: 15981.76
[INFO 2017-06-30 11:08:44,242 main.py:52] epoch 11209, training loss: 5011.44, average training loss: 5292.07, base loss: 15981.49
[INFO 2017-06-30 11:08:47,379 main.py:52] epoch 11210, training loss: 5505.14, average training loss: 5292.05, base loss: 15981.63
[INFO 2017-06-30 11:08:50,538 main.py:52] epoch 11211, training loss: 5121.14, average training loss: 5291.95, base loss: 15981.74
[INFO 2017-06-30 11:08:53,711 main.py:52] epoch 11212, training loss: 5439.96, average training loss: 5292.18, base loss: 15981.54
[INFO 2017-06-30 11:08:56,841 main.py:52] epoch 11213, training loss: 5267.04, average training loss: 5292.19, base loss: 15981.50
[INFO 2017-06-30 11:09:00,004 main.py:52] epoch 11214, training loss: 5248.09, average training loss: 5292.26, base loss: 15981.37
[INFO 2017-06-30 11:09:03,137 main.py:52] epoch 11215, training loss: 5296.55, average training loss: 5292.19, base loss: 15981.18
[INFO 2017-06-30 11:09:06,297 main.py:52] epoch 11216, training loss: 5294.10, average training loss: 5291.91, base loss: 15981.29
[INFO 2017-06-30 11:09:09,447 main.py:52] epoch 11217, training loss: 5085.92, average training loss: 5291.48, base loss: 15981.25
[INFO 2017-06-30 11:09:12,568 main.py:52] epoch 11218, training loss: 5398.33, average training loss: 5291.24, base loss: 15981.32
[INFO 2017-06-30 11:09:15,705 main.py:52] epoch 11219, training loss: 4878.02, average training loss: 5290.64, base loss: 15981.13
[INFO 2017-06-30 11:09:18,849 main.py:52] epoch 11220, training loss: 5281.41, average training loss: 5290.48, base loss: 15981.20
[INFO 2017-06-30 11:09:22,032 main.py:52] epoch 11221, training loss: 5696.32, average training loss: 5290.59, base loss: 15981.46
[INFO 2017-06-30 11:09:25,113 main.py:52] epoch 11222, training loss: 5453.66, average training loss: 5290.74, base loss: 15981.71
[INFO 2017-06-30 11:09:28,224 main.py:52] epoch 11223, training loss: 5497.57, average training loss: 5290.75, base loss: 15981.95
[INFO 2017-06-30 11:09:31,376 main.py:52] epoch 11224, training loss: 5204.37, average training loss: 5290.58, base loss: 15981.93
[INFO 2017-06-30 11:09:34,545 main.py:52] epoch 11225, training loss: 5320.33, average training loss: 5290.93, base loss: 15982.12
[INFO 2017-06-30 11:09:37,682 main.py:52] epoch 11226, training loss: 5261.63, average training loss: 5290.71, base loss: 15982.29
[INFO 2017-06-30 11:09:40,830 main.py:52] epoch 11227, training loss: 4986.10, average training loss: 5289.86, base loss: 15982.14
[INFO 2017-06-30 11:09:43,955 main.py:52] epoch 11228, training loss: 5026.15, average training loss: 5289.81, base loss: 15982.17
[INFO 2017-06-30 11:09:47,135 main.py:52] epoch 11229, training loss: 5454.20, average training loss: 5290.21, base loss: 15982.26
[INFO 2017-06-30 11:09:50,318 main.py:52] epoch 11230, training loss: 5217.58, average training loss: 5290.04, base loss: 15982.31
[INFO 2017-06-30 11:09:53,423 main.py:52] epoch 11231, training loss: 5508.12, average training loss: 5290.03, base loss: 15982.65
[INFO 2017-06-30 11:09:56,569 main.py:52] epoch 11232, training loss: 5706.09, average training loss: 5290.41, base loss: 15983.02
[INFO 2017-06-30 11:09:59,697 main.py:52] epoch 11233, training loss: 5228.23, average training loss: 5290.16, base loss: 15982.82
[INFO 2017-06-30 11:10:02,818 main.py:52] epoch 11234, training loss: 5025.05, average training loss: 5290.07, base loss: 15982.64
[INFO 2017-06-30 11:10:05,958 main.py:52] epoch 11235, training loss: 4994.86, average training loss: 5289.95, base loss: 15982.60
[INFO 2017-06-30 11:10:09,120 main.py:52] epoch 11236, training loss: 5440.91, average training loss: 5290.06, base loss: 15982.60
[INFO 2017-06-30 11:10:12,228 main.py:52] epoch 11237, training loss: 5042.55, average training loss: 5289.73, base loss: 15982.23
[INFO 2017-06-30 11:10:15,398 main.py:52] epoch 11238, training loss: 5314.56, average training loss: 5289.85, base loss: 15982.35
[INFO 2017-06-30 11:10:18,535 main.py:52] epoch 11239, training loss: 5104.55, average training loss: 5289.86, base loss: 15982.45
[INFO 2017-06-30 11:10:21,634 main.py:52] epoch 11240, training loss: 5285.00, average training loss: 5289.92, base loss: 15982.32
[INFO 2017-06-30 11:10:24,769 main.py:52] epoch 11241, training loss: 5057.81, average training loss: 5289.48, base loss: 15982.26
[INFO 2017-06-30 11:10:27,933 main.py:52] epoch 11242, training loss: 5419.15, average training loss: 5289.53, base loss: 15982.36
[INFO 2017-06-30 11:10:31,059 main.py:52] epoch 11243, training loss: 5439.23, average training loss: 5290.03, base loss: 15982.91
[INFO 2017-06-30 11:10:34,193 main.py:52] epoch 11244, training loss: 5058.08, average training loss: 5290.03, base loss: 15982.75
[INFO 2017-06-30 11:10:37,315 main.py:52] epoch 11245, training loss: 5083.41, average training loss: 5290.17, base loss: 15982.65
[INFO 2017-06-30 11:10:40,446 main.py:52] epoch 11246, training loss: 4942.76, average training loss: 5289.70, base loss: 15982.36
[INFO 2017-06-30 11:10:43,595 main.py:52] epoch 11247, training loss: 4981.78, average training loss: 5289.60, base loss: 15982.04
[INFO 2017-06-30 11:10:46,742 main.py:52] epoch 11248, training loss: 5407.89, average training loss: 5289.90, base loss: 15982.14
[INFO 2017-06-30 11:10:49,929 main.py:52] epoch 11249, training loss: 5228.98, average training loss: 5289.99, base loss: 15982.37
[INFO 2017-06-30 11:10:53,093 main.py:52] epoch 11250, training loss: 5541.39, average training loss: 5290.07, base loss: 15982.77
[INFO 2017-06-30 11:10:56,211 main.py:52] epoch 11251, training loss: 5196.87, average training loss: 5289.86, base loss: 15982.78
[INFO 2017-06-30 11:10:59,362 main.py:52] epoch 11252, training loss: 5224.31, average training loss: 5289.89, base loss: 15982.86
[INFO 2017-06-30 11:11:02,507 main.py:52] epoch 11253, training loss: 5178.50, average training loss: 5289.73, base loss: 15983.05
[INFO 2017-06-30 11:11:05,661 main.py:52] epoch 11254, training loss: 4840.46, average training loss: 5289.54, base loss: 15982.89
[INFO 2017-06-30 11:11:08,765 main.py:52] epoch 11255, training loss: 5159.68, average training loss: 5289.46, base loss: 15983.00
[INFO 2017-06-30 11:11:11,946 main.py:52] epoch 11256, training loss: 5741.33, average training loss: 5289.54, base loss: 15983.23
[INFO 2017-06-30 11:11:15,112 main.py:52] epoch 11257, training loss: 5025.82, average training loss: 5289.21, base loss: 15982.93
[INFO 2017-06-30 11:11:18,241 main.py:52] epoch 11258, training loss: 5121.45, average training loss: 5288.93, base loss: 15982.86
[INFO 2017-06-30 11:11:21,368 main.py:52] epoch 11259, training loss: 5184.81, average training loss: 5288.83, base loss: 15982.44
[INFO 2017-06-30 11:11:24,488 main.py:52] epoch 11260, training loss: 5067.80, average training loss: 5288.43, base loss: 15982.71
[INFO 2017-06-30 11:11:27,635 main.py:52] epoch 11261, training loss: 5244.79, average training loss: 5288.11, base loss: 15982.79
[INFO 2017-06-30 11:11:30,755 main.py:52] epoch 11262, training loss: 5341.09, average training loss: 5288.00, base loss: 15983.11
[INFO 2017-06-30 11:11:33,895 main.py:52] epoch 11263, training loss: 5267.93, average training loss: 5287.90, base loss: 15983.34
[INFO 2017-06-30 11:11:37,038 main.py:52] epoch 11264, training loss: 5359.77, average training loss: 5287.72, base loss: 15983.29
[INFO 2017-06-30 11:11:40,208 main.py:52] epoch 11265, training loss: 5467.26, average training loss: 5287.86, base loss: 15983.25
[INFO 2017-06-30 11:11:43,346 main.py:52] epoch 11266, training loss: 5198.80, average training loss: 5287.89, base loss: 15983.32
[INFO 2017-06-30 11:11:46,478 main.py:52] epoch 11267, training loss: 5696.03, average training loss: 5288.06, base loss: 15983.39
[INFO 2017-06-30 11:11:49,613 main.py:52] epoch 11268, training loss: 5344.10, average training loss: 5288.02, base loss: 15983.84
[INFO 2017-06-30 11:11:52,701 main.py:52] epoch 11269, training loss: 5542.58, average training loss: 5288.05, base loss: 15984.06
[INFO 2017-06-30 11:11:55,826 main.py:52] epoch 11270, training loss: 5212.00, average training loss: 5287.93, base loss: 15983.99
[INFO 2017-06-30 11:11:58,997 main.py:52] epoch 11271, training loss: 5194.34, average training loss: 5288.00, base loss: 15984.23
[INFO 2017-06-30 11:12:02,193 main.py:52] epoch 11272, training loss: 5196.58, average training loss: 5288.22, base loss: 15984.06
[INFO 2017-06-30 11:12:05,303 main.py:52] epoch 11273, training loss: 5040.61, average training loss: 5288.07, base loss: 15983.89
[INFO 2017-06-30 11:12:08,436 main.py:52] epoch 11274, training loss: 5351.80, average training loss: 5287.69, base loss: 15984.00
[INFO 2017-06-30 11:12:11,581 main.py:52] epoch 11275, training loss: 5551.50, average training loss: 5288.02, base loss: 15984.19
[INFO 2017-06-30 11:12:14,702 main.py:52] epoch 11276, training loss: 5168.44, average training loss: 5287.52, base loss: 15984.26
[INFO 2017-06-30 11:12:17,856 main.py:52] epoch 11277, training loss: 5128.93, average training loss: 5287.73, base loss: 15984.33
[INFO 2017-06-30 11:12:20,984 main.py:52] epoch 11278, training loss: 5486.77, average training loss: 5288.21, base loss: 15984.19
[INFO 2017-06-30 11:12:24,121 main.py:52] epoch 11279, training loss: 4900.10, average training loss: 5287.71, base loss: 15984.02
[INFO 2017-06-30 11:12:27,230 main.py:52] epoch 11280, training loss: 5168.95, average training loss: 5287.35, base loss: 15983.89
[INFO 2017-06-30 11:12:30,401 main.py:52] epoch 11281, training loss: 5569.84, average training loss: 5287.74, base loss: 15983.82
[INFO 2017-06-30 11:12:33,546 main.py:52] epoch 11282, training loss: 5496.45, average training loss: 5287.91, base loss: 15984.06
[INFO 2017-06-30 11:12:36,652 main.py:52] epoch 11283, training loss: 5215.15, average training loss: 5287.70, base loss: 15984.26
[INFO 2017-06-30 11:12:39,791 main.py:52] epoch 11284, training loss: 5150.11, average training loss: 5287.73, base loss: 15984.11
[INFO 2017-06-30 11:12:42,923 main.py:52] epoch 11285, training loss: 5613.54, average training loss: 5287.89, base loss: 15984.46
[INFO 2017-06-30 11:12:46,055 main.py:52] epoch 11286, training loss: 5729.94, average training loss: 5288.72, base loss: 15984.76
[INFO 2017-06-30 11:12:49,225 main.py:52] epoch 11287, training loss: 5539.53, average training loss: 5289.05, base loss: 15984.95
[INFO 2017-06-30 11:12:52,372 main.py:52] epoch 11288, training loss: 5335.42, average training loss: 5288.92, base loss: 15984.88
[INFO 2017-06-30 11:12:55,496 main.py:52] epoch 11289, training loss: 5578.45, average training loss: 5289.03, base loss: 15984.87
[INFO 2017-06-30 11:12:58,625 main.py:52] epoch 11290, training loss: 5114.38, average training loss: 5288.83, base loss: 15984.76
[INFO 2017-06-30 11:13:01,771 main.py:52] epoch 11291, training loss: 5180.05, average training loss: 5289.03, base loss: 15984.58
[INFO 2017-06-30 11:13:04,901 main.py:52] epoch 11292, training loss: 5251.82, average training loss: 5288.63, base loss: 15984.56
[INFO 2017-06-30 11:13:08,051 main.py:52] epoch 11293, training loss: 5686.75, average training loss: 5289.27, base loss: 15984.46
[INFO 2017-06-30 11:13:11,182 main.py:52] epoch 11294, training loss: 5314.32, average training loss: 5289.53, base loss: 15984.43
[INFO 2017-06-30 11:13:14,292 main.py:52] epoch 11295, training loss: 5310.77, average training loss: 5289.43, base loss: 15984.28
[INFO 2017-06-30 11:13:17,421 main.py:52] epoch 11296, training loss: 4870.27, average training loss: 5289.20, base loss: 15983.96
[INFO 2017-06-30 11:13:20,607 main.py:52] epoch 11297, training loss: 5280.99, average training loss: 5288.93, base loss: 15984.03
[INFO 2017-06-30 11:13:23,726 main.py:52] epoch 11298, training loss: 5254.91, average training loss: 5289.09, base loss: 15983.89
[INFO 2017-06-30 11:13:26,899 main.py:52] epoch 11299, training loss: 5312.64, average training loss: 5289.00, base loss: 15983.59
[INFO 2017-06-30 11:13:26,899 main.py:54] epoch 11299, testing
[INFO 2017-06-30 11:13:40,152 main.py:97] average testing loss: 5174.80, base loss: 15635.00
[INFO 2017-06-30 11:13:40,153 main.py:98] improve_loss: 10460.20, improve_percent: 0.67
[INFO 2017-06-30 11:13:40,154 main.py:66] current best improved percent: 0.68
[INFO 2017-06-30 11:13:43,326 main.py:52] epoch 11300, training loss: 5442.87, average training loss: 5289.23, base loss: 15983.65
[INFO 2017-06-30 11:13:46,503 main.py:52] epoch 11301, training loss: 4975.92, average training loss: 5289.07, base loss: 15983.90
[INFO 2017-06-30 11:13:49,627 main.py:52] epoch 11302, training loss: 5465.98, average training loss: 5289.29, base loss: 15984.19
[INFO 2017-06-30 11:13:52,771 main.py:52] epoch 11303, training loss: 5000.81, average training loss: 5288.99, base loss: 15984.00
[INFO 2017-06-30 11:13:55,927 main.py:52] epoch 11304, training loss: 5298.03, average training loss: 5288.72, base loss: 15983.75
[INFO 2017-06-30 11:13:59,038 main.py:52] epoch 11305, training loss: 5034.17, average training loss: 5288.56, base loss: 15983.29
[INFO 2017-06-30 11:14:02,189 main.py:52] epoch 11306, training loss: 5429.82, average training loss: 5288.89, base loss: 15983.37
[INFO 2017-06-30 11:14:05,314 main.py:52] epoch 11307, training loss: 4887.09, average training loss: 5288.48, base loss: 15983.13
[INFO 2017-06-30 11:14:08,668 main.py:52] epoch 11308, training loss: 4898.45, average training loss: 5287.85, base loss: 15983.09
[INFO 2017-06-30 11:14:11,791 main.py:52] epoch 11309, training loss: 5373.48, average training loss: 5287.90, base loss: 15983.17
[INFO 2017-06-30 11:14:14,930 main.py:52] epoch 11310, training loss: 5274.22, average training loss: 5288.00, base loss: 15983.35
[INFO 2017-06-30 11:14:18,116 main.py:52] epoch 11311, training loss: 5837.01, average training loss: 5288.56, base loss: 15983.63
[INFO 2017-06-30 11:14:21,286 main.py:52] epoch 11312, training loss: 5122.98, average training loss: 5288.25, base loss: 15983.62
[INFO 2017-06-30 11:14:24,427 main.py:52] epoch 11313, training loss: 5418.86, average training loss: 5288.51, base loss: 15983.56
[INFO 2017-06-30 11:14:27,629 main.py:52] epoch 11314, training loss: 5065.16, average training loss: 5288.45, base loss: 15983.33
[INFO 2017-06-30 11:14:30,793 main.py:52] epoch 11315, training loss: 5083.28, average training loss: 5288.28, base loss: 15983.36
[INFO 2017-06-30 11:14:33,902 main.py:52] epoch 11316, training loss: 5364.47, average training loss: 5288.52, base loss: 15983.33
[INFO 2017-06-30 11:14:37,028 main.py:52] epoch 11317, training loss: 4858.14, average training loss: 5288.15, base loss: 15983.30
[INFO 2017-06-30 11:14:40,173 main.py:52] epoch 11318, training loss: 5191.65, average training loss: 5288.16, base loss: 15983.14
[INFO 2017-06-30 11:14:43,306 main.py:52] epoch 11319, training loss: 5547.88, average training loss: 5288.44, base loss: 15983.49
[INFO 2017-06-30 11:14:46,461 main.py:52] epoch 11320, training loss: 5167.83, average training loss: 5288.25, base loss: 15983.38
[INFO 2017-06-30 11:14:49,585 main.py:52] epoch 11321, training loss: 5364.09, average training loss: 5288.32, base loss: 15983.44
[INFO 2017-06-30 11:14:52,728 main.py:52] epoch 11322, training loss: 5312.25, average training loss: 5288.47, base loss: 15983.67
[INFO 2017-06-30 11:14:55,869 main.py:52] epoch 11323, training loss: 5109.15, average training loss: 5288.59, base loss: 15983.71
[INFO 2017-06-30 11:14:59,071 main.py:52] epoch 11324, training loss: 5064.64, average training loss: 5288.16, base loss: 15983.55
[INFO 2017-06-30 11:15:02,239 main.py:52] epoch 11325, training loss: 5357.99, average training loss: 5288.71, base loss: 15983.64
[INFO 2017-06-30 11:15:05,339 main.py:52] epoch 11326, training loss: 5404.58, average training loss: 5288.94, base loss: 15983.50
[INFO 2017-06-30 11:15:08,521 main.py:52] epoch 11327, training loss: 4919.84, average training loss: 5288.69, base loss: 15983.24
[INFO 2017-06-30 11:15:11,645 main.py:52] epoch 11328, training loss: 5611.17, average training loss: 5289.01, base loss: 15983.36
[INFO 2017-06-30 11:15:14,807 main.py:52] epoch 11329, training loss: 5182.24, average training loss: 5289.09, base loss: 15983.36
[INFO 2017-06-30 11:15:17,959 main.py:52] epoch 11330, training loss: 5253.81, average training loss: 5289.05, base loss: 15983.28
[INFO 2017-06-30 11:15:21,100 main.py:52] epoch 11331, training loss: 4745.25, average training loss: 5289.03, base loss: 15983.12
[INFO 2017-06-30 11:15:24,253 main.py:52] epoch 11332, training loss: 5199.82, average training loss: 5289.35, base loss: 15982.98
[INFO 2017-06-30 11:15:27,394 main.py:52] epoch 11333, training loss: 5325.87, average training loss: 5289.37, base loss: 15982.70
[INFO 2017-06-30 11:15:30,552 main.py:52] epoch 11334, training loss: 5405.22, average training loss: 5289.46, base loss: 15982.92
[INFO 2017-06-30 11:15:33,712 main.py:52] epoch 11335, training loss: 5690.12, average training loss: 5290.09, base loss: 15983.21
[INFO 2017-06-30 11:15:36,874 main.py:52] epoch 11336, training loss: 5033.36, average training loss: 5289.80, base loss: 15982.90
[INFO 2017-06-30 11:15:40,033 main.py:52] epoch 11337, training loss: 4984.76, average training loss: 5289.58, base loss: 15983.13
[INFO 2017-06-30 11:15:43,147 main.py:52] epoch 11338, training loss: 5593.09, average training loss: 5289.93, base loss: 15983.34
[INFO 2017-06-30 11:15:46,285 main.py:52] epoch 11339, training loss: 5132.97, average training loss: 5289.72, base loss: 15983.23
[INFO 2017-06-30 11:15:49,414 main.py:52] epoch 11340, training loss: 5533.06, average training loss: 5289.97, base loss: 15983.19
[INFO 2017-06-30 11:15:52,583 main.py:52] epoch 11341, training loss: 5288.77, average training loss: 5290.23, base loss: 15983.03
[INFO 2017-06-30 11:15:55,685 main.py:52] epoch 11342, training loss: 5410.07, average training loss: 5290.25, base loss: 15983.03
[INFO 2017-06-30 11:15:58,831 main.py:52] epoch 11343, training loss: 5664.10, average training loss: 5291.10, base loss: 15983.10
[INFO 2017-06-30 11:16:02,060 main.py:52] epoch 11344, training loss: 5205.92, average training loss: 5290.95, base loss: 15983.30
[INFO 2017-06-30 11:16:05,189 main.py:52] epoch 11345, training loss: 4992.26, average training loss: 5290.35, base loss: 15983.27
[INFO 2017-06-30 11:16:08,349 main.py:52] epoch 11346, training loss: 5469.84, average training loss: 5290.23, base loss: 15983.55
[INFO 2017-06-30 11:16:11,512 main.py:52] epoch 11347, training loss: 5578.81, average training loss: 5290.68, base loss: 15983.88
[INFO 2017-06-30 11:16:14,670 main.py:52] epoch 11348, training loss: 5012.37, average training loss: 5290.35, base loss: 15983.47
[INFO 2017-06-30 11:16:17,859 main.py:52] epoch 11349, training loss: 4903.20, average training loss: 5290.10, base loss: 15983.18
[INFO 2017-06-30 11:16:21,003 main.py:52] epoch 11350, training loss: 5150.24, average training loss: 5290.38, base loss: 15982.81
[INFO 2017-06-30 11:16:24,341 main.py:52] epoch 11351, training loss: 5413.71, average training loss: 5290.77, base loss: 15982.60
[INFO 2017-06-30 11:16:27,479 main.py:52] epoch 11352, training loss: 5558.77, average training loss: 5290.84, base loss: 15982.64
[INFO 2017-06-30 11:16:30,625 main.py:52] epoch 11353, training loss: 4967.39, average training loss: 5290.13, base loss: 15982.35
[INFO 2017-06-30 11:16:33,803 main.py:52] epoch 11354, training loss: 4942.34, average training loss: 5289.71, base loss: 15982.08
[INFO 2017-06-30 11:16:36,994 main.py:52] epoch 11355, training loss: 5004.38, average training loss: 5289.40, base loss: 15981.71
[INFO 2017-06-30 11:16:40,144 main.py:52] epoch 11356, training loss: 5209.89, average training loss: 5289.24, base loss: 15981.76
[INFO 2017-06-30 11:16:43,290 main.py:52] epoch 11357, training loss: 4964.36, average training loss: 5289.14, base loss: 15981.76
[INFO 2017-06-30 11:16:46,416 main.py:52] epoch 11358, training loss: 4789.63, average training loss: 5288.74, base loss: 15981.60
[INFO 2017-06-30 11:16:49,541 main.py:52] epoch 11359, training loss: 5245.61, average training loss: 5288.20, base loss: 15981.84
[INFO 2017-06-30 11:16:52,645 main.py:52] epoch 11360, training loss: 5685.23, average training loss: 5288.83, base loss: 15981.99
[INFO 2017-06-30 11:16:55,795 main.py:52] epoch 11361, training loss: 5143.33, average training loss: 5288.32, base loss: 15982.31
[INFO 2017-06-30 11:16:58,959 main.py:52] epoch 11362, training loss: 5214.49, average training loss: 5288.26, base loss: 15982.29
[INFO 2017-06-30 11:17:02,065 main.py:52] epoch 11363, training loss: 5065.23, average training loss: 5288.21, base loss: 15982.13
[INFO 2017-06-30 11:17:05,212 main.py:52] epoch 11364, training loss: 5688.04, average training loss: 5288.79, base loss: 15982.31
[INFO 2017-06-30 11:17:08,349 main.py:52] epoch 11365, training loss: 5173.12, average training loss: 5288.68, base loss: 15981.96
[INFO 2017-06-30 11:17:11,466 main.py:52] epoch 11366, training loss: 5603.01, average training loss: 5288.87, base loss: 15981.97
[INFO 2017-06-30 11:17:14,647 main.py:52] epoch 11367, training loss: 5244.73, average training loss: 5288.70, base loss: 15981.84
[INFO 2017-06-30 11:17:17,758 main.py:52] epoch 11368, training loss: 5472.79, average training loss: 5288.71, base loss: 15981.68
[INFO 2017-06-30 11:17:20,910 main.py:52] epoch 11369, training loss: 5067.10, average training loss: 5288.72, base loss: 15981.44
[INFO 2017-06-30 11:17:24,018 main.py:52] epoch 11370, training loss: 5014.02, average training loss: 5288.62, base loss: 15981.33
[INFO 2017-06-30 11:17:27,155 main.py:52] epoch 11371, training loss: 5424.73, average training loss: 5288.86, base loss: 15981.42
[INFO 2017-06-30 11:17:30,306 main.py:52] epoch 11372, training loss: 5165.84, average training loss: 5288.96, base loss: 15981.43
[INFO 2017-06-30 11:17:33,469 main.py:52] epoch 11373, training loss: 5255.72, average training loss: 5288.94, base loss: 15981.28
[INFO 2017-06-30 11:17:36,575 main.py:52] epoch 11374, training loss: 5509.21, average training loss: 5289.08, base loss: 15981.47
[INFO 2017-06-30 11:17:39,696 main.py:52] epoch 11375, training loss: 5277.68, average training loss: 5288.45, base loss: 15981.44
[INFO 2017-06-30 11:17:42,826 main.py:52] epoch 11376, training loss: 5229.94, average training loss: 5287.76, base loss: 15981.54
[INFO 2017-06-30 11:17:45,983 main.py:52] epoch 11377, training loss: 5560.99, average training loss: 5287.85, base loss: 15981.86
[INFO 2017-06-30 11:17:49,133 main.py:52] epoch 11378, training loss: 5159.04, average training loss: 5287.70, base loss: 15981.64
[INFO 2017-06-30 11:17:52,255 main.py:52] epoch 11379, training loss: 5433.76, average training loss: 5287.83, base loss: 15981.80
[INFO 2017-06-30 11:17:55,391 main.py:52] epoch 11380, training loss: 5653.69, average training loss: 5287.92, base loss: 15982.14
[INFO 2017-06-30 11:17:58,536 main.py:52] epoch 11381, training loss: 5637.46, average training loss: 5287.79, base loss: 15982.16
[INFO 2017-06-30 11:18:01,713 main.py:52] epoch 11382, training loss: 5214.57, average training loss: 5287.37, base loss: 15982.01
[INFO 2017-06-30 11:18:04,829 main.py:52] epoch 11383, training loss: 5061.67, average training loss: 5287.20, base loss: 15981.66
[INFO 2017-06-30 11:18:07,971 main.py:52] epoch 11384, training loss: 5539.62, average training loss: 5287.26, base loss: 15981.97
[INFO 2017-06-30 11:18:11,098 main.py:52] epoch 11385, training loss: 5316.40, average training loss: 5287.34, base loss: 15981.98
[INFO 2017-06-30 11:18:14,242 main.py:52] epoch 11386, training loss: 5024.06, average training loss: 5286.51, base loss: 15981.55
[INFO 2017-06-30 11:18:17,353 main.py:52] epoch 11387, training loss: 5674.42, average training loss: 5286.82, base loss: 15981.55
[INFO 2017-06-30 11:18:20,504 main.py:52] epoch 11388, training loss: 5113.48, average training loss: 5286.43, base loss: 15981.63
[INFO 2017-06-30 11:18:23,626 main.py:52] epoch 11389, training loss: 5330.04, average training loss: 5286.55, base loss: 15981.66
[INFO 2017-06-30 11:18:26,809 main.py:52] epoch 11390, training loss: 5314.79, average training loss: 5286.50, base loss: 15981.96
[INFO 2017-06-30 11:18:29,950 main.py:52] epoch 11391, training loss: 5209.36, average training loss: 5286.73, base loss: 15982.11
[INFO 2017-06-30 11:18:33,182 main.py:52] epoch 11392, training loss: 5597.90, average training loss: 5286.99, base loss: 15982.40
[INFO 2017-06-30 11:18:36,329 main.py:52] epoch 11393, training loss: 5303.27, average training loss: 5286.95, base loss: 15982.45
[INFO 2017-06-30 11:18:39,443 main.py:52] epoch 11394, training loss: 5606.19, average training loss: 5287.31, base loss: 15982.84
[INFO 2017-06-30 11:18:42,576 main.py:52] epoch 11395, training loss: 5336.06, average training loss: 5287.36, base loss: 15983.04
[INFO 2017-06-30 11:18:45,759 main.py:52] epoch 11396, training loss: 5458.70, average training loss: 5287.52, base loss: 15983.06
[INFO 2017-06-30 11:18:48,902 main.py:52] epoch 11397, training loss: 5235.80, average training loss: 5287.49, base loss: 15983.14
[INFO 2017-06-30 11:18:52,003 main.py:52] epoch 11398, training loss: 5387.66, average training loss: 5287.57, base loss: 15983.15
[INFO 2017-06-30 11:18:55,138 main.py:52] epoch 11399, training loss: 5717.18, average training loss: 5288.35, base loss: 15983.31
[INFO 2017-06-30 11:18:55,139 main.py:54] epoch 11399, testing
[INFO 2017-06-30 11:19:08,219 main.py:97] average testing loss: 5160.36, base loss: 15393.69
[INFO 2017-06-30 11:19:08,220 main.py:98] improve_loss: 10233.33, improve_percent: 0.66
[INFO 2017-06-30 11:19:08,222 main.py:66] current best improved percent: 0.68
[INFO 2017-06-30 11:19:11,366 main.py:52] epoch 11400, training loss: 5376.69, average training loss: 5288.31, base loss: 15983.07
[INFO 2017-06-30 11:19:14,496 main.py:52] epoch 11401, training loss: 5137.21, average training loss: 5288.35, base loss: 15982.94
[INFO 2017-06-30 11:19:17,633 main.py:52] epoch 11402, training loss: 5149.84, average training loss: 5288.12, base loss: 15982.88
[INFO 2017-06-30 11:19:20,823 main.py:52] epoch 11403, training loss: 5394.07, average training loss: 5288.10, base loss: 15983.02
[INFO 2017-06-30 11:19:23,954 main.py:52] epoch 11404, training loss: 5029.60, average training loss: 5287.92, base loss: 15983.24
[INFO 2017-06-30 11:19:27,112 main.py:52] epoch 11405, training loss: 5544.14, average training loss: 5288.39, base loss: 15983.44
[INFO 2017-06-30 11:19:30,240 main.py:52] epoch 11406, training loss: 5258.26, average training loss: 5287.95, base loss: 15983.37
[INFO 2017-06-30 11:19:33,392 main.py:52] epoch 11407, training loss: 5692.83, average training loss: 5288.45, base loss: 15983.59
[INFO 2017-06-30 11:19:36,517 main.py:52] epoch 11408, training loss: 5035.77, average training loss: 5288.24, base loss: 15983.15
[INFO 2017-06-30 11:19:39,665 main.py:52] epoch 11409, training loss: 5119.03, average training loss: 5288.39, base loss: 15982.83
[INFO 2017-06-30 11:19:42,830 main.py:52] epoch 11410, training loss: 5040.18, average training loss: 5288.20, base loss: 15982.87
[INFO 2017-06-30 11:19:45,948 main.py:52] epoch 11411, training loss: 5291.95, average training loss: 5287.87, base loss: 15983.05
[INFO 2017-06-30 11:19:49,081 main.py:52] epoch 11412, training loss: 5176.97, average training loss: 5288.06, base loss: 15982.96
[INFO 2017-06-30 11:19:52,190 main.py:52] epoch 11413, training loss: 5099.25, average training loss: 5287.67, base loss: 15982.79
[INFO 2017-06-30 11:19:55,346 main.py:52] epoch 11414, training loss: 5378.51, average training loss: 5287.76, base loss: 15982.86
[INFO 2017-06-30 11:19:58,444 main.py:52] epoch 11415, training loss: 5402.30, average training loss: 5287.99, base loss: 15982.65
[INFO 2017-06-30 11:20:01,610 main.py:52] epoch 11416, training loss: 5178.77, average training loss: 5288.19, base loss: 15982.42
[INFO 2017-06-30 11:20:04,734 main.py:52] epoch 11417, training loss: 4938.19, average training loss: 5287.72, base loss: 15982.26
[INFO 2017-06-30 11:20:07,900 main.py:52] epoch 11418, training loss: 5243.30, average training loss: 5287.70, base loss: 15982.30
[INFO 2017-06-30 11:20:11,057 main.py:52] epoch 11419, training loss: 5159.43, average training loss: 5287.54, base loss: 15982.27
[INFO 2017-06-30 11:20:14,194 main.py:52] epoch 11420, training loss: 5478.86, average training loss: 5287.79, base loss: 15982.42
[INFO 2017-06-30 11:20:17,410 main.py:52] epoch 11421, training loss: 5281.49, average training loss: 5287.74, base loss: 15982.44
[INFO 2017-06-30 11:20:20,601 main.py:52] epoch 11422, training loss: 4748.57, average training loss: 5287.38, base loss: 15982.18
[INFO 2017-06-30 11:20:23,750 main.py:52] epoch 11423, training loss: 5305.26, average training loss: 5287.68, base loss: 15981.90
[INFO 2017-06-30 11:20:26,891 main.py:52] epoch 11424, training loss: 5494.91, average training loss: 5287.48, base loss: 15982.05
[INFO 2017-06-30 11:20:30,027 main.py:52] epoch 11425, training loss: 5217.25, average training loss: 5287.54, base loss: 15981.89
[INFO 2017-06-30 11:20:33,179 main.py:52] epoch 11426, training loss: 5308.87, average training loss: 5287.84, base loss: 15981.72
[INFO 2017-06-30 11:20:36,316 main.py:52] epoch 11427, training loss: 5250.50, average training loss: 5287.61, base loss: 15981.55
[INFO 2017-06-30 11:20:39,473 main.py:52] epoch 11428, training loss: 5307.54, average training loss: 5287.61, base loss: 15981.54
[INFO 2017-06-30 11:20:42,636 main.py:52] epoch 11429, training loss: 5604.25, average training loss: 5287.78, base loss: 15981.64
[INFO 2017-06-30 11:20:45,822 main.py:52] epoch 11430, training loss: 5063.42, average training loss: 5287.99, base loss: 15981.49
[INFO 2017-06-30 11:20:48,935 main.py:52] epoch 11431, training loss: 5334.05, average training loss: 5288.37, base loss: 15981.56
[INFO 2017-06-30 11:20:52,067 main.py:52] epoch 11432, training loss: 5683.48, average training loss: 5289.11, base loss: 15981.85
[INFO 2017-06-30 11:20:55,215 main.py:52] epoch 11433, training loss: 5225.35, average training loss: 5289.47, base loss: 15981.88
[INFO 2017-06-30 11:20:58,362 main.py:52] epoch 11434, training loss: 5438.33, average training loss: 5289.45, base loss: 15981.82
[INFO 2017-06-30 11:21:01,495 main.py:52] epoch 11435, training loss: 5145.32, average training loss: 5289.11, base loss: 15981.71
[INFO 2017-06-30 11:21:04,615 main.py:52] epoch 11436, training loss: 5213.03, average training loss: 5289.07, base loss: 15981.60
[INFO 2017-06-30 11:21:07,768 main.py:52] epoch 11437, training loss: 5324.82, average training loss: 5289.12, base loss: 15981.63
[INFO 2017-06-30 11:21:10,925 main.py:52] epoch 11438, training loss: 5147.63, average training loss: 5288.85, base loss: 15981.65
[INFO 2017-06-30 11:21:14,068 main.py:52] epoch 11439, training loss: 5223.83, average training loss: 5288.81, base loss: 15981.91
[INFO 2017-06-30 11:21:17,206 main.py:52] epoch 11440, training loss: 5505.92, average training loss: 5288.47, base loss: 15982.16
[INFO 2017-06-30 11:21:20,339 main.py:52] epoch 11441, training loss: 4945.55, average training loss: 5288.31, base loss: 15981.98
[INFO 2017-06-30 11:21:23,521 main.py:52] epoch 11442, training loss: 5384.92, average training loss: 5288.49, base loss: 15982.20
[INFO 2017-06-30 11:21:26,635 main.py:52] epoch 11443, training loss: 5883.04, average training loss: 5288.59, base loss: 15982.49
[INFO 2017-06-30 11:21:29,789 main.py:52] epoch 11444, training loss: 5297.60, average training loss: 5288.35, base loss: 15982.75
[INFO 2017-06-30 11:21:32,934 main.py:52] epoch 11445, training loss: 4869.13, average training loss: 5287.86, base loss: 15982.60
[INFO 2017-06-30 11:21:36,065 main.py:52] epoch 11446, training loss: 5179.96, average training loss: 5287.70, base loss: 15982.73
[INFO 2017-06-30 11:21:39,214 main.py:52] epoch 11447, training loss: 4878.12, average training loss: 5287.53, base loss: 15982.57
[INFO 2017-06-30 11:21:42,358 main.py:52] epoch 11448, training loss: 4909.29, average training loss: 5287.35, base loss: 15982.20
[INFO 2017-06-30 11:21:45,497 main.py:52] epoch 11449, training loss: 5298.49, average training loss: 5287.27, base loss: 15982.22
[INFO 2017-06-30 11:21:48,616 main.py:52] epoch 11450, training loss: 5519.47, average training loss: 5287.07, base loss: 15982.54
[INFO 2017-06-30 11:21:51,793 main.py:52] epoch 11451, training loss: 5068.29, average training loss: 5286.81, base loss: 15982.27
[INFO 2017-06-30 11:21:54,938 main.py:52] epoch 11452, training loss: 5247.58, average training loss: 5287.04, base loss: 15982.46
[INFO 2017-06-30 11:21:58,081 main.py:52] epoch 11453, training loss: 5316.46, average training loss: 5287.10, base loss: 15982.86
[INFO 2017-06-30 11:22:01,237 main.py:52] epoch 11454, training loss: 5774.95, average training loss: 5287.75, base loss: 15983.19
[INFO 2017-06-30 11:22:04,428 main.py:52] epoch 11455, training loss: 5397.16, average training loss: 5287.89, base loss: 15983.35
[INFO 2017-06-30 11:22:07,536 main.py:52] epoch 11456, training loss: 4959.32, average training loss: 5287.65, base loss: 15983.37
[INFO 2017-06-30 11:22:10,716 main.py:52] epoch 11457, training loss: 5373.56, average training loss: 5288.05, base loss: 15983.28
[INFO 2017-06-30 11:22:13,850 main.py:52] epoch 11458, training loss: 5338.93, average training loss: 5288.24, base loss: 15983.04
[INFO 2017-06-30 11:22:16,985 main.py:52] epoch 11459, training loss: 5199.80, average training loss: 5288.08, base loss: 15982.84
[INFO 2017-06-30 11:22:20,116 main.py:52] epoch 11460, training loss: 5591.34, average training loss: 5288.57, base loss: 15983.21
[INFO 2017-06-30 11:22:23,284 main.py:52] epoch 11461, training loss: 5739.29, average training loss: 5289.00, base loss: 15983.48
[INFO 2017-06-30 11:22:26,415 main.py:52] epoch 11462, training loss: 5055.27, average training loss: 5288.70, base loss: 15983.30
[INFO 2017-06-30 11:22:29,546 main.py:52] epoch 11463, training loss: 5315.31, average training loss: 5288.59, base loss: 15983.37
[INFO 2017-06-30 11:22:32,704 main.py:52] epoch 11464, training loss: 4926.59, average training loss: 5288.38, base loss: 15983.04
[INFO 2017-06-30 11:22:35,818 main.py:52] epoch 11465, training loss: 5268.76, average training loss: 5288.13, base loss: 15983.10
[INFO 2017-06-30 11:22:38,928 main.py:52] epoch 11466, training loss: 5312.29, average training loss: 5288.04, base loss: 15983.09
[INFO 2017-06-30 11:22:42,098 main.py:52] epoch 11467, training loss: 5181.46, average training loss: 5287.86, base loss: 15982.94
[INFO 2017-06-30 11:22:45,198 main.py:52] epoch 11468, training loss: 4764.85, average training loss: 5287.27, base loss: 15982.40
[INFO 2017-06-30 11:22:48,320 main.py:52] epoch 11469, training loss: 5433.44, average training loss: 5287.42, base loss: 15982.43
[INFO 2017-06-30 11:22:51,478 main.py:52] epoch 11470, training loss: 5480.61, average training loss: 5287.57, base loss: 15982.45
[INFO 2017-06-30 11:22:54,631 main.py:52] epoch 11471, training loss: 5235.33, average training loss: 5287.47, base loss: 15982.22
[INFO 2017-06-30 11:22:57,812 main.py:52] epoch 11472, training loss: 4904.84, average training loss: 5287.43, base loss: 15982.17
[INFO 2017-06-30 11:23:00,982 main.py:52] epoch 11473, training loss: 5254.16, average training loss: 5287.36, base loss: 15982.34
[INFO 2017-06-30 11:23:04,112 main.py:52] epoch 11474, training loss: 5301.96, average training loss: 5287.15, base loss: 15982.03
[INFO 2017-06-30 11:23:07,264 main.py:52] epoch 11475, training loss: 5406.77, average training loss: 5287.63, base loss: 15981.85
[INFO 2017-06-30 11:23:10,410 main.py:52] epoch 11476, training loss: 5507.24, average training loss: 5288.06, base loss: 15982.03
[INFO 2017-06-30 11:23:13,555 main.py:52] epoch 11477, training loss: 5212.99, average training loss: 5287.56, base loss: 15982.09
[INFO 2017-06-30 11:23:16,707 main.py:52] epoch 11478, training loss: 5216.04, average training loss: 5287.63, base loss: 15981.88
[INFO 2017-06-30 11:23:19,878 main.py:52] epoch 11479, training loss: 5357.97, average training loss: 5287.56, base loss: 15981.76
[INFO 2017-06-30 11:23:23,051 main.py:52] epoch 11480, training loss: 5134.64, average training loss: 5287.52, base loss: 15981.80
[INFO 2017-06-30 11:23:26,189 main.py:52] epoch 11481, training loss: 4955.89, average training loss: 5287.10, base loss: 15981.43
[INFO 2017-06-30 11:23:29,321 main.py:52] epoch 11482, training loss: 5690.16, average training loss: 5287.69, base loss: 15981.62
[INFO 2017-06-30 11:23:32,467 main.py:52] epoch 11483, training loss: 4993.34, average training loss: 5287.36, base loss: 15981.12
[INFO 2017-06-30 11:23:35,653 main.py:52] epoch 11484, training loss: 5297.15, average training loss: 5287.39, base loss: 15981.15
[INFO 2017-06-30 11:23:38,792 main.py:52] epoch 11485, training loss: 5407.31, average training loss: 5287.39, base loss: 15981.49
[INFO 2017-06-30 11:23:41,920 main.py:52] epoch 11486, training loss: 5288.82, average training loss: 5287.44, base loss: 15981.44
[INFO 2017-06-30 11:23:45,070 main.py:52] epoch 11487, training loss: 5705.24, average training loss: 5287.70, base loss: 15981.55
[INFO 2017-06-30 11:23:48,223 main.py:52] epoch 11488, training loss: 5241.94, average training loss: 5287.82, base loss: 15981.60
[INFO 2017-06-30 11:23:51,307 main.py:52] epoch 11489, training loss: 5263.63, average training loss: 5287.76, base loss: 15981.54
[INFO 2017-06-30 11:23:54,430 main.py:52] epoch 11490, training loss: 5555.13, average training loss: 5288.19, base loss: 15981.67
[INFO 2017-06-30 11:23:57,646 main.py:52] epoch 11491, training loss: 5187.32, average training loss: 5288.20, base loss: 15981.73
[INFO 2017-06-30 11:24:00,787 main.py:52] epoch 11492, training loss: 5352.45, average training loss: 5288.12, base loss: 15981.65
[INFO 2017-06-30 11:24:03,978 main.py:52] epoch 11493, training loss: 5419.39, average training loss: 5288.15, base loss: 15981.65
[INFO 2017-06-30 11:24:07,116 main.py:52] epoch 11494, training loss: 5364.58, average training loss: 5288.35, base loss: 15981.95
[INFO 2017-06-30 11:24:10,226 main.py:52] epoch 11495, training loss: 5212.17, average training loss: 5288.38, base loss: 15981.93
[INFO 2017-06-30 11:24:13,388 main.py:52] epoch 11496, training loss: 5050.42, average training loss: 5288.07, base loss: 15981.63
[INFO 2017-06-30 11:24:16,537 main.py:52] epoch 11497, training loss: 5713.02, average training loss: 5288.29, base loss: 15981.91
[INFO 2017-06-30 11:24:19,670 main.py:52] epoch 11498, training loss: 5070.92, average training loss: 5288.05, base loss: 15981.55
[INFO 2017-06-30 11:24:22,838 main.py:52] epoch 11499, training loss: 5502.27, average training loss: 5288.73, base loss: 15981.68
[INFO 2017-06-30 11:24:22,839 main.py:54] epoch 11499, testing
[INFO 2017-06-30 11:24:35,917 main.py:97] average testing loss: 5304.08, base loss: 16039.72
[INFO 2017-06-30 11:24:35,917 main.py:98] improve_loss: 10735.64, improve_percent: 0.67
[INFO 2017-06-30 11:24:35,919 main.py:66] current best improved percent: 0.68
[INFO 2017-06-30 11:24:39,073 main.py:52] epoch 11500, training loss: 5105.51, average training loss: 5288.54, base loss: 15981.72
[INFO 2017-06-30 11:24:42,231 main.py:52] epoch 11501, training loss: 5481.18, average training loss: 5288.78, base loss: 15981.85
[INFO 2017-06-30 11:24:45,355 main.py:52] epoch 11502, training loss: 5073.23, average training loss: 5288.49, base loss: 15981.70
[INFO 2017-06-30 11:24:48,480 main.py:52] epoch 11503, training loss: 5039.39, average training loss: 5288.17, base loss: 15981.34
[INFO 2017-06-30 11:24:51,615 main.py:52] epoch 11504, training loss: 5231.89, average training loss: 5287.59, base loss: 15981.29
[INFO 2017-06-30 11:24:54,785 main.py:52] epoch 11505, training loss: 5149.34, average training loss: 5287.70, base loss: 15981.10
[INFO 2017-06-30 11:24:57,972 main.py:52] epoch 11506, training loss: 5036.69, average training loss: 5287.58, base loss: 15980.99
[INFO 2017-06-30 11:25:01,080 main.py:52] epoch 11507, training loss: 5206.78, average training loss: 5287.36, base loss: 15980.90
[INFO 2017-06-30 11:25:04,243 main.py:52] epoch 11508, training loss: 5023.49, average training loss: 5287.16, base loss: 15980.71
[INFO 2017-06-30 11:25:07,398 main.py:52] epoch 11509, training loss: 5457.43, average training loss: 5287.12, base loss: 15980.91
[INFO 2017-06-30 11:25:10,543 main.py:52] epoch 11510, training loss: 5093.68, average training loss: 5286.85, base loss: 15980.77
[INFO 2017-06-30 11:25:13,658 main.py:52] epoch 11511, training loss: 5085.80, average training loss: 5286.51, base loss: 15980.65
[INFO 2017-06-30 11:25:16,785 main.py:52] epoch 11512, training loss: 5232.34, average training loss: 5286.39, base loss: 15980.55
[INFO 2017-06-30 11:25:19,872 main.py:52] epoch 11513, training loss: 5450.88, average training loss: 5286.58, base loss: 15980.78
[INFO 2017-06-30 11:25:23,019 main.py:52] epoch 11514, training loss: 5300.63, average training loss: 5286.76, base loss: 15980.68
[INFO 2017-06-30 11:25:26,191 main.py:52] epoch 11515, training loss: 5294.32, average training loss: 5286.54, base loss: 15980.45
[INFO 2017-06-30 11:25:29,312 main.py:52] epoch 11516, training loss: 4952.95, average training loss: 5286.12, base loss: 15980.35
[INFO 2017-06-30 11:25:32,474 main.py:52] epoch 11517, training loss: 5155.85, average training loss: 5286.49, base loss: 15980.52
[INFO 2017-06-30 11:25:35,622 main.py:52] epoch 11518, training loss: 5254.17, average training loss: 5286.10, base loss: 15980.20
[INFO 2017-06-30 11:25:38,784 main.py:52] epoch 11519, training loss: 5611.42, average training loss: 5286.73, base loss: 15980.36
[INFO 2017-06-30 11:25:41,959 main.py:52] epoch 11520, training loss: 5007.57, average training loss: 5286.29, base loss: 15979.97
[INFO 2017-06-30 11:25:45,090 main.py:52] epoch 11521, training loss: 5153.55, average training loss: 5286.43, base loss: 15979.61
[INFO 2017-06-30 11:25:48,250 main.py:52] epoch 11522, training loss: 5020.12, average training loss: 5286.19, base loss: 15979.45
[INFO 2017-06-30 11:25:51,396 main.py:52] epoch 11523, training loss: 5028.72, average training loss: 5285.77, base loss: 15979.19
[INFO 2017-06-30 11:25:54,562 main.py:52] epoch 11524, training loss: 5212.44, average training loss: 5285.56, base loss: 15979.05
[INFO 2017-06-30 11:25:57,706 main.py:52] epoch 11525, training loss: 5551.88, average training loss: 5285.73, base loss: 15979.14
[INFO 2017-06-30 11:26:00,841 main.py:52] epoch 11526, training loss: 5414.12, average training loss: 5286.07, base loss: 15979.24
[INFO 2017-06-30 11:26:03,958 main.py:52] epoch 11527, training loss: 5232.93, average training loss: 5285.98, base loss: 15979.45
[INFO 2017-06-30 11:26:07,107 main.py:52] epoch 11528, training loss: 5422.20, average training loss: 5286.19, base loss: 15979.68
[INFO 2017-06-30 11:26:10,246 main.py:52] epoch 11529, training loss: 5523.49, average training loss: 5286.44, base loss: 15979.93
[INFO 2017-06-30 11:26:13,420 main.py:52] epoch 11530, training loss: 5241.95, average training loss: 5286.23, base loss: 15979.88
[INFO 2017-06-30 11:26:16,587 main.py:52] epoch 11531, training loss: 4850.76, average training loss: 5285.74, base loss: 15979.54
[INFO 2017-06-30 11:26:19,720 main.py:52] epoch 11532, training loss: 5587.44, average training loss: 5285.80, base loss: 15980.16
[INFO 2017-06-30 11:26:22,847 main.py:52] epoch 11533, training loss: 5268.48, average training loss: 5285.88, base loss: 15980.35
[INFO 2017-06-30 11:26:25,956 main.py:52] epoch 11534, training loss: 5138.16, average training loss: 5285.66, base loss: 15980.26
[INFO 2017-06-30 11:26:29,120 main.py:52] epoch 11535, training loss: 5212.00, average training loss: 5285.43, base loss: 15980.15
[INFO 2017-06-30 11:26:32,286 main.py:52] epoch 11536, training loss: 5224.94, average training loss: 5285.28, base loss: 15979.97
[INFO 2017-06-30 11:26:35,439 main.py:52] epoch 11537, training loss: 5290.32, average training loss: 5285.49, base loss: 15979.80
[INFO 2017-06-30 11:26:38,608 main.py:52] epoch 11538, training loss: 5210.89, average training loss: 5285.88, base loss: 15979.80
[INFO 2017-06-30 11:26:41,775 main.py:52] epoch 11539, training loss: 5278.01, average training loss: 5285.82, base loss: 15979.70
[INFO 2017-06-30 11:26:44,903 main.py:52] epoch 11540, training loss: 4975.85, average training loss: 5285.47, base loss: 15979.56
[INFO 2017-06-30 11:26:48,058 main.py:52] epoch 11541, training loss: 5550.97, average training loss: 5285.98, base loss: 15979.58
[INFO 2017-06-30 11:26:51,206 main.py:52] epoch 11542, training loss: 5443.78, average training loss: 5286.26, base loss: 15979.80
[INFO 2017-06-30 11:26:54,348 main.py:52] epoch 11543, training loss: 5401.95, average training loss: 5286.25, base loss: 15979.81
[INFO 2017-06-30 11:26:57,503 main.py:52] epoch 11544, training loss: 5276.67, average training loss: 5286.32, base loss: 15979.72
[INFO 2017-06-30 11:27:00,622 main.py:52] epoch 11545, training loss: 4924.43, average training loss: 5285.71, base loss: 15979.41
[INFO 2017-06-30 11:27:03,751 main.py:52] epoch 11546, training loss: 5165.53, average training loss: 5285.60, base loss: 15979.22
[INFO 2017-06-30 11:27:06,910 main.py:52] epoch 11547, training loss: 5361.44, average training loss: 5285.42, base loss: 15979.21
[INFO 2017-06-30 11:27:10,076 main.py:52] epoch 11548, training loss: 5385.40, average training loss: 5285.44, base loss: 15979.09
[INFO 2017-06-30 11:27:13,225 main.py:52] epoch 11549, training loss: 5392.80, average training loss: 5285.47, base loss: 15978.93
[INFO 2017-06-30 11:27:16,359 main.py:52] epoch 11550, training loss: 5264.72, average training loss: 5285.81, base loss: 15978.91
[INFO 2017-06-30 11:27:19,524 main.py:52] epoch 11551, training loss: 5086.52, average training loss: 5285.69, base loss: 15978.75
[INFO 2017-06-30 11:27:22,753 main.py:52] epoch 11552, training loss: 5253.39, average training loss: 5285.64, base loss: 15978.62
[INFO 2017-06-30 11:27:25,883 main.py:52] epoch 11553, training loss: 5470.24, average training loss: 5286.33, base loss: 15978.88
[INFO 2017-06-30 11:27:29,033 main.py:52] epoch 11554, training loss: 5611.92, average training loss: 5286.66, base loss: 15979.20
[INFO 2017-06-30 11:27:32,178 main.py:52] epoch 11555, training loss: 5051.14, average training loss: 5286.36, base loss: 15979.18
[INFO 2017-06-30 11:27:35,354 main.py:52] epoch 11556, training loss: 5082.14, average training loss: 5286.20, base loss: 15978.97
[INFO 2017-06-30 11:27:38,470 main.py:52] epoch 11557, training loss: 5184.27, average training loss: 5286.22, base loss: 15978.87
[INFO 2017-06-30 11:27:41,618 main.py:52] epoch 11558, training loss: 5435.44, average training loss: 5286.57, base loss: 15979.07
[INFO 2017-06-30 11:27:44,755 main.py:52] epoch 11559, training loss: 5340.18, average training loss: 5286.89, base loss: 15979.02
[INFO 2017-06-30 11:27:47,875 main.py:52] epoch 11560, training loss: 5333.00, average training loss: 5286.81, base loss: 15979.36
[INFO 2017-06-30 11:27:51,096 main.py:52] epoch 11561, training loss: 5376.92, average training loss: 5287.01, base loss: 15979.78
[INFO 2017-06-30 11:27:54,254 main.py:52] epoch 11562, training loss: 5086.41, average training loss: 5286.98, base loss: 15979.71
[INFO 2017-06-30 11:27:57,390 main.py:52] epoch 11563, training loss: 5663.29, average training loss: 5287.56, base loss: 15979.94
[INFO 2017-06-30 11:28:00,517 main.py:52] epoch 11564, training loss: 5515.29, average training loss: 5287.98, base loss: 15979.99
[INFO 2017-06-30 11:28:03,656 main.py:52] epoch 11565, training loss: 5396.75, average training loss: 5288.08, base loss: 15979.72
[INFO 2017-06-30 11:28:06,778 main.py:52] epoch 11566, training loss: 4966.99, average training loss: 5287.63, base loss: 15979.57
[INFO 2017-06-30 11:28:09,911 main.py:52] epoch 11567, training loss: 5385.35, average training loss: 5287.64, base loss: 15979.55
[INFO 2017-06-30 11:28:13,034 main.py:52] epoch 11568, training loss: 5006.33, average training loss: 5287.51, base loss: 15979.36
[INFO 2017-06-30 11:28:16,192 main.py:52] epoch 11569, training loss: 5459.15, average training loss: 5287.42, base loss: 15979.21
[INFO 2017-06-30 11:28:19,319 main.py:52] epoch 11570, training loss: 5597.06, average training loss: 5287.63, base loss: 15979.43
[INFO 2017-06-30 11:28:22,473 main.py:52] epoch 11571, training loss: 5263.69, average training loss: 5287.84, base loss: 15979.17
[INFO 2017-06-30 11:28:25,608 main.py:52] epoch 11572, training loss: 5167.69, average training loss: 5287.99, base loss: 15979.04
[INFO 2017-06-30 11:28:28,723 main.py:52] epoch 11573, training loss: 5191.69, average training loss: 5287.88, base loss: 15978.90
[INFO 2017-06-30 11:28:31,917 main.py:52] epoch 11574, training loss: 5193.75, average training loss: 5288.12, base loss: 15978.79
[INFO 2017-06-30 11:28:35,055 main.py:52] epoch 11575, training loss: 5317.55, average training loss: 5288.24, base loss: 15978.93
[INFO 2017-06-30 11:28:38,219 main.py:52] epoch 11576, training loss: 5421.99, average training loss: 5288.17, base loss: 15979.04
[INFO 2017-06-30 11:28:41,342 main.py:52] epoch 11577, training loss: 5119.42, average training loss: 5288.20, base loss: 15979.13
[INFO 2017-06-30 11:28:44,495 main.py:52] epoch 11578, training loss: 5409.53, average training loss: 5287.78, base loss: 15979.21
[INFO 2017-06-30 11:28:47,633 main.py:52] epoch 11579, training loss: 5512.66, average training loss: 5288.14, base loss: 15979.46
[INFO 2017-06-30 11:28:50,753 main.py:52] epoch 11580, training loss: 5326.33, average training loss: 5287.97, base loss: 15979.53
[INFO 2017-06-30 11:28:53,935 main.py:52] epoch 11581, training loss: 5353.73, average training loss: 5287.63, base loss: 15979.58
[INFO 2017-06-30 11:28:57,057 main.py:52] epoch 11582, training loss: 5020.03, average training loss: 5287.70, base loss: 15979.32
[INFO 2017-06-30 11:29:00,238 main.py:52] epoch 11583, training loss: 4927.14, average training loss: 5287.47, base loss: 15978.90
[INFO 2017-06-30 11:29:03,361 main.py:52] epoch 11584, training loss: 5425.00, average training loss: 5287.60, base loss: 15979.03
[INFO 2017-06-30 11:29:06,498 main.py:52] epoch 11585, training loss: 5125.56, average training loss: 5287.31, base loss: 15978.88
[INFO 2017-06-30 11:29:09,647 main.py:52] epoch 11586, training loss: 5284.89, average training loss: 5287.27, base loss: 15978.83
[INFO 2017-06-30 11:29:12,810 main.py:52] epoch 11587, training loss: 5391.76, average training loss: 5287.19, base loss: 15978.92
[INFO 2017-06-30 11:29:15,967 main.py:52] epoch 11588, training loss: 5307.56, average training loss: 5287.46, base loss: 15978.89
[INFO 2017-06-30 11:29:19,094 main.py:52] epoch 11589, training loss: 5129.86, average training loss: 5287.22, base loss: 15978.75
[INFO 2017-06-30 11:29:22,227 main.py:52] epoch 11590, training loss: 5178.35, average training loss: 5286.87, base loss: 15978.74
[INFO 2017-06-30 11:29:25,339 main.py:52] epoch 11591, training loss: 5409.24, average training loss: 5287.09, base loss: 15978.90
[INFO 2017-06-30 11:29:28,493 main.py:52] epoch 11592, training loss: 5242.10, average training loss: 5286.88, base loss: 15978.81
[INFO 2017-06-30 11:29:31,667 main.py:52] epoch 11593, training loss: 5341.52, average training loss: 5286.74, base loss: 15978.69
[INFO 2017-06-30 11:29:34,780 main.py:52] epoch 11594, training loss: 5026.07, average training loss: 5286.26, base loss: 15978.48
[INFO 2017-06-30 11:29:37,945 main.py:52] epoch 11595, training loss: 5330.80, average training loss: 5286.35, base loss: 15978.63
[INFO 2017-06-30 11:29:41,082 main.py:52] epoch 11596, training loss: 5078.07, average training loss: 5286.03, base loss: 15978.57
[INFO 2017-06-30 11:29:44,225 main.py:52] epoch 11597, training loss: 5025.20, average training loss: 5285.48, base loss: 15978.56
[INFO 2017-06-30 11:29:47,365 main.py:52] epoch 11598, training loss: 5311.70, average training loss: 5285.47, base loss: 15978.59
[INFO 2017-06-30 11:29:50,501 main.py:52] epoch 11599, training loss: 5325.97, average training loss: 5285.45, base loss: 15978.31
[INFO 2017-06-30 11:29:50,502 main.py:54] epoch 11599, testing
[INFO 2017-06-30 11:30:03,387 main.py:97] average testing loss: 5294.49, base loss: 16000.27
[INFO 2017-06-30 11:30:03,387 main.py:98] improve_loss: 10705.78, improve_percent: 0.67
[INFO 2017-06-30 11:30:03,389 main.py:66] current best improved percent: 0.68
[INFO 2017-06-30 11:30:06,562 main.py:52] epoch 11600, training loss: 5411.21, average training loss: 5285.76, base loss: 15978.49
[INFO 2017-06-30 11:30:09,693 main.py:52] epoch 11601, training loss: 4845.19, average training loss: 5285.49, base loss: 15978.24
[INFO 2017-06-30 11:30:12,841 main.py:52] epoch 11602, training loss: 5470.90, average training loss: 5285.45, base loss: 15978.36
[INFO 2017-06-30 11:30:15,974 main.py:52] epoch 11603, training loss: 5249.77, average training loss: 5285.26, base loss: 15978.39
[INFO 2017-06-30 11:30:19,135 main.py:52] epoch 11604, training loss: 5307.34, average training loss: 5284.91, base loss: 15978.57
[INFO 2017-06-30 11:30:22,256 main.py:52] epoch 11605, training loss: 5127.99, average training loss: 5285.10, base loss: 15978.65
[INFO 2017-06-30 11:30:25,353 main.py:52] epoch 11606, training loss: 5322.39, average training loss: 5285.47, base loss: 15978.74
[INFO 2017-06-30 11:30:28,503 main.py:52] epoch 11607, training loss: 5190.81, average training loss: 5285.66, base loss: 15978.73
[INFO 2017-06-30 11:30:31,620 main.py:52] epoch 11608, training loss: 5341.62, average training loss: 5285.50, base loss: 15978.94
[INFO 2017-06-30 11:30:34,725 main.py:52] epoch 11609, training loss: 5100.65, average training loss: 5285.33, base loss: 15978.87
[INFO 2017-06-30 11:30:37,890 main.py:52] epoch 11610, training loss: 5318.63, average training loss: 5285.11, base loss: 15978.83
[INFO 2017-06-30 11:30:41,038 main.py:52] epoch 11611, training loss: 5532.26, average training loss: 5285.33, base loss: 15978.91
[INFO 2017-06-30 11:30:44,158 main.py:52] epoch 11612, training loss: 5414.30, average training loss: 5285.57, base loss: 15978.89
[INFO 2017-06-30 11:30:47,353 main.py:52] epoch 11613, training loss: 5673.63, average training loss: 5285.89, base loss: 15978.96
[INFO 2017-06-30 11:30:50,442 main.py:52] epoch 11614, training loss: 4967.78, average training loss: 5285.33, base loss: 15978.93
[INFO 2017-06-30 11:30:53,586 main.py:52] epoch 11615, training loss: 5521.64, average training loss: 5285.29, base loss: 15979.38
[INFO 2017-06-30 11:30:56,740 main.py:52] epoch 11616, training loss: 5180.43, average training loss: 5285.39, base loss: 15979.32
[INFO 2017-06-30 11:30:59,866 main.py:52] epoch 11617, training loss: 5126.47, average training loss: 5285.10, base loss: 15979.22
[INFO 2017-06-30 11:31:03,012 main.py:52] epoch 11618, training loss: 4785.60, average training loss: 5284.55, base loss: 15979.14
[INFO 2017-06-30 11:31:06,200 main.py:52] epoch 11619, training loss: 5378.19, average training loss: 5284.36, base loss: 15979.08
[INFO 2017-06-30 11:31:09,376 main.py:52] epoch 11620, training loss: 5101.33, average training loss: 5284.51, base loss: 15979.23
[INFO 2017-06-30 11:31:12,584 main.py:52] epoch 11621, training loss: 5487.67, average training loss: 5284.90, base loss: 15979.32
[INFO 2017-06-30 11:31:15,732 main.py:52] epoch 11622, training loss: 5782.42, average training loss: 5285.14, base loss: 15979.68
[INFO 2017-06-30 11:31:18,896 main.py:52] epoch 11623, training loss: 5523.99, average training loss: 5285.32, base loss: 15979.94
[INFO 2017-06-30 11:31:22,053 main.py:52] epoch 11624, training loss: 5066.65, average training loss: 5285.30, base loss: 15979.88
[INFO 2017-06-30 11:31:25,232 main.py:52] epoch 11625, training loss: 5293.02, average training loss: 5285.48, base loss: 15980.01
[INFO 2017-06-30 11:31:28,391 main.py:52] epoch 11626, training loss: 5089.17, average training loss: 5285.69, base loss: 15979.97
[INFO 2017-06-30 11:31:31,516 main.py:52] epoch 11627, training loss: 5187.49, average training loss: 5285.91, base loss: 15979.91
[INFO 2017-06-30 11:31:34,627 main.py:52] epoch 11628, training loss: 5220.35, average training loss: 5286.11, base loss: 15979.72
[INFO 2017-06-30 11:31:37,769 main.py:52] epoch 11629, training loss: 5159.14, average training loss: 5286.38, base loss: 15979.75
[INFO 2017-06-30 11:31:40,903 main.py:52] epoch 11630, training loss: 5089.74, average training loss: 5285.99, base loss: 15979.76
[INFO 2017-06-30 11:31:44,028 main.py:52] epoch 11631, training loss: 5240.44, average training loss: 5286.04, base loss: 15979.56
[INFO 2017-06-30 11:31:47,152 main.py:52] epoch 11632, training loss: 5475.62, average training loss: 5286.37, base loss: 15979.68
[INFO 2017-06-30 11:31:50,263 main.py:52] epoch 11633, training loss: 5302.71, average training loss: 5286.38, base loss: 15979.61
[INFO 2017-06-30 11:31:53,410 main.py:52] epoch 11634, training loss: 5039.54, average training loss: 5286.12, base loss: 15979.53
[INFO 2017-06-30 11:31:56,589 main.py:52] epoch 11635, training loss: 5138.14, average training loss: 5286.48, base loss: 15979.61
[INFO 2017-06-30 11:31:59,745 main.py:52] epoch 11636, training loss: 4956.23, average training loss: 5286.30, base loss: 15979.61
[INFO 2017-06-30 11:32:02,942 main.py:52] epoch 11637, training loss: 5428.00, average training loss: 5286.67, base loss: 15979.87
[INFO 2017-06-30 11:32:06,075 main.py:52] epoch 11638, training loss: 5435.88, average training loss: 5286.93, base loss: 15980.01
[INFO 2017-06-30 11:32:09,230 main.py:52] epoch 11639, training loss: 5521.18, average training loss: 5287.11, base loss: 15980.12
[INFO 2017-06-30 11:32:12,415 main.py:52] epoch 11640, training loss: 5295.44, average training loss: 5286.97, base loss: 15980.22
[INFO 2017-06-30 11:32:15,522 main.py:52] epoch 11641, training loss: 5216.67, average training loss: 5287.21, base loss: 15980.09
[INFO 2017-06-30 11:32:18,670 main.py:52] epoch 11642, training loss: 5300.39, average training loss: 5286.50, base loss: 15979.90
[INFO 2017-06-30 11:32:21,844 main.py:52] epoch 11643, training loss: 5097.35, average training loss: 5285.84, base loss: 15979.71
[INFO 2017-06-30 11:32:24,978 main.py:52] epoch 11644, training loss: 5369.91, average training loss: 5285.54, base loss: 15979.83
[INFO 2017-06-30 11:32:28,098 main.py:52] epoch 11645, training loss: 5296.84, average training loss: 5285.36, base loss: 15979.70
[INFO 2017-06-30 11:32:31,277 main.py:52] epoch 11646, training loss: 5196.84, average training loss: 5285.40, base loss: 15979.64
[INFO 2017-06-30 11:32:34,429 main.py:52] epoch 11647, training loss: 5687.37, average training loss: 5285.31, base loss: 15980.11
[INFO 2017-06-30 11:32:37,564 main.py:52] epoch 11648, training loss: 5206.49, average training loss: 5285.30, base loss: 15979.95
[INFO 2017-06-30 11:32:40,764 main.py:52] epoch 11649, training loss: 5486.99, average training loss: 5285.43, base loss: 15979.85
[INFO 2017-06-30 11:32:43,887 main.py:52] epoch 11650, training loss: 5274.54, average training loss: 5284.87, base loss: 15980.05
[INFO 2017-06-30 11:32:47,068 main.py:52] epoch 11651, training loss: 5674.82, average training loss: 5285.27, base loss: 15980.27
[INFO 2017-06-30 11:32:50,226 main.py:52] epoch 11652, training loss: 5396.13, average training loss: 5285.56, base loss: 15980.41
[INFO 2017-06-30 11:32:53,347 main.py:52] epoch 11653, training loss: 5071.15, average training loss: 5285.38, base loss: 15980.42
[INFO 2017-06-30 11:32:56,578 main.py:52] epoch 11654, training loss: 5114.87, average training loss: 5285.30, base loss: 15980.32
[INFO 2017-06-30 11:32:59,692 main.py:52] epoch 11655, training loss: 4958.86, average training loss: 5284.67, base loss: 15980.09
[INFO 2017-06-30 11:33:02,822 main.py:52] epoch 11656, training loss: 5384.46, average training loss: 5284.47, base loss: 15980.18
[INFO 2017-06-30 11:33:05,965 main.py:52] epoch 11657, training loss: 4802.69, average training loss: 5283.84, base loss: 15979.88
[INFO 2017-06-30 11:33:09,097 main.py:52] epoch 11658, training loss: 5225.00, average training loss: 5283.83, base loss: 15979.71
[INFO 2017-06-30 11:33:12,282 main.py:52] epoch 11659, training loss: 4996.18, average training loss: 5283.86, base loss: 15979.59
[INFO 2017-06-30 11:33:15,408 main.py:52] epoch 11660, training loss: 5050.14, average training loss: 5283.79, base loss: 15979.35
[INFO 2017-06-30 11:33:18,573 main.py:52] epoch 11661, training loss: 5483.36, average training loss: 5284.03, base loss: 15979.49
[INFO 2017-06-30 11:33:21,716 main.py:52] epoch 11662, training loss: 5449.80, average training loss: 5284.04, base loss: 15979.59
[INFO 2017-06-30 11:33:24,865 main.py:52] epoch 11663, training loss: 5816.03, average training loss: 5284.53, base loss: 15979.62
[INFO 2017-06-30 11:33:28,016 main.py:52] epoch 11664, training loss: 4926.35, average training loss: 5284.48, base loss: 15979.30
[INFO 2017-06-30 11:33:31,150 main.py:52] epoch 11665, training loss: 5386.98, average training loss: 5284.41, base loss: 15979.18
[INFO 2017-06-30 11:33:34,285 main.py:52] epoch 11666, training loss: 5016.87, average training loss: 5284.16, base loss: 15978.81
[INFO 2017-06-30 11:33:37,432 main.py:52] epoch 11667, training loss: 5132.56, average training loss: 5283.99, base loss: 15978.71
[INFO 2017-06-30 11:33:40,552 main.py:52] epoch 11668, training loss: 5458.51, average training loss: 5283.82, base loss: 15978.89
[INFO 2017-06-30 11:33:43,703 main.py:52] epoch 11669, training loss: 5193.50, average training loss: 5284.03, base loss: 15978.67
[INFO 2017-06-30 11:33:46,838 main.py:52] epoch 11670, training loss: 5436.65, average training loss: 5284.21, base loss: 15978.79
[INFO 2017-06-30 11:33:49,952 main.py:52] epoch 11671, training loss: 5170.42, average training loss: 5284.49, base loss: 15978.96
[INFO 2017-06-30 11:33:53,093 main.py:52] epoch 11672, training loss: 5225.79, average training loss: 5284.43, base loss: 15978.87
[INFO 2017-06-30 11:33:56,254 main.py:52] epoch 11673, training loss: 5122.40, average training loss: 5284.37, base loss: 15978.83
[INFO 2017-06-30 11:33:59,425 main.py:52] epoch 11674, training loss: 5154.93, average training loss: 5284.26, base loss: 15978.74
[INFO 2017-06-30 11:34:02,516 main.py:52] epoch 11675, training loss: 5180.96, average training loss: 5283.87, base loss: 15978.40
[INFO 2017-06-30 11:34:05,696 main.py:52] epoch 11676, training loss: 5221.08, average training loss: 5283.50, base loss: 15978.37
[INFO 2017-06-30 11:34:08,869 main.py:52] epoch 11677, training loss: 5007.96, average training loss: 5283.20, base loss: 15978.09
[INFO 2017-06-30 11:34:12,024 main.py:52] epoch 11678, training loss: 5347.02, average training loss: 5283.14, base loss: 15978.16
[INFO 2017-06-30 11:34:15,173 main.py:52] epoch 11679, training loss: 4911.08, average training loss: 5282.58, base loss: 15977.95
[INFO 2017-06-30 11:34:18,291 main.py:52] epoch 11680, training loss: 5355.23, average training loss: 5282.39, base loss: 15978.13
[INFO 2017-06-30 11:34:21,470 main.py:52] epoch 11681, training loss: 5355.96, average training loss: 5282.53, base loss: 15978.21
[INFO 2017-06-30 11:34:24,662 main.py:52] epoch 11682, training loss: 5192.73, average training loss: 5282.29, base loss: 15978.13
[INFO 2017-06-30 11:34:27,843 main.py:52] epoch 11683, training loss: 5605.98, average training loss: 5282.82, base loss: 15978.34
[INFO 2017-06-30 11:34:31,012 main.py:52] epoch 11684, training loss: 5004.60, average training loss: 5282.71, base loss: 15978.31
[INFO 2017-06-30 11:34:34,173 main.py:52] epoch 11685, training loss: 5377.91, average training loss: 5282.61, base loss: 15978.42
[INFO 2017-06-30 11:34:37,286 main.py:52] epoch 11686, training loss: 5268.41, average training loss: 5282.85, base loss: 15978.20
[INFO 2017-06-30 11:34:40,396 main.py:52] epoch 11687, training loss: 5179.78, average training loss: 5282.33, base loss: 15978.12
[INFO 2017-06-30 11:34:43,523 main.py:52] epoch 11688, training loss: 4874.44, average training loss: 5282.09, base loss: 15977.80
[INFO 2017-06-30 11:34:46,704 main.py:52] epoch 11689, training loss: 5307.61, average training loss: 5282.26, base loss: 15977.59
[INFO 2017-06-30 11:34:49,905 main.py:52] epoch 11690, training loss: 5160.98, average training loss: 5282.02, base loss: 15977.40
[INFO 2017-06-30 11:34:53,023 main.py:52] epoch 11691, training loss: 5593.42, average training loss: 5282.32, base loss: 15977.65
[INFO 2017-06-30 11:34:56,184 main.py:52] epoch 11692, training loss: 5993.71, average training loss: 5282.94, base loss: 15978.39
[INFO 2017-06-30 11:34:59,328 main.py:52] epoch 11693, training loss: 5474.07, average training loss: 5282.71, base loss: 15978.59
[INFO 2017-06-30 11:35:02,440 main.py:52] epoch 11694, training loss: 5510.89, average training loss: 5282.96, base loss: 15979.00
[INFO 2017-06-30 11:35:05,607 main.py:52] epoch 11695, training loss: 5327.09, average training loss: 5282.62, base loss: 15979.04
[INFO 2017-06-30 11:35:08,743 main.py:52] epoch 11696, training loss: 5256.97, average training loss: 5282.66, base loss: 15979.35
[INFO 2017-06-30 11:35:11,918 main.py:52] epoch 11697, training loss: 5111.80, average training loss: 5282.30, base loss: 15979.27
[INFO 2017-06-30 11:35:15,091 main.py:52] epoch 11698, training loss: 5097.06, average training loss: 5282.27, base loss: 15979.20
[INFO 2017-06-30 11:35:18,253 main.py:52] epoch 11699, training loss: 5210.76, average training loss: 5282.03, base loss: 15979.23
[INFO 2017-06-30 11:35:18,254 main.py:54] epoch 11699, testing
[INFO 2017-06-30 11:35:31,406 main.py:97] average testing loss: 5153.24, base loss: 15609.10
[INFO 2017-06-30 11:35:31,406 main.py:98] improve_loss: 10455.85, improve_percent: 0.67
[INFO 2017-06-30 11:35:31,409 main.py:66] current best improved percent: 0.68
[INFO 2017-06-30 11:35:34,884 main.py:52] epoch 11700, training loss: 4908.25, average training loss: 5281.90, base loss: 15979.13
[INFO 2017-06-30 11:35:38,062 main.py:52] epoch 11701, training loss: 5319.10, average training loss: 5282.29, base loss: 15979.28
[INFO 2017-06-30 11:35:41,182 main.py:52] epoch 11702, training loss: 5420.86, average training loss: 5282.83, base loss: 15979.41
[INFO 2017-06-30 11:35:44,344 main.py:52] epoch 11703, training loss: 5632.43, average training loss: 5283.36, base loss: 15979.84
[INFO 2017-06-30 11:35:47,509 main.py:52] epoch 11704, training loss: 5119.64, average training loss: 5283.50, base loss: 15979.75
[INFO 2017-06-30 11:35:50,655 main.py:52] epoch 11705, training loss: 5370.78, average training loss: 5283.23, base loss: 15979.83
[INFO 2017-06-30 11:35:54,048 main.py:52] epoch 11706, training loss: 5349.76, average training loss: 5283.29, base loss: 15979.67
[INFO 2017-06-30 11:35:57,154 main.py:52] epoch 11707, training loss: 5249.73, average training loss: 5283.55, base loss: 15979.41
[INFO 2017-06-30 11:36:00,354 main.py:52] epoch 11708, training loss: 5447.28, average training loss: 5283.22, base loss: 15979.61
[INFO 2017-06-30 11:36:03,489 main.py:52] epoch 11709, training loss: 5191.03, average training loss: 5282.99, base loss: 15979.59
[INFO 2017-06-30 11:36:06,673 main.py:52] epoch 11710, training loss: 4939.59, average training loss: 5282.61, base loss: 15979.63
[INFO 2017-06-30 11:36:09,791 main.py:52] epoch 11711, training loss: 5096.17, average training loss: 5282.08, base loss: 15979.58
[INFO 2017-06-30 11:36:12,936 main.py:52] epoch 11712, training loss: 5622.78, average training loss: 5282.42, base loss: 15979.79
[INFO 2017-06-30 11:36:16,081 main.py:52] epoch 11713, training loss: 4991.32, average training loss: 5281.66, base loss: 15979.62
[INFO 2017-06-30 11:36:19,252 main.py:52] epoch 11714, training loss: 5072.22, average training loss: 5281.37, base loss: 15979.56
[INFO 2017-06-30 11:36:22,395 main.py:52] epoch 11715, training loss: 5179.47, average training loss: 5280.90, base loss: 15979.46
[INFO 2017-06-30 11:36:25,560 main.py:52] epoch 11716, training loss: 5198.06, average training loss: 5280.73, base loss: 15979.35
[INFO 2017-06-30 11:36:28,713 main.py:52] epoch 11717, training loss: 5302.48, average training loss: 5280.68, base loss: 15979.65
[INFO 2017-06-30 11:36:31,856 main.py:52] epoch 11718, training loss: 5470.87, average training loss: 5281.09, base loss: 15979.40
[INFO 2017-06-30 11:36:34,968 main.py:52] epoch 11719, training loss: 5364.83, average training loss: 5281.30, base loss: 15979.51
[INFO 2017-06-30 11:36:38,078 main.py:52] epoch 11720, training loss: 5665.83, average training loss: 5281.71, base loss: 15979.80
[INFO 2017-06-30 11:36:41,236 main.py:52] epoch 11721, training loss: 5161.65, average training loss: 5281.77, base loss: 15979.78
[INFO 2017-06-30 11:36:44,312 main.py:52] epoch 11722, training loss: 5398.66, average training loss: 5282.36, base loss: 15979.58
[INFO 2017-06-30 11:36:47,497 main.py:52] epoch 11723, training loss: 5413.65, average training loss: 5282.49, base loss: 15979.92
[INFO 2017-06-30 11:36:50,635 main.py:52] epoch 11724, training loss: 4826.55, average training loss: 5281.76, base loss: 15979.85
[INFO 2017-06-30 11:36:53,778 main.py:52] epoch 11725, training loss: 5048.35, average training loss: 5281.63, base loss: 15979.86
[INFO 2017-06-30 11:36:56,911 main.py:52] epoch 11726, training loss: 5038.77, average training loss: 5281.46, base loss: 15979.65
[INFO 2017-06-30 11:37:00,054 main.py:52] epoch 11727, training loss: 5823.75, average training loss: 5281.92, base loss: 15979.76
[INFO 2017-06-30 11:37:03,219 main.py:52] epoch 11728, training loss: 5201.81, average training loss: 5281.87, base loss: 15979.67
[INFO 2017-06-30 11:37:06,335 main.py:52] epoch 11729, training loss: 5465.18, average training loss: 5281.80, base loss: 15979.75
[INFO 2017-06-30 11:37:09,468 main.py:52] epoch 11730, training loss: 5458.39, average training loss: 5282.21, base loss: 15980.04
[INFO 2017-06-30 11:37:12,601 main.py:52] epoch 11731, training loss: 5249.64, average training loss: 5282.28, base loss: 15980.22
[INFO 2017-06-30 11:37:15,773 main.py:52] epoch 11732, training loss: 5120.44, average training loss: 5282.43, base loss: 15980.18
[INFO 2017-06-30 11:37:18,922 main.py:52] epoch 11733, training loss: 5203.72, average training loss: 5282.31, base loss: 15980.11
[INFO 2017-06-30 11:37:22,070 main.py:52] epoch 11734, training loss: 4820.53, average training loss: 5281.74, base loss: 15979.89
[INFO 2017-06-30 11:37:25,223 main.py:52] epoch 11735, training loss: 5251.44, average training loss: 5281.87, base loss: 15980.11
[INFO 2017-06-30 11:37:28,363 main.py:52] epoch 11736, training loss: 5220.95, average training loss: 5281.72, base loss: 15980.11
[INFO 2017-06-30 11:37:31,481 main.py:52] epoch 11737, training loss: 5225.61, average training loss: 5281.83, base loss: 15980.06
[INFO 2017-06-30 11:37:34,623 main.py:52] epoch 11738, training loss: 5150.55, average training loss: 5281.94, base loss: 15979.90
[INFO 2017-06-30 11:37:37,759 main.py:52] epoch 11739, training loss: 5336.17, average training loss: 5281.76, base loss: 15979.87
[INFO 2017-06-30 11:37:40,918 main.py:52] epoch 11740, training loss: 4950.74, average training loss: 5281.03, base loss: 15979.52
[INFO 2017-06-30 11:37:44,020 main.py:52] epoch 11741, training loss: 5587.91, average training loss: 5281.37, base loss: 15979.43
[INFO 2017-06-30 11:37:47,153 main.py:52] epoch 11742, training loss: 5261.37, average training loss: 5281.32, base loss: 15979.40
[INFO 2017-06-30 11:37:50,262 main.py:52] epoch 11743, training loss: 5347.89, average training loss: 5281.33, base loss: 15979.62
[INFO 2017-06-30 11:37:53,431 main.py:52] epoch 11744, training loss: 5164.51, average training loss: 5280.88, base loss: 15979.54
[INFO 2017-06-30 11:37:56,562 main.py:52] epoch 11745, training loss: 4998.50, average training loss: 5280.37, base loss: 15979.20
[INFO 2017-06-30 11:37:59,657 main.py:52] epoch 11746, training loss: 5270.17, average training loss: 5280.46, base loss: 15979.34
[INFO 2017-06-30 11:38:02,803 main.py:52] epoch 11747, training loss: 5351.56, average training loss: 5280.67, base loss: 15979.47
[INFO 2017-06-30 11:38:05,939 main.py:52] epoch 11748, training loss: 5042.61, average training loss: 5280.33, base loss: 15979.50
[INFO 2017-06-30 11:38:09,072 main.py:52] epoch 11749, training loss: 5396.62, average training loss: 5280.26, base loss: 15979.76
[INFO 2017-06-30 11:38:12,250 main.py:52] epoch 11750, training loss: 5150.10, average training loss: 5280.47, base loss: 15979.76
[INFO 2017-06-30 11:38:15,408 main.py:52] epoch 11751, training loss: 5037.66, average training loss: 5279.95, base loss: 15979.94
[INFO 2017-06-30 11:38:18,582 main.py:52] epoch 11752, training loss: 5178.43, average training loss: 5279.91, base loss: 15979.68
[INFO 2017-06-30 11:38:21,704 main.py:52] epoch 11753, training loss: 5488.18, average training loss: 5279.95, base loss: 15980.08
[INFO 2017-06-30 11:38:24,851 main.py:52] epoch 11754, training loss: 5412.01, average training loss: 5280.04, base loss: 15980.19
[INFO 2017-06-30 11:38:27,987 main.py:52] epoch 11755, training loss: 5353.19, average training loss: 5280.26, base loss: 15980.11
[INFO 2017-06-30 11:38:31,095 main.py:52] epoch 11756, training loss: 5065.14, average training loss: 5280.02, base loss: 15979.93
[INFO 2017-06-30 11:38:34,219 main.py:52] epoch 11757, training loss: 4937.79, average training loss: 5279.53, base loss: 15979.75
[INFO 2017-06-30 11:38:37,422 main.py:52] epoch 11758, training loss: 5301.09, average training loss: 5279.56, base loss: 15979.84
[INFO 2017-06-30 11:38:40,586 main.py:52] epoch 11759, training loss: 5342.22, average training loss: 5279.60, base loss: 15979.92
[INFO 2017-06-30 11:38:43,681 main.py:52] epoch 11760, training loss: 5326.64, average training loss: 5279.63, base loss: 15979.80
[INFO 2017-06-30 11:38:46,830 main.py:52] epoch 11761, training loss: 5018.76, average training loss: 5279.25, base loss: 15979.26
[INFO 2017-06-30 11:38:49,965 main.py:52] epoch 11762, training loss: 5511.97, average training loss: 5279.80, base loss: 15979.59
[INFO 2017-06-30 11:38:53,091 main.py:52] epoch 11763, training loss: 5178.48, average training loss: 5280.08, base loss: 15979.67
[INFO 2017-06-30 11:38:56,238 main.py:52] epoch 11764, training loss: 5357.23, average training loss: 5280.10, base loss: 15979.84
[INFO 2017-06-30 11:38:59,401 main.py:52] epoch 11765, training loss: 5382.45, average training loss: 5280.28, base loss: 15980.02
[INFO 2017-06-30 11:39:02,567 main.py:52] epoch 11766, training loss: 5293.24, average training loss: 5280.12, base loss: 15979.95
[INFO 2017-06-30 11:39:05,775 main.py:52] epoch 11767, training loss: 5026.33, average training loss: 5280.22, base loss: 15979.61
[INFO 2017-06-30 11:39:08,902 main.py:52] epoch 11768, training loss: 5252.90, average training loss: 5280.17, base loss: 15979.72
[INFO 2017-06-30 11:39:12,087 main.py:52] epoch 11769, training loss: 5305.60, average training loss: 5280.55, base loss: 15979.58
[INFO 2017-06-30 11:39:15,232 main.py:52] epoch 11770, training loss: 5437.84, average training loss: 5280.37, base loss: 15979.68
[INFO 2017-06-30 11:39:18,368 main.py:52] epoch 11771, training loss: 5419.01, average training loss: 5280.46, base loss: 15980.11
[INFO 2017-06-30 11:39:21,517 main.py:52] epoch 11772, training loss: 5371.59, average training loss: 5280.63, base loss: 15980.25
[INFO 2017-06-30 11:39:24,679 main.py:52] epoch 11773, training loss: 4992.63, average training loss: 5280.56, base loss: 15979.91
[INFO 2017-06-30 11:39:27,783 main.py:52] epoch 11774, training loss: 4964.54, average training loss: 5280.47, base loss: 15979.74
[INFO 2017-06-30 11:39:30,982 main.py:52] epoch 11775, training loss: 5254.95, average training loss: 5280.43, base loss: 15979.86
[INFO 2017-06-30 11:39:34,126 main.py:52] epoch 11776, training loss: 5577.72, average training loss: 5280.48, base loss: 15979.82
[INFO 2017-06-30 11:39:37,304 main.py:52] epoch 11777, training loss: 5691.82, average training loss: 5280.54, base loss: 15980.08
[INFO 2017-06-30 11:39:40,456 main.py:52] epoch 11778, training loss: 5413.82, average training loss: 5280.83, base loss: 15980.28
[INFO 2017-06-30 11:39:43,633 main.py:52] epoch 11779, training loss: 5374.25, average training loss: 5281.00, base loss: 15980.17
[INFO 2017-06-30 11:39:46,753 main.py:52] epoch 11780, training loss: 5305.04, average training loss: 5281.05, base loss: 15980.21
[INFO 2017-06-30 11:39:49,976 main.py:52] epoch 11781, training loss: 5286.27, average training loss: 5281.27, base loss: 15980.14
[INFO 2017-06-30 11:39:53,098 main.py:52] epoch 11782, training loss: 5406.95, average training loss: 5281.40, base loss: 15980.17
[INFO 2017-06-30 11:39:56,225 main.py:52] epoch 11783, training loss: 5464.67, average training loss: 5281.95, base loss: 15980.25
[INFO 2017-06-30 11:39:59,329 main.py:52] epoch 11784, training loss: 5265.37, average training loss: 5282.18, base loss: 15980.44
[INFO 2017-06-30 11:40:02,471 main.py:52] epoch 11785, training loss: 5222.95, average training loss: 5282.15, base loss: 15980.63
[INFO 2017-06-30 11:40:05,634 main.py:52] epoch 11786, training loss: 5425.24, average training loss: 5282.11, base loss: 15980.66
[INFO 2017-06-30 11:40:08,762 main.py:52] epoch 11787, training loss: 5082.64, average training loss: 5282.14, base loss: 15980.62
[INFO 2017-06-30 11:40:11,902 main.py:52] epoch 11788, training loss: 5095.17, average training loss: 5281.78, base loss: 15980.68
[INFO 2017-06-30 11:40:15,066 main.py:52] epoch 11789, training loss: 5131.06, average training loss: 5281.20, base loss: 15980.63
[INFO 2017-06-30 11:40:18,196 main.py:52] epoch 11790, training loss: 5061.68, average training loss: 5280.66, base loss: 15980.64
[INFO 2017-06-30 11:40:21,380 main.py:52] epoch 11791, training loss: 5412.89, average training loss: 5280.64, base loss: 15980.76
[INFO 2017-06-30 11:40:24,492 main.py:52] epoch 11792, training loss: 5241.67, average training loss: 5280.71, base loss: 15980.69
[INFO 2017-06-30 11:40:27,680 main.py:52] epoch 11793, training loss: 4828.35, average training loss: 5280.22, base loss: 15980.44
[INFO 2017-06-30 11:40:30,837 main.py:52] epoch 11794, training loss: 5253.06, average training loss: 5279.92, base loss: 15980.30
[INFO 2017-06-30 11:40:33,992 main.py:52] epoch 11795, training loss: 5052.06, average training loss: 5279.78, base loss: 15980.03
[INFO 2017-06-30 11:40:37,097 main.py:52] epoch 11796, training loss: 5036.51, average training loss: 5279.73, base loss: 15979.86
[INFO 2017-06-30 11:40:40,205 main.py:52] epoch 11797, training loss: 5122.46, average training loss: 5279.12, base loss: 15979.88
[INFO 2017-06-30 11:40:43,321 main.py:52] epoch 11798, training loss: 5156.08, average training loss: 5278.99, base loss: 15979.98
[INFO 2017-06-30 11:40:46,505 main.py:52] epoch 11799, training loss: 4839.20, average training loss: 5278.56, base loss: 15979.86
[INFO 2017-06-30 11:40:46,506 main.py:54] epoch 11799, testing
[INFO 2017-06-30 11:40:59,514 main.py:97] average testing loss: 5405.80, base loss: 16275.09
[INFO 2017-06-30 11:40:59,514 main.py:98] improve_loss: 10869.29, improve_percent: 0.67
[INFO 2017-06-30 11:40:59,515 main.py:66] current best improved percent: 0.68
[INFO 2017-06-30 11:41:02,630 main.py:52] epoch 11800, training loss: 5563.64, average training loss: 5278.78, base loss: 15979.96
[INFO 2017-06-30 11:41:05,790 main.py:52] epoch 11801, training loss: 5390.06, average training loss: 5278.54, base loss: 15980.00
[INFO 2017-06-30 11:41:08,942 main.py:52] epoch 11802, training loss: 5067.49, average training loss: 5278.42, base loss: 15979.58
[INFO 2017-06-30 11:41:12,063 main.py:52] epoch 11803, training loss: 5145.00, average training loss: 5278.32, base loss: 15979.48
[INFO 2017-06-30 11:41:15,186 main.py:52] epoch 11804, training loss: 5283.78, average training loss: 5278.48, base loss: 15979.56
[INFO 2017-06-30 11:41:18,337 main.py:52] epoch 11805, training loss: 5520.23, average training loss: 5278.70, base loss: 15979.75
[INFO 2017-06-30 11:41:21,510 main.py:52] epoch 11806, training loss: 5108.71, average training loss: 5278.40, base loss: 15979.68
[INFO 2017-06-30 11:41:24,652 main.py:52] epoch 11807, training loss: 4954.65, average training loss: 5278.18, base loss: 15979.37
[INFO 2017-06-30 11:41:27,784 main.py:52] epoch 11808, training loss: 5307.77, average training loss: 5278.04, base loss: 15979.55
[INFO 2017-06-30 11:41:30,908 main.py:52] epoch 11809, training loss: 5381.03, average training loss: 5278.37, base loss: 15979.56
[INFO 2017-06-30 11:41:34,112 main.py:52] epoch 11810, training loss: 5127.87, average training loss: 5278.30, base loss: 15979.20
[INFO 2017-06-30 11:41:37,241 main.py:52] epoch 11811, training loss: 5006.30, average training loss: 5277.83, base loss: 15978.92
[INFO 2017-06-30 11:41:40,385 main.py:52] epoch 11812, training loss: 5322.81, average training loss: 5278.24, base loss: 15979.06
[INFO 2017-06-30 11:41:43,530 main.py:52] epoch 11813, training loss: 5554.94, average training loss: 5278.28, base loss: 15979.49
[INFO 2017-06-30 11:41:46,671 main.py:52] epoch 11814, training loss: 5000.95, average training loss: 5277.87, base loss: 15979.45
[INFO 2017-06-30 11:41:49,830 main.py:52] epoch 11815, training loss: 5114.53, average training loss: 5277.47, base loss: 15979.41
[INFO 2017-06-30 11:41:52,985 main.py:52] epoch 11816, training loss: 4926.64, average training loss: 5277.02, base loss: 15979.78
[INFO 2017-06-30 11:41:56,091 main.py:52] epoch 11817, training loss: 5597.36, average training loss: 5277.42, base loss: 15980.29
[INFO 2017-06-30 11:41:59,209 main.py:52] epoch 11818, training loss: 4957.83, average training loss: 5277.09, base loss: 15979.98
[INFO 2017-06-30 11:42:02,367 main.py:52] epoch 11819, training loss: 4914.81, average training loss: 5276.43, base loss: 15979.51
[INFO 2017-06-30 11:42:05,533 main.py:52] epoch 11820, training loss: 5331.66, average training loss: 5276.49, base loss: 15979.66
[INFO 2017-06-30 11:42:08,627 main.py:52] epoch 11821, training loss: 5224.87, average training loss: 5275.92, base loss: 15979.68
[INFO 2017-06-30 11:42:11,782 main.py:52] epoch 11822, training loss: 5194.06, average training loss: 5275.68, base loss: 15979.47
[INFO 2017-06-30 11:42:14,889 main.py:52] epoch 11823, training loss: 5254.67, average training loss: 5275.83, base loss: 15979.56
[INFO 2017-06-30 11:42:18,002 main.py:52] epoch 11824, training loss: 5317.62, average training loss: 5275.64, base loss: 15979.80
[INFO 2017-06-30 11:42:21,160 main.py:52] epoch 11825, training loss: 4996.10, average training loss: 5275.51, base loss: 15979.78
[INFO 2017-06-30 11:42:24,320 main.py:52] epoch 11826, training loss: 5245.78, average training loss: 5275.55, base loss: 15979.85
[INFO 2017-06-30 11:42:27,473 main.py:52] epoch 11827, training loss: 5067.13, average training loss: 5275.46, base loss: 15979.62
[INFO 2017-06-30 11:42:30,607 main.py:52] epoch 11828, training loss: 5390.69, average training loss: 5275.52, base loss: 15979.72
[INFO 2017-06-30 11:42:33,728 main.py:52] epoch 11829, training loss: 4816.60, average training loss: 5274.99, base loss: 15979.43
[INFO 2017-06-30 11:42:36,902 main.py:52] epoch 11830, training loss: 5267.52, average training loss: 5274.89, base loss: 15979.48
[INFO 2017-06-30 11:42:40,080 main.py:52] epoch 11831, training loss: 5168.30, average training loss: 5274.97, base loss: 15979.31
[INFO 2017-06-30 11:42:43,248 main.py:52] epoch 11832, training loss: 5317.74, average training loss: 5275.20, base loss: 15979.52
[INFO 2017-06-30 11:42:46,382 main.py:52] epoch 11833, training loss: 5295.54, average training loss: 5275.30, base loss: 15979.71
[INFO 2017-06-30 11:42:49,610 main.py:52] epoch 11834, training loss: 5425.83, average training loss: 5275.27, base loss: 15979.64
[INFO 2017-06-30 11:42:52,744 main.py:52] epoch 11835, training loss: 5225.18, average training loss: 5275.35, base loss: 15979.66
[INFO 2017-06-30 11:42:55,903 main.py:52] epoch 11836, training loss: 4945.82, average training loss: 5274.84, base loss: 15979.29
[INFO 2017-06-30 11:42:59,015 main.py:52] epoch 11837, training loss: 5172.44, average training loss: 5274.88, base loss: 15979.39
[INFO 2017-06-30 11:43:02,166 main.py:52] epoch 11838, training loss: 5169.49, average training loss: 5274.87, base loss: 15979.16
[INFO 2017-06-30 11:43:05,306 main.py:52] epoch 11839, training loss: 5719.98, average training loss: 5275.25, base loss: 15979.03
[INFO 2017-06-30 11:43:08,433 main.py:52] epoch 11840, training loss: 5089.84, average training loss: 5275.33, base loss: 15979.18
[INFO 2017-06-30 11:43:11,566 main.py:52] epoch 11841, training loss: 5130.07, average training loss: 5275.21, base loss: 15979.33
[INFO 2017-06-30 11:43:14,675 main.py:52] epoch 11842, training loss: 4858.04, average training loss: 5274.83, base loss: 15979.16
[INFO 2017-06-30 11:43:17,782 main.py:52] epoch 11843, training loss: 4841.35, average training loss: 5274.87, base loss: 15979.01
[INFO 2017-06-30 11:43:20,948 main.py:52] epoch 11844, training loss: 5328.93, average training loss: 5274.76, base loss: 15979.22
[INFO 2017-06-30 11:43:24,055 main.py:52] epoch 11845, training loss: 4833.76, average training loss: 5274.08, base loss: 15979.01
[INFO 2017-06-30 11:43:27,209 main.py:52] epoch 11846, training loss: 5544.41, average training loss: 5274.19, base loss: 15979.34
[INFO 2017-06-30 11:43:30,378 main.py:52] epoch 11847, training loss: 5506.00, average training loss: 5274.64, base loss: 15979.26
[INFO 2017-06-30 11:43:33,513 main.py:52] epoch 11848, training loss: 5362.59, average training loss: 5274.62, base loss: 15979.34
[INFO 2017-06-30 11:43:36,639 main.py:52] epoch 11849, training loss: 4829.52, average training loss: 5274.26, base loss: 15979.08
[INFO 2017-06-30 11:43:39,822 main.py:52] epoch 11850, training loss: 5133.84, average training loss: 5273.79, base loss: 15978.98
[INFO 2017-06-30 11:43:42,974 main.py:52] epoch 11851, training loss: 5347.15, average training loss: 5273.84, base loss: 15979.18
[INFO 2017-06-30 11:43:46,135 main.py:52] epoch 11852, training loss: 5157.83, average training loss: 5273.60, base loss: 15979.11
[INFO 2017-06-30 11:43:49,312 main.py:52] epoch 11853, training loss: 5487.19, average training loss: 5274.00, base loss: 15979.19
[INFO 2017-06-30 11:43:52,440 main.py:52] epoch 11854, training loss: 4992.46, average training loss: 5273.84, base loss: 15979.14
[INFO 2017-06-30 11:43:55,602 main.py:52] epoch 11855, training loss: 5129.68, average training loss: 5273.73, base loss: 15978.99
[INFO 2017-06-30 11:43:58,750 main.py:52] epoch 11856, training loss: 5458.88, average training loss: 5274.06, base loss: 15978.95
[INFO 2017-06-30 11:44:01,869 main.py:52] epoch 11857, training loss: 5151.90, average training loss: 5273.82, base loss: 15979.14
[INFO 2017-06-30 11:44:05,002 main.py:52] epoch 11858, training loss: 5179.29, average training loss: 5273.56, base loss: 15978.94
[INFO 2017-06-30 11:44:08,149 main.py:52] epoch 11859, training loss: 5313.26, average training loss: 5273.36, base loss: 15978.75
[INFO 2017-06-30 11:44:11,290 main.py:52] epoch 11860, training loss: 5190.95, average training loss: 5273.14, base loss: 15979.04
[INFO 2017-06-30 11:44:14,490 main.py:52] epoch 11861, training loss: 5404.96, average training loss: 5273.20, base loss: 15979.08
[INFO 2017-06-30 11:44:17,632 main.py:52] epoch 11862, training loss: 5526.45, average training loss: 5273.05, base loss: 15979.34
[INFO 2017-06-30 11:44:20,748 main.py:52] epoch 11863, training loss: 5068.31, average training loss: 5273.01, base loss: 15979.10
[INFO 2017-06-30 11:44:23,897 main.py:52] epoch 11864, training loss: 5127.23, average training loss: 5273.09, base loss: 15978.95
[INFO 2017-06-30 11:44:27,046 main.py:52] epoch 11865, training loss: 5402.54, average training loss: 5272.92, base loss: 15979.25
[INFO 2017-06-30 11:44:30,158 main.py:52] epoch 11866, training loss: 5273.33, average training loss: 5273.12, base loss: 15979.26
[INFO 2017-06-30 11:44:33,276 main.py:52] epoch 11867, training loss: 5603.67, average training loss: 5273.37, base loss: 15979.57
[INFO 2017-06-30 11:44:36,436 main.py:52] epoch 11868, training loss: 5128.36, average training loss: 5272.97, base loss: 15979.50
[INFO 2017-06-30 11:44:39,582 main.py:52] epoch 11869, training loss: 4910.54, average training loss: 5272.42, base loss: 15979.12
[INFO 2017-06-30 11:44:42,697 main.py:52] epoch 11870, training loss: 5340.90, average training loss: 5272.63, base loss: 15979.11
[INFO 2017-06-30 11:44:45,844 main.py:52] epoch 11871, training loss: 5203.61, average training loss: 5272.28, base loss: 15979.09
[INFO 2017-06-30 11:44:48,991 main.py:52] epoch 11872, training loss: 5496.07, average training loss: 5272.68, base loss: 15979.46
[INFO 2017-06-30 11:44:52,141 main.py:52] epoch 11873, training loss: 5607.27, average training loss: 5273.11, base loss: 15979.71
[INFO 2017-06-30 11:44:55,297 main.py:52] epoch 11874, training loss: 5082.98, average training loss: 5272.92, base loss: 15979.85
[INFO 2017-06-30 11:44:58,430 main.py:52] epoch 11875, training loss: 4811.20, average training loss: 5272.59, base loss: 15979.67
[INFO 2017-06-30 11:45:01,569 main.py:52] epoch 11876, training loss: 5038.53, average training loss: 5272.23, base loss: 15979.60
[INFO 2017-06-30 11:45:04,803 main.py:52] epoch 11877, training loss: 5124.68, average training loss: 5272.10, base loss: 15979.61
[INFO 2017-06-30 11:45:07,970 main.py:52] epoch 11878, training loss: 4981.38, average training loss: 5271.62, base loss: 15979.32
[INFO 2017-06-30 11:45:11,113 main.py:52] epoch 11879, training loss: 5379.94, average training loss: 5272.01, base loss: 15979.02
[INFO 2017-06-30 11:45:14,277 main.py:52] epoch 11880, training loss: 5648.70, average training loss: 5271.98, base loss: 15979.35
[INFO 2017-06-30 11:45:17,426 main.py:52] epoch 11881, training loss: 5330.02, average training loss: 5271.82, base loss: 15979.53
[INFO 2017-06-30 11:45:20,581 main.py:52] epoch 11882, training loss: 5157.03, average training loss: 5271.86, base loss: 15979.35
[INFO 2017-06-30 11:45:23,753 main.py:52] epoch 11883, training loss: 5097.80, average training loss: 5271.83, base loss: 15979.22
[INFO 2017-06-30 11:45:26,949 main.py:52] epoch 11884, training loss: 5551.29, average training loss: 5272.20, base loss: 15979.10
[INFO 2017-06-30 11:45:30,094 main.py:52] epoch 11885, training loss: 5229.12, average training loss: 5272.15, base loss: 15979.08
[INFO 2017-06-30 11:45:33,219 main.py:52] epoch 11886, training loss: 4830.99, average training loss: 5271.68, base loss: 15979.02
[INFO 2017-06-30 11:45:36,425 main.py:52] epoch 11887, training loss: 5416.21, average training loss: 5272.22, base loss: 15979.18
[INFO 2017-06-30 11:45:39,584 main.py:52] epoch 11888, training loss: 5202.21, average training loss: 5271.96, base loss: 15979.05
[INFO 2017-06-30 11:45:42,710 main.py:52] epoch 11889, training loss: 5290.16, average training loss: 5272.08, base loss: 15979.07
[INFO 2017-06-30 11:45:45,872 main.py:52] epoch 11890, training loss: 5464.44, average training loss: 5272.31, base loss: 15979.07
[INFO 2017-06-30 11:45:49,066 main.py:52] epoch 11891, training loss: 5383.02, average training loss: 5272.55, base loss: 15979.06
[INFO 2017-06-30 11:45:52,171 main.py:52] epoch 11892, training loss: 5350.99, average training loss: 5272.57, base loss: 15978.92
[INFO 2017-06-30 11:45:55,302 main.py:52] epoch 11893, training loss: 5255.22, average training loss: 5272.59, base loss: 15978.64
[INFO 2017-06-30 11:45:58,481 main.py:52] epoch 11894, training loss: 5292.24, average training loss: 5272.64, base loss: 15978.68
[INFO 2017-06-30 11:46:01,637 main.py:52] epoch 11895, training loss: 5646.15, average training loss: 5273.08, base loss: 15978.98
[INFO 2017-06-30 11:46:04,790 main.py:52] epoch 11896, training loss: 5295.34, average training loss: 5273.12, base loss: 15978.93
[INFO 2017-06-30 11:46:07,986 main.py:52] epoch 11897, training loss: 5332.24, average training loss: 5273.07, base loss: 15978.89
[INFO 2017-06-30 11:46:11,151 main.py:52] epoch 11898, training loss: 5152.76, average training loss: 5272.78, base loss: 15978.92
[INFO 2017-06-30 11:46:14,311 main.py:52] epoch 11899, training loss: 5050.65, average training loss: 5272.14, base loss: 15979.19
[INFO 2017-06-30 11:46:14,312 main.py:54] epoch 11899, testing
[INFO 2017-06-30 11:46:27,420 main.py:97] average testing loss: 5190.49, base loss: 15853.25
[INFO 2017-06-30 11:46:27,420 main.py:98] improve_loss: 10662.76, improve_percent: 0.67
[INFO 2017-06-30 11:46:27,423 main.py:66] current best improved percent: 0.68
[INFO 2017-06-30 11:46:30,583 main.py:52] epoch 11900, training loss: 4833.80, average training loss: 5271.74, base loss: 15979.17
[INFO 2017-06-30 11:46:33,753 main.py:52] epoch 11901, training loss: 5520.11, average training loss: 5271.80, base loss: 15979.52
[INFO 2017-06-30 11:46:36,962 main.py:52] epoch 11902, training loss: 5032.67, average training loss: 5271.12, base loss: 15979.44
[INFO 2017-06-30 11:46:40,101 main.py:52] epoch 11903, training loss: 5636.98, average training loss: 5271.63, base loss: 15979.84
[INFO 2017-06-30 11:46:43,253 main.py:52] epoch 11904, training loss: 5162.74, average training loss: 5271.71, base loss: 15979.84
[INFO 2017-06-30 11:46:46,390 main.py:52] epoch 11905, training loss: 5084.52, average training loss: 5271.68, base loss: 15979.79
[INFO 2017-06-30 11:46:49,503 main.py:52] epoch 11906, training loss: 5298.65, average training loss: 5271.68, base loss: 15979.74
[INFO 2017-06-30 11:46:52,647 main.py:52] epoch 11907, training loss: 4982.14, average training loss: 5271.32, base loss: 15979.85
[INFO 2017-06-30 11:46:55,752 main.py:52] epoch 11908, training loss: 4882.75, average training loss: 5270.65, base loss: 15979.72
[INFO 2017-06-30 11:46:58,886 main.py:52] epoch 11909, training loss: 4880.76, average training loss: 5269.83, base loss: 15979.43
[INFO 2017-06-30 11:47:02,036 main.py:52] epoch 11910, training loss: 5047.18, average training loss: 5269.45, base loss: 15979.59
[INFO 2017-06-30 11:47:05,178 main.py:52] epoch 11911, training loss: 5254.41, average training loss: 5269.36, base loss: 15979.56
[INFO 2017-06-30 11:47:08,300 main.py:52] epoch 11912, training loss: 5183.90, average training loss: 5269.29, base loss: 15979.65
[INFO 2017-06-30 11:47:11,409 main.py:52] epoch 11913, training loss: 4947.82, average training loss: 5268.96, base loss: 15979.51
[INFO 2017-06-30 11:47:14,549 main.py:52] epoch 11914, training loss: 5093.98, average training loss: 5268.96, base loss: 15979.39
[INFO 2017-06-30 11:47:17,690 main.py:52] epoch 11915, training loss: 5841.14, average training loss: 5269.32, base loss: 15979.83
[INFO 2017-06-30 11:47:20,882 main.py:52] epoch 11916, training loss: 5099.80, average training loss: 5269.09, base loss: 15979.47
[INFO 2017-06-30 11:47:24,013 main.py:52] epoch 11917, training loss: 5418.42, average training loss: 5269.26, base loss: 15979.47
[INFO 2017-06-30 11:47:27,138 main.py:52] epoch 11918, training loss: 4767.80, average training loss: 5269.05, base loss: 15979.25
[INFO 2017-06-30 11:47:30,284 main.py:52] epoch 11919, training loss: 5488.53, average training loss: 5268.94, base loss: 15979.50
[INFO 2017-06-30 11:47:33,438 main.py:52] epoch 11920, training loss: 5363.81, average training loss: 5269.05, base loss: 15979.59
[INFO 2017-06-30 11:47:36,581 main.py:52] epoch 11921, training loss: 5102.39, average training loss: 5268.90, base loss: 15979.49
[INFO 2017-06-30 11:47:39,713 main.py:52] epoch 11922, training loss: 5310.98, average training loss: 5269.13, base loss: 15979.67
[INFO 2017-06-30 11:47:42,883 main.py:52] epoch 11923, training loss: 5508.27, average training loss: 5269.40, base loss: 15979.80
[INFO 2017-06-30 11:47:46,106 main.py:52] epoch 11924, training loss: 5330.01, average training loss: 5269.32, base loss: 15980.11
[INFO 2017-06-30 11:47:49,249 main.py:52] epoch 11925, training loss: 5241.97, average training loss: 5268.97, base loss: 15980.36
[INFO 2017-06-30 11:47:52,380 main.py:52] epoch 11926, training loss: 5244.41, average training loss: 5268.94, base loss: 15980.65
[INFO 2017-06-30 11:47:55,532 main.py:52] epoch 11927, training loss: 5340.80, average training loss: 5268.67, base loss: 15981.12
[INFO 2017-06-30 11:47:58,672 main.py:52] epoch 11928, training loss: 5087.54, average training loss: 5267.70, base loss: 15981.08
[INFO 2017-06-30 11:48:01,808 main.py:52] epoch 11929, training loss: 5132.69, average training loss: 5267.62, base loss: 15981.23
[INFO 2017-06-30 11:48:04,917 main.py:52] epoch 11930, training loss: 5317.25, average training loss: 5267.53, base loss: 15981.22
[INFO 2017-06-30 11:48:08,078 main.py:52] epoch 11931, training loss: 5443.40, average training loss: 5267.85, base loss: 15981.37
[INFO 2017-06-30 11:48:11,187 main.py:52] epoch 11932, training loss: 5389.50, average training loss: 5267.81, base loss: 15981.39
[INFO 2017-06-30 11:48:14,319 main.py:52] epoch 11933, training loss: 5437.66, average training loss: 5267.92, base loss: 15981.38
[INFO 2017-06-30 11:48:17,502 main.py:52] epoch 11934, training loss: 5236.15, average training loss: 5267.85, base loss: 15981.46
[INFO 2017-06-30 11:48:20,677 main.py:52] epoch 11935, training loss: 5448.92, average training loss: 5267.95, base loss: 15981.75
[INFO 2017-06-30 11:48:23,879 main.py:52] epoch 11936, training loss: 5239.57, average training loss: 5267.69, base loss: 15981.62
[INFO 2017-06-30 11:48:27,038 main.py:52] epoch 11937, training loss: 5490.48, average training loss: 5267.75, base loss: 15981.57
[INFO 2017-06-30 11:48:30,207 main.py:52] epoch 11938, training loss: 5643.06, average training loss: 5268.18, base loss: 15981.75
[INFO 2017-06-30 11:48:33,389 main.py:52] epoch 11939, training loss: 5041.85, average training loss: 5267.97, base loss: 15981.83
[INFO 2017-06-30 11:48:36,494 main.py:52] epoch 11940, training loss: 5324.31, average training loss: 5268.16, base loss: 15981.79
[INFO 2017-06-30 11:48:39,622 main.py:52] epoch 11941, training loss: 5209.42, average training loss: 5268.10, base loss: 15981.80
[INFO 2017-06-30 11:48:42,762 main.py:52] epoch 11942, training loss: 4907.75, average training loss: 5267.44, base loss: 15981.60
[INFO 2017-06-30 11:48:45,882 main.py:52] epoch 11943, training loss: 5087.69, average training loss: 5267.11, base loss: 15981.74
[INFO 2017-06-30 11:48:49,061 main.py:52] epoch 11944, training loss: 4961.86, average training loss: 5266.73, base loss: 15981.51
[INFO 2017-06-30 11:48:52,165 main.py:52] epoch 11945, training loss: 5303.12, average training loss: 5266.93, base loss: 15981.59
[INFO 2017-06-30 11:48:55,335 main.py:52] epoch 11946, training loss: 5378.54, average training loss: 5266.85, base loss: 15981.55
[INFO 2017-06-30 11:48:58,535 main.py:52] epoch 11947, training loss: 5120.02, average training loss: 5266.52, base loss: 15981.59
[INFO 2017-06-30 11:49:01,638 main.py:52] epoch 11948, training loss: 5223.13, average training loss: 5266.47, base loss: 15981.72
[INFO 2017-06-30 11:49:04,743 main.py:52] epoch 11949, training loss: 5190.83, average training loss: 5266.29, base loss: 15981.66
[INFO 2017-06-30 11:49:07,913 main.py:52] epoch 11950, training loss: 5112.17, average training loss: 5265.93, base loss: 15981.40
[INFO 2017-06-30 11:49:11,069 main.py:52] epoch 11951, training loss: 5406.94, average training loss: 5266.32, base loss: 15981.63
[INFO 2017-06-30 11:49:14,198 main.py:52] epoch 11952, training loss: 5188.99, average training loss: 5266.33, base loss: 15981.34
[INFO 2017-06-30 11:49:17,362 main.py:52] epoch 11953, training loss: 5088.50, average training loss: 5266.06, base loss: 15981.23
[INFO 2017-06-30 11:49:20,514 main.py:52] epoch 11954, training loss: 5126.28, average training loss: 5265.84, base loss: 15980.92
[INFO 2017-06-30 11:49:23,699 main.py:52] epoch 11955, training loss: 4962.99, average training loss: 5265.32, base loss: 15980.57
[INFO 2017-06-30 11:49:26,840 main.py:52] epoch 11956, training loss: 5439.01, average training loss: 5265.35, base loss: 15980.55
[INFO 2017-06-30 11:49:30,026 main.py:52] epoch 11957, training loss: 5355.13, average training loss: 5265.43, base loss: 15980.61
[INFO 2017-06-30 11:49:33,154 main.py:52] epoch 11958, training loss: 5278.59, average training loss: 5265.50, base loss: 15980.68
[INFO 2017-06-30 11:49:36,319 main.py:52] epoch 11959, training loss: 4886.36, average training loss: 5265.21, base loss: 15980.51
[INFO 2017-06-30 11:49:39,443 main.py:52] epoch 11960, training loss: 4910.01, average training loss: 5264.79, base loss: 15980.37
[INFO 2017-06-30 11:49:42,569 main.py:52] epoch 11961, training loss: 5195.42, average training loss: 5264.99, base loss: 15980.62
[INFO 2017-06-30 11:49:45,734 main.py:52] epoch 11962, training loss: 5613.80, average training loss: 5265.02, base loss: 15981.11
[INFO 2017-06-30 11:49:48,937 main.py:52] epoch 11963, training loss: 5449.06, average training loss: 5265.19, base loss: 15981.23
[INFO 2017-06-30 11:49:52,059 main.py:52] epoch 11964, training loss: 5230.02, average training loss: 5265.37, base loss: 15981.32
[INFO 2017-06-30 11:49:55,254 main.py:52] epoch 11965, training loss: 4908.96, average training loss: 5265.18, base loss: 15981.04
[INFO 2017-06-30 11:49:58,405 main.py:52] epoch 11966, training loss: 5082.90, average training loss: 5264.93, base loss: 15980.88
[INFO 2017-06-30 11:50:01,532 main.py:52] epoch 11967, training loss: 5060.05, average training loss: 5264.30, base loss: 15980.88
[INFO 2017-06-30 11:50:04,694 main.py:52] epoch 11968, training loss: 5476.99, average training loss: 5264.27, base loss: 15981.05
[INFO 2017-06-30 11:50:07,853 main.py:52] epoch 11969, training loss: 5304.38, average training loss: 5263.88, base loss: 15981.09
[INFO 2017-06-30 11:50:10,994 main.py:52] epoch 11970, training loss: 5123.49, average training loss: 5263.88, base loss: 15981.15
[INFO 2017-06-30 11:50:14,094 main.py:52] epoch 11971, training loss: 5266.39, average training loss: 5263.98, base loss: 15981.29
[INFO 2017-06-30 11:50:17,213 main.py:52] epoch 11972, training loss: 5452.44, average training loss: 5264.08, base loss: 15981.52
[INFO 2017-06-30 11:50:20,352 main.py:52] epoch 11973, training loss: 5196.04, average training loss: 5263.87, base loss: 15981.40
[INFO 2017-06-30 11:50:23,505 main.py:52] epoch 11974, training loss: 4897.75, average training loss: 5263.46, base loss: 15981.29
[INFO 2017-06-30 11:50:26,666 main.py:52] epoch 11975, training loss: 5142.95, average training loss: 5263.26, base loss: 15981.12
[INFO 2017-06-30 11:50:29,770 main.py:52] epoch 11976, training loss: 5327.06, average training loss: 5263.17, base loss: 15981.00
[INFO 2017-06-30 11:50:32,900 main.py:52] epoch 11977, training loss: 5271.14, average training loss: 5263.21, base loss: 15980.91
[INFO 2017-06-30 11:50:36,079 main.py:52] epoch 11978, training loss: 5015.40, average training loss: 5262.64, base loss: 15980.68
[INFO 2017-06-30 11:50:39,199 main.py:52] epoch 11979, training loss: 5442.91, average training loss: 5262.85, base loss: 15980.98
[INFO 2017-06-30 11:50:42,335 main.py:52] epoch 11980, training loss: 5080.31, average training loss: 5262.66, base loss: 15981.03
[INFO 2017-06-30 11:50:45,481 main.py:52] epoch 11981, training loss: 5187.34, average training loss: 5262.26, base loss: 15981.04
[INFO 2017-06-30 11:50:48,628 main.py:52] epoch 11982, training loss: 5465.30, average training loss: 5262.57, base loss: 15981.26
[INFO 2017-06-30 11:50:51,785 main.py:52] epoch 11983, training loss: 5397.96, average training loss: 5262.72, base loss: 15981.27
[INFO 2017-06-30 11:50:54,929 main.py:52] epoch 11984, training loss: 5069.32, average training loss: 5262.61, base loss: 15981.35
[INFO 2017-06-30 11:50:58,038 main.py:52] epoch 11985, training loss: 5038.18, average training loss: 5262.16, base loss: 15981.17
[INFO 2017-06-30 11:51:01,175 main.py:52] epoch 11986, training loss: 5141.59, average training loss: 5261.79, base loss: 15981.28
[INFO 2017-06-30 11:51:04,306 main.py:52] epoch 11987, training loss: 5172.62, average training loss: 5261.72, base loss: 15981.40
[INFO 2017-06-30 11:51:07,415 main.py:52] epoch 11988, training loss: 5027.13, average training loss: 5261.51, base loss: 15981.28
[INFO 2017-06-30 11:51:10,513 main.py:52] epoch 11989, training loss: 5164.79, average training loss: 5261.10, base loss: 15981.32
[INFO 2017-06-30 11:51:13,666 main.py:52] epoch 11990, training loss: 5028.45, average training loss: 5260.72, base loss: 15981.39
[INFO 2017-06-30 11:51:16,807 main.py:52] epoch 11991, training loss: 5064.85, average training loss: 5260.66, base loss: 15981.26
[INFO 2017-06-30 11:51:19,973 main.py:52] epoch 11992, training loss: 5240.02, average training loss: 5260.48, base loss: 15981.23
[INFO 2017-06-30 11:51:23,167 main.py:52] epoch 11993, training loss: 5136.46, average training loss: 5260.47, base loss: 15981.21
[INFO 2017-06-30 11:51:26,333 main.py:52] epoch 11994, training loss: 4803.80, average training loss: 5260.13, base loss: 15980.97
[INFO 2017-06-30 11:51:29,516 main.py:52] epoch 11995, training loss: 5210.87, average training loss: 5260.43, base loss: 15980.94
[INFO 2017-06-30 11:51:32,674 main.py:52] epoch 11996, training loss: 5738.16, average training loss: 5260.82, base loss: 15981.44
[INFO 2017-06-30 11:51:35,846 main.py:52] epoch 11997, training loss: 4939.60, average training loss: 5260.50, base loss: 15981.48
[INFO 2017-06-30 11:51:38,975 main.py:52] epoch 11998, training loss: 4928.00, average training loss: 5260.03, base loss: 15981.16
[INFO 2017-06-30 11:51:42,137 main.py:52] epoch 11999, training loss: 5040.88, average training loss: 5259.61, base loss: 15980.92
[INFO 2017-06-30 11:51:42,138 main.py:54] epoch 11999, testing
[INFO 2017-06-30 11:51:55,144 main.py:97] average testing loss: 5330.55, base loss: 16129.93
[INFO 2017-06-30 11:51:55,144 main.py:98] improve_loss: 10799.39, improve_percent: 0.67
[INFO 2017-06-30 11:51:55,146 main.py:66] current best improved percent: 0.68
[INFO 2017-06-30 11:51:58,280 main.py:52] epoch 12000, training loss: 5512.57, average training loss: 5260.05, base loss: 15980.97
[INFO 2017-06-30 11:52:01,477 main.py:52] epoch 12001, training loss: 5319.56, average training loss: 5260.37, base loss: 15980.85
[INFO 2017-06-30 11:52:04,654 main.py:52] epoch 12002, training loss: 5636.22, average training loss: 5260.91, base loss: 15981.27
[INFO 2017-06-30 11:52:07,825 main.py:52] epoch 12003, training loss: 5038.50, average training loss: 5260.51, base loss: 15981.11
[INFO 2017-06-30 11:52:11,022 main.py:52] epoch 12004, training loss: 5065.21, average training loss: 5260.30, base loss: 15980.89
[INFO 2017-06-30 11:52:14,161 main.py:52] epoch 12005, training loss: 5035.96, average training loss: 5260.06, base loss: 15980.63
[INFO 2017-06-30 11:52:17,305 main.py:52] epoch 12006, training loss: 5120.36, average training loss: 5259.66, base loss: 15980.55
[INFO 2017-06-30 11:52:20,667 main.py:52] epoch 12007, training loss: 5420.41, average training loss: 5259.95, base loss: 15980.58
[INFO 2017-06-30 11:52:23,811 main.py:52] epoch 12008, training loss: 5628.59, average training loss: 5260.48, base loss: 15980.70
[INFO 2017-06-30 11:52:26,985 main.py:52] epoch 12009, training loss: 5053.80, average training loss: 5260.28, base loss: 15980.28
[INFO 2017-06-30 11:52:30,155 main.py:52] epoch 12010, training loss: 4907.17, average training loss: 5259.87, base loss: 15980.15
[INFO 2017-06-30 11:52:33,300 main.py:52] epoch 12011, training loss: 5817.15, average training loss: 5260.66, base loss: 15980.49
[INFO 2017-06-30 11:52:36,460 main.py:52] epoch 12012, training loss: 4944.67, average training loss: 5260.57, base loss: 15980.31
[INFO 2017-06-30 11:52:39,604 main.py:52] epoch 12013, training loss: 5042.96, average training loss: 5260.00, base loss: 15980.36
[INFO 2017-06-30 11:52:42,759 main.py:52] epoch 12014, training loss: 5196.12, average training loss: 5259.90, base loss: 15980.60
[INFO 2017-06-30 11:52:45,933 main.py:52] epoch 12015, training loss: 5136.68, average training loss: 5260.29, base loss: 15980.68
[INFO 2017-06-30 11:52:49,049 main.py:52] epoch 12016, training loss: 5267.01, average training loss: 5260.44, base loss: 15980.49
[INFO 2017-06-30 11:52:52,166 main.py:52] epoch 12017, training loss: 5239.47, average training loss: 5260.55, base loss: 15980.54
[INFO 2017-06-30 11:52:55,265 main.py:52] epoch 12018, training loss: 5317.06, average training loss: 5260.57, base loss: 15980.30
[INFO 2017-06-30 11:52:58,418 main.py:52] epoch 12019, training loss: 5611.33, average training loss: 5261.20, base loss: 15980.34
[INFO 2017-06-30 11:53:01,590 main.py:52] epoch 12020, training loss: 5296.30, average training loss: 5261.03, base loss: 15980.39
[INFO 2017-06-30 11:53:04,751 main.py:52] epoch 12021, training loss: 5252.93, average training loss: 5260.80, base loss: 15980.50
[INFO 2017-06-30 11:53:07,904 main.py:52] epoch 12022, training loss: 5410.71, average training loss: 5261.08, base loss: 15980.60
[INFO 2017-06-30 11:53:11,046 main.py:52] epoch 12023, training loss: 5294.12, average training loss: 5260.99, base loss: 15980.68
[INFO 2017-06-30 11:53:14,159 main.py:52] epoch 12024, training loss: 4952.02, average training loss: 5260.69, base loss: 15980.75
[INFO 2017-06-30 11:53:17,371 main.py:52] epoch 12025, training loss: 5031.63, average training loss: 5260.20, base loss: 15980.72
[INFO 2017-06-30 11:53:20,505 main.py:52] epoch 12026, training loss: 5138.89, average training loss: 5260.22, base loss: 15980.72
[INFO 2017-06-30 11:53:23,644 main.py:52] epoch 12027, training loss: 4966.55, average training loss: 5259.85, base loss: 15980.78
[INFO 2017-06-30 11:53:26,824 main.py:52] epoch 12028, training loss: 5528.30, average training loss: 5260.28, base loss: 15980.90
[INFO 2017-06-30 11:53:30,014 main.py:52] epoch 12029, training loss: 5305.26, average training loss: 5260.45, base loss: 15981.11
[INFO 2017-06-30 11:53:33,165 main.py:52] epoch 12030, training loss: 5092.81, average training loss: 5260.53, base loss: 15981.00
[INFO 2017-06-30 11:53:36,317 main.py:52] epoch 12031, training loss: 5197.81, average training loss: 5260.18, base loss: 15981.06
[INFO 2017-06-30 11:53:39,452 main.py:52] epoch 12032, training loss: 4973.10, average training loss: 5259.83, base loss: 15981.27
[INFO 2017-06-30 11:53:42,562 main.py:52] epoch 12033, training loss: 4949.58, average training loss: 5259.50, base loss: 15981.26
[INFO 2017-06-30 11:53:45,711 main.py:52] epoch 12034, training loss: 5420.12, average training loss: 5259.53, base loss: 15981.19
[INFO 2017-06-30 11:53:48,888 main.py:52] epoch 12035, training loss: 5407.90, average training loss: 5259.42, base loss: 15980.91
[INFO 2017-06-30 11:53:52,097 main.py:52] epoch 12036, training loss: 5389.40, average training loss: 5259.49, base loss: 15980.88
[INFO 2017-06-30 11:53:55,249 main.py:52] epoch 12037, training loss: 4688.89, average training loss: 5259.03, base loss: 15980.66
[INFO 2017-06-30 11:53:58,438 main.py:52] epoch 12038, training loss: 5178.63, average training loss: 5259.17, base loss: 15980.84
[INFO 2017-06-30 11:54:01,571 main.py:52] epoch 12039, training loss: 5156.45, average training loss: 5259.07, base loss: 15980.66
[INFO 2017-06-30 11:54:04,733 main.py:52] epoch 12040, training loss: 5405.30, average training loss: 5259.32, base loss: 15980.90
[INFO 2017-06-30 11:54:07,863 main.py:52] epoch 12041, training loss: 5340.71, average training loss: 5258.97, base loss: 15980.80
[INFO 2017-06-30 11:54:11,016 main.py:52] epoch 12042, training loss: 5057.84, average training loss: 5258.57, base loss: 15980.77
[INFO 2017-06-30 11:54:14,151 main.py:52] epoch 12043, training loss: 5533.13, average training loss: 5258.50, base loss: 15981.05
[INFO 2017-06-30 11:54:17,301 main.py:52] epoch 12044, training loss: 5243.71, average training loss: 5258.72, base loss: 15981.09
[INFO 2017-06-30 11:54:20,480 main.py:52] epoch 12045, training loss: 5343.87, average training loss: 5258.93, base loss: 15981.22
[INFO 2017-06-30 11:54:23,636 main.py:52] epoch 12046, training loss: 5343.10, average training loss: 5259.02, base loss: 15981.46
[INFO 2017-06-30 11:54:26,793 main.py:52] epoch 12047, training loss: 4995.21, average training loss: 5258.82, base loss: 15981.48
[INFO 2017-06-30 11:54:29,932 main.py:52] epoch 12048, training loss: 5375.74, average training loss: 5258.96, base loss: 15981.62
[INFO 2017-06-30 11:54:33,047 main.py:52] epoch 12049, training loss: 5183.89, average training loss: 5258.89, base loss: 15981.57
[INFO 2017-06-30 11:54:36,184 main.py:52] epoch 12050, training loss: 5476.44, average training loss: 5259.03, base loss: 15981.44
[INFO 2017-06-30 11:54:39,327 main.py:52] epoch 12051, training loss: 5173.91, average training loss: 5258.83, base loss: 15981.38
[INFO 2017-06-30 11:54:42,473 main.py:52] epoch 12052, training loss: 5164.83, average training loss: 5258.65, base loss: 15981.13
[INFO 2017-06-30 11:54:45,608 main.py:52] epoch 12053, training loss: 5367.64, average training loss: 5258.80, base loss: 15980.70
[INFO 2017-06-30 11:54:48,740 main.py:52] epoch 12054, training loss: 5633.81, average training loss: 5259.09, base loss: 15981.00
[INFO 2017-06-30 11:54:51,872 main.py:52] epoch 12055, training loss: 5363.03, average training loss: 5259.20, base loss: 15981.00
[INFO 2017-06-30 11:54:55,002 main.py:52] epoch 12056, training loss: 5214.26, average training loss: 5259.16, base loss: 15980.96
[INFO 2017-06-30 11:54:58,174 main.py:52] epoch 12057, training loss: 4907.79, average training loss: 5258.60, base loss: 15980.73
[INFO 2017-06-30 11:55:01,359 main.py:52] epoch 12058, training loss: 5664.93, average training loss: 5258.81, base loss: 15981.21
[INFO 2017-06-30 11:55:04,535 main.py:52] epoch 12059, training loss: 5030.34, average training loss: 5258.45, base loss: 15980.96
[INFO 2017-06-30 11:55:07,686 main.py:52] epoch 12060, training loss: 5192.27, average training loss: 5258.36, base loss: 15981.13
[INFO 2017-06-30 11:55:10,881 main.py:52] epoch 12061, training loss: 5231.58, average training loss: 5258.47, base loss: 15981.28
[INFO 2017-06-30 11:55:14,030 main.py:52] epoch 12062, training loss: 5397.46, average training loss: 5258.50, base loss: 15981.67
[INFO 2017-06-30 11:55:17,150 main.py:52] epoch 12063, training loss: 5230.86, average training loss: 5258.21, base loss: 15981.71
[INFO 2017-06-30 11:55:20,295 main.py:52] epoch 12064, training loss: 5997.59, average training loss: 5258.65, base loss: 15982.06
[INFO 2017-06-30 11:55:23,447 main.py:52] epoch 12065, training loss: 4999.90, average training loss: 5258.07, base loss: 15981.87
[INFO 2017-06-30 11:55:26,595 main.py:52] epoch 12066, training loss: 4878.65, average training loss: 5257.56, base loss: 15981.57
[INFO 2017-06-30 11:55:29,728 main.py:52] epoch 12067, training loss: 5492.82, average training loss: 5258.29, base loss: 15981.66
[INFO 2017-06-30 11:55:32,887 main.py:52] epoch 12068, training loss: 5532.70, average training loss: 5258.09, base loss: 15981.90
[INFO 2017-06-30 11:55:36,047 main.py:52] epoch 12069, training loss: 5127.72, average training loss: 5257.81, base loss: 15981.84
[INFO 2017-06-30 11:55:39,177 main.py:52] epoch 12070, training loss: 5276.03, average training loss: 5258.06, base loss: 15981.82
[INFO 2017-06-30 11:55:42,279 main.py:52] epoch 12071, training loss: 5525.04, average training loss: 5258.11, base loss: 15982.07
[INFO 2017-06-30 11:55:45,411 main.py:52] epoch 12072, training loss: 5083.66, average training loss: 5257.44, base loss: 15982.39
[INFO 2017-06-30 11:55:48,583 main.py:52] epoch 12073, training loss: 5110.20, average training loss: 5257.33, base loss: 15982.24
[INFO 2017-06-30 11:55:51,738 main.py:52] epoch 12074, training loss: 5408.83, average training loss: 5257.59, base loss: 15982.46
[INFO 2017-06-30 11:55:54,897 main.py:52] epoch 12075, training loss: 5572.31, average training loss: 5257.68, base loss: 15982.44
[INFO 2017-06-30 11:55:58,061 main.py:52] epoch 12076, training loss: 4893.79, average training loss: 5257.01, base loss: 15982.12
[INFO 2017-06-30 11:56:01,198 main.py:52] epoch 12077, training loss: 5207.08, average training loss: 5256.99, base loss: 15981.79
[INFO 2017-06-30 11:56:04,316 main.py:52] epoch 12078, training loss: 5142.76, average training loss: 5256.92, base loss: 15981.48
[INFO 2017-06-30 11:56:07,485 main.py:52] epoch 12079, training loss: 5188.94, average training loss: 5256.76, base loss: 15981.49
[INFO 2017-06-30 11:56:10,620 main.py:52] epoch 12080, training loss: 5384.92, average training loss: 5256.69, base loss: 15981.69
[INFO 2017-06-30 11:56:13,799 main.py:52] epoch 12081, training loss: 5141.92, average training loss: 5256.57, base loss: 15981.82
[INFO 2017-06-30 11:56:16,964 main.py:52] epoch 12082, training loss: 4866.63, average training loss: 5256.31, base loss: 15981.80
[INFO 2017-06-30 11:56:20,139 main.py:52] epoch 12083, training loss: 5551.14, average training loss: 5255.87, base loss: 15982.09
[INFO 2017-06-30 11:56:23,263 main.py:52] epoch 12084, training loss: 5081.77, average training loss: 5255.33, base loss: 15981.84
[INFO 2017-06-30 11:56:26,470 main.py:52] epoch 12085, training loss: 5339.86, average training loss: 5255.51, base loss: 15981.57
[INFO 2017-06-30 11:56:29,584 main.py:52] epoch 12086, training loss: 5738.41, average training loss: 5255.91, base loss: 15981.79
[INFO 2017-06-30 11:56:32,741 main.py:52] epoch 12087, training loss: 5379.20, average training loss: 5255.75, base loss: 15981.86
[INFO 2017-06-30 11:56:35,904 main.py:52] epoch 12088, training loss: 5565.26, average training loss: 5255.94, base loss: 15982.23
[INFO 2017-06-30 11:56:39,041 main.py:52] epoch 12089, training loss: 5200.76, average training loss: 5255.45, base loss: 15982.14
[INFO 2017-06-30 11:56:42,218 main.py:52] epoch 12090, training loss: 5241.33, average training loss: 5255.51, base loss: 15982.02
[INFO 2017-06-30 11:56:45,319 main.py:52] epoch 12091, training loss: 5251.10, average training loss: 5255.45, base loss: 15981.82
[INFO 2017-06-30 11:56:48,471 main.py:52] epoch 12092, training loss: 5219.47, average training loss: 5255.44, base loss: 15981.85
[INFO 2017-06-30 11:56:51,580 main.py:52] epoch 12093, training loss: 5422.15, average training loss: 5255.43, base loss: 15982.10
[INFO 2017-06-30 11:56:54,723 main.py:52] epoch 12094, training loss: 5278.02, average training loss: 5255.80, base loss: 15981.81
[INFO 2017-06-30 11:56:57,874 main.py:52] epoch 12095, training loss: 5429.16, average training loss: 5256.16, base loss: 15981.76
[INFO 2017-06-30 11:57:01,003 main.py:52] epoch 12096, training loss: 5243.43, average training loss: 5256.27, base loss: 15981.85
[INFO 2017-06-30 11:57:04,128 main.py:52] epoch 12097, training loss: 5108.49, average training loss: 5256.54, base loss: 15981.69
[INFO 2017-06-30 11:57:07,247 main.py:52] epoch 12098, training loss: 5550.01, average training loss: 5256.82, base loss: 15982.01
[INFO 2017-06-30 11:57:10,429 main.py:52] epoch 12099, training loss: 5206.58, average training loss: 5256.91, base loss: 15982.17
[INFO 2017-06-30 11:57:10,430 main.py:54] epoch 12099, testing
[INFO 2017-06-30 11:57:23,462 main.py:97] average testing loss: 5244.33, base loss: 16727.50
[INFO 2017-06-30 11:57:23,462 main.py:98] improve_loss: 11483.17, improve_percent: 0.69
[INFO 2017-06-30 11:57:23,464 main.py:62] model save to ./model/final.pth
[INFO 2017-06-30 11:57:23,499 main.py:66] current best improved percent: 0.69
[INFO 2017-06-30 11:57:26,608 main.py:52] epoch 12100, training loss: 5309.08, average training loss: 5257.04, base loss: 15982.00
[INFO 2017-06-30 11:57:29,718 main.py:52] epoch 12101, training loss: 5284.55, average training loss: 5257.07, base loss: 15981.86
[INFO 2017-06-30 11:57:32,882 main.py:52] epoch 12102, training loss: 5105.38, average training loss: 5257.07, base loss: 15981.88
[INFO 2017-06-30 11:57:36,050 main.py:52] epoch 12103, training loss: 5296.19, average training loss: 5257.13, base loss: 15981.90
[INFO 2017-06-30 11:57:39,182 main.py:52] epoch 12104, training loss: 5074.02, average training loss: 5256.62, base loss: 15981.81
[INFO 2017-06-30 11:57:42,277 main.py:52] epoch 12105, training loss: 5299.83, average training loss: 5256.41, base loss: 15981.96
[INFO 2017-06-30 11:57:45,415 main.py:52] epoch 12106, training loss: 4881.41, average training loss: 5255.99, base loss: 15981.86
[INFO 2017-06-30 11:57:48,549 main.py:52] epoch 12107, training loss: 5308.94, average training loss: 5255.86, base loss: 15982.05
[INFO 2017-06-30 11:57:51,729 main.py:52] epoch 12108, training loss: 5003.37, average training loss: 5255.48, base loss: 15981.97
[INFO 2017-06-30 11:57:54,867 main.py:52] epoch 12109, training loss: 5425.29, average training loss: 5255.75, base loss: 15982.09
[INFO 2017-06-30 11:57:57,974 main.py:52] epoch 12110, training loss: 5483.20, average training loss: 5255.63, base loss: 15982.14
[INFO 2017-06-30 11:58:01,084 main.py:52] epoch 12111, training loss: 5452.45, average training loss: 5255.49, base loss: 15982.47
[INFO 2017-06-30 11:58:04,265 main.py:52] epoch 12112, training loss: 4746.52, average training loss: 5255.07, base loss: 15982.29
[INFO 2017-06-30 11:58:07,405 main.py:52] epoch 12113, training loss: 5265.15, average training loss: 5255.00, base loss: 15982.52
[INFO 2017-06-30 11:58:10,575 main.py:52] epoch 12114, training loss: 5201.50, average training loss: 5255.19, base loss: 15982.43
[INFO 2017-06-30 11:58:13,701 main.py:52] epoch 12115, training loss: 5456.13, average training loss: 5255.53, base loss: 15982.52
[INFO 2017-06-30 11:58:16,813 main.py:52] epoch 12116, training loss: 5206.01, average training loss: 5255.51, base loss: 15982.34
[INFO 2017-06-30 11:58:19,953 main.py:52] epoch 12117, training loss: 5313.61, average training loss: 5255.51, base loss: 15982.33
[INFO 2017-06-30 11:58:23,077 main.py:52] epoch 12118, training loss: 5188.11, average training loss: 5255.87, base loss: 15982.37
[INFO 2017-06-30 11:58:26,206 main.py:52] epoch 12119, training loss: 5741.83, average training loss: 5256.20, base loss: 15982.65
[INFO 2017-06-30 11:58:29,369 main.py:52] epoch 12120, training loss: 5379.97, average training loss: 5256.41, base loss: 15982.76
[INFO 2017-06-30 11:58:32,524 main.py:52] epoch 12121, training loss: 4818.15, average training loss: 5255.82, base loss: 15982.62
[INFO 2017-06-30 11:58:35,637 main.py:52] epoch 12122, training loss: 5180.77, average training loss: 5255.87, base loss: 15982.61
[INFO 2017-06-30 11:58:38,739 main.py:52] epoch 12123, training loss: 5396.22, average training loss: 5256.02, base loss: 15982.65
[INFO 2017-06-30 11:58:41,867 main.py:52] epoch 12124, training loss: 5441.05, average training loss: 5256.04, base loss: 15982.82
[INFO 2017-06-30 11:58:44,998 main.py:52] epoch 12125, training loss: 5504.28, average training loss: 5256.32, base loss: 15982.57
[INFO 2017-06-30 11:58:48,110 main.py:52] epoch 12126, training loss: 5042.02, average training loss: 5256.11, base loss: 15982.47
[INFO 2017-06-30 11:58:51,234 main.py:52] epoch 12127, training loss: 5238.55, average training loss: 5255.88, base loss: 15982.63
[INFO 2017-06-30 11:58:54,412 main.py:52] epoch 12128, training loss: 5253.54, average training loss: 5255.38, base loss: 15982.95
[INFO 2017-06-30 11:58:57,544 main.py:52] epoch 12129, training loss: 4982.91, average training loss: 5255.06, base loss: 15982.99
[INFO 2017-06-30 11:59:00,690 main.py:52] epoch 12130, training loss: 5267.07, average training loss: 5255.04, base loss: 15982.94
[INFO 2017-06-30 11:59:03,827 main.py:52] epoch 12131, training loss: 5277.58, average training loss: 5255.25, base loss: 15983.00
[INFO 2017-06-30 11:59:06,966 main.py:52] epoch 12132, training loss: 5548.57, average training loss: 5255.66, base loss: 15983.31
[INFO 2017-06-30 11:59:10,108 main.py:52] epoch 12133, training loss: 5204.87, average training loss: 5255.83, base loss: 15983.46
[INFO 2017-06-30 11:59:13,263 main.py:52] epoch 12134, training loss: 5262.38, average training loss: 5255.66, base loss: 15983.64
[INFO 2017-06-30 11:59:16,358 main.py:52] epoch 12135, training loss: 5273.22, average training loss: 5255.60, base loss: 15983.95
[INFO 2017-06-30 11:59:19,452 main.py:52] epoch 12136, training loss: 5019.51, average training loss: 5255.66, base loss: 15983.67
[INFO 2017-06-30 11:59:22,553 main.py:52] epoch 12137, training loss: 5124.19, average training loss: 5255.52, base loss: 15983.71
[INFO 2017-06-30 11:59:25,710 main.py:52] epoch 12138, training loss: 5871.46, average training loss: 5256.01, base loss: 15984.17
[INFO 2017-06-30 11:59:28,829 main.py:52] epoch 12139, training loss: 5066.75, average training loss: 5256.17, base loss: 15984.23
[INFO 2017-06-30 11:59:31,920 main.py:52] epoch 12140, training loss: 4937.04, average training loss: 5256.00, base loss: 15984.13
[INFO 2017-06-30 11:59:35,056 main.py:52] epoch 12141, training loss: 5320.60, average training loss: 5255.95, base loss: 15984.16
[INFO 2017-06-30 11:59:38,209 main.py:52] epoch 12142, training loss: 5413.04, average training loss: 5256.52, base loss: 15984.23
[INFO 2017-06-30 11:59:41,335 main.py:52] epoch 12143, training loss: 5409.42, average training loss: 5256.60, base loss: 15984.38
[INFO 2017-06-30 11:59:44,480 main.py:52] epoch 12144, training loss: 5127.51, average training loss: 5256.58, base loss: 15984.22
[INFO 2017-06-30 11:59:47,595 main.py:52] epoch 12145, training loss: 5732.71, average training loss: 5257.10, base loss: 15984.51
[INFO 2017-06-30 11:59:50,724 main.py:52] epoch 12146, training loss: 5043.89, average training loss: 5256.84, base loss: 15984.24
[INFO 2017-06-30 11:59:53,859 main.py:52] epoch 12147, training loss: 5050.82, average training loss: 5256.94, base loss: 15984.06
[INFO 2017-06-30 11:59:57,014 main.py:52] epoch 12148, training loss: 5241.27, average training loss: 5256.82, base loss: 15983.91
[INFO 2017-06-30 12:00:00,134 main.py:52] epoch 12149, training loss: 5340.81, average training loss: 5256.98, base loss: 15984.16
[INFO 2017-06-30 12:00:03,289 main.py:52] epoch 12150, training loss: 5129.21, average training loss: 5257.23, base loss: 15984.09
[INFO 2017-06-30 12:00:06,421 main.py:52] epoch 12151, training loss: 5056.59, average training loss: 5257.21, base loss: 15983.77
[INFO 2017-06-30 12:00:09,564 main.py:52] epoch 12152, training loss: 5169.78, average training loss: 5257.12, base loss: 15983.53
[INFO 2017-06-30 12:00:12,750 main.py:52] epoch 12153, training loss: 5060.75, average training loss: 5256.46, base loss: 15983.53
[INFO 2017-06-30 12:00:15,845 main.py:52] epoch 12154, training loss: 5251.99, average training loss: 5256.59, base loss: 15983.46
[INFO 2017-06-30 12:00:19,008 main.py:52] epoch 12155, training loss: 5154.12, average training loss: 5256.28, base loss: 15983.54
[INFO 2017-06-30 12:00:22,126 main.py:52] epoch 12156, training loss: 4966.58, average training loss: 5255.59, base loss: 15983.39
[INFO 2017-06-30 12:00:25,254 main.py:52] epoch 12157, training loss: 5208.42, average training loss: 5255.44, base loss: 15983.43
[INFO 2017-06-30 12:00:28,427 main.py:52] epoch 12158, training loss: 5344.91, average training loss: 5255.26, base loss: 15983.20
[INFO 2017-06-30 12:00:31,560 main.py:52] epoch 12159, training loss: 5291.18, average training loss: 5254.82, base loss: 15983.25
[INFO 2017-06-30 12:00:34,698 main.py:52] epoch 12160, training loss: 5179.36, average training loss: 5254.65, base loss: 15983.06
[INFO 2017-06-30 12:00:37,876 main.py:52] epoch 12161, training loss: 5315.29, average training loss: 5254.73, base loss: 15982.96
[INFO 2017-06-30 12:00:40,986 main.py:52] epoch 12162, training loss: 5227.83, average training loss: 5254.65, base loss: 15983.12
[INFO 2017-06-30 12:00:44,141 main.py:52] epoch 12163, training loss: 5411.03, average training loss: 5254.91, base loss: 15983.16
[INFO 2017-06-30 12:00:47,255 main.py:52] epoch 12164, training loss: 4715.72, average training loss: 5254.62, base loss: 15982.76
[INFO 2017-06-30 12:00:50,465 main.py:52] epoch 12165, training loss: 5467.26, average training loss: 5254.79, base loss: 15982.81
[INFO 2017-06-30 12:00:53,623 main.py:52] epoch 12166, training loss: 5478.94, average training loss: 5255.02, base loss: 15983.28
[INFO 2017-06-30 12:00:56,766 main.py:52] epoch 12167, training loss: 5333.37, average training loss: 5254.96, base loss: 15983.38
[INFO 2017-06-30 12:00:59,895 main.py:52] epoch 12168, training loss: 5155.79, average training loss: 5255.10, base loss: 15983.56
[INFO 2017-06-30 12:01:03,016 main.py:52] epoch 12169, training loss: 5084.78, average training loss: 5254.77, base loss: 15983.78
[INFO 2017-06-30 12:01:06,160 main.py:52] epoch 12170, training loss: 5099.32, average training loss: 5254.46, base loss: 15983.68
[INFO 2017-06-30 12:01:09,304 main.py:52] epoch 12171, training loss: 5008.96, average training loss: 5254.13, base loss: 15983.59
[INFO 2017-06-30 12:01:12,441 main.py:52] epoch 12172, training loss: 5010.76, average training loss: 5253.81, base loss: 15983.65
[INFO 2017-06-30 12:01:15,583 main.py:52] epoch 12173, training loss: 5519.16, average training loss: 5254.06, base loss: 15983.74
[INFO 2017-06-30 12:01:18,689 main.py:52] epoch 12174, training loss: 5042.69, average training loss: 5254.34, base loss: 15983.76
[INFO 2017-06-30 12:01:21,811 main.py:52] epoch 12175, training loss: 5214.71, average training loss: 5254.39, base loss: 15983.90
[INFO 2017-06-30 12:01:24,973 main.py:52] epoch 12176, training loss: 5296.57, average training loss: 5254.52, base loss: 15983.95
[INFO 2017-06-30 12:01:28,122 main.py:52] epoch 12177, training loss: 5098.14, average training loss: 5254.45, base loss: 15983.95
[INFO 2017-06-30 12:01:31,249 main.py:52] epoch 12178, training loss: 4916.71, average training loss: 5254.05, base loss: 15984.03
[INFO 2017-06-30 12:01:34,440 main.py:52] epoch 12179, training loss: 5350.08, average training loss: 5254.14, base loss: 15984.00
[INFO 2017-06-30 12:01:37,550 main.py:52] epoch 12180, training loss: 4990.27, average training loss: 5253.80, base loss: 15984.06
[INFO 2017-06-30 12:01:40,787 main.py:52] epoch 12181, training loss: 5492.73, average training loss: 5254.32, base loss: 15984.28
[INFO 2017-06-30 12:01:43,959 main.py:52] epoch 12182, training loss: 5333.16, average training loss: 5254.62, base loss: 15984.35
[INFO 2017-06-30 12:01:47,125 main.py:52] epoch 12183, training loss: 5316.92, average training loss: 5254.70, base loss: 15984.57
[INFO 2017-06-30 12:01:50,292 main.py:52] epoch 12184, training loss: 5067.84, average training loss: 5254.35, base loss: 15984.42
[INFO 2017-06-30 12:01:53,407 main.py:52] epoch 12185, training loss: 4909.39, average training loss: 5253.80, base loss: 15984.32
[INFO 2017-06-30 12:01:56,534 main.py:52] epoch 12186, training loss: 5137.78, average training loss: 5253.28, base loss: 15984.47
[INFO 2017-06-30 12:01:59,720 main.py:52] epoch 12187, training loss: 5071.64, average training loss: 5253.04, base loss: 15984.43
[INFO 2017-06-30 12:02:02,861 main.py:52] epoch 12188, training loss: 5378.18, average training loss: 5253.09, base loss: 15984.37
[INFO 2017-06-30 12:02:05,986 main.py:52] epoch 12189, training loss: 5209.04, average training loss: 5252.93, base loss: 15984.40
[INFO 2017-06-30 12:02:09,172 main.py:52] epoch 12190, training loss: 5233.79, average training loss: 5252.92, base loss: 15984.27
[INFO 2017-06-30 12:02:12,344 main.py:52] epoch 12191, training loss: 5170.39, average training loss: 5252.95, base loss: 15984.01
[INFO 2017-06-30 12:02:15,493 main.py:52] epoch 12192, training loss: 5301.27, average training loss: 5253.44, base loss: 15984.11
[INFO 2017-06-30 12:02:18,627 main.py:52] epoch 12193, training loss: 4902.66, average training loss: 5252.97, base loss: 15983.98
[INFO 2017-06-30 12:02:21,755 main.py:52] epoch 12194, training loss: 5223.26, average training loss: 5253.18, base loss: 15983.96
[INFO 2017-06-30 12:02:24,893 main.py:52] epoch 12195, training loss: 4931.05, average training loss: 5252.85, base loss: 15983.89
[INFO 2017-06-30 12:02:27,995 main.py:52] epoch 12196, training loss: 5292.59, average training loss: 5252.76, base loss: 15983.87
[INFO 2017-06-30 12:02:31,155 main.py:52] epoch 12197, training loss: 4887.79, average training loss: 5252.33, base loss: 15983.65
[INFO 2017-06-30 12:02:34,270 main.py:52] epoch 12198, training loss: 5457.02, average training loss: 5252.42, base loss: 15983.65
[INFO 2017-06-30 12:02:37,447 main.py:52] epoch 12199, training loss: 5121.26, average training loss: 5252.30, base loss: 15983.64
[INFO 2017-06-30 12:02:37,447 main.py:54] epoch 12199, testing
[INFO 2017-06-30 12:02:50,579 main.py:97] average testing loss: 5178.44, base loss: 16003.96
[INFO 2017-06-30 12:02:50,579 main.py:98] improve_loss: 10825.52, improve_percent: 0.68
[INFO 2017-06-30 12:02:50,581 main.py:66] current best improved percent: 0.69
[INFO 2017-06-30 12:02:53,716 main.py:52] epoch 12200, training loss: 5237.02, average training loss: 5252.50, base loss: 15983.58
[INFO 2017-06-30 12:02:56,853 main.py:52] epoch 12201, training loss: 4986.05, average training loss: 5252.19, base loss: 15983.37
[INFO 2017-06-30 12:02:59,980 main.py:52] epoch 12202, training loss: 4950.19, average training loss: 5252.20, base loss: 15983.28
[INFO 2017-06-30 12:03:03,120 main.py:52] epoch 12203, training loss: 5052.00, average training loss: 5252.43, base loss: 15982.98
[INFO 2017-06-30 12:03:06,256 main.py:52] epoch 12204, training loss: 5542.66, average training loss: 5252.58, base loss: 15983.15
[INFO 2017-06-30 12:03:09,412 main.py:52] epoch 12205, training loss: 5097.74, average training loss: 5251.82, base loss: 15982.96
[INFO 2017-06-30 12:03:12,551 main.py:52] epoch 12206, training loss: 5359.01, average training loss: 5251.32, base loss: 15983.07
[INFO 2017-06-30 12:03:15,705 main.py:52] epoch 12207, training loss: 5222.21, average training loss: 5251.51, base loss: 15982.77
[INFO 2017-06-30 12:03:18,816 main.py:52] epoch 12208, training loss: 5029.28, average training loss: 5251.49, base loss: 15982.60
[INFO 2017-06-30 12:03:22,016 main.py:52] epoch 12209, training loss: 4723.61, average training loss: 5251.20, base loss: 15982.08
[INFO 2017-06-30 12:03:25,157 main.py:52] epoch 12210, training loss: 5533.05, average training loss: 5251.23, base loss: 15982.22
[INFO 2017-06-30 12:03:28,281 main.py:52] epoch 12211, training loss: 5302.99, average training loss: 5251.41, base loss: 15982.23
[INFO 2017-06-30 12:03:31,414 main.py:52] epoch 12212, training loss: 5744.75, average training loss: 5251.72, base loss: 15982.76
[INFO 2017-06-30 12:03:34,543 main.py:52] epoch 12213, training loss: 5214.02, average training loss: 5251.66, base loss: 15982.91
[INFO 2017-06-30 12:03:37,702 main.py:52] epoch 12214, training loss: 5187.68, average training loss: 5251.60, base loss: 15983.06
[INFO 2017-06-30 12:03:40,885 main.py:52] epoch 12215, training loss: 5229.78, average training loss: 5251.54, base loss: 15983.19
[INFO 2017-06-30 12:03:44,004 main.py:52] epoch 12216, training loss: 5379.45, average training loss: 5251.62, base loss: 15983.50
[INFO 2017-06-30 12:03:47,152 main.py:52] epoch 12217, training loss: 5029.89, average training loss: 5251.57, base loss: 15983.52
[INFO 2017-06-30 12:03:50,266 main.py:52] epoch 12218, training loss: 5205.64, average training loss: 5251.37, base loss: 15983.35
[INFO 2017-06-30 12:03:53,384 main.py:52] epoch 12219, training loss: 5140.72, average training loss: 5251.64, base loss: 15983.46
[INFO 2017-06-30 12:03:56,582 main.py:52] epoch 12220, training loss: 5187.96, average training loss: 5251.54, base loss: 15983.42
[INFO 2017-06-30 12:03:59,730 main.py:52] epoch 12221, training loss: 5302.16, average training loss: 5251.15, base loss: 15983.53
[INFO 2017-06-30 12:04:02,912 main.py:52] epoch 12222, training loss: 5147.97, average training loss: 5250.84, base loss: 15983.32
[INFO 2017-06-30 12:04:06,076 main.py:52] epoch 12223, training loss: 5178.83, average training loss: 5250.52, base loss: 15983.16
[INFO 2017-06-30 12:04:09,205 main.py:52] epoch 12224, training loss: 5249.31, average training loss: 5250.57, base loss: 15983.23
[INFO 2017-06-30 12:04:12,314 main.py:52] epoch 12225, training loss: 4859.98, average training loss: 5250.11, base loss: 15983.02
[INFO 2017-06-30 12:04:15,500 main.py:52] epoch 12226, training loss: 5204.75, average training loss: 5250.05, base loss: 15983.13
[INFO 2017-06-30 12:04:18,632 main.py:52] epoch 12227, training loss: 5355.82, average training loss: 5250.42, base loss: 15983.33
[INFO 2017-06-30 12:04:21,799 main.py:52] epoch 12228, training loss: 5007.98, average training loss: 5250.40, base loss: 15983.50
[INFO 2017-06-30 12:04:24,922 main.py:52] epoch 12229, training loss: 5341.13, average training loss: 5250.29, base loss: 15983.85
[INFO 2017-06-30 12:04:28,053 main.py:52] epoch 12230, training loss: 5375.06, average training loss: 5250.45, base loss: 15983.97
[INFO 2017-06-30 12:04:31,236 main.py:52] epoch 12231, training loss: 5529.12, average training loss: 5250.47, base loss: 15983.76
[INFO 2017-06-30 12:04:34,349 main.py:52] epoch 12232, training loss: 5040.18, average training loss: 5249.80, base loss: 15983.55
[INFO 2017-06-30 12:04:37,505 main.py:52] epoch 12233, training loss: 4756.26, average training loss: 5249.33, base loss: 15983.27
[INFO 2017-06-30 12:04:40,657 main.py:52] epoch 12234, training loss: 5037.74, average training loss: 5249.34, base loss: 15983.28
[INFO 2017-06-30 12:04:43,786 main.py:52] epoch 12235, training loss: 5093.87, average training loss: 5249.44, base loss: 15983.29
[INFO 2017-06-30 12:04:46,926 main.py:52] epoch 12236, training loss: 5397.75, average training loss: 5249.40, base loss: 15983.25
[INFO 2017-06-30 12:04:50,064 main.py:52] epoch 12237, training loss: 5720.06, average training loss: 5250.08, base loss: 15983.58
[INFO 2017-06-30 12:04:53,236 main.py:52] epoch 12238, training loss: 5023.29, average training loss: 5249.78, base loss: 15983.30
[INFO 2017-06-30 12:04:56,385 main.py:52] epoch 12239, training loss: 5444.58, average training loss: 5250.12, base loss: 15983.22
[INFO 2017-06-30 12:04:59,568 main.py:52] epoch 12240, training loss: 5093.77, average training loss: 5249.93, base loss: 15983.35
[INFO 2017-06-30 12:05:02,705 main.py:52] epoch 12241, training loss: 4956.85, average training loss: 5249.83, base loss: 15983.26
[INFO 2017-06-30 12:05:05,864 main.py:52] epoch 12242, training loss: 5665.18, average training loss: 5250.08, base loss: 15983.41
[INFO 2017-06-30 12:05:09,024 main.py:52] epoch 12243, training loss: 5021.69, average training loss: 5249.66, base loss: 15983.43
[INFO 2017-06-30 12:05:12,149 main.py:52] epoch 12244, training loss: 5402.80, average training loss: 5250.01, base loss: 15983.18
[INFO 2017-06-30 12:05:15,285 main.py:52] epoch 12245, training loss: 5090.71, average training loss: 5250.01, base loss: 15982.99
[INFO 2017-06-30 12:05:18,409 main.py:52] epoch 12246, training loss: 5023.58, average training loss: 5250.09, base loss: 15982.92
[INFO 2017-06-30 12:05:21,585 main.py:52] epoch 12247, training loss: 5447.18, average training loss: 5250.56, base loss: 15982.98
[INFO 2017-06-30 12:05:24,726 main.py:52] epoch 12248, training loss: 5153.54, average training loss: 5250.31, base loss: 15982.89
[INFO 2017-06-30 12:05:27,914 main.py:52] epoch 12249, training loss: 5332.15, average training loss: 5250.41, base loss: 15982.88
[INFO 2017-06-30 12:05:31,032 main.py:52] epoch 12250, training loss: 5400.80, average training loss: 5250.27, base loss: 15982.92
[INFO 2017-06-30 12:05:34,155 main.py:52] epoch 12251, training loss: 5163.93, average training loss: 5250.23, base loss: 15982.74
[INFO 2017-06-30 12:05:37,264 main.py:52] epoch 12252, training loss: 5328.64, average training loss: 5250.34, base loss: 15982.65
[INFO 2017-06-30 12:05:40,434 main.py:52] epoch 12253, training loss: 5372.73, average training loss: 5250.53, base loss: 15982.41
[INFO 2017-06-30 12:05:43,593 main.py:52] epoch 12254, training loss: 5253.42, average training loss: 5250.95, base loss: 15982.36
[INFO 2017-06-30 12:05:46,764 main.py:52] epoch 12255, training loss: 4834.75, average training loss: 5250.62, base loss: 15982.25
[INFO 2017-06-30 12:05:49,919 main.py:52] epoch 12256, training loss: 5364.52, average training loss: 5250.24, base loss: 15982.47
[INFO 2017-06-30 12:05:53,071 main.py:52] epoch 12257, training loss: 5277.21, average training loss: 5250.50, base loss: 15982.57
[INFO 2017-06-30 12:05:56,249 main.py:52] epoch 12258, training loss: 5338.39, average training loss: 5250.71, base loss: 15982.69
[INFO 2017-06-30 12:05:59,378 main.py:52] epoch 12259, training loss: 5099.03, average training loss: 5250.63, base loss: 15982.71
[INFO 2017-06-30 12:06:02,527 main.py:52] epoch 12260, training loss: 5128.07, average training loss: 5250.69, base loss: 15982.54
[INFO 2017-06-30 12:06:05,670 main.py:52] epoch 12261, training loss: 5219.57, average training loss: 5250.66, base loss: 15982.48
[INFO 2017-06-30 12:06:08,800 main.py:52] epoch 12262, training loss: 5338.91, average training loss: 5250.66, base loss: 15982.42
[INFO 2017-06-30 12:06:11,912 main.py:52] epoch 12263, training loss: 5057.83, average training loss: 5250.45, base loss: 15982.53
[INFO 2017-06-30 12:06:15,070 main.py:52] epoch 12264, training loss: 5309.67, average training loss: 5250.40, base loss: 15982.42
[INFO 2017-06-30 12:06:18,226 main.py:52] epoch 12265, training loss: 5443.25, average training loss: 5250.38, base loss: 15982.53
[INFO 2017-06-30 12:06:21,338 main.py:52] epoch 12266, training loss: 5324.28, average training loss: 5250.50, base loss: 15982.62
[INFO 2017-06-30 12:06:24,467 main.py:52] epoch 12267, training loss: 4947.03, average training loss: 5249.75, base loss: 15982.68
[INFO 2017-06-30 12:06:27,605 main.py:52] epoch 12268, training loss: 5117.74, average training loss: 5249.53, base loss: 15982.65
[INFO 2017-06-30 12:06:30,755 main.py:52] epoch 12269, training loss: 4825.15, average training loss: 5248.81, base loss: 15982.26
[INFO 2017-06-30 12:06:33,914 main.py:52] epoch 12270, training loss: 5297.17, average training loss: 5248.89, base loss: 15982.13
[INFO 2017-06-30 12:06:37,064 main.py:52] epoch 12271, training loss: 5541.80, average training loss: 5249.24, base loss: 15982.22
[INFO 2017-06-30 12:06:40,217 main.py:52] epoch 12272, training loss: 5386.22, average training loss: 5249.43, base loss: 15982.35
[INFO 2017-06-30 12:06:43,361 main.py:52] epoch 12273, training loss: 5113.82, average training loss: 5249.50, base loss: 15982.50
[INFO 2017-06-30 12:06:46,532 main.py:52] epoch 12274, training loss: 5137.20, average training loss: 5249.29, base loss: 15982.48
[INFO 2017-06-30 12:06:49,653 main.py:52] epoch 12275, training loss: 5515.36, average training loss: 5249.25, base loss: 15982.68
[INFO 2017-06-30 12:06:52,843 main.py:52] epoch 12276, training loss: 5013.28, average training loss: 5249.10, base loss: 15982.70
[INFO 2017-06-30 12:06:55,985 main.py:52] epoch 12277, training loss: 5483.58, average training loss: 5249.45, base loss: 15983.06
[INFO 2017-06-30 12:06:59,123 main.py:52] epoch 12278, training loss: 5116.17, average training loss: 5249.08, base loss: 15983.05
[INFO 2017-06-30 12:07:02,317 main.py:52] epoch 12279, training loss: 5348.57, average training loss: 5249.53, base loss: 15982.93
[INFO 2017-06-30 12:07:05,458 main.py:52] epoch 12280, training loss: 4987.23, average training loss: 5249.35, base loss: 15982.74
[INFO 2017-06-30 12:07:08,594 main.py:52] epoch 12281, training loss: 5060.50, average training loss: 5248.84, base loss: 15982.77
[INFO 2017-06-30 12:07:11,726 main.py:52] epoch 12282, training loss: 5323.50, average training loss: 5248.67, base loss: 15982.79
[INFO 2017-06-30 12:07:14,903 main.py:52] epoch 12283, training loss: 5142.55, average training loss: 5248.59, base loss: 15982.67
[INFO 2017-06-30 12:07:18,062 main.py:52] epoch 12284, training loss: 5249.16, average training loss: 5248.69, base loss: 15982.74
[INFO 2017-06-30 12:07:21,201 main.py:52] epoch 12285, training loss: 5082.40, average training loss: 5248.16, base loss: 15982.45
[INFO 2017-06-30 12:07:24,356 main.py:52] epoch 12286, training loss: 5324.58, average training loss: 5247.76, base loss: 15982.62
[INFO 2017-06-30 12:07:27,560 main.py:52] epoch 12287, training loss: 5327.19, average training loss: 5247.54, base loss: 15982.86
[INFO 2017-06-30 12:07:30,743 main.py:52] epoch 12288, training loss: 5450.40, average training loss: 5247.66, base loss: 15982.96
[INFO 2017-06-30 12:07:33,887 main.py:52] epoch 12289, training loss: 4829.69, average training loss: 5246.91, base loss: 15982.90
[INFO 2017-06-30 12:07:37,016 main.py:52] epoch 12290, training loss: 5117.82, average training loss: 5246.91, base loss: 15982.90
[INFO 2017-06-30 12:07:40,194 main.py:52] epoch 12291, training loss: 5336.57, average training loss: 5247.07, base loss: 15982.89
[INFO 2017-06-30 12:07:43,340 main.py:52] epoch 12292, training loss: 5077.78, average training loss: 5246.90, base loss: 15982.96
[INFO 2017-06-30 12:07:46,529 main.py:52] epoch 12293, training loss: 4696.25, average training loss: 5245.91, base loss: 15982.88
[INFO 2017-06-30 12:07:49,681 main.py:52] epoch 12294, training loss: 5231.84, average training loss: 5245.82, base loss: 15982.89
[INFO 2017-06-30 12:07:52,836 main.py:52] epoch 12295, training loss: 5094.25, average training loss: 5245.61, base loss: 15982.81
[INFO 2017-06-30 12:07:56,029 main.py:52] epoch 12296, training loss: 5184.27, average training loss: 5245.92, base loss: 15982.68
[INFO 2017-06-30 12:07:59,182 main.py:52] epoch 12297, training loss: 4954.56, average training loss: 5245.59, base loss: 15982.59
[INFO 2017-06-30 12:08:02,372 main.py:52] epoch 12298, training loss: 5258.40, average training loss: 5245.60, base loss: 15982.43
[INFO 2017-06-30 12:08:05,496 main.py:52] epoch 12299, training loss: 5603.94, average training loss: 5245.89, base loss: 15982.50
[INFO 2017-06-30 12:08:05,497 main.py:54] epoch 12299, testing
[INFO 2017-06-30 12:08:18,625 main.py:97] average testing loss: 5150.51, base loss: 16362.97
[INFO 2017-06-30 12:08:18,625 main.py:98] improve_loss: 11212.47, improve_percent: 0.69
[INFO 2017-06-30 12:08:18,627 main.py:66] current best improved percent: 0.69
[INFO 2017-06-30 12:08:21,765 main.py:52] epoch 12300, training loss: 5062.75, average training loss: 5245.51, base loss: 15982.50
[INFO 2017-06-30 12:08:24,909 main.py:52] epoch 12301, training loss: 4904.01, average training loss: 5245.44, base loss: 15982.47
[INFO 2017-06-30 12:08:28,083 main.py:52] epoch 12302, training loss: 5844.96, average training loss: 5245.82, base loss: 15982.74
[INFO 2017-06-30 12:08:31,205 main.py:52] epoch 12303, training loss: 4815.40, average training loss: 5245.63, base loss: 15982.68
[INFO 2017-06-30 12:08:34,411 main.py:52] epoch 12304, training loss: 5003.10, average training loss: 5245.34, base loss: 15982.64
[INFO 2017-06-30 12:08:37,564 main.py:52] epoch 12305, training loss: 5214.68, average training loss: 5245.52, base loss: 15982.70
[INFO 2017-06-30 12:08:40,679 main.py:52] epoch 12306, training loss: 5510.16, average training loss: 5245.60, base loss: 15982.64
[INFO 2017-06-30 12:08:43,820 main.py:52] epoch 12307, training loss: 5166.63, average training loss: 5245.88, base loss: 15982.36
[INFO 2017-06-30 12:08:46,979 main.py:52] epoch 12308, training loss: 5204.56, average training loss: 5246.18, base loss: 15982.19
[INFO 2017-06-30 12:08:50,131 main.py:52] epoch 12309, training loss: 4894.11, average training loss: 5245.70, base loss: 15981.84
[INFO 2017-06-30 12:08:53,298 main.py:52] epoch 12310, training loss: 5260.74, average training loss: 5245.69, base loss: 15982.16
[INFO 2017-06-30 12:08:56,456 main.py:52] epoch 12311, training loss: 4958.72, average training loss: 5244.81, base loss: 15981.93
[INFO 2017-06-30 12:08:59,557 main.py:52] epoch 12312, training loss: 5208.21, average training loss: 5244.90, base loss: 15981.92
[INFO 2017-06-30 12:09:02,679 main.py:52] epoch 12313, training loss: 5144.69, average training loss: 5244.62, base loss: 15981.71
[INFO 2017-06-30 12:09:05,828 main.py:52] epoch 12314, training loss: 5476.31, average training loss: 5245.03, base loss: 15982.08
[INFO 2017-06-30 12:09:08,911 main.py:52] epoch 12315, training loss: 4865.00, average training loss: 5244.82, base loss: 15982.11
[INFO 2017-06-30 12:09:12,076 main.py:52] epoch 12316, training loss: 5315.43, average training loss: 5244.77, base loss: 15982.29
[INFO 2017-06-30 12:09:15,187 main.py:52] epoch 12317, training loss: 5045.90, average training loss: 5244.95, base loss: 15982.14
[INFO 2017-06-30 12:09:18,331 main.py:52] epoch 12318, training loss: 5123.51, average training loss: 5244.89, base loss: 15982.17
[INFO 2017-06-30 12:09:21,501 main.py:52] epoch 12319, training loss: 5346.44, average training loss: 5244.68, base loss: 15982.07
[INFO 2017-06-30 12:09:24,649 main.py:52] epoch 12320, training loss: 5201.39, average training loss: 5244.72, base loss: 15982.00
[INFO 2017-06-30 12:09:27,805 main.py:52] epoch 12321, training loss: 5496.19, average training loss: 5244.85, base loss: 15982.11
[INFO 2017-06-30 12:09:30,942 main.py:52] epoch 12322, training loss: 5210.68, average training loss: 5244.75, base loss: 15982.07
[INFO 2017-06-30 12:09:34,103 main.py:52] epoch 12323, training loss: 5453.30, average training loss: 5245.09, base loss: 15982.12
[INFO 2017-06-30 12:09:37,227 main.py:52] epoch 12324, training loss: 5192.10, average training loss: 5245.22, base loss: 15982.29
[INFO 2017-06-30 12:09:40,401 main.py:52] epoch 12325, training loss: 5095.66, average training loss: 5244.96, base loss: 15982.32
[INFO 2017-06-30 12:09:43,529 main.py:52] epoch 12326, training loss: 4771.32, average training loss: 5244.32, base loss: 15982.07
[INFO 2017-06-30 12:09:46,675 main.py:52] epoch 12327, training loss: 5092.97, average training loss: 5244.50, base loss: 15981.93
[INFO 2017-06-30 12:09:49,820 main.py:52] epoch 12328, training loss: 5501.34, average training loss: 5244.39, base loss: 15982.03
[INFO 2017-06-30 12:09:52,970 main.py:52] epoch 12329, training loss: 5100.19, average training loss: 5244.31, base loss: 15981.76
[INFO 2017-06-30 12:09:56,116 main.py:52] epoch 12330, training loss: 5697.78, average training loss: 5244.75, base loss: 15982.07
[INFO 2017-06-30 12:09:59,266 main.py:52] epoch 12331, training loss: 5218.43, average training loss: 5245.22, base loss: 15982.15
[INFO 2017-06-30 12:10:02,453 main.py:52] epoch 12332, training loss: 5664.05, average training loss: 5245.69, base loss: 15982.32
[INFO 2017-06-30 12:10:05,605 main.py:52] epoch 12333, training loss: 4690.03, average training loss: 5245.05, base loss: 15982.08
[INFO 2017-06-30 12:10:08,781 main.py:52] epoch 12334, training loss: 5319.39, average training loss: 5244.97, base loss: 15981.88
[INFO 2017-06-30 12:10:11,924 main.py:52] epoch 12335, training loss: 5155.45, average training loss: 5244.43, base loss: 15981.79
[INFO 2017-06-30 12:10:15,124 main.py:52] epoch 12336, training loss: 5185.47, average training loss: 5244.58, base loss: 15981.80
[INFO 2017-06-30 12:10:18,250 main.py:52] epoch 12337, training loss: 5219.23, average training loss: 5244.82, base loss: 15981.98
[INFO 2017-06-30 12:10:21,383 main.py:52] epoch 12338, training loss: 5125.31, average training loss: 5244.35, base loss: 15982.12
[INFO 2017-06-30 12:10:24,504 main.py:52] epoch 12339, training loss: 5218.62, average training loss: 5244.43, base loss: 15982.25
[INFO 2017-06-30 12:10:27,682 main.py:52] epoch 12340, training loss: 4897.22, average training loss: 5243.80, base loss: 15982.04
[INFO 2017-06-30 12:10:30,797 main.py:52] epoch 12341, training loss: 4946.38, average training loss: 5243.46, base loss: 15982.01
[INFO 2017-06-30 12:10:33,939 main.py:52] epoch 12342, training loss: 5850.75, average training loss: 5243.90, base loss: 15982.35
[INFO 2017-06-30 12:10:37,087 main.py:52] epoch 12343, training loss: 5447.73, average training loss: 5243.68, base loss: 15982.62
[INFO 2017-06-30 12:10:40,229 main.py:52] epoch 12344, training loss: 5342.23, average training loss: 5243.82, base loss: 15982.77
[INFO 2017-06-30 12:10:43,386 main.py:52] epoch 12345, training loss: 5103.77, average training loss: 5243.93, base loss: 15982.69
[INFO 2017-06-30 12:10:46,516 main.py:52] epoch 12346, training loss: 5204.64, average training loss: 5243.66, base loss: 15982.70
[INFO 2017-06-30 12:10:49,676 main.py:52] epoch 12347, training loss: 5051.08, average training loss: 5243.14, base loss: 15982.34
[INFO 2017-06-30 12:10:52,866 main.py:52] epoch 12348, training loss: 4964.72, average training loss: 5243.09, base loss: 15982.04
[INFO 2017-06-30 12:10:55,979 main.py:52] epoch 12349, training loss: 5212.86, average training loss: 5243.40, base loss: 15982.13
[INFO 2017-06-30 12:10:59,117 main.py:52] epoch 12350, training loss: 5403.69, average training loss: 5243.65, base loss: 15982.45
[INFO 2017-06-30 12:11:02,318 main.py:52] epoch 12351, training loss: 4983.23, average training loss: 5243.22, base loss: 15982.44
[INFO 2017-06-30 12:11:05,464 main.py:52] epoch 12352, training loss: 5116.09, average training loss: 5242.78, base loss: 15982.49
[INFO 2017-06-30 12:11:08,582 main.py:52] epoch 12353, training loss: 5517.20, average training loss: 5243.33, base loss: 15982.78
[INFO 2017-06-30 12:11:11,710 main.py:52] epoch 12354, training loss: 5108.96, average training loss: 5243.49, base loss: 15982.91
[INFO 2017-06-30 12:11:14,826 main.py:52] epoch 12355, training loss: 5137.78, average training loss: 5243.63, base loss: 15982.86
[INFO 2017-06-30 12:11:17,949 main.py:52] epoch 12356, training loss: 5015.44, average training loss: 5243.43, base loss: 15982.74
[INFO 2017-06-30 12:11:21,096 main.py:52] epoch 12357, training loss: 5038.35, average training loss: 5243.51, base loss: 15982.62
[INFO 2017-06-30 12:11:24,253 main.py:52] epoch 12358, training loss: 5393.51, average training loss: 5244.11, base loss: 15982.61
[INFO 2017-06-30 12:11:27,381 main.py:52] epoch 12359, training loss: 5174.26, average training loss: 5244.04, base loss: 15982.63
[INFO 2017-06-30 12:11:30,538 main.py:52] epoch 12360, training loss: 5492.55, average training loss: 5243.85, base loss: 15983.08
[INFO 2017-06-30 12:11:33,779 main.py:52] epoch 12361, training loss: 5235.61, average training loss: 5243.94, base loss: 15983.34
[INFO 2017-06-30 12:11:36,909 main.py:52] epoch 12362, training loss: 5371.57, average training loss: 5244.10, base loss: 15983.34
[INFO 2017-06-30 12:11:40,095 main.py:52] epoch 12363, training loss: 5077.35, average training loss: 5244.11, base loss: 15983.19
[INFO 2017-06-30 12:11:43,245 main.py:52] epoch 12364, training loss: 5087.81, average training loss: 5243.51, base loss: 15983.08
[INFO 2017-06-30 12:11:46,373 main.py:52] epoch 12365, training loss: 5475.57, average training loss: 5243.81, base loss: 15983.13
[INFO 2017-06-30 12:11:49,493 main.py:52] epoch 12366, training loss: 5545.69, average training loss: 5243.75, base loss: 15983.63
[INFO 2017-06-30 12:11:52,662 main.py:52] epoch 12367, training loss: 4953.17, average training loss: 5243.46, base loss: 15983.51
[INFO 2017-06-30 12:11:55,795 main.py:52] epoch 12368, training loss: 5219.74, average training loss: 5243.21, base loss: 15983.48
[INFO 2017-06-30 12:11:58,962 main.py:52] epoch 12369, training loss: 5585.85, average training loss: 5243.73, base loss: 15983.77
[INFO 2017-06-30 12:12:02,111 main.py:52] epoch 12370, training loss: 5378.89, average training loss: 5244.09, base loss: 15983.80
[INFO 2017-06-30 12:12:05,235 main.py:52] epoch 12371, training loss: 5427.49, average training loss: 5244.10, base loss: 15984.08
[INFO 2017-06-30 12:12:08,401 main.py:52] epoch 12372, training loss: 5287.00, average training loss: 5244.22, base loss: 15983.85
[INFO 2017-06-30 12:12:11,501 main.py:52] epoch 12373, training loss: 5357.62, average training loss: 5244.32, base loss: 15983.83
[INFO 2017-06-30 12:12:14,625 main.py:52] epoch 12374, training loss: 5043.80, average training loss: 5243.85, base loss: 15983.66
[INFO 2017-06-30 12:12:17,758 main.py:52] epoch 12375, training loss: 5213.44, average training loss: 5243.79, base loss: 15983.39
[INFO 2017-06-30 12:12:20,889 main.py:52] epoch 12376, training loss: 5124.58, average training loss: 5243.68, base loss: 15983.21
[INFO 2017-06-30 12:12:24,036 main.py:52] epoch 12377, training loss: 5420.90, average training loss: 5243.54, base loss: 15983.20
[INFO 2017-06-30 12:12:27,158 main.py:52] epoch 12378, training loss: 4920.70, average training loss: 5243.31, base loss: 15982.84
[INFO 2017-06-30 12:12:30,317 main.py:52] epoch 12379, training loss: 4949.50, average training loss: 5242.82, base loss: 15982.58
[INFO 2017-06-30 12:12:33,479 main.py:52] epoch 12380, training loss: 5771.94, average training loss: 5242.94, base loss: 15982.78
[INFO 2017-06-30 12:12:36,688 main.py:52] epoch 12381, training loss: 5147.85, average training loss: 5242.45, base loss: 15982.78
[INFO 2017-06-30 12:12:39,858 main.py:52] epoch 12382, training loss: 5117.71, average training loss: 5242.35, base loss: 15982.41
[INFO 2017-06-30 12:12:42,978 main.py:52] epoch 12383, training loss: 5277.37, average training loss: 5242.57, base loss: 15982.26
[INFO 2017-06-30 12:12:46,120 main.py:52] epoch 12384, training loss: 5064.20, average training loss: 5242.09, base loss: 15982.16
[INFO 2017-06-30 12:12:49,247 main.py:52] epoch 12385, training loss: 5187.25, average training loss: 5241.96, base loss: 15982.01
[INFO 2017-06-30 12:12:52,419 main.py:52] epoch 12386, training loss: 5297.48, average training loss: 5242.24, base loss: 15982.17
[INFO 2017-06-30 12:12:55,600 main.py:52] epoch 12387, training loss: 5399.59, average training loss: 5241.96, base loss: 15982.12
[INFO 2017-06-30 12:12:58,770 main.py:52] epoch 12388, training loss: 5512.34, average training loss: 5242.36, base loss: 15982.45
[INFO 2017-06-30 12:13:01,909 main.py:52] epoch 12389, training loss: 5105.15, average training loss: 5242.14, base loss: 15982.47
[INFO 2017-06-30 12:13:05,057 main.py:52] epoch 12390, training loss: 5561.83, average training loss: 5242.38, base loss: 15982.70
[INFO 2017-06-30 12:13:08,168 main.py:52] epoch 12391, training loss: 5643.24, average training loss: 5242.82, base loss: 15982.82
[INFO 2017-06-30 12:13:11,308 main.py:52] epoch 12392, training loss: 5276.25, average training loss: 5242.50, base loss: 15982.57
[INFO 2017-06-30 12:13:14,431 main.py:52] epoch 12393, training loss: 4878.69, average training loss: 5242.07, base loss: 15982.40
[INFO 2017-06-30 12:13:17,557 main.py:52] epoch 12394, training loss: 5075.73, average training loss: 5241.54, base loss: 15982.34
[INFO 2017-06-30 12:13:20,785 main.py:52] epoch 12395, training loss: 5218.24, average training loss: 5241.42, base loss: 15982.41
[INFO 2017-06-30 12:13:23,956 main.py:52] epoch 12396, training loss: 5057.81, average training loss: 5241.02, base loss: 15982.46
[INFO 2017-06-30 12:13:27,123 main.py:52] epoch 12397, training loss: 5091.81, average training loss: 5240.88, base loss: 15982.17
[INFO 2017-06-30 12:13:30,211 main.py:52] epoch 12398, training loss: 4920.67, average training loss: 5240.41, base loss: 15982.05
[INFO 2017-06-30 12:13:33,375 main.py:52] epoch 12399, training loss: 5311.75, average training loss: 5240.01, base loss: 15982.10
[INFO 2017-06-30 12:13:33,375 main.py:54] epoch 12399, testing
[INFO 2017-06-30 12:13:46,488 main.py:97] average testing loss: 5173.97, base loss: 15832.36
[INFO 2017-06-30 12:13:46,489 main.py:98] improve_loss: 10658.39, improve_percent: 0.67
[INFO 2017-06-30 12:13:46,490 main.py:66] current best improved percent: 0.69
[INFO 2017-06-30 12:13:49,609 main.py:52] epoch 12400, training loss: 5486.71, average training loss: 5240.12, base loss: 15982.49
[INFO 2017-06-30 12:13:52,773 main.py:52] epoch 12401, training loss: 5306.85, average training loss: 5240.29, base loss: 15982.72
[INFO 2017-06-30 12:13:55,922 main.py:52] epoch 12402, training loss: 5256.42, average training loss: 5240.39, base loss: 15982.78
[INFO 2017-06-30 12:13:59,087 main.py:52] epoch 12403, training loss: 5236.31, average training loss: 5240.23, base loss: 15982.90
[INFO 2017-06-30 12:14:02,255 main.py:52] epoch 12404, training loss: 5314.14, average training loss: 5240.52, base loss: 15983.07
[INFO 2017-06-30 12:14:05,372 main.py:52] epoch 12405, training loss: 5445.54, average training loss: 5240.42, base loss: 15983.24
[INFO 2017-06-30 12:14:08,487 main.py:52] epoch 12406, training loss: 5541.56, average training loss: 5240.70, base loss: 15982.88
[INFO 2017-06-30 12:14:11,643 main.py:52] epoch 12407, training loss: 5707.24, average training loss: 5240.72, base loss: 15982.86
[INFO 2017-06-30 12:14:14,769 main.py:52] epoch 12408, training loss: 5437.37, average training loss: 5241.12, base loss: 15982.93
[INFO 2017-06-30 12:14:17,892 main.py:52] epoch 12409, training loss: 4980.49, average training loss: 5240.98, base loss: 15982.87
[INFO 2017-06-30 12:14:21,054 main.py:52] epoch 12410, training loss: 5581.10, average training loss: 5241.52, base loss: 15983.19
[INFO 2017-06-30 12:14:24,221 main.py:52] epoch 12411, training loss: 5168.50, average training loss: 5241.40, base loss: 15983.18
[INFO 2017-06-30 12:14:27,361 main.py:52] epoch 12412, training loss: 5108.29, average training loss: 5241.33, base loss: 15982.95
[INFO 2017-06-30 12:14:30,475 main.py:52] epoch 12413, training loss: 5246.90, average training loss: 5241.48, base loss: 15982.99
[INFO 2017-06-30 12:14:33,600 main.py:52] epoch 12414, training loss: 5137.54, average training loss: 5241.24, base loss: 15983.04
[INFO 2017-06-30 12:14:36,738 main.py:52] epoch 12415, training loss: 5262.88, average training loss: 5241.10, base loss: 15982.99
[INFO 2017-06-30 12:14:39,856 main.py:52] epoch 12416, training loss: 5200.17, average training loss: 5241.12, base loss: 15983.06
[INFO 2017-06-30 12:14:43,038 main.py:52] epoch 12417, training loss: 5109.55, average training loss: 5241.29, base loss: 15983.13
[INFO 2017-06-30 12:14:46,209 main.py:52] epoch 12418, training loss: 5072.99, average training loss: 5241.12, base loss: 15982.88
[INFO 2017-06-30 12:14:49,345 main.py:52] epoch 12419, training loss: 5414.87, average training loss: 5241.37, base loss: 15983.05
[INFO 2017-06-30 12:14:52,498 main.py:52] epoch 12420, training loss: 5237.23, average training loss: 5241.13, base loss: 15982.92
[INFO 2017-06-30 12:14:55,614 main.py:52] epoch 12421, training loss: 5313.72, average training loss: 5241.17, base loss: 15982.78
[INFO 2017-06-30 12:14:58,778 main.py:52] epoch 12422, training loss: 5356.21, average training loss: 5241.77, base loss: 15982.41
[INFO 2017-06-30 12:15:01,943 main.py:52] epoch 12423, training loss: 5050.06, average training loss: 5241.52, base loss: 15982.11
[INFO 2017-06-30 12:15:05,172 main.py:52] epoch 12424, training loss: 5207.31, average training loss: 5241.23, base loss: 15982.07
[INFO 2017-06-30 12:15:08,346 main.py:52] epoch 12425, training loss: 5072.58, average training loss: 5241.09, base loss: 15981.97
[INFO 2017-06-30 12:15:11,504 main.py:52] epoch 12426, training loss: 5196.66, average training loss: 5240.97, base loss: 15981.79
[INFO 2017-06-30 12:15:14,626 main.py:52] epoch 12427, training loss: 5067.35, average training loss: 5240.79, base loss: 15981.77
[INFO 2017-06-30 12:15:17,750 main.py:52] epoch 12428, training loss: 5102.35, average training loss: 5240.58, base loss: 15981.81
[INFO 2017-06-30 12:15:20,890 main.py:52] epoch 12429, training loss: 5215.57, average training loss: 5240.20, base loss: 15981.77
[INFO 2017-06-30 12:15:24,090 main.py:52] epoch 12430, training loss: 5182.43, average training loss: 5240.32, base loss: 15981.62
[INFO 2017-06-30 12:15:27,318 main.py:52] epoch 12431, training loss: 5189.34, average training loss: 5240.17, base loss: 15981.53
[INFO 2017-06-30 12:15:30,446 main.py:52] epoch 12432, training loss: 5240.76, average training loss: 5239.73, base loss: 15981.59
[INFO 2017-06-30 12:15:33,608 main.py:52] epoch 12433, training loss: 5299.54, average training loss: 5239.80, base loss: 15981.66
[INFO 2017-06-30 12:15:36,775 main.py:52] epoch 12434, training loss: 4901.51, average training loss: 5239.27, base loss: 15981.35
[INFO 2017-06-30 12:15:39,899 main.py:52] epoch 12435, training loss: 5216.67, average training loss: 5239.34, base loss: 15981.09
[INFO 2017-06-30 12:15:43,006 main.py:52] epoch 12436, training loss: 5409.97, average training loss: 5239.53, base loss: 15981.14
[INFO 2017-06-30 12:15:46,172 main.py:52] epoch 12437, training loss: 5182.38, average training loss: 5239.39, base loss: 15981.18
[INFO 2017-06-30 12:15:49,294 main.py:52] epoch 12438, training loss: 5170.38, average training loss: 5239.41, base loss: 15980.98
[INFO 2017-06-30 12:15:52,468 main.py:52] epoch 12439, training loss: 5221.97, average training loss: 5239.41, base loss: 15980.87
[INFO 2017-06-30 12:15:55,621 main.py:52] epoch 12440, training loss: 5046.59, average training loss: 5238.95, base loss: 15981.03
[INFO 2017-06-30 12:15:58,766 main.py:52] epoch 12441, training loss: 5092.52, average training loss: 5239.10, base loss: 15981.15
[INFO 2017-06-30 12:16:01,900 main.py:52] epoch 12442, training loss: 5078.91, average training loss: 5238.79, base loss: 15981.26
[INFO 2017-06-30 12:16:05,048 main.py:52] epoch 12443, training loss: 5057.94, average training loss: 5237.97, base loss: 15981.37
[INFO 2017-06-30 12:16:08,225 main.py:52] epoch 12444, training loss: 5406.83, average training loss: 5238.08, base loss: 15981.55
[INFO 2017-06-30 12:16:11,351 main.py:52] epoch 12445, training loss: 5553.23, average training loss: 5238.76, base loss: 15981.55
[INFO 2017-06-30 12:16:14,487 main.py:52] epoch 12446, training loss: 4862.69, average training loss: 5238.44, base loss: 15981.26
[INFO 2017-06-30 12:16:17,628 main.py:52] epoch 12447, training loss: 5486.95, average training loss: 5239.05, base loss: 15981.48
[INFO 2017-06-30 12:16:20,767 main.py:52] epoch 12448, training loss: 5063.29, average training loss: 5239.21, base loss: 15981.08
[INFO 2017-06-30 12:16:23,910 main.py:52] epoch 12449, training loss: 5384.09, average training loss: 5239.29, base loss: 15981.18
[INFO 2017-06-30 12:16:27,094 main.py:52] epoch 12450, training loss: 5328.38, average training loss: 5239.10, base loss: 15981.00
[INFO 2017-06-30 12:16:30,251 main.py:52] epoch 12451, training loss: 5293.86, average training loss: 5239.33, base loss: 15981.03
[INFO 2017-06-30 12:16:33,422 main.py:52] epoch 12452, training loss: 5379.92, average training loss: 5239.46, base loss: 15980.94
[INFO 2017-06-30 12:16:36,585 main.py:52] epoch 12453, training loss: 5269.31, average training loss: 5239.41, base loss: 15980.90
[INFO 2017-06-30 12:16:39,690 main.py:52] epoch 12454, training loss: 5314.43, average training loss: 5238.95, base loss: 15980.85
[INFO 2017-06-30 12:16:42,842 main.py:52] epoch 12455, training loss: 5507.67, average training loss: 5239.06, base loss: 15980.94
[INFO 2017-06-30 12:16:46,003 main.py:52] epoch 12456, training loss: 4871.65, average training loss: 5238.97, base loss: 15980.80
[INFO 2017-06-30 12:16:49,164 main.py:52] epoch 12457, training loss: 5270.97, average training loss: 5238.87, base loss: 15980.76
[INFO 2017-06-30 12:16:52,310 main.py:52] epoch 12458, training loss: 4870.30, average training loss: 5238.40, base loss: 15980.67
[INFO 2017-06-30 12:16:55,472 main.py:52] epoch 12459, training loss: 5151.05, average training loss: 5238.36, base loss: 15980.83
[INFO 2017-06-30 12:16:58,653 main.py:52] epoch 12460, training loss: 5418.18, average training loss: 5238.18, base loss: 15980.98
[INFO 2017-06-30 12:17:01,823 main.py:52] epoch 12461, training loss: 4970.94, average training loss: 5237.41, base loss: 15980.81
[INFO 2017-06-30 12:17:04,936 main.py:52] epoch 12462, training loss: 5262.85, average training loss: 5237.62, base loss: 15980.85
[INFO 2017-06-30 12:17:08,070 main.py:52] epoch 12463, training loss: 5348.36, average training loss: 5237.65, base loss: 15981.04
[INFO 2017-06-30 12:17:11,215 main.py:52] epoch 12464, training loss: 4953.04, average training loss: 5237.68, base loss: 15981.05
[INFO 2017-06-30 12:17:14,340 main.py:52] epoch 12465, training loss: 5272.21, average training loss: 5237.68, base loss: 15981.18
[INFO 2017-06-30 12:17:17,466 main.py:52] epoch 12466, training loss: 5360.33, average training loss: 5237.73, base loss: 15981.14
[INFO 2017-06-30 12:17:20,628 main.py:52] epoch 12467, training loss: 5561.21, average training loss: 5238.11, base loss: 15981.39
[INFO 2017-06-30 12:17:23,725 main.py:52] epoch 12468, training loss: 5270.77, average training loss: 5238.62, base loss: 15981.46
[INFO 2017-06-30 12:17:26,828 main.py:52] epoch 12469, training loss: 5383.50, average training loss: 5238.57, base loss: 15981.49
[INFO 2017-06-30 12:17:30,030 main.py:52] epoch 12470, training loss: 5145.26, average training loss: 5238.23, base loss: 15981.57
[INFO 2017-06-30 12:17:33,207 main.py:52] epoch 12471, training loss: 5336.98, average training loss: 5238.33, base loss: 15981.89
[INFO 2017-06-30 12:17:36,318 main.py:52] epoch 12472, training loss: 5054.11, average training loss: 5238.48, base loss: 15982.00
[INFO 2017-06-30 12:17:39,433 main.py:52] epoch 12473, training loss: 5311.76, average training loss: 5238.54, base loss: 15982.36
[INFO 2017-06-30 12:17:42,585 main.py:52] epoch 12474, training loss: 5312.56, average training loss: 5238.55, base loss: 15982.71
[INFO 2017-06-30 12:17:45,730 main.py:52] epoch 12475, training loss: 5373.83, average training loss: 5238.52, base loss: 15982.92
[INFO 2017-06-30 12:17:48,851 main.py:52] epoch 12476, training loss: 5274.80, average training loss: 5238.29, base loss: 15982.83
[INFO 2017-06-30 12:17:52,059 main.py:52] epoch 12477, training loss: 4972.74, average training loss: 5238.05, base loss: 15982.64
[INFO 2017-06-30 12:17:55,201 main.py:52] epoch 12478, training loss: 5306.15, average training loss: 5238.14, base loss: 15982.75
[INFO 2017-06-30 12:17:58,348 main.py:52] epoch 12479, training loss: 5264.61, average training loss: 5238.04, base loss: 15982.94
[INFO 2017-06-30 12:18:01,511 main.py:52] epoch 12480, training loss: 5535.57, average training loss: 5238.44, base loss: 15982.94
[INFO 2017-06-30 12:18:04,657 main.py:52] epoch 12481, training loss: 5404.96, average training loss: 5238.89, base loss: 15983.16
[INFO 2017-06-30 12:18:07,813 main.py:52] epoch 12482, training loss: 5340.25, average training loss: 5238.54, base loss: 15983.28
[INFO 2017-06-30 12:18:10,972 main.py:52] epoch 12483, training loss: 5351.69, average training loss: 5238.90, base loss: 15983.33
[INFO 2017-06-30 12:18:14,114 main.py:52] epoch 12484, training loss: 5430.41, average training loss: 5239.03, base loss: 15983.33
[INFO 2017-06-30 12:18:17,274 main.py:52] epoch 12485, training loss: 5244.01, average training loss: 5238.87, base loss: 15983.31
[INFO 2017-06-30 12:18:20,415 main.py:52] epoch 12486, training loss: 5700.17, average training loss: 5239.28, base loss: 15983.46
[INFO 2017-06-30 12:18:23,543 main.py:52] epoch 12487, training loss: 5174.75, average training loss: 5238.75, base loss: 15983.49
[INFO 2017-06-30 12:18:26,681 main.py:52] epoch 12488, training loss: 5055.13, average training loss: 5238.57, base loss: 15983.27
[INFO 2017-06-30 12:18:29,883 main.py:52] epoch 12489, training loss: 5336.79, average training loss: 5238.64, base loss: 15983.34
[INFO 2017-06-30 12:18:33,076 main.py:52] epoch 12490, training loss: 5192.29, average training loss: 5238.28, base loss: 15983.55
[INFO 2017-06-30 12:18:36,203 main.py:52] epoch 12491, training loss: 5131.21, average training loss: 5238.22, base loss: 15983.82
[INFO 2017-06-30 12:18:39,336 main.py:52] epoch 12492, training loss: 5477.97, average training loss: 5238.34, base loss: 15983.98
[INFO 2017-06-30 12:18:42,524 main.py:52] epoch 12493, training loss: 5093.49, average training loss: 5238.02, base loss: 15983.96
[INFO 2017-06-30 12:18:45,620 main.py:52] epoch 12494, training loss: 5228.89, average training loss: 5237.88, base loss: 15983.80
[INFO 2017-06-30 12:18:48,752 main.py:52] epoch 12495, training loss: 5094.10, average training loss: 5237.77, base loss: 15983.71
[INFO 2017-06-30 12:18:51,920 main.py:52] epoch 12496, training loss: 5463.68, average training loss: 5238.18, base loss: 15983.90
[INFO 2017-06-30 12:18:55,044 main.py:52] epoch 12497, training loss: 5229.42, average training loss: 5237.69, base loss: 15984.12
[INFO 2017-06-30 12:18:58,178 main.py:52] epoch 12498, training loss: 5439.71, average training loss: 5238.06, base loss: 15984.21
[INFO 2017-06-30 12:19:01,346 main.py:52] epoch 12499, training loss: 5031.29, average training loss: 5237.59, base loss: 15984.20
[INFO 2017-06-30 12:19:01,347 main.py:54] epoch 12499, testing
[INFO 2017-06-30 12:19:14,378 main.py:97] average testing loss: 5074.19, base loss: 15287.41
[INFO 2017-06-30 12:19:14,378 main.py:98] improve_loss: 10213.22, improve_percent: 0.67
[INFO 2017-06-30 12:19:14,381 main.py:66] current best improved percent: 0.69
[INFO 2017-06-30 12:19:17,481 main.py:52] epoch 12500, training loss: 5240.63, average training loss: 5237.73, base loss: 15984.45
[INFO 2017-06-30 12:19:20,599 main.py:52] epoch 12501, training loss: 5162.94, average training loss: 5237.41, base loss: 15984.45
[INFO 2017-06-30 12:19:23,766 main.py:52] epoch 12502, training loss: 5119.01, average training loss: 5237.46, base loss: 15984.57
[INFO 2017-06-30 12:19:26,939 main.py:52] epoch 12503, training loss: 5161.38, average training loss: 5237.58, base loss: 15984.57
[INFO 2017-06-30 12:19:30,102 main.py:52] epoch 12504, training loss: 5032.67, average training loss: 5237.38, base loss: 15984.42
[INFO 2017-06-30 12:19:33,233 main.py:52] epoch 12505, training loss: 5433.11, average training loss: 5237.66, base loss: 15984.73
[INFO 2017-06-30 12:19:36,358 main.py:52] epoch 12506, training loss: 5064.26, average training loss: 5237.69, base loss: 15984.98
[INFO 2017-06-30 12:19:39,520 main.py:52] epoch 12507, training loss: 5308.16, average training loss: 5237.79, base loss: 15985.12
[INFO 2017-06-30 12:19:42,684 main.py:52] epoch 12508, training loss: 5024.91, average training loss: 5237.79, base loss: 15985.02
[INFO 2017-06-30 12:19:45,846 main.py:52] epoch 12509, training loss: 5015.30, average training loss: 5237.35, base loss: 15984.91
[INFO 2017-06-30 12:19:48,934 main.py:52] epoch 12510, training loss: 5036.57, average training loss: 5237.29, base loss: 15985.12
[INFO 2017-06-30 12:19:52,098 main.py:52] epoch 12511, training loss: 5080.96, average training loss: 5237.29, base loss: 15985.21
[INFO 2017-06-30 12:19:55,223 main.py:52] epoch 12512, training loss: 5250.11, average training loss: 5237.31, base loss: 15984.96
[INFO 2017-06-30 12:19:58,410 main.py:52] epoch 12513, training loss: 5154.94, average training loss: 5237.01, base loss: 15984.67
[INFO 2017-06-30 12:20:01,525 main.py:52] epoch 12514, training loss: 5117.44, average training loss: 5236.83, base loss: 15984.47
[INFO 2017-06-30 12:20:04,663 main.py:52] epoch 12515, training loss: 4992.93, average training loss: 5236.53, base loss: 15984.30
[INFO 2017-06-30 12:20:07,825 main.py:52] epoch 12516, training loss: 5128.12, average training loss: 5236.70, base loss: 15984.20
[INFO 2017-06-30 12:20:10,970 main.py:52] epoch 12517, training loss: 5066.07, average training loss: 5236.61, base loss: 15984.32
[INFO 2017-06-30 12:20:14,108 main.py:52] epoch 12518, training loss: 5047.38, average training loss: 5236.40, base loss: 15984.23
[INFO 2017-06-30 12:20:17,246 main.py:52] epoch 12519, training loss: 5383.41, average training loss: 5236.18, base loss: 15984.33
[INFO 2017-06-30 12:20:20,374 main.py:52] epoch 12520, training loss: 5290.65, average training loss: 5236.46, base loss: 15984.52
[INFO 2017-06-30 12:20:23,485 main.py:52] epoch 12521, training loss: 5033.97, average training loss: 5236.34, base loss: 15984.60
[INFO 2017-06-30 12:20:26,618 main.py:52] epoch 12522, training loss: 5189.68, average training loss: 5236.51, base loss: 15984.57
[INFO 2017-06-30 12:20:29,797 main.py:52] epoch 12523, training loss: 5419.22, average training loss: 5236.90, base loss: 15984.85
[INFO 2017-06-30 12:20:32,966 main.py:52] epoch 12524, training loss: 5122.28, average training loss: 5236.81, base loss: 15984.39
[INFO 2017-06-30 12:20:36,104 main.py:52] epoch 12525, training loss: 5353.46, average training loss: 5236.61, base loss: 15984.42
[INFO 2017-06-30 12:20:39,223 main.py:52] epoch 12526, training loss: 5288.19, average training loss: 5236.49, base loss: 15984.32
[INFO 2017-06-30 12:20:42,372 main.py:52] epoch 12527, training loss: 5314.35, average training loss: 5236.57, base loss: 15984.11
[INFO 2017-06-30 12:20:45,507 main.py:52] epoch 12528, training loss: 5568.79, average training loss: 5236.71, base loss: 15984.37
[INFO 2017-06-30 12:20:48,607 main.py:52] epoch 12529, training loss: 5383.80, average training loss: 5236.57, base loss: 15984.60
[INFO 2017-06-30 12:20:51,720 main.py:52] epoch 12530, training loss: 5026.47, average training loss: 5236.36, base loss: 15984.28
[INFO 2017-06-30 12:20:54,848 main.py:52] epoch 12531, training loss: 5481.34, average training loss: 5236.99, base loss: 15984.47
[INFO 2017-06-30 12:20:57,981 main.py:52] epoch 12532, training loss: 5143.96, average training loss: 5236.54, base loss: 15984.55
[INFO 2017-06-30 12:21:01,129 main.py:52] epoch 12533, training loss: 4832.71, average training loss: 5236.11, base loss: 15984.30
[INFO 2017-06-30 12:21:04,306 main.py:52] epoch 12534, training loss: 5155.40, average training loss: 5236.13, base loss: 15984.40
[INFO 2017-06-30 12:21:07,446 main.py:52] epoch 12535, training loss: 5266.44, average training loss: 5236.18, base loss: 15984.58
[INFO 2017-06-30 12:21:10,606 main.py:52] epoch 12536, training loss: 5325.86, average training loss: 5236.28, base loss: 15984.53
[INFO 2017-06-30 12:21:13,768 main.py:52] epoch 12537, training loss: 4997.41, average training loss: 5235.99, base loss: 15984.12
[INFO 2017-06-30 12:21:16,909 main.py:52] epoch 12538, training loss: 5078.31, average training loss: 5235.86, base loss: 15984.08
[INFO 2017-06-30 12:21:20,065 main.py:52] epoch 12539, training loss: 5421.89, average training loss: 5236.00, base loss: 15983.98
[INFO 2017-06-30 12:21:23,210 main.py:52] epoch 12540, training loss: 5148.80, average training loss: 5236.17, base loss: 15983.94
[INFO 2017-06-30 12:21:26,340 main.py:52] epoch 12541, training loss: 5440.39, average training loss: 5236.06, base loss: 15983.97
[INFO 2017-06-30 12:21:29,497 main.py:52] epoch 12542, training loss: 5194.40, average training loss: 5235.81, base loss: 15983.90
[INFO 2017-06-30 12:21:32,597 main.py:52] epoch 12543, training loss: 5137.71, average training loss: 5235.55, base loss: 15983.76
[INFO 2017-06-30 12:21:35,726 main.py:52] epoch 12544, training loss: 5535.00, average training loss: 5235.81, base loss: 15984.11
[INFO 2017-06-30 12:21:38,851 main.py:52] epoch 12545, training loss: 5236.19, average training loss: 5236.12, base loss: 15984.07
[INFO 2017-06-30 12:21:41,991 main.py:52] epoch 12546, training loss: 5049.51, average training loss: 5236.00, base loss: 15983.82
[INFO 2017-06-30 12:21:45,148 main.py:52] epoch 12547, training loss: 5153.01, average training loss: 5235.79, base loss: 15983.92
[INFO 2017-06-30 12:21:48,258 main.py:52] epoch 12548, training loss: 5038.69, average training loss: 5235.45, base loss: 15983.85
[INFO 2017-06-30 12:21:51,359 main.py:52] epoch 12549, training loss: 5013.55, average training loss: 5235.07, base loss: 15983.77
[INFO 2017-06-30 12:21:54,480 main.py:52] epoch 12550, training loss: 5182.78, average training loss: 5234.99, base loss: 15983.50
[INFO 2017-06-30 12:21:57,617 main.py:52] epoch 12551, training loss: 4830.65, average training loss: 5234.73, base loss: 15983.34
[INFO 2017-06-30 12:22:00,760 main.py:52] epoch 12552, training loss: 4760.59, average training loss: 5234.24, base loss: 15983.08
[INFO 2017-06-30 12:22:03,894 main.py:52] epoch 12553, training loss: 5201.52, average training loss: 5233.97, base loss: 15982.99
[INFO 2017-06-30 12:22:07,081 main.py:52] epoch 12554, training loss: 5273.93, average training loss: 5233.63, base loss: 15983.01
[INFO 2017-06-30 12:22:10,220 main.py:52] epoch 12555, training loss: 5380.27, average training loss: 5233.96, base loss: 15983.03
[INFO 2017-06-30 12:22:13,383 main.py:52] epoch 12556, training loss: 5339.84, average training loss: 5234.22, base loss: 15983.33
[INFO 2017-06-30 12:22:16,482 main.py:52] epoch 12557, training loss: 5267.40, average training loss: 5234.30, base loss: 15983.44
[INFO 2017-06-30 12:22:19,603 main.py:52] epoch 12558, training loss: 5433.88, average training loss: 5234.30, base loss: 15983.48
[INFO 2017-06-30 12:22:22,747 main.py:52] epoch 12559, training loss: 4715.53, average training loss: 5233.68, base loss: 15983.15
[INFO 2017-06-30 12:22:25,893 main.py:52] epoch 12560, training loss: 5164.96, average training loss: 5233.51, base loss: 15983.41
[INFO 2017-06-30 12:22:29,035 main.py:52] epoch 12561, training loss: 5195.90, average training loss: 5233.33, base loss: 15983.35
[INFO 2017-06-30 12:22:32,161 main.py:52] epoch 12562, training loss: 5159.74, average training loss: 5233.40, base loss: 15983.45
[INFO 2017-06-30 12:22:35,263 main.py:52] epoch 12563, training loss: 5353.86, average training loss: 5233.09, base loss: 15983.57
[INFO 2017-06-30 12:22:38,475 main.py:52] epoch 12564, training loss: 5135.08, average training loss: 5232.71, base loss: 15983.58
[INFO 2017-06-30 12:22:41,592 main.py:52] epoch 12565, training loss: 5262.49, average training loss: 5232.58, base loss: 15983.80
[INFO 2017-06-30 12:22:44,767 main.py:52] epoch 12566, training loss: 5482.00, average training loss: 5233.09, base loss: 15984.06
[INFO 2017-06-30 12:22:47,918 main.py:52] epoch 12567, training loss: 4854.02, average training loss: 5232.56, base loss: 15983.67
[INFO 2017-06-30 12:22:51,022 main.py:52] epoch 12568, training loss: 5337.35, average training loss: 5232.89, base loss: 15983.64
[INFO 2017-06-30 12:22:54,154 main.py:52] epoch 12569, training loss: 5217.05, average training loss: 5232.65, base loss: 15983.66
[INFO 2017-06-30 12:22:57,283 main.py:52] epoch 12570, training loss: 5136.18, average training loss: 5232.19, base loss: 15983.57
[INFO 2017-06-30 12:23:00,441 main.py:52] epoch 12571, training loss: 5131.95, average training loss: 5232.06, base loss: 15983.52
[INFO 2017-06-30 12:23:03,602 main.py:52] epoch 12572, training loss: 5073.68, average training loss: 5231.96, base loss: 15983.39
[INFO 2017-06-30 12:23:06,788 main.py:52] epoch 12573, training loss: 5125.85, average training loss: 5231.90, base loss: 15983.54
[INFO 2017-06-30 12:23:09,978 main.py:52] epoch 12574, training loss: 5255.51, average training loss: 5231.96, base loss: 15983.55
[INFO 2017-06-30 12:23:13,121 main.py:52] epoch 12575, training loss: 5225.95, average training loss: 5231.87, base loss: 15983.67
[INFO 2017-06-30 12:23:16,327 main.py:52] epoch 12576, training loss: 5297.94, average training loss: 5231.74, base loss: 15983.77
[INFO 2017-06-30 12:23:19,480 main.py:52] epoch 12577, training loss: 5440.45, average training loss: 5232.06, base loss: 15984.04
[INFO 2017-06-30 12:23:22,606 main.py:52] epoch 12578, training loss: 5185.93, average training loss: 5231.84, base loss: 15983.82
[INFO 2017-06-30 12:23:25,723 main.py:52] epoch 12579, training loss: 5319.88, average training loss: 5231.65, base loss: 15983.69
[INFO 2017-06-30 12:23:28,892 main.py:52] epoch 12580, training loss: 4923.14, average training loss: 5231.24, base loss: 15983.52
[INFO 2017-06-30 12:23:32,023 main.py:52] epoch 12581, training loss: 5553.71, average training loss: 5231.44, base loss: 15983.43
[INFO 2017-06-30 12:23:35,123 main.py:52] epoch 12582, training loss: 5204.88, average training loss: 5231.63, base loss: 15983.38
[INFO 2017-06-30 12:23:38,267 main.py:52] epoch 12583, training loss: 5242.81, average training loss: 5231.94, base loss: 15983.45
[INFO 2017-06-30 12:23:41,389 main.py:52] epoch 12584, training loss: 4806.68, average training loss: 5231.33, base loss: 15983.15
[INFO 2017-06-30 12:23:44,522 main.py:52] epoch 12585, training loss: 4904.34, average training loss: 5231.10, base loss: 15983.19
[INFO 2017-06-30 12:23:47,720 main.py:52] epoch 12586, training loss: 5349.55, average training loss: 5231.17, base loss: 15983.23
[INFO 2017-06-30 12:23:50,831 main.py:52] epoch 12587, training loss: 5336.89, average training loss: 5231.11, base loss: 15983.24
[INFO 2017-06-30 12:23:53,962 main.py:52] epoch 12588, training loss: 5108.61, average training loss: 5230.91, base loss: 15983.23
[INFO 2017-06-30 12:23:57,094 main.py:52] epoch 12589, training loss: 4953.69, average training loss: 5230.74, base loss: 15982.84
[INFO 2017-06-30 12:24:00,220 main.py:52] epoch 12590, training loss: 5360.37, average training loss: 5230.92, base loss: 15982.95
[INFO 2017-06-30 12:24:03,356 main.py:52] epoch 12591, training loss: 5145.47, average training loss: 5230.66, base loss: 15982.99
[INFO 2017-06-30 12:24:06,480 main.py:52] epoch 12592, training loss: 5601.44, average training loss: 5231.02, base loss: 15983.30
[INFO 2017-06-30 12:24:09,617 main.py:52] epoch 12593, training loss: 5107.63, average training loss: 5230.78, base loss: 15983.26
[INFO 2017-06-30 12:24:12,751 main.py:52] epoch 12594, training loss: 5460.30, average training loss: 5231.22, base loss: 15983.41
[INFO 2017-06-30 12:24:15,862 main.py:52] epoch 12595, training loss: 5322.32, average training loss: 5231.21, base loss: 15983.69
[INFO 2017-06-30 12:24:18,991 main.py:52] epoch 12596, training loss: 5051.91, average training loss: 5231.18, base loss: 15983.38
[INFO 2017-06-30 12:24:22,139 main.py:52] epoch 12597, training loss: 5189.46, average training loss: 5231.35, base loss: 15983.35
[INFO 2017-06-30 12:24:25,349 main.py:52] epoch 12598, training loss: 5247.33, average training loss: 5231.28, base loss: 15983.33
[INFO 2017-06-30 12:24:28,475 main.py:52] epoch 12599, training loss: 5537.83, average training loss: 5231.49, base loss: 15983.36
[INFO 2017-06-30 12:24:28,475 main.py:54] epoch 12599, testing
[INFO 2017-06-30 12:24:41,577 main.py:97] average testing loss: 5180.10, base loss: 15413.32
[INFO 2017-06-30 12:24:41,578 main.py:98] improve_loss: 10233.22, improve_percent: 0.66
[INFO 2017-06-30 12:24:41,579 main.py:66] current best improved percent: 0.69
[INFO 2017-06-30 12:24:44,770 main.py:52] epoch 12600, training loss: 5151.29, average training loss: 5231.23, base loss: 15983.21
[INFO 2017-06-30 12:24:47,866 main.py:52] epoch 12601, training loss: 5571.03, average training loss: 5231.96, base loss: 15983.51
[INFO 2017-06-30 12:24:51,002 main.py:52] epoch 12602, training loss: 5578.54, average training loss: 5232.07, base loss: 15983.79
[INFO 2017-06-30 12:24:54,119 main.py:52] epoch 12603, training loss: 4962.17, average training loss: 5231.78, base loss: 15983.74
[INFO 2017-06-30 12:24:57,298 main.py:52] epoch 12604, training loss: 5333.66, average training loss: 5231.81, base loss: 15983.86
[INFO 2017-06-30 12:25:00,417 main.py:52] epoch 12605, training loss: 5473.06, average training loss: 5232.15, base loss: 15984.25
[INFO 2017-06-30 12:25:03,588 main.py:52] epoch 12606, training loss: 4941.98, average training loss: 5231.77, base loss: 15984.08
[INFO 2017-06-30 12:25:06,730 main.py:52] epoch 12607, training loss: 4970.10, average training loss: 5231.55, base loss: 15984.04
[INFO 2017-06-30 12:25:09,855 main.py:52] epoch 12608, training loss: 5020.61, average training loss: 5231.23, base loss: 15983.82
[INFO 2017-06-30 12:25:13,025 main.py:52] epoch 12609, training loss: 5183.29, average training loss: 5231.31, base loss: 15983.94
[INFO 2017-06-30 12:25:16,174 main.py:52] epoch 12610, training loss: 5454.41, average training loss: 5231.45, base loss: 15984.36
[INFO 2017-06-30 12:25:19,295 main.py:52] epoch 12611, training loss: 5351.89, average training loss: 5231.27, base loss: 15984.47
[INFO 2017-06-30 12:25:22,409 main.py:52] epoch 12612, training loss: 5092.35, average training loss: 5230.95, base loss: 15984.29
[INFO 2017-06-30 12:25:25,545 main.py:52] epoch 12613, training loss: 5297.13, average training loss: 5230.57, base loss: 15984.46
[INFO 2017-06-30 12:25:28,696 main.py:52] epoch 12614, training loss: 5415.93, average training loss: 5231.02, base loss: 15984.83
[INFO 2017-06-30 12:25:31,893 main.py:52] epoch 12615, training loss: 5299.73, average training loss: 5230.79, base loss: 15985.09
[INFO 2017-06-30 12:25:35,026 main.py:52] epoch 12616, training loss: 5230.93, average training loss: 5230.85, base loss: 15985.02
[INFO 2017-06-30 12:25:38,159 main.py:52] epoch 12617, training loss: 5498.75, average training loss: 5231.22, base loss: 15985.42
[INFO 2017-06-30 12:25:41,329 main.py:52] epoch 12618, training loss: 5231.33, average training loss: 5231.66, base loss: 15985.63
[INFO 2017-06-30 12:25:44,479 main.py:52] epoch 12619, training loss: 5293.33, average training loss: 5231.58, base loss: 15985.80
[INFO 2017-06-30 12:25:47,613 main.py:52] epoch 12620, training loss: 5329.70, average training loss: 5231.81, base loss: 15985.55
[INFO 2017-06-30 12:25:50,784 main.py:52] epoch 12621, training loss: 4949.93, average training loss: 5231.27, base loss: 15985.13
[INFO 2017-06-30 12:25:53,940 main.py:52] epoch 12622, training loss: 5403.16, average training loss: 5230.89, base loss: 15985.36
[INFO 2017-06-30 12:25:57,057 main.py:52] epoch 12623, training loss: 5059.15, average training loss: 5230.42, base loss: 15985.26
[INFO 2017-06-30 12:26:00,247 main.py:52] epoch 12624, training loss: 5196.78, average training loss: 5230.56, base loss: 15985.03
[INFO 2017-06-30 12:26:03,397 main.py:52] epoch 12625, training loss: 5030.22, average training loss: 5230.29, base loss: 15984.86
[INFO 2017-06-30 12:26:06,521 main.py:52] epoch 12626, training loss: 5277.92, average training loss: 5230.48, base loss: 15984.90
[INFO 2017-06-30 12:26:09,683 main.py:52] epoch 12627, training loss: 5266.90, average training loss: 5230.56, base loss: 15984.70
[INFO 2017-06-30 12:26:12,817 main.py:52] epoch 12628, training loss: 5229.50, average training loss: 5230.57, base loss: 15984.58
[INFO 2017-06-30 12:26:15,972 main.py:52] epoch 12629, training loss: 5291.25, average training loss: 5230.70, base loss: 15984.58
[INFO 2017-06-30 12:26:19,118 main.py:52] epoch 12630, training loss: 4888.21, average training loss: 5230.50, base loss: 15984.29
[INFO 2017-06-30 12:26:22,229 main.py:52] epoch 12631, training loss: 5006.51, average training loss: 5230.27, base loss: 15984.20
[INFO 2017-06-30 12:26:25,340 main.py:52] epoch 12632, training loss: 5022.79, average training loss: 5229.81, base loss: 15984.14
[INFO 2017-06-30 12:26:28,472 main.py:52] epoch 12633, training loss: 5316.40, average training loss: 5229.83, base loss: 15984.15
[INFO 2017-06-30 12:26:31,610 main.py:52] epoch 12634, training loss: 5328.05, average training loss: 5230.12, base loss: 15984.40
[INFO 2017-06-30 12:26:34,744 main.py:52] epoch 12635, training loss: 4967.47, average training loss: 5229.94, base loss: 15984.45
[INFO 2017-06-30 12:26:37,875 main.py:52] epoch 12636, training loss: 5284.52, average training loss: 5230.27, base loss: 15984.67
[INFO 2017-06-30 12:26:41,034 main.py:52] epoch 12637, training loss: 5241.47, average training loss: 5230.09, base loss: 15984.73
[INFO 2017-06-30 12:26:44,184 main.py:52] epoch 12638, training loss: 4560.69, average training loss: 5229.21, base loss: 15984.36
[INFO 2017-06-30 12:26:47,292 main.py:52] epoch 12639, training loss: 5627.18, average training loss: 5229.32, base loss: 15984.60
[INFO 2017-06-30 12:26:50,440 main.py:52] epoch 12640, training loss: 5727.01, average training loss: 5229.75, base loss: 15985.07
[INFO 2017-06-30 12:26:53,584 main.py:52] epoch 12641, training loss: 5080.43, average training loss: 5229.61, base loss: 15985.03
[INFO 2017-06-30 12:26:56,707 main.py:52] epoch 12642, training loss: 5177.31, average training loss: 5229.49, base loss: 15985.20
[INFO 2017-06-30 12:26:59,837 main.py:52] epoch 12643, training loss: 5040.82, average training loss: 5229.43, base loss: 15985.08
[INFO 2017-06-30 12:27:03,033 main.py:52] epoch 12644, training loss: 5409.08, average training loss: 5229.47, base loss: 15985.26
[INFO 2017-06-30 12:27:06,207 main.py:52] epoch 12645, training loss: 4728.29, average training loss: 5228.90, base loss: 15984.91
[INFO 2017-06-30 12:27:09,353 main.py:52] epoch 12646, training loss: 4984.53, average training loss: 5228.69, base loss: 15984.79
[INFO 2017-06-30 12:27:12,491 main.py:52] epoch 12647, training loss: 5422.89, average training loss: 5228.43, base loss: 15984.81
[INFO 2017-06-30 12:27:15,618 main.py:52] epoch 12648, training loss: 5043.55, average training loss: 5228.26, base loss: 15984.94
[INFO 2017-06-30 12:27:18,800 main.py:52] epoch 12649, training loss: 5317.92, average training loss: 5228.10, base loss: 15985.15
[INFO 2017-06-30 12:27:21,942 main.py:52] epoch 12650, training loss: 5434.45, average training loss: 5228.25, base loss: 15985.50
[INFO 2017-06-30 12:27:25,093 main.py:52] epoch 12651, training loss: 5233.93, average training loss: 5227.81, base loss: 15985.47
[INFO 2017-06-30 12:27:28,241 main.py:52] epoch 12652, training loss: 5305.71, average training loss: 5227.72, base loss: 15985.40
[INFO 2017-06-30 12:27:31,371 main.py:52] epoch 12653, training loss: 5237.23, average training loss: 5227.89, base loss: 15985.35
[INFO 2017-06-30 12:27:34,532 main.py:52] epoch 12654, training loss: 5531.87, average training loss: 5228.31, base loss: 15985.59
[INFO 2017-06-30 12:27:37,694 main.py:52] epoch 12655, training loss: 4898.28, average training loss: 5228.25, base loss: 15985.59
[INFO 2017-06-30 12:27:40,875 main.py:52] epoch 12656, training loss: 5468.58, average training loss: 5228.33, base loss: 15985.86
[INFO 2017-06-30 12:27:44,036 main.py:52] epoch 12657, training loss: 5378.50, average training loss: 5228.91, base loss: 15985.91
[INFO 2017-06-30 12:27:47,174 main.py:52] epoch 12658, training loss: 5414.15, average training loss: 5229.10, base loss: 15986.14
[INFO 2017-06-30 12:27:50,317 main.py:52] epoch 12659, training loss: 5092.85, average training loss: 5229.19, base loss: 15985.75
[INFO 2017-06-30 12:27:53,440 main.py:52] epoch 12660, training loss: 5332.39, average training loss: 5229.47, base loss: 15985.70
[INFO 2017-06-30 12:27:56,542 main.py:52] epoch 12661, training loss: 5330.26, average training loss: 5229.32, base loss: 15985.62
[INFO 2017-06-30 12:27:59,701 main.py:52] epoch 12662, training loss: 5142.70, average training loss: 5229.01, base loss: 15985.75
[INFO 2017-06-30 12:28:02,833 main.py:52] epoch 12663, training loss: 5226.53, average training loss: 5228.42, base loss: 15986.08
[INFO 2017-06-30 12:28:05,940 main.py:52] epoch 12664, training loss: 5284.21, average training loss: 5228.78, base loss: 15986.25
[INFO 2017-06-30 12:28:09,090 main.py:52] epoch 12665, training loss: 5501.55, average training loss: 5228.90, base loss: 15986.59
[INFO 2017-06-30 12:28:12,205 main.py:52] epoch 12666, training loss: 5196.58, average training loss: 5229.08, base loss: 15986.65
[INFO 2017-06-30 12:28:15,350 main.py:52] epoch 12667, training loss: 4824.99, average training loss: 5228.77, base loss: 15986.64
[INFO 2017-06-30 12:28:18,473 main.py:52] epoch 12668, training loss: 5313.49, average training loss: 5228.62, base loss: 15986.69
[INFO 2017-06-30 12:28:21,619 main.py:52] epoch 12669, training loss: 5049.60, average training loss: 5228.48, base loss: 15986.46
[INFO 2017-06-30 12:28:24,733 main.py:52] epoch 12670, training loss: 5397.41, average training loss: 5228.44, base loss: 15986.61
[INFO 2017-06-30 12:28:27,871 main.py:52] epoch 12671, training loss: 4896.23, average training loss: 5228.17, base loss: 15986.53
[INFO 2017-06-30 12:28:31,033 main.py:52] epoch 12672, training loss: 5349.43, average training loss: 5228.29, base loss: 15986.72
[INFO 2017-06-30 12:28:34,183 main.py:52] epoch 12673, training loss: 5371.39, average training loss: 5228.54, base loss: 15986.81
[INFO 2017-06-30 12:28:37,341 main.py:52] epoch 12674, training loss: 5137.45, average training loss: 5228.52, base loss: 15986.58
[INFO 2017-06-30 12:28:40,514 main.py:52] epoch 12675, training loss: 5202.29, average training loss: 5228.54, base loss: 15986.45
[INFO 2017-06-30 12:28:43,687 main.py:52] epoch 12676, training loss: 5108.10, average training loss: 5228.43, base loss: 15986.45
[INFO 2017-06-30 12:28:46,850 main.py:52] epoch 12677, training loss: 5201.77, average training loss: 5228.62, base loss: 15986.58
[INFO 2017-06-30 12:28:50,018 main.py:52] epoch 12678, training loss: 5224.73, average training loss: 5228.50, base loss: 15986.64
[INFO 2017-06-30 12:28:53,172 main.py:52] epoch 12679, training loss: 5232.03, average training loss: 5228.82, base loss: 15986.68
[INFO 2017-06-30 12:28:56,369 main.py:52] epoch 12680, training loss: 5218.00, average training loss: 5228.69, base loss: 15986.62
[INFO 2017-06-30 12:28:59,501 main.py:52] epoch 12681, training loss: 4955.03, average training loss: 5228.28, base loss: 15986.58
[INFO 2017-06-30 12:29:02,660 main.py:52] epoch 12682, training loss: 5174.47, average training loss: 5228.27, base loss: 15986.62
[INFO 2017-06-30 12:29:05,770 main.py:52] epoch 12683, training loss: 5272.46, average training loss: 5227.93, base loss: 15986.46
[INFO 2017-06-30 12:29:08,912 main.py:52] epoch 12684, training loss: 5134.31, average training loss: 5228.06, base loss: 15986.33
[INFO 2017-06-30 12:29:12,033 main.py:52] epoch 12685, training loss: 5578.92, average training loss: 5228.26, base loss: 15986.49
[INFO 2017-06-30 12:29:15,154 main.py:52] epoch 12686, training loss: 5383.13, average training loss: 5228.38, base loss: 15986.67
[INFO 2017-06-30 12:29:18,320 main.py:52] epoch 12687, training loss: 5314.42, average training loss: 5228.51, base loss: 15986.78
[INFO 2017-06-30 12:29:21,425 main.py:52] epoch 12688, training loss: 4900.59, average training loss: 5228.54, base loss: 15986.54
[INFO 2017-06-30 12:29:24,566 main.py:52] epoch 12689, training loss: 5147.99, average training loss: 5228.38, base loss: 15986.56
[INFO 2017-06-30 12:29:27,709 main.py:52] epoch 12690, training loss: 5238.28, average training loss: 5228.46, base loss: 15986.72
[INFO 2017-06-30 12:29:30,850 main.py:52] epoch 12691, training loss: 4901.31, average training loss: 5227.76, base loss: 15986.43
[INFO 2017-06-30 12:29:33,977 main.py:52] epoch 12692, training loss: 5317.54, average training loss: 5227.09, base loss: 15986.51
[INFO 2017-06-30 12:29:37,128 main.py:52] epoch 12693, training loss: 5178.10, average training loss: 5226.79, base loss: 15986.61
[INFO 2017-06-30 12:29:40,272 main.py:52] epoch 12694, training loss: 5290.21, average training loss: 5226.57, base loss: 15986.68
[INFO 2017-06-30 12:29:43,423 main.py:52] epoch 12695, training loss: 5246.65, average training loss: 5226.49, base loss: 15986.89
[INFO 2017-06-30 12:29:46,552 main.py:52] epoch 12696, training loss: 4997.72, average training loss: 5226.23, base loss: 15986.93
[INFO 2017-06-30 12:29:49,689 main.py:52] epoch 12697, training loss: 5075.40, average training loss: 5226.20, base loss: 15986.79
[INFO 2017-06-30 12:29:52,798 main.py:52] epoch 12698, training loss: 5145.15, average training loss: 5226.24, base loss: 15986.95
[INFO 2017-06-30 12:29:55,968 main.py:52] epoch 12699, training loss: 5215.12, average training loss: 5226.25, base loss: 15986.61
[INFO 2017-06-30 12:29:55,968 main.py:54] epoch 12699, testing
[INFO 2017-06-30 12:30:08,983 main.py:97] average testing loss: 5221.32, base loss: 16373.33
[INFO 2017-06-30 12:30:08,983 main.py:98] improve_loss: 11152.01, improve_percent: 0.68
[INFO 2017-06-30 12:30:08,985 main.py:66] current best improved percent: 0.69
[INFO 2017-06-30 12:30:12,119 main.py:52] epoch 12700, training loss: 5170.00, average training loss: 5226.51, base loss: 15986.60
[INFO 2017-06-30 12:30:15,259 main.py:52] epoch 12701, training loss: 5177.94, average training loss: 5226.37, base loss: 15986.49
[INFO 2017-06-30 12:30:18,376 main.py:52] epoch 12702, training loss: 5157.65, average training loss: 5226.11, base loss: 15986.58
[INFO 2017-06-30 12:30:21,533 main.py:52] epoch 12703, training loss: 5312.43, average training loss: 5225.79, base loss: 15986.50
[INFO 2017-06-30 12:30:24,682 main.py:52] epoch 12704, training loss: 5264.74, average training loss: 5225.93, base loss: 15986.72
[INFO 2017-06-30 12:30:27,777 main.py:52] epoch 12705, training loss: 5374.73, average training loss: 5225.93, base loss: 15986.73
[INFO 2017-06-30 12:30:30,915 main.py:52] epoch 12706, training loss: 5215.35, average training loss: 5225.80, base loss: 15986.65
