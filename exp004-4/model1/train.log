[INFO 2017-06-29 19:53:02,676 main.py:175] Namespace(batch_size=32, display=False, flow_dir='flow', flow_video_dir='flow-video', flow_video_fps=1, image_size=64, init_model_path='', learning_rate=0.001, method='unsupervised', motion_range=4, num_channel=3, num_inputs=2, save_dir='./model', test=False, test_dir='/home/yi/Downloads/mpii-test-64', test_epoch=10, test_interval=100, test_video=False, train=True, train_dir='/home/yi/Downloads/mpii-test-64', train_epoch=100000)
[INFO 2017-06-29 19:53:07,068 main.py:57] epoch 0, training loss: 39605.25, average training loss: 39605.25, base loss: 16883.70
[INFO 2017-06-29 19:53:09,292 main.py:57] epoch 1, training loss: 35286.16, average training loss: 37445.71, base loss: 17708.82
[INFO 2017-06-29 19:53:11,473 main.py:57] epoch 2, training loss: 30770.18, average training loss: 35220.53, base loss: 17687.40
[INFO 2017-06-29 19:53:13,706 main.py:57] epoch 3, training loss: 26618.72, average training loss: 33070.08, base loss: 17034.11
[INFO 2017-06-29 19:53:15,897 main.py:57] epoch 4, training loss: 24385.12, average training loss: 31333.09, base loss: 16770.10
[INFO 2017-06-29 19:53:18,153 main.py:57] epoch 5, training loss: 23508.23, average training loss: 30028.94, base loss: 16725.04
[INFO 2017-06-29 19:53:20,466 main.py:57] epoch 6, training loss: 20817.18, average training loss: 28712.98, base loss: 16544.84
[INFO 2017-06-29 19:53:23,048 main.py:57] epoch 7, training loss: 18864.02, average training loss: 27481.86, base loss: 16330.60
[INFO 2017-06-29 19:53:25,892 main.py:57] epoch 8, training loss: 18048.69, average training loss: 26433.73, base loss: 16248.95
[INFO 2017-06-29 19:53:28,770 main.py:57] epoch 9, training loss: 17706.80, average training loss: 25561.03, base loss: 16202.37
[INFO 2017-06-29 19:53:31,619 main.py:57] epoch 10, training loss: 15297.32, average training loss: 24627.97, base loss: 16098.98
[INFO 2017-06-29 19:53:34,458 main.py:57] epoch 11, training loss: 14428.71, average training loss: 23778.03, base loss: 15998.17
[INFO 2017-06-29 19:53:37,366 main.py:57] epoch 12, training loss: 14697.82, average training loss: 23079.55, base loss: 16018.64
[INFO 2017-06-29 19:53:40,212 main.py:57] epoch 13, training loss: 13482.52, average training loss: 22394.05, base loss: 15968.02
[INFO 2017-06-29 19:53:43,090 main.py:57] epoch 14, training loss: 15202.64, average training loss: 21914.62, base loss: 16106.19
[INFO 2017-06-29 19:53:45,960 main.py:57] epoch 15, training loss: 13454.31, average training loss: 21385.85, base loss: 16068.62
[INFO 2017-06-29 19:53:48,888 main.py:57] epoch 16, training loss: 13226.57, average training loss: 20905.90, base loss: 16061.28
[INFO 2017-06-29 19:53:51,740 main.py:57] epoch 17, training loss: 12936.09, average training loss: 20463.13, base loss: 16062.58
[INFO 2017-06-29 19:53:54,619 main.py:57] epoch 18, training loss: 11756.77, average training loss: 20004.90, base loss: 16037.74
[INFO 2017-06-29 19:53:57,472 main.py:57] epoch 19, training loss: 13081.58, average training loss: 19658.73, base loss: 16106.09
[INFO 2017-06-29 19:54:00,319 main.py:57] epoch 20, training loss: 12654.50, average training loss: 19325.20, base loss: 16128.82
[INFO 2017-06-29 19:54:03,218 main.py:57] epoch 21, training loss: 11978.70, average training loss: 18991.27, base loss: 16132.69
[INFO 2017-06-29 19:54:06,117 main.py:57] epoch 22, training loss: 10582.17, average training loss: 18625.65, base loss: 16048.75
[INFO 2017-06-29 19:54:08,986 main.py:57] epoch 23, training loss: 10930.10, average training loss: 18305.01, base loss: 16034.97
[INFO 2017-06-29 19:54:11,861 main.py:57] epoch 24, training loss: 13096.56, average training loss: 18096.67, base loss: 16143.59
[INFO 2017-06-29 19:54:14,710 main.py:57] epoch 25, training loss: 11007.96, average training loss: 17824.03, base loss: 16120.41
[INFO 2017-06-29 19:54:17,590 main.py:57] epoch 26, training loss: 11526.83, average training loss: 17590.80, base loss: 16126.94
[INFO 2017-06-29 19:54:20,502 main.py:57] epoch 27, training loss: 11109.75, average training loss: 17359.33, base loss: 16137.74
[INFO 2017-06-29 19:54:23,345 main.py:57] epoch 28, training loss: 10919.46, average training loss: 17137.27, base loss: 16131.34
[INFO 2017-06-29 19:54:26,208 main.py:57] epoch 29, training loss: 11224.92, average training loss: 16940.19, base loss: 16147.32
[INFO 2017-06-29 19:54:29,083 main.py:57] epoch 30, training loss: 9542.54, average training loss: 16701.55, base loss: 16093.20
[INFO 2017-06-29 19:54:31,991 main.py:57] epoch 31, training loss: 10746.72, average training loss: 16515.46, base loss: 16096.25
[INFO 2017-06-29 19:54:34,844 main.py:57] epoch 32, training loss: 11879.00, average training loss: 16374.97, base loss: 16137.81
[INFO 2017-06-29 19:54:37,702 main.py:57] epoch 33, training loss: 12030.06, average training loss: 16247.17, base loss: 16187.68
[INFO 2017-06-29 19:54:40,573 main.py:57] epoch 34, training loss: 10630.61, average training loss: 16086.70, base loss: 16187.30
[INFO 2017-06-29 19:54:43,480 main.py:57] epoch 35, training loss: 10783.26, average training loss: 15939.38, base loss: 16172.75
[INFO 2017-06-29 19:54:46,344 main.py:57] epoch 36, training loss: 11173.54, average training loss: 15810.58, base loss: 16176.11
[INFO 2017-06-29 19:54:49,165 main.py:57] epoch 37, training loss: 9864.19, average training loss: 15654.09, base loss: 16138.68
[INFO 2017-06-29 19:54:52,046 main.py:57] epoch 38, training loss: 10820.95, average training loss: 15530.17, base loss: 16137.29
[INFO 2017-06-29 19:54:54,930 main.py:57] epoch 39, training loss: 10897.71, average training loss: 15414.36, base loss: 16143.72
[INFO 2017-06-29 19:54:57,748 main.py:57] epoch 40, training loss: 12000.09, average training loss: 15331.08, base loss: 16185.31
[INFO 2017-06-29 19:55:00,657 main.py:57] epoch 41, training loss: 11506.82, average training loss: 15240.03, base loss: 16206.92
[INFO 2017-06-29 19:55:03,534 main.py:57] epoch 42, training loss: 10032.09, average training loss: 15118.91, base loss: 16191.29
[INFO 2017-06-29 19:55:06,404 main.py:57] epoch 43, training loss: 10518.97, average training loss: 15014.37, base loss: 16185.66
[INFO 2017-06-29 19:55:09,270 main.py:57] epoch 44, training loss: 9834.37, average training loss: 14899.26, base loss: 16159.17
[INFO 2017-06-29 19:55:12,126 main.py:57] epoch 45, training loss: 10777.41, average training loss: 14809.65, base loss: 16154.29
[INFO 2017-06-29 19:55:14,987 main.py:57] epoch 46, training loss: 9761.44, average training loss: 14702.24, base loss: 16122.34
[INFO 2017-06-29 19:55:17,875 main.py:57] epoch 47, training loss: 11341.81, average training loss: 14632.23, base loss: 16146.58
[INFO 2017-06-29 19:55:20,767 main.py:57] epoch 48, training loss: 10564.88, average training loss: 14549.23, base loss: 16148.40
[INFO 2017-06-29 19:55:23,629 main.py:57] epoch 49, training loss: 10312.23, average training loss: 14464.49, base loss: 16139.68
[INFO 2017-06-29 19:55:26,464 main.py:57] epoch 50, training loss: 11620.97, average training loss: 14408.73, base loss: 16164.60
[INFO 2017-06-29 19:55:29,349 main.py:57] epoch 51, training loss: 9224.45, average training loss: 14309.03, base loss: 16132.88
[INFO 2017-06-29 19:55:32,159 main.py:57] epoch 52, training loss: 9828.76, average training loss: 14224.50, base loss: 16116.15
[INFO 2017-06-29 19:55:34,988 main.py:57] epoch 53, training loss: 9591.81, average training loss: 14138.71, base loss: 16096.34
[INFO 2017-06-29 19:55:37,906 main.py:57] epoch 54, training loss: 10305.07, average training loss: 14069.01, base loss: 16096.54
[INFO 2017-06-29 19:55:40,780 main.py:57] epoch 55, training loss: 11271.44, average training loss: 14019.05, base loss: 16116.31
[INFO 2017-06-29 19:55:43,693 main.py:57] epoch 56, training loss: 9055.54, average training loss: 13931.97, base loss: 16084.99
[INFO 2017-06-29 19:55:46,566 main.py:57] epoch 57, training loss: 9414.45, average training loss: 13854.08, base loss: 16071.20
[INFO 2017-06-29 19:55:49,506 main.py:57] epoch 58, training loss: 9040.34, average training loss: 13772.49, base loss: 16038.14
[INFO 2017-06-29 19:55:52,362 main.py:57] epoch 59, training loss: 10224.61, average training loss: 13713.36, base loss: 16037.84
[INFO 2017-06-29 19:55:55,244 main.py:57] epoch 60, training loss: 10584.78, average training loss: 13662.07, base loss: 16045.94
[INFO 2017-06-29 19:55:58,096 main.py:57] epoch 61, training loss: 10466.87, average training loss: 13610.54, base loss: 16055.84
[INFO 2017-06-29 19:56:00,979 main.py:57] epoch 62, training loss: 10035.17, average training loss: 13553.79, base loss: 16041.98
[INFO 2017-06-29 19:56:03,875 main.py:57] epoch 63, training loss: 8873.59, average training loss: 13480.66, base loss: 16016.21
[INFO 2017-06-29 19:56:06,737 main.py:57] epoch 64, training loss: 9082.94, average training loss: 13413.00, base loss: 15989.97
[INFO 2017-06-29 19:56:09,605 main.py:57] epoch 65, training loss: 10315.39, average training loss: 13366.07, base loss: 15999.21
[INFO 2017-06-29 19:56:12,448 main.py:57] epoch 66, training loss: 10290.75, average training loss: 13320.17, base loss: 16000.34
[INFO 2017-06-29 19:56:15,330 main.py:57] epoch 67, training loss: 9406.21, average training loss: 13262.61, base loss: 15977.84
[INFO 2017-06-29 19:56:18,203 main.py:57] epoch 68, training loss: 9710.75, average training loss: 13211.13, base loss: 15973.67
[INFO 2017-06-29 19:56:21,058 main.py:57] epoch 69, training loss: 9736.08, average training loss: 13161.49, base loss: 15962.90
[INFO 2017-06-29 19:56:23,915 main.py:57] epoch 70, training loss: 9910.21, average training loss: 13115.70, base loss: 15961.64
[INFO 2017-06-29 19:56:26,739 main.py:57] epoch 71, training loss: 10121.40, average training loss: 13074.11, base loss: 15964.87
[INFO 2017-06-29 19:56:29,659 main.py:57] epoch 72, training loss: 9360.81, average training loss: 13023.24, base loss: 15957.19
[INFO 2017-06-29 19:56:32,510 main.py:57] epoch 73, training loss: 10913.94, average training loss: 12994.74, base loss: 15981.54
[INFO 2017-06-29 19:56:35,369 main.py:57] epoch 74, training loss: 9711.43, average training loss: 12950.96, base loss: 15978.16
[INFO 2017-06-29 19:56:38,292 main.py:57] epoch 75, training loss: 9558.43, average training loss: 12906.32, base loss: 15974.52
[INFO 2017-06-29 19:56:41,180 main.py:57] epoch 76, training loss: 10291.88, average training loss: 12872.37, base loss: 15980.21
[INFO 2017-06-29 19:56:44,018 main.py:57] epoch 77, training loss: 9672.84, average training loss: 12831.35, base loss: 15981.09
[INFO 2017-06-29 19:56:46,861 main.py:57] epoch 78, training loss: 9852.88, average training loss: 12793.65, base loss: 15980.43
[INFO 2017-06-29 19:56:49,714 main.py:57] epoch 79, training loss: 10184.59, average training loss: 12761.03, base loss: 15997.49
[INFO 2017-06-29 19:56:52,585 main.py:57] epoch 80, training loss: 9350.27, average training loss: 12718.93, base loss: 15989.89
[INFO 2017-06-29 19:56:55,446 main.py:57] epoch 81, training loss: 10709.86, average training loss: 12694.42, base loss: 16009.59
[INFO 2017-06-29 19:56:58,353 main.py:57] epoch 82, training loss: 9430.61, average training loss: 12655.10, base loss: 16011.11
[INFO 2017-06-29 19:57:01,231 main.py:57] epoch 83, training loss: 10713.51, average training loss: 12631.99, base loss: 16028.77
[INFO 2017-06-29 19:57:04,075 main.py:57] epoch 84, training loss: 9115.65, average training loss: 12590.62, base loss: 16019.41
[INFO 2017-06-29 19:57:06,937 main.py:57] epoch 85, training loss: 9697.87, average training loss: 12556.98, base loss: 16023.74
[INFO 2017-06-29 19:57:09,823 main.py:57] epoch 86, training loss: 9994.98, average training loss: 12527.53, base loss: 16032.79
[INFO 2017-06-29 19:57:12,700 main.py:57] epoch 87, training loss: 9769.61, average training loss: 12496.19, base loss: 16036.53
[INFO 2017-06-29 19:57:15,592 main.py:57] epoch 88, training loss: 10003.22, average training loss: 12468.18, base loss: 16043.97
[INFO 2017-06-29 19:57:18,462 main.py:57] epoch 89, training loss: 11752.90, average training loss: 12460.24, base loss: 16071.62
[INFO 2017-06-29 19:57:21,309 main.py:57] epoch 90, training loss: 9997.56, average training loss: 12433.17, base loss: 16082.40
[INFO 2017-06-29 19:57:24,219 main.py:57] epoch 91, training loss: 8813.47, average training loss: 12393.83, base loss: 16065.81
[INFO 2017-06-29 19:57:27,095 main.py:57] epoch 92, training loss: 8689.47, average training loss: 12354.00, base loss: 16049.99
[INFO 2017-06-29 19:57:29,969 main.py:57] epoch 93, training loss: 8979.09, average training loss: 12318.09, base loss: 16043.63
[INFO 2017-06-29 19:57:32,872 main.py:57] epoch 94, training loss: 9123.51, average training loss: 12284.47, base loss: 16036.04
[INFO 2017-06-29 19:57:35,748 main.py:57] epoch 95, training loss: 8860.17, average training loss: 12248.80, base loss: 16016.88
[INFO 2017-06-29 19:57:38,620 main.py:57] epoch 96, training loss: 8936.70, average training loss: 12214.65, base loss: 16012.24
[INFO 2017-06-29 19:57:41,461 main.py:57] epoch 97, training loss: 9101.15, average training loss: 12182.88, base loss: 16006.21
[INFO 2017-06-29 19:57:44,356 main.py:57] epoch 98, training loss: 8743.82, average training loss: 12148.14, base loss: 15997.73
[INFO 2017-06-29 19:57:47,235 main.py:57] epoch 99, training loss: 9187.97, average training loss: 12118.54, base loss: 15986.48
[INFO 2017-06-29 19:57:47,236 main.py:59] epoch 99, testing
[INFO 2017-06-29 19:57:59,440 main.py:104] average testing loss: 9367.28, base loss: 16070.05
[INFO 2017-06-29 19:57:59,440 main.py:105] improve_loss: 6702.77, improve_percent: 0.42
[INFO 2017-06-29 19:57:59,442 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 19:57:59,466 main.py:71] current best improved percent: 0.42
[INFO 2017-06-29 19:58:02,328 main.py:57] epoch 100, training loss: 8511.00, average training loss: 12082.82, base loss: 15967.72
[INFO 2017-06-29 19:58:05,217 main.py:57] epoch 101, training loss: 9282.01, average training loss: 12055.36, base loss: 15975.42
[INFO 2017-06-29 19:58:08,092 main.py:57] epoch 102, training loss: 10686.65, average training loss: 12042.08, base loss: 15994.40
[INFO 2017-06-29 19:58:10,991 main.py:57] epoch 103, training loss: 9571.44, average training loss: 12018.32, base loss: 16000.14
[INFO 2017-06-29 19:58:13,860 main.py:57] epoch 104, training loss: 9451.12, average training loss: 11993.87, base loss: 16009.01
[INFO 2017-06-29 19:58:16,723 main.py:57] epoch 105, training loss: 8229.21, average training loss: 11958.35, base loss: 15998.11
[INFO 2017-06-29 19:58:19,580 main.py:57] epoch 106, training loss: 8406.79, average training loss: 11925.16, base loss: 15984.69
[INFO 2017-06-29 19:58:22,472 main.py:57] epoch 107, training loss: 9025.37, average training loss: 11898.31, base loss: 15986.69
[INFO 2017-06-29 19:58:25,378 main.py:57] epoch 108, training loss: 9665.22, average training loss: 11877.82, base loss: 15998.39
[INFO 2017-06-29 19:58:28,240 main.py:57] epoch 109, training loss: 9507.68, average training loss: 11856.28, base loss: 16005.64
[INFO 2017-06-29 19:58:31,112 main.py:57] epoch 110, training loss: 9852.28, average training loss: 11838.22, base loss: 16011.27
[INFO 2017-06-29 19:58:34,037 main.py:57] epoch 111, training loss: 9610.38, average training loss: 11818.33, base loss: 16009.66
[INFO 2017-06-29 19:58:36,932 main.py:57] epoch 112, training loss: 9755.88, average training loss: 11800.08, base loss: 16016.35
[INFO 2017-06-29 19:58:39,799 main.py:57] epoch 113, training loss: 9304.14, average training loss: 11778.19, base loss: 16017.34
[INFO 2017-06-29 19:58:42,635 main.py:57] epoch 114, training loss: 8644.82, average training loss: 11750.94, base loss: 16010.25
[INFO 2017-06-29 19:58:45,525 main.py:57] epoch 115, training loss: 8958.38, average training loss: 11726.87, base loss: 16007.93
[INFO 2017-06-29 19:58:48,413 main.py:57] epoch 116, training loss: 10549.26, average training loss: 11716.80, base loss: 16036.34
[INFO 2017-06-29 19:58:51,234 main.py:57] epoch 117, training loss: 9005.63, average training loss: 11693.83, base loss: 16046.18
[INFO 2017-06-29 19:58:54,115 main.py:57] epoch 118, training loss: 10120.29, average training loss: 11680.60, base loss: 16057.13
[INFO 2017-06-29 19:58:57,014 main.py:57] epoch 119, training loss: 9454.63, average training loss: 11662.05, base loss: 16069.38
[INFO 2017-06-29 19:58:59,858 main.py:57] epoch 120, training loss: 9123.10, average training loss: 11641.07, base loss: 16066.68
[INFO 2017-06-29 19:59:02,739 main.py:57] epoch 121, training loss: 9642.26, average training loss: 11624.69, base loss: 16073.26
[INFO 2017-06-29 19:59:05,597 main.py:57] epoch 122, training loss: 9437.78, average training loss: 11606.91, base loss: 16086.79
[INFO 2017-06-29 19:59:08,421 main.py:57] epoch 123, training loss: 9423.64, average training loss: 11589.30, base loss: 16098.02
[INFO 2017-06-29 19:59:11,299 main.py:57] epoch 124, training loss: 8809.65, average training loss: 11567.06, base loss: 16099.62
[INFO 2017-06-29 19:59:14,156 main.py:57] epoch 125, training loss: 8681.91, average training loss: 11544.16, base loss: 16091.18
[INFO 2017-06-29 19:59:17,041 main.py:57] epoch 126, training loss: 8507.22, average training loss: 11520.25, base loss: 16085.97
[INFO 2017-06-29 19:59:19,920 main.py:57] epoch 127, training loss: 9827.66, average training loss: 11507.03, base loss: 16094.83
[INFO 2017-06-29 19:59:22,774 main.py:57] epoch 128, training loss: 8910.49, average training loss: 11486.90, base loss: 16101.73
[INFO 2017-06-29 19:59:25,646 main.py:57] epoch 129, training loss: 8066.11, average training loss: 11460.59, base loss: 16084.79
[INFO 2017-06-29 19:59:28,529 main.py:57] epoch 130, training loss: 8627.67, average training loss: 11438.96, base loss: 16084.21
[INFO 2017-06-29 19:59:31,373 main.py:57] epoch 131, training loss: 8575.98, average training loss: 11417.27, base loss: 16079.67
[INFO 2017-06-29 19:59:34,277 main.py:57] epoch 132, training loss: 8253.99, average training loss: 11393.49, base loss: 16072.65
[INFO 2017-06-29 19:59:37,191 main.py:57] epoch 133, training loss: 9138.68, average training loss: 11376.66, base loss: 16076.17
[INFO 2017-06-29 19:59:40,081 main.py:57] epoch 134, training loss: 8844.26, average training loss: 11357.90, base loss: 16071.72
[INFO 2017-06-29 19:59:43,028 main.py:57] epoch 135, training loss: 9483.58, average training loss: 11344.12, base loss: 16074.36
[INFO 2017-06-29 19:59:45,911 main.py:57] epoch 136, training loss: 8358.74, average training loss: 11322.33, base loss: 16064.02
[INFO 2017-06-29 19:59:48,789 main.py:57] epoch 137, training loss: 8890.80, average training loss: 11304.71, base loss: 16063.97
[INFO 2017-06-29 19:59:51,720 main.py:57] epoch 138, training loss: 9200.93, average training loss: 11289.57, base loss: 16073.77
[INFO 2017-06-29 19:59:54,572 main.py:57] epoch 139, training loss: 8690.99, average training loss: 11271.01, base loss: 16071.89
[INFO 2017-06-29 19:59:57,401 main.py:57] epoch 140, training loss: 8541.40, average training loss: 11251.65, base loss: 16061.07
[INFO 2017-06-29 20:00:00,224 main.py:57] epoch 141, training loss: 8519.41, average training loss: 11232.41, base loss: 16055.83
[INFO 2017-06-29 20:00:03,113 main.py:57] epoch 142, training loss: 10025.32, average training loss: 11223.97, base loss: 16070.49
[INFO 2017-06-29 20:00:05,994 main.py:57] epoch 143, training loss: 9480.08, average training loss: 11211.86, base loss: 16077.35
[INFO 2017-06-29 20:00:08,883 main.py:57] epoch 144, training loss: 9032.17, average training loss: 11196.83, base loss: 16078.57
[INFO 2017-06-29 20:00:11,750 main.py:57] epoch 145, training loss: 7577.25, average training loss: 11172.04, base loss: 16058.59
[INFO 2017-06-29 20:00:14,622 main.py:57] epoch 146, training loss: 8402.02, average training loss: 11153.19, base loss: 16053.92
[INFO 2017-06-29 20:00:17,523 main.py:57] epoch 147, training loss: 9538.77, average training loss: 11142.28, base loss: 16061.75
[INFO 2017-06-29 20:00:20,386 main.py:57] epoch 148, training loss: 7816.93, average training loss: 11119.97, base loss: 16046.93
[INFO 2017-06-29 20:00:23,257 main.py:57] epoch 149, training loss: 8016.95, average training loss: 11099.28, base loss: 16039.57
[INFO 2017-06-29 20:00:26,099 main.py:57] epoch 150, training loss: 9062.89, average training loss: 11085.79, base loss: 16036.91
[INFO 2017-06-29 20:00:28,948 main.py:57] epoch 151, training loss: 8888.30, average training loss: 11071.34, base loss: 16037.94
[INFO 2017-06-29 20:00:31,841 main.py:57] epoch 152, training loss: 9179.94, average training loss: 11058.97, base loss: 16042.51
[INFO 2017-06-29 20:00:34,721 main.py:57] epoch 153, training loss: 8409.59, average training loss: 11041.77, base loss: 16040.56
[INFO 2017-06-29 20:00:37,602 main.py:57] epoch 154, training loss: 9213.45, average training loss: 11029.98, base loss: 16046.98
[INFO 2017-06-29 20:00:40,476 main.py:57] epoch 155, training loss: 8610.47, average training loss: 11014.47, base loss: 16045.15
[INFO 2017-06-29 20:00:43,378 main.py:57] epoch 156, training loss: 9341.54, average training loss: 11003.81, base loss: 16056.92
[INFO 2017-06-29 20:00:46,235 main.py:57] epoch 157, training loss: 8276.26, average training loss: 10986.55, base loss: 16052.82
[INFO 2017-06-29 20:00:49,088 main.py:57] epoch 158, training loss: 9139.19, average training loss: 10974.93, base loss: 16058.75
[INFO 2017-06-29 20:00:51,958 main.py:57] epoch 159, training loss: 8893.92, average training loss: 10961.92, base loss: 16060.82
[INFO 2017-06-29 20:00:54,868 main.py:57] epoch 160, training loss: 8386.01, average training loss: 10945.92, base loss: 16055.48
[INFO 2017-06-29 20:00:57,775 main.py:57] epoch 161, training loss: 8968.21, average training loss: 10933.71, base loss: 16058.17
[INFO 2017-06-29 20:01:00,642 main.py:57] epoch 162, training loss: 9344.46, average training loss: 10923.96, base loss: 16067.39
[INFO 2017-06-29 20:01:03,500 main.py:57] epoch 163, training loss: 8461.06, average training loss: 10908.95, base loss: 16068.02
[INFO 2017-06-29 20:01:06,398 main.py:57] epoch 164, training loss: 8497.83, average training loss: 10894.33, base loss: 16065.23
[INFO 2017-06-29 20:01:09,271 main.py:57] epoch 165, training loss: 8243.19, average training loss: 10878.36, base loss: 16060.57
[INFO 2017-06-29 20:01:12,116 main.py:57] epoch 166, training loss: 9216.67, average training loss: 10868.41, base loss: 16066.14
[INFO 2017-06-29 20:01:14,962 main.py:57] epoch 167, training loss: 8594.92, average training loss: 10854.88, base loss: 16067.09
[INFO 2017-06-29 20:01:17,806 main.py:57] epoch 168, training loss: 8516.82, average training loss: 10841.05, base loss: 16071.31
[INFO 2017-06-29 20:01:20,724 main.py:57] epoch 169, training loss: 9351.23, average training loss: 10832.28, base loss: 16078.45
[INFO 2017-06-29 20:01:23,584 main.py:57] epoch 170, training loss: 8382.21, average training loss: 10817.95, base loss: 16081.94
[INFO 2017-06-29 20:01:26,467 main.py:57] epoch 171, training loss: 7695.67, average training loss: 10799.80, base loss: 16074.01
[INFO 2017-06-29 20:01:29,324 main.py:57] epoch 172, training loss: 8802.35, average training loss: 10788.26, base loss: 16077.64
[INFO 2017-06-29 20:01:32,237 main.py:57] epoch 173, training loss: 8857.42, average training loss: 10777.16, base loss: 16074.46
[INFO 2017-06-29 20:01:35,052 main.py:57] epoch 174, training loss: 7401.77, average training loss: 10757.87, base loss: 16056.36
[INFO 2017-06-29 20:01:37,933 main.py:57] epoch 175, training loss: 9504.07, average training loss: 10750.75, base loss: 16063.67
[INFO 2017-06-29 20:01:40,831 main.py:57] epoch 176, training loss: 8375.22, average training loss: 10737.33, base loss: 16058.55
[INFO 2017-06-29 20:01:43,712 main.py:57] epoch 177, training loss: 8825.61, average training loss: 10726.59, base loss: 16059.33
[INFO 2017-06-29 20:01:46,600 main.py:57] epoch 178, training loss: 7646.15, average training loss: 10709.38, base loss: 16045.25
[INFO 2017-06-29 20:01:49,482 main.py:57] epoch 179, training loss: 9677.69, average training loss: 10703.64, base loss: 16053.39
[INFO 2017-06-29 20:01:52,354 main.py:57] epoch 180, training loss: 8470.94, average training loss: 10691.31, base loss: 16048.98
[INFO 2017-06-29 20:01:55,233 main.py:57] epoch 181, training loss: 9026.27, average training loss: 10682.16, base loss: 16051.04
[INFO 2017-06-29 20:01:58,086 main.py:57] epoch 182, training loss: 8489.65, average training loss: 10670.18, base loss: 16049.25
[INFO 2017-06-29 20:02:00,931 main.py:57] epoch 183, training loss: 9472.02, average training loss: 10663.67, base loss: 16052.99
[INFO 2017-06-29 20:02:03,843 main.py:57] epoch 184, training loss: 8434.42, average training loss: 10651.62, base loss: 16047.69
[INFO 2017-06-29 20:02:06,716 main.py:57] epoch 185, training loss: 8793.67, average training loss: 10641.63, base loss: 16048.16
[INFO 2017-06-29 20:02:09,585 main.py:57] epoch 186, training loss: 8343.87, average training loss: 10629.34, base loss: 16042.34
[INFO 2017-06-29 20:02:12,427 main.py:57] epoch 187, training loss: 8604.02, average training loss: 10618.57, base loss: 16036.97
[INFO 2017-06-29 20:02:15,388 main.py:57] epoch 188, training loss: 8998.47, average training loss: 10610.00, base loss: 16040.78
[INFO 2017-06-29 20:02:18,269 main.py:57] epoch 189, training loss: 9972.02, average training loss: 10606.64, base loss: 16044.11
[INFO 2017-06-29 20:02:21,146 main.py:57] epoch 190, training loss: 8457.30, average training loss: 10595.39, base loss: 16043.67
[INFO 2017-06-29 20:02:24,061 main.py:57] epoch 191, training loss: 9249.49, average training loss: 10588.38, base loss: 16048.82
[INFO 2017-06-29 20:02:26,923 main.py:57] epoch 192, training loss: 8757.15, average training loss: 10578.89, base loss: 16041.39
[INFO 2017-06-29 20:02:29,823 main.py:57] epoch 193, training loss: 8837.62, average training loss: 10569.91, base loss: 16044.71
[INFO 2017-06-29 20:02:32,744 main.py:57] epoch 194, training loss: 8634.79, average training loss: 10559.99, base loss: 16042.42
[INFO 2017-06-29 20:02:35,588 main.py:57] epoch 195, training loss: 8459.13, average training loss: 10549.27, base loss: 16040.00
[INFO 2017-06-29 20:02:38,493 main.py:57] epoch 196, training loss: 9203.65, average training loss: 10542.44, base loss: 16049.37
[INFO 2017-06-29 20:02:41,382 main.py:57] epoch 197, training loss: 8492.98, average training loss: 10532.09, base loss: 16050.51
[INFO 2017-06-29 20:02:44,253 main.py:57] epoch 198, training loss: 9107.72, average training loss: 10524.93, base loss: 16054.81
[INFO 2017-06-29 20:02:47,104 main.py:57] epoch 199, training loss: 8316.48, average training loss: 10513.89, base loss: 16055.61
[INFO 2017-06-29 20:02:47,105 main.py:59] epoch 199, testing
[INFO 2017-06-29 20:02:59,419 main.py:104] average testing loss: 8892.04, base loss: 16410.98
[INFO 2017-06-29 20:02:59,420 main.py:105] improve_loss: 7518.95, improve_percent: 0.46
[INFO 2017-06-29 20:02:59,421 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 20:02:59,447 main.py:71] current best improved percent: 0.46
[INFO 2017-06-29 20:03:02,326 main.py:57] epoch 200, training loss: 8878.61, average training loss: 10505.75, base loss: 16058.32
[INFO 2017-06-29 20:03:05,210 main.py:57] epoch 201, training loss: 8151.37, average training loss: 10494.10, base loss: 16053.00
[INFO 2017-06-29 20:03:08,097 main.py:57] epoch 202, training loss: 8976.60, average training loss: 10486.62, base loss: 16053.16
[INFO 2017-06-29 20:03:10,923 main.py:57] epoch 203, training loss: 8562.96, average training loss: 10477.19, base loss: 16050.42
[INFO 2017-06-29 20:03:13,778 main.py:57] epoch 204, training loss: 8097.04, average training loss: 10465.58, base loss: 16044.35
[INFO 2017-06-29 20:03:16,712 main.py:57] epoch 205, training loss: 8633.02, average training loss: 10456.69, base loss: 16044.11
[INFO 2017-06-29 20:03:19,550 main.py:57] epoch 206, training loss: 8263.33, average training loss: 10446.09, base loss: 16039.67
[INFO 2017-06-29 20:03:22,392 main.py:57] epoch 207, training loss: 9449.09, average training loss: 10441.30, base loss: 16050.00
[INFO 2017-06-29 20:03:25,266 main.py:57] epoch 208, training loss: 9136.21, average training loss: 10435.05, base loss: 16055.24
[INFO 2017-06-29 20:03:28,132 main.py:57] epoch 209, training loss: 8918.12, average training loss: 10427.83, base loss: 16054.51
[INFO 2017-06-29 20:03:31,001 main.py:57] epoch 210, training loss: 8887.36, average training loss: 10420.53, base loss: 16054.68
[INFO 2017-06-29 20:03:33,851 main.py:57] epoch 211, training loss: 8374.90, average training loss: 10410.88, base loss: 16050.67
[INFO 2017-06-29 20:03:36,747 main.py:57] epoch 212, training loss: 8577.73, average training loss: 10402.27, base loss: 16050.77
[INFO 2017-06-29 20:03:39,641 main.py:57] epoch 213, training loss: 8020.32, average training loss: 10391.14, base loss: 16047.57
[INFO 2017-06-29 20:03:42,504 main.py:57] epoch 214, training loss: 8041.88, average training loss: 10380.22, base loss: 16039.80
[INFO 2017-06-29 20:03:45,393 main.py:57] epoch 215, training loss: 8219.72, average training loss: 10370.21, base loss: 16039.53
[INFO 2017-06-29 20:03:48,286 main.py:57] epoch 216, training loss: 9067.03, average training loss: 10364.21, base loss: 16042.96
[INFO 2017-06-29 20:03:51,198 main.py:57] epoch 217, training loss: 8576.96, average training loss: 10356.01, base loss: 16042.99
[INFO 2017-06-29 20:03:54,045 main.py:57] epoch 218, training loss: 7828.47, average training loss: 10344.47, base loss: 16040.50
[INFO 2017-06-29 20:03:56,919 main.py:57] epoch 219, training loss: 8679.94, average training loss: 10336.90, base loss: 16040.55
[INFO 2017-06-29 20:03:59,749 main.py:57] epoch 220, training loss: 8523.45, average training loss: 10328.70, base loss: 16042.45
[INFO 2017-06-29 20:04:02,637 main.py:57] epoch 221, training loss: 8521.17, average training loss: 10320.55, base loss: 16043.69
[INFO 2017-06-29 20:04:05,542 main.py:57] epoch 222, training loss: 9307.88, average training loss: 10316.01, base loss: 16045.90
[INFO 2017-06-29 20:04:08,414 main.py:57] epoch 223, training loss: 8788.95, average training loss: 10309.20, base loss: 16047.53
[INFO 2017-06-29 20:04:11,303 main.py:57] epoch 224, training loss: 9204.70, average training loss: 10304.29, base loss: 16052.58
[INFO 2017-06-29 20:04:14,161 main.py:57] epoch 225, training loss: 8053.70, average training loss: 10294.33, base loss: 16046.21
[INFO 2017-06-29 20:04:17,075 main.py:57] epoch 226, training loss: 8167.30, average training loss: 10284.96, base loss: 16047.26
[INFO 2017-06-29 20:04:19,942 main.py:57] epoch 227, training loss: 9005.03, average training loss: 10279.34, base loss: 16054.87
[INFO 2017-06-29 20:04:22,833 main.py:57] epoch 228, training loss: 8172.05, average training loss: 10270.14, base loss: 16052.74
[INFO 2017-06-29 20:04:25,704 main.py:57] epoch 229, training loss: 8029.55, average training loss: 10260.40, base loss: 16048.02
[INFO 2017-06-29 20:04:28,585 main.py:57] epoch 230, training loss: 7723.35, average training loss: 10249.42, base loss: 16039.94
[INFO 2017-06-29 20:04:31,450 main.py:57] epoch 231, training loss: 8525.56, average training loss: 10241.99, base loss: 16045.22
[INFO 2017-06-29 20:04:34,344 main.py:57] epoch 232, training loss: 8133.79, average training loss: 10232.94, base loss: 16037.39
[INFO 2017-06-29 20:04:37,186 main.py:57] epoch 233, training loss: 8676.35, average training loss: 10226.29, base loss: 16038.77
[INFO 2017-06-29 20:04:40,068 main.py:57] epoch 234, training loss: 9436.98, average training loss: 10222.93, base loss: 16045.84
[INFO 2017-06-29 20:04:42,940 main.py:57] epoch 235, training loss: 8436.50, average training loss: 10215.36, base loss: 16042.70
[INFO 2017-06-29 20:04:45,804 main.py:57] epoch 236, training loss: 8077.36, average training loss: 10206.34, base loss: 16040.54
[INFO 2017-06-29 20:04:48,660 main.py:57] epoch 237, training loss: 8857.04, average training loss: 10200.67, base loss: 16043.12
[INFO 2017-06-29 20:04:51,529 main.py:57] epoch 238, training loss: 8861.94, average training loss: 10195.07, base loss: 16047.20
[INFO 2017-06-29 20:04:54,396 main.py:57] epoch 239, training loss: 8434.79, average training loss: 10187.73, base loss: 16046.48
[INFO 2017-06-29 20:04:57,237 main.py:57] epoch 240, training loss: 8906.51, average training loss: 10182.42, base loss: 16052.10
[INFO 2017-06-29 20:05:00,084 main.py:57] epoch 241, training loss: 8649.24, average training loss: 10176.08, base loss: 16053.29
[INFO 2017-06-29 20:05:02,930 main.py:57] epoch 242, training loss: 9250.31, average training loss: 10172.27, base loss: 16058.18
[INFO 2017-06-29 20:05:05,791 main.py:57] epoch 243, training loss: 8228.09, average training loss: 10164.30, base loss: 16056.40
[INFO 2017-06-29 20:05:08,671 main.py:57] epoch 244, training loss: 8053.94, average training loss: 10155.69, base loss: 16056.17
[INFO 2017-06-29 20:05:11,556 main.py:57] epoch 245, training loss: 8146.71, average training loss: 10147.52, base loss: 16051.88
[INFO 2017-06-29 20:05:14,425 main.py:57] epoch 246, training loss: 8425.23, average training loss: 10140.55, base loss: 16052.31
[INFO 2017-06-29 20:05:17,309 main.py:57] epoch 247, training loss: 8657.24, average training loss: 10134.57, base loss: 16056.83
[INFO 2017-06-29 20:05:20,190 main.py:57] epoch 248, training loss: 8031.10, average training loss: 10126.12, base loss: 16055.20
[INFO 2017-06-29 20:05:23,106 main.py:57] epoch 249, training loss: 8846.25, average training loss: 10121.00, base loss: 16058.97
[INFO 2017-06-29 20:05:25,975 main.py:57] epoch 250, training loss: 8937.95, average training loss: 10116.29, base loss: 16067.56
[INFO 2017-06-29 20:05:28,829 main.py:57] epoch 251, training loss: 8010.89, average training loss: 10107.93, base loss: 16066.83
[INFO 2017-06-29 20:05:31,754 main.py:57] epoch 252, training loss: 7136.05, average training loss: 10096.19, base loss: 16057.13
[INFO 2017-06-29 20:05:34,650 main.py:57] epoch 253, training loss: 8941.14, average training loss: 10091.64, base loss: 16062.79
[INFO 2017-06-29 20:05:37,568 main.py:57] epoch 254, training loss: 8415.96, average training loss: 10085.07, base loss: 16062.77
[INFO 2017-06-29 20:05:40,448 main.py:57] epoch 255, training loss: 9335.91, average training loss: 10082.14, base loss: 16069.53
[INFO 2017-06-29 20:05:43,328 main.py:57] epoch 256, training loss: 8171.12, average training loss: 10074.71, base loss: 16069.57
[INFO 2017-06-29 20:05:46,181 main.py:57] epoch 257, training loss: 8204.35, average training loss: 10067.46, base loss: 16069.05
[INFO 2017-06-29 20:05:49,120 main.py:57] epoch 258, training loss: 8965.04, average training loss: 10063.20, base loss: 16074.35
[INFO 2017-06-29 20:05:51,986 main.py:57] epoch 259, training loss: 8197.94, average training loss: 10056.03, base loss: 16077.80
[INFO 2017-06-29 20:05:54,903 main.py:57] epoch 260, training loss: 8083.79, average training loss: 10048.47, base loss: 16076.70
[INFO 2017-06-29 20:05:57,770 main.py:57] epoch 261, training loss: 8634.95, average training loss: 10043.07, base loss: 16073.11
[INFO 2017-06-29 20:06:00,687 main.py:57] epoch 262, training loss: 8144.70, average training loss: 10035.86, base loss: 16071.71
[INFO 2017-06-29 20:06:03,566 main.py:57] epoch 263, training loss: 7572.95, average training loss: 10026.53, base loss: 16063.72
[INFO 2017-06-29 20:06:06,407 main.py:57] epoch 264, training loss: 8078.55, average training loss: 10019.18, base loss: 16065.65
[INFO 2017-06-29 20:06:09,293 main.py:57] epoch 265, training loss: 9258.34, average training loss: 10016.32, base loss: 16071.41
[INFO 2017-06-29 20:06:12,151 main.py:57] epoch 266, training loss: 8059.55, average training loss: 10008.99, base loss: 16069.71
[INFO 2017-06-29 20:06:15,011 main.py:57] epoch 267, training loss: 9498.47, average training loss: 10007.08, base loss: 16079.33
[INFO 2017-06-29 20:06:17,880 main.py:57] epoch 268, training loss: 8541.48, average training loss: 10001.63, base loss: 16078.42
[INFO 2017-06-29 20:06:20,786 main.py:57] epoch 269, training loss: 7758.53, average training loss: 9993.33, base loss: 16069.12
[INFO 2017-06-29 20:06:23,668 main.py:57] epoch 270, training loss: 8258.70, average training loss: 9986.93, base loss: 16071.05
[INFO 2017-06-29 20:06:26,518 main.py:57] epoch 271, training loss: 8175.28, average training loss: 9980.27, base loss: 16069.48
[INFO 2017-06-29 20:06:29,383 main.py:57] epoch 272, training loss: 7872.65, average training loss: 9972.54, base loss: 16067.18
[INFO 2017-06-29 20:06:32,284 main.py:57] epoch 273, training loss: 8057.92, average training loss: 9965.56, base loss: 16067.98
[INFO 2017-06-29 20:06:35,130 main.py:57] epoch 274, training loss: 8538.53, average training loss: 9960.37, base loss: 16069.58
[INFO 2017-06-29 20:06:37,995 main.py:57] epoch 275, training loss: 8463.15, average training loss: 9954.94, base loss: 16068.47
[INFO 2017-06-29 20:06:40,864 main.py:57] epoch 276, training loss: 8182.63, average training loss: 9948.55, base loss: 16069.71
[INFO 2017-06-29 20:06:43,692 main.py:57] epoch 277, training loss: 8535.07, average training loss: 9943.46, base loss: 16071.74
[INFO 2017-06-29 20:06:46,541 main.py:57] epoch 278, training loss: 8125.82, average training loss: 9936.95, base loss: 16072.05
[INFO 2017-06-29 20:06:49,399 main.py:57] epoch 279, training loss: 7776.67, average training loss: 9929.23, base loss: 16071.21
[INFO 2017-06-29 20:06:52,327 main.py:57] epoch 280, training loss: 8571.83, average training loss: 9924.40, base loss: 16073.76
[INFO 2017-06-29 20:06:55,215 main.py:57] epoch 281, training loss: 7902.33, average training loss: 9917.23, base loss: 16069.49
[INFO 2017-06-29 20:06:58,103 main.py:57] epoch 282, training loss: 9071.18, average training loss: 9914.24, base loss: 16075.64
[INFO 2017-06-29 20:07:00,974 main.py:57] epoch 283, training loss: 8040.87, average training loss: 9907.64, base loss: 16072.19
[INFO 2017-06-29 20:07:03,858 main.py:57] epoch 284, training loss: 8004.47, average training loss: 9900.97, base loss: 16068.86
[INFO 2017-06-29 20:07:06,785 main.py:57] epoch 285, training loss: 7748.73, average training loss: 9893.44, base loss: 16065.61
[INFO 2017-06-29 20:07:09,704 main.py:57] epoch 286, training loss: 9422.85, average training loss: 9891.80, base loss: 16072.06
[INFO 2017-06-29 20:07:12,648 main.py:57] epoch 287, training loss: 8628.41, average training loss: 9887.41, base loss: 16073.07
[INFO 2017-06-29 20:07:15,541 main.py:57] epoch 288, training loss: 8709.40, average training loss: 9883.34, base loss: 16073.64
[INFO 2017-06-29 20:07:18,353 main.py:57] epoch 289, training loss: 6917.87, average training loss: 9873.11, base loss: 16065.70
[INFO 2017-06-29 20:07:21,204 main.py:57] epoch 290, training loss: 8051.61, average training loss: 9866.85, base loss: 16065.32
[INFO 2017-06-29 20:07:24,082 main.py:57] epoch 291, training loss: 7773.33, average training loss: 9859.68, base loss: 16059.76
[INFO 2017-06-29 20:07:26,972 main.py:57] epoch 292, training loss: 8112.58, average training loss: 9853.72, base loss: 16058.05
[INFO 2017-06-29 20:07:29,859 main.py:57] epoch 293, training loss: 7674.04, average training loss: 9846.31, base loss: 16055.38
[INFO 2017-06-29 20:07:32,729 main.py:57] epoch 294, training loss: 8881.38, average training loss: 9843.04, base loss: 16055.32
[INFO 2017-06-29 20:07:35,582 main.py:57] epoch 295, training loss: 8137.71, average training loss: 9837.27, base loss: 16052.49
[INFO 2017-06-29 20:07:38,445 main.py:57] epoch 296, training loss: 8454.06, average training loss: 9832.62, base loss: 16051.61
[INFO 2017-06-29 20:07:41,316 main.py:57] epoch 297, training loss: 8888.23, average training loss: 9829.45, base loss: 16056.09
[INFO 2017-06-29 20:07:44,165 main.py:57] epoch 298, training loss: 7646.50, average training loss: 9822.15, base loss: 16056.18
[INFO 2017-06-29 20:07:47,029 main.py:57] epoch 299, training loss: 7653.69, average training loss: 9814.92, base loss: 16052.16
[INFO 2017-06-29 20:07:47,030 main.py:59] epoch 299, testing
[INFO 2017-06-29 20:07:59,328 main.py:104] average testing loss: 8103.47, base loss: 16096.10
[INFO 2017-06-29 20:07:59,328 main.py:105] improve_loss: 7992.63, improve_percent: 0.50
[INFO 2017-06-29 20:07:59,330 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 20:07:59,355 main.py:71] current best improved percent: 0.50
[INFO 2017-06-29 20:08:02,229 main.py:57] epoch 300, training loss: 7710.25, average training loss: 9807.93, base loss: 16048.57
[INFO 2017-06-29 20:08:05,142 main.py:57] epoch 301, training loss: 8142.45, average training loss: 9802.41, base loss: 16051.06
[INFO 2017-06-29 20:08:08,025 main.py:57] epoch 302, training loss: 7406.99, average training loss: 9794.51, base loss: 16045.56
[INFO 2017-06-29 20:08:10,871 main.py:57] epoch 303, training loss: 9119.47, average training loss: 9792.29, base loss: 16050.67
[INFO 2017-06-29 20:08:13,738 main.py:57] epoch 304, training loss: 8848.09, average training loss: 9789.19, base loss: 16055.57
[INFO 2017-06-29 20:08:16,615 main.py:57] epoch 305, training loss: 7506.17, average training loss: 9781.73, base loss: 16050.74
[INFO 2017-06-29 20:08:19,507 main.py:57] epoch 306, training loss: 8080.28, average training loss: 9776.19, base loss: 16050.53
[INFO 2017-06-29 20:08:22,374 main.py:57] epoch 307, training loss: 8040.80, average training loss: 9770.55, base loss: 16050.39
[INFO 2017-06-29 20:08:25,228 main.py:57] epoch 308, training loss: 8668.72, average training loss: 9766.99, base loss: 16054.07
[INFO 2017-06-29 20:08:28,109 main.py:57] epoch 309, training loss: 7807.63, average training loss: 9760.67, base loss: 16052.80
[INFO 2017-06-29 20:08:31,004 main.py:57] epoch 310, training loss: 7680.67, average training loss: 9753.98, base loss: 16050.72
[INFO 2017-06-29 20:08:33,886 main.py:57] epoch 311, training loss: 7941.33, average training loss: 9748.17, base loss: 16051.22
[INFO 2017-06-29 20:08:36,783 main.py:57] epoch 312, training loss: 8533.44, average training loss: 9744.29, base loss: 16054.80
[INFO 2017-06-29 20:08:39,690 main.py:57] epoch 313, training loss: 8820.72, average training loss: 9741.35, base loss: 16059.16
[INFO 2017-06-29 20:08:42,617 main.py:57] epoch 314, training loss: 8610.35, average training loss: 9737.76, base loss: 16061.40
[INFO 2017-06-29 20:08:45,511 main.py:57] epoch 315, training loss: 7908.28, average training loss: 9731.97, base loss: 16059.27
[INFO 2017-06-29 20:08:48,368 main.py:57] epoch 316, training loss: 8062.89, average training loss: 9726.70, base loss: 16055.82
[INFO 2017-06-29 20:08:51,212 main.py:57] epoch 317, training loss: 8148.20, average training loss: 9721.74, base loss: 16055.63
[INFO 2017-06-29 20:08:54,059 main.py:57] epoch 318, training loss: 7393.65, average training loss: 9714.44, base loss: 16049.67
[INFO 2017-06-29 20:08:57,003 main.py:57] epoch 319, training loss: 8648.29, average training loss: 9711.11, base loss: 16053.87
[INFO 2017-06-29 20:08:59,867 main.py:57] epoch 320, training loss: 7974.39, average training loss: 9705.70, base loss: 16054.85
[INFO 2017-06-29 20:09:02,707 main.py:57] epoch 321, training loss: 8076.49, average training loss: 9700.64, base loss: 16052.14
[INFO 2017-06-29 20:09:05,584 main.py:57] epoch 322, training loss: 8405.24, average training loss: 9696.63, base loss: 16051.91
[INFO 2017-06-29 20:09:08,456 main.py:57] epoch 323, training loss: 8532.04, average training loss: 9693.03, base loss: 16053.50
[INFO 2017-06-29 20:09:11,336 main.py:57] epoch 324, training loss: 8552.99, average training loss: 9689.52, base loss: 16058.74
[INFO 2017-06-29 20:09:14,220 main.py:57] epoch 325, training loss: 7830.14, average training loss: 9683.82, base loss: 16056.67
[INFO 2017-06-29 20:09:17,087 main.py:57] epoch 326, training loss: 8399.62, average training loss: 9679.89, base loss: 16060.16
[INFO 2017-06-29 20:09:20,010 main.py:57] epoch 327, training loss: 7801.81, average training loss: 9674.17, base loss: 16056.89
[INFO 2017-06-29 20:09:22,864 main.py:57] epoch 328, training loss: 8653.82, average training loss: 9671.07, base loss: 16061.53
[INFO 2017-06-29 20:09:25,763 main.py:57] epoch 329, training loss: 7623.00, average training loss: 9664.86, base loss: 16059.49
[INFO 2017-06-29 20:09:28,627 main.py:57] epoch 330, training loss: 7950.67, average training loss: 9659.68, base loss: 16059.84
[INFO 2017-06-29 20:09:31,522 main.py:57] epoch 331, training loss: 8814.03, average training loss: 9657.13, base loss: 16067.13
[INFO 2017-06-29 20:09:34,468 main.py:57] epoch 332, training loss: 7824.98, average training loss: 9651.63, base loss: 16066.40
[INFO 2017-06-29 20:09:37,315 main.py:57] epoch 333, training loss: 8315.39, average training loss: 9647.63, base loss: 16067.04
[INFO 2017-06-29 20:09:40,203 main.py:57] epoch 334, training loss: 7775.48, average training loss: 9642.04, base loss: 16067.42
[INFO 2017-06-29 20:09:43,118 main.py:57] epoch 335, training loss: 8282.10, average training loss: 9638.00, base loss: 16065.11
[INFO 2017-06-29 20:09:45,992 main.py:57] epoch 336, training loss: 8328.53, average training loss: 9634.11, base loss: 16068.06
[INFO 2017-06-29 20:09:48,851 main.py:57] epoch 337, training loss: 7043.27, average training loss: 9626.44, base loss: 16062.17
[INFO 2017-06-29 20:09:51,735 main.py:57] epoch 338, training loss: 7294.99, average training loss: 9619.57, base loss: 16055.82
[INFO 2017-06-29 20:09:54,613 main.py:57] epoch 339, training loss: 7534.55, average training loss: 9613.43, base loss: 16052.18
[INFO 2017-06-29 20:09:57,478 main.py:57] epoch 340, training loss: 7635.11, average training loss: 9607.63, base loss: 16049.62
[INFO 2017-06-29 20:10:00,325 main.py:57] epoch 341, training loss: 8341.07, average training loss: 9603.93, base loss: 16050.00
[INFO 2017-06-29 20:10:03,198 main.py:57] epoch 342, training loss: 9040.74, average training loss: 9602.29, base loss: 16055.59
[INFO 2017-06-29 20:10:06,063 main.py:57] epoch 343, training loss: 7647.35, average training loss: 9596.60, base loss: 16053.42
[INFO 2017-06-29 20:10:08,978 main.py:57] epoch 344, training loss: 8045.05, average training loss: 9592.11, base loss: 16054.14
[INFO 2017-06-29 20:10:11,863 main.py:57] epoch 345, training loss: 7437.81, average training loss: 9585.88, base loss: 16049.88
[INFO 2017-06-29 20:10:14,762 main.py:57] epoch 346, training loss: 8281.59, average training loss: 9582.12, base loss: 16052.19
[INFO 2017-06-29 20:10:17,651 main.py:57] epoch 347, training loss: 7937.34, average training loss: 9577.40, base loss: 16053.88
[INFO 2017-06-29 20:10:20,503 main.py:57] epoch 348, training loss: 8016.63, average training loss: 9572.92, base loss: 16054.12
[INFO 2017-06-29 20:10:23,384 main.py:57] epoch 349, training loss: 7784.68, average training loss: 9567.81, base loss: 16054.10
[INFO 2017-06-29 20:10:26,267 main.py:57] epoch 350, training loss: 8641.13, average training loss: 9565.17, base loss: 16054.75
[INFO 2017-06-29 20:10:29,217 main.py:57] epoch 351, training loss: 8029.83, average training loss: 9560.81, base loss: 16056.15
[INFO 2017-06-29 20:10:32,120 main.py:57] epoch 352, training loss: 7268.10, average training loss: 9554.32, base loss: 16052.62
[INFO 2017-06-29 20:10:34,998 main.py:57] epoch 353, training loss: 7926.45, average training loss: 9549.72, base loss: 16051.20
[INFO 2017-06-29 20:10:37,888 main.py:57] epoch 354, training loss: 7967.93, average training loss: 9545.26, base loss: 16053.29
[INFO 2017-06-29 20:10:40,750 main.py:57] epoch 355, training loss: 7576.18, average training loss: 9539.73, base loss: 16049.96
[INFO 2017-06-29 20:10:43,610 main.py:57] epoch 356, training loss: 8492.00, average training loss: 9536.80, base loss: 16053.94
[INFO 2017-06-29 20:10:46,449 main.py:57] epoch 357, training loss: 8461.79, average training loss: 9533.80, base loss: 16056.12
[INFO 2017-06-29 20:10:49,339 main.py:57] epoch 358, training loss: 7937.65, average training loss: 9529.35, base loss: 16057.76
[INFO 2017-06-29 20:10:52,224 main.py:57] epoch 359, training loss: 8732.61, average training loss: 9527.14, base loss: 16062.04
[INFO 2017-06-29 20:10:55,047 main.py:57] epoch 360, training loss: 7964.30, average training loss: 9522.81, base loss: 16064.20
[INFO 2017-06-29 20:10:57,892 main.py:57] epoch 361, training loss: 7594.88, average training loss: 9517.48, base loss: 16061.74
[INFO 2017-06-29 20:11:00,785 main.py:57] epoch 362, training loss: 8580.93, average training loss: 9514.90, base loss: 16059.85
[INFO 2017-06-29 20:11:03,662 main.py:57] epoch 363, training loss: 7840.56, average training loss: 9510.30, base loss: 16058.21
[INFO 2017-06-29 20:11:06,500 main.py:57] epoch 364, training loss: 9063.47, average training loss: 9509.08, base loss: 16064.87
[INFO 2017-06-29 20:11:09,367 main.py:57] epoch 365, training loss: 8760.58, average training loss: 9507.03, base loss: 16068.50
[INFO 2017-06-29 20:11:12,248 main.py:57] epoch 366, training loss: 8081.29, average training loss: 9503.15, base loss: 16069.16
[INFO 2017-06-29 20:11:15,117 main.py:57] epoch 367, training loss: 8360.50, average training loss: 9500.04, base loss: 16074.39
[INFO 2017-06-29 20:11:17,994 main.py:57] epoch 368, training loss: 7856.42, average training loss: 9495.59, base loss: 16072.72
[INFO 2017-06-29 20:11:20,890 main.py:57] epoch 369, training loss: 7198.80, average training loss: 9489.38, base loss: 16065.66
[INFO 2017-06-29 20:11:23,776 main.py:57] epoch 370, training loss: 7669.07, average training loss: 9484.47, base loss: 16061.94
[INFO 2017-06-29 20:11:26,661 main.py:57] epoch 371, training loss: 8319.26, average training loss: 9481.34, base loss: 16062.37
[INFO 2017-06-29 20:11:29,539 main.py:57] epoch 372, training loss: 8075.40, average training loss: 9477.57, base loss: 16061.58
[INFO 2017-06-29 20:11:32,416 main.py:57] epoch 373, training loss: 7422.12, average training loss: 9472.08, base loss: 16057.64
[INFO 2017-06-29 20:11:35,288 main.py:57] epoch 374, training loss: 8286.92, average training loss: 9468.92, base loss: 16059.17
[INFO 2017-06-29 20:11:38,121 main.py:57] epoch 375, training loss: 8244.30, average training loss: 9465.66, base loss: 16062.50
[INFO 2017-06-29 20:11:40,992 main.py:57] epoch 376, training loss: 8427.55, average training loss: 9462.91, base loss: 16062.56
[INFO 2017-06-29 20:11:43,860 main.py:57] epoch 377, training loss: 8654.74, average training loss: 9460.77, base loss: 16066.49
[INFO 2017-06-29 20:11:46,776 main.py:57] epoch 378, training loss: 7700.23, average training loss: 9456.12, base loss: 16065.99
[INFO 2017-06-29 20:11:49,653 main.py:57] epoch 379, training loss: 8145.47, average training loss: 9452.67, base loss: 16067.62
[INFO 2017-06-29 20:11:52,493 main.py:57] epoch 380, training loss: 7850.49, average training loss: 9448.47, base loss: 16067.67
[INFO 2017-06-29 20:11:55,354 main.py:57] epoch 381, training loss: 8059.44, average training loss: 9444.83, base loss: 16069.30
[INFO 2017-06-29 20:11:58,235 main.py:57] epoch 382, training loss: 7626.91, average training loss: 9440.08, base loss: 16067.28
[INFO 2017-06-29 20:12:01,134 main.py:57] epoch 383, training loss: 6929.17, average training loss: 9433.55, base loss: 16062.34
[INFO 2017-06-29 20:12:03,973 main.py:57] epoch 384, training loss: 7680.74, average training loss: 9428.99, base loss: 16059.99
[INFO 2017-06-29 20:12:06,831 main.py:57] epoch 385, training loss: 7400.25, average training loss: 9423.74, base loss: 16056.42
[INFO 2017-06-29 20:12:09,742 main.py:57] epoch 386, training loss: 8165.33, average training loss: 9420.49, base loss: 16056.35
[INFO 2017-06-29 20:12:12,637 main.py:57] epoch 387, training loss: 8091.33, average training loss: 9417.06, base loss: 16057.87
[INFO 2017-06-29 20:12:15,520 main.py:57] epoch 388, training loss: 8282.21, average training loss: 9414.14, base loss: 16062.34
[INFO 2017-06-29 20:12:18,404 main.py:57] epoch 389, training loss: 8048.35, average training loss: 9410.64, base loss: 16062.69
[INFO 2017-06-29 20:12:21,265 main.py:57] epoch 390, training loss: 8099.14, average training loss: 9407.29, base loss: 16063.75
[INFO 2017-06-29 20:12:24,132 main.py:57] epoch 391, training loss: 9220.91, average training loss: 9406.81, base loss: 16070.55
[INFO 2017-06-29 20:12:27,014 main.py:57] epoch 392, training loss: 8143.71, average training loss: 9403.60, base loss: 16074.50
[INFO 2017-06-29 20:12:29,878 main.py:57] epoch 393, training loss: 8357.28, average training loss: 9400.94, base loss: 16078.21
[INFO 2017-06-29 20:12:32,707 main.py:57] epoch 394, training loss: 7538.76, average training loss: 9396.23, base loss: 16075.57
[INFO 2017-06-29 20:12:35,564 main.py:57] epoch 395, training loss: 7516.74, average training loss: 9391.48, base loss: 16070.97
[INFO 2017-06-29 20:12:38,429 main.py:57] epoch 396, training loss: 8200.73, average training loss: 9388.48, base loss: 16073.02
[INFO 2017-06-29 20:12:41,307 main.py:57] epoch 397, training loss: 7858.68, average training loss: 9384.64, base loss: 16073.87
[INFO 2017-06-29 20:12:44,168 main.py:57] epoch 398, training loss: 7461.07, average training loss: 9379.82, base loss: 16069.10
[INFO 2017-06-29 20:12:47,016 main.py:57] epoch 399, training loss: 8037.17, average training loss: 9376.46, base loss: 16072.35
[INFO 2017-06-29 20:12:47,017 main.py:59] epoch 399, testing
[INFO 2017-06-29 20:12:59,462 main.py:104] average testing loss: 7895.23, base loss: 15787.16
[INFO 2017-06-29 20:12:59,463 main.py:105] improve_loss: 7891.93, improve_percent: 0.50
[INFO 2017-06-29 20:12:59,465 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 20:12:59,490 main.py:71] current best improved percent: 0.50
[INFO 2017-06-29 20:13:02,366 main.py:57] epoch 400, training loss: 7656.24, average training loss: 9372.17, base loss: 16071.21
[INFO 2017-06-29 20:13:05,277 main.py:57] epoch 401, training loss: 7717.13, average training loss: 9368.05, base loss: 16070.53
[INFO 2017-06-29 20:13:08,139 main.py:57] epoch 402, training loss: 7804.39, average training loss: 9364.17, base loss: 16070.12
[INFO 2017-06-29 20:13:10,973 main.py:57] epoch 403, training loss: 8459.16, average training loss: 9361.93, base loss: 16072.28
[INFO 2017-06-29 20:13:13,833 main.py:57] epoch 404, training loss: 8201.11, average training loss: 9359.07, base loss: 16073.86
[INFO 2017-06-29 20:13:16,730 main.py:57] epoch 405, training loss: 7723.49, average training loss: 9355.04, base loss: 16071.00
[INFO 2017-06-29 20:13:19,643 main.py:57] epoch 406, training loss: 7471.40, average training loss: 9350.41, base loss: 16071.74
[INFO 2017-06-29 20:13:22,558 main.py:57] epoch 407, training loss: 7750.03, average training loss: 9346.49, base loss: 16071.08
[INFO 2017-06-29 20:13:25,402 main.py:57] epoch 408, training loss: 7433.71, average training loss: 9341.81, base loss: 16068.84
[INFO 2017-06-29 20:13:28,339 main.py:57] epoch 409, training loss: 7800.50, average training loss: 9338.05, base loss: 16069.88
[INFO 2017-06-29 20:13:31,191 main.py:57] epoch 410, training loss: 8011.48, average training loss: 9334.82, base loss: 16071.41
[INFO 2017-06-29 20:13:34,029 main.py:57] epoch 411, training loss: 7951.59, average training loss: 9331.47, base loss: 16071.32
[INFO 2017-06-29 20:13:36,887 main.py:57] epoch 412, training loss: 7208.16, average training loss: 9326.33, base loss: 16066.71
[INFO 2017-06-29 20:13:39,744 main.py:57] epoch 413, training loss: 8064.99, average training loss: 9323.28, base loss: 16065.58
[INFO 2017-06-29 20:13:42,578 main.py:57] epoch 414, training loss: 7833.38, average training loss: 9319.69, base loss: 16065.50
[INFO 2017-06-29 20:13:45,448 main.py:57] epoch 415, training loss: 8313.16, average training loss: 9317.27, base loss: 16065.32
[INFO 2017-06-29 20:13:48,371 main.py:57] epoch 416, training loss: 7669.12, average training loss: 9313.32, base loss: 16064.03
[INFO 2017-06-29 20:13:51,251 main.py:57] epoch 417, training loss: 7496.15, average training loss: 9308.97, base loss: 16061.82
[INFO 2017-06-29 20:13:54,169 main.py:57] epoch 418, training loss: 7806.29, average training loss: 9305.38, base loss: 16063.11
[INFO 2017-06-29 20:13:57,068 main.py:57] epoch 419, training loss: 7906.46, average training loss: 9302.05, base loss: 16062.71
[INFO 2017-06-29 20:13:59,929 main.py:57] epoch 420, training loss: 8721.32, average training loss: 9300.67, base loss: 16065.67
[INFO 2017-06-29 20:14:02,845 main.py:57] epoch 421, training loss: 8053.62, average training loss: 9297.72, base loss: 16067.15
[INFO 2017-06-29 20:14:05,675 main.py:57] epoch 422, training loss: 8684.41, average training loss: 9296.27, base loss: 16070.09
[INFO 2017-06-29 20:14:08,556 main.py:57] epoch 423, training loss: 8554.71, average training loss: 9294.52, base loss: 16073.92
[INFO 2017-06-29 20:14:11,417 main.py:57] epoch 424, training loss: 7868.25, average training loss: 9291.16, base loss: 16073.75
[INFO 2017-06-29 20:14:14,268 main.py:57] epoch 425, training loss: 7617.59, average training loss: 9287.23, base loss: 16073.89
[INFO 2017-06-29 20:14:17,121 main.py:57] epoch 426, training loss: 7382.44, average training loss: 9282.77, base loss: 16073.29
[INFO 2017-06-29 20:14:19,960 main.py:57] epoch 427, training loss: 7979.42, average training loss: 9279.73, base loss: 16071.18
[INFO 2017-06-29 20:14:22,916 main.py:57] epoch 428, training loss: 8478.46, average training loss: 9277.86, base loss: 16074.24
[INFO 2017-06-29 20:14:25,785 main.py:57] epoch 429, training loss: 8139.39, average training loss: 9275.21, base loss: 16073.02
[INFO 2017-06-29 20:14:28,644 main.py:57] epoch 430, training loss: 7858.96, average training loss: 9271.93, base loss: 16073.47
[INFO 2017-06-29 20:14:31,500 main.py:57] epoch 431, training loss: 7969.96, average training loss: 9268.91, base loss: 16074.10
[INFO 2017-06-29 20:14:34,347 main.py:57] epoch 432, training loss: 7757.73, average training loss: 9265.42, base loss: 16074.27
[INFO 2017-06-29 20:14:37,206 main.py:57] epoch 433, training loss: 8241.13, average training loss: 9263.06, base loss: 16075.99
[INFO 2017-06-29 20:14:40,061 main.py:57] epoch 434, training loss: 7379.69, average training loss: 9258.73, base loss: 16074.66
[INFO 2017-06-29 20:14:42,946 main.py:57] epoch 435, training loss: 8075.76, average training loss: 9256.02, base loss: 16076.36
[INFO 2017-06-29 20:14:45,811 main.py:57] epoch 436, training loss: 7758.39, average training loss: 9252.59, base loss: 16074.82
[INFO 2017-06-29 20:14:48,697 main.py:57] epoch 437, training loss: 7186.09, average training loss: 9247.88, base loss: 16072.03
[INFO 2017-06-29 20:14:51,556 main.py:57] epoch 438, training loss: 7611.98, average training loss: 9244.15, base loss: 16071.58
[INFO 2017-06-29 20:14:54,405 main.py:57] epoch 439, training loss: 7175.42, average training loss: 9239.45, base loss: 16066.56
[INFO 2017-06-29 20:14:57,237 main.py:57] epoch 440, training loss: 8899.12, average training loss: 9238.68, base loss: 16071.74
[INFO 2017-06-29 20:15:00,094 main.py:57] epoch 441, training loss: 7584.72, average training loss: 9234.93, base loss: 16071.05
[INFO 2017-06-29 20:15:02,962 main.py:57] epoch 442, training loss: 7497.07, average training loss: 9231.01, base loss: 16069.53
[INFO 2017-06-29 20:15:05,825 main.py:57] epoch 443, training loss: 7474.74, average training loss: 9227.05, base loss: 16066.40
[INFO 2017-06-29 20:15:08,694 main.py:57] epoch 444, training loss: 7656.15, average training loss: 9223.52, base loss: 16066.97
[INFO 2017-06-29 20:15:11,558 main.py:57] epoch 445, training loss: 7330.22, average training loss: 9219.28, base loss: 16065.04
[INFO 2017-06-29 20:15:14,439 main.py:57] epoch 446, training loss: 7653.83, average training loss: 9215.78, base loss: 16065.82
[INFO 2017-06-29 20:15:17,297 main.py:57] epoch 447, training loss: 8085.24, average training loss: 9213.25, base loss: 16065.54
[INFO 2017-06-29 20:15:20,156 main.py:57] epoch 448, training loss: 7055.86, average training loss: 9208.45, base loss: 16062.77
[INFO 2017-06-29 20:15:23,048 main.py:57] epoch 449, training loss: 7132.69, average training loss: 9203.84, base loss: 16060.70
[INFO 2017-06-29 20:15:25,941 main.py:57] epoch 450, training loss: 7678.16, average training loss: 9200.45, base loss: 16060.18
[INFO 2017-06-29 20:15:28,813 main.py:57] epoch 451, training loss: 7231.25, average training loss: 9196.10, base loss: 16057.75
[INFO 2017-06-29 20:15:31,722 main.py:57] epoch 452, training loss: 7894.88, average training loss: 9193.22, base loss: 16058.27
[INFO 2017-06-29 20:15:34,604 main.py:57] epoch 453, training loss: 7624.25, average training loss: 9189.77, base loss: 16055.71
[INFO 2017-06-29 20:15:37,468 main.py:57] epoch 454, training loss: 8037.78, average training loss: 9187.24, base loss: 16059.01
[INFO 2017-06-29 20:15:40,312 main.py:57] epoch 455, training loss: 7546.06, average training loss: 9183.64, base loss: 16057.19
[INFO 2017-06-29 20:15:43,208 main.py:57] epoch 456, training loss: 7797.47, average training loss: 9180.60, base loss: 16056.41
[INFO 2017-06-29 20:15:46,042 main.py:57] epoch 457, training loss: 8240.80, average training loss: 9178.55, base loss: 16060.14
[INFO 2017-06-29 20:15:48,907 main.py:57] epoch 458, training loss: 8270.87, average training loss: 9176.57, base loss: 16063.18
[INFO 2017-06-29 20:15:51,752 main.py:57] epoch 459, training loss: 7762.43, average training loss: 9173.50, base loss: 16063.32
[INFO 2017-06-29 20:15:54,605 main.py:57] epoch 460, training loss: 8208.18, average training loss: 9171.41, base loss: 16065.39
[INFO 2017-06-29 20:15:57,502 main.py:57] epoch 461, training loss: 7363.65, average training loss: 9167.49, base loss: 16062.66
[INFO 2017-06-29 20:16:00,382 main.py:57] epoch 462, training loss: 7592.30, average training loss: 9164.09, base loss: 16061.18
[INFO 2017-06-29 20:16:03,240 main.py:57] epoch 463, training loss: 7759.29, average training loss: 9161.06, base loss: 16060.35
[INFO 2017-06-29 20:16:06,070 main.py:57] epoch 464, training loss: 7320.52, average training loss: 9157.11, base loss: 16060.54
[INFO 2017-06-29 20:16:08,924 main.py:57] epoch 465, training loss: 7836.80, average training loss: 9154.27, base loss: 16061.96
[INFO 2017-06-29 20:16:11,789 main.py:57] epoch 466, training loss: 8490.38, average training loss: 9152.85, base loss: 16065.46
[INFO 2017-06-29 20:16:14,655 main.py:57] epoch 467, training loss: 8218.85, average training loss: 9150.86, base loss: 16070.71
[INFO 2017-06-29 20:16:17,568 main.py:57] epoch 468, training loss: 7812.95, average training loss: 9148.00, base loss: 16072.60
[INFO 2017-06-29 20:16:20,458 main.py:57] epoch 469, training loss: 8231.76, average training loss: 9146.05, base loss: 16070.70
[INFO 2017-06-29 20:16:23,289 main.py:57] epoch 470, training loss: 7697.58, average training loss: 9142.98, base loss: 16069.67
[INFO 2017-06-29 20:16:26,151 main.py:57] epoch 471, training loss: 7534.66, average training loss: 9139.57, base loss: 16067.22
[INFO 2017-06-29 20:16:29,011 main.py:57] epoch 472, training loss: 6927.34, average training loss: 9134.89, base loss: 16063.55
[INFO 2017-06-29 20:16:31,887 main.py:57] epoch 473, training loss: 7472.66, average training loss: 9131.39, base loss: 16061.03
[INFO 2017-06-29 20:16:34,790 main.py:57] epoch 474, training loss: 7731.58, average training loss: 9128.44, base loss: 16060.14
[INFO 2017-06-29 20:16:37,703 main.py:57] epoch 475, training loss: 7874.22, average training loss: 9125.80, base loss: 16059.98
[INFO 2017-06-29 20:16:40,586 main.py:57] epoch 476, training loss: 7620.72, average training loss: 9122.65, base loss: 16058.62
[INFO 2017-06-29 20:16:43,460 main.py:57] epoch 477, training loss: 7644.01, average training loss: 9119.56, base loss: 16057.43
[INFO 2017-06-29 20:16:46,306 main.py:57] epoch 478, training loss: 6770.23, average training loss: 9114.65, base loss: 16053.30
[INFO 2017-06-29 20:16:49,220 main.py:57] epoch 479, training loss: 6975.78, average training loss: 9110.20, base loss: 16049.24
[INFO 2017-06-29 20:16:52,122 main.py:57] epoch 480, training loss: 7583.66, average training loss: 9107.02, base loss: 16048.63
[INFO 2017-06-29 20:16:54,991 main.py:57] epoch 481, training loss: 8291.45, average training loss: 9105.33, base loss: 16052.05
[INFO 2017-06-29 20:16:57,847 main.py:57] epoch 482, training loss: 7294.42, average training loss: 9101.58, base loss: 16049.88
[INFO 2017-06-29 20:17:00,735 main.py:57] epoch 483, training loss: 7466.51, average training loss: 9098.20, base loss: 16047.72
[INFO 2017-06-29 20:17:03,628 main.py:57] epoch 484, training loss: 7464.40, average training loss: 9094.83, base loss: 16047.81
[INFO 2017-06-29 20:17:06,473 main.py:57] epoch 485, training loss: 7130.23, average training loss: 9090.79, base loss: 16046.67
[INFO 2017-06-29 20:17:09,318 main.py:57] epoch 486, training loss: 7606.02, average training loss: 9087.74, base loss: 16046.52
[INFO 2017-06-29 20:17:12,179 main.py:57] epoch 487, training loss: 8199.66, average training loss: 9085.92, base loss: 16048.31
[INFO 2017-06-29 20:17:15,029 main.py:57] epoch 488, training loss: 7767.86, average training loss: 9083.23, base loss: 16049.34
[INFO 2017-06-29 20:17:17,885 main.py:57] epoch 489, training loss: 7097.16, average training loss: 9079.17, base loss: 16046.71
[INFO 2017-06-29 20:17:20,711 main.py:57] epoch 490, training loss: 8627.62, average training loss: 9078.25, base loss: 16051.05
[INFO 2017-06-29 20:17:23,566 main.py:57] epoch 491, training loss: 7474.31, average training loss: 9074.99, base loss: 16050.59
[INFO 2017-06-29 20:17:26,445 main.py:57] epoch 492, training loss: 7295.30, average training loss: 9071.38, base loss: 16047.68
[INFO 2017-06-29 20:17:29,295 main.py:57] epoch 493, training loss: 7463.24, average training loss: 9068.13, base loss: 16046.81
[INFO 2017-06-29 20:17:32,177 main.py:57] epoch 494, training loss: 7972.69, average training loss: 9065.92, base loss: 16047.70
[INFO 2017-06-29 20:17:35,033 main.py:57] epoch 495, training loss: 7134.47, average training loss: 9062.02, base loss: 16043.18
[INFO 2017-06-29 20:17:37,909 main.py:57] epoch 496, training loss: 7222.61, average training loss: 9058.32, base loss: 16038.18
[INFO 2017-06-29 20:17:40,758 main.py:57] epoch 497, training loss: 7089.29, average training loss: 9054.37, base loss: 16036.65
[INFO 2017-06-29 20:17:43,626 main.py:57] epoch 498, training loss: 8142.07, average training loss: 9052.54, base loss: 16038.31
[INFO 2017-06-29 20:17:46,472 main.py:57] epoch 499, training loss: 8599.07, average training loss: 9051.63, base loss: 16042.47
[INFO 2017-06-29 20:17:46,472 main.py:59] epoch 499, testing
[INFO 2017-06-29 20:17:58,832 main.py:104] average testing loss: 7745.16, base loss: 16079.22
[INFO 2017-06-29 20:17:58,832 main.py:105] improve_loss: 8334.06, improve_percent: 0.52
[INFO 2017-06-29 20:17:58,833 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 20:17:58,859 main.py:71] current best improved percent: 0.52
[INFO 2017-06-29 20:18:01,737 main.py:57] epoch 500, training loss: 8474.96, average training loss: 9050.48, base loss: 16044.95
[INFO 2017-06-29 20:18:04,653 main.py:57] epoch 501, training loss: 7468.71, average training loss: 9047.33, base loss: 16042.60
[INFO 2017-06-29 20:18:07,519 main.py:57] epoch 502, training loss: 7753.69, average training loss: 9044.76, base loss: 16041.05
[INFO 2017-06-29 20:18:10,400 main.py:57] epoch 503, training loss: 7376.72, average training loss: 9041.45, base loss: 16035.31
[INFO 2017-06-29 20:18:13,297 main.py:57] epoch 504, training loss: 7936.54, average training loss: 9039.26, base loss: 16035.17
[INFO 2017-06-29 20:18:16,162 main.py:57] epoch 505, training loss: 7868.18, average training loss: 9036.95, base loss: 16033.18
[INFO 2017-06-29 20:18:19,065 main.py:57] epoch 506, training loss: 7290.76, average training loss: 9033.50, base loss: 16032.58
[INFO 2017-06-29 20:18:21,908 main.py:57] epoch 507, training loss: 7801.68, average training loss: 9031.08, base loss: 16036.05
[INFO 2017-06-29 20:18:24,790 main.py:57] epoch 508, training loss: 7444.70, average training loss: 9027.96, base loss: 16034.96
[INFO 2017-06-29 20:18:27,669 main.py:57] epoch 509, training loss: 6709.45, average training loss: 9023.41, base loss: 16030.83
[INFO 2017-06-29 20:18:30,588 main.py:57] epoch 510, training loss: 7214.86, average training loss: 9019.87, base loss: 16027.65
[INFO 2017-06-29 20:18:33,501 main.py:57] epoch 511, training loss: 7898.77, average training loss: 9017.69, base loss: 16027.29
[INFO 2017-06-29 20:18:36,399 main.py:57] epoch 512, training loss: 8296.22, average training loss: 9016.28, base loss: 16029.23
[INFO 2017-06-29 20:18:39,286 main.py:57] epoch 513, training loss: 7498.68, average training loss: 9013.33, base loss: 16029.54
[INFO 2017-06-29 20:18:42,192 main.py:57] epoch 514, training loss: 7578.00, average training loss: 9010.54, base loss: 16030.77
[INFO 2017-06-29 20:18:45,058 main.py:57] epoch 515, training loss: 7461.50, average training loss: 9007.54, base loss: 16031.51
[INFO 2017-06-29 20:18:47,943 main.py:57] epoch 516, training loss: 8014.87, average training loss: 9005.62, base loss: 16034.48
[INFO 2017-06-29 20:18:50,806 main.py:57] epoch 517, training loss: 7358.20, average training loss: 9002.44, base loss: 16031.67
[INFO 2017-06-29 20:18:53,685 main.py:57] epoch 518, training loss: 7707.72, average training loss: 8999.94, base loss: 16033.45
[INFO 2017-06-29 20:18:56,610 main.py:57] epoch 519, training loss: 7290.24, average training loss: 8996.65, base loss: 16034.26
[INFO 2017-06-29 20:18:59,553 main.py:57] epoch 520, training loss: 7522.28, average training loss: 8993.82, base loss: 16034.97
[INFO 2017-06-29 20:19:02,446 main.py:57] epoch 521, training loss: 7977.53, average training loss: 8991.88, base loss: 16036.48
[INFO 2017-06-29 20:19:05,297 main.py:57] epoch 522, training loss: 8543.17, average training loss: 8991.02, base loss: 16041.65
[INFO 2017-06-29 20:19:08,153 main.py:57] epoch 523, training loss: 7242.78, average training loss: 8987.68, base loss: 16042.27
[INFO 2017-06-29 20:19:11,052 main.py:57] epoch 524, training loss: 7846.72, average training loss: 8985.51, base loss: 16041.95
[INFO 2017-06-29 20:19:13,942 main.py:57] epoch 525, training loss: 7196.56, average training loss: 8982.11, base loss: 16038.10
[INFO 2017-06-29 20:19:16,793 main.py:57] epoch 526, training loss: 8039.03, average training loss: 8980.32, base loss: 16042.60
[INFO 2017-06-29 20:19:19,713 main.py:57] epoch 527, training loss: 7591.64, average training loss: 8977.69, base loss: 16044.95
[INFO 2017-06-29 20:19:22,609 main.py:57] epoch 528, training loss: 8800.61, average training loss: 8977.35, base loss: 16049.65
[INFO 2017-06-29 20:19:25,496 main.py:57] epoch 529, training loss: 7582.48, average training loss: 8974.72, base loss: 16046.58
[INFO 2017-06-29 20:19:28,354 main.py:57] epoch 530, training loss: 7498.93, average training loss: 8971.94, base loss: 16047.12
[INFO 2017-06-29 20:19:31,194 main.py:57] epoch 531, training loss: 7799.40, average training loss: 8969.74, base loss: 16049.01
[INFO 2017-06-29 20:19:34,081 main.py:57] epoch 532, training loss: 7352.13, average training loss: 8966.70, base loss: 16047.77
[INFO 2017-06-29 20:19:36,985 main.py:57] epoch 533, training loss: 7086.06, average training loss: 8963.18, base loss: 16047.68
[INFO 2017-06-29 20:19:39,861 main.py:57] epoch 534, training loss: 7410.27, average training loss: 8960.28, base loss: 16046.18
[INFO 2017-06-29 20:19:42,721 main.py:57] epoch 535, training loss: 7340.15, average training loss: 8957.26, base loss: 16045.04
[INFO 2017-06-29 20:19:45,615 main.py:57] epoch 536, training loss: 7203.62, average training loss: 8953.99, base loss: 16045.12
[INFO 2017-06-29 20:19:48,477 main.py:57] epoch 537, training loss: 8207.62, average training loss: 8952.60, base loss: 16045.81
[INFO 2017-06-29 20:19:51,373 main.py:57] epoch 538, training loss: 8214.64, average training loss: 8951.24, base loss: 16047.62
[INFO 2017-06-29 20:19:54,294 main.py:57] epoch 539, training loss: 7173.75, average training loss: 8947.94, base loss: 16045.47
[INFO 2017-06-29 20:19:57,209 main.py:57] epoch 540, training loss: 6692.87, average training loss: 8943.78, base loss: 16041.39
[INFO 2017-06-29 20:20:00,108 main.py:57] epoch 541, training loss: 7775.06, average training loss: 8941.62, base loss: 16042.13
[INFO 2017-06-29 20:20:02,987 main.py:57] epoch 542, training loss: 7144.48, average training loss: 8938.31, base loss: 16038.85
[INFO 2017-06-29 20:20:05,814 main.py:57] epoch 543, training loss: 9046.49, average training loss: 8938.51, base loss: 16043.92
[INFO 2017-06-29 20:20:08,677 main.py:57] epoch 544, training loss: 7320.61, average training loss: 8935.54, base loss: 16044.10
[INFO 2017-06-29 20:20:11,527 main.py:57] epoch 545, training loss: 7344.38, average training loss: 8932.63, base loss: 16044.16
[INFO 2017-06-29 20:20:14,422 main.py:57] epoch 546, training loss: 7516.97, average training loss: 8930.04, base loss: 16045.07
[INFO 2017-06-29 20:20:17,348 main.py:57] epoch 547, training loss: 7009.42, average training loss: 8926.53, base loss: 16042.59
[INFO 2017-06-29 20:20:20,163 main.py:57] epoch 548, training loss: 7751.33, average training loss: 8924.39, base loss: 16040.23
[INFO 2017-06-29 20:20:23,022 main.py:57] epoch 549, training loss: 7856.00, average training loss: 8922.45, base loss: 16038.20
[INFO 2017-06-29 20:20:25,901 main.py:57] epoch 550, training loss: 8094.54, average training loss: 8920.95, base loss: 16040.00
[INFO 2017-06-29 20:20:28,785 main.py:57] epoch 551, training loss: 7283.17, average training loss: 8917.98, base loss: 16037.93
[INFO 2017-06-29 20:20:31,661 main.py:57] epoch 552, training loss: 7265.12, average training loss: 8914.99, base loss: 16035.70
[INFO 2017-06-29 20:20:34,535 main.py:57] epoch 553, training loss: 7206.14, average training loss: 8911.91, base loss: 16032.52
[INFO 2017-06-29 20:20:37,441 main.py:57] epoch 554, training loss: 7358.24, average training loss: 8909.11, base loss: 16031.41
[INFO 2017-06-29 20:20:40,363 main.py:57] epoch 555, training loss: 7311.87, average training loss: 8906.23, base loss: 16028.73
[INFO 2017-06-29 20:20:43,234 main.py:57] epoch 556, training loss: 7691.09, average training loss: 8904.05, base loss: 16029.21
[INFO 2017-06-29 20:20:46,125 main.py:57] epoch 557, training loss: 7779.71, average training loss: 8902.04, base loss: 16031.19
[INFO 2017-06-29 20:20:49,050 main.py:57] epoch 558, training loss: 8071.37, average training loss: 8900.55, base loss: 16033.17
[INFO 2017-06-29 20:20:51,936 main.py:57] epoch 559, training loss: 7228.72, average training loss: 8897.57, base loss: 16029.45
[INFO 2017-06-29 20:20:54,797 main.py:57] epoch 560, training loss: 8128.89, average training loss: 8896.20, base loss: 16034.03
[INFO 2017-06-29 20:20:57,654 main.py:57] epoch 561, training loss: 6973.07, average training loss: 8892.77, base loss: 16032.84
[INFO 2017-06-29 20:21:00,502 main.py:57] epoch 562, training loss: 7764.68, average training loss: 8890.77, base loss: 16035.20
[INFO 2017-06-29 20:21:03,369 main.py:57] epoch 563, training loss: 7217.06, average training loss: 8887.80, base loss: 16033.10
[INFO 2017-06-29 20:21:06,256 main.py:57] epoch 564, training loss: 8130.45, average training loss: 8886.46, base loss: 16034.31
[INFO 2017-06-29 20:21:09,102 main.py:57] epoch 565, training loss: 6761.53, average training loss: 8882.71, base loss: 16029.37
[INFO 2017-06-29 20:21:11,959 main.py:57] epoch 566, training loss: 7621.33, average training loss: 8880.48, base loss: 16028.45
[INFO 2017-06-29 20:21:14,805 main.py:57] epoch 567, training loss: 7892.56, average training loss: 8878.74, base loss: 16028.38
[INFO 2017-06-29 20:21:17,654 main.py:57] epoch 568, training loss: 7653.58, average training loss: 8876.59, base loss: 16026.55
[INFO 2017-06-29 20:21:20,551 main.py:57] epoch 569, training loss: 7786.60, average training loss: 8874.68, base loss: 16026.35
[INFO 2017-06-29 20:21:23,437 main.py:57] epoch 570, training loss: 7314.50, average training loss: 8871.95, base loss: 16026.06
[INFO 2017-06-29 20:21:26,362 main.py:57] epoch 571, training loss: 7755.53, average training loss: 8869.99, base loss: 16026.78
[INFO 2017-06-29 20:21:29,241 main.py:57] epoch 572, training loss: 7937.39, average training loss: 8868.37, base loss: 16026.06
[INFO 2017-06-29 20:21:32,151 main.py:57] epoch 573, training loss: 8392.84, average training loss: 8867.54, base loss: 16029.32
[INFO 2017-06-29 20:21:35,066 main.py:57] epoch 574, training loss: 6592.25, average training loss: 8863.58, base loss: 16026.49
[INFO 2017-06-29 20:21:37,963 main.py:57] epoch 575, training loss: 7194.94, average training loss: 8860.68, base loss: 16025.40
[INFO 2017-06-29 20:21:40,878 main.py:57] epoch 576, training loss: 7830.28, average training loss: 8858.90, base loss: 16024.13
[INFO 2017-06-29 20:21:43,763 main.py:57] epoch 577, training loss: 7267.87, average training loss: 8856.15, base loss: 16020.68
[INFO 2017-06-29 20:21:46,630 main.py:57] epoch 578, training loss: 7485.84, average training loss: 8853.78, base loss: 16019.39
[INFO 2017-06-29 20:21:49,476 main.py:57] epoch 579, training loss: 7065.45, average training loss: 8850.70, base loss: 16017.44
[INFO 2017-06-29 20:21:52,327 main.py:57] epoch 580, training loss: 7184.19, average training loss: 8847.83, base loss: 16014.77
[INFO 2017-06-29 20:21:55,198 main.py:57] epoch 581, training loss: 7429.71, average training loss: 8845.39, base loss: 16011.94
[INFO 2017-06-29 20:21:58,096 main.py:57] epoch 582, training loss: 7707.82, average training loss: 8843.44, base loss: 16012.24
[INFO 2017-06-29 20:22:00,961 main.py:57] epoch 583, training loss: 7073.34, average training loss: 8840.41, base loss: 16008.72
[INFO 2017-06-29 20:22:03,835 main.py:57] epoch 584, training loss: 7959.41, average training loss: 8838.90, base loss: 16012.22
[INFO 2017-06-29 20:22:06,721 main.py:57] epoch 585, training loss: 7236.76, average training loss: 8836.17, base loss: 16011.46
[INFO 2017-06-29 20:22:09,583 main.py:57] epoch 586, training loss: 7153.93, average training loss: 8833.30, base loss: 16010.33
[INFO 2017-06-29 20:22:12,432 main.py:57] epoch 587, training loss: 7255.82, average training loss: 8830.62, base loss: 16009.41
[INFO 2017-06-29 20:22:15,330 main.py:57] epoch 588, training loss: 6901.74, average training loss: 8827.35, base loss: 16007.56
[INFO 2017-06-29 20:22:18,195 main.py:57] epoch 589, training loss: 7538.07, average training loss: 8825.16, base loss: 16007.95
[INFO 2017-06-29 20:22:21,045 main.py:57] epoch 590, training loss: 7894.85, average training loss: 8823.59, base loss: 16008.40
[INFO 2017-06-29 20:22:23,865 main.py:57] epoch 591, training loss: 7805.32, average training loss: 8821.87, base loss: 16009.07
[INFO 2017-06-29 20:22:26,749 main.py:57] epoch 592, training loss: 7965.06, average training loss: 8820.42, base loss: 16012.81
[INFO 2017-06-29 20:22:29,613 main.py:57] epoch 593, training loss: 7456.08, average training loss: 8818.12, base loss: 16015.20
[INFO 2017-06-29 20:22:32,494 main.py:57] epoch 594, training loss: 7957.01, average training loss: 8816.68, base loss: 16016.19
[INFO 2017-06-29 20:22:35,353 main.py:57] epoch 595, training loss: 7950.98, average training loss: 8815.22, base loss: 16016.35
[INFO 2017-06-29 20:22:38,265 main.py:57] epoch 596, training loss: 7242.29, average training loss: 8812.59, base loss: 16014.51
[INFO 2017-06-29 20:22:41,158 main.py:57] epoch 597, training loss: 7247.67, average training loss: 8809.97, base loss: 16012.12
[INFO 2017-06-29 20:22:44,031 main.py:57] epoch 598, training loss: 7763.95, average training loss: 8808.23, base loss: 16014.87
[INFO 2017-06-29 20:22:46,923 main.py:57] epoch 599, training loss: 8144.69, average training loss: 8807.12, base loss: 16015.36
[INFO 2017-06-29 20:22:46,924 main.py:59] epoch 599, testing
[INFO 2017-06-29 20:22:59,063 main.py:104] average testing loss: 7702.87, base loss: 15996.35
[INFO 2017-06-29 20:22:59,063 main.py:105] improve_loss: 8293.48, improve_percent: 0.52
[INFO 2017-06-29 20:22:59,065 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 20:22:59,090 main.py:71] current best improved percent: 0.52
[INFO 2017-06-29 20:23:01,951 main.py:57] epoch 600, training loss: 8339.46, average training loss: 8806.34, base loss: 16017.63
[INFO 2017-06-29 20:23:04,830 main.py:57] epoch 601, training loss: 7427.40, average training loss: 8804.05, base loss: 16015.15
[INFO 2017-06-29 20:23:07,691 main.py:57] epoch 602, training loss: 7379.88, average training loss: 8801.69, base loss: 16015.07
[INFO 2017-06-29 20:23:10,563 main.py:57] epoch 603, training loss: 7057.79, average training loss: 8798.80, base loss: 16015.03
[INFO 2017-06-29 20:23:13,417 main.py:57] epoch 604, training loss: 6821.53, average training loss: 8795.53, base loss: 16011.58
[INFO 2017-06-29 20:23:16,294 main.py:57] epoch 605, training loss: 7126.92, average training loss: 8792.78, base loss: 16009.53
[INFO 2017-06-29 20:23:19,207 main.py:57] epoch 606, training loss: 6626.64, average training loss: 8789.21, base loss: 16006.10
[INFO 2017-06-29 20:23:22,067 main.py:57] epoch 607, training loss: 7992.84, average training loss: 8787.90, base loss: 16007.09
[INFO 2017-06-29 20:23:24,944 main.py:57] epoch 608, training loss: 7309.82, average training loss: 8785.48, base loss: 16007.53
[INFO 2017-06-29 20:23:27,765 main.py:57] epoch 609, training loss: 7028.01, average training loss: 8782.59, base loss: 16004.97
[INFO 2017-06-29 20:23:30,624 main.py:57] epoch 610, training loss: 7922.46, average training loss: 8781.19, base loss: 16005.81
[INFO 2017-06-29 20:23:33,483 main.py:57] epoch 611, training loss: 7908.49, average training loss: 8779.76, base loss: 16006.83
[INFO 2017-06-29 20:23:36,321 main.py:57] epoch 612, training loss: 8143.40, average training loss: 8778.72, base loss: 16008.56
[INFO 2017-06-29 20:23:39,193 main.py:57] epoch 613, training loss: 8026.25, average training loss: 8777.50, base loss: 16013.75
[INFO 2017-06-29 20:23:42,052 main.py:57] epoch 614, training loss: 7693.74, average training loss: 8775.74, base loss: 16014.92
[INFO 2017-06-29 20:23:44,965 main.py:57] epoch 615, training loss: 7227.72, average training loss: 8773.22, base loss: 16013.74
[INFO 2017-06-29 20:23:47,809 main.py:57] epoch 616, training loss: 7384.03, average training loss: 8770.97, base loss: 16015.27
[INFO 2017-06-29 20:23:50,652 main.py:57] epoch 617, training loss: 7806.39, average training loss: 8769.41, base loss: 16018.52
[INFO 2017-06-29 20:23:53,528 main.py:57] epoch 618, training loss: 7252.44, average training loss: 8766.96, base loss: 16017.23
[INFO 2017-06-29 20:23:56,401 main.py:57] epoch 619, training loss: 7302.04, average training loss: 8764.60, base loss: 16017.68
[INFO 2017-06-29 20:23:59,284 main.py:57] epoch 620, training loss: 7596.17, average training loss: 8762.71, base loss: 16017.50
[INFO 2017-06-29 20:24:02,185 main.py:57] epoch 621, training loss: 7488.74, average training loss: 8760.67, base loss: 16016.68
[INFO 2017-06-29 20:24:05,077 main.py:57] epoch 622, training loss: 7699.55, average training loss: 8758.96, base loss: 16017.24
[INFO 2017-06-29 20:24:07,928 main.py:57] epoch 623, training loss: 7052.88, average training loss: 8756.23, base loss: 16015.81
[INFO 2017-06-29 20:24:10,808 main.py:57] epoch 624, training loss: 7858.97, average training loss: 8754.79, base loss: 16017.58
[INFO 2017-06-29 20:24:13,687 main.py:57] epoch 625, training loss: 7680.82, average training loss: 8753.08, base loss: 16020.07
[INFO 2017-06-29 20:24:16,515 main.py:57] epoch 626, training loss: 7479.90, average training loss: 8751.05, base loss: 16020.74
[INFO 2017-06-29 20:24:19,375 main.py:57] epoch 627, training loss: 8068.75, average training loss: 8749.96, base loss: 16025.33
[INFO 2017-06-29 20:24:22,209 main.py:57] epoch 628, training loss: 7054.87, average training loss: 8747.27, base loss: 16024.48
[INFO 2017-06-29 20:24:25,065 main.py:57] epoch 629, training loss: 7414.06, average training loss: 8745.15, base loss: 16024.21
[INFO 2017-06-29 20:24:27,940 main.py:57] epoch 630, training loss: 7067.64, average training loss: 8742.49, base loss: 16021.19
[INFO 2017-06-29 20:24:30,796 main.py:57] epoch 631, training loss: 6884.61, average training loss: 8739.55, base loss: 16016.30
[INFO 2017-06-29 20:24:33,657 main.py:57] epoch 632, training loss: 7849.46, average training loss: 8738.15, base loss: 16016.76
[INFO 2017-06-29 20:24:36,542 main.py:57] epoch 633, training loss: 6960.11, average training loss: 8735.34, base loss: 16015.88
[INFO 2017-06-29 20:24:39,393 main.py:57] epoch 634, training loss: 8105.59, average training loss: 8734.35, base loss: 16020.55
[INFO 2017-06-29 20:24:42,280 main.py:57] epoch 635, training loss: 7815.25, average training loss: 8732.90, base loss: 16023.40
[INFO 2017-06-29 20:24:45,145 main.py:57] epoch 636, training loss: 8928.82, average training loss: 8733.21, base loss: 16028.55
[INFO 2017-06-29 20:24:47,989 main.py:57] epoch 637, training loss: 7703.50, average training loss: 8731.60, base loss: 16027.69
[INFO 2017-06-29 20:24:50,880 main.py:57] epoch 638, training loss: 8161.80, average training loss: 8730.71, base loss: 16029.77
[INFO 2017-06-29 20:24:53,773 main.py:57] epoch 639, training loss: 7341.82, average training loss: 8728.54, base loss: 16029.53
[INFO 2017-06-29 20:24:56,646 main.py:57] epoch 640, training loss: 6922.93, average training loss: 8725.72, base loss: 16024.90
[INFO 2017-06-29 20:24:59,582 main.py:57] epoch 641, training loss: 7090.87, average training loss: 8723.17, base loss: 16023.03
[INFO 2017-06-29 20:25:02,426 main.py:57] epoch 642, training loss: 7879.56, average training loss: 8721.86, base loss: 16024.64
[INFO 2017-06-29 20:25:05,333 main.py:57] epoch 643, training loss: 7840.49, average training loss: 8720.49, base loss: 16025.41
[INFO 2017-06-29 20:25:08,196 main.py:57] epoch 644, training loss: 7438.01, average training loss: 8718.50, base loss: 16025.18
[INFO 2017-06-29 20:25:11,065 main.py:57] epoch 645, training loss: 7962.63, average training loss: 8717.33, base loss: 16027.60
[INFO 2017-06-29 20:25:13,941 main.py:57] epoch 646, training loss: 8042.15, average training loss: 8716.29, base loss: 16028.56
[INFO 2017-06-29 20:25:16,843 main.py:57] epoch 647, training loss: 7553.96, average training loss: 8714.50, base loss: 16026.66
[INFO 2017-06-29 20:25:19,731 main.py:57] epoch 648, training loss: 7411.85, average training loss: 8712.49, base loss: 16026.09
[INFO 2017-06-29 20:25:22,604 main.py:57] epoch 649, training loss: 7202.54, average training loss: 8710.17, base loss: 16023.30
[INFO 2017-06-29 20:25:25,479 main.py:57] epoch 650, training loss: 6961.36, average training loss: 8707.48, base loss: 16021.81
[INFO 2017-06-29 20:25:28,383 main.py:57] epoch 651, training loss: 7539.40, average training loss: 8705.69, base loss: 16022.44
[INFO 2017-06-29 20:25:31,254 main.py:57] epoch 652, training loss: 7756.71, average training loss: 8704.24, base loss: 16022.88
[INFO 2017-06-29 20:25:34,133 main.py:57] epoch 653, training loss: 7920.30, average training loss: 8703.04, base loss: 16024.38
[INFO 2017-06-29 20:25:37,015 main.py:57] epoch 654, training loss: 7038.16, average training loss: 8700.49, base loss: 16021.43
[INFO 2017-06-29 20:25:39,862 main.py:57] epoch 655, training loss: 7605.95, average training loss: 8698.83, base loss: 16020.51
[INFO 2017-06-29 20:25:42,734 main.py:57] epoch 656, training loss: 7255.16, average training loss: 8696.63, base loss: 16019.98
[INFO 2017-06-29 20:25:45,602 main.py:57] epoch 657, training loss: 7878.00, average training loss: 8695.38, base loss: 16021.89
[INFO 2017-06-29 20:25:48,421 main.py:57] epoch 658, training loss: 7188.88, average training loss: 8693.10, base loss: 16019.97
[INFO 2017-06-29 20:25:51,295 main.py:57] epoch 659, training loss: 7311.32, average training loss: 8691.00, base loss: 16016.90
[INFO 2017-06-29 20:25:54,185 main.py:57] epoch 660, training loss: 7047.05, average training loss: 8688.52, base loss: 16016.21
[INFO 2017-06-29 20:25:57,044 main.py:57] epoch 661, training loss: 7488.70, average training loss: 8686.71, base loss: 16015.69
[INFO 2017-06-29 20:25:59,931 main.py:57] epoch 662, training loss: 7871.90, average training loss: 8685.48, base loss: 16015.05
[INFO 2017-06-29 20:26:02,770 main.py:57] epoch 663, training loss: 7009.58, average training loss: 8682.95, base loss: 16011.86
[INFO 2017-06-29 20:26:05,635 main.py:57] epoch 664, training loss: 7869.39, average training loss: 8681.73, base loss: 16011.70
[INFO 2017-06-29 20:26:08,544 main.py:57] epoch 665, training loss: 8205.19, average training loss: 8681.01, base loss: 16014.69
[INFO 2017-06-29 20:26:11,410 main.py:57] epoch 666, training loss: 7253.63, average training loss: 8678.87, base loss: 16012.89
[INFO 2017-06-29 20:26:14,343 main.py:57] epoch 667, training loss: 7013.74, average training loss: 8676.38, base loss: 16009.72
[INFO 2017-06-29 20:26:17,182 main.py:57] epoch 668, training loss: 7628.30, average training loss: 8674.81, base loss: 16012.69
[INFO 2017-06-29 20:26:20,073 main.py:57] epoch 669, training loss: 7576.73, average training loss: 8673.18, base loss: 16012.96
[INFO 2017-06-29 20:26:22,965 main.py:57] epoch 670, training loss: 6771.96, average training loss: 8670.34, base loss: 16010.22
[INFO 2017-06-29 20:26:25,821 main.py:57] epoch 671, training loss: 7510.21, average training loss: 8668.62, base loss: 16009.20
[INFO 2017-06-29 20:26:28,669 main.py:57] epoch 672, training loss: 6939.30, average training loss: 8666.05, base loss: 16005.43
[INFO 2017-06-29 20:26:31,513 main.py:57] epoch 673, training loss: 7712.08, average training loss: 8664.63, base loss: 16005.64
[INFO 2017-06-29 20:26:34,404 main.py:57] epoch 674, training loss: 7938.07, average training loss: 8663.55, base loss: 16006.96
[INFO 2017-06-29 20:26:37,296 main.py:57] epoch 675, training loss: 7241.41, average training loss: 8661.45, base loss: 16006.07
[INFO 2017-06-29 20:26:40,147 main.py:57] epoch 676, training loss: 7474.78, average training loss: 8659.70, base loss: 16005.03
[INFO 2017-06-29 20:26:43,029 main.py:57] epoch 677, training loss: 8510.87, average training loss: 8659.48, base loss: 16008.87
[INFO 2017-06-29 20:26:45,897 main.py:57] epoch 678, training loss: 7528.29, average training loss: 8657.81, base loss: 16007.38
[INFO 2017-06-29 20:26:48,720 main.py:57] epoch 679, training loss: 6996.72, average training loss: 8655.37, base loss: 16001.36
[INFO 2017-06-29 20:26:51,558 main.py:57] epoch 680, training loss: 7737.80, average training loss: 8654.02, base loss: 16003.90
[INFO 2017-06-29 20:26:54,416 main.py:57] epoch 681, training loss: 7291.43, average training loss: 8652.02, base loss: 16004.78
[INFO 2017-06-29 20:26:57,273 main.py:57] epoch 682, training loss: 7173.58, average training loss: 8649.86, base loss: 16004.82
[INFO 2017-06-29 20:27:00,124 main.py:57] epoch 683, training loss: 6894.71, average training loss: 8647.29, base loss: 16003.30
[INFO 2017-06-29 20:27:02,998 main.py:57] epoch 684, training loss: 8437.69, average training loss: 8646.99, base loss: 16009.26
[INFO 2017-06-29 20:27:05,871 main.py:57] epoch 685, training loss: 7231.34, average training loss: 8644.92, base loss: 16009.83
[INFO 2017-06-29 20:27:08,716 main.py:57] epoch 686, training loss: 7377.72, average training loss: 8643.08, base loss: 16008.52
[INFO 2017-06-29 20:27:11,580 main.py:57] epoch 687, training loss: 7735.70, average training loss: 8641.76, base loss: 16011.21
[INFO 2017-06-29 20:27:14,434 main.py:57] epoch 688, training loss: 8023.58, average training loss: 8640.86, base loss: 16012.91
[INFO 2017-06-29 20:27:17,328 main.py:57] epoch 689, training loss: 7186.29, average training loss: 8638.76, base loss: 16012.90
[INFO 2017-06-29 20:27:20,201 main.py:57] epoch 690, training loss: 6902.85, average training loss: 8636.24, base loss: 16014.43
[INFO 2017-06-29 20:27:23,050 main.py:57] epoch 691, training loss: 7631.47, average training loss: 8634.79, base loss: 16017.83
[INFO 2017-06-29 20:27:25,929 main.py:57] epoch 692, training loss: 6886.03, average training loss: 8632.27, base loss: 16015.31
[INFO 2017-06-29 20:27:28,793 main.py:57] epoch 693, training loss: 9102.00, average training loss: 8632.94, base loss: 16023.59
[INFO 2017-06-29 20:27:31,645 main.py:57] epoch 694, training loss: 6557.05, average training loss: 8629.96, base loss: 16020.53
[INFO 2017-06-29 20:27:34,566 main.py:57] epoch 695, training loss: 7601.02, average training loss: 8628.48, base loss: 16021.83
[INFO 2017-06-29 20:27:37,474 main.py:57] epoch 696, training loss: 8074.93, average training loss: 8627.68, base loss: 16025.35
[INFO 2017-06-29 20:27:40,334 main.py:57] epoch 697, training loss: 6833.54, average training loss: 8625.11, base loss: 16022.73
[INFO 2017-06-29 20:27:43,218 main.py:57] epoch 698, training loss: 7830.89, average training loss: 8623.98, base loss: 16025.96
[INFO 2017-06-29 20:27:46,046 main.py:57] epoch 699, training loss: 6804.16, average training loss: 8621.38, base loss: 16023.39
[INFO 2017-06-29 20:27:46,046 main.py:59] epoch 699, testing
[INFO 2017-06-29 20:27:58,384 main.py:104] average testing loss: 7314.88, base loss: 15695.76
[INFO 2017-06-29 20:27:58,384 main.py:105] improve_loss: 8380.88, improve_percent: 0.53
[INFO 2017-06-29 20:27:58,386 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 20:27:58,411 main.py:71] current best improved percent: 0.53
[INFO 2017-06-29 20:28:01,281 main.py:57] epoch 700, training loss: 7959.13, average training loss: 8620.43, base loss: 16026.26
[INFO 2017-06-29 20:28:04,165 main.py:57] epoch 701, training loss: 7869.25, average training loss: 8619.36, base loss: 16028.96
[INFO 2017-06-29 20:28:07,019 main.py:57] epoch 702, training loss: 7500.33, average training loss: 8617.77, base loss: 16028.84
[INFO 2017-06-29 20:28:09,908 main.py:57] epoch 703, training loss: 7297.06, average training loss: 8615.90, base loss: 16027.05
[INFO 2017-06-29 20:28:12,810 main.py:57] epoch 704, training loss: 7432.49, average training loss: 8614.22, base loss: 16026.17
[INFO 2017-06-29 20:28:15,632 main.py:57] epoch 705, training loss: 6777.39, average training loss: 8611.62, base loss: 16021.84
[INFO 2017-06-29 20:28:18,522 main.py:57] epoch 706, training loss: 7411.43, average training loss: 8609.92, base loss: 16020.56
[INFO 2017-06-29 20:28:21,445 main.py:57] epoch 707, training loss: 7352.83, average training loss: 8608.14, base loss: 16018.15
[INFO 2017-06-29 20:28:24,291 main.py:57] epoch 708, training loss: 7855.88, average training loss: 8607.08, base loss: 16017.92
[INFO 2017-06-29 20:28:27,122 main.py:57] epoch 709, training loss: 7213.60, average training loss: 8605.12, base loss: 16016.69
[INFO 2017-06-29 20:28:30,001 main.py:57] epoch 710, training loss: 7200.28, average training loss: 8603.14, base loss: 16015.92
[INFO 2017-06-29 20:28:32,847 main.py:57] epoch 711, training loss: 7651.58, average training loss: 8601.81, base loss: 16018.43
[INFO 2017-06-29 20:28:35,693 main.py:57] epoch 712, training loss: 7744.16, average training loss: 8600.60, base loss: 16020.38
[INFO 2017-06-29 20:28:38,593 main.py:57] epoch 713, training loss: 7860.41, average training loss: 8599.57, base loss: 16023.79
[INFO 2017-06-29 20:28:41,458 main.py:57] epoch 714, training loss: 7691.86, average training loss: 8598.30, base loss: 16023.69
[INFO 2017-06-29 20:28:44,325 main.py:57] epoch 715, training loss: 7014.79, average training loss: 8596.09, base loss: 16021.21
[INFO 2017-06-29 20:28:47,240 main.py:57] epoch 716, training loss: 7836.08, average training loss: 8595.03, base loss: 16022.48
[INFO 2017-06-29 20:28:50,103 main.py:57] epoch 717, training loss: 6867.16, average training loss: 8592.62, base loss: 16020.06
[INFO 2017-06-29 20:28:52,946 main.py:57] epoch 718, training loss: 7493.69, average training loss: 8591.09, base loss: 16020.84
[INFO 2017-06-29 20:28:55,844 main.py:57] epoch 719, training loss: 7171.17, average training loss: 8589.12, base loss: 16018.72
[INFO 2017-06-29 20:28:58,703 main.py:57] epoch 720, training loss: 7523.50, average training loss: 8587.64, base loss: 16019.31
[INFO 2017-06-29 20:29:01,552 main.py:57] epoch 721, training loss: 7553.31, average training loss: 8586.21, base loss: 16018.98
[INFO 2017-06-29 20:29:04,395 main.py:57] epoch 722, training loss: 7936.23, average training loss: 8585.31, base loss: 16021.11
[INFO 2017-06-29 20:29:07,285 main.py:57] epoch 723, training loss: 7990.14, average training loss: 8584.49, base loss: 16022.82
[INFO 2017-06-29 20:29:10,136 main.py:57] epoch 724, training loss: 7095.06, average training loss: 8582.43, base loss: 16020.02
[INFO 2017-06-29 20:29:13,014 main.py:57] epoch 725, training loss: 7547.64, average training loss: 8581.01, base loss: 16019.53
[INFO 2017-06-29 20:29:15,887 main.py:57] epoch 726, training loss: 7779.93, average training loss: 8579.91, base loss: 16020.44
[INFO 2017-06-29 20:29:18,793 main.py:57] epoch 727, training loss: 7596.68, average training loss: 8578.55, base loss: 16020.84
[INFO 2017-06-29 20:29:21,718 main.py:57] epoch 728, training loss: 8084.16, average training loss: 8577.88, base loss: 16024.86
[INFO 2017-06-29 20:29:24,597 main.py:57] epoch 729, training loss: 8322.77, average training loss: 8577.53, base loss: 16029.18
[INFO 2017-06-29 20:29:27,452 main.py:57] epoch 730, training loss: 7168.80, average training loss: 8575.60, base loss: 16028.99
[INFO 2017-06-29 20:29:30,320 main.py:57] epoch 731, training loss: 7962.15, average training loss: 8574.76, base loss: 16031.88
[INFO 2017-06-29 20:29:33,208 main.py:57] epoch 732, training loss: 6555.21, average training loss: 8572.01, base loss: 16027.46
[INFO 2017-06-29 20:29:36,059 main.py:57] epoch 733, training loss: 7178.18, average training loss: 8570.11, base loss: 16023.50
[INFO 2017-06-29 20:29:38,919 main.py:57] epoch 734, training loss: 7072.73, average training loss: 8568.07, base loss: 16020.64
[INFO 2017-06-29 20:29:41,794 main.py:57] epoch 735, training loss: 8323.59, average training loss: 8567.74, base loss: 16023.69
[INFO 2017-06-29 20:29:44,684 main.py:57] epoch 736, training loss: 6983.75, average training loss: 8565.59, base loss: 16022.70
[INFO 2017-06-29 20:29:47,564 main.py:57] epoch 737, training loss: 6338.88, average training loss: 8562.57, base loss: 16017.27
[INFO 2017-06-29 20:29:50,422 main.py:57] epoch 738, training loss: 7176.36, average training loss: 8560.70, base loss: 16016.70
[INFO 2017-06-29 20:29:53,322 main.py:57] epoch 739, training loss: 7057.61, average training loss: 8558.67, base loss: 16013.76
[INFO 2017-06-29 20:29:56,156 main.py:57] epoch 740, training loss: 7307.67, average training loss: 8556.98, base loss: 16014.78
[INFO 2017-06-29 20:29:59,025 main.py:57] epoch 741, training loss: 7318.58, average training loss: 8555.31, base loss: 16015.82
[INFO 2017-06-29 20:30:01,951 main.py:57] epoch 742, training loss: 6716.09, average training loss: 8552.83, base loss: 16012.57
[INFO 2017-06-29 20:30:04,815 main.py:57] epoch 743, training loss: 6623.75, average training loss: 8550.24, base loss: 16009.35
[INFO 2017-06-29 20:30:07,672 main.py:57] epoch 744, training loss: 7696.40, average training loss: 8549.09, base loss: 16010.64
[INFO 2017-06-29 20:30:10,554 main.py:57] epoch 745, training loss: 7443.04, average training loss: 8547.61, base loss: 16008.53
[INFO 2017-06-29 20:30:13,389 main.py:57] epoch 746, training loss: 7234.19, average training loss: 8545.85, base loss: 16006.07
[INFO 2017-06-29 20:30:16,253 main.py:57] epoch 747, training loss: 6878.64, average training loss: 8543.62, base loss: 16001.98
[INFO 2017-06-29 20:30:19,179 main.py:57] epoch 748, training loss: 6460.99, average training loss: 8540.84, base loss: 15997.94
[INFO 2017-06-29 20:30:22,060 main.py:57] epoch 749, training loss: 6704.09, average training loss: 8538.39, base loss: 15993.46
[INFO 2017-06-29 20:30:24,933 main.py:57] epoch 750, training loss: 8058.28, average training loss: 8537.75, base loss: 15996.15
[INFO 2017-06-29 20:30:27,882 main.py:57] epoch 751, training loss: 7033.99, average training loss: 8535.76, base loss: 15995.91
[INFO 2017-06-29 20:30:30,753 main.py:57] epoch 752, training loss: 7906.28, average training loss: 8534.92, base loss: 15999.29
[INFO 2017-06-29 20:30:33,653 main.py:57] epoch 753, training loss: 7520.23, average training loss: 8533.57, base loss: 16000.64
[INFO 2017-06-29 20:30:36,540 main.py:57] epoch 754, training loss: 7901.37, average training loss: 8532.74, base loss: 15999.74
[INFO 2017-06-29 20:30:39,399 main.py:57] epoch 755, training loss: 7148.83, average training loss: 8530.91, base loss: 15998.10
[INFO 2017-06-29 20:30:42,281 main.py:57] epoch 756, training loss: 7912.99, average training loss: 8530.09, base loss: 16000.86
[INFO 2017-06-29 20:30:45,188 main.py:57] epoch 757, training loss: 8432.61, average training loss: 8529.96, base loss: 16005.65
[INFO 2017-06-29 20:30:48,086 main.py:57] epoch 758, training loss: 7636.64, average training loss: 8528.78, base loss: 16008.56
[INFO 2017-06-29 20:30:50,942 main.py:57] epoch 759, training loss: 8161.45, average training loss: 8528.30, base loss: 16011.90
[INFO 2017-06-29 20:30:53,827 main.py:57] epoch 760, training loss: 7217.56, average training loss: 8526.58, base loss: 16010.53
[INFO 2017-06-29 20:30:56,745 main.py:57] epoch 761, training loss: 7032.38, average training loss: 8524.62, base loss: 16009.33
[INFO 2017-06-29 20:30:59,629 main.py:57] epoch 762, training loss: 7333.63, average training loss: 8523.06, base loss: 16010.36
[INFO 2017-06-29 20:31:02,479 main.py:57] epoch 763, training loss: 6962.81, average training loss: 8521.01, base loss: 16008.29
[INFO 2017-06-29 20:31:05,362 main.py:57] epoch 764, training loss: 7717.65, average training loss: 8519.96, base loss: 16008.69
[INFO 2017-06-29 20:31:08,229 main.py:57] epoch 765, training loss: 7574.14, average training loss: 8518.73, base loss: 16009.76
[INFO 2017-06-29 20:31:11,113 main.py:57] epoch 766, training loss: 7966.80, average training loss: 8518.01, base loss: 16011.67
[INFO 2017-06-29 20:31:14,012 main.py:57] epoch 767, training loss: 7533.56, average training loss: 8516.73, base loss: 16011.73
[INFO 2017-06-29 20:31:16,919 main.py:57] epoch 768, training loss: 7566.22, average training loss: 8515.49, base loss: 16013.72
[INFO 2017-06-29 20:31:19,783 main.py:57] epoch 769, training loss: 7163.70, average training loss: 8513.74, base loss: 16013.56
[INFO 2017-06-29 20:31:22,694 main.py:57] epoch 770, training loss: 7888.67, average training loss: 8512.93, base loss: 16015.24
[INFO 2017-06-29 20:31:25,578 main.py:57] epoch 771, training loss: 6887.15, average training loss: 8510.82, base loss: 16013.02
[INFO 2017-06-29 20:31:28,453 main.py:57] epoch 772, training loss: 7166.74, average training loss: 8509.08, base loss: 16013.75
[INFO 2017-06-29 20:31:31,339 main.py:57] epoch 773, training loss: 7443.57, average training loss: 8507.70, base loss: 16015.12
[INFO 2017-06-29 20:31:34,246 main.py:57] epoch 774, training loss: 7499.35, average training loss: 8506.40, base loss: 16015.86
[INFO 2017-06-29 20:31:37,119 main.py:57] epoch 775, training loss: 7470.52, average training loss: 8505.07, base loss: 16017.01
[INFO 2017-06-29 20:31:39,998 main.py:57] epoch 776, training loss: 7308.07, average training loss: 8503.53, base loss: 16016.63
[INFO 2017-06-29 20:31:42,868 main.py:57] epoch 777, training loss: 7483.31, average training loss: 8502.22, base loss: 16016.13
[INFO 2017-06-29 20:31:45,759 main.py:57] epoch 778, training loss: 7523.47, average training loss: 8500.96, base loss: 16017.24
[INFO 2017-06-29 20:31:48,646 main.py:57] epoch 779, training loss: 7599.23, average training loss: 8499.80, base loss: 16018.85
[INFO 2017-06-29 20:31:51,523 main.py:57] epoch 780, training loss: 7144.23, average training loss: 8498.07, base loss: 16019.84
[INFO 2017-06-29 20:31:54,434 main.py:57] epoch 781, training loss: 7458.68, average training loss: 8496.74, base loss: 16020.07
[INFO 2017-06-29 20:31:57,262 main.py:57] epoch 782, training loss: 7222.17, average training loss: 8495.11, base loss: 16020.79
[INFO 2017-06-29 20:32:00,109 main.py:57] epoch 783, training loss: 7305.27, average training loss: 8493.59, base loss: 16020.03
[INFO 2017-06-29 20:32:02,966 main.py:57] epoch 784, training loss: 7193.15, average training loss: 8491.94, base loss: 16018.28
[INFO 2017-06-29 20:32:05,884 main.py:57] epoch 785, training loss: 7891.00, average training loss: 8491.17, base loss: 16020.07
[INFO 2017-06-29 20:32:08,732 main.py:57] epoch 786, training loss: 7813.18, average training loss: 8490.31, base loss: 16020.83
[INFO 2017-06-29 20:32:11,602 main.py:57] epoch 787, training loss: 7186.78, average training loss: 8488.66, base loss: 16019.47
[INFO 2017-06-29 20:32:14,471 main.py:57] epoch 788, training loss: 7801.93, average training loss: 8487.79, base loss: 16021.10
[INFO 2017-06-29 20:32:17,356 main.py:57] epoch 789, training loss: 7148.23, average training loss: 8486.09, base loss: 16020.53
[INFO 2017-06-29 20:32:20,217 main.py:57] epoch 790, training loss: 7075.33, average training loss: 8484.31, base loss: 16021.30
[INFO 2017-06-29 20:32:23,112 main.py:57] epoch 791, training loss: 7356.96, average training loss: 8482.88, base loss: 16025.96
[INFO 2017-06-29 20:32:25,982 main.py:57] epoch 792, training loss: 7335.75, average training loss: 8481.44, base loss: 16025.95
[INFO 2017-06-29 20:32:28,838 main.py:57] epoch 793, training loss: 7677.67, average training loss: 8480.42, base loss: 16028.88
[INFO 2017-06-29 20:32:31,742 main.py:57] epoch 794, training loss: 7130.08, average training loss: 8478.73, base loss: 16027.06
[INFO 2017-06-29 20:32:34,609 main.py:57] epoch 795, training loss: 7367.42, average training loss: 8477.33, base loss: 16025.98
[INFO 2017-06-29 20:32:37,461 main.py:57] epoch 796, training loss: 7668.92, average training loss: 8476.32, base loss: 16028.57
[INFO 2017-06-29 20:32:40,319 main.py:57] epoch 797, training loss: 7166.94, average training loss: 8474.67, base loss: 16030.04
[INFO 2017-06-29 20:32:43,165 main.py:57] epoch 798, training loss: 6485.17, average training loss: 8472.18, base loss: 16028.54
[INFO 2017-06-29 20:32:46,058 main.py:57] epoch 799, training loss: 7026.40, average training loss: 8470.38, base loss: 16030.11
[INFO 2017-06-29 20:32:46,059 main.py:59] epoch 799, testing
[INFO 2017-06-29 20:32:58,359 main.py:104] average testing loss: 7305.03, base loss: 15712.88
[INFO 2017-06-29 20:32:58,359 main.py:105] improve_loss: 8407.85, improve_percent: 0.54
[INFO 2017-06-29 20:32:58,362 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 20:32:58,387 main.py:71] current best improved percent: 0.54
[INFO 2017-06-29 20:33:01,242 main.py:57] epoch 800, training loss: 6717.59, average training loss: 8468.19, base loss: 16026.91
[INFO 2017-06-29 20:33:04,117 main.py:57] epoch 801, training loss: 7661.67, average training loss: 8467.18, base loss: 16027.33
[INFO 2017-06-29 20:33:07,052 main.py:57] epoch 802, training loss: 7872.98, average training loss: 8466.44, base loss: 16029.23
[INFO 2017-06-29 20:33:09,912 main.py:57] epoch 803, training loss: 7559.75, average training loss: 8465.32, base loss: 16029.31
[INFO 2017-06-29 20:33:12,798 main.py:57] epoch 804, training loss: 7245.76, average training loss: 8463.80, base loss: 16029.26
[INFO 2017-06-29 20:33:15,660 main.py:57] epoch 805, training loss: 7533.83, average training loss: 8462.65, base loss: 16030.23
[INFO 2017-06-29 20:33:18,545 main.py:57] epoch 806, training loss: 8241.52, average training loss: 8462.37, base loss: 16033.31
[INFO 2017-06-29 20:33:21,440 main.py:57] epoch 807, training loss: 6721.00, average training loss: 8460.22, base loss: 16030.92
[INFO 2017-06-29 20:33:24,354 main.py:57] epoch 808, training loss: 6575.30, average training loss: 8457.89, base loss: 16028.14
[INFO 2017-06-29 20:33:27,199 main.py:57] epoch 809, training loss: 7085.82, average training loss: 8456.19, base loss: 16026.95
[INFO 2017-06-29 20:33:30,065 main.py:57] epoch 810, training loss: 6799.26, average training loss: 8454.15, base loss: 16025.21
[INFO 2017-06-29 20:33:32,932 main.py:57] epoch 811, training loss: 7295.55, average training loss: 8452.72, base loss: 16024.16
[INFO 2017-06-29 20:33:35,810 main.py:57] epoch 812, training loss: 6830.85, average training loss: 8450.73, base loss: 16021.59
[INFO 2017-06-29 20:33:38,671 main.py:57] epoch 813, training loss: 7449.55, average training loss: 8449.50, base loss: 16021.94
[INFO 2017-06-29 20:33:41,540 main.py:57] epoch 814, training loss: 7111.19, average training loss: 8447.86, base loss: 16021.42
[INFO 2017-06-29 20:33:44,411 main.py:57] epoch 815, training loss: 7926.19, average training loss: 8447.22, base loss: 16022.92
[INFO 2017-06-29 20:33:47,265 main.py:57] epoch 816, training loss: 7268.44, average training loss: 8445.78, base loss: 16020.99
[INFO 2017-06-29 20:33:50,139 main.py:57] epoch 817, training loss: 7486.99, average training loss: 8444.60, base loss: 16019.24
[INFO 2017-06-29 20:33:52,998 main.py:57] epoch 818, training loss: 6873.88, average training loss: 8442.69, base loss: 16018.20
[INFO 2017-06-29 20:33:55,865 main.py:57] epoch 819, training loss: 7296.08, average training loss: 8441.29, base loss: 16017.57
[INFO 2017-06-29 20:33:58,740 main.py:57] epoch 820, training loss: 7115.63, average training loss: 8439.67, base loss: 16017.21
[INFO 2017-06-29 20:34:01,582 main.py:57] epoch 821, training loss: 7127.82, average training loss: 8438.08, base loss: 16017.97
[INFO 2017-06-29 20:34:04,419 main.py:57] epoch 822, training loss: 7179.39, average training loss: 8436.55, base loss: 16017.21
[INFO 2017-06-29 20:34:07,305 main.py:57] epoch 823, training loss: 8224.39, average training loss: 8436.29, base loss: 16019.12
[INFO 2017-06-29 20:34:10,201 main.py:57] epoch 824, training loss: 7198.47, average training loss: 8434.79, base loss: 16018.17
[INFO 2017-06-29 20:34:13,104 main.py:57] epoch 825, training loss: 7157.85, average training loss: 8433.24, base loss: 16017.24
[INFO 2017-06-29 20:34:15,960 main.py:57] epoch 826, training loss: 7566.85, average training loss: 8432.20, base loss: 16018.38
[INFO 2017-06-29 20:34:18,849 main.py:57] epoch 827, training loss: 7332.77, average training loss: 8430.87, base loss: 16016.76
[INFO 2017-06-29 20:34:21,720 main.py:57] epoch 828, training loss: 7720.05, average training loss: 8430.01, base loss: 16017.95
[INFO 2017-06-29 20:34:24,565 main.py:57] epoch 829, training loss: 7436.22, average training loss: 8428.81, base loss: 16018.04
[INFO 2017-06-29 20:34:27,499 main.py:57] epoch 830, training loss: 7075.27, average training loss: 8427.18, base loss: 16016.92
[INFO 2017-06-29 20:34:30,365 main.py:57] epoch 831, training loss: 7510.96, average training loss: 8426.08, base loss: 16017.85
[INFO 2017-06-29 20:34:33,236 main.py:57] epoch 832, training loss: 7854.85, average training loss: 8425.40, base loss: 16018.25
[INFO 2017-06-29 20:34:36,157 main.py:57] epoch 833, training loss: 7101.16, average training loss: 8423.81, base loss: 16016.33
[INFO 2017-06-29 20:34:39,001 main.py:57] epoch 834, training loss: 7943.47, average training loss: 8423.23, base loss: 16016.69
[INFO 2017-06-29 20:34:41,861 main.py:57] epoch 835, training loss: 6866.93, average training loss: 8421.37, base loss: 16014.34
[INFO 2017-06-29 20:34:44,732 main.py:57] epoch 836, training loss: 6693.88, average training loss: 8419.31, base loss: 16012.56
[INFO 2017-06-29 20:34:47,565 main.py:57] epoch 837, training loss: 7348.58, average training loss: 8418.03, base loss: 16012.58
[INFO 2017-06-29 20:34:50,437 main.py:57] epoch 838, training loss: 7622.93, average training loss: 8417.08, base loss: 16013.88
[INFO 2017-06-29 20:34:53,317 main.py:57] epoch 839, training loss: 6667.71, average training loss: 8415.00, base loss: 16011.24
[INFO 2017-06-29 20:34:56,190 main.py:57] epoch 840, training loss: 7283.39, average training loss: 8413.65, base loss: 16011.88
[INFO 2017-06-29 20:34:59,098 main.py:57] epoch 841, training loss: 7422.75, average training loss: 8412.48, base loss: 16012.45
[INFO 2017-06-29 20:35:02,011 main.py:57] epoch 842, training loss: 6356.75, average training loss: 8410.04, base loss: 16010.44
[INFO 2017-06-29 20:35:04,864 main.py:57] epoch 843, training loss: 7510.11, average training loss: 8408.97, base loss: 16012.39
[INFO 2017-06-29 20:35:07,737 main.py:57] epoch 844, training loss: 7250.88, average training loss: 8407.60, base loss: 16013.27
[INFO 2017-06-29 20:35:10,602 main.py:57] epoch 845, training loss: 7420.28, average training loss: 8406.44, base loss: 16012.59
[INFO 2017-06-29 20:35:13,486 main.py:57] epoch 846, training loss: 7565.16, average training loss: 8405.44, base loss: 16015.11
[INFO 2017-06-29 20:35:16,371 main.py:57] epoch 847, training loss: 7448.30, average training loss: 8404.31, base loss: 16016.51
[INFO 2017-06-29 20:35:19,243 main.py:57] epoch 848, training loss: 7365.61, average training loss: 8403.09, base loss: 16019.51
[INFO 2017-06-29 20:35:22,074 main.py:57] epoch 849, training loss: 7546.12, average training loss: 8402.08, base loss: 16023.60
[INFO 2017-06-29 20:35:24,913 main.py:57] epoch 850, training loss: 7806.99, average training loss: 8401.38, base loss: 16023.57
[INFO 2017-06-29 20:35:27,768 main.py:57] epoch 851, training loss: 8025.22, average training loss: 8400.94, base loss: 16025.71
[INFO 2017-06-29 20:35:30,634 main.py:57] epoch 852, training loss: 7265.44, average training loss: 8399.61, base loss: 16027.15
[INFO 2017-06-29 20:35:33,523 main.py:57] epoch 853, training loss: 7141.81, average training loss: 8398.14, base loss: 16026.14
[INFO 2017-06-29 20:35:36,402 main.py:57] epoch 854, training loss: 7063.72, average training loss: 8396.58, base loss: 16027.00
[INFO 2017-06-29 20:35:39,317 main.py:57] epoch 855, training loss: 7067.54, average training loss: 8395.02, base loss: 16025.10
[INFO 2017-06-29 20:35:42,186 main.py:57] epoch 856, training loss: 7114.62, average training loss: 8393.53, base loss: 16026.48
[INFO 2017-06-29 20:35:45,052 main.py:57] epoch 857, training loss: 7577.32, average training loss: 8392.58, base loss: 16028.94
[INFO 2017-06-29 20:35:47,938 main.py:57] epoch 858, training loss: 7107.56, average training loss: 8391.08, base loss: 16028.55
[INFO 2017-06-29 20:35:50,817 main.py:57] epoch 859, training loss: 6955.73, average training loss: 8389.41, base loss: 16025.91
[INFO 2017-06-29 20:35:53,688 main.py:57] epoch 860, training loss: 7203.09, average training loss: 8388.04, base loss: 16025.85
[INFO 2017-06-29 20:35:56,546 main.py:57] epoch 861, training loss: 7266.88, average training loss: 8386.74, base loss: 16026.21
[INFO 2017-06-29 20:35:59,357 main.py:57] epoch 862, training loss: 7675.94, average training loss: 8385.91, base loss: 16028.04
[INFO 2017-06-29 20:36:02,258 main.py:57] epoch 863, training loss: 6898.30, average training loss: 8384.19, base loss: 16025.90
[INFO 2017-06-29 20:36:05,168 main.py:57] epoch 864, training loss: 7389.75, average training loss: 8383.04, base loss: 16027.00
[INFO 2017-06-29 20:36:08,039 main.py:57] epoch 865, training loss: 7299.71, average training loss: 8381.79, base loss: 16029.02
[INFO 2017-06-29 20:36:10,928 main.py:57] epoch 866, training loss: 6924.65, average training loss: 8380.11, base loss: 16028.21
[INFO 2017-06-29 20:36:13,804 main.py:57] epoch 867, training loss: 7600.32, average training loss: 8379.21, base loss: 16028.92
[INFO 2017-06-29 20:36:16,653 main.py:57] epoch 868, training loss: 7003.29, average training loss: 8377.63, base loss: 16028.54
[INFO 2017-06-29 20:36:19,476 main.py:57] epoch 869, training loss: 6974.10, average training loss: 8376.01, base loss: 16028.94
[INFO 2017-06-29 20:36:22,363 main.py:57] epoch 870, training loss: 6998.32, average training loss: 8374.43, base loss: 16028.70
[INFO 2017-06-29 20:36:25,225 main.py:57] epoch 871, training loss: 7151.71, average training loss: 8373.03, base loss: 16027.21
[INFO 2017-06-29 20:36:28,073 main.py:57] epoch 872, training loss: 7937.00, average training loss: 8372.53, base loss: 16030.40
[INFO 2017-06-29 20:36:30,939 main.py:57] epoch 873, training loss: 6818.49, average training loss: 8370.75, base loss: 16029.98
[INFO 2017-06-29 20:36:33,835 main.py:57] epoch 874, training loss: 7669.59, average training loss: 8369.95, base loss: 16030.21
[INFO 2017-06-29 20:36:36,678 main.py:57] epoch 875, training loss: 6644.08, average training loss: 8367.98, base loss: 16029.84
[INFO 2017-06-29 20:36:39,586 main.py:57] epoch 876, training loss: 7161.32, average training loss: 8366.60, base loss: 16029.86
[INFO 2017-06-29 20:36:42,475 main.py:57] epoch 877, training loss: 6838.46, average training loss: 8364.86, base loss: 16026.63
[INFO 2017-06-29 20:36:45,350 main.py:57] epoch 878, training loss: 7149.50, average training loss: 8363.48, base loss: 16026.69
[INFO 2017-06-29 20:36:48,203 main.py:57] epoch 879, training loss: 6497.97, average training loss: 8361.36, base loss: 16022.82
[INFO 2017-06-29 20:36:51,089 main.py:57] epoch 880, training loss: 7307.53, average training loss: 8360.17, base loss: 16023.50
[INFO 2017-06-29 20:36:53,955 main.py:57] epoch 881, training loss: 7125.87, average training loss: 8358.77, base loss: 16022.96
[INFO 2017-06-29 20:36:56,806 main.py:57] epoch 882, training loss: 6740.86, average training loss: 8356.93, base loss: 16019.00
[INFO 2017-06-29 20:36:59,699 main.py:57] epoch 883, training loss: 8120.39, average training loss: 8356.67, base loss: 16020.73
[INFO 2017-06-29 20:37:02,527 main.py:57] epoch 884, training loss: 6630.79, average training loss: 8354.72, base loss: 16020.93
[INFO 2017-06-29 20:37:05,448 main.py:57] epoch 885, training loss: 7505.88, average training loss: 8353.76, base loss: 16023.31
[INFO 2017-06-29 20:37:08,346 main.py:57] epoch 886, training loss: 7966.49, average training loss: 8353.32, base loss: 16026.03
[INFO 2017-06-29 20:37:11,229 main.py:57] epoch 887, training loss: 7054.79, average training loss: 8351.86, base loss: 16026.09
[INFO 2017-06-29 20:37:14,142 main.py:57] epoch 888, training loss: 7255.24, average training loss: 8350.63, base loss: 16028.98
[INFO 2017-06-29 20:37:16,998 main.py:57] epoch 889, training loss: 6955.18, average training loss: 8349.06, base loss: 16028.88
[INFO 2017-06-29 20:37:19,837 main.py:57] epoch 890, training loss: 7020.70, average training loss: 8347.57, base loss: 16026.70
[INFO 2017-06-29 20:37:22,722 main.py:57] epoch 891, training loss: 6626.95, average training loss: 8345.64, base loss: 16025.70
[INFO 2017-06-29 20:37:25,584 main.py:57] epoch 892, training loss: 6829.20, average training loss: 8343.94, base loss: 16023.05
[INFO 2017-06-29 20:37:28,456 main.py:57] epoch 893, training loss: 6382.20, average training loss: 8341.75, base loss: 16019.16
[INFO 2017-06-29 20:37:31,322 main.py:57] epoch 894, training loss: 7195.38, average training loss: 8340.46, base loss: 16017.74
[INFO 2017-06-29 20:37:34,176 main.py:57] epoch 895, training loss: 7437.96, average training loss: 8339.46, base loss: 16017.83
[INFO 2017-06-29 20:37:37,038 main.py:57] epoch 896, training loss: 7303.14, average training loss: 8338.30, base loss: 16018.08
[INFO 2017-06-29 20:37:39,933 main.py:57] epoch 897, training loss: 7248.67, average training loss: 8337.09, base loss: 16017.82
[INFO 2017-06-29 20:37:42,797 main.py:57] epoch 898, training loss: 6501.04, average training loss: 8335.05, base loss: 16014.24
[INFO 2017-06-29 20:37:45,633 main.py:57] epoch 899, training loss: 6887.05, average training loss: 8333.44, base loss: 16012.99
[INFO 2017-06-29 20:37:45,634 main.py:59] epoch 899, testing
[INFO 2017-06-29 20:37:58,014 main.py:104] average testing loss: 7111.53, base loss: 15900.37
[INFO 2017-06-29 20:37:58,015 main.py:105] improve_loss: 8788.85, improve_percent: 0.55
[INFO 2017-06-29 20:37:58,017 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 20:37:58,042 main.py:71] current best improved percent: 0.55
[INFO 2017-06-29 20:38:00,927 main.py:57] epoch 900, training loss: 7063.27, average training loss: 8332.03, base loss: 16012.70
[INFO 2017-06-29 20:38:03,843 main.py:57] epoch 901, training loss: 6919.50, average training loss: 8330.46, base loss: 16009.96
[INFO 2017-06-29 20:38:06,706 main.py:57] epoch 902, training loss: 8179.01, average training loss: 8330.29, base loss: 16014.55
[INFO 2017-06-29 20:38:09,571 main.py:57] epoch 903, training loss: 6704.60, average training loss: 8328.50, base loss: 16013.92
[INFO 2017-06-29 20:38:12,440 main.py:57] epoch 904, training loss: 6844.83, average training loss: 8326.86, base loss: 16012.97
[INFO 2017-06-29 20:38:15,328 main.py:57] epoch 905, training loss: 7890.01, average training loss: 8326.37, base loss: 16015.40
[INFO 2017-06-29 20:38:18,231 main.py:57] epoch 906, training loss: 7196.91, average training loss: 8325.13, base loss: 16015.47
[INFO 2017-06-29 20:38:21,099 main.py:57] epoch 907, training loss: 7191.98, average training loss: 8323.88, base loss: 16016.09
[INFO 2017-06-29 20:38:23,960 main.py:57] epoch 908, training loss: 6730.39, average training loss: 8322.13, base loss: 16016.23
[INFO 2017-06-29 20:38:26,833 main.py:57] epoch 909, training loss: 7368.37, average training loss: 8321.08, base loss: 16018.32
[INFO 2017-06-29 20:38:29,710 main.py:57] epoch 910, training loss: 7571.02, average training loss: 8320.26, base loss: 16020.25
[INFO 2017-06-29 20:38:32,572 main.py:57] epoch 911, training loss: 7597.28, average training loss: 8319.46, base loss: 16021.61
[INFO 2017-06-29 20:38:35,433 main.py:57] epoch 912, training loss: 7092.17, average training loss: 8318.12, base loss: 16019.35
[INFO 2017-06-29 20:38:38,276 main.py:57] epoch 913, training loss: 7026.50, average training loss: 8316.71, base loss: 16019.54
[INFO 2017-06-29 20:38:41,134 main.py:57] epoch 914, training loss: 8739.90, average training loss: 8317.17, base loss: 16023.21
[INFO 2017-06-29 20:38:43,968 main.py:57] epoch 915, training loss: 6827.86, average training loss: 8315.54, base loss: 16019.93
[INFO 2017-06-29 20:38:46,863 main.py:57] epoch 916, training loss: 7041.51, average training loss: 8314.15, base loss: 16019.38
[INFO 2017-06-29 20:38:49,709 main.py:57] epoch 917, training loss: 7055.16, average training loss: 8312.78, base loss: 16018.57
[INFO 2017-06-29 20:38:52,611 main.py:57] epoch 918, training loss: 7220.57, average training loss: 8311.59, base loss: 16019.33
[INFO 2017-06-29 20:38:55,509 main.py:57] epoch 919, training loss: 6694.69, average training loss: 8309.84, base loss: 16017.49
[INFO 2017-06-29 20:38:58,391 main.py:57] epoch 920, training loss: 7107.59, average training loss: 8308.53, base loss: 16017.08
[INFO 2017-06-29 20:39:01,298 main.py:57] epoch 921, training loss: 7055.85, average training loss: 8307.17, base loss: 16015.90
[INFO 2017-06-29 20:39:04,183 main.py:57] epoch 922, training loss: 7113.20, average training loss: 8305.88, base loss: 16016.96
[INFO 2017-06-29 20:39:07,029 main.py:57] epoch 923, training loss: 6910.03, average training loss: 8304.37, base loss: 16016.93
[INFO 2017-06-29 20:39:09,946 main.py:57] epoch 924, training loss: 7358.33, average training loss: 8303.34, base loss: 16018.08
[INFO 2017-06-29 20:39:12,790 main.py:57] epoch 925, training loss: 7242.57, average training loss: 8302.20, base loss: 16019.17
[INFO 2017-06-29 20:39:15,655 main.py:57] epoch 926, training loss: 7207.97, average training loss: 8301.02, base loss: 16019.65
[INFO 2017-06-29 20:39:18,559 main.py:57] epoch 927, training loss: 7236.45, average training loss: 8299.87, base loss: 16020.12
[INFO 2017-06-29 20:39:21,436 main.py:57] epoch 928, training loss: 6780.18, average training loss: 8298.24, base loss: 16018.95
[INFO 2017-06-29 20:39:24,312 main.py:57] epoch 929, training loss: 7144.83, average training loss: 8297.00, base loss: 16020.66
[INFO 2017-06-29 20:39:27,228 main.py:57] epoch 930, training loss: 7070.17, average training loss: 8295.68, base loss: 16019.72
[INFO 2017-06-29 20:39:30,144 main.py:57] epoch 931, training loss: 7555.13, average training loss: 8294.88, base loss: 16020.73
[INFO 2017-06-29 20:39:33,065 main.py:57] epoch 932, training loss: 7508.22, average training loss: 8294.04, base loss: 16022.29
[INFO 2017-06-29 20:39:35,936 main.py:57] epoch 933, training loss: 7037.84, average training loss: 8292.70, base loss: 16020.09
[INFO 2017-06-29 20:39:38,776 main.py:57] epoch 934, training loss: 7106.66, average training loss: 8291.43, base loss: 16020.27
[INFO 2017-06-29 20:39:41,643 main.py:57] epoch 935, training loss: 6998.52, average training loss: 8290.05, base loss: 16019.00
[INFO 2017-06-29 20:39:44,514 main.py:57] epoch 936, training loss: 7251.67, average training loss: 8288.94, base loss: 16019.88
[INFO 2017-06-29 20:39:47,359 main.py:57] epoch 937, training loss: 7417.48, average training loss: 8288.01, base loss: 16020.44
[INFO 2017-06-29 20:39:50,208 main.py:57] epoch 938, training loss: 7413.23, average training loss: 8287.08, base loss: 16021.13
[INFO 2017-06-29 20:39:53,082 main.py:57] epoch 939, training loss: 7383.01, average training loss: 8286.11, base loss: 16022.74
[INFO 2017-06-29 20:39:55,956 main.py:57] epoch 940, training loss: 7699.52, average training loss: 8285.49, base loss: 16025.49
[INFO 2017-06-29 20:39:58,821 main.py:57] epoch 941, training loss: 7036.38, average training loss: 8284.17, base loss: 16026.63
[INFO 2017-06-29 20:40:01,727 main.py:57] epoch 942, training loss: 6719.19, average training loss: 8282.51, base loss: 16025.34
[INFO 2017-06-29 20:40:04,617 main.py:57] epoch 943, training loss: 7429.34, average training loss: 8281.60, base loss: 16024.85
[INFO 2017-06-29 20:40:07,455 main.py:57] epoch 944, training loss: 6991.63, average training loss: 8280.24, base loss: 16022.81
[INFO 2017-06-29 20:40:10,341 main.py:57] epoch 945, training loss: 6952.84, average training loss: 8278.83, base loss: 16021.62
[INFO 2017-06-29 20:40:13,216 main.py:57] epoch 946, training loss: 6843.19, average training loss: 8277.32, base loss: 16020.87
[INFO 2017-06-29 20:40:16,076 main.py:57] epoch 947, training loss: 7382.39, average training loss: 8276.37, base loss: 16021.25
[INFO 2017-06-29 20:40:18,960 main.py:57] epoch 948, training loss: 7552.53, average training loss: 8275.61, base loss: 16022.91
[INFO 2017-06-29 20:40:21,839 main.py:57] epoch 949, training loss: 6662.81, average training loss: 8273.91, base loss: 16020.50
[INFO 2017-06-29 20:40:24,732 main.py:57] epoch 950, training loss: 7468.87, average training loss: 8273.07, base loss: 16022.67
[INFO 2017-06-29 20:40:27,601 main.py:57] epoch 951, training loss: 6484.86, average training loss: 8271.19, base loss: 16021.46
[INFO 2017-06-29 20:40:30,481 main.py:57] epoch 952, training loss: 6897.30, average training loss: 8269.75, base loss: 16021.10
[INFO 2017-06-29 20:40:33,368 main.py:57] epoch 953, training loss: 7168.61, average training loss: 8268.59, base loss: 16020.43
[INFO 2017-06-29 20:40:36,238 main.py:57] epoch 954, training loss: 7698.25, average training loss: 8268.00, base loss: 16019.46
[INFO 2017-06-29 20:40:39,185 main.py:57] epoch 955, training loss: 7558.14, average training loss: 8267.25, base loss: 16019.96
[INFO 2017-06-29 20:40:42,078 main.py:57] epoch 956, training loss: 7388.37, average training loss: 8266.33, base loss: 16021.62
[INFO 2017-06-29 20:40:44,967 main.py:57] epoch 957, training loss: 7894.72, average training loss: 8265.95, base loss: 16024.44
[INFO 2017-06-29 20:40:47,815 main.py:57] epoch 958, training loss: 6996.51, average training loss: 8264.62, base loss: 16025.54
[INFO 2017-06-29 20:40:50,701 main.py:57] epoch 959, training loss: 7298.83, average training loss: 8263.62, base loss: 16027.21
[INFO 2017-06-29 20:40:53,538 main.py:57] epoch 960, training loss: 7686.41, average training loss: 8263.02, base loss: 16029.51
[INFO 2017-06-29 20:40:56,451 main.py:57] epoch 961, training loss: 7014.15, average training loss: 8261.72, base loss: 16031.08
[INFO 2017-06-29 20:40:59,341 main.py:57] epoch 962, training loss: 7058.78, average training loss: 8260.47, base loss: 16029.86
[INFO 2017-06-29 20:41:02,223 main.py:57] epoch 963, training loss: 7420.82, average training loss: 8259.60, base loss: 16029.98
[INFO 2017-06-29 20:41:05,101 main.py:57] epoch 964, training loss: 7268.10, average training loss: 8258.57, base loss: 16028.70
[INFO 2017-06-29 20:41:07,989 main.py:57] epoch 965, training loss: 7154.61, average training loss: 8257.43, base loss: 16031.07
[INFO 2017-06-29 20:41:10,852 main.py:57] epoch 966, training loss: 7084.34, average training loss: 8256.21, base loss: 16030.91
[INFO 2017-06-29 20:41:13,697 main.py:57] epoch 967, training loss: 6694.97, average training loss: 8254.60, base loss: 16030.23
[INFO 2017-06-29 20:41:16,578 main.py:57] epoch 968, training loss: 6500.74, average training loss: 8252.79, base loss: 16025.79
[INFO 2017-06-29 20:41:19,451 main.py:57] epoch 969, training loss: 6936.15, average training loss: 8251.43, base loss: 16023.50
[INFO 2017-06-29 20:41:22,298 main.py:57] epoch 970, training loss: 6945.82, average training loss: 8250.09, base loss: 16021.60
[INFO 2017-06-29 20:41:25,143 main.py:57] epoch 971, training loss: 7566.21, average training loss: 8249.39, base loss: 16023.30
[INFO 2017-06-29 20:41:27,997 main.py:57] epoch 972, training loss: 6951.57, average training loss: 8248.05, base loss: 16023.01
[INFO 2017-06-29 20:41:30,939 main.py:57] epoch 973, training loss: 7215.02, average training loss: 8246.99, base loss: 16023.43
[INFO 2017-06-29 20:41:33,793 main.py:57] epoch 974, training loss: 7308.97, average training loss: 8246.03, base loss: 16024.65
[INFO 2017-06-29 20:41:36,652 main.py:57] epoch 975, training loss: 6672.48, average training loss: 8244.42, base loss: 16024.08
[INFO 2017-06-29 20:41:39,496 main.py:57] epoch 976, training loss: 6620.93, average training loss: 8242.76, base loss: 16022.51
[INFO 2017-06-29 20:41:42,328 main.py:57] epoch 977, training loss: 7126.39, average training loss: 8241.61, base loss: 16020.72
[INFO 2017-06-29 20:41:45,192 main.py:57] epoch 978, training loss: 6695.21, average training loss: 8240.03, base loss: 16018.14
[INFO 2017-06-29 20:41:48,045 main.py:57] epoch 979, training loss: 7039.43, average training loss: 8238.81, base loss: 16016.81
[INFO 2017-06-29 20:41:50,901 main.py:57] epoch 980, training loss: 6870.30, average training loss: 8237.41, base loss: 16014.44
[INFO 2017-06-29 20:41:53,743 main.py:57] epoch 981, training loss: 7195.95, average training loss: 8236.35, base loss: 16012.29
[INFO 2017-06-29 20:41:56,593 main.py:57] epoch 982, training loss: 6842.26, average training loss: 8234.94, base loss: 16010.96
[INFO 2017-06-29 20:41:59,467 main.py:57] epoch 983, training loss: 7362.50, average training loss: 8234.05, base loss: 16011.71
[INFO 2017-06-29 20:42:02,304 main.py:57] epoch 984, training loss: 7036.23, average training loss: 8232.83, base loss: 16011.46
[INFO 2017-06-29 20:42:05,159 main.py:57] epoch 985, training loss: 7013.53, average training loss: 8231.60, base loss: 16010.81
[INFO 2017-06-29 20:42:08,043 main.py:57] epoch 986, training loss: 6729.54, average training loss: 8230.07, base loss: 16010.61
[INFO 2017-06-29 20:42:10,904 main.py:57] epoch 987, training loss: 7202.64, average training loss: 8229.03, base loss: 16012.66
[INFO 2017-06-29 20:42:13,754 main.py:57] epoch 988, training loss: 7823.93, average training loss: 8228.63, base loss: 16014.63
[INFO 2017-06-29 20:42:16,637 main.py:57] epoch 989, training loss: 7136.82, average training loss: 8227.52, base loss: 16013.23
[INFO 2017-06-29 20:42:19,528 main.py:57] epoch 990, training loss: 7148.73, average training loss: 8226.43, base loss: 16014.22
[INFO 2017-06-29 20:42:22,434 main.py:57] epoch 991, training loss: 7031.11, average training loss: 8225.23, base loss: 16014.06
[INFO 2017-06-29 20:42:25,290 main.py:57] epoch 992, training loss: 6754.12, average training loss: 8223.75, base loss: 16013.07
[INFO 2017-06-29 20:42:28,163 main.py:57] epoch 993, training loss: 6653.31, average training loss: 8222.17, base loss: 16012.55
[INFO 2017-06-29 20:42:31,038 main.py:57] epoch 994, training loss: 7405.72, average training loss: 8221.35, base loss: 16013.58
[INFO 2017-06-29 20:42:33,895 main.py:57] epoch 995, training loss: 7266.40, average training loss: 8220.39, base loss: 16013.87
[INFO 2017-06-29 20:42:36,777 main.py:57] epoch 996, training loss: 6894.49, average training loss: 8219.06, base loss: 16012.81
[INFO 2017-06-29 20:42:39,588 main.py:57] epoch 997, training loss: 6999.64, average training loss: 8217.84, base loss: 16013.03
[INFO 2017-06-29 20:42:42,492 main.py:57] epoch 998, training loss: 7246.51, average training loss: 8216.86, base loss: 16010.74
[INFO 2017-06-29 20:42:45,389 main.py:57] epoch 999, training loss: 7666.42, average training loss: 8216.31, base loss: 16012.21
[INFO 2017-06-29 20:42:45,389 main.py:59] epoch 999, testing
[INFO 2017-06-29 20:42:57,765 main.py:104] average testing loss: 6974.35, base loss: 15591.02
[INFO 2017-06-29 20:42:57,766 main.py:105] improve_loss: 8616.68, improve_percent: 0.55
[INFO 2017-06-29 20:42:57,768 main.py:71] current best improved percent: 0.55
[INFO 2017-06-29 20:43:00,624 main.py:57] epoch 1000, training loss: 7118.77, average training loss: 8183.83, base loss: 16009.48
[INFO 2017-06-29 20:43:03,514 main.py:57] epoch 1001, training loss: 6788.73, average training loss: 8155.33, base loss: 16006.84
[INFO 2017-06-29 20:43:06,428 main.py:57] epoch 1002, training loss: 6750.05, average training loss: 8131.31, base loss: 16005.68
[INFO 2017-06-29 20:43:09,294 main.py:57] epoch 1003, training loss: 6901.04, average training loss: 8111.59, base loss: 16006.21
[INFO 2017-06-29 20:43:12,144 main.py:57] epoch 1004, training loss: 7196.08, average training loss: 8094.40, base loss: 16007.50
[INFO 2017-06-29 20:43:15,051 main.py:57] epoch 1005, training loss: 7043.47, average training loss: 8077.94, base loss: 16008.89
[INFO 2017-06-29 20:43:17,915 main.py:57] epoch 1006, training loss: 7076.62, average training loss: 8064.20, base loss: 16007.82
[INFO 2017-06-29 20:43:20,788 main.py:57] epoch 1007, training loss: 6961.62, average training loss: 8052.30, base loss: 16007.31
[INFO 2017-06-29 20:43:23,711 main.py:57] epoch 1008, training loss: 7324.31, average training loss: 8041.57, base loss: 16007.78
[INFO 2017-06-29 20:43:26,583 main.py:57] epoch 1009, training loss: 7344.97, average training loss: 8031.21, base loss: 16009.05
[INFO 2017-06-29 20:43:29,432 main.py:57] epoch 1010, training loss: 6763.34, average training loss: 8022.67, base loss: 16007.65
[INFO 2017-06-29 20:43:32,290 main.py:57] epoch 1011, training loss: 7012.00, average training loss: 8015.26, base loss: 16007.42
[INFO 2017-06-29 20:43:35,164 main.py:57] epoch 1012, training loss: 7381.86, average training loss: 8007.94, base loss: 16009.48
[INFO 2017-06-29 20:43:38,007 main.py:57] epoch 1013, training loss: 7153.12, average training loss: 8001.61, base loss: 16010.60
[INFO 2017-06-29 20:43:40,887 main.py:57] epoch 1014, training loss: 6856.97, average training loss: 7993.27, base loss: 16007.97
[INFO 2017-06-29 20:43:43,789 main.py:57] epoch 1015, training loss: 6578.89, average training loss: 7986.39, base loss: 16005.75
[INFO 2017-06-29 20:43:46,688 main.py:57] epoch 1016, training loss: 7050.61, average training loss: 7980.22, base loss: 16005.68
[INFO 2017-06-29 20:43:49,520 main.py:57] epoch 1017, training loss: 7401.45, average training loss: 7974.68, base loss: 16006.35
[INFO 2017-06-29 20:43:52,373 main.py:57] epoch 1018, training loss: 7017.13, average training loss: 7969.94, base loss: 16006.96
[INFO 2017-06-29 20:43:55,298 main.py:57] epoch 1019, training loss: 6484.83, average training loss: 7963.34, base loss: 16003.65
[INFO 2017-06-29 20:43:58,202 main.py:57] epoch 1020, training loss: 6940.24, average training loss: 7957.63, base loss: 16003.19
[INFO 2017-06-29 20:44:01,076 main.py:57] epoch 1021, training loss: 7209.86, average training loss: 7952.86, base loss: 16003.00
[INFO 2017-06-29 20:44:03,988 main.py:57] epoch 1022, training loss: 6696.04, average training loss: 7948.98, base loss: 16002.30
[INFO 2017-06-29 20:44:06,839 main.py:57] epoch 1023, training loss: 7093.80, average training loss: 7945.14, base loss: 16002.22
[INFO 2017-06-29 20:44:09,680 main.py:57] epoch 1024, training loss: 6444.28, average training loss: 7938.49, base loss: 16001.47
[INFO 2017-06-29 20:44:12,526 main.py:57] epoch 1025, training loss: 6487.24, average training loss: 7933.97, base loss: 15999.03
[INFO 2017-06-29 20:44:15,390 main.py:57] epoch 1026, training loss: 6707.91, average training loss: 7929.15, base loss: 16000.39
[INFO 2017-06-29 20:44:18,256 main.py:57] epoch 1027, training loss: 6979.48, average training loss: 7925.02, base loss: 16000.24
[INFO 2017-06-29 20:44:21,147 main.py:57] epoch 1028, training loss: 6967.09, average training loss: 7921.06, base loss: 15999.21
[INFO 2017-06-29 20:44:24,049 main.py:57] epoch 1029, training loss: 7320.43, average training loss: 7917.16, base loss: 15998.76
[INFO 2017-06-29 20:44:26,919 main.py:57] epoch 1030, training loss: 7167.72, average training loss: 7914.79, base loss: 15998.19
[INFO 2017-06-29 20:44:29,791 main.py:57] epoch 1031, training loss: 7151.08, average training loss: 7911.19, base loss: 15997.32
[INFO 2017-06-29 20:44:32,625 main.py:57] epoch 1032, training loss: 7070.80, average training loss: 7906.38, base loss: 15997.53
[INFO 2017-06-29 20:44:35,497 main.py:57] epoch 1033, training loss: 6171.11, average training loss: 7900.52, base loss: 15994.81
[INFO 2017-06-29 20:44:38,351 main.py:57] epoch 1034, training loss: 8014.04, average training loss: 7897.91, base loss: 15997.37
[INFO 2017-06-29 20:44:41,228 main.py:57] epoch 1035, training loss: 7113.04, average training loss: 7894.24, base loss: 15997.06
[INFO 2017-06-29 20:44:44,057 main.py:57] epoch 1036, training loss: 7027.56, average training loss: 7890.09, base loss: 15997.32
[INFO 2017-06-29 20:44:46,944 main.py:57] epoch 1037, training loss: 7469.27, average training loss: 7887.69, base loss: 15999.88
[INFO 2017-06-29 20:44:49,821 main.py:57] epoch 1038, training loss: 7532.07, average training loss: 7884.41, base loss: 16003.09
[INFO 2017-06-29 20:44:52,710 main.py:57] epoch 1039, training loss: 7603.99, average training loss: 7881.11, base loss: 16004.95
[INFO 2017-06-29 20:44:55,586 main.py:57] epoch 1040, training loss: 7127.17, average training loss: 7876.24, base loss: 16005.02
[INFO 2017-06-29 20:44:58,445 main.py:57] epoch 1041, training loss: 6672.69, average training loss: 7871.41, base loss: 16004.44
[INFO 2017-06-29 20:45:01,305 main.py:57] epoch 1042, training loss: 7651.98, average training loss: 7869.03, base loss: 16005.34
[INFO 2017-06-29 20:45:04,128 main.py:57] epoch 1043, training loss: 7065.70, average training loss: 7865.57, base loss: 16004.22
[INFO 2017-06-29 20:45:06,998 main.py:57] epoch 1044, training loss: 7273.39, average training loss: 7863.01, base loss: 16005.53
[INFO 2017-06-29 20:45:09,844 main.py:57] epoch 1045, training loss: 7187.41, average training loss: 7859.42, base loss: 16007.40
[INFO 2017-06-29 20:45:12,746 main.py:57] epoch 1046, training loss: 6407.90, average training loss: 7856.07, base loss: 16004.96
[INFO 2017-06-29 20:45:15,626 main.py:57] epoch 1047, training loss: 6563.22, average training loss: 7851.29, base loss: 16003.76
[INFO 2017-06-29 20:45:18,539 main.py:57] epoch 1048, training loss: 7334.41, average training loss: 7848.06, base loss: 16003.68
[INFO 2017-06-29 20:45:21,408 main.py:57] epoch 1049, training loss: 6917.92, average training loss: 7844.66, base loss: 16002.38
[INFO 2017-06-29 20:45:24,297 main.py:57] epoch 1050, training loss: 7304.56, average training loss: 7840.35, base loss: 16002.58
[INFO 2017-06-29 20:45:27,183 main.py:57] epoch 1051, training loss: 7123.81, average training loss: 7838.25, base loss: 16002.06
[INFO 2017-06-29 20:45:30,085 main.py:57] epoch 1052, training loss: 6930.84, average training loss: 7835.35, base loss: 16001.79
[INFO 2017-06-29 20:45:32,975 main.py:57] epoch 1053, training loss: 7826.66, average training loss: 7833.58, base loss: 16003.97
[INFO 2017-06-29 20:45:35,829 main.py:57] epoch 1054, training loss: 7443.49, average training loss: 7830.72, base loss: 16004.30
[INFO 2017-06-29 20:45:38,726 main.py:57] epoch 1055, training loss: 7270.70, average training loss: 7826.72, base loss: 16004.48
[INFO 2017-06-29 20:45:41,594 main.py:57] epoch 1056, training loss: 7487.23, average training loss: 7825.15, base loss: 16005.43
[INFO 2017-06-29 20:45:44,521 main.py:57] epoch 1057, training loss: 7437.69, average training loss: 7823.18, base loss: 16005.79
[INFO 2017-06-29 20:45:47,372 main.py:57] epoch 1058, training loss: 7613.42, average training loss: 7821.75, base loss: 16009.04
[INFO 2017-06-29 20:45:50,233 main.py:57] epoch 1059, training loss: 7002.22, average training loss: 7818.53, base loss: 16007.57
[INFO 2017-06-29 20:45:53,138 main.py:57] epoch 1060, training loss: 6914.42, average training loss: 7814.86, base loss: 16007.01
[INFO 2017-06-29 20:45:56,014 main.py:57] epoch 1061, training loss: 6611.04, average training loss: 7811.00, base loss: 16005.03
[INFO 2017-06-29 20:45:58,863 main.py:57] epoch 1062, training loss: 6702.37, average training loss: 7807.67, base loss: 16002.74
[INFO 2017-06-29 20:46:01,753 main.py:57] epoch 1063, training loss: 6918.61, average training loss: 7805.71, base loss: 16002.35
[INFO 2017-06-29 20:46:04,626 main.py:57] epoch 1064, training loss: 6587.02, average training loss: 7803.22, base loss: 15999.42
[INFO 2017-06-29 20:46:07,506 main.py:57] epoch 1065, training loss: 6813.00, average training loss: 7799.71, base loss: 15997.90
[INFO 2017-06-29 20:46:10,476 main.py:57] epoch 1066, training loss: 7295.42, average training loss: 7796.72, base loss: 15998.91
[INFO 2017-06-29 20:46:13,345 main.py:57] epoch 1067, training loss: 7546.78, average training loss: 7794.86, base loss: 16001.14
[INFO 2017-06-29 20:46:16,204 main.py:57] epoch 1068, training loss: 6577.42, average training loss: 7791.73, base loss: 15999.86
[INFO 2017-06-29 20:46:19,090 main.py:57] epoch 1069, training loss: 6885.02, average training loss: 7788.88, base loss: 15999.22
[INFO 2017-06-29 20:46:21,934 main.py:57] epoch 1070, training loss: 6636.11, average training loss: 7785.60, base loss: 15999.58
[INFO 2017-06-29 20:46:24,823 main.py:57] epoch 1071, training loss: 7715.82, average training loss: 7783.20, base loss: 16001.82
[INFO 2017-06-29 20:46:27,673 main.py:57] epoch 1072, training loss: 7642.31, average training loss: 7781.48, base loss: 16002.53
[INFO 2017-06-29 20:46:30,513 main.py:57] epoch 1073, training loss: 7402.69, average training loss: 7777.97, base loss: 16002.94
[INFO 2017-06-29 20:46:33,443 main.py:57] epoch 1074, training loss: 6949.48, average training loss: 7775.20, base loss: 16001.52
[INFO 2017-06-29 20:46:36,328 main.py:57] epoch 1075, training loss: 7554.10, average training loss: 7773.20, base loss: 16005.05
[INFO 2017-06-29 20:46:39,165 main.py:57] epoch 1076, training loss: 7104.74, average training loss: 7770.01, base loss: 16003.43
[INFO 2017-06-29 20:46:42,012 main.py:57] epoch 1077, training loss: 7272.86, average training loss: 7767.61, base loss: 16004.79
[INFO 2017-06-29 20:46:44,874 main.py:57] epoch 1078, training loss: 7072.22, average training loss: 7764.83, base loss: 16005.73
[INFO 2017-06-29 20:46:47,749 main.py:57] epoch 1079, training loss: 7684.12, average training loss: 7762.33, base loss: 16008.82
[INFO 2017-06-29 20:46:50,590 main.py:57] epoch 1080, training loss: 6843.84, average training loss: 7759.83, base loss: 16008.95
[INFO 2017-06-29 20:46:53,445 main.py:57] epoch 1081, training loss: 7258.55, average training loss: 7756.37, base loss: 16009.28
[INFO 2017-06-29 20:46:56,292 main.py:57] epoch 1082, training loss: 7895.17, average training loss: 7754.84, base loss: 16012.14
[INFO 2017-06-29 20:46:59,224 main.py:57] epoch 1083, training loss: 7349.69, average training loss: 7751.47, base loss: 16014.26
[INFO 2017-06-29 20:47:02,134 main.py:57] epoch 1084, training loss: 6953.65, average training loss: 7749.31, base loss: 16013.72
[INFO 2017-06-29 20:47:05,054 main.py:57] epoch 1085, training loss: 7714.68, average training loss: 7747.33, base loss: 16016.34
[INFO 2017-06-29 20:47:07,922 main.py:57] epoch 1086, training loss: 6383.55, average training loss: 7743.72, base loss: 16013.61
[INFO 2017-06-29 20:47:10,743 main.py:57] epoch 1087, training loss: 7137.22, average training loss: 7741.09, base loss: 16014.25
[INFO 2017-06-29 20:47:13,598 main.py:57] epoch 1088, training loss: 6763.22, average training loss: 7737.85, base loss: 16013.85
[INFO 2017-06-29 20:47:16,455 main.py:57] epoch 1089, training loss: 7220.30, average training loss: 7733.31, base loss: 16013.51
[INFO 2017-06-29 20:47:19,336 main.py:57] epoch 1090, training loss: 7321.67, average training loss: 7730.64, base loss: 16014.39
[INFO 2017-06-29 20:47:22,169 main.py:57] epoch 1091, training loss: 7737.62, average training loss: 7729.56, base loss: 16017.41
[INFO 2017-06-29 20:47:25,022 main.py:57] epoch 1092, training loss: 7108.26, average training loss: 7727.98, base loss: 16016.90
[INFO 2017-06-29 20:47:27,948 main.py:57] epoch 1093, training loss: 7222.76, average training loss: 7726.22, base loss: 16017.78
[INFO 2017-06-29 20:47:30,802 main.py:57] epoch 1094, training loss: 6794.01, average training loss: 7723.89, base loss: 16017.31
[INFO 2017-06-29 20:47:33,678 main.py:57] epoch 1095, training loss: 6565.67, average training loss: 7721.60, base loss: 16015.04
[INFO 2017-06-29 20:47:36,507 main.py:57] epoch 1096, training loss: 6934.92, average training loss: 7719.60, base loss: 16015.02
[INFO 2017-06-29 20:47:39,400 main.py:57] epoch 1097, training loss: 7440.55, average training loss: 7717.94, base loss: 16016.22
[INFO 2017-06-29 20:47:42,247 main.py:57] epoch 1098, training loss: 6730.80, average training loss: 7715.92, base loss: 16015.86
[INFO 2017-06-29 20:47:45,084 main.py:57] epoch 1099, training loss: 7155.72, average training loss: 7713.89, base loss: 16017.24
[INFO 2017-06-29 20:47:45,085 main.py:59] epoch 1099, testing
[INFO 2017-06-29 20:47:57,507 main.py:104] average testing loss: 6926.28, base loss: 15378.34
[INFO 2017-06-29 20:47:57,507 main.py:105] improve_loss: 8452.06, improve_percent: 0.55
[INFO 2017-06-29 20:47:57,509 main.py:71] current best improved percent: 0.55
[INFO 2017-06-29 20:48:00,445 main.py:57] epoch 1100, training loss: 6995.63, average training loss: 7712.38, base loss: 16017.95
[INFO 2017-06-29 20:48:03,316 main.py:57] epoch 1101, training loss: 6408.28, average training loss: 7709.50, base loss: 16017.31
[INFO 2017-06-29 20:48:06,131 main.py:57] epoch 1102, training loss: 7049.83, average training loss: 7705.87, base loss: 16017.01
[INFO 2017-06-29 20:48:09,052 main.py:57] epoch 1103, training loss: 7044.84, average training loss: 7703.34, base loss: 16015.78
[INFO 2017-06-29 20:48:11,906 main.py:57] epoch 1104, training loss: 6924.47, average training loss: 7700.81, base loss: 16018.18
[INFO 2017-06-29 20:48:14,760 main.py:57] epoch 1105, training loss: 7596.52, average training loss: 7700.18, base loss: 16021.35
[INFO 2017-06-29 20:48:17,644 main.py:57] epoch 1106, training loss: 6242.02, average training loss: 7698.02, base loss: 16018.54
[INFO 2017-06-29 20:48:20,487 main.py:57] epoch 1107, training loss: 7921.51, average training loss: 7696.91, base loss: 16019.82
[INFO 2017-06-29 20:48:23,349 main.py:57] epoch 1108, training loss: 6727.59, average training loss: 7693.97, base loss: 16017.41
[INFO 2017-06-29 20:48:26,235 main.py:57] epoch 1109, training loss: 7134.08, average training loss: 7691.60, base loss: 16017.10
[INFO 2017-06-29 20:48:29,127 main.py:57] epoch 1110, training loss: 7172.92, average training loss: 7688.92, base loss: 16018.72
[INFO 2017-06-29 20:48:32,036 main.py:57] epoch 1111, training loss: 7613.92, average training loss: 7686.92, base loss: 16020.77
[INFO 2017-06-29 20:48:34,886 main.py:57] epoch 1112, training loss: 7680.01, average training loss: 7684.85, base loss: 16022.85
[INFO 2017-06-29 20:48:37,775 main.py:57] epoch 1113, training loss: 7094.11, average training loss: 7682.64, base loss: 16022.34
[INFO 2017-06-29 20:48:40,647 main.py:57] epoch 1114, training loss: 6534.55, average training loss: 7680.53, base loss: 16019.73
[INFO 2017-06-29 20:48:43,523 main.py:57] epoch 1115, training loss: 6819.88, average training loss: 7678.39, base loss: 16019.01
[INFO 2017-06-29 20:48:46,450 main.py:57] epoch 1116, training loss: 7190.63, average training loss: 7675.03, base loss: 16018.97
[INFO 2017-06-29 20:48:49,303 main.py:57] epoch 1117, training loss: 7193.21, average training loss: 7673.22, base loss: 16017.60
[INFO 2017-06-29 20:48:52,170 main.py:57] epoch 1118, training loss: 7346.82, average training loss: 7670.45, base loss: 16018.05
[INFO 2017-06-29 20:48:55,048 main.py:57] epoch 1119, training loss: 6914.40, average training loss: 7667.91, base loss: 16017.71
[INFO 2017-06-29 20:48:57,945 main.py:57] epoch 1120, training loss: 7182.98, average training loss: 7665.97, base loss: 16018.50
[INFO 2017-06-29 20:49:00,795 main.py:57] epoch 1121, training loss: 7147.05, average training loss: 7663.47, base loss: 16019.55
[INFO 2017-06-29 20:49:03,671 main.py:57] epoch 1122, training loss: 6969.13, average training loss: 7661.00, base loss: 16018.97
[INFO 2017-06-29 20:49:06,554 main.py:57] epoch 1123, training loss: 7126.63, average training loss: 7658.70, base loss: 16020.26
[INFO 2017-06-29 20:49:09,415 main.py:57] epoch 1124, training loss: 7077.46, average training loss: 7656.97, base loss: 16020.41
[INFO 2017-06-29 20:49:12,298 main.py:57] epoch 1125, training loss: 6905.97, average training loss: 7655.20, base loss: 16019.06
[INFO 2017-06-29 20:49:15,219 main.py:57] epoch 1126, training loss: 7105.92, average training loss: 7653.79, base loss: 16019.16
[INFO 2017-06-29 20:49:18,054 main.py:57] epoch 1127, training loss: 6740.11, average training loss: 7650.71, base loss: 16015.90
[INFO 2017-06-29 20:49:20,934 main.py:57] epoch 1128, training loss: 6738.58, average training loss: 7648.54, base loss: 16015.42
[INFO 2017-06-29 20:49:23,775 main.py:57] epoch 1129, training loss: 6526.72, average training loss: 7647.00, base loss: 16013.07
[INFO 2017-06-29 20:49:26,661 main.py:57] epoch 1130, training loss: 6890.74, average training loss: 7645.26, base loss: 16012.07
[INFO 2017-06-29 20:49:29,526 main.py:57] epoch 1131, training loss: 7332.66, average training loss: 7644.02, base loss: 16012.93
[INFO 2017-06-29 20:49:32,364 main.py:57] epoch 1132, training loss: 7297.04, average training loss: 7643.06, base loss: 16014.89
[INFO 2017-06-29 20:49:35,216 main.py:57] epoch 1133, training loss: 6763.07, average training loss: 7640.68, base loss: 16013.80
[INFO 2017-06-29 20:49:38,109 main.py:57] epoch 1134, training loss: 6940.04, average training loss: 7638.78, base loss: 16013.86
[INFO 2017-06-29 20:49:40,981 main.py:57] epoch 1135, training loss: 7285.79, average training loss: 7636.58, base loss: 16015.00
[INFO 2017-06-29 20:49:43,843 main.py:57] epoch 1136, training loss: 6758.09, average training loss: 7634.98, base loss: 16015.59
[INFO 2017-06-29 20:49:46,721 main.py:57] epoch 1137, training loss: 7810.06, average training loss: 7633.90, base loss: 16017.85
[INFO 2017-06-29 20:49:49,612 main.py:57] epoch 1138, training loss: 6619.60, average training loss: 7631.32, base loss: 16016.55
[INFO 2017-06-29 20:49:52,480 main.py:57] epoch 1139, training loss: 6346.97, average training loss: 7628.97, base loss: 16013.44
[INFO 2017-06-29 20:49:55,361 main.py:57] epoch 1140, training loss: 6782.71, average training loss: 7627.22, base loss: 16012.52
[INFO 2017-06-29 20:49:58,210 main.py:57] epoch 1141, training loss: 6737.63, average training loss: 7625.43, base loss: 16012.02
[INFO 2017-06-29 20:50:01,095 main.py:57] epoch 1142, training loss: 7343.91, average training loss: 7622.75, base loss: 16013.12
[INFO 2017-06-29 20:50:03,955 main.py:57] epoch 1143, training loss: 7034.73, average training loss: 7620.31, base loss: 16014.11
[INFO 2017-06-29 20:50:06,858 main.py:57] epoch 1144, training loss: 6738.82, average training loss: 7618.01, base loss: 16012.35
[INFO 2017-06-29 20:50:09,736 main.py:57] epoch 1145, training loss: 6720.41, average training loss: 7617.16, base loss: 16010.87
[INFO 2017-06-29 20:50:12,595 main.py:57] epoch 1146, training loss: 6746.82, average training loss: 7615.50, base loss: 16010.96
[INFO 2017-06-29 20:50:15,476 main.py:57] epoch 1147, training loss: 6700.52, average training loss: 7612.66, base loss: 16009.90
[INFO 2017-06-29 20:50:18,318 main.py:57] epoch 1148, training loss: 7185.14, average training loss: 7612.03, base loss: 16008.19
[INFO 2017-06-29 20:50:21,199 main.py:57] epoch 1149, training loss: 6991.26, average training loss: 7611.01, base loss: 16007.58
[INFO 2017-06-29 20:50:24,066 main.py:57] epoch 1150, training loss: 6471.71, average training loss: 7608.41, base loss: 16005.37
[INFO 2017-06-29 20:50:26,904 main.py:57] epoch 1151, training loss: 7098.37, average training loss: 7606.62, base loss: 16004.69
[INFO 2017-06-29 20:50:29,797 main.py:57] epoch 1152, training loss: 6784.32, average training loss: 7604.23, base loss: 16002.42
[INFO 2017-06-29 20:50:32,676 main.py:57] epoch 1153, training loss: 7100.93, average training loss: 7602.92, base loss: 16002.25
[INFO 2017-06-29 20:50:35,540 main.py:57] epoch 1154, training loss: 6914.33, average training loss: 7600.62, base loss: 16002.88
[INFO 2017-06-29 20:50:38,405 main.py:57] epoch 1155, training loss: 7452.24, average training loss: 7599.46, base loss: 16006.58
[INFO 2017-06-29 20:50:41,284 main.py:57] epoch 1156, training loss: 7463.31, average training loss: 7597.59, base loss: 16007.63
[INFO 2017-06-29 20:50:44,229 main.py:57] epoch 1157, training loss: 6823.31, average training loss: 7596.13, base loss: 16006.66
[INFO 2017-06-29 20:50:47,081 main.py:57] epoch 1158, training loss: 6592.13, average training loss: 7593.59, base loss: 16005.82
[INFO 2017-06-29 20:50:49,973 main.py:57] epoch 1159, training loss: 7666.69, average training loss: 7592.36, base loss: 16008.35
[INFO 2017-06-29 20:50:52,797 main.py:57] epoch 1160, training loss: 7363.10, average training loss: 7591.33, base loss: 16008.29
[INFO 2017-06-29 20:50:55,698 main.py:57] epoch 1161, training loss: 6464.93, average training loss: 7588.83, base loss: 16007.01
[INFO 2017-06-29 20:50:58,587 main.py:57] epoch 1162, training loss: 7572.29, average training loss: 7587.06, base loss: 16007.31
[INFO 2017-06-29 20:51:01,465 main.py:57] epoch 1163, training loss: 7018.35, average training loss: 7585.62, base loss: 16005.91
[INFO 2017-06-29 20:51:04,300 main.py:57] epoch 1164, training loss: 6641.88, average training loss: 7583.76, base loss: 16005.18
[INFO 2017-06-29 20:51:07,129 main.py:57] epoch 1165, training loss: 6415.70, average training loss: 7581.93, base loss: 16003.94
[INFO 2017-06-29 20:51:10,011 main.py:57] epoch 1166, training loss: 6734.47, average training loss: 7579.45, base loss: 16002.11
[INFO 2017-06-29 20:51:12,930 main.py:57] epoch 1167, training loss: 6330.82, average training loss: 7577.19, base loss: 16000.91
[INFO 2017-06-29 20:51:15,762 main.py:57] epoch 1168, training loss: 7091.52, average training loss: 7575.76, base loss: 16000.02
[INFO 2017-06-29 20:51:18,633 main.py:57] epoch 1169, training loss: 6491.41, average training loss: 7572.90, base loss: 15998.24
[INFO 2017-06-29 20:51:21,537 main.py:57] epoch 1170, training loss: 7597.43, average training loss: 7572.12, base loss: 16000.71
[INFO 2017-06-29 20:51:24,403 main.py:57] epoch 1171, training loss: 7182.78, average training loss: 7571.60, base loss: 16001.71
[INFO 2017-06-29 20:51:27,273 main.py:57] epoch 1172, training loss: 7145.19, average training loss: 7569.95, base loss: 16001.25
[INFO 2017-06-29 20:51:30,089 main.py:57] epoch 1173, training loss: 6833.36, average training loss: 7567.92, base loss: 16000.02
[INFO 2017-06-29 20:51:32,980 main.py:57] epoch 1174, training loss: 6742.44, average training loss: 7567.26, base loss: 15999.79
[INFO 2017-06-29 20:51:35,887 main.py:57] epoch 1175, training loss: 7237.44, average training loss: 7565.00, base loss: 16003.14
[INFO 2017-06-29 20:51:38,750 main.py:57] epoch 1176, training loss: 6636.32, average training loss: 7563.26, base loss: 16003.21
[INFO 2017-06-29 20:51:41,643 main.py:57] epoch 1177, training loss: 7147.11, average training loss: 7561.58, base loss: 16005.31
[INFO 2017-06-29 20:51:44,568 main.py:57] epoch 1178, training loss: 7645.04, average training loss: 7561.58, base loss: 16007.27
[INFO 2017-06-29 20:51:47,458 main.py:57] epoch 1179, training loss: 6793.72, average training loss: 7558.69, base loss: 16006.40
[INFO 2017-06-29 20:51:50,282 main.py:57] epoch 1180, training loss: 7260.54, average training loss: 7557.48, base loss: 16009.34
[INFO 2017-06-29 20:51:53,180 main.py:57] epoch 1181, training loss: 7589.05, average training loss: 7556.05, base loss: 16012.28
[INFO 2017-06-29 20:51:56,001 main.py:57] epoch 1182, training loss: 6727.19, average training loss: 7554.28, base loss: 16011.57
[INFO 2017-06-29 20:51:58,845 main.py:57] epoch 1183, training loss: 6650.32, average training loss: 7551.46, base loss: 16009.96
[INFO 2017-06-29 20:52:01,798 main.py:57] epoch 1184, training loss: 6909.97, average training loss: 7549.94, base loss: 16007.48
[INFO 2017-06-29 20:52:04,658 main.py:57] epoch 1185, training loss: 6986.90, average training loss: 7548.13, base loss: 16006.12
[INFO 2017-06-29 20:52:07,512 main.py:57] epoch 1186, training loss: 7131.50, average training loss: 7546.92, base loss: 16006.39
[INFO 2017-06-29 20:52:10,409 main.py:57] epoch 1187, training loss: 7555.45, average training loss: 7545.87, base loss: 16008.73
[INFO 2017-06-29 20:52:13,276 main.py:57] epoch 1188, training loss: 7018.11, average training loss: 7543.89, base loss: 16007.78
[INFO 2017-06-29 20:52:16,175 main.py:57] epoch 1189, training loss: 6888.12, average training loss: 7540.81, base loss: 16006.55
[INFO 2017-06-29 20:52:19,059 main.py:57] epoch 1190, training loss: 6774.92, average training loss: 7539.12, base loss: 16008.21
[INFO 2017-06-29 20:52:21,929 main.py:57] epoch 1191, training loss: 7072.23, average training loss: 7536.95, base loss: 16008.85
[INFO 2017-06-29 20:52:24,808 main.py:57] epoch 1192, training loss: 7137.58, average training loss: 7535.33, base loss: 16009.47
[INFO 2017-06-29 20:52:27,700 main.py:57] epoch 1193, training loss: 6524.10, average training loss: 7533.01, base loss: 16007.42
[INFO 2017-06-29 20:52:30,600 main.py:57] epoch 1194, training loss: 6794.97, average training loss: 7531.17, base loss: 16007.15
[INFO 2017-06-29 20:52:33,445 main.py:57] epoch 1195, training loss: 7760.14, average training loss: 7530.47, base loss: 16009.04
[INFO 2017-06-29 20:52:36,324 main.py:57] epoch 1196, training loss: 6834.06, average training loss: 7528.11, base loss: 16008.02
[INFO 2017-06-29 20:52:39,204 main.py:57] epoch 1197, training loss: 7260.83, average training loss: 7526.87, base loss: 16008.98
[INFO 2017-06-29 20:52:42,088 main.py:57] epoch 1198, training loss: 7123.07, average training loss: 7524.89, base loss: 16009.34
[INFO 2017-06-29 20:52:44,989 main.py:57] epoch 1199, training loss: 6940.60, average training loss: 7523.51, base loss: 16010.10
[INFO 2017-06-29 20:52:44,990 main.py:59] epoch 1199, testing
[INFO 2017-06-29 20:52:57,360 main.py:104] average testing loss: 6932.43, base loss: 15789.71
[INFO 2017-06-29 20:52:57,361 main.py:105] improve_loss: 8857.27, improve_percent: 0.56
[INFO 2017-06-29 20:52:57,362 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 20:52:57,387 main.py:71] current best improved percent: 0.56
[INFO 2017-06-29 20:53:00,272 main.py:57] epoch 1200, training loss: 7189.09, average training loss: 7521.82, base loss: 16010.02
[INFO 2017-06-29 20:53:03,157 main.py:57] epoch 1201, training loss: 7654.07, average training loss: 7521.33, base loss: 16010.60
[INFO 2017-06-29 20:53:06,042 main.py:57] epoch 1202, training loss: 6649.56, average training loss: 7519.00, base loss: 16008.71
[INFO 2017-06-29 20:53:08,875 main.py:57] epoch 1203, training loss: 6694.91, average training loss: 7517.13, base loss: 16007.54
[INFO 2017-06-29 20:53:11,704 main.py:57] epoch 1204, training loss: 6578.85, average training loss: 7515.61, base loss: 16008.69
[INFO 2017-06-29 20:53:14,565 main.py:57] epoch 1205, training loss: 6957.17, average training loss: 7513.94, base loss: 16009.86
[INFO 2017-06-29 20:53:17,442 main.py:57] epoch 1206, training loss: 6710.55, average training loss: 7512.38, base loss: 16008.53
[INFO 2017-06-29 20:53:20,291 main.py:57] epoch 1207, training loss: 6882.08, average training loss: 7509.82, base loss: 16008.47
[INFO 2017-06-29 20:53:23,159 main.py:57] epoch 1208, training loss: 6651.57, average training loss: 7507.33, base loss: 16005.18
[INFO 2017-06-29 20:53:25,958 main.py:57] epoch 1209, training loss: 6939.58, average training loss: 7505.35, base loss: 16004.10
[INFO 2017-06-29 20:53:28,862 main.py:57] epoch 1210, training loss: 7051.85, average training loss: 7503.52, base loss: 16005.82
[INFO 2017-06-29 20:53:31,725 main.py:57] epoch 1211, training loss: 7206.03, average training loss: 7502.35, base loss: 16007.09
[INFO 2017-06-29 20:53:34,628 main.py:57] epoch 1212, training loss: 6989.90, average training loss: 7500.76, base loss: 16006.41
[INFO 2017-06-29 20:53:37,437 main.py:57] epoch 1213, training loss: 6230.77, average training loss: 7498.97, base loss: 16003.40
[INFO 2017-06-29 20:53:40,298 main.py:57] epoch 1214, training loss: 7124.65, average training loss: 7498.05, base loss: 16003.33
[INFO 2017-06-29 20:53:43,164 main.py:57] epoch 1215, training loss: 6562.61, average training loss: 7496.40, base loss: 16002.23
[INFO 2017-06-29 20:53:46,033 main.py:57] epoch 1216, training loss: 6842.17, average training loss: 7494.17, base loss: 16000.31
[INFO 2017-06-29 20:53:48,887 main.py:57] epoch 1217, training loss: 7070.91, average training loss: 7492.67, base loss: 16000.12
[INFO 2017-06-29 20:53:51,743 main.py:57] epoch 1218, training loss: 6527.91, average training loss: 7491.37, base loss: 15998.34
[INFO 2017-06-29 20:53:54,635 main.py:57] epoch 1219, training loss: 7280.18, average training loss: 7489.97, base loss: 15998.88
[INFO 2017-06-29 20:53:57,507 main.py:57] epoch 1220, training loss: 7166.13, average training loss: 7488.61, base loss: 15999.87
[INFO 2017-06-29 20:54:00,400 main.py:57] epoch 1221, training loss: 7364.91, average training loss: 7487.45, base loss: 16000.01
[INFO 2017-06-29 20:54:03,251 main.py:57] epoch 1222, training loss: 6193.91, average training loss: 7484.34, base loss: 15998.13
[INFO 2017-06-29 20:54:06,130 main.py:57] epoch 1223, training loss: 7445.88, average training loss: 7483.00, base loss: 16000.16
[INFO 2017-06-29 20:54:08,969 main.py:57] epoch 1224, training loss: 6969.12, average training loss: 7480.76, base loss: 16000.21
[INFO 2017-06-29 20:54:11,805 main.py:57] epoch 1225, training loss: 6440.42, average training loss: 7479.15, base loss: 15998.79
[INFO 2017-06-29 20:54:14,717 main.py:57] epoch 1226, training loss: 6738.50, average training loss: 7477.72, base loss: 15999.03
[INFO 2017-06-29 20:54:17,580 main.py:57] epoch 1227, training loss: 6348.83, average training loss: 7475.06, base loss: 15998.50
[INFO 2017-06-29 20:54:20,466 main.py:57] epoch 1228, training loss: 7166.91, average training loss: 7474.06, base loss: 15997.86
[INFO 2017-06-29 20:54:23,318 main.py:57] epoch 1229, training loss: 6429.79, average training loss: 7472.46, base loss: 15995.08
[INFO 2017-06-29 20:54:26,186 main.py:57] epoch 1230, training loss: 7035.90, average training loss: 7471.77, base loss: 15994.97
[INFO 2017-06-29 20:54:29,039 main.py:57] epoch 1231, training loss: 6956.44, average training loss: 7470.20, base loss: 15994.20
[INFO 2017-06-29 20:54:31,908 main.py:57] epoch 1232, training loss: 7398.67, average training loss: 7469.47, base loss: 15995.76
[INFO 2017-06-29 20:54:34,790 main.py:57] epoch 1233, training loss: 7369.57, average training loss: 7468.16, base loss: 15996.60
[INFO 2017-06-29 20:54:37,649 main.py:57] epoch 1234, training loss: 6616.41, average training loss: 7465.34, base loss: 15994.71
[INFO 2017-06-29 20:54:40,510 main.py:57] epoch 1235, training loss: 6913.23, average training loss: 7463.81, base loss: 15992.94
[INFO 2017-06-29 20:54:43,373 main.py:57] epoch 1236, training loss: 7017.51, average training loss: 7462.75, base loss: 15993.73
[INFO 2017-06-29 20:54:46,229 main.py:57] epoch 1237, training loss: 7212.68, average training loss: 7461.11, base loss: 15996.57
[INFO 2017-06-29 20:54:49,105 main.py:57] epoch 1238, training loss: 7387.33, average training loss: 7459.64, base loss: 15999.98
[INFO 2017-06-29 20:54:51,983 main.py:57] epoch 1239, training loss: 7086.50, average training loss: 7458.29, base loss: 16001.06
[INFO 2017-06-29 20:54:54,849 main.py:57] epoch 1240, training loss: 6641.40, average training loss: 7456.02, base loss: 16000.04
[INFO 2017-06-29 20:54:57,688 main.py:57] epoch 1241, training loss: 6161.26, average training loss: 7453.53, base loss: 15999.54
[INFO 2017-06-29 20:55:00,569 main.py:57] epoch 1242, training loss: 7210.97, average training loss: 7451.50, base loss: 16001.51
[INFO 2017-06-29 20:55:03,398 main.py:57] epoch 1243, training loss: 7561.85, average training loss: 7450.83, base loss: 16003.65
[INFO 2017-06-29 20:55:06,235 main.py:57] epoch 1244, training loss: 6768.43, average training loss: 7449.54, base loss: 16003.64
[INFO 2017-06-29 20:55:09,072 main.py:57] epoch 1245, training loss: 7273.45, average training loss: 7448.67, base loss: 16004.67
[INFO 2017-06-29 20:55:11,998 main.py:57] epoch 1246, training loss: 7938.67, average training loss: 7448.18, base loss: 16008.55
[INFO 2017-06-29 20:55:14,834 main.py:57] epoch 1247, training loss: 7535.42, average training loss: 7447.06, base loss: 16010.64
[INFO 2017-06-29 20:55:17,671 main.py:57] epoch 1248, training loss: 7494.82, average training loss: 7446.53, base loss: 16012.59
[INFO 2017-06-29 20:55:20,480 main.py:57] epoch 1249, training loss: 6479.84, average training loss: 7444.16, base loss: 16008.31
[INFO 2017-06-29 20:55:23,323 main.py:57] epoch 1250, training loss: 6911.46, average training loss: 7442.13, base loss: 16006.36
[INFO 2017-06-29 20:55:26,187 main.py:57] epoch 1251, training loss: 6364.37, average training loss: 7440.49, base loss: 16002.95
[INFO 2017-06-29 20:55:29,087 main.py:57] epoch 1252, training loss: 7365.65, average training loss: 7440.72, base loss: 16005.02
[INFO 2017-06-29 20:55:31,961 main.py:57] epoch 1253, training loss: 7417.67, average training loss: 7439.19, base loss: 16007.39
[INFO 2017-06-29 20:55:34,820 main.py:57] epoch 1254, training loss: 7655.14, average training loss: 7438.43, base loss: 16009.18
[INFO 2017-06-29 20:55:37,671 main.py:57] epoch 1255, training loss: 6877.38, average training loss: 7435.97, base loss: 16007.81
[INFO 2017-06-29 20:55:40,547 main.py:57] epoch 1256, training loss: 6891.95, average training loss: 7434.69, base loss: 16007.31
[INFO 2017-06-29 20:55:43,409 main.py:57] epoch 1257, training loss: 6814.44, average training loss: 7433.30, base loss: 16006.48
[INFO 2017-06-29 20:55:46,246 main.py:57] epoch 1258, training loss: 7009.15, average training loss: 7431.35, base loss: 16005.87
[INFO 2017-06-29 20:55:49,137 main.py:57] epoch 1259, training loss: 6646.54, average training loss: 7429.80, base loss: 16004.18
[INFO 2017-06-29 20:55:52,020 main.py:57] epoch 1260, training loss: 6891.01, average training loss: 7428.60, base loss: 16004.85
[INFO 2017-06-29 20:55:54,888 main.py:57] epoch 1261, training loss: 7299.29, average training loss: 7427.27, base loss: 16006.22
[INFO 2017-06-29 20:55:57,749 main.py:57] epoch 1262, training loss: 6353.75, average training loss: 7425.48, base loss: 16002.94
[INFO 2017-06-29 20:56:00,662 main.py:57] epoch 1263, training loss: 7094.44, average training loss: 7425.00, base loss: 16003.77
[INFO 2017-06-29 20:56:03,552 main.py:57] epoch 1264, training loss: 7750.53, average training loss: 7424.67, base loss: 16006.64
[INFO 2017-06-29 20:56:06,465 main.py:57] epoch 1265, training loss: 6571.78, average training loss: 7421.98, base loss: 16006.20
[INFO 2017-06-29 20:56:09,407 main.py:57] epoch 1266, training loss: 6804.67, average training loss: 7420.73, base loss: 16005.39
[INFO 2017-06-29 20:56:12,283 main.py:57] epoch 1267, training loss: 6757.93, average training loss: 7417.99, base loss: 16005.89
[INFO 2017-06-29 20:56:15,141 main.py:57] epoch 1268, training loss: 7146.75, average training loss: 7416.59, base loss: 16007.25
[INFO 2017-06-29 20:56:18,019 main.py:57] epoch 1269, training loss: 6884.39, average training loss: 7415.72, base loss: 16007.56
[INFO 2017-06-29 20:56:20,906 main.py:57] epoch 1270, training loss: 6796.28, average training loss: 7414.26, base loss: 16007.20
[INFO 2017-06-29 20:56:23,772 main.py:57] epoch 1271, training loss: 6886.35, average training loss: 7412.97, base loss: 16007.92
[INFO 2017-06-29 20:56:26,605 main.py:57] epoch 1272, training loss: 7068.44, average training loss: 7412.16, base loss: 16007.81
[INFO 2017-06-29 20:56:29,440 main.py:57] epoch 1273, training loss: 6544.99, average training loss: 7410.65, base loss: 16006.48
[INFO 2017-06-29 20:56:32,283 main.py:57] epoch 1274, training loss: 7250.79, average training loss: 7409.36, base loss: 16007.42
[INFO 2017-06-29 20:56:35,130 main.py:57] epoch 1275, training loss: 7525.70, average training loss: 7408.43, base loss: 16008.82
[INFO 2017-06-29 20:56:37,979 main.py:57] epoch 1276, training loss: 7281.49, average training loss: 7407.52, base loss: 16009.44
[INFO 2017-06-29 20:56:40,813 main.py:57] epoch 1277, training loss: 7174.26, average training loss: 7406.16, base loss: 16009.26
[INFO 2017-06-29 20:56:43,642 main.py:57] epoch 1278, training loss: 6851.48, average training loss: 7404.89, base loss: 16006.27
[INFO 2017-06-29 20:56:46,519 main.py:57] epoch 1279, training loss: 6280.08, average training loss: 7403.39, base loss: 16004.13
[INFO 2017-06-29 20:56:49,397 main.py:57] epoch 1280, training loss: 6402.93, average training loss: 7401.22, base loss: 16002.41
[INFO 2017-06-29 20:56:52,312 main.py:57] epoch 1281, training loss: 7258.87, average training loss: 7400.58, base loss: 16003.14
[INFO 2017-06-29 20:56:55,142 main.py:57] epoch 1282, training loss: 6790.02, average training loss: 7398.30, base loss: 16001.82
[INFO 2017-06-29 20:56:58,017 main.py:57] epoch 1283, training loss: 7367.75, average training loss: 7397.63, base loss: 16001.30
[INFO 2017-06-29 20:57:00,909 main.py:57] epoch 1284, training loss: 7867.99, average training loss: 7397.49, base loss: 16003.11
[INFO 2017-06-29 20:57:03,756 main.py:57] epoch 1285, training loss: 6624.86, average training loss: 7396.37, base loss: 16001.54
[INFO 2017-06-29 20:57:06,620 main.py:57] epoch 1286, training loss: 7402.96, average training loss: 7394.35, base loss: 16002.50
[INFO 2017-06-29 20:57:09,512 main.py:57] epoch 1287, training loss: 7592.14, average training loss: 7393.31, base loss: 16003.84
[INFO 2017-06-29 20:57:12,365 main.py:57] epoch 1288, training loss: 7643.46, average training loss: 7392.24, base loss: 16007.31
[INFO 2017-06-29 20:57:15,222 main.py:57] epoch 1289, training loss: 6507.16, average training loss: 7391.83, base loss: 16006.35
[INFO 2017-06-29 20:57:18,118 main.py:57] epoch 1290, training loss: 7205.88, average training loss: 7390.99, base loss: 16009.08
[INFO 2017-06-29 20:57:20,992 main.py:57] epoch 1291, training loss: 6651.25, average training loss: 7389.87, base loss: 16007.06
[INFO 2017-06-29 20:57:23,890 main.py:57] epoch 1292, training loss: 7451.85, average training loss: 7389.20, base loss: 16007.15
[INFO 2017-06-29 20:57:26,790 main.py:57] epoch 1293, training loss: 6872.15, average training loss: 7388.40, base loss: 16006.64
[INFO 2017-06-29 20:57:29,695 main.py:57] epoch 1294, training loss: 7228.11, average training loss: 7386.75, base loss: 16006.57
[INFO 2017-06-29 20:57:32,563 main.py:57] epoch 1295, training loss: 7380.71, average training loss: 7385.99, base loss: 16008.07
[INFO 2017-06-29 20:57:35,406 main.py:57] epoch 1296, training loss: 6765.93, average training loss: 7384.30, base loss: 16008.85
[INFO 2017-06-29 20:57:38,341 main.py:57] epoch 1297, training loss: 6931.98, average training loss: 7382.35, base loss: 16008.56
[INFO 2017-06-29 20:57:41,195 main.py:57] epoch 1298, training loss: 6797.16, average training loss: 7381.50, base loss: 16006.65
[INFO 2017-06-29 20:57:44,059 main.py:57] epoch 1299, training loss: 7182.24, average training loss: 7381.03, base loss: 16005.60
[INFO 2017-06-29 20:57:44,060 main.py:59] epoch 1299, testing
[INFO 2017-06-29 20:57:56,276 main.py:104] average testing loss: 7041.15, base loss: 16314.59
[INFO 2017-06-29 20:57:56,276 main.py:105] improve_loss: 9273.44, improve_percent: 0.57
[INFO 2017-06-29 20:57:56,278 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 20:57:56,303 main.py:71] current best improved percent: 0.57
[INFO 2017-06-29 20:57:59,144 main.py:57] epoch 1300, training loss: 7265.08, average training loss: 7380.58, base loss: 16006.92
[INFO 2017-06-29 20:58:01,992 main.py:57] epoch 1301, training loss: 7074.58, average training loss: 7379.51, base loss: 16007.74
[INFO 2017-06-29 20:58:04,902 main.py:57] epoch 1302, training loss: 6548.07, average training loss: 7378.66, base loss: 16006.17
[INFO 2017-06-29 20:58:07,772 main.py:57] epoch 1303, training loss: 6924.75, average training loss: 7376.46, base loss: 16005.74
[INFO 2017-06-29 20:58:10,629 main.py:57] epoch 1304, training loss: 6907.66, average training loss: 7374.52, base loss: 16006.03
[INFO 2017-06-29 20:58:13,454 main.py:57] epoch 1305, training loss: 7326.89, average training loss: 7374.34, base loss: 16007.31
[INFO 2017-06-29 20:58:16,323 main.py:57] epoch 1306, training loss: 7191.90, average training loss: 7373.45, base loss: 16008.27
[INFO 2017-06-29 20:58:19,184 main.py:57] epoch 1307, training loss: 6663.92, average training loss: 7372.08, base loss: 16007.87
[INFO 2017-06-29 20:58:22,044 main.py:57] epoch 1308, training loss: 6789.67, average training loss: 7370.20, base loss: 16007.10
[INFO 2017-06-29 20:58:24,892 main.py:57] epoch 1309, training loss: 7052.52, average training loss: 7369.44, base loss: 16007.68
[INFO 2017-06-29 20:58:27,779 main.py:57] epoch 1310, training loss: 7108.88, average training loss: 7368.87, base loss: 16008.98
[INFO 2017-06-29 20:58:30,631 main.py:57] epoch 1311, training loss: 6570.08, average training loss: 7367.50, base loss: 16009.20
[INFO 2017-06-29 20:58:33,491 main.py:57] epoch 1312, training loss: 6805.47, average training loss: 7365.77, base loss: 16006.94
[INFO 2017-06-29 20:58:36,373 main.py:57] epoch 1313, training loss: 7033.91, average training loss: 7363.98, base loss: 16006.94
[INFO 2017-06-29 20:58:39,263 main.py:57] epoch 1314, training loss: 6438.57, average training loss: 7361.81, base loss: 16005.59
[INFO 2017-06-29 20:58:42,162 main.py:57] epoch 1315, training loss: 6836.45, average training loss: 7360.74, base loss: 16004.86
[INFO 2017-06-29 20:58:45,008 main.py:57] epoch 1316, training loss: 6419.63, average training loss: 7359.10, base loss: 16003.47
[INFO 2017-06-29 20:58:47,889 main.py:57] epoch 1317, training loss: 6963.01, average training loss: 7357.91, base loss: 16003.79
[INFO 2017-06-29 20:58:50,732 main.py:57] epoch 1318, training loss: 6329.53, average training loss: 7356.85, base loss: 16002.47
[INFO 2017-06-29 20:58:53,577 main.py:57] epoch 1319, training loss: 7219.87, average training loss: 7355.42, base loss: 16003.48
[INFO 2017-06-29 20:58:56,474 main.py:57] epoch 1320, training loss: 7166.64, average training loss: 7354.61, base loss: 16003.97
[INFO 2017-06-29 20:58:59,375 main.py:57] epoch 1321, training loss: 6722.92, average training loss: 7353.26, base loss: 16004.38
[INFO 2017-06-29 20:59:02,272 main.py:57] epoch 1322, training loss: 7111.33, average training loss: 7351.96, base loss: 16003.77
[INFO 2017-06-29 20:59:05,129 main.py:57] epoch 1323, training loss: 6454.67, average training loss: 7349.89, base loss: 16002.89
[INFO 2017-06-29 20:59:08,023 main.py:57] epoch 1324, training loss: 7481.39, average training loss: 7348.81, base loss: 16006.85
[INFO 2017-06-29 20:59:10,891 main.py:57] epoch 1325, training loss: 6472.51, average training loss: 7347.46, base loss: 16006.03
[INFO 2017-06-29 20:59:13,770 main.py:57] epoch 1326, training loss: 6704.28, average training loss: 7345.76, base loss: 16005.45
[INFO 2017-06-29 20:59:16,669 main.py:57] epoch 1327, training loss: 6728.98, average training loss: 7344.69, base loss: 16005.57
[INFO 2017-06-29 20:59:19,539 main.py:57] epoch 1328, training loss: 6549.66, average training loss: 7342.58, base loss: 16005.30
[INFO 2017-06-29 20:59:22,409 main.py:57] epoch 1329, training loss: 7081.59, average training loss: 7342.04, base loss: 16006.23
[INFO 2017-06-29 20:59:25,277 main.py:57] epoch 1330, training loss: 7079.80, average training loss: 7341.17, base loss: 16007.30
[INFO 2017-06-29 20:59:28,152 main.py:57] epoch 1331, training loss: 7009.00, average training loss: 7339.37, base loss: 16006.78
[INFO 2017-06-29 20:59:30,997 main.py:57] epoch 1332, training loss: 6765.13, average training loss: 7338.31, base loss: 16005.75
[INFO 2017-06-29 20:59:33,863 main.py:57] epoch 1333, training loss: 6449.96, average training loss: 7336.44, base loss: 16004.49
[INFO 2017-06-29 20:59:36,749 main.py:57] epoch 1334, training loss: 7765.24, average training loss: 7336.43, base loss: 16006.25
[INFO 2017-06-29 20:59:39,611 main.py:57] epoch 1335, training loss: 6204.56, average training loss: 7334.35, base loss: 16004.75
[INFO 2017-06-29 20:59:42,493 main.py:57] epoch 1336, training loss: 7032.13, average training loss: 7333.06, base loss: 16005.24
[INFO 2017-06-29 20:59:45,362 main.py:57] epoch 1337, training loss: 6843.97, average training loss: 7332.86, base loss: 16005.98
[INFO 2017-06-29 20:59:48,253 main.py:57] epoch 1338, training loss: 6640.64, average training loss: 7332.20, base loss: 16005.23
[INFO 2017-06-29 20:59:51,110 main.py:57] epoch 1339, training loss: 6885.79, average training loss: 7331.56, base loss: 16005.45
[INFO 2017-06-29 20:59:54,006 main.py:57] epoch 1340, training loss: 6978.44, average training loss: 7330.90, base loss: 16004.62
[INFO 2017-06-29 20:59:56,889 main.py:57] epoch 1341, training loss: 6424.48, average training loss: 7328.98, base loss: 16002.27
[INFO 2017-06-29 20:59:59,799 main.py:57] epoch 1342, training loss: 6594.31, average training loss: 7326.54, base loss: 16000.42
[INFO 2017-06-29 21:00:02,667 main.py:57] epoch 1343, training loss: 6835.97, average training loss: 7325.72, base loss: 15998.18
[INFO 2017-06-29 21:00:05,570 main.py:57] epoch 1344, training loss: 6632.56, average training loss: 7324.31, base loss: 15996.89
[INFO 2017-06-29 21:00:08,446 main.py:57] epoch 1345, training loss: 7049.65, average training loss: 7323.92, base loss: 15996.65
[INFO 2017-06-29 21:00:11,298 main.py:57] epoch 1346, training loss: 6454.48, average training loss: 7322.10, base loss: 15993.79
[INFO 2017-06-29 21:00:14,148 main.py:57] epoch 1347, training loss: 6817.21, average training loss: 7320.98, base loss: 15991.74
[INFO 2017-06-29 21:00:16,989 main.py:57] epoch 1348, training loss: 6963.59, average training loss: 7319.92, base loss: 15991.25
[INFO 2017-06-29 21:00:19,850 main.py:57] epoch 1349, training loss: 7288.83, average training loss: 7319.43, base loss: 15992.46
[INFO 2017-06-29 21:00:22,729 main.py:57] epoch 1350, training loss: 7139.41, average training loss: 7317.93, base loss: 15993.94
[INFO 2017-06-29 21:00:25,571 main.py:57] epoch 1351, training loss: 7101.15, average training loss: 7317.00, base loss: 15994.87
[INFO 2017-06-29 21:00:28,427 main.py:57] epoch 1352, training loss: 6476.18, average training loss: 7316.21, base loss: 15994.36
[INFO 2017-06-29 21:00:31,313 main.py:57] epoch 1353, training loss: 7244.69, average training loss: 7315.52, base loss: 15996.62
[INFO 2017-06-29 21:00:34,195 main.py:57] epoch 1354, training loss: 6563.97, average training loss: 7314.12, base loss: 15995.70
[INFO 2017-06-29 21:00:37,043 main.py:57] epoch 1355, training loss: 6750.09, average training loss: 7313.29, base loss: 15995.14
[INFO 2017-06-29 21:00:39,908 main.py:57] epoch 1356, training loss: 7294.33, average training loss: 7312.10, base loss: 15995.53
[INFO 2017-06-29 21:00:42,763 main.py:57] epoch 1357, training loss: 7165.68, average training loss: 7310.80, base loss: 15994.99
[INFO 2017-06-29 21:00:45,657 main.py:57] epoch 1358, training loss: 6483.76, average training loss: 7309.35, base loss: 15994.11
[INFO 2017-06-29 21:00:48,506 main.py:57] epoch 1359, training loss: 6762.63, average training loss: 7307.38, base loss: 15993.45
[INFO 2017-06-29 21:00:51,407 main.py:57] epoch 1360, training loss: 7870.81, average training loss: 7307.28, base loss: 15996.46
[INFO 2017-06-29 21:00:54,303 main.py:57] epoch 1361, training loss: 7127.06, average training loss: 7306.81, base loss: 15997.00
[INFO 2017-06-29 21:00:57,142 main.py:57] epoch 1362, training loss: 7027.36, average training loss: 7305.26, base loss: 15996.40
[INFO 2017-06-29 21:01:00,017 main.py:57] epoch 1363, training loss: 7111.90, average training loss: 7304.53, base loss: 15996.74
[INFO 2017-06-29 21:01:02,919 main.py:57] epoch 1364, training loss: 6711.39, average training loss: 7302.18, base loss: 15996.76
[INFO 2017-06-29 21:01:05,774 main.py:57] epoch 1365, training loss: 6716.10, average training loss: 7300.14, base loss: 15995.61
[INFO 2017-06-29 21:01:08,654 main.py:57] epoch 1366, training loss: 7098.71, average training loss: 7299.15, base loss: 15996.50
[INFO 2017-06-29 21:01:11,520 main.py:57] epoch 1367, training loss: 6173.79, average training loss: 7296.97, base loss: 15994.23
[INFO 2017-06-29 21:01:14,360 main.py:57] epoch 1368, training loss: 6858.33, average training loss: 7295.97, base loss: 15994.80
[INFO 2017-06-29 21:01:17,208 main.py:57] epoch 1369, training loss: 6676.39, average training loss: 7295.45, base loss: 15994.27
[INFO 2017-06-29 21:01:20,080 main.py:57] epoch 1370, training loss: 6712.19, average training loss: 7294.49, base loss: 15994.20
[INFO 2017-06-29 21:01:22,945 main.py:57] epoch 1371, training loss: 7040.18, average training loss: 7293.21, base loss: 15994.34
[INFO 2017-06-29 21:01:25,844 main.py:57] epoch 1372, training loss: 6690.54, average training loss: 7291.83, base loss: 15993.81
[INFO 2017-06-29 21:01:28,693 main.py:57] epoch 1373, training loss: 6713.00, average training loss: 7291.12, base loss: 15992.95
[INFO 2017-06-29 21:01:31,584 main.py:57] epoch 1374, training loss: 6681.48, average training loss: 7289.51, base loss: 15992.85
[INFO 2017-06-29 21:01:34,434 main.py:57] epoch 1375, training loss: 6521.84, average training loss: 7287.79, base loss: 15992.19
[INFO 2017-06-29 21:01:37,249 main.py:57] epoch 1376, training loss: 7044.29, average training loss: 7286.41, base loss: 15993.05
[INFO 2017-06-29 21:01:40,101 main.py:57] epoch 1377, training loss: 7636.06, average training loss: 7285.39, base loss: 15994.26
[INFO 2017-06-29 21:01:42,961 main.py:57] epoch 1378, training loss: 7250.92, average training loss: 7284.94, base loss: 15996.32
[INFO 2017-06-29 21:01:45,818 main.py:57] epoch 1379, training loss: 6730.29, average training loss: 7283.52, base loss: 15998.36
[INFO 2017-06-29 21:01:48,664 main.py:57] epoch 1380, training loss: 6775.36, average training loss: 7282.45, base loss: 15996.92
[INFO 2017-06-29 21:01:51,532 main.py:57] epoch 1381, training loss: 6510.22, average training loss: 7280.90, base loss: 15995.88
[INFO 2017-06-29 21:01:54,430 main.py:57] epoch 1382, training loss: 7505.08, average training loss: 7280.78, base loss: 15997.81
[INFO 2017-06-29 21:01:57,318 main.py:57] epoch 1383, training loss: 6659.20, average training loss: 7280.51, base loss: 15996.53
[INFO 2017-06-29 21:02:00,164 main.py:57] epoch 1384, training loss: 6965.98, average training loss: 7279.79, base loss: 15997.25
[INFO 2017-06-29 21:02:03,050 main.py:57] epoch 1385, training loss: 6757.36, average training loss: 7279.15, base loss: 15997.13
[INFO 2017-06-29 21:02:05,937 main.py:57] epoch 1386, training loss: 6660.10, average training loss: 7277.64, base loss: 15996.96
[INFO 2017-06-29 21:02:08,818 main.py:57] epoch 1387, training loss: 6853.73, average training loss: 7276.41, base loss: 15996.61
[INFO 2017-06-29 21:02:11,689 main.py:57] epoch 1388, training loss: 7007.47, average training loss: 7275.13, base loss: 15997.06
[INFO 2017-06-29 21:02:14,544 main.py:57] epoch 1389, training loss: 6403.45, average training loss: 7273.49, base loss: 15995.07
[INFO 2017-06-29 21:02:17,421 main.py:57] epoch 1390, training loss: 6895.57, average training loss: 7272.28, base loss: 15993.72
[INFO 2017-06-29 21:02:20,278 main.py:57] epoch 1391, training loss: 6881.99, average training loss: 7269.94, base loss: 15994.55
[INFO 2017-06-29 21:02:23,174 main.py:57] epoch 1392, training loss: 7185.89, average training loss: 7268.99, base loss: 15995.73
[INFO 2017-06-29 21:02:26,041 main.py:57] epoch 1393, training loss: 6592.33, average training loss: 7267.22, base loss: 15993.40
[INFO 2017-06-29 21:02:28,958 main.py:57] epoch 1394, training loss: 6795.06, average training loss: 7266.48, base loss: 15993.50
[INFO 2017-06-29 21:02:31,814 main.py:57] epoch 1395, training loss: 7759.02, average training loss: 7266.72, base loss: 15996.23
[INFO 2017-06-29 21:02:34,660 main.py:57] epoch 1396, training loss: 6872.61, average training loss: 7265.39, base loss: 15996.07
[INFO 2017-06-29 21:02:37,528 main.py:57] epoch 1397, training loss: 7071.17, average training loss: 7264.60, base loss: 15997.25
[INFO 2017-06-29 21:02:40,406 main.py:57] epoch 1398, training loss: 7351.37, average training loss: 7264.49, base loss: 15999.02
[INFO 2017-06-29 21:02:43,294 main.py:57] epoch 1399, training loss: 7252.82, average training loss: 7263.71, base loss: 15999.83
[INFO 2017-06-29 21:02:43,295 main.py:59] epoch 1399, testing
[INFO 2017-06-29 21:02:55,671 main.py:104] average testing loss: 6776.15, base loss: 15762.40
[INFO 2017-06-29 21:02:55,671 main.py:105] improve_loss: 8986.25, improve_percent: 0.57
[INFO 2017-06-29 21:02:55,672 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 21:02:55,698 main.py:71] current best improved percent: 0.57
[INFO 2017-06-29 21:02:58,601 main.py:57] epoch 1400, training loss: 6854.62, average training loss: 7262.91, base loss: 16000.66
[INFO 2017-06-29 21:03:01,492 main.py:57] epoch 1401, training loss: 6878.10, average training loss: 7262.07, base loss: 16000.52
[INFO 2017-06-29 21:03:04,412 main.py:57] epoch 1402, training loss: 6856.93, average training loss: 7261.12, base loss: 16001.44
[INFO 2017-06-29 21:03:07,274 main.py:57] epoch 1403, training loss: 7599.01, average training loss: 7260.26, base loss: 16003.13
[INFO 2017-06-29 21:03:10,123 main.py:57] epoch 1404, training loss: 6481.51, average training loss: 7258.54, base loss: 16002.43
[INFO 2017-06-29 21:03:12,997 main.py:57] epoch 1405, training loss: 6609.38, average training loss: 7257.43, base loss: 16000.59
[INFO 2017-06-29 21:03:15,832 main.py:57] epoch 1406, training loss: 7375.09, average training loss: 7257.33, base loss: 16003.55
[INFO 2017-06-29 21:03:18,668 main.py:57] epoch 1407, training loss: 7595.75, average training loss: 7257.18, base loss: 16005.37
[INFO 2017-06-29 21:03:21,519 main.py:57] epoch 1408, training loss: 7303.73, average training loss: 7257.05, base loss: 16006.01
[INFO 2017-06-29 21:03:24,389 main.py:57] epoch 1409, training loss: 7050.51, average training loss: 7256.30, base loss: 16005.01
[INFO 2017-06-29 21:03:27,237 main.py:57] epoch 1410, training loss: 7007.12, average training loss: 7255.29, base loss: 16005.03
[INFO 2017-06-29 21:03:30,056 main.py:57] epoch 1411, training loss: 6730.81, average training loss: 7254.07, base loss: 16004.59
[INFO 2017-06-29 21:03:32,928 main.py:57] epoch 1412, training loss: 7017.32, average training loss: 7253.88, base loss: 16004.80
[INFO 2017-06-29 21:03:35,794 main.py:57] epoch 1413, training loss: 7243.56, average training loss: 7253.06, base loss: 16006.82
[INFO 2017-06-29 21:03:38,649 main.py:57] epoch 1414, training loss: 6754.67, average training loss: 7251.98, base loss: 16004.78
[INFO 2017-06-29 21:03:41,532 main.py:57] epoch 1415, training loss: 6692.87, average training loss: 7250.36, base loss: 16002.98
[INFO 2017-06-29 21:03:44,404 main.py:57] epoch 1416, training loss: 7123.86, average training loss: 7249.82, base loss: 16002.64
[INFO 2017-06-29 21:03:47,275 main.py:57] epoch 1417, training loss: 7157.63, average training loss: 7249.48, base loss: 16002.64
[INFO 2017-06-29 21:03:50,145 main.py:57] epoch 1418, training loss: 6907.13, average training loss: 7248.58, base loss: 16003.04
[INFO 2017-06-29 21:03:53,058 main.py:57] epoch 1419, training loss: 6907.93, average training loss: 7247.58, base loss: 16003.25
[INFO 2017-06-29 21:03:55,938 main.py:57] epoch 1420, training loss: 7074.33, average training loss: 7245.93, base loss: 16004.41
[INFO 2017-06-29 21:03:58,816 main.py:57] epoch 1421, training loss: 7508.15, average training loss: 7245.39, base loss: 16005.40
[INFO 2017-06-29 21:04:01,668 main.py:57] epoch 1422, training loss: 6413.06, average training loss: 7243.12, base loss: 16002.39
[INFO 2017-06-29 21:04:04,510 main.py:57] epoch 1423, training loss: 7163.56, average training loss: 7241.72, base loss: 16003.02
[INFO 2017-06-29 21:04:07,433 main.py:57] epoch 1424, training loss: 7754.93, average training loss: 7241.61, base loss: 16006.44
[INFO 2017-06-29 21:04:10,320 main.py:57] epoch 1425, training loss: 6707.49, average training loss: 7240.70, base loss: 16007.54
[INFO 2017-06-29 21:04:13,191 main.py:57] epoch 1426, training loss: 6965.92, average training loss: 7240.28, base loss: 16007.48
[INFO 2017-06-29 21:04:16,024 main.py:57] epoch 1427, training loss: 6912.68, average training loss: 7239.22, base loss: 16006.96
[INFO 2017-06-29 21:04:18,917 main.py:57] epoch 1428, training loss: 7361.82, average training loss: 7238.10, base loss: 16007.21
[INFO 2017-06-29 21:04:21,754 main.py:57] epoch 1429, training loss: 6928.10, average training loss: 7236.89, base loss: 16006.97
[INFO 2017-06-29 21:04:24,589 main.py:57] epoch 1430, training loss: 6909.10, average training loss: 7235.94, base loss: 16006.12
[INFO 2017-06-29 21:04:27,441 main.py:57] epoch 1431, training loss: 7248.40, average training loss: 7235.22, base loss: 16007.40
[INFO 2017-06-29 21:04:30,308 main.py:57] epoch 1432, training loss: 7650.98, average training loss: 7235.11, base loss: 16008.24
[INFO 2017-06-29 21:04:33,156 main.py:57] epoch 1433, training loss: 6828.47, average training loss: 7233.70, base loss: 16007.64
[INFO 2017-06-29 21:04:35,972 main.py:57] epoch 1434, training loss: 7194.64, average training loss: 7233.51, base loss: 16008.47
[INFO 2017-06-29 21:04:38,840 main.py:57] epoch 1435, training loss: 6657.11, average training loss: 7232.09, base loss: 16006.98
[INFO 2017-06-29 21:04:41,702 main.py:57] epoch 1436, training loss: 6759.87, average training loss: 7231.10, base loss: 16006.49
[INFO 2017-06-29 21:04:44,587 main.py:57] epoch 1437, training loss: 7247.73, average training loss: 7231.16, base loss: 16007.73
[INFO 2017-06-29 21:04:47,449 main.py:57] epoch 1438, training loss: 7077.46, average training loss: 7230.62, base loss: 16008.26
[INFO 2017-06-29 21:04:50,312 main.py:57] epoch 1439, training loss: 7065.27, average training loss: 7230.51, base loss: 16009.15
[INFO 2017-06-29 21:04:53,150 main.py:57] epoch 1440, training loss: 6106.31, average training loss: 7227.72, base loss: 16007.45
[INFO 2017-06-29 21:04:55,996 main.py:57] epoch 1441, training loss: 6709.17, average training loss: 7226.85, base loss: 16008.29
[INFO 2017-06-29 21:04:58,840 main.py:57] epoch 1442, training loss: 7148.77, average training loss: 7226.50, base loss: 16010.19
[INFO 2017-06-29 21:05:01,686 main.py:57] epoch 1443, training loss: 6305.92, average training loss: 7225.33, base loss: 16009.01
[INFO 2017-06-29 21:05:04,551 main.py:57] epoch 1444, training loss: 6722.12, average training loss: 7224.39, base loss: 16008.67
[INFO 2017-06-29 21:05:07,414 main.py:57] epoch 1445, training loss: 6367.41, average training loss: 7223.43, base loss: 16007.88
[INFO 2017-06-29 21:05:10,326 main.py:57] epoch 1446, training loss: 6782.67, average training loss: 7222.56, base loss: 16007.63
[INFO 2017-06-29 21:05:13,220 main.py:57] epoch 1447, training loss: 6875.57, average training loss: 7221.35, base loss: 16008.66
[INFO 2017-06-29 21:05:16,127 main.py:57] epoch 1448, training loss: 6507.93, average training loss: 7220.80, base loss: 16008.27
[INFO 2017-06-29 21:05:18,995 main.py:57] epoch 1449, training loss: 6675.66, average training loss: 7220.35, base loss: 16007.88
[INFO 2017-06-29 21:05:21,841 main.py:57] epoch 1450, training loss: 7075.31, average training loss: 7219.74, base loss: 16007.83
[INFO 2017-06-29 21:05:24,653 main.py:57] epoch 1451, training loss: 6730.38, average training loss: 7219.24, base loss: 16007.81
[INFO 2017-06-29 21:05:27,553 main.py:57] epoch 1452, training loss: 6789.70, average training loss: 7218.14, base loss: 16008.62
[INFO 2017-06-29 21:05:30,422 main.py:57] epoch 1453, training loss: 6582.32, average training loss: 7217.09, base loss: 16009.44
[INFO 2017-06-29 21:05:33,330 main.py:57] epoch 1454, training loss: 6282.83, average training loss: 7215.34, base loss: 16007.82
[INFO 2017-06-29 21:05:36,213 main.py:57] epoch 1455, training loss: 6709.86, average training loss: 7214.50, base loss: 16007.82
[INFO 2017-06-29 21:05:39,089 main.py:57] epoch 1456, training loss: 6479.29, average training loss: 7213.19, base loss: 16007.24
[INFO 2017-06-29 21:05:41,970 main.py:57] epoch 1457, training loss: 6567.84, average training loss: 7211.51, base loss: 16007.57
[INFO 2017-06-29 21:05:44,839 main.py:57] epoch 1458, training loss: 6812.59, average training loss: 7210.05, base loss: 16008.54
[INFO 2017-06-29 21:05:47,707 main.py:57] epoch 1459, training loss: 7508.73, average training loss: 7209.80, base loss: 16011.62
[INFO 2017-06-29 21:05:50,587 main.py:57] epoch 1460, training loss: 6494.76, average training loss: 7208.09, base loss: 16011.63
[INFO 2017-06-29 21:05:53,452 main.py:57] epoch 1461, training loss: 6728.80, average training loss: 7207.45, base loss: 16011.37
[INFO 2017-06-29 21:05:56,304 main.py:57] epoch 1462, training loss: 6976.56, average training loss: 7206.84, base loss: 16011.42
[INFO 2017-06-29 21:05:59,175 main.py:57] epoch 1463, training loss: 6890.51, average training loss: 7205.97, base loss: 16010.49
[INFO 2017-06-29 21:06:02,043 main.py:57] epoch 1464, training loss: 6796.75, average training loss: 7205.44, base loss: 16011.03
[INFO 2017-06-29 21:06:04,929 main.py:57] epoch 1465, training loss: 6240.10, average training loss: 7203.85, base loss: 16010.82
[INFO 2017-06-29 21:06:07,825 main.py:57] epoch 1466, training loss: 6823.93, average training loss: 7202.18, base loss: 16012.22
[INFO 2017-06-29 21:06:10,763 main.py:57] epoch 1467, training loss: 7122.42, average training loss: 7201.08, base loss: 16014.37
[INFO 2017-06-29 21:06:13,637 main.py:57] epoch 1468, training loss: 6945.05, average training loss: 7200.22, base loss: 16015.97
[INFO 2017-06-29 21:06:16,552 main.py:57] epoch 1469, training loss: 6564.64, average training loss: 7198.55, base loss: 16015.93
[INFO 2017-06-29 21:06:19,431 main.py:57] epoch 1470, training loss: 6494.32, average training loss: 7197.35, base loss: 16015.34
[INFO 2017-06-29 21:06:22,318 main.py:57] epoch 1471, training loss: 6904.59, average training loss: 7196.72, base loss: 16016.38
[INFO 2017-06-29 21:06:25,224 main.py:57] epoch 1472, training loss: 6269.01, average training loss: 7196.06, base loss: 16015.30
[INFO 2017-06-29 21:06:28,077 main.py:57] epoch 1473, training loss: 7567.57, average training loss: 7196.15, base loss: 16017.90
[INFO 2017-06-29 21:06:30,959 main.py:57] epoch 1474, training loss: 6618.77, average training loss: 7195.04, base loss: 16018.05
[INFO 2017-06-29 21:06:33,780 main.py:57] epoch 1475, training loss: 6196.15, average training loss: 7193.36, base loss: 16014.74
[INFO 2017-06-29 21:06:36,676 main.py:57] epoch 1476, training loss: 6500.30, average training loss: 7192.24, base loss: 16014.91
[INFO 2017-06-29 21:06:39,556 main.py:57] epoch 1477, training loss: 6856.53, average training loss: 7191.45, base loss: 16014.47
[INFO 2017-06-29 21:06:42,447 main.py:57] epoch 1478, training loss: 6453.50, average training loss: 7191.14, base loss: 16013.54
[INFO 2017-06-29 21:06:45,322 main.py:57] epoch 1479, training loss: 6800.97, average training loss: 7190.96, base loss: 16014.11
[INFO 2017-06-29 21:06:48,195 main.py:57] epoch 1480, training loss: 6420.98, average training loss: 7189.80, base loss: 16012.26
[INFO 2017-06-29 21:06:51,027 main.py:57] epoch 1481, training loss: 7405.87, average training loss: 7188.91, base loss: 16012.12
[INFO 2017-06-29 21:06:53,893 main.py:57] epoch 1482, training loss: 6579.44, average training loss: 7188.20, base loss: 16011.97
[INFO 2017-06-29 21:06:56,773 main.py:57] epoch 1483, training loss: 6557.80, average training loss: 7187.29, base loss: 16011.39
[INFO 2017-06-29 21:06:59,604 main.py:57] epoch 1484, training loss: 7106.33, average training loss: 7186.93, base loss: 16013.61
[INFO 2017-06-29 21:07:02,510 main.py:57] epoch 1485, training loss: 6784.76, average training loss: 7186.59, base loss: 16013.88
[INFO 2017-06-29 21:07:05,389 main.py:57] epoch 1486, training loss: 6744.91, average training loss: 7185.73, base loss: 16013.32
[INFO 2017-06-29 21:07:08,272 main.py:57] epoch 1487, training loss: 6693.88, average training loss: 7184.22, base loss: 16012.97
[INFO 2017-06-29 21:07:11,145 main.py:57] epoch 1488, training loss: 6548.71, average training loss: 7183.00, base loss: 16013.00
[INFO 2017-06-29 21:07:14,014 main.py:57] epoch 1489, training loss: 7546.01, average training loss: 7183.45, base loss: 16014.99
[INFO 2017-06-29 21:07:16,900 main.py:57] epoch 1490, training loss: 7023.77, average training loss: 7181.85, base loss: 16016.77
[INFO 2017-06-29 21:07:19,778 main.py:57] epoch 1491, training loss: 6401.35, average training loss: 7180.77, base loss: 16016.45
[INFO 2017-06-29 21:07:22,591 main.py:57] epoch 1492, training loss: 7212.41, average training loss: 7180.69, base loss: 16018.28
[INFO 2017-06-29 21:07:25,474 main.py:57] epoch 1493, training loss: 7081.30, average training loss: 7180.31, base loss: 16020.35
[INFO 2017-06-29 21:07:28,347 main.py:57] epoch 1494, training loss: 6512.56, average training loss: 7178.85, base loss: 16019.13
[INFO 2017-06-29 21:07:31,174 main.py:57] epoch 1495, training loss: 6875.52, average training loss: 7178.59, base loss: 16020.04
[INFO 2017-06-29 21:07:34,036 main.py:57] epoch 1496, training loss: 6963.70, average training loss: 7178.33, base loss: 16020.02
[INFO 2017-06-29 21:07:36,879 main.py:57] epoch 1497, training loss: 6591.87, average training loss: 7177.83, base loss: 16017.60
[INFO 2017-06-29 21:07:39,751 main.py:57] epoch 1498, training loss: 7044.79, average training loss: 7176.73, base loss: 16017.30
[INFO 2017-06-29 21:07:42,647 main.py:57] epoch 1499, training loss: 6551.18, average training loss: 7174.69, base loss: 16016.22
[INFO 2017-06-29 21:07:42,648 main.py:59] epoch 1499, testing
[INFO 2017-06-29 21:07:55,051 main.py:104] average testing loss: 7017.01, base loss: 16455.18
[INFO 2017-06-29 21:07:55,051 main.py:105] improve_loss: 9438.17, improve_percent: 0.57
[INFO 2017-06-29 21:07:55,053 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 21:07:55,090 main.py:71] current best improved percent: 0.57
[INFO 2017-06-29 21:07:57,997 main.py:57] epoch 1500, training loss: 6889.66, average training loss: 7173.10, base loss: 16016.16
[INFO 2017-06-29 21:08:00,887 main.py:57] epoch 1501, training loss: 7082.59, average training loss: 7172.72, base loss: 16016.37
[INFO 2017-06-29 21:08:03,737 main.py:57] epoch 1502, training loss: 6791.27, average training loss: 7171.75, base loss: 16016.59
[INFO 2017-06-29 21:08:06,596 main.py:57] epoch 1503, training loss: 7054.89, average training loss: 7171.43, base loss: 16017.59
[INFO 2017-06-29 21:08:09,480 main.py:57] epoch 1504, training loss: 6734.99, average training loss: 7170.23, base loss: 16017.30
[INFO 2017-06-29 21:08:12,381 main.py:57] epoch 1505, training loss: 6681.24, average training loss: 7169.04, base loss: 16015.55
[INFO 2017-06-29 21:08:15,244 main.py:57] epoch 1506, training loss: 6682.39, average training loss: 7168.43, base loss: 16016.32
[INFO 2017-06-29 21:08:18,134 main.py:57] epoch 1507, training loss: 6789.06, average training loss: 7167.42, base loss: 16017.36
[INFO 2017-06-29 21:08:20,978 main.py:57] epoch 1508, training loss: 6534.95, average training loss: 7166.51, base loss: 16016.52
[INFO 2017-06-29 21:08:23,799 main.py:57] epoch 1509, training loss: 6995.04, average training loss: 7166.80, base loss: 16017.08
[INFO 2017-06-29 21:08:26,623 main.py:57] epoch 1510, training loss: 6465.56, average training loss: 7166.05, base loss: 16016.16
[INFO 2017-06-29 21:08:29,467 main.py:57] epoch 1511, training loss: 6426.71, average training loss: 7164.58, base loss: 16014.63
[INFO 2017-06-29 21:08:32,352 main.py:57] epoch 1512, training loss: 7107.83, average training loss: 7163.39, base loss: 16015.34
[INFO 2017-06-29 21:08:35,178 main.py:57] epoch 1513, training loss: 6837.83, average training loss: 7162.73, base loss: 16016.15
[INFO 2017-06-29 21:08:38,104 main.py:57] epoch 1514, training loss: 6771.20, average training loss: 7161.92, base loss: 16015.02
[INFO 2017-06-29 21:08:40,997 main.py:57] epoch 1515, training loss: 7225.66, average training loss: 7161.68, base loss: 16016.84
[INFO 2017-06-29 21:08:43,846 main.py:57] epoch 1516, training loss: 6633.94, average training loss: 7160.30, base loss: 16015.45
[INFO 2017-06-29 21:08:46,732 main.py:57] epoch 1517, training loss: 6254.20, average training loss: 7159.20, base loss: 16014.11
[INFO 2017-06-29 21:08:49,630 main.py:57] epoch 1518, training loss: 6602.22, average training loss: 7158.09, base loss: 16014.32
[INFO 2017-06-29 21:08:52,468 main.py:57] epoch 1519, training loss: 7178.64, average training loss: 7157.98, base loss: 16017.59
[INFO 2017-06-29 21:08:55,345 main.py:57] epoch 1520, training loss: 6497.80, average training loss: 7156.96, base loss: 16017.47
[INFO 2017-06-29 21:08:58,254 main.py:57] epoch 1521, training loss: 6759.76, average training loss: 7155.74, base loss: 16017.75
[INFO 2017-06-29 21:09:01,135 main.py:57] epoch 1522, training loss: 7273.87, average training loss: 7154.47, base loss: 16017.97
[INFO 2017-06-29 21:09:04,002 main.py:57] epoch 1523, training loss: 6798.22, average training loss: 7154.03, base loss: 16017.09
[INFO 2017-06-29 21:09:06,866 main.py:57] epoch 1524, training loss: 7469.95, average training loss: 7153.65, base loss: 16017.48
[INFO 2017-06-29 21:09:09,749 main.py:57] epoch 1525, training loss: 7274.76, average training loss: 7153.73, base loss: 16019.63
[INFO 2017-06-29 21:09:12,625 main.py:57] epoch 1526, training loss: 6666.58, average training loss: 7152.36, base loss: 16019.29
[INFO 2017-06-29 21:09:15,538 main.py:57] epoch 1527, training loss: 6847.00, average training loss: 7151.61, base loss: 16020.06
[INFO 2017-06-29 21:09:18,460 main.py:57] epoch 1528, training loss: 5978.30, average training loss: 7148.79, base loss: 16017.77
[INFO 2017-06-29 21:09:21,315 main.py:57] epoch 1529, training loss: 6921.82, average training loss: 7148.13, base loss: 16018.70
[INFO 2017-06-29 21:09:24,201 main.py:57] epoch 1530, training loss: 6442.75, average training loss: 7147.07, base loss: 16018.29
[INFO 2017-06-29 21:09:27,111 main.py:57] epoch 1531, training loss: 7686.21, average training loss: 7146.96, base loss: 16021.48
[INFO 2017-06-29 21:09:30,006 main.py:57] epoch 1532, training loss: 7185.38, average training loss: 7146.79, base loss: 16022.47
[INFO 2017-06-29 21:09:32,880 main.py:57] epoch 1533, training loss: 7093.15, average training loss: 7146.80, base loss: 16022.86
[INFO 2017-06-29 21:09:35,753 main.py:57] epoch 1534, training loss: 6856.41, average training loss: 7146.24, base loss: 16023.66
[INFO 2017-06-29 21:09:38,617 main.py:57] epoch 1535, training loss: 6804.13, average training loss: 7145.71, base loss: 16024.35
[INFO 2017-06-29 21:09:41,432 main.py:57] epoch 1536, training loss: 6626.84, average training loss: 7145.13, base loss: 16022.33
[INFO 2017-06-29 21:09:44,325 main.py:57] epoch 1537, training loss: 6779.89, average training loss: 7143.70, base loss: 16021.14
[INFO 2017-06-29 21:09:47,283 main.py:57] epoch 1538, training loss: 7139.90, average training loss: 7142.63, base loss: 16022.62
[INFO 2017-06-29 21:09:50,122 main.py:57] epoch 1539, training loss: 6750.60, average training loss: 7142.21, base loss: 16022.06
[INFO 2017-06-29 21:09:52,976 main.py:57] epoch 1540, training loss: 6762.14, average training loss: 7142.28, base loss: 16020.52
[INFO 2017-06-29 21:09:55,824 main.py:57] epoch 1541, training loss: 7209.69, average training loss: 7141.71, base loss: 16021.97
[INFO 2017-06-29 21:09:58,720 main.py:57] epoch 1542, training loss: 7008.54, average training loss: 7141.57, base loss: 16022.97
[INFO 2017-06-29 21:10:01,571 main.py:57] epoch 1543, training loss: 6589.25, average training loss: 7139.12, base loss: 16022.39
[INFO 2017-06-29 21:10:04,427 main.py:57] epoch 1544, training loss: 6385.40, average training loss: 7138.18, base loss: 16020.63
[INFO 2017-06-29 21:10:07,306 main.py:57] epoch 1545, training loss: 6786.98, average training loss: 7137.62, base loss: 16018.80
[INFO 2017-06-29 21:10:10,131 main.py:57] epoch 1546, training loss: 6809.09, average training loss: 7136.92, base loss: 16018.17
[INFO 2017-06-29 21:10:12,981 main.py:57] epoch 1547, training loss: 6956.16, average training loss: 7136.86, base loss: 16017.42
[INFO 2017-06-29 21:10:15,927 main.py:57] epoch 1548, training loss: 7255.54, average training loss: 7136.37, base loss: 16018.98
[INFO 2017-06-29 21:10:18,827 main.py:57] epoch 1549, training loss: 6657.17, average training loss: 7135.17, base loss: 16018.41
[INFO 2017-06-29 21:10:21,717 main.py:57] epoch 1550, training loss: 7234.34, average training loss: 7134.31, base loss: 16019.04
[INFO 2017-06-29 21:10:24,541 main.py:57] epoch 1551, training loss: 6978.14, average training loss: 7134.00, base loss: 16019.04
[INFO 2017-06-29 21:10:27,447 main.py:57] epoch 1552, training loss: 7222.36, average training loss: 7133.96, base loss: 16018.38
[INFO 2017-06-29 21:10:30,339 main.py:57] epoch 1553, training loss: 6690.50, average training loss: 7133.45, base loss: 16016.00
[INFO 2017-06-29 21:10:33,203 main.py:57] epoch 1554, training loss: 7295.92, average training loss: 7133.38, base loss: 16017.66
[INFO 2017-06-29 21:10:36,093 main.py:57] epoch 1555, training loss: 6670.68, average training loss: 7132.74, base loss: 16016.91
[INFO 2017-06-29 21:10:38,939 main.py:57] epoch 1556, training loss: 6432.69, average training loss: 7131.48, base loss: 16014.12
[INFO 2017-06-29 21:10:41,779 main.py:57] epoch 1557, training loss: 6514.14, average training loss: 7130.22, base loss: 16012.76
[INFO 2017-06-29 21:10:44,715 main.py:57] epoch 1558, training loss: 6782.60, average training loss: 7128.93, base loss: 16012.68
[INFO 2017-06-29 21:10:47,576 main.py:57] epoch 1559, training loss: 6539.67, average training loss: 7128.24, base loss: 16013.16
[INFO 2017-06-29 21:10:50,480 main.py:57] epoch 1560, training loss: 6344.41, average training loss: 7126.46, base loss: 16011.62
[INFO 2017-06-29 21:10:53,331 main.py:57] epoch 1561, training loss: 6938.09, average training loss: 7126.42, base loss: 16012.12
[INFO 2017-06-29 21:10:56,242 main.py:57] epoch 1562, training loss: 6894.45, average training loss: 7125.55, base loss: 16011.94
[INFO 2017-06-29 21:10:59,125 main.py:57] epoch 1563, training loss: 6584.83, average training loss: 7124.92, base loss: 16010.71
[INFO 2017-06-29 21:11:01,970 main.py:57] epoch 1564, training loss: 6957.56, average training loss: 7123.75, base loss: 16011.88
[INFO 2017-06-29 21:11:04,892 main.py:57] epoch 1565, training loss: 6333.89, average training loss: 7123.32, base loss: 16011.96
[INFO 2017-06-29 21:11:07,801 main.py:57] epoch 1566, training loss: 6989.00, average training loss: 7122.69, base loss: 16012.95
[INFO 2017-06-29 21:11:10,680 main.py:57] epoch 1567, training loss: 6634.12, average training loss: 7121.43, base loss: 16012.37
[INFO 2017-06-29 21:11:13,582 main.py:57] epoch 1568, training loss: 6520.29, average training loss: 7120.29, base loss: 16011.58
[INFO 2017-06-29 21:11:16,446 main.py:57] epoch 1569, training loss: 6539.18, average training loss: 7119.05, base loss: 16011.51
[INFO 2017-06-29 21:11:19,318 main.py:57] epoch 1570, training loss: 6374.24, average training loss: 7118.11, base loss: 16010.23
[INFO 2017-06-29 21:11:22,195 main.py:57] epoch 1571, training loss: 7233.42, average training loss: 7117.58, base loss: 16011.60
[INFO 2017-06-29 21:11:25,043 main.py:57] epoch 1572, training loss: 6846.66, average training loss: 7116.49, base loss: 16011.29
[INFO 2017-06-29 21:11:27,929 main.py:57] epoch 1573, training loss: 7122.40, average training loss: 7115.22, base loss: 16013.48
[INFO 2017-06-29 21:11:30,804 main.py:57] epoch 1574, training loss: 7122.10, average training loss: 7115.75, base loss: 16013.94
[INFO 2017-06-29 21:11:33,669 main.py:57] epoch 1575, training loss: 7331.28, average training loss: 7115.89, base loss: 16015.86
[INFO 2017-06-29 21:11:36,534 main.py:57] epoch 1576, training loss: 6927.12, average training loss: 7114.99, base loss: 16015.66
[INFO 2017-06-29 21:11:39,369 main.py:57] epoch 1577, training loss: 6654.86, average training loss: 7114.37, base loss: 16013.82
[INFO 2017-06-29 21:11:42,257 main.py:57] epoch 1578, training loss: 6686.65, average training loss: 7113.57, base loss: 16013.38
[INFO 2017-06-29 21:11:45,154 main.py:57] epoch 1579, training loss: 6902.14, average training loss: 7113.41, base loss: 16013.99
[INFO 2017-06-29 21:11:48,015 main.py:57] epoch 1580, training loss: 6832.26, average training loss: 7113.06, base loss: 16014.84
[INFO 2017-06-29 21:11:50,960 main.py:57] epoch 1581, training loss: 6480.25, average training loss: 7112.11, base loss: 16015.26
[INFO 2017-06-29 21:11:53,852 main.py:57] epoch 1582, training loss: 6687.55, average training loss: 7111.09, base loss: 16014.95
[INFO 2017-06-29 21:11:56,742 main.py:57] epoch 1583, training loss: 6993.76, average training loss: 7111.01, base loss: 16015.95
[INFO 2017-06-29 21:11:59,591 main.py:57] epoch 1584, training loss: 6347.78, average training loss: 7109.40, base loss: 16015.38
[INFO 2017-06-29 21:12:02,461 main.py:57] epoch 1585, training loss: 6202.99, average training loss: 7108.36, base loss: 16015.00
[INFO 2017-06-29 21:12:05,348 main.py:57] epoch 1586, training loss: 6526.45, average training loss: 7107.74, base loss: 16013.04
[INFO 2017-06-29 21:12:08,171 main.py:57] epoch 1587, training loss: 6717.19, average training loss: 7107.20, base loss: 16011.55
[INFO 2017-06-29 21:12:11,073 main.py:57] epoch 1588, training loss: 6901.54, average training loss: 7107.20, base loss: 16011.02
[INFO 2017-06-29 21:12:14,003 main.py:57] epoch 1589, training loss: 7365.97, average training loss: 7107.02, base loss: 16012.95
[INFO 2017-06-29 21:12:16,889 main.py:57] epoch 1590, training loss: 7015.80, average training loss: 7106.15, base loss: 16013.95
[INFO 2017-06-29 21:12:19,784 main.py:57] epoch 1591, training loss: 6432.87, average training loss: 7104.77, base loss: 16014.20
[INFO 2017-06-29 21:12:22,663 main.py:57] epoch 1592, training loss: 7059.75, average training loss: 7103.87, base loss: 16014.69
[INFO 2017-06-29 21:12:25,521 main.py:57] epoch 1593, training loss: 7244.77, average training loss: 7103.66, base loss: 16017.19
[INFO 2017-06-29 21:12:28,383 main.py:57] epoch 1594, training loss: 7271.87, average training loss: 7102.97, base loss: 16020.05
[INFO 2017-06-29 21:12:31,238 main.py:57] epoch 1595, training loss: 7046.94, average training loss: 7102.07, base loss: 16022.39
[INFO 2017-06-29 21:12:34,107 main.py:57] epoch 1596, training loss: 6472.83, average training loss: 7101.30, base loss: 16020.64
[INFO 2017-06-29 21:12:36,981 main.py:57] epoch 1597, training loss: 6900.92, average training loss: 7100.95, base loss: 16019.48
[INFO 2017-06-29 21:12:39,858 main.py:57] epoch 1598, training loss: 6799.53, average training loss: 7099.99, base loss: 16019.42
[INFO 2017-06-29 21:12:42,685 main.py:57] epoch 1599, training loss: 6762.88, average training loss: 7098.61, base loss: 16018.65
[INFO 2017-06-29 21:12:42,685 main.py:59] epoch 1599, testing
[INFO 2017-06-29 21:12:54,961 main.py:104] average testing loss: 6755.31, base loss: 15730.69
[INFO 2017-06-29 21:12:54,961 main.py:105] improve_loss: 8975.38, improve_percent: 0.57
[INFO 2017-06-29 21:12:54,964 main.py:71] current best improved percent: 0.57
[INFO 2017-06-29 21:12:57,804 main.py:57] epoch 1600, training loss: 7018.10, average training loss: 7097.28, base loss: 16019.16
[INFO 2017-06-29 21:13:00,646 main.py:57] epoch 1601, training loss: 6983.87, average training loss: 7096.84, base loss: 16019.55
[INFO 2017-06-29 21:13:03,508 main.py:57] epoch 1602, training loss: 7554.71, average training loss: 7097.02, base loss: 16021.93
[INFO 2017-06-29 21:13:06,324 main.py:57] epoch 1603, training loss: 6672.51, average training loss: 7096.63, base loss: 16022.03
[INFO 2017-06-29 21:13:09,191 main.py:57] epoch 1604, training loss: 6740.53, average training loss: 7096.55, base loss: 16022.22
[INFO 2017-06-29 21:13:12,058 main.py:57] epoch 1605, training loss: 6634.93, average training loss: 7096.06, base loss: 16021.61
[INFO 2017-06-29 21:13:14,912 main.py:57] epoch 1606, training loss: 6339.29, average training loss: 7095.77, base loss: 16022.43
[INFO 2017-06-29 21:13:17,794 main.py:57] epoch 1607, training loss: 6774.14, average training loss: 7094.55, base loss: 16023.11
[INFO 2017-06-29 21:13:20,713 main.py:57] epoch 1608, training loss: 6411.17, average training loss: 7093.65, base loss: 16022.41
[INFO 2017-06-29 21:13:23,632 main.py:57] epoch 1609, training loss: 7006.52, average training loss: 7093.63, base loss: 16023.43
[INFO 2017-06-29 21:13:26,482 main.py:57] epoch 1610, training loss: 6659.78, average training loss: 7092.37, base loss: 16024.46
[INFO 2017-06-29 21:13:29,340 main.py:57] epoch 1611, training loss: 6260.05, average training loss: 7090.72, base loss: 16024.15
[INFO 2017-06-29 21:13:32,221 main.py:57] epoch 1612, training loss: 6908.64, average training loss: 7089.48, base loss: 16025.04
[INFO 2017-06-29 21:13:35,083 main.py:57] epoch 1613, training loss: 6265.65, average training loss: 7087.72, base loss: 16023.34
[INFO 2017-06-29 21:13:37,964 main.py:57] epoch 1614, training loss: 6832.21, average training loss: 7086.86, base loss: 16022.19
[INFO 2017-06-29 21:13:40,843 main.py:57] epoch 1615, training loss: 6466.19, average training loss: 7086.10, base loss: 16019.88
[INFO 2017-06-29 21:13:43,737 main.py:57] epoch 1616, training loss: 6267.67, average training loss: 7084.98, base loss: 16018.42
[INFO 2017-06-29 21:13:46,643 main.py:57] epoch 1617, training loss: 6759.26, average training loss: 7083.94, base loss: 16017.75
[INFO 2017-06-29 21:13:49,521 main.py:57] epoch 1618, training loss: 6286.50, average training loss: 7082.97, base loss: 16017.21
[INFO 2017-06-29 21:13:52,390 main.py:57] epoch 1619, training loss: 6428.89, average training loss: 7082.10, base loss: 16016.79
[INFO 2017-06-29 21:13:55,318 main.py:57] epoch 1620, training loss: 7227.51, average training loss: 7081.73, base loss: 16017.23
[INFO 2017-06-29 21:13:58,173 main.py:57] epoch 1621, training loss: 6312.76, average training loss: 7080.55, base loss: 16016.89
[INFO 2017-06-29 21:14:01,069 main.py:57] epoch 1622, training loss: 6314.92, average training loss: 7079.17, base loss: 16015.96
[INFO 2017-06-29 21:14:03,907 main.py:57] epoch 1623, training loss: 6596.95, average training loss: 7078.71, base loss: 16014.58
[INFO 2017-06-29 21:14:06,751 main.py:57] epoch 1624, training loss: 6656.21, average training loss: 7077.51, base loss: 16013.11
[INFO 2017-06-29 21:14:09,634 main.py:57] epoch 1625, training loss: 6350.16, average training loss: 7076.18, base loss: 16012.13
[INFO 2017-06-29 21:14:12,489 main.py:57] epoch 1626, training loss: 6702.20, average training loss: 7075.40, base loss: 16012.52
[INFO 2017-06-29 21:14:15,353 main.py:57] epoch 1627, training loss: 6872.73, average training loss: 7074.21, base loss: 16012.96
[INFO 2017-06-29 21:14:18,240 main.py:57] epoch 1628, training loss: 6312.72, average training loss: 7073.46, base loss: 16010.96
[INFO 2017-06-29 21:14:21,095 main.py:57] epoch 1629, training loss: 6118.41, average training loss: 7072.17, base loss: 16009.43
[INFO 2017-06-29 21:14:23,986 main.py:57] epoch 1630, training loss: 6994.98, average training loss: 7072.10, base loss: 16012.16
[INFO 2017-06-29 21:14:26,867 main.py:57] epoch 1631, training loss: 6372.25, average training loss: 7071.58, base loss: 16011.62
[INFO 2017-06-29 21:14:29,750 main.py:57] epoch 1632, training loss: 6592.92, average training loss: 7070.33, base loss: 16011.73
[INFO 2017-06-29 21:14:32,624 main.py:57] epoch 1633, training loss: 6596.58, average training loss: 7069.96, base loss: 16010.98
[INFO 2017-06-29 21:14:35,493 main.py:57] epoch 1634, training loss: 6346.50, average training loss: 7068.20, base loss: 16009.95
[INFO 2017-06-29 21:14:38,372 main.py:57] epoch 1635, training loss: 6550.04, average training loss: 7066.94, base loss: 16009.71
[INFO 2017-06-29 21:14:41,236 main.py:57] epoch 1636, training loss: 6565.33, average training loss: 7064.58, base loss: 16009.45
[INFO 2017-06-29 21:14:44,096 main.py:57] epoch 1637, training loss: 7052.50, average training loss: 7063.92, base loss: 16010.42
[INFO 2017-06-29 21:14:47,004 main.py:57] epoch 1638, training loss: 6537.82, average training loss: 7062.30, base loss: 16009.01
[INFO 2017-06-29 21:14:49,838 main.py:57] epoch 1639, training loss: 6274.23, average training loss: 7061.23, base loss: 16008.46
[INFO 2017-06-29 21:14:52,697 main.py:57] epoch 1640, training loss: 7462.54, average training loss: 7061.77, base loss: 16010.46
[INFO 2017-06-29 21:14:55,554 main.py:57] epoch 1641, training loss: 6843.95, average training loss: 7061.53, base loss: 16010.13
[INFO 2017-06-29 21:14:58,483 main.py:57] epoch 1642, training loss: 6180.32, average training loss: 7059.83, base loss: 16007.97
[INFO 2017-06-29 21:15:01,320 main.py:57] epoch 1643, training loss: 6900.12, average training loss: 7058.89, base loss: 16009.19
[INFO 2017-06-29 21:15:04,209 main.py:57] epoch 1644, training loss: 6197.10, average training loss: 7057.65, base loss: 16007.93
[INFO 2017-06-29 21:15:07,096 main.py:57] epoch 1645, training loss: 6614.38, average training loss: 7056.30, base loss: 16007.98
[INFO 2017-06-29 21:15:10,007 main.py:57] epoch 1646, training loss: 6469.48, average training loss: 7054.72, base loss: 16005.62
[INFO 2017-06-29 21:15:12,884 main.py:57] epoch 1647, training loss: 5962.05, average training loss: 7053.13, base loss: 16003.70
[INFO 2017-06-29 21:15:15,759 main.py:57] epoch 1648, training loss: 6396.76, average training loss: 7052.12, base loss: 16003.14
[INFO 2017-06-29 21:15:18,648 main.py:57] epoch 1649, training loss: 7417.71, average training loss: 7052.33, base loss: 16005.53
[INFO 2017-06-29 21:15:21,506 main.py:57] epoch 1650, training loss: 6654.86, average training loss: 7052.03, base loss: 16005.55
[INFO 2017-06-29 21:15:24,399 main.py:57] epoch 1651, training loss: 7148.83, average training loss: 7051.64, base loss: 16006.02
[INFO 2017-06-29 21:15:27,297 main.py:57] epoch 1652, training loss: 6560.14, average training loss: 7050.44, base loss: 16004.93
[INFO 2017-06-29 21:15:30,142 main.py:57] epoch 1653, training loss: 6867.74, average training loss: 7049.39, base loss: 16005.37
[INFO 2017-06-29 21:15:33,083 main.py:57] epoch 1654, training loss: 6387.58, average training loss: 7048.74, base loss: 16004.93
[INFO 2017-06-29 21:15:35,966 main.py:57] epoch 1655, training loss: 6453.33, average training loss: 7047.58, base loss: 16005.15
[INFO 2017-06-29 21:15:38,880 main.py:57] epoch 1656, training loss: 6698.57, average training loss: 7047.03, base loss: 16006.14
[INFO 2017-06-29 21:15:41,769 main.py:57] epoch 1657, training loss: 6227.17, average training loss: 7045.38, base loss: 16005.75
[INFO 2017-06-29 21:15:44,624 main.py:57] epoch 1658, training loss: 6831.75, average training loss: 7045.02, base loss: 16006.07
[INFO 2017-06-29 21:15:47,465 main.py:57] epoch 1659, training loss: 6695.29, average training loss: 7044.40, base loss: 16006.01
[INFO 2017-06-29 21:15:50,312 main.py:57] epoch 1660, training loss: 6624.55, average training loss: 7043.98, base loss: 16006.06
[INFO 2017-06-29 21:15:53,211 main.py:57] epoch 1661, training loss: 6279.45, average training loss: 7042.77, base loss: 16005.54
[INFO 2017-06-29 21:15:56,060 main.py:57] epoch 1662, training loss: 7459.08, average training loss: 7042.36, base loss: 16007.86
[INFO 2017-06-29 21:15:58,908 main.py:57] epoch 1663, training loss: 6728.76, average training loss: 7042.08, base loss: 16008.06
[INFO 2017-06-29 21:16:01,799 main.py:57] epoch 1664, training loss: 6663.06, average training loss: 7040.87, base loss: 16007.35
[INFO 2017-06-29 21:16:04,705 main.py:57] epoch 1665, training loss: 6550.72, average training loss: 7039.22, base loss: 16006.88
[INFO 2017-06-29 21:16:07,586 main.py:57] epoch 1666, training loss: 6608.83, average training loss: 7038.57, base loss: 16006.84
[INFO 2017-06-29 21:16:10,471 main.py:57] epoch 1667, training loss: 7105.65, average training loss: 7038.66, base loss: 16008.45
[INFO 2017-06-29 21:16:13,328 main.py:57] epoch 1668, training loss: 6883.53, average training loss: 7037.92, base loss: 16008.13
[INFO 2017-06-29 21:16:16,200 main.py:57] epoch 1669, training loss: 7113.87, average training loss: 7037.46, base loss: 16008.70
[INFO 2017-06-29 21:16:19,059 main.py:57] epoch 1670, training loss: 6667.53, average training loss: 7037.35, base loss: 16008.49
[INFO 2017-06-29 21:16:21,937 main.py:57] epoch 1671, training loss: 7254.30, average training loss: 7037.10, base loss: 16009.54
[INFO 2017-06-29 21:16:24,823 main.py:57] epoch 1672, training loss: 6241.14, average training loss: 7036.40, base loss: 16008.15
[INFO 2017-06-29 21:16:27,660 main.py:57] epoch 1673, training loss: 7016.50, average training loss: 7035.70, base loss: 16008.91
[INFO 2017-06-29 21:16:30,499 main.py:57] epoch 1674, training loss: 6564.20, average training loss: 7034.33, base loss: 16009.21
[INFO 2017-06-29 21:16:33,344 main.py:57] epoch 1675, training loss: 5925.84, average training loss: 7033.01, base loss: 16007.54
[INFO 2017-06-29 21:16:36,198 main.py:57] epoch 1676, training loss: 6610.18, average training loss: 7032.15, base loss: 16008.99
[INFO 2017-06-29 21:16:39,101 main.py:57] epoch 1677, training loss: 7067.47, average training loss: 7030.70, base loss: 16010.94
[INFO 2017-06-29 21:16:41,995 main.py:57] epoch 1678, training loss: 6969.86, average training loss: 7030.15, base loss: 16011.67
[INFO 2017-06-29 21:16:44,897 main.py:57] epoch 1679, training loss: 7575.50, average training loss: 7030.72, base loss: 16012.25
[INFO 2017-06-29 21:16:47,803 main.py:57] epoch 1680, training loss: 6447.70, average training loss: 7029.43, base loss: 16011.57
[INFO 2017-06-29 21:16:50,685 main.py:57] epoch 1681, training loss: 6856.09, average training loss: 7029.00, base loss: 16011.42
[INFO 2017-06-29 21:16:53,520 main.py:57] epoch 1682, training loss: 7161.97, average training loss: 7028.99, base loss: 16012.44
[INFO 2017-06-29 21:16:56,385 main.py:57] epoch 1683, training loss: 5961.70, average training loss: 7028.05, base loss: 16011.10
[INFO 2017-06-29 21:16:59,328 main.py:57] epoch 1684, training loss: 6106.11, average training loss: 7025.72, base loss: 16010.05
[INFO 2017-06-29 21:17:02,179 main.py:57] epoch 1685, training loss: 6287.99, average training loss: 7024.78, base loss: 16008.06
[INFO 2017-06-29 21:17:05,048 main.py:57] epoch 1686, training loss: 6761.46, average training loss: 7024.16, base loss: 16007.30
[INFO 2017-06-29 21:17:07,885 main.py:57] epoch 1687, training loss: 6429.43, average training loss: 7022.86, base loss: 16005.39
[INFO 2017-06-29 21:17:10,732 main.py:57] epoch 1688, training loss: 7165.71, average training loss: 7022.00, base loss: 16004.63
[INFO 2017-06-29 21:17:13,606 main.py:57] epoch 1689, training loss: 6446.19, average training loss: 7021.26, base loss: 16004.17
[INFO 2017-06-29 21:17:16,483 main.py:57] epoch 1690, training loss: 7214.23, average training loss: 7021.57, base loss: 16004.28
[INFO 2017-06-29 21:17:19,353 main.py:57] epoch 1691, training loss: 6740.78, average training loss: 7020.68, base loss: 16004.21
[INFO 2017-06-29 21:17:22,258 main.py:57] epoch 1692, training loss: 7094.87, average training loss: 7020.89, base loss: 16006.40
[INFO 2017-06-29 21:17:25,109 main.py:57] epoch 1693, training loss: 6076.23, average training loss: 7017.86, base loss: 16006.19
[INFO 2017-06-29 21:17:27,988 main.py:57] epoch 1694, training loss: 6026.63, average training loss: 7017.33, base loss: 16005.32
[INFO 2017-06-29 21:17:30,836 main.py:57] epoch 1695, training loss: 7000.28, average training loss: 7016.73, base loss: 16006.88
[INFO 2017-06-29 21:17:33,716 main.py:57] epoch 1696, training loss: 7508.45, average training loss: 7016.17, base loss: 16009.38
[INFO 2017-06-29 21:17:36,581 main.py:57] epoch 1697, training loss: 6715.10, average training loss: 7016.05, base loss: 16008.97
[INFO 2017-06-29 21:17:39,435 main.py:57] epoch 1698, training loss: 6923.65, average training loss: 7015.14, base loss: 16009.20
[INFO 2017-06-29 21:17:42,298 main.py:57] epoch 1699, training loss: 7056.85, average training loss: 7015.39, base loss: 16009.80
[INFO 2017-06-29 21:17:42,298 main.py:59] epoch 1699, testing
[INFO 2017-06-29 21:17:54,614 main.py:104] average testing loss: 6655.70, base loss: 15899.71
[INFO 2017-06-29 21:17:54,614 main.py:105] improve_loss: 9244.01, improve_percent: 0.58
[INFO 2017-06-29 21:17:54,616 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 21:17:54,641 main.py:71] current best improved percent: 0.58
[INFO 2017-06-29 21:17:57,504 main.py:57] epoch 1700, training loss: 6999.18, average training loss: 7014.43, base loss: 16009.16
[INFO 2017-06-29 21:18:00,390 main.py:57] epoch 1701, training loss: 6836.48, average training loss: 7013.40, base loss: 16007.38
[INFO 2017-06-29 21:18:03,273 main.py:57] epoch 1702, training loss: 6819.10, average training loss: 7012.72, base loss: 16007.67
[INFO 2017-06-29 21:18:06,112 main.py:57] epoch 1703, training loss: 6070.62, average training loss: 7011.49, base loss: 16006.61
[INFO 2017-06-29 21:18:09,016 main.py:57] epoch 1704, training loss: 7137.46, average training loss: 7011.20, base loss: 16008.05
[INFO 2017-06-29 21:18:11,896 main.py:57] epoch 1705, training loss: 6786.19, average training loss: 7011.21, base loss: 16009.02
[INFO 2017-06-29 21:18:14,767 main.py:57] epoch 1706, training loss: 6562.45, average training loss: 7010.36, base loss: 16009.16
[INFO 2017-06-29 21:18:17,650 main.py:57] epoch 1707, training loss: 6598.46, average training loss: 7009.60, base loss: 16009.01
[INFO 2017-06-29 21:18:20,501 main.py:57] epoch 1708, training loss: 6842.90, average training loss: 7008.59, base loss: 16009.71
[INFO 2017-06-29 21:18:23,374 main.py:57] epoch 1709, training loss: 6151.22, average training loss: 7007.53, base loss: 16009.83
[INFO 2017-06-29 21:18:26,219 main.py:57] epoch 1710, training loss: 6917.90, average training loss: 7007.24, base loss: 16012.01
[INFO 2017-06-29 21:18:29,100 main.py:57] epoch 1711, training loss: 6535.81, average training loss: 7006.13, base loss: 16012.57
[INFO 2017-06-29 21:18:31,960 main.py:57] epoch 1712, training loss: 6571.45, average training loss: 7004.96, base loss: 16012.57
[INFO 2017-06-29 21:18:34,852 main.py:57] epoch 1713, training loss: 6460.57, average training loss: 7003.56, base loss: 16013.63
[INFO 2017-06-29 21:18:37,715 main.py:57] epoch 1714, training loss: 7052.01, average training loss: 7002.92, base loss: 16014.24
[INFO 2017-06-29 21:18:40,620 main.py:57] epoch 1715, training loss: 6226.13, average training loss: 7002.13, base loss: 16012.72
[INFO 2017-06-29 21:18:43,474 main.py:57] epoch 1716, training loss: 6623.58, average training loss: 7000.92, base loss: 16012.05
[INFO 2017-06-29 21:18:46,340 main.py:57] epoch 1717, training loss: 6620.16, average training loss: 7000.67, base loss: 16012.05
[INFO 2017-06-29 21:18:49,213 main.py:57] epoch 1718, training loss: 6431.85, average training loss: 6999.61, base loss: 16012.29
[INFO 2017-06-29 21:18:52,045 main.py:57] epoch 1719, training loss: 6500.60, average training loss: 6998.94, base loss: 16012.59
[INFO 2017-06-29 21:18:54,911 main.py:57] epoch 1720, training loss: 6961.37, average training loss: 6998.37, base loss: 16013.51
[INFO 2017-06-29 21:18:57,769 main.py:57] epoch 1721, training loss: 6918.46, average training loss: 6997.74, base loss: 16014.05
[INFO 2017-06-29 21:19:00,633 main.py:57] epoch 1722, training loss: 6053.31, average training loss: 6995.86, base loss: 16013.66
[INFO 2017-06-29 21:19:03,494 main.py:57] epoch 1723, training loss: 6510.81, average training loss: 6994.38, base loss: 16012.40
[INFO 2017-06-29 21:19:06,340 main.py:57] epoch 1724, training loss: 6357.78, average training loss: 6993.64, base loss: 16011.37
[INFO 2017-06-29 21:19:09,196 main.py:57] epoch 1725, training loss: 6969.30, average training loss: 6993.06, base loss: 16012.52
[INFO 2017-06-29 21:19:12,036 main.py:57] epoch 1726, training loss: 6998.23, average training loss: 6992.28, base loss: 16013.12
[INFO 2017-06-29 21:19:14,882 main.py:57] epoch 1727, training loss: 6233.64, average training loss: 6990.92, base loss: 16010.63
[INFO 2017-06-29 21:19:17,780 main.py:57] epoch 1728, training loss: 6332.54, average training loss: 6989.16, base loss: 16009.99
[INFO 2017-06-29 21:19:20,637 main.py:57] epoch 1729, training loss: 6573.60, average training loss: 6987.42, base loss: 16010.17
[INFO 2017-06-29 21:19:23,521 main.py:57] epoch 1730, training loss: 6544.79, average training loss: 6986.79, base loss: 16009.81
[INFO 2017-06-29 21:19:26,380 main.py:57] epoch 1731, training loss: 6325.02, average training loss: 6985.15, base loss: 16009.70
[INFO 2017-06-29 21:19:29,252 main.py:57] epoch 1732, training loss: 6257.38, average training loss: 6984.86, base loss: 16008.72
[INFO 2017-06-29 21:19:32,113 main.py:57] epoch 1733, training loss: 6383.62, average training loss: 6984.06, base loss: 16008.25
[INFO 2017-06-29 21:19:34,981 main.py:57] epoch 1734, training loss: 6420.77, average training loss: 6983.41, base loss: 16006.57
[INFO 2017-06-29 21:19:37,842 main.py:57] epoch 1735, training loss: 6584.60, average training loss: 6981.67, base loss: 16005.07
[INFO 2017-06-29 21:19:40,741 main.py:57] epoch 1736, training loss: 6482.30, average training loss: 6981.17, base loss: 16005.29
[INFO 2017-06-29 21:19:43,609 main.py:57] epoch 1737, training loss: 6664.07, average training loss: 6981.49, base loss: 16006.14
[INFO 2017-06-29 21:19:46,470 main.py:57] epoch 1738, training loss: 7121.53, average training loss: 6981.44, base loss: 16007.69
[INFO 2017-06-29 21:19:49,356 main.py:57] epoch 1739, training loss: 6669.24, average training loss: 6981.05, base loss: 16007.38
[INFO 2017-06-29 21:19:52,248 main.py:57] epoch 1740, training loss: 6843.18, average training loss: 6980.59, base loss: 16008.03
[INFO 2017-06-29 21:19:55,141 main.py:57] epoch 1741, training loss: 6752.83, average training loss: 6980.02, base loss: 16008.52
[INFO 2017-06-29 21:19:58,053 main.py:57] epoch 1742, training loss: 6116.28, average training loss: 6979.42, base loss: 16007.76
[INFO 2017-06-29 21:20:00,952 main.py:57] epoch 1743, training loss: 7151.74, average training loss: 6979.95, base loss: 16008.97
[INFO 2017-06-29 21:20:03,837 main.py:57] epoch 1744, training loss: 7253.77, average training loss: 6979.51, base loss: 16009.88
[INFO 2017-06-29 21:20:06,745 main.py:57] epoch 1745, training loss: 6754.72, average training loss: 6978.82, base loss: 16010.69
[INFO 2017-06-29 21:20:09,659 main.py:57] epoch 1746, training loss: 6425.25, average training loss: 6978.01, base loss: 16010.82
[INFO 2017-06-29 21:20:12,531 main.py:57] epoch 1747, training loss: 6788.35, average training loss: 6977.92, base loss: 16011.92
[INFO 2017-06-29 21:20:15,402 main.py:57] epoch 1748, training loss: 5957.87, average training loss: 6977.42, base loss: 16009.47
[INFO 2017-06-29 21:20:18,268 main.py:57] epoch 1749, training loss: 6799.66, average training loss: 6977.51, base loss: 16009.90
[INFO 2017-06-29 21:20:21,123 main.py:57] epoch 1750, training loss: 6735.56, average training loss: 6976.19, base loss: 16009.14
[INFO 2017-06-29 21:20:24,030 main.py:57] epoch 1751, training loss: 6482.07, average training loss: 6975.64, base loss: 16009.11
[INFO 2017-06-29 21:20:26,925 main.py:57] epoch 1752, training loss: 6346.09, average training loss: 6974.08, base loss: 16009.25
[INFO 2017-06-29 21:20:29,826 main.py:57] epoch 1753, training loss: 7358.03, average training loss: 6973.91, base loss: 16011.97
[INFO 2017-06-29 21:20:32,744 main.py:57] epoch 1754, training loss: 7192.80, average training loss: 6973.21, base loss: 16012.80
[INFO 2017-06-29 21:20:35,641 main.py:57] epoch 1755, training loss: 6804.63, average training loss: 6972.86, base loss: 16012.62
[INFO 2017-06-29 21:20:38,485 main.py:57] epoch 1756, training loss: 6081.04, average training loss: 6971.03, base loss: 16011.61
[INFO 2017-06-29 21:20:41,332 main.py:57] epoch 1757, training loss: 6343.71, average training loss: 6968.94, base loss: 16011.19
[INFO 2017-06-29 21:20:44,263 main.py:57] epoch 1758, training loss: 6585.31, average training loss: 6967.89, base loss: 16011.83
[INFO 2017-06-29 21:20:47,149 main.py:57] epoch 1759, training loss: 6972.10, average training loss: 6966.70, base loss: 16012.52
[INFO 2017-06-29 21:20:50,022 main.py:57] epoch 1760, training loss: 6506.18, average training loss: 6965.99, base loss: 16012.26
[INFO 2017-06-29 21:20:52,875 main.py:57] epoch 1761, training loss: 6902.88, average training loss: 6965.86, base loss: 16013.59
[INFO 2017-06-29 21:20:55,769 main.py:57] epoch 1762, training loss: 6856.06, average training loss: 6965.38, base loss: 16013.72
[INFO 2017-06-29 21:20:58,735 main.py:57] epoch 1763, training loss: 6069.65, average training loss: 6964.49, base loss: 16013.85
[INFO 2017-06-29 21:21:01,559 main.py:57] epoch 1764, training loss: 6215.73, average training loss: 6962.99, base loss: 16011.81
[INFO 2017-06-29 21:21:04,462 main.py:57] epoch 1765, training loss: 6441.70, average training loss: 6961.85, base loss: 16010.67
[INFO 2017-06-29 21:21:07,363 main.py:57] epoch 1766, training loss: 6586.29, average training loss: 6960.47, base loss: 16010.33
[INFO 2017-06-29 21:21:10,246 main.py:57] epoch 1767, training loss: 6558.20, average training loss: 6959.50, base loss: 16010.48
[INFO 2017-06-29 21:21:13,132 main.py:57] epoch 1768, training loss: 6842.42, average training loss: 6958.77, base loss: 16010.36
[INFO 2017-06-29 21:21:15,997 main.py:57] epoch 1769, training loss: 7175.39, average training loss: 6958.79, base loss: 16010.65
[INFO 2017-06-29 21:21:18,858 main.py:57] epoch 1770, training loss: 6284.14, average training loss: 6957.18, base loss: 16009.01
[INFO 2017-06-29 21:21:21,711 main.py:57] epoch 1771, training loss: 6782.56, average training loss: 6957.08, base loss: 16009.19
[INFO 2017-06-29 21:21:24,546 main.py:57] epoch 1772, training loss: 6416.11, average training loss: 6956.33, base loss: 16007.25
[INFO 2017-06-29 21:21:27,475 main.py:57] epoch 1773, training loss: 7055.75, average training loss: 6955.94, base loss: 16007.03
[INFO 2017-06-29 21:21:30,345 main.py:57] epoch 1774, training loss: 6165.32, average training loss: 6954.60, base loss: 16007.05
[INFO 2017-06-29 21:21:33,229 main.py:57] epoch 1775, training loss: 6721.10, average training loss: 6953.86, base loss: 16007.75
[INFO 2017-06-29 21:21:36,101 main.py:57] epoch 1776, training loss: 6557.06, average training loss: 6953.10, base loss: 16006.40
[INFO 2017-06-29 21:21:39,018 main.py:57] epoch 1777, training loss: 6611.56, average training loss: 6952.23, base loss: 16006.15
[INFO 2017-06-29 21:21:41,918 main.py:57] epoch 1778, training loss: 6841.79, average training loss: 6951.55, base loss: 16007.44
[INFO 2017-06-29 21:21:44,811 main.py:57] epoch 1779, training loss: 6722.00, average training loss: 6950.67, base loss: 16007.63
[INFO 2017-06-29 21:21:47,666 main.py:57] epoch 1780, training loss: 6735.09, average training loss: 6950.26, base loss: 16009.38
[INFO 2017-06-29 21:21:50,556 main.py:57] epoch 1781, training loss: 7188.94, average training loss: 6949.99, base loss: 16012.14
[INFO 2017-06-29 21:21:53,432 main.py:57] epoch 1782, training loss: 6663.29, average training loss: 6949.44, base loss: 16012.28
[INFO 2017-06-29 21:21:56,299 main.py:57] epoch 1783, training loss: 7013.78, average training loss: 6949.14, base loss: 16013.18
[INFO 2017-06-29 21:21:59,172 main.py:57] epoch 1784, training loss: 6543.84, average training loss: 6948.50, base loss: 16012.05
[INFO 2017-06-29 21:22:02,031 main.py:57] epoch 1785, training loss: 6966.82, average training loss: 6947.57, base loss: 16012.51
[INFO 2017-06-29 21:22:04,915 main.py:57] epoch 1786, training loss: 6807.79, average training loss: 6946.57, base loss: 16012.77
[INFO 2017-06-29 21:22:07,777 main.py:57] epoch 1787, training loss: 6183.56, average training loss: 6945.56, base loss: 16011.26
[INFO 2017-06-29 21:22:10,606 main.py:57] epoch 1788, training loss: 6656.01, average training loss: 6944.42, base loss: 16011.85
[INFO 2017-06-29 21:22:13,517 main.py:57] epoch 1789, training loss: 6594.87, average training loss: 6943.86, base loss: 16011.88
[INFO 2017-06-29 21:22:16,386 main.py:57] epoch 1790, training loss: 6491.41, average training loss: 6943.28, base loss: 16010.95
[INFO 2017-06-29 21:22:19,248 main.py:57] epoch 1791, training loss: 7283.66, average training loss: 6943.21, base loss: 16011.27
[INFO 2017-06-29 21:22:22,147 main.py:57] epoch 1792, training loss: 6790.55, average training loss: 6942.66, base loss: 16011.16
[INFO 2017-06-29 21:22:25,028 main.py:57] epoch 1793, training loss: 6072.17, average training loss: 6941.06, base loss: 16008.93
[INFO 2017-06-29 21:22:27,957 main.py:57] epoch 1794, training loss: 6802.27, average training loss: 6940.73, base loss: 16009.45
[INFO 2017-06-29 21:22:30,806 main.py:57] epoch 1795, training loss: 6822.42, average training loss: 6940.18, base loss: 16010.03
[INFO 2017-06-29 21:22:33,702 main.py:57] epoch 1796, training loss: 7683.65, average training loss: 6940.20, base loss: 16012.50
[INFO 2017-06-29 21:22:36,593 main.py:57] epoch 1797, training loss: 7102.61, average training loss: 6940.13, base loss: 16012.94
[INFO 2017-06-29 21:22:39,458 main.py:57] epoch 1798, training loss: 6198.60, average training loss: 6939.85, base loss: 16012.88
[INFO 2017-06-29 21:22:42,326 main.py:57] epoch 1799, training loss: 6704.30, average training loss: 6939.52, base loss: 16013.78
[INFO 2017-06-29 21:22:42,327 main.py:59] epoch 1799, testing
[INFO 2017-06-29 21:22:54,687 main.py:104] average testing loss: 6531.67, base loss: 15785.23
[INFO 2017-06-29 21:22:54,687 main.py:105] improve_loss: 9253.56, improve_percent: 0.59
[INFO 2017-06-29 21:22:54,688 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 21:22:54,714 main.py:71] current best improved percent: 0.59
[INFO 2017-06-29 21:22:57,561 main.py:57] epoch 1800, training loss: 6170.32, average training loss: 6938.98, base loss: 16012.41
[INFO 2017-06-29 21:23:00,407 main.py:57] epoch 1801, training loss: 6966.06, average training loss: 6938.28, base loss: 16013.76
[INFO 2017-06-29 21:23:03,256 main.py:57] epoch 1802, training loss: 6372.95, average training loss: 6936.78, base loss: 16012.35
[INFO 2017-06-29 21:23:06,115 main.py:57] epoch 1803, training loss: 6911.18, average training loss: 6936.13, base loss: 16012.13
[INFO 2017-06-29 21:23:08,970 main.py:57] epoch 1804, training loss: 6417.82, average training loss: 6935.30, base loss: 16011.46
[INFO 2017-06-29 21:23:11,836 main.py:57] epoch 1805, training loss: 6780.07, average training loss: 6934.55, base loss: 16011.52
[INFO 2017-06-29 21:23:14,734 main.py:57] epoch 1806, training loss: 6593.09, average training loss: 6932.90, base loss: 16010.58
[INFO 2017-06-29 21:23:17,574 main.py:57] epoch 1807, training loss: 6812.89, average training loss: 6932.99, base loss: 16010.75
[INFO 2017-06-29 21:23:20,459 main.py:57] epoch 1808, training loss: 6347.66, average training loss: 6932.77, base loss: 16009.54
[INFO 2017-06-29 21:23:23,312 main.py:57] epoch 1809, training loss: 6803.08, average training loss: 6932.48, base loss: 16010.36
[INFO 2017-06-29 21:23:26,203 main.py:57] epoch 1810, training loss: 6617.72, average training loss: 6932.30, base loss: 16010.79
[INFO 2017-06-29 21:23:29,060 main.py:57] epoch 1811, training loss: 6861.55, average training loss: 6931.87, base loss: 16011.65
[INFO 2017-06-29 21:23:31,934 main.py:57] epoch 1812, training loss: 6640.79, average training loss: 6931.68, base loss: 16011.51
[INFO 2017-06-29 21:23:34,810 main.py:57] epoch 1813, training loss: 6480.11, average training loss: 6930.71, base loss: 16011.20
[INFO 2017-06-29 21:23:37,668 main.py:57] epoch 1814, training loss: 6420.35, average training loss: 6930.02, base loss: 16009.80
[INFO 2017-06-29 21:23:40,545 main.py:57] epoch 1815, training loss: 6740.52, average training loss: 6928.83, base loss: 16010.15
[INFO 2017-06-29 21:23:43,438 main.py:57] epoch 1816, training loss: 6590.83, average training loss: 6928.15, base loss: 16009.65
[INFO 2017-06-29 21:23:46,293 main.py:57] epoch 1817, training loss: 6841.47, average training loss: 6927.51, base loss: 16010.68
[INFO 2017-06-29 21:23:49,187 main.py:57] epoch 1818, training loss: 6999.33, average training loss: 6927.63, base loss: 16012.68
[INFO 2017-06-29 21:23:52,036 main.py:57] epoch 1819, training loss: 7013.14, average training loss: 6927.35, base loss: 16015.17
[INFO 2017-06-29 21:23:54,935 main.py:57] epoch 1820, training loss: 6655.68, average training loss: 6926.89, base loss: 16016.45
[INFO 2017-06-29 21:23:57,809 main.py:57] epoch 1821, training loss: 6886.79, average training loss: 6926.65, base loss: 16017.66
[INFO 2017-06-29 21:24:00,698 main.py:57] epoch 1822, training loss: 6217.42, average training loss: 6925.69, base loss: 16016.28
[INFO 2017-06-29 21:24:03,565 main.py:57] epoch 1823, training loss: 6123.90, average training loss: 6923.59, base loss: 16014.92
[INFO 2017-06-29 21:24:06,419 main.py:57] epoch 1824, training loss: 7121.35, average training loss: 6923.51, base loss: 16015.30
[INFO 2017-06-29 21:24:09,336 main.py:57] epoch 1825, training loss: 6314.89, average training loss: 6922.67, base loss: 16014.74
[INFO 2017-06-29 21:24:12,204 main.py:57] epoch 1826, training loss: 7125.30, average training loss: 6922.23, base loss: 16016.46
[INFO 2017-06-29 21:24:15,075 main.py:57] epoch 1827, training loss: 6847.52, average training loss: 6921.74, base loss: 16016.80
[INFO 2017-06-29 21:24:17,912 main.py:57] epoch 1828, training loss: 6974.05, average training loss: 6921.00, base loss: 16017.23
[INFO 2017-06-29 21:24:20,774 main.py:57] epoch 1829, training loss: 6382.87, average training loss: 6919.94, base loss: 16016.56
[INFO 2017-06-29 21:24:23,694 main.py:57] epoch 1830, training loss: 6857.77, average training loss: 6919.72, base loss: 16015.32
[INFO 2017-06-29 21:24:26,579 main.py:57] epoch 1831, training loss: 6586.94, average training loss: 6918.80, base loss: 16014.24
[INFO 2017-06-29 21:24:29,487 main.py:57] epoch 1832, training loss: 5779.61, average training loss: 6916.73, base loss: 16013.17
[INFO 2017-06-29 21:24:32,369 main.py:57] epoch 1833, training loss: 6282.88, average training loss: 6915.91, base loss: 16012.60
[INFO 2017-06-29 21:24:35,254 main.py:57] epoch 1834, training loss: 6451.62, average training loss: 6914.42, base loss: 16011.91
[INFO 2017-06-29 21:24:38,096 main.py:57] epoch 1835, training loss: 6696.26, average training loss: 6914.24, base loss: 16011.74
[INFO 2017-06-29 21:24:41,003 main.py:57] epoch 1836, training loss: 6218.40, average training loss: 6913.77, base loss: 16009.18
[INFO 2017-06-29 21:24:43,855 main.py:57] epoch 1837, training loss: 6945.87, average training loss: 6913.37, base loss: 16008.97
[INFO 2017-06-29 21:24:46,702 main.py:57] epoch 1838, training loss: 6695.44, average training loss: 6912.44, base loss: 16009.38
[INFO 2017-06-29 21:24:49,585 main.py:57] epoch 1839, training loss: 6965.90, average training loss: 6912.74, base loss: 16010.34
[INFO 2017-06-29 21:24:52,452 main.py:57] epoch 1840, training loss: 7334.96, average training loss: 6912.79, base loss: 16013.11
[INFO 2017-06-29 21:24:55,408 main.py:57] epoch 1841, training loss: 6677.75, average training loss: 6912.04, base loss: 16013.44
[INFO 2017-06-29 21:24:58,254 main.py:57] epoch 1842, training loss: 6528.00, average training loss: 6912.21, base loss: 16013.39
[INFO 2017-06-29 21:25:01,130 main.py:57] epoch 1843, training loss: 6358.67, average training loss: 6911.06, base loss: 16012.89
[INFO 2017-06-29 21:25:04,016 main.py:57] epoch 1844, training loss: 6087.98, average training loss: 6909.90, base loss: 16012.67
[INFO 2017-06-29 21:25:06,897 main.py:57] epoch 1845, training loss: 7101.37, average training loss: 6909.58, base loss: 16013.40
[INFO 2017-06-29 21:25:09,789 main.py:57] epoch 1846, training loss: 6595.64, average training loss: 6908.61, base loss: 16012.61
[INFO 2017-06-29 21:25:12,688 main.py:57] epoch 1847, training loss: 6565.75, average training loss: 6907.73, base loss: 16011.75
[INFO 2017-06-29 21:25:15,569 main.py:57] epoch 1848, training loss: 7113.03, average training loss: 6907.48, base loss: 16012.13
[INFO 2017-06-29 21:25:18,433 main.py:57] epoch 1849, training loss: 7568.55, average training loss: 6907.50, base loss: 16013.72
[INFO 2017-06-29 21:25:21,319 main.py:57] epoch 1850, training loss: 6798.88, average training loss: 6906.49, base loss: 16014.54
[INFO 2017-06-29 21:25:24,162 main.py:57] epoch 1851, training loss: 6983.97, average training loss: 6905.45, base loss: 16015.33
[INFO 2017-06-29 21:25:27,036 main.py:57] epoch 1852, training loss: 6380.84, average training loss: 6904.57, base loss: 16014.12
[INFO 2017-06-29 21:25:29,921 main.py:57] epoch 1853, training loss: 7074.23, average training loss: 6904.50, base loss: 16015.42
[INFO 2017-06-29 21:25:32,820 main.py:57] epoch 1854, training loss: 7127.72, average training loss: 6904.56, base loss: 16015.08
[INFO 2017-06-29 21:25:35,716 main.py:57] epoch 1855, training loss: 7150.22, average training loss: 6904.64, base loss: 16014.85
[INFO 2017-06-29 21:25:38,567 main.py:57] epoch 1856, training loss: 6722.86, average training loss: 6904.25, base loss: 16015.16
[INFO 2017-06-29 21:25:41,447 main.py:57] epoch 1857, training loss: 6824.01, average training loss: 6903.50, base loss: 16015.34
[INFO 2017-06-29 21:25:44,386 main.py:57] epoch 1858, training loss: 6468.15, average training loss: 6902.86, base loss: 16016.13
[INFO 2017-06-29 21:25:47,240 main.py:57] epoch 1859, training loss: 6367.49, average training loss: 6902.27, base loss: 16016.55
[INFO 2017-06-29 21:25:50,102 main.py:57] epoch 1860, training loss: 7000.50, average training loss: 6902.07, base loss: 16016.79
[INFO 2017-06-29 21:25:52,929 main.py:57] epoch 1861, training loss: 6444.72, average training loss: 6901.25, base loss: 16015.78
[INFO 2017-06-29 21:25:55,833 main.py:57] epoch 1862, training loss: 6514.31, average training loss: 6900.09, base loss: 16014.59
[INFO 2017-06-29 21:25:58,724 main.py:57] epoch 1863, training loss: 6441.40, average training loss: 6899.63, base loss: 16012.56
[INFO 2017-06-29 21:26:01,579 main.py:57] epoch 1864, training loss: 6882.70, average training loss: 6899.12, base loss: 16013.77
[INFO 2017-06-29 21:26:04,504 main.py:57] epoch 1865, training loss: 6452.53, average training loss: 6898.27, base loss: 16013.81
[INFO 2017-06-29 21:26:07,399 main.py:57] epoch 1866, training loss: 6488.92, average training loss: 6897.84, base loss: 16013.63
[INFO 2017-06-29 21:26:10,259 main.py:57] epoch 1867, training loss: 6395.38, average training loss: 6896.63, base loss: 16014.04
[INFO 2017-06-29 21:26:13,138 main.py:57] epoch 1868, training loss: 6608.15, average training loss: 6896.24, base loss: 16012.94
[INFO 2017-06-29 21:26:16,030 main.py:57] epoch 1869, training loss: 6676.94, average training loss: 6895.94, base loss: 16012.58
[INFO 2017-06-29 21:26:18,938 main.py:57] epoch 1870, training loss: 6297.75, average training loss: 6895.24, base loss: 16012.35
[INFO 2017-06-29 21:26:21,823 main.py:57] epoch 1871, training loss: 6528.10, average training loss: 6894.62, base loss: 16011.67
[INFO 2017-06-29 21:26:24,693 main.py:57] epoch 1872, training loss: 7261.56, average training loss: 6893.94, base loss: 16014.09
[INFO 2017-06-29 21:26:27,560 main.py:57] epoch 1873, training loss: 6377.83, average training loss: 6893.50, base loss: 16014.23
[INFO 2017-06-29 21:26:30,443 main.py:57] epoch 1874, training loss: 6867.43, average training loss: 6892.70, base loss: 16014.62
[INFO 2017-06-29 21:26:33,305 main.py:57] epoch 1875, training loss: 6951.59, average training loss: 6893.01, base loss: 16015.52
[INFO 2017-06-29 21:26:36,181 main.py:57] epoch 1876, training loss: 6615.99, average training loss: 6892.46, base loss: 16014.19
[INFO 2017-06-29 21:26:39,034 main.py:57] epoch 1877, training loss: 6773.03, average training loss: 6892.40, base loss: 16013.87
[INFO 2017-06-29 21:26:41,845 main.py:57] epoch 1878, training loss: 6270.62, average training loss: 6891.52, base loss: 16013.32
[INFO 2017-06-29 21:26:44,754 main.py:57] epoch 1879, training loss: 6217.45, average training loss: 6891.24, base loss: 16012.62
[INFO 2017-06-29 21:26:47,635 main.py:57] epoch 1880, training loss: 6727.98, average training loss: 6890.66, base loss: 16012.51
[INFO 2017-06-29 21:26:50,479 main.py:57] epoch 1881, training loss: 7733.61, average training loss: 6891.26, base loss: 16014.90
[INFO 2017-06-29 21:26:53,384 main.py:57] epoch 1882, training loss: 6500.14, average training loss: 6891.02, base loss: 16014.54
[INFO 2017-06-29 21:26:56,266 main.py:57] epoch 1883, training loss: 6516.12, average training loss: 6889.42, base loss: 16014.51
[INFO 2017-06-29 21:26:59,095 main.py:57] epoch 1884, training loss: 6689.07, average training loss: 6889.48, base loss: 16015.72
[INFO 2017-06-29 21:27:01,967 main.py:57] epoch 1885, training loss: 6557.97, average training loss: 6888.53, base loss: 16016.48
[INFO 2017-06-29 21:27:04,890 main.py:57] epoch 1886, training loss: 7081.89, average training loss: 6887.65, base loss: 16016.03
[INFO 2017-06-29 21:27:07,740 main.py:57] epoch 1887, training loss: 7063.52, average training loss: 6887.65, base loss: 16015.50
[INFO 2017-06-29 21:27:10,605 main.py:57] epoch 1888, training loss: 6100.48, average training loss: 6886.50, base loss: 16015.55
[INFO 2017-06-29 21:27:13,471 main.py:57] epoch 1889, training loss: 7328.67, average training loss: 6886.87, base loss: 16018.34
[INFO 2017-06-29 21:27:16,297 main.py:57] epoch 1890, training loss: 6622.73, average training loss: 6886.47, base loss: 16018.69
[INFO 2017-06-29 21:27:19,174 main.py:57] epoch 1891, training loss: 6386.41, average training loss: 6886.23, base loss: 16018.80
[INFO 2017-06-29 21:27:22,112 main.py:57] epoch 1892, training loss: 6281.95, average training loss: 6885.69, base loss: 16017.65
[INFO 2017-06-29 21:27:24,976 main.py:57] epoch 1893, training loss: 6843.20, average training loss: 6886.15, base loss: 16017.89
[INFO 2017-06-29 21:27:27,855 main.py:57] epoch 1894, training loss: 6857.89, average training loss: 6885.81, base loss: 16018.86
[INFO 2017-06-29 21:27:30,749 main.py:57] epoch 1895, training loss: 6701.60, average training loss: 6885.07, base loss: 16018.69
[INFO 2017-06-29 21:27:33,631 main.py:57] epoch 1896, training loss: 6530.84, average training loss: 6884.30, base loss: 16017.91
[INFO 2017-06-29 21:27:36,524 main.py:57] epoch 1897, training loss: 6902.35, average training loss: 6883.96, base loss: 16017.31
[INFO 2017-06-29 21:27:39,395 main.py:57] epoch 1898, training loss: 6432.97, average training loss: 6883.89, base loss: 16017.95
[INFO 2017-06-29 21:27:42,275 main.py:57] epoch 1899, training loss: 6494.32, average training loss: 6883.49, base loss: 16017.81
[INFO 2017-06-29 21:27:42,275 main.py:59] epoch 1899, testing
[INFO 2017-06-29 21:27:54,637 main.py:104] average testing loss: 6563.08, base loss: 16390.53
[INFO 2017-06-29 21:27:54,637 main.py:105] improve_loss: 9827.45, improve_percent: 0.60
[INFO 2017-06-29 21:27:54,639 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 21:27:54,664 main.py:71] current best improved percent: 0.60
[INFO 2017-06-29 21:27:57,614 main.py:57] epoch 1900, training loss: 6297.01, average training loss: 6882.73, base loss: 16015.51
[INFO 2017-06-29 21:28:00,521 main.py:57] epoch 1901, training loss: 6104.27, average training loss: 6881.91, base loss: 16013.62
[INFO 2017-06-29 21:28:03,362 main.py:57] epoch 1902, training loss: 6524.55, average training loss: 6880.26, base loss: 16013.61
[INFO 2017-06-29 21:28:06,252 main.py:57] epoch 1903, training loss: 6240.23, average training loss: 6879.79, base loss: 16012.85
[INFO 2017-06-29 21:28:09,144 main.py:57] epoch 1904, training loss: 6707.66, average training loss: 6879.66, base loss: 16012.42
[INFO 2017-06-29 21:28:12,028 main.py:57] epoch 1905, training loss: 6386.04, average training loss: 6878.15, base loss: 16011.69
[INFO 2017-06-29 21:28:14,858 main.py:57] epoch 1906, training loss: 6364.64, average training loss: 6877.32, base loss: 16011.72
[INFO 2017-06-29 21:28:17,749 main.py:57] epoch 1907, training loss: 6652.44, average training loss: 6876.78, base loss: 16011.82
[INFO 2017-06-29 21:28:20,566 main.py:57] epoch 1908, training loss: 7091.60, average training loss: 6877.14, base loss: 16012.08
[INFO 2017-06-29 21:28:23,486 main.py:57] epoch 1909, training loss: 6339.00, average training loss: 6876.11, base loss: 16011.02
[INFO 2017-06-29 21:28:26,350 main.py:57] epoch 1910, training loss: 6530.28, average training loss: 6875.07, base loss: 16011.58
[INFO 2017-06-29 21:28:29,219 main.py:57] epoch 1911, training loss: 6805.53, average training loss: 6874.28, base loss: 16013.17
[INFO 2017-06-29 21:28:32,090 main.py:57] epoch 1912, training loss: 6499.11, average training loss: 6873.69, base loss: 16013.53
[INFO 2017-06-29 21:28:34,990 main.py:57] epoch 1913, training loss: 6757.98, average training loss: 6873.42, base loss: 16014.70
[INFO 2017-06-29 21:28:37,919 main.py:57] epoch 1914, training loss: 7054.80, average training loss: 6871.73, base loss: 16016.25
[INFO 2017-06-29 21:28:40,773 main.py:57] epoch 1915, training loss: 6387.94, average training loss: 6871.29, base loss: 16017.24
[INFO 2017-06-29 21:28:43,660 main.py:57] epoch 1916, training loss: 6243.83, average training loss: 6870.50, base loss: 16015.79
[INFO 2017-06-29 21:28:46,549 main.py:57] epoch 1917, training loss: 6304.65, average training loss: 6869.75, base loss: 16014.48
[INFO 2017-06-29 21:28:49,397 main.py:57] epoch 1918, training loss: 7230.65, average training loss: 6869.76, base loss: 16015.56
[INFO 2017-06-29 21:28:52,288 main.py:57] epoch 1919, training loss: 6873.08, average training loss: 6869.93, base loss: 16016.14
[INFO 2017-06-29 21:28:55,160 main.py:57] epoch 1920, training loss: 6364.49, average training loss: 6869.19, base loss: 16015.91
[INFO 2017-06-29 21:28:58,029 main.py:57] epoch 1921, training loss: 6872.31, average training loss: 6869.01, base loss: 16016.79
[INFO 2017-06-29 21:29:00,900 main.py:57] epoch 1922, training loss: 6712.95, average training loss: 6868.61, base loss: 16016.76
[INFO 2017-06-29 21:29:03,781 main.py:57] epoch 1923, training loss: 6412.09, average training loss: 6868.11, base loss: 16015.84
[INFO 2017-06-29 21:29:06,698 main.py:57] epoch 1924, training loss: 6048.26, average training loss: 6866.80, base loss: 16014.69
[INFO 2017-06-29 21:29:09,561 main.py:57] epoch 1925, training loss: 7314.11, average training loss: 6866.87, base loss: 16015.41
[INFO 2017-06-29 21:29:12,400 main.py:57] epoch 1926, training loss: 6939.20, average training loss: 6866.60, base loss: 16014.49
[INFO 2017-06-29 21:29:15,281 main.py:57] epoch 1927, training loss: 6743.36, average training loss: 6866.11, base loss: 16013.22
[INFO 2017-06-29 21:29:18,178 main.py:57] epoch 1928, training loss: 6243.76, average training loss: 6865.57, base loss: 16011.45
[INFO 2017-06-29 21:29:21,052 main.py:57] epoch 1929, training loss: 6610.75, average training loss: 6865.04, base loss: 16011.74
[INFO 2017-06-29 21:29:23,905 main.py:57] epoch 1930, training loss: 6693.96, average training loss: 6864.66, base loss: 16012.36
[INFO 2017-06-29 21:29:26,797 main.py:57] epoch 1931, training loss: 7011.31, average training loss: 6864.12, base loss: 16013.59
[INFO 2017-06-29 21:29:29,653 main.py:57] epoch 1932, training loss: 6304.39, average training loss: 6862.91, base loss: 16012.37
[INFO 2017-06-29 21:29:32,560 main.py:57] epoch 1933, training loss: 6198.91, average training loss: 6862.08, base loss: 16011.28
[INFO 2017-06-29 21:29:35,410 main.py:57] epoch 1934, training loss: 6747.95, average training loss: 6861.72, base loss: 16011.71
[INFO 2017-06-29 21:29:38,260 main.py:57] epoch 1935, training loss: 6617.56, average training loss: 6861.34, base loss: 16011.92
[INFO 2017-06-29 21:29:41,125 main.py:57] epoch 1936, training loss: 6654.62, average training loss: 6860.74, base loss: 16011.60
[INFO 2017-06-29 21:29:43,993 main.py:57] epoch 1937, training loss: 6690.25, average training loss: 6860.01, base loss: 16010.87
[INFO 2017-06-29 21:29:46,839 main.py:57] epoch 1938, training loss: 6176.21, average training loss: 6858.77, base loss: 16009.84
[INFO 2017-06-29 21:29:49,762 main.py:57] epoch 1939, training loss: 6446.57, average training loss: 6857.84, base loss: 16010.81
[INFO 2017-06-29 21:29:52,670 main.py:57] epoch 1940, training loss: 6757.08, average training loss: 6856.90, base loss: 16010.75
[INFO 2017-06-29 21:29:55,543 main.py:57] epoch 1941, training loss: 6883.82, average training loss: 6856.74, base loss: 16010.56
[INFO 2017-06-29 21:29:58,413 main.py:57] epoch 1942, training loss: 7117.45, average training loss: 6857.14, base loss: 16011.05
[INFO 2017-06-29 21:30:01,331 main.py:57] epoch 1943, training loss: 7005.62, average training loss: 6856.72, base loss: 16012.51
[INFO 2017-06-29 21:30:04,192 main.py:57] epoch 1944, training loss: 6317.80, average training loss: 6856.04, base loss: 16010.91
[INFO 2017-06-29 21:30:07,028 main.py:57] epoch 1945, training loss: 6726.72, average training loss: 6855.82, base loss: 16011.11
[INFO 2017-06-29 21:30:09,931 main.py:57] epoch 1946, training loss: 6453.02, average training loss: 6855.43, base loss: 16011.21
[INFO 2017-06-29 21:30:12,781 main.py:57] epoch 1947, training loss: 7031.29, average training loss: 6855.08, base loss: 16012.42
[INFO 2017-06-29 21:30:15,676 main.py:57] epoch 1948, training loss: 6331.57, average training loss: 6853.86, base loss: 16012.75
[INFO 2017-06-29 21:30:18,569 main.py:57] epoch 1949, training loss: 7029.70, average training loss: 6854.22, base loss: 16014.96
[INFO 2017-06-29 21:30:21,433 main.py:57] epoch 1950, training loss: 6498.96, average training loss: 6853.25, base loss: 16014.74
[INFO 2017-06-29 21:30:24,293 main.py:57] epoch 1951, training loss: 6551.07, average training loss: 6853.32, base loss: 16014.32
[INFO 2017-06-29 21:30:27,247 main.py:57] epoch 1952, training loss: 7141.36, average training loss: 6853.56, base loss: 16015.70
[INFO 2017-06-29 21:30:30,121 main.py:57] epoch 1953, training loss: 6540.84, average training loss: 6852.94, base loss: 16014.80
[INFO 2017-06-29 21:30:32,987 main.py:57] epoch 1954, training loss: 6182.76, average training loss: 6851.42, base loss: 16014.13
[INFO 2017-06-29 21:30:35,834 main.py:57] epoch 1955, training loss: 7215.05, average training loss: 6851.08, base loss: 16015.66
[INFO 2017-06-29 21:30:38,681 main.py:57] epoch 1956, training loss: 6714.38, average training loss: 6850.40, base loss: 16014.28
[INFO 2017-06-29 21:30:41,555 main.py:57] epoch 1957, training loss: 7544.66, average training loss: 6850.05, base loss: 16016.08
[INFO 2017-06-29 21:30:44,488 main.py:57] epoch 1958, training loss: 6629.91, average training loss: 6849.69, base loss: 16015.17
[INFO 2017-06-29 21:30:47,345 main.py:57] epoch 1959, training loss: 5987.47, average training loss: 6848.37, base loss: 16012.60
[INFO 2017-06-29 21:30:50,205 main.py:57] epoch 1960, training loss: 5908.07, average training loss: 6846.60, base loss: 16011.22
[INFO 2017-06-29 21:30:53,090 main.py:57] epoch 1961, training loss: 7052.03, average training loss: 6846.63, base loss: 16011.34
[INFO 2017-06-29 21:30:55,950 main.py:57] epoch 1962, training loss: 6686.19, average training loss: 6846.26, base loss: 16010.83
[INFO 2017-06-29 21:30:58,843 main.py:57] epoch 1963, training loss: 6692.16, average training loss: 6845.53, base loss: 16011.41
[INFO 2017-06-29 21:31:01,688 main.py:57] epoch 1964, training loss: 6546.46, average training loss: 6844.81, base loss: 16011.00
[INFO 2017-06-29 21:31:04,588 main.py:57] epoch 1965, training loss: 6885.45, average training loss: 6844.54, base loss: 16010.74
[INFO 2017-06-29 21:31:07,505 main.py:57] epoch 1966, training loss: 6497.07, average training loss: 6843.95, base loss: 16011.12
[INFO 2017-06-29 21:31:10,391 main.py:57] epoch 1967, training loss: 7356.36, average training loss: 6844.62, base loss: 16013.09
[INFO 2017-06-29 21:31:13,233 main.py:57] epoch 1968, training loss: 6190.78, average training loss: 6844.31, base loss: 16011.55
[INFO 2017-06-29 21:31:16,094 main.py:57] epoch 1969, training loss: 6171.63, average training loss: 6843.54, base loss: 16011.39
[INFO 2017-06-29 21:31:18,968 main.py:57] epoch 1970, training loss: 6286.16, average training loss: 6842.88, base loss: 16009.84
[INFO 2017-06-29 21:31:21,805 main.py:57] epoch 1971, training loss: 6608.19, average training loss: 6841.92, base loss: 16008.46
[INFO 2017-06-29 21:31:24,686 main.py:57] epoch 1972, training loss: 7429.91, average training loss: 6842.40, base loss: 16010.35
[INFO 2017-06-29 21:31:27,609 main.py:57] epoch 1973, training loss: 6595.22, average training loss: 6841.78, base loss: 16010.27
[INFO 2017-06-29 21:31:30,514 main.py:57] epoch 1974, training loss: 6683.72, average training loss: 6841.16, base loss: 16011.15
[INFO 2017-06-29 21:31:33,390 main.py:57] epoch 1975, training loss: 6608.72, average training loss: 6841.09, base loss: 16012.51
[INFO 2017-06-29 21:31:36,241 main.py:57] epoch 1976, training loss: 6674.23, average training loss: 6841.15, base loss: 16012.92
[INFO 2017-06-29 21:31:39,177 main.py:57] epoch 1977, training loss: 6719.11, average training loss: 6840.74, base loss: 16013.44
[INFO 2017-06-29 21:31:42,070 main.py:57] epoch 1978, training loss: 6844.46, average training loss: 6840.89, base loss: 16014.81
[INFO 2017-06-29 21:31:44,958 main.py:57] epoch 1979, training loss: 6691.15, average training loss: 6840.54, base loss: 16015.27
[INFO 2017-06-29 21:31:47,869 main.py:57] epoch 1980, training loss: 6128.87, average training loss: 6839.80, base loss: 16014.51
[INFO 2017-06-29 21:31:50,763 main.py:57] epoch 1981, training loss: 6120.56, average training loss: 6838.72, base loss: 16012.72
[INFO 2017-06-29 21:31:53,615 main.py:57] epoch 1982, training loss: 6370.99, average training loss: 6838.25, base loss: 16011.96
[INFO 2017-06-29 21:31:56,470 main.py:57] epoch 1983, training loss: 6321.09, average training loss: 6837.21, base loss: 16011.85
[INFO 2017-06-29 21:31:59,319 main.py:57] epoch 1984, training loss: 6517.55, average training loss: 6836.69, base loss: 16012.63
[INFO 2017-06-29 21:32:02,191 main.py:57] epoch 1985, training loss: 6181.32, average training loss: 6835.86, base loss: 16012.50
[INFO 2017-06-29 21:32:05,095 main.py:57] epoch 1986, training loss: 7074.11, average training loss: 6836.20, base loss: 16014.45
[INFO 2017-06-29 21:32:07,972 main.py:57] epoch 1987, training loss: 6751.67, average training loss: 6835.75, base loss: 16015.03
[INFO 2017-06-29 21:32:10,842 main.py:57] epoch 1988, training loss: 6632.91, average training loss: 6834.56, base loss: 16014.72
[INFO 2017-06-29 21:32:13,732 main.py:57] epoch 1989, training loss: 6390.45, average training loss: 6833.82, base loss: 16013.47
[INFO 2017-06-29 21:32:16,611 main.py:57] epoch 1990, training loss: 6166.59, average training loss: 6832.83, base loss: 16012.72
[INFO 2017-06-29 21:32:19,471 main.py:57] epoch 1991, training loss: 6242.54, average training loss: 6832.05, base loss: 16011.73
[INFO 2017-06-29 21:32:22,361 main.py:57] epoch 1992, training loss: 7558.04, average training loss: 6832.85, base loss: 16014.72
[INFO 2017-06-29 21:32:25,254 main.py:57] epoch 1993, training loss: 6482.62, average training loss: 6832.68, base loss: 16015.23
[INFO 2017-06-29 21:32:28,147 main.py:57] epoch 1994, training loss: 6972.88, average training loss: 6832.25, base loss: 16016.96
[INFO 2017-06-29 21:32:31,027 main.py:57] epoch 1995, training loss: 6354.00, average training loss: 6831.33, base loss: 16016.23
[INFO 2017-06-29 21:32:33,923 main.py:57] epoch 1996, training loss: 6840.12, average training loss: 6831.28, base loss: 16016.99
[INFO 2017-06-29 21:32:36,775 main.py:57] epoch 1997, training loss: 6735.84, average training loss: 6831.02, base loss: 16018.74
[INFO 2017-06-29 21:32:39,686 main.py:57] epoch 1998, training loss: 6184.20, average training loss: 6829.95, base loss: 16018.34
[INFO 2017-06-29 21:32:42,509 main.py:57] epoch 1999, training loss: 6825.00, average training loss: 6829.11, base loss: 16018.73
[INFO 2017-06-29 21:32:42,510 main.py:59] epoch 1999, testing
[INFO 2017-06-29 21:32:54,881 main.py:104] average testing loss: 6581.19, base loss: 15976.80
[INFO 2017-06-29 21:32:54,881 main.py:105] improve_loss: 9395.62, improve_percent: 0.59
[INFO 2017-06-29 21:32:54,883 main.py:71] current best improved percent: 0.60
[INFO 2017-06-29 21:32:57,743 main.py:57] epoch 2000, training loss: 6693.86, average training loss: 6828.69, base loss: 16018.67
[INFO 2017-06-29 21:33:00,583 main.py:57] epoch 2001, training loss: 6601.68, average training loss: 6828.50, base loss: 16017.86
[INFO 2017-06-29 21:33:03,451 main.py:57] epoch 2002, training loss: 6895.46, average training loss: 6828.65, base loss: 16018.74
[INFO 2017-06-29 21:33:06,312 main.py:57] epoch 2003, training loss: 6127.87, average training loss: 6827.87, base loss: 16018.36
[INFO 2017-06-29 21:33:09,133 main.py:57] epoch 2004, training loss: 7232.17, average training loss: 6827.91, base loss: 16018.83
[INFO 2017-06-29 21:33:12,024 main.py:57] epoch 2005, training loss: 7202.06, average training loss: 6828.07, base loss: 16019.17
[INFO 2017-06-29 21:33:14,923 main.py:57] epoch 2006, training loss: 7295.70, average training loss: 6828.29, base loss: 16021.01
[INFO 2017-06-29 21:33:17,807 main.py:57] epoch 2007, training loss: 6496.11, average training loss: 6827.82, base loss: 16020.86
[INFO 2017-06-29 21:33:20,649 main.py:57] epoch 2008, training loss: 6700.50, average training loss: 6827.20, base loss: 16021.43
[INFO 2017-06-29 21:33:23,536 main.py:57] epoch 2009, training loss: 6303.43, average training loss: 6826.16, base loss: 16020.16
[INFO 2017-06-29 21:33:26,442 main.py:57] epoch 2010, training loss: 6480.09, average training loss: 6825.87, base loss: 16020.06
[INFO 2017-06-29 21:33:29,312 main.py:57] epoch 2011, training loss: 6342.93, average training loss: 6825.20, base loss: 16020.35
[INFO 2017-06-29 21:33:32,177 main.py:57] epoch 2012, training loss: 7335.59, average training loss: 6825.16, base loss: 16021.59
[INFO 2017-06-29 21:33:35,082 main.py:57] epoch 2013, training loss: 7112.97, average training loss: 6825.12, base loss: 16022.24
[INFO 2017-06-29 21:33:37,953 main.py:57] epoch 2014, training loss: 6627.07, average training loss: 6824.89, base loss: 16021.33
[INFO 2017-06-29 21:33:40,805 main.py:57] epoch 2015, training loss: 6702.37, average training loss: 6825.01, base loss: 16020.08
[INFO 2017-06-29 21:33:43,685 main.py:57] epoch 2016, training loss: 6557.87, average training loss: 6824.52, base loss: 16019.77
[INFO 2017-06-29 21:33:46,547 main.py:57] epoch 2017, training loss: 6639.58, average training loss: 6823.76, base loss: 16019.36
[INFO 2017-06-29 21:33:49,428 main.py:57] epoch 2018, training loss: 6721.04, average training loss: 6823.46, base loss: 16019.91
[INFO 2017-06-29 21:33:52,345 main.py:57] epoch 2019, training loss: 6721.84, average training loss: 6823.70, base loss: 16019.16
[INFO 2017-06-29 21:33:55,225 main.py:57] epoch 2020, training loss: 6465.67, average training loss: 6823.22, base loss: 16019.32
[INFO 2017-06-29 21:33:58,096 main.py:57] epoch 2021, training loss: 6517.65, average training loss: 6822.53, base loss: 16018.84
[INFO 2017-06-29 21:34:00,998 main.py:57] epoch 2022, training loss: 6700.90, average training loss: 6822.53, base loss: 16019.35
[INFO 2017-06-29 21:34:03,862 main.py:57] epoch 2023, training loss: 6255.82, average training loss: 6821.70, base loss: 16018.38
[INFO 2017-06-29 21:34:06,725 main.py:57] epoch 2024, training loss: 6491.35, average training loss: 6821.74, base loss: 16018.12
[INFO 2017-06-29 21:34:09,586 main.py:57] epoch 2025, training loss: 6756.53, average training loss: 6822.01, base loss: 16018.49
[INFO 2017-06-29 21:34:12,443 main.py:57] epoch 2026, training loss: 6270.43, average training loss: 6821.58, base loss: 16018.85
[INFO 2017-06-29 21:34:15,311 main.py:57] epoch 2027, training loss: 6136.93, average training loss: 6820.73, base loss: 16017.18
[INFO 2017-06-29 21:34:18,198 main.py:57] epoch 2028, training loss: 6755.92, average training loss: 6820.52, base loss: 16017.12
[INFO 2017-06-29 21:34:21,054 main.py:57] epoch 2029, training loss: 6713.83, average training loss: 6819.91, base loss: 16018.09
[INFO 2017-06-29 21:34:23,930 main.py:57] epoch 2030, training loss: 6328.07, average training loss: 6819.08, base loss: 16017.58
[INFO 2017-06-29 21:34:26,813 main.py:57] epoch 2031, training loss: 6085.73, average training loss: 6818.01, base loss: 16016.69
[INFO 2017-06-29 21:34:29,674 main.py:57] epoch 2032, training loss: 6141.51, average training loss: 6817.08, base loss: 16014.72
[INFO 2017-06-29 21:34:32,540 main.py:57] epoch 2033, training loss: 7444.49, average training loss: 6818.35, base loss: 16016.07
[INFO 2017-06-29 21:34:35,342 main.py:57] epoch 2034, training loss: 6689.60, average training loss: 6817.03, base loss: 16015.91
[INFO 2017-06-29 21:34:38,225 main.py:57] epoch 2035, training loss: 6873.71, average training loss: 6816.79, base loss: 16016.44
[INFO 2017-06-29 21:34:41,072 main.py:57] epoch 2036, training loss: 6270.91, average training loss: 6816.03, base loss: 16015.46
[INFO 2017-06-29 21:34:43,994 main.py:57] epoch 2037, training loss: 6378.00, average training loss: 6814.94, base loss: 16014.82
[INFO 2017-06-29 21:34:46,874 main.py:57] epoch 2038, training loss: 6505.87, average training loss: 6813.92, base loss: 16015.49
[INFO 2017-06-29 21:34:49,794 main.py:57] epoch 2039, training loss: 6659.18, average training loss: 6812.97, base loss: 16016.03
[INFO 2017-06-29 21:34:52,654 main.py:57] epoch 2040, training loss: 6834.02, average training loss: 6812.68, base loss: 16015.36
[INFO 2017-06-29 21:34:55,565 main.py:57] epoch 2041, training loss: 6478.41, average training loss: 6812.48, base loss: 16014.85
[INFO 2017-06-29 21:34:58,410 main.py:57] epoch 2042, training loss: 6427.92, average training loss: 6811.26, base loss: 16015.19
[INFO 2017-06-29 21:35:01,272 main.py:57] epoch 2043, training loss: 6388.94, average training loss: 6810.58, base loss: 16016.01
[INFO 2017-06-29 21:35:04,178 main.py:57] epoch 2044, training loss: 6630.62, average training loss: 6809.94, base loss: 16016.71
[INFO 2017-06-29 21:35:07,109 main.py:57] epoch 2045, training loss: 6050.29, average training loss: 6808.80, base loss: 16015.68
[INFO 2017-06-29 21:35:09,963 main.py:57] epoch 2046, training loss: 7007.85, average training loss: 6809.40, base loss: 16016.00
[INFO 2017-06-29 21:35:12,813 main.py:57] epoch 2047, training loss: 6164.97, average training loss: 6809.00, base loss: 16014.82
[INFO 2017-06-29 21:35:15,631 main.py:57] epoch 2048, training loss: 6837.03, average training loss: 6808.51, base loss: 16014.91
[INFO 2017-06-29 21:35:18,535 main.py:57] epoch 2049, training loss: 6389.49, average training loss: 6807.98, base loss: 16014.24
[INFO 2017-06-29 21:35:21,419 main.py:57] epoch 2050, training loss: 6455.90, average training loss: 6807.13, base loss: 16014.19
[INFO 2017-06-29 21:35:24,291 main.py:57] epoch 2051, training loss: 6804.95, average training loss: 6806.81, base loss: 16014.37
[INFO 2017-06-29 21:35:27,135 main.py:57] epoch 2052, training loss: 6620.63, average training loss: 6806.50, base loss: 16014.67
[INFO 2017-06-29 21:35:30,020 main.py:57] epoch 2053, training loss: 7350.36, average training loss: 6806.03, base loss: 16016.50
[INFO 2017-06-29 21:35:32,939 main.py:57] epoch 2054, training loss: 6698.29, average training loss: 6805.28, base loss: 16016.44
[INFO 2017-06-29 21:35:35,828 main.py:57] epoch 2055, training loss: 6451.43, average training loss: 6804.46, base loss: 16016.10
[INFO 2017-06-29 21:35:38,701 main.py:57] epoch 2056, training loss: 6398.88, average training loss: 6803.37, base loss: 16014.70
[INFO 2017-06-29 21:35:41,556 main.py:57] epoch 2057, training loss: 6417.34, average training loss: 6802.35, base loss: 16014.72
[INFO 2017-06-29 21:35:44,439 main.py:57] epoch 2058, training loss: 7233.36, average training loss: 6801.97, base loss: 16015.32
[INFO 2017-06-29 21:35:47,289 main.py:57] epoch 2059, training loss: 6364.74, average training loss: 6801.33, base loss: 16014.51
[INFO 2017-06-29 21:35:50,146 main.py:57] epoch 2060, training loss: 6994.59, average training loss: 6801.41, base loss: 16015.20
[INFO 2017-06-29 21:35:53,039 main.py:57] epoch 2061, training loss: 7197.18, average training loss: 6802.00, base loss: 16016.38
[INFO 2017-06-29 21:35:55,909 main.py:57] epoch 2062, training loss: 6410.22, average training loss: 6801.71, base loss: 16016.09
[INFO 2017-06-29 21:35:58,806 main.py:57] epoch 2063, training loss: 6418.85, average training loss: 6801.21, base loss: 16016.58
[INFO 2017-06-29 21:36:01,715 main.py:57] epoch 2064, training loss: 6665.55, average training loss: 6801.29, base loss: 16017.07
[INFO 2017-06-29 21:36:04,544 main.py:57] epoch 2065, training loss: 6012.47, average training loss: 6800.49, base loss: 16015.64
[INFO 2017-06-29 21:36:07,432 main.py:57] epoch 2066, training loss: 6576.08, average training loss: 6799.77, base loss: 16015.66
[INFO 2017-06-29 21:36:10,268 main.py:57] epoch 2067, training loss: 7222.88, average training loss: 6799.44, base loss: 16017.45
[INFO 2017-06-29 21:36:13,098 main.py:57] epoch 2068, training loss: 6881.52, average training loss: 6799.75, base loss: 16018.29
[INFO 2017-06-29 21:36:15,955 main.py:57] epoch 2069, training loss: 6596.20, average training loss: 6799.46, base loss: 16018.73
[INFO 2017-06-29 21:36:18,826 main.py:57] epoch 2070, training loss: 6245.17, average training loss: 6799.07, base loss: 16018.32
[INFO 2017-06-29 21:36:21,717 main.py:57] epoch 2071, training loss: 7470.81, average training loss: 6798.82, base loss: 16019.94
[INFO 2017-06-29 21:36:24,582 main.py:57] epoch 2072, training loss: 6729.17, average training loss: 6797.91, base loss: 16020.39
[INFO 2017-06-29 21:36:27,436 main.py:57] epoch 2073, training loss: 6533.75, average training loss: 6797.04, base loss: 16019.70
[INFO 2017-06-29 21:36:30,305 main.py:57] epoch 2074, training loss: 6584.46, average training loss: 6796.68, base loss: 16019.37
[INFO 2017-06-29 21:36:33,178 main.py:57] epoch 2075, training loss: 6450.85, average training loss: 6795.57, base loss: 16019.81
[INFO 2017-06-29 21:36:36,049 main.py:57] epoch 2076, training loss: 6168.71, average training loss: 6794.64, base loss: 16017.79
[INFO 2017-06-29 21:36:38,882 main.py:57] epoch 2077, training loss: 6659.05, average training loss: 6794.02, base loss: 16017.05
[INFO 2017-06-29 21:36:41,731 main.py:57] epoch 2078, training loss: 7109.56, average training loss: 6794.06, base loss: 16018.30
[INFO 2017-06-29 21:36:44,624 main.py:57] epoch 2079, training loss: 7411.26, average training loss: 6793.79, base loss: 16020.91
[INFO 2017-06-29 21:36:47,562 main.py:57] epoch 2080, training loss: 6167.48, average training loss: 6793.11, base loss: 16020.24
[INFO 2017-06-29 21:36:50,497 main.py:57] epoch 2081, training loss: 6751.65, average training loss: 6792.60, base loss: 16021.80
[INFO 2017-06-29 21:36:53,359 main.py:57] epoch 2082, training loss: 6554.26, average training loss: 6791.26, base loss: 16021.27
[INFO 2017-06-29 21:36:56,237 main.py:57] epoch 2083, training loss: 7017.13, average training loss: 6790.93, base loss: 16021.39
[INFO 2017-06-29 21:36:59,103 main.py:57] epoch 2084, training loss: 6501.30, average training loss: 6790.48, base loss: 16021.35
[INFO 2017-06-29 21:37:01,998 main.py:57] epoch 2085, training loss: 6218.03, average training loss: 6788.98, base loss: 16021.04
[INFO 2017-06-29 21:37:04,846 main.py:57] epoch 2086, training loss: 6366.09, average training loss: 6788.96, base loss: 16020.70
[INFO 2017-06-29 21:37:07,734 main.py:57] epoch 2087, training loss: 6808.28, average training loss: 6788.63, base loss: 16021.09
[INFO 2017-06-29 21:37:10,609 main.py:57] epoch 2088, training loss: 6185.71, average training loss: 6788.06, base loss: 16021.29
[INFO 2017-06-29 21:37:13,508 main.py:57] epoch 2089, training loss: 6198.10, average training loss: 6787.04, base loss: 16021.04
[INFO 2017-06-29 21:37:16,357 main.py:57] epoch 2090, training loss: 6811.82, average training loss: 6786.53, base loss: 16021.42
[INFO 2017-06-29 21:37:19,180 main.py:57] epoch 2091, training loss: 6932.82, average training loss: 6785.72, base loss: 16022.39
[INFO 2017-06-29 21:37:22,035 main.py:57] epoch 2092, training loss: 6465.68, average training loss: 6785.08, base loss: 16021.68
[INFO 2017-06-29 21:37:24,907 main.py:57] epoch 2093, training loss: 6841.06, average training loss: 6784.70, base loss: 16022.19
[INFO 2017-06-29 21:37:27,795 main.py:57] epoch 2094, training loss: 6470.90, average training loss: 6784.37, base loss: 16022.14
[INFO 2017-06-29 21:37:30,651 main.py:57] epoch 2095, training loss: 6698.05, average training loss: 6784.51, base loss: 16022.19
[INFO 2017-06-29 21:37:33,496 main.py:57] epoch 2096, training loss: 5920.43, average training loss: 6783.49, base loss: 16021.52
[INFO 2017-06-29 21:37:36,395 main.py:57] epoch 2097, training loss: 6976.94, average training loss: 6783.03, base loss: 16023.07
[INFO 2017-06-29 21:37:39,253 main.py:57] epoch 2098, training loss: 6772.29, average training loss: 6783.07, base loss: 16023.19
[INFO 2017-06-29 21:37:42,081 main.py:57] epoch 2099, training loss: 6489.62, average training loss: 6782.40, base loss: 16022.54
[INFO 2017-06-29 21:37:42,082 main.py:59] epoch 2099, testing
[INFO 2017-06-29 21:37:54,417 main.py:104] average testing loss: 6496.20, base loss: 15436.12
[INFO 2017-06-29 21:37:54,418 main.py:105] improve_loss: 8939.92, improve_percent: 0.58
[INFO 2017-06-29 21:37:54,419 main.py:71] current best improved percent: 0.60
[INFO 2017-06-29 21:37:57,280 main.py:57] epoch 2100, training loss: 6411.67, average training loss: 6781.82, base loss: 16022.99
[INFO 2017-06-29 21:38:00,152 main.py:57] epoch 2101, training loss: 6288.76, average training loss: 6781.70, base loss: 16023.40
[INFO 2017-06-29 21:38:03,025 main.py:57] epoch 2102, training loss: 6856.49, average training loss: 6781.51, base loss: 16023.76
[INFO 2017-06-29 21:38:05,929 main.py:57] epoch 2103, training loss: 6598.15, average training loss: 6781.06, base loss: 16023.74
[INFO 2017-06-29 21:38:08,775 main.py:57] epoch 2104, training loss: 6822.70, average training loss: 6780.96, base loss: 16023.64
[INFO 2017-06-29 21:38:11,648 main.py:57] epoch 2105, training loss: 5833.85, average training loss: 6779.19, base loss: 16022.47
[INFO 2017-06-29 21:38:14,522 main.py:57] epoch 2106, training loss: 6855.37, average training loss: 6779.81, base loss: 16023.35
[INFO 2017-06-29 21:38:17,409 main.py:57] epoch 2107, training loss: 6752.38, average training loss: 6778.64, base loss: 16023.89
[INFO 2017-06-29 21:38:20,292 main.py:57] epoch 2108, training loss: 6422.30, average training loss: 6778.33, base loss: 16024.46
[INFO 2017-06-29 21:38:23,157 main.py:57] epoch 2109, training loss: 6416.47, average training loss: 6777.62, base loss: 16024.40
[INFO 2017-06-29 21:38:26,016 main.py:57] epoch 2110, training loss: 6736.32, average training loss: 6777.18, base loss: 16024.32
[INFO 2017-06-29 21:38:28,863 main.py:57] epoch 2111, training loss: 6440.67, average training loss: 6776.01, base loss: 16023.45
[INFO 2017-06-29 21:38:31,710 main.py:57] epoch 2112, training loss: 6623.25, average training loss: 6774.95, base loss: 16023.27
[INFO 2017-06-29 21:38:34,592 main.py:57] epoch 2113, training loss: 6677.70, average training loss: 6774.53, base loss: 16022.28
[INFO 2017-06-29 21:38:37,441 main.py:57] epoch 2114, training loss: 5952.15, average training loss: 6773.95, base loss: 16021.88
[INFO 2017-06-29 21:38:40,277 main.py:57] epoch 2115, training loss: 6285.63, average training loss: 6773.42, base loss: 16021.58
[INFO 2017-06-29 21:38:43,203 main.py:57] epoch 2116, training loss: 6524.94, average training loss: 6772.75, base loss: 16022.34
[INFO 2017-06-29 21:38:46,056 main.py:57] epoch 2117, training loss: 6598.22, average training loss: 6772.16, base loss: 16022.37
[INFO 2017-06-29 21:38:48,957 main.py:57] epoch 2118, training loss: 6550.78, average training loss: 6771.36, base loss: 16021.68
[INFO 2017-06-29 21:38:51,836 main.py:57] epoch 2119, training loss: 6159.81, average training loss: 6770.61, base loss: 16021.07
[INFO 2017-06-29 21:38:54,705 main.py:57] epoch 2120, training loss: 6808.30, average training loss: 6770.23, base loss: 16023.01
[INFO 2017-06-29 21:38:57,613 main.py:57] epoch 2121, training loss: 6920.04, average training loss: 6770.00, base loss: 16024.32
[INFO 2017-06-29 21:39:00,469 main.py:57] epoch 2122, training loss: 6224.21, average training loss: 6769.26, base loss: 16023.59
[INFO 2017-06-29 21:39:03,381 main.py:57] epoch 2123, training loss: 6293.13, average training loss: 6768.43, base loss: 16023.54
[INFO 2017-06-29 21:39:06,270 main.py:57] epoch 2124, training loss: 6597.72, average training loss: 6767.95, base loss: 16024.27
[INFO 2017-06-29 21:39:09,205 main.py:57] epoch 2125, training loss: 6764.96, average training loss: 6767.80, base loss: 16025.41
[INFO 2017-06-29 21:39:12,072 main.py:57] epoch 2126, training loss: 7136.71, average training loss: 6767.84, base loss: 16025.75
[INFO 2017-06-29 21:39:14,950 main.py:57] epoch 2127, training loss: 6538.51, average training loss: 6767.63, base loss: 16025.14
[INFO 2017-06-29 21:39:17,883 main.py:57] epoch 2128, training loss: 6604.71, average training loss: 6767.50, base loss: 16024.19
[INFO 2017-06-29 21:39:20,733 main.py:57] epoch 2129, training loss: 6463.59, average training loss: 6767.44, base loss: 16024.41
[INFO 2017-06-29 21:39:23,614 main.py:57] epoch 2130, training loss: 6376.11, average training loss: 6766.92, base loss: 16024.45
[INFO 2017-06-29 21:39:26,499 main.py:57] epoch 2131, training loss: 6176.27, average training loss: 6765.77, base loss: 16022.63
[INFO 2017-06-29 21:39:29,341 main.py:57] epoch 2132, training loss: 6093.41, average training loss: 6764.56, base loss: 16021.42
[INFO 2017-06-29 21:39:32,217 main.py:57] epoch 2133, training loss: 6190.93, average training loss: 6763.99, base loss: 16020.65
[INFO 2017-06-29 21:39:35,105 main.py:57] epoch 2134, training loss: 6373.44, average training loss: 6763.42, base loss: 16020.89
[INFO 2017-06-29 21:39:37,990 main.py:57] epoch 2135, training loss: 6261.25, average training loss: 6762.40, base loss: 16020.35
[INFO 2017-06-29 21:39:40,878 main.py:57] epoch 2136, training loss: 6803.06, average training loss: 6762.44, base loss: 16021.07
[INFO 2017-06-29 21:39:43,722 main.py:57] epoch 2137, training loss: 6539.44, average training loss: 6761.17, base loss: 16021.49
[INFO 2017-06-29 21:39:46,539 main.py:57] epoch 2138, training loss: 6597.95, average training loss: 6761.15, base loss: 16022.22
[INFO 2017-06-29 21:39:49,430 main.py:57] epoch 2139, training loss: 6686.38, average training loss: 6761.49, base loss: 16022.52
[INFO 2017-06-29 21:39:52,279 main.py:57] epoch 2140, training loss: 6451.92, average training loss: 6761.16, base loss: 16022.56
[INFO 2017-06-29 21:39:55,151 main.py:57] epoch 2141, training loss: 6365.84, average training loss: 6760.79, base loss: 16022.94
[INFO 2017-06-29 21:39:57,981 main.py:57] epoch 2142, training loss: 6485.56, average training loss: 6759.93, base loss: 16022.25
[INFO 2017-06-29 21:40:00,831 main.py:57] epoch 2143, training loss: 5844.04, average training loss: 6758.74, base loss: 16020.20
[INFO 2017-06-29 21:40:03,734 main.py:57] epoch 2144, training loss: 7028.62, average training loss: 6759.03, base loss: 16021.35
[INFO 2017-06-29 21:40:06,636 main.py:57] epoch 2145, training loss: 6747.77, average training loss: 6759.06, base loss: 16022.43
[INFO 2017-06-29 21:40:09,576 main.py:57] epoch 2146, training loss: 6434.08, average training loss: 6758.74, base loss: 16021.53
[INFO 2017-06-29 21:40:12,425 main.py:57] epoch 2147, training loss: 6815.60, average training loss: 6758.86, base loss: 16022.06
[INFO 2017-06-29 21:40:15,291 main.py:57] epoch 2148, training loss: 7066.85, average training loss: 6758.74, base loss: 16023.36
[INFO 2017-06-29 21:40:18,116 main.py:57] epoch 2149, training loss: 6884.16, average training loss: 6758.63, base loss: 16023.99
[INFO 2017-06-29 21:40:20,954 main.py:57] epoch 2150, training loss: 6689.69, average training loss: 6758.85, base loss: 16024.30
[INFO 2017-06-29 21:40:23,861 main.py:57] epoch 2151, training loss: 6102.07, average training loss: 6757.85, base loss: 16023.62
[INFO 2017-06-29 21:40:26,707 main.py:57] epoch 2152, training loss: 6196.80, average training loss: 6757.27, base loss: 16022.69
[INFO 2017-06-29 21:40:29,580 main.py:57] epoch 2153, training loss: 6680.61, average training loss: 6756.85, base loss: 16023.70
[INFO 2017-06-29 21:40:32,432 main.py:57] epoch 2154, training loss: 6434.16, average training loss: 6756.37, base loss: 16023.83
[INFO 2017-06-29 21:40:35,276 main.py:57] epoch 2155, training loss: 7411.98, average training loss: 6756.33, base loss: 16027.06
[INFO 2017-06-29 21:40:38,188 main.py:57] epoch 2156, training loss: 6495.37, average training loss: 6755.36, base loss: 16027.03
[INFO 2017-06-29 21:40:41,088 main.py:57] epoch 2157, training loss: 6593.74, average training loss: 6755.13, base loss: 16026.80
[INFO 2017-06-29 21:40:43,994 main.py:57] epoch 2158, training loss: 6357.18, average training loss: 6754.89, base loss: 16026.42
[INFO 2017-06-29 21:40:46,853 main.py:57] epoch 2159, training loss: 6546.83, average training loss: 6753.77, base loss: 16025.38
[INFO 2017-06-29 21:40:49,740 main.py:57] epoch 2160, training loss: 6730.56, average training loss: 6753.14, base loss: 16026.20
[INFO 2017-06-29 21:40:52,674 main.py:57] epoch 2161, training loss: 6065.26, average training loss: 6752.74, base loss: 16025.48
[INFO 2017-06-29 21:40:55,556 main.py:57] epoch 2162, training loss: 7124.47, average training loss: 6752.29, base loss: 16026.28
[INFO 2017-06-29 21:40:58,372 main.py:57] epoch 2163, training loss: 6260.34, average training loss: 6751.54, base loss: 16025.68
[INFO 2017-06-29 21:41:01,222 main.py:57] epoch 2164, training loss: 6664.55, average training loss: 6751.56, base loss: 16025.15
[INFO 2017-06-29 21:41:04,107 main.py:57] epoch 2165, training loss: 6348.98, average training loss: 6751.49, base loss: 16024.53
[INFO 2017-06-29 21:41:07,001 main.py:57] epoch 2166, training loss: 6426.46, average training loss: 6751.18, base loss: 16024.23
[INFO 2017-06-29 21:41:09,883 main.py:57] epoch 2167, training loss: 6149.57, average training loss: 6751.00, base loss: 16023.10
[INFO 2017-06-29 21:41:12,734 main.py:57] epoch 2168, training loss: 6403.62, average training loss: 6750.32, base loss: 16022.54
[INFO 2017-06-29 21:41:15,642 main.py:57] epoch 2169, training loss: 6821.90, average training loss: 6750.65, base loss: 16022.70
[INFO 2017-06-29 21:41:18,471 main.py:57] epoch 2170, training loss: 6740.28, average training loss: 6749.79, base loss: 16023.50
[INFO 2017-06-29 21:41:21,341 main.py:57] epoch 2171, training loss: 6934.02, average training loss: 6749.54, base loss: 16025.09
[INFO 2017-06-29 21:41:24,193 main.py:57] epoch 2172, training loss: 6176.59, average training loss: 6748.57, base loss: 16024.65
[INFO 2017-06-29 21:41:27,110 main.py:57] epoch 2173, training loss: 5966.56, average training loss: 6747.70, base loss: 16023.59
[INFO 2017-06-29 21:41:30,008 main.py:57] epoch 2174, training loss: 6453.41, average training loss: 6747.42, base loss: 16023.06
[INFO 2017-06-29 21:41:32,876 main.py:57] epoch 2175, training loss: 6866.83, average training loss: 6747.04, base loss: 16024.60
[INFO 2017-06-29 21:41:35,727 main.py:57] epoch 2176, training loss: 6372.19, average training loss: 6746.78, base loss: 16023.78
[INFO 2017-06-29 21:41:38,640 main.py:57] epoch 2177, training loss: 6777.26, average training loss: 6746.41, base loss: 16024.57
[INFO 2017-06-29 21:41:41,542 main.py:57] epoch 2178, training loss: 6563.17, average training loss: 6745.33, base loss: 16024.46
[INFO 2017-06-29 21:41:44,417 main.py:57] epoch 2179, training loss: 6268.84, average training loss: 6744.80, base loss: 16023.86
[INFO 2017-06-29 21:41:47,260 main.py:57] epoch 2180, training loss: 6425.80, average training loss: 6743.97, base loss: 16023.79
[INFO 2017-06-29 21:41:50,174 main.py:57] epoch 2181, training loss: 6842.72, average training loss: 6743.22, base loss: 16024.25
[INFO 2017-06-29 21:41:53,057 main.py:57] epoch 2182, training loss: 6468.95, average training loss: 6742.96, base loss: 16022.19
[INFO 2017-06-29 21:41:55,931 main.py:57] epoch 2183, training loss: 6535.57, average training loss: 6742.85, base loss: 16021.17
[INFO 2017-06-29 21:41:58,861 main.py:57] epoch 2184, training loss: 6826.74, average training loss: 6742.77, base loss: 16021.83
[INFO 2017-06-29 21:42:01,739 main.py:57] epoch 2185, training loss: 6998.04, average training loss: 6742.78, base loss: 16023.27
[INFO 2017-06-29 21:42:04,629 main.py:57] epoch 2186, training loss: 6320.76, average training loss: 6741.97, base loss: 16021.89
[INFO 2017-06-29 21:42:07,511 main.py:57] epoch 2187, training loss: 6533.57, average training loss: 6740.95, base loss: 16021.45
[INFO 2017-06-29 21:42:10,393 main.py:57] epoch 2188, training loss: 6636.67, average training loss: 6740.56, base loss: 16021.45
[INFO 2017-06-29 21:42:13,279 main.py:57] epoch 2189, training loss: 6555.86, average training loss: 6740.23, base loss: 16020.72
[INFO 2017-06-29 21:42:16,165 main.py:57] epoch 2190, training loss: 6559.50, average training loss: 6740.02, base loss: 16020.24
[INFO 2017-06-29 21:42:19,049 main.py:57] epoch 2191, training loss: 6743.75, average training loss: 6739.69, base loss: 16020.44
[INFO 2017-06-29 21:42:21,931 main.py:57] epoch 2192, training loss: 6537.82, average training loss: 6739.09, base loss: 16020.34
[INFO 2017-06-29 21:42:24,850 main.py:57] epoch 2193, training loss: 7014.88, average training loss: 6739.58, base loss: 16022.11
[INFO 2017-06-29 21:42:27,682 main.py:57] epoch 2194, training loss: 6780.07, average training loss: 6739.56, base loss: 16021.77
[INFO 2017-06-29 21:42:30,570 main.py:57] epoch 2195, training loss: 6935.92, average training loss: 6738.74, base loss: 16022.22
[INFO 2017-06-29 21:42:33,424 main.py:57] epoch 2196, training loss: 6395.74, average training loss: 6738.30, base loss: 16022.57
[INFO 2017-06-29 21:42:36,334 main.py:57] epoch 2197, training loss: 6214.11, average training loss: 6737.25, base loss: 16021.80
[INFO 2017-06-29 21:42:39,157 main.py:57] epoch 2198, training loss: 6413.24, average training loss: 6736.54, base loss: 16021.12
[INFO 2017-06-29 21:42:42,041 main.py:57] epoch 2199, training loss: 6949.56, average training loss: 6736.55, base loss: 16021.35
[INFO 2017-06-29 21:42:42,041 main.py:59] epoch 2199, testing
[INFO 2017-06-29 21:42:54,416 main.py:104] average testing loss: 6402.67, base loss: 15825.78
[INFO 2017-06-29 21:42:54,416 main.py:105] improve_loss: 9423.10, improve_percent: 0.60
[INFO 2017-06-29 21:42:54,418 main.py:71] current best improved percent: 0.60
[INFO 2017-06-29 21:42:57,261 main.py:57] epoch 2200, training loss: 7241.68, average training loss: 6736.61, base loss: 16022.69
[INFO 2017-06-29 21:43:00,172 main.py:57] epoch 2201, training loss: 6660.73, average training loss: 6735.61, base loss: 16023.18
[INFO 2017-06-29 21:43:03,029 main.py:57] epoch 2202, training loss: 6322.59, average training loss: 6735.29, base loss: 16021.43
[INFO 2017-06-29 21:43:05,919 main.py:57] epoch 2203, training loss: 6582.79, average training loss: 6735.17, base loss: 16020.71
[INFO 2017-06-29 21:43:08,805 main.py:57] epoch 2204, training loss: 6527.10, average training loss: 6735.12, base loss: 16020.39
[INFO 2017-06-29 21:43:11,724 main.py:57] epoch 2205, training loss: 6421.25, average training loss: 6734.59, base loss: 16019.45
[INFO 2017-06-29 21:43:14,590 main.py:57] epoch 2206, training loss: 6821.86, average training loss: 6734.70, base loss: 16020.26
[INFO 2017-06-29 21:43:17,448 main.py:57] epoch 2207, training loss: 6902.71, average training loss: 6734.72, base loss: 16021.18
[INFO 2017-06-29 21:43:20,361 main.py:57] epoch 2208, training loss: 6443.76, average training loss: 6734.51, base loss: 16020.24
[INFO 2017-06-29 21:43:23,214 main.py:57] epoch 2209, training loss: 6837.55, average training loss: 6734.41, base loss: 16020.49
[INFO 2017-06-29 21:43:26,078 main.py:57] epoch 2210, training loss: 6444.22, average training loss: 6733.80, base loss: 16022.01
[INFO 2017-06-29 21:43:28,976 main.py:57] epoch 2211, training loss: 6512.44, average training loss: 6733.11, base loss: 16022.81
[INFO 2017-06-29 21:43:31,874 main.py:57] epoch 2212, training loss: 5997.92, average training loss: 6732.11, base loss: 16021.36
[INFO 2017-06-29 21:43:34,717 main.py:57] epoch 2213, training loss: 6039.35, average training loss: 6731.92, base loss: 16020.30
[INFO 2017-06-29 21:43:37,643 main.py:57] epoch 2214, training loss: 6292.84, average training loss: 6731.09, base loss: 16019.17
[INFO 2017-06-29 21:43:40,505 main.py:57] epoch 2215, training loss: 6407.63, average training loss: 6730.94, base loss: 16018.04
[INFO 2017-06-29 21:43:43,392 main.py:57] epoch 2216, training loss: 6917.69, average training loss: 6731.01, base loss: 16018.67
[INFO 2017-06-29 21:43:46,238 main.py:57] epoch 2217, training loss: 7052.16, average training loss: 6730.99, base loss: 16019.56
[INFO 2017-06-29 21:43:49,113 main.py:57] epoch 2218, training loss: 5813.10, average training loss: 6730.28, base loss: 16018.32
[INFO 2017-06-29 21:43:52,015 main.py:57] epoch 2219, training loss: 6231.73, average training loss: 6729.23, base loss: 16017.91
[INFO 2017-06-29 21:43:54,906 main.py:57] epoch 2220, training loss: 6585.31, average training loss: 6728.65, base loss: 16017.97
[INFO 2017-06-29 21:43:57,792 main.py:57] epoch 2221, training loss: 6465.77, average training loss: 6727.75, base loss: 16018.46
[INFO 2017-06-29 21:44:00,694 main.py:57] epoch 2222, training loss: 7007.57, average training loss: 6728.56, base loss: 16019.95
[INFO 2017-06-29 21:44:03,522 main.py:57] epoch 2223, training loss: 6559.44, average training loss: 6727.68, base loss: 16020.59
[INFO 2017-06-29 21:44:06,371 main.py:57] epoch 2224, training loss: 6741.58, average training loss: 6727.45, base loss: 16020.72
[INFO 2017-06-29 21:44:09,289 main.py:57] epoch 2225, training loss: 6662.38, average training loss: 6727.67, base loss: 16020.48
[INFO 2017-06-29 21:44:12,123 main.py:57] epoch 2226, training loss: 6395.16, average training loss: 6727.33, base loss: 16021.71
[INFO 2017-06-29 21:44:14,970 main.py:57] epoch 2227, training loss: 6587.21, average training loss: 6727.57, base loss: 16022.20
[INFO 2017-06-29 21:44:17,874 main.py:57] epoch 2228, training loss: 6634.17, average training loss: 6727.03, base loss: 16022.39
[INFO 2017-06-29 21:44:20,738 main.py:57] epoch 2229, training loss: 6045.38, average training loss: 6726.65, base loss: 16021.11
[INFO 2017-06-29 21:44:23,619 main.py:57] epoch 2230, training loss: 6313.16, average training loss: 6725.93, base loss: 16020.55
[INFO 2017-06-29 21:44:26,473 main.py:57] epoch 2231, training loss: 6057.11, average training loss: 6725.03, base loss: 16019.11
[INFO 2017-06-29 21:44:29,345 main.py:57] epoch 2232, training loss: 6491.60, average training loss: 6724.12, base loss: 16018.31
[INFO 2017-06-29 21:44:32,266 main.py:57] epoch 2233, training loss: 6627.29, average training loss: 6723.38, base loss: 16018.91
[INFO 2017-06-29 21:44:35,173 main.py:57] epoch 2234, training loss: 6578.61, average training loss: 6723.34, base loss: 16018.24
[INFO 2017-06-29 21:44:38,030 main.py:57] epoch 2235, training loss: 6494.83, average training loss: 6722.92, base loss: 16018.12
[INFO 2017-06-29 21:44:40,891 main.py:57] epoch 2236, training loss: 6062.58, average training loss: 6721.97, base loss: 16016.90
[INFO 2017-06-29 21:44:43,756 main.py:57] epoch 2237, training loss: 7027.82, average training loss: 6721.78, base loss: 16018.32
[INFO 2017-06-29 21:44:46,627 main.py:57] epoch 2238, training loss: 6347.20, average training loss: 6720.74, base loss: 16018.30
[INFO 2017-06-29 21:44:49,548 main.py:57] epoch 2239, training loss: 6368.68, average training loss: 6720.02, base loss: 16018.32
[INFO 2017-06-29 21:44:52,428 main.py:57] epoch 2240, training loss: 6617.35, average training loss: 6720.00, base loss: 16017.86
[INFO 2017-06-29 21:44:55,280 main.py:57] epoch 2241, training loss: 6096.96, average training loss: 6719.94, base loss: 16016.77
[INFO 2017-06-29 21:44:58,171 main.py:57] epoch 2242, training loss: 6240.19, average training loss: 6718.97, base loss: 16016.71
[INFO 2017-06-29 21:45:01,022 main.py:57] epoch 2243, training loss: 6561.75, average training loss: 6717.97, base loss: 16017.07
[INFO 2017-06-29 21:45:03,926 main.py:57] epoch 2244, training loss: 6654.14, average training loss: 6717.85, base loss: 16016.92
[INFO 2017-06-29 21:45:06,805 main.py:57] epoch 2245, training loss: 6368.99, average training loss: 6716.95, base loss: 16015.86
[INFO 2017-06-29 21:45:09,653 main.py:57] epoch 2246, training loss: 6429.51, average training loss: 6715.44, base loss: 16016.83
[INFO 2017-06-29 21:45:12,523 main.py:57] epoch 2247, training loss: 6271.91, average training loss: 6714.17, base loss: 16015.70
[INFO 2017-06-29 21:45:15,360 main.py:57] epoch 2248, training loss: 6497.81, average training loss: 6713.18, base loss: 16014.51
[INFO 2017-06-29 21:45:18,229 main.py:57] epoch 2249, training loss: 6428.29, average training loss: 6713.13, base loss: 16014.02
[INFO 2017-06-29 21:45:21,085 main.py:57] epoch 2250, training loss: 6384.76, average training loss: 6712.60, base loss: 16014.52
[INFO 2017-06-29 21:45:23,922 main.py:57] epoch 2251, training loss: 6927.92, average training loss: 6713.16, base loss: 16015.86
[INFO 2017-06-29 21:45:26,790 main.py:57] epoch 2252, training loss: 6846.03, average training loss: 6712.64, base loss: 16016.58
[INFO 2017-06-29 21:45:29,648 main.py:57] epoch 2253, training loss: 6338.91, average training loss: 6711.56, base loss: 16015.96
[INFO 2017-06-29 21:45:32,538 main.py:57] epoch 2254, training loss: 6718.28, average training loss: 6710.63, base loss: 16017.43
[INFO 2017-06-29 21:45:35,354 main.py:57] epoch 2255, training loss: 6464.12, average training loss: 6710.21, base loss: 16018.12
[INFO 2017-06-29 21:45:38,227 main.py:57] epoch 2256, training loss: 6930.57, average training loss: 6710.25, base loss: 16018.01
[INFO 2017-06-29 21:45:41,071 main.py:57] epoch 2257, training loss: 6814.12, average training loss: 6710.25, base loss: 16018.84
[INFO 2017-06-29 21:45:43,958 main.py:57] epoch 2258, training loss: 6245.12, average training loss: 6709.49, base loss: 16019.03
[INFO 2017-06-29 21:45:46,851 main.py:57] epoch 2259, training loss: 6885.97, average training loss: 6709.73, base loss: 16021.21
[INFO 2017-06-29 21:45:49,695 main.py:57] epoch 2260, training loss: 6563.65, average training loss: 6709.40, base loss: 16021.98
[INFO 2017-06-29 21:45:52,541 main.py:57] epoch 2261, training loss: 6303.27, average training loss: 6708.40, base loss: 16021.19
[INFO 2017-06-29 21:45:55,419 main.py:57] epoch 2262, training loss: 6586.22, average training loss: 6708.64, base loss: 16021.73
[INFO 2017-06-29 21:45:58,286 main.py:57] epoch 2263, training loss: 6457.55, average training loss: 6708.00, base loss: 16021.86
[INFO 2017-06-29 21:46:01,171 main.py:57] epoch 2264, training loss: 6866.05, average training loss: 6707.12, base loss: 16022.52
[INFO 2017-06-29 21:46:04,031 main.py:57] epoch 2265, training loss: 6790.37, average training loss: 6707.33, base loss: 16022.98
[INFO 2017-06-29 21:46:06,928 main.py:57] epoch 2266, training loss: 7086.89, average training loss: 6707.62, base loss: 16023.46
[INFO 2017-06-29 21:46:09,777 main.py:57] epoch 2267, training loss: 6657.45, average training loss: 6707.52, base loss: 16023.10
[INFO 2017-06-29 21:46:12,653 main.py:57] epoch 2268, training loss: 6452.45, average training loss: 6706.82, base loss: 16023.09
[INFO 2017-06-29 21:46:15,515 main.py:57] epoch 2269, training loss: 6525.70, average training loss: 6706.46, base loss: 16022.73
[INFO 2017-06-29 21:46:18,375 main.py:57] epoch 2270, training loss: 6444.13, average training loss: 6706.11, base loss: 16022.58
[INFO 2017-06-29 21:46:21,221 main.py:57] epoch 2271, training loss: 6440.05, average training loss: 6705.66, base loss: 16021.84
[INFO 2017-06-29 21:46:24,113 main.py:57] epoch 2272, training loss: 6746.89, average training loss: 6705.34, base loss: 16020.66
[INFO 2017-06-29 21:46:26,949 main.py:57] epoch 2273, training loss: 6691.36, average training loss: 6705.49, base loss: 16019.81
[INFO 2017-06-29 21:46:29,825 main.py:57] epoch 2274, training loss: 6263.59, average training loss: 6704.50, base loss: 16019.43
[INFO 2017-06-29 21:46:32,708 main.py:57] epoch 2275, training loss: 6394.81, average training loss: 6703.37, base loss: 16018.31
[INFO 2017-06-29 21:46:35,557 main.py:57] epoch 2276, training loss: 6913.51, average training loss: 6703.00, base loss: 16018.25
[INFO 2017-06-29 21:46:38,391 main.py:57] epoch 2277, training loss: 6468.16, average training loss: 6702.30, base loss: 16018.00
[INFO 2017-06-29 21:46:41,247 main.py:57] epoch 2278, training loss: 6648.93, average training loss: 6702.09, base loss: 16018.03
[INFO 2017-06-29 21:46:44,123 main.py:57] epoch 2279, training loss: 6307.67, average training loss: 6702.12, base loss: 16016.83
[INFO 2017-06-29 21:46:47,004 main.py:57] epoch 2280, training loss: 6632.98, average training loss: 6702.35, base loss: 16018.09
[INFO 2017-06-29 21:46:49,887 main.py:57] epoch 2281, training loss: 6048.76, average training loss: 6701.14, base loss: 16017.85
[INFO 2017-06-29 21:46:52,780 main.py:57] epoch 2282, training loss: 6598.67, average training loss: 6700.95, base loss: 16017.70
[INFO 2017-06-29 21:46:55,678 main.py:57] epoch 2283, training loss: 6939.91, average training loss: 6700.52, base loss: 16019.78
[INFO 2017-06-29 21:46:58,581 main.py:57] epoch 2284, training loss: 6536.64, average training loss: 6699.19, base loss: 16019.52
[INFO 2017-06-29 21:47:01,456 main.py:57] epoch 2285, training loss: 6462.01, average training loss: 6699.03, base loss: 16019.86
[INFO 2017-06-29 21:47:04,333 main.py:57] epoch 2286, training loss: 6692.89, average training loss: 6698.32, base loss: 16021.03
[INFO 2017-06-29 21:47:07,189 main.py:57] epoch 2287, training loss: 6752.92, average training loss: 6697.48, base loss: 16021.84
[INFO 2017-06-29 21:47:10,059 main.py:57] epoch 2288, training loss: 6423.70, average training loss: 6696.26, base loss: 16022.67
[INFO 2017-06-29 21:47:12,961 main.py:57] epoch 2289, training loss: 6702.05, average training loss: 6696.45, base loss: 16024.04
[INFO 2017-06-29 21:47:15,788 main.py:57] epoch 2290, training loss: 6287.64, average training loss: 6695.54, base loss: 16023.07
[INFO 2017-06-29 21:47:18,646 main.py:57] epoch 2291, training loss: 6713.08, average training loss: 6695.60, base loss: 16022.92
[INFO 2017-06-29 21:47:21,581 main.py:57] epoch 2292, training loss: 6624.38, average training loss: 6694.77, base loss: 16023.15
[INFO 2017-06-29 21:47:24,465 main.py:57] epoch 2293, training loss: 6574.98, average training loss: 6694.47, base loss: 16022.93
[INFO 2017-06-29 21:47:27,325 main.py:57] epoch 2294, training loss: 5979.85, average training loss: 6693.22, base loss: 16021.72
[INFO 2017-06-29 21:47:30,205 main.py:57] epoch 2295, training loss: 6594.51, average training loss: 6692.44, base loss: 16021.48
[INFO 2017-06-29 21:47:33,076 main.py:57] epoch 2296, training loss: 6295.50, average training loss: 6691.97, base loss: 16021.67
[INFO 2017-06-29 21:47:35,952 main.py:57] epoch 2297, training loss: 7004.47, average training loss: 6692.04, base loss: 16023.83
[INFO 2017-06-29 21:47:38,791 main.py:57] epoch 2298, training loss: 6841.79, average training loss: 6692.09, base loss: 16023.72
[INFO 2017-06-29 21:47:41,661 main.py:57] epoch 2299, training loss: 6298.13, average training loss: 6691.20, base loss: 16023.05
[INFO 2017-06-29 21:47:41,661 main.py:59] epoch 2299, testing
[INFO 2017-06-29 21:47:53,935 main.py:104] average testing loss: 6374.05, base loss: 15502.93
[INFO 2017-06-29 21:47:53,935 main.py:105] improve_loss: 9128.88, improve_percent: 0.59
[INFO 2017-06-29 21:47:53,937 main.py:71] current best improved percent: 0.60
[INFO 2017-06-29 21:47:56,834 main.py:57] epoch 2300, training loss: 6172.65, average training loss: 6690.11, base loss: 16022.79
[INFO 2017-06-29 21:47:59,683 main.py:57] epoch 2301, training loss: 6256.25, average training loss: 6689.29, base loss: 16021.68
[INFO 2017-06-29 21:48:02,513 main.py:57] epoch 2302, training loss: 7009.30, average training loss: 6689.75, base loss: 16023.13
[INFO 2017-06-29 21:48:05,369 main.py:57] epoch 2303, training loss: 6764.78, average training loss: 6689.59, base loss: 16023.88
[INFO 2017-06-29 21:48:08,201 main.py:57] epoch 2304, training loss: 6876.17, average training loss: 6689.56, base loss: 16023.90
[INFO 2017-06-29 21:48:11,078 main.py:57] epoch 2305, training loss: 7199.18, average training loss: 6689.43, base loss: 16023.96
[INFO 2017-06-29 21:48:13,977 main.py:57] epoch 2306, training loss: 7138.11, average training loss: 6689.38, base loss: 16024.97
[INFO 2017-06-29 21:48:16,872 main.py:57] epoch 2307, training loss: 6412.59, average training loss: 6689.13, base loss: 16024.80
[INFO 2017-06-29 21:48:19,747 main.py:57] epoch 2308, training loss: 6254.45, average training loss: 6688.59, base loss: 16023.82
[INFO 2017-06-29 21:48:22,618 main.py:57] epoch 2309, training loss: 6122.19, average training loss: 6687.66, base loss: 16023.25
[INFO 2017-06-29 21:48:25,445 main.py:57] epoch 2310, training loss: 6713.29, average training loss: 6687.27, base loss: 16022.92
[INFO 2017-06-29 21:48:28,378 main.py:57] epoch 2311, training loss: 6297.20, average training loss: 6686.99, base loss: 16021.99
[INFO 2017-06-29 21:48:31,226 main.py:57] epoch 2312, training loss: 5997.98, average training loss: 6686.19, base loss: 16020.01
[INFO 2017-06-29 21:48:34,074 main.py:57] epoch 2313, training loss: 7131.51, average training loss: 6686.28, base loss: 16020.98
[INFO 2017-06-29 21:48:36,960 main.py:57] epoch 2314, training loss: 6526.74, average training loss: 6686.37, base loss: 16021.62
[INFO 2017-06-29 21:48:39,838 main.py:57] epoch 2315, training loss: 6189.96, average training loss: 6685.73, base loss: 16020.04
[INFO 2017-06-29 21:48:42,680 main.py:57] epoch 2316, training loss: 6713.63, average training loss: 6686.02, base loss: 16019.99
[INFO 2017-06-29 21:48:45,569 main.py:57] epoch 2317, training loss: 6222.85, average training loss: 6685.28, base loss: 16019.83
[INFO 2017-06-29 21:48:48,453 main.py:57] epoch 2318, training loss: 7039.13, average training loss: 6685.99, base loss: 16020.61
[INFO 2017-06-29 21:48:51,348 main.py:57] epoch 2319, training loss: 6568.23, average training loss: 6685.34, base loss: 16020.11
[INFO 2017-06-29 21:48:54,239 main.py:57] epoch 2320, training loss: 6466.53, average training loss: 6684.64, base loss: 16019.04
[INFO 2017-06-29 21:48:57,094 main.py:57] epoch 2321, training loss: 6113.29, average training loss: 6684.03, base loss: 16017.39
[INFO 2017-06-29 21:48:59,965 main.py:57] epoch 2322, training loss: 6232.44, average training loss: 6683.15, base loss: 16016.92
[INFO 2017-06-29 21:49:02,852 main.py:57] epoch 2323, training loss: 6923.13, average training loss: 6683.62, base loss: 16017.66
[INFO 2017-06-29 21:49:05,686 main.py:57] epoch 2324, training loss: 6886.59, average training loss: 6683.02, base loss: 16019.17
[INFO 2017-06-29 21:49:08,626 main.py:57] epoch 2325, training loss: 7331.20, average training loss: 6683.88, base loss: 16021.01
[INFO 2017-06-29 21:49:11,509 main.py:57] epoch 2326, training loss: 6888.83, average training loss: 6684.07, base loss: 16021.68
[INFO 2017-06-29 21:49:14,446 main.py:57] epoch 2327, training loss: 6629.48, average training loss: 6683.97, base loss: 16021.75
[INFO 2017-06-29 21:49:17,324 main.py:57] epoch 2328, training loss: 6462.66, average training loss: 6683.88, base loss: 16019.52
[INFO 2017-06-29 21:49:20,213 main.py:57] epoch 2329, training loss: 6868.16, average training loss: 6683.67, base loss: 16018.97
[INFO 2017-06-29 21:49:23,046 main.py:57] epoch 2330, training loss: 6500.73, average training loss: 6683.09, base loss: 16019.52
[INFO 2017-06-29 21:49:25,875 main.py:57] epoch 2331, training loss: 6305.96, average training loss: 6682.38, base loss: 16020.23
[INFO 2017-06-29 21:49:28,778 main.py:57] epoch 2332, training loss: 6514.78, average training loss: 6682.13, base loss: 16020.73
[INFO 2017-06-29 21:49:31,672 main.py:57] epoch 2333, training loss: 6667.02, average training loss: 6682.35, base loss: 16021.17
[INFO 2017-06-29 21:49:34,602 main.py:57] epoch 2334, training loss: 6271.79, average training loss: 6680.86, base loss: 16020.92
[INFO 2017-06-29 21:49:37,476 main.py:57] epoch 2335, training loss: 6476.32, average training loss: 6681.13, base loss: 16020.06
[INFO 2017-06-29 21:49:40,324 main.py:57] epoch 2336, training loss: 6184.48, average training loss: 6680.28, base loss: 16018.86
[INFO 2017-06-29 21:49:43,195 main.py:57] epoch 2337, training loss: 6835.00, average training loss: 6680.27, base loss: 16019.88
[INFO 2017-06-29 21:49:46,127 main.py:57] epoch 2338, training loss: 6950.61, average training loss: 6680.58, base loss: 16020.72
[INFO 2017-06-29 21:49:48,992 main.py:57] epoch 2339, training loss: 6064.40, average training loss: 6679.76, base loss: 16020.09
[INFO 2017-06-29 21:49:51,902 main.py:57] epoch 2340, training loss: 6050.29, average training loss: 6678.83, base loss: 16018.60
[INFO 2017-06-29 21:49:54,781 main.py:57] epoch 2341, training loss: 6806.16, average training loss: 6679.21, base loss: 16018.35
[INFO 2017-06-29 21:49:57,681 main.py:57] epoch 2342, training loss: 6281.01, average training loss: 6678.90, base loss: 16018.68
[INFO 2017-06-29 21:50:00,570 main.py:57] epoch 2343, training loss: 6416.64, average training loss: 6678.48, base loss: 16018.73
[INFO 2017-06-29 21:50:03,446 main.py:57] epoch 2344, training loss: 6930.95, average training loss: 6678.78, base loss: 16019.15
[INFO 2017-06-29 21:50:06,288 main.py:57] epoch 2345, training loss: 6777.51, average training loss: 6678.51, base loss: 16020.00
[INFO 2017-06-29 21:50:09,170 main.py:57] epoch 2346, training loss: 6847.17, average training loss: 6678.90, base loss: 16020.92
[INFO 2017-06-29 21:50:12,036 main.py:57] epoch 2347, training loss: 6602.84, average training loss: 6678.69, base loss: 16021.34
[INFO 2017-06-29 21:50:14,898 main.py:57] epoch 2348, training loss: 6076.08, average training loss: 6677.80, base loss: 16019.16
[INFO 2017-06-29 21:50:17,736 main.py:57] epoch 2349, training loss: 6344.08, average training loss: 6676.85, base loss: 16018.26
[INFO 2017-06-29 21:50:20,638 main.py:57] epoch 2350, training loss: 6512.72, average training loss: 6676.23, base loss: 16017.85
[INFO 2017-06-29 21:50:23,482 main.py:57] epoch 2351, training loss: 6188.01, average training loss: 6675.31, base loss: 16016.87
[INFO 2017-06-29 21:50:26,342 main.py:57] epoch 2352, training loss: 6318.62, average training loss: 6675.16, base loss: 16016.48
[INFO 2017-06-29 21:50:29,191 main.py:57] epoch 2353, training loss: 7631.78, average training loss: 6675.54, base loss: 16019.44
[INFO 2017-06-29 21:50:32,090 main.py:57] epoch 2354, training loss: 6818.75, average training loss: 6675.80, base loss: 16019.05
[INFO 2017-06-29 21:50:35,038 main.py:57] epoch 2355, training loss: 6572.18, average training loss: 6675.62, base loss: 16019.55
[INFO 2017-06-29 21:50:37,915 main.py:57] epoch 2356, training loss: 7070.41, average training loss: 6675.40, base loss: 16021.49
[INFO 2017-06-29 21:50:40,796 main.py:57] epoch 2357, training loss: 6457.61, average training loss: 6674.69, base loss: 16021.04
[INFO 2017-06-29 21:50:43,651 main.py:57] epoch 2358, training loss: 6895.52, average training loss: 6675.10, base loss: 16021.34
[INFO 2017-06-29 21:50:46,542 main.py:57] epoch 2359, training loss: 6517.93, average training loss: 6674.85, base loss: 16021.33
[INFO 2017-06-29 21:50:49,389 main.py:57] epoch 2360, training loss: 6046.30, average training loss: 6673.03, base loss: 16021.40
[INFO 2017-06-29 21:50:52,247 main.py:57] epoch 2361, training loss: 6738.63, average training loss: 6672.64, base loss: 16021.67
[INFO 2017-06-29 21:50:55,096 main.py:57] epoch 2362, training loss: 6386.83, average training loss: 6672.00, base loss: 16021.59
[INFO 2017-06-29 21:50:57,951 main.py:57] epoch 2363, training loss: 6220.49, average training loss: 6671.11, base loss: 16021.37
[INFO 2017-06-29 21:51:00,814 main.py:57] epoch 2364, training loss: 6139.05, average training loss: 6670.54, base loss: 16020.91
[INFO 2017-06-29 21:51:03,658 main.py:57] epoch 2365, training loss: 6201.80, average training loss: 6670.02, base loss: 16019.99
[INFO 2017-06-29 21:51:06,507 main.py:57] epoch 2366, training loss: 6451.33, average training loss: 6669.38, base loss: 16019.56
[INFO 2017-06-29 21:51:09,340 main.py:57] epoch 2367, training loss: 6976.58, average training loss: 6670.18, base loss: 16020.55
[INFO 2017-06-29 21:51:12,201 main.py:57] epoch 2368, training loss: 6937.37, average training loss: 6670.26, base loss: 16020.46
[INFO 2017-06-29 21:51:15,091 main.py:57] epoch 2369, training loss: 6905.43, average training loss: 6670.49, base loss: 16021.46
[INFO 2017-06-29 21:51:17,944 main.py:57] epoch 2370, training loss: 6475.56, average training loss: 6670.25, base loss: 16021.86
[INFO 2017-06-29 21:51:20,817 main.py:57] epoch 2371, training loss: 6775.64, average training loss: 6669.99, base loss: 16022.20
[INFO 2017-06-29 21:51:23,656 main.py:57] epoch 2372, training loss: 6652.73, average training loss: 6669.95, base loss: 16021.72
[INFO 2017-06-29 21:51:26,541 main.py:57] epoch 2373, training loss: 6273.88, average training loss: 6669.51, base loss: 16020.89
[INFO 2017-06-29 21:51:29,428 main.py:57] epoch 2374, training loss: 6185.47, average training loss: 6669.01, base loss: 16020.26
[INFO 2017-06-29 21:51:32,266 main.py:57] epoch 2375, training loss: 6487.67, average training loss: 6668.98, base loss: 16020.02
[INFO 2017-06-29 21:51:35,237 main.py:57] epoch 2376, training loss: 6535.23, average training loss: 6668.47, base loss: 16019.72
[INFO 2017-06-29 21:51:38,107 main.py:57] epoch 2377, training loss: 6114.63, average training loss: 6666.95, base loss: 16019.04
[INFO 2017-06-29 21:51:40,959 main.py:57] epoch 2378, training loss: 7207.69, average training loss: 6666.90, base loss: 16019.21
[INFO 2017-06-29 21:51:43,846 main.py:57] epoch 2379, training loss: 6452.44, average training loss: 6666.63, base loss: 16017.79
[INFO 2017-06-29 21:51:46,679 main.py:57] epoch 2380, training loss: 6052.96, average training loss: 6665.90, base loss: 16016.18
[INFO 2017-06-29 21:51:49,558 main.py:57] epoch 2381, training loss: 6263.87, average training loss: 6665.66, base loss: 16014.48
[INFO 2017-06-29 21:51:52,423 main.py:57] epoch 2382, training loss: 6520.11, average training loss: 6664.67, base loss: 16014.37
[INFO 2017-06-29 21:51:55,265 main.py:57] epoch 2383, training loss: 6294.14, average training loss: 6664.31, base loss: 16013.53
[INFO 2017-06-29 21:51:58,114 main.py:57] epoch 2384, training loss: 6302.49, average training loss: 6663.64, base loss: 16013.07
[INFO 2017-06-29 21:52:00,960 main.py:57] epoch 2385, training loss: 6391.17, average training loss: 6663.28, base loss: 16012.91
[INFO 2017-06-29 21:52:03,805 main.py:57] epoch 2386, training loss: 6506.24, average training loss: 6663.12, base loss: 16013.09
[INFO 2017-06-29 21:52:06,698 main.py:57] epoch 2387, training loss: 5845.16, average training loss: 6662.12, base loss: 16012.42
[INFO 2017-06-29 21:52:09,572 main.py:57] epoch 2388, training loss: 6961.96, average training loss: 6662.07, base loss: 16013.42
[INFO 2017-06-29 21:52:12,466 main.py:57] epoch 2389, training loss: 7186.86, average training loss: 6662.85, base loss: 16015.31
[INFO 2017-06-29 21:52:15,315 main.py:57] epoch 2390, training loss: 6717.18, average training loss: 6662.68, base loss: 16016.82
[INFO 2017-06-29 21:52:18,210 main.py:57] epoch 2391, training loss: 6483.01, average training loss: 6662.28, base loss: 16017.24
[INFO 2017-06-29 21:52:21,129 main.py:57] epoch 2392, training loss: 6364.44, average training loss: 6661.46, base loss: 16017.47
[INFO 2017-06-29 21:52:23,981 main.py:57] epoch 2393, training loss: 5915.22, average training loss: 6660.78, base loss: 16016.33
[INFO 2017-06-29 21:52:26,843 main.py:57] epoch 2394, training loss: 6593.45, average training loss: 6660.58, base loss: 16016.71
[INFO 2017-06-29 21:52:29,732 main.py:57] epoch 2395, training loss: 6799.73, average training loss: 6659.62, base loss: 16016.27
[INFO 2017-06-29 21:52:32,588 main.py:57] epoch 2396, training loss: 6484.29, average training loss: 6659.23, base loss: 16016.02
[INFO 2017-06-29 21:52:35,474 main.py:57] epoch 2397, training loss: 6575.08, average training loss: 6658.73, base loss: 16015.96
[INFO 2017-06-29 21:52:38,341 main.py:57] epoch 2398, training loss: 6431.75, average training loss: 6657.81, base loss: 16016.77
[INFO 2017-06-29 21:52:41,228 main.py:57] epoch 2399, training loss: 6386.69, average training loss: 6656.95, base loss: 16016.54
[INFO 2017-06-29 21:52:41,229 main.py:59] epoch 2399, testing
[INFO 2017-06-29 21:52:53,595 main.py:104] average testing loss: 6471.85, base loss: 16087.95
[INFO 2017-06-29 21:52:53,595 main.py:105] improve_loss: 9616.10, improve_percent: 0.60
[INFO 2017-06-29 21:52:53,596 main.py:71] current best improved percent: 0.60
[INFO 2017-06-29 21:52:56,502 main.py:57] epoch 2400, training loss: 6854.08, average training loss: 6656.95, base loss: 16017.15
[INFO 2017-06-29 21:52:59,372 main.py:57] epoch 2401, training loss: 6075.92, average training loss: 6656.14, base loss: 16015.78
[INFO 2017-06-29 21:53:02,266 main.py:57] epoch 2402, training loss: 6235.55, average training loss: 6655.52, base loss: 16016.35
[INFO 2017-06-29 21:53:05,122 main.py:57] epoch 2403, training loss: 6488.74, average training loss: 6654.41, base loss: 16017.08
[INFO 2017-06-29 21:53:08,007 main.py:57] epoch 2404, training loss: 6548.81, average training loss: 6654.48, base loss: 16017.32
[INFO 2017-06-29 21:53:10,899 main.py:57] epoch 2405, training loss: 6248.95, average training loss: 6654.12, base loss: 16017.22
[INFO 2017-06-29 21:53:13,772 main.py:57] epoch 2406, training loss: 6308.82, average training loss: 6653.05, base loss: 16016.94
[INFO 2017-06-29 21:53:16,656 main.py:57] epoch 2407, training loss: 6751.37, average training loss: 6652.21, base loss: 16017.79
[INFO 2017-06-29 21:53:19,506 main.py:57] epoch 2408, training loss: 6201.54, average training loss: 6651.11, base loss: 16016.93
[INFO 2017-06-29 21:53:22,385 main.py:57] epoch 2409, training loss: 6144.77, average training loss: 6650.20, base loss: 16015.47
[INFO 2017-06-29 21:53:25,269 main.py:57] epoch 2410, training loss: 6604.01, average training loss: 6649.80, base loss: 16015.40
[INFO 2017-06-29 21:53:28,142 main.py:57] epoch 2411, training loss: 6484.35, average training loss: 6649.55, base loss: 16015.14
[INFO 2017-06-29 21:53:30,983 main.py:57] epoch 2412, training loss: 6671.18, average training loss: 6649.21, base loss: 16015.55
[INFO 2017-06-29 21:53:33,797 main.py:57] epoch 2413, training loss: 7150.77, average training loss: 6649.11, base loss: 16016.48
[INFO 2017-06-29 21:53:36,670 main.py:57] epoch 2414, training loss: 6108.01, average training loss: 6648.47, base loss: 16014.23
[INFO 2017-06-29 21:53:39,541 main.py:57] epoch 2415, training loss: 6414.34, average training loss: 6648.19, base loss: 16014.01
[INFO 2017-06-29 21:53:42,442 main.py:57] epoch 2416, training loss: 6706.86, average training loss: 6647.77, base loss: 16014.83
[INFO 2017-06-29 21:53:45,305 main.py:57] epoch 2417, training loss: 6326.85, average training loss: 6646.94, base loss: 16014.75
[INFO 2017-06-29 21:53:48,163 main.py:57] epoch 2418, training loss: 6137.59, average training loss: 6646.17, base loss: 16013.89
[INFO 2017-06-29 21:53:51,023 main.py:57] epoch 2419, training loss: 6470.46, average training loss: 6645.73, base loss: 16013.57
[INFO 2017-06-29 21:53:53,867 main.py:57] epoch 2420, training loss: 6626.55, average training loss: 6645.28, base loss: 16013.58
[INFO 2017-06-29 21:53:56,749 main.py:57] epoch 2421, training loss: 5907.83, average training loss: 6643.68, base loss: 16012.18
[INFO 2017-06-29 21:53:59,644 main.py:57] epoch 2422, training loss: 6583.56, average training loss: 6643.85, base loss: 16012.71
[INFO 2017-06-29 21:54:02,560 main.py:57] epoch 2423, training loss: 6783.98, average training loss: 6643.48, base loss: 16012.71
[INFO 2017-06-29 21:54:05,414 main.py:57] epoch 2424, training loss: 6663.20, average training loss: 6642.38, base loss: 16012.71
[INFO 2017-06-29 21:54:08,250 main.py:57] epoch 2425, training loss: 6751.79, average training loss: 6642.43, base loss: 16013.23
[INFO 2017-06-29 21:54:11,086 main.py:57] epoch 2426, training loss: 6317.95, average training loss: 6641.78, base loss: 16013.10
[INFO 2017-06-29 21:54:13,933 main.py:57] epoch 2427, training loss: 6451.19, average training loss: 6641.32, base loss: 16012.80
[INFO 2017-06-29 21:54:16,809 main.py:57] epoch 2428, training loss: 6407.44, average training loss: 6640.36, base loss: 16014.04
[INFO 2017-06-29 21:54:19,644 main.py:57] epoch 2429, training loss: 7193.99, average training loss: 6640.63, base loss: 16016.05
[INFO 2017-06-29 21:54:22,475 main.py:57] epoch 2430, training loss: 6716.75, average training loss: 6640.44, base loss: 16016.54
[INFO 2017-06-29 21:54:25,342 main.py:57] epoch 2431, training loss: 6401.79, average training loss: 6639.59, base loss: 16015.70
[INFO 2017-06-29 21:54:28,216 main.py:57] epoch 2432, training loss: 6537.87, average training loss: 6638.48, base loss: 16015.30
[INFO 2017-06-29 21:54:31,089 main.py:57] epoch 2433, training loss: 6289.02, average training loss: 6637.94, base loss: 16014.51
[INFO 2017-06-29 21:54:34,005 main.py:57] epoch 2434, training loss: 6584.31, average training loss: 6637.33, base loss: 16014.74
[INFO 2017-06-29 21:54:36,843 main.py:57] epoch 2435, training loss: 6437.12, average training loss: 6637.11, base loss: 16014.81
[INFO 2017-06-29 21:54:39,700 main.py:57] epoch 2436, training loss: 6430.07, average training loss: 6636.78, base loss: 16015.85
[INFO 2017-06-29 21:54:42,592 main.py:57] epoch 2437, training loss: 6776.80, average training loss: 6636.31, base loss: 16016.93
[INFO 2017-06-29 21:54:45,446 main.py:57] epoch 2438, training loss: 7017.94, average training loss: 6636.25, base loss: 16018.24
[INFO 2017-06-29 21:54:48,279 main.py:57] epoch 2439, training loss: 6091.67, average training loss: 6635.27, base loss: 16017.30
[INFO 2017-06-29 21:54:51,154 main.py:57] epoch 2440, training loss: 6186.84, average training loss: 6635.35, base loss: 16017.37
[INFO 2017-06-29 21:54:54,016 main.py:57] epoch 2441, training loss: 6509.02, average training loss: 6635.15, base loss: 16018.87
[INFO 2017-06-29 21:54:56,922 main.py:57] epoch 2442, training loss: 6299.21, average training loss: 6634.30, base loss: 16018.51
[INFO 2017-06-29 21:54:59,832 main.py:57] epoch 2443, training loss: 6934.25, average training loss: 6634.93, base loss: 16018.97
[INFO 2017-06-29 21:55:02,731 main.py:57] epoch 2444, training loss: 6454.08, average training loss: 6634.67, base loss: 16018.75
[INFO 2017-06-29 21:55:05,599 main.py:57] epoch 2445, training loss: 6482.86, average training loss: 6634.78, base loss: 16019.08
[INFO 2017-06-29 21:55:08,545 main.py:57] epoch 2446, training loss: 6606.84, average training loss: 6634.60, base loss: 16019.45
[INFO 2017-06-29 21:55:11,481 main.py:57] epoch 2447, training loss: 6879.33, average training loss: 6634.61, base loss: 16019.89
[INFO 2017-06-29 21:55:14,428 main.py:57] epoch 2448, training loss: 6613.25, average training loss: 6634.71, base loss: 16019.60
[INFO 2017-06-29 21:55:17,254 main.py:57] epoch 2449, training loss: 6585.31, average training loss: 6634.62, base loss: 16018.71
[INFO 2017-06-29 21:55:20,161 main.py:57] epoch 2450, training loss: 6234.92, average training loss: 6633.78, base loss: 16018.09
[INFO 2017-06-29 21:55:23,034 main.py:57] epoch 2451, training loss: 6514.03, average training loss: 6633.57, base loss: 16018.43
[INFO 2017-06-29 21:55:25,973 main.py:57] epoch 2452, training loss: 6289.47, average training loss: 6633.07, base loss: 16018.32
[INFO 2017-06-29 21:55:28,861 main.py:57] epoch 2453, training loss: 6032.71, average training loss: 6632.52, base loss: 16017.94
[INFO 2017-06-29 21:55:31,724 main.py:57] epoch 2454, training loss: 6229.79, average training loss: 6632.46, base loss: 16017.63
[INFO 2017-06-29 21:55:34,580 main.py:57] epoch 2455, training loss: 6318.78, average training loss: 6632.07, base loss: 16017.76
[INFO 2017-06-29 21:55:37,455 main.py:57] epoch 2456, training loss: 6901.58, average training loss: 6632.50, base loss: 16019.43
[INFO 2017-06-29 21:55:40,317 main.py:57] epoch 2457, training loss: 6890.77, average training loss: 6632.82, base loss: 16020.77
[INFO 2017-06-29 21:55:43,174 main.py:57] epoch 2458, training loss: 6670.17, average training loss: 6632.68, base loss: 16021.26
[INFO 2017-06-29 21:55:46,049 main.py:57] epoch 2459, training loss: 6577.38, average training loss: 6631.74, base loss: 16021.99
[INFO 2017-06-29 21:55:48,915 main.py:57] epoch 2460, training loss: 6693.97, average training loss: 6631.94, base loss: 16022.77
[INFO 2017-06-29 21:55:51,760 main.py:57] epoch 2461, training loss: 6107.47, average training loss: 6631.32, base loss: 16022.22
[INFO 2017-06-29 21:55:54,648 main.py:57] epoch 2462, training loss: 6517.92, average training loss: 6630.86, base loss: 16022.34
[INFO 2017-06-29 21:55:57,534 main.py:57] epoch 2463, training loss: 6706.29, average training loss: 6630.68, base loss: 16023.51
[INFO 2017-06-29 21:56:00,389 main.py:57] epoch 2464, training loss: 6729.16, average training loss: 6630.61, base loss: 16023.63
[INFO 2017-06-29 21:56:03,260 main.py:57] epoch 2465, training loss: 6719.87, average training loss: 6631.09, base loss: 16023.81
[INFO 2017-06-29 21:56:06,155 main.py:57] epoch 2466, training loss: 6327.42, average training loss: 6630.59, base loss: 16023.14
[INFO 2017-06-29 21:56:09,008 main.py:57] epoch 2467, training loss: 5801.64, average training loss: 6629.27, base loss: 16021.51
[INFO 2017-06-29 21:56:11,903 main.py:57] epoch 2468, training loss: 6321.74, average training loss: 6628.65, base loss: 16021.07
[INFO 2017-06-29 21:56:14,795 main.py:57] epoch 2469, training loss: 6488.86, average training loss: 6628.58, base loss: 16020.86
[INFO 2017-06-29 21:56:17,703 main.py:57] epoch 2470, training loss: 6454.14, average training loss: 6628.53, base loss: 16020.93
[INFO 2017-06-29 21:56:20,591 main.py:57] epoch 2471, training loss: 6136.64, average training loss: 6627.77, base loss: 16019.98
[INFO 2017-06-29 21:56:23,485 main.py:57] epoch 2472, training loss: 6804.17, average training loss: 6628.30, base loss: 16020.29
[INFO 2017-06-29 21:56:26,379 main.py:57] epoch 2473, training loss: 6213.73, average training loss: 6626.95, base loss: 16019.91
[INFO 2017-06-29 21:56:29,236 main.py:57] epoch 2474, training loss: 6657.00, average training loss: 6626.99, base loss: 16019.93
[INFO 2017-06-29 21:56:32,061 main.py:57] epoch 2475, training loss: 6096.35, average training loss: 6626.89, base loss: 16018.36
[INFO 2017-06-29 21:56:34,914 main.py:57] epoch 2476, training loss: 5907.81, average training loss: 6626.29, base loss: 16017.34
[INFO 2017-06-29 21:56:37,761 main.py:57] epoch 2477, training loss: 6324.37, average training loss: 6625.76, base loss: 16015.86
[INFO 2017-06-29 21:56:40,686 main.py:57] epoch 2478, training loss: 6283.64, average training loss: 6625.59, base loss: 16016.20
[INFO 2017-06-29 21:56:43,527 main.py:57] epoch 2479, training loss: 6795.83, average training loss: 6625.59, base loss: 16016.76
[INFO 2017-06-29 21:56:46,417 main.py:57] epoch 2480, training loss: 5660.64, average training loss: 6624.83, base loss: 16015.10
[INFO 2017-06-29 21:56:49,292 main.py:57] epoch 2481, training loss: 6405.11, average training loss: 6623.83, base loss: 16014.44
[INFO 2017-06-29 21:56:52,176 main.py:57] epoch 2482, training loss: 6252.11, average training loss: 6623.50, base loss: 16014.61
[INFO 2017-06-29 21:56:55,039 main.py:57] epoch 2483, training loss: 6080.74, average training loss: 6623.02, base loss: 16013.91
[INFO 2017-06-29 21:56:57,886 main.py:57] epoch 2484, training loss: 6394.45, average training loss: 6622.31, base loss: 16013.67
[INFO 2017-06-29 21:57:00,748 main.py:57] epoch 2485, training loss: 6715.15, average training loss: 6622.24, base loss: 16014.19
[INFO 2017-06-29 21:57:03,626 main.py:57] epoch 2486, training loss: 6912.87, average training loss: 6622.41, base loss: 16015.23
[INFO 2017-06-29 21:57:06,497 main.py:57] epoch 2487, training loss: 6533.05, average training loss: 6622.25, base loss: 16015.63
[INFO 2017-06-29 21:57:09,409 main.py:57] epoch 2488, training loss: 6635.50, average training loss: 6622.33, base loss: 16014.86
[INFO 2017-06-29 21:57:12,279 main.py:57] epoch 2489, training loss: 6131.54, average training loss: 6620.92, base loss: 16013.61
[INFO 2017-06-29 21:57:15,147 main.py:57] epoch 2490, training loss: 6469.13, average training loss: 6620.36, base loss: 16013.53
[INFO 2017-06-29 21:57:17,998 main.py:57] epoch 2491, training loss: 6383.89, average training loss: 6620.35, base loss: 16013.35
[INFO 2017-06-29 21:57:20,892 main.py:57] epoch 2492, training loss: 6393.18, average training loss: 6619.53, base loss: 16013.72
[INFO 2017-06-29 21:57:23,752 main.py:57] epoch 2493, training loss: 6562.55, average training loss: 6619.01, base loss: 16013.87
[INFO 2017-06-29 21:57:26,630 main.py:57] epoch 2494, training loss: 7140.19, average training loss: 6619.64, base loss: 16016.09
[INFO 2017-06-29 21:57:29,490 main.py:57] epoch 2495, training loss: 6631.27, average training loss: 6619.39, base loss: 16016.38
[INFO 2017-06-29 21:57:32,331 main.py:57] epoch 2496, training loss: 6430.77, average training loss: 6618.86, base loss: 16016.66
[INFO 2017-06-29 21:57:35,174 main.py:57] epoch 2497, training loss: 6023.93, average training loss: 6618.29, base loss: 16016.04
[INFO 2017-06-29 21:57:38,058 main.py:57] epoch 2498, training loss: 6794.93, average training loss: 6618.04, base loss: 16016.94
[INFO 2017-06-29 21:57:40,907 main.py:57] epoch 2499, training loss: 6290.65, average training loss: 6617.78, base loss: 16016.41
[INFO 2017-06-29 21:57:40,907 main.py:59] epoch 2499, testing
[INFO 2017-06-29 21:57:53,358 main.py:104] average testing loss: 6410.46, base loss: 15236.32
[INFO 2017-06-29 21:57:53,359 main.py:105] improve_loss: 8825.86, improve_percent: 0.58
[INFO 2017-06-29 21:57:53,360 main.py:71] current best improved percent: 0.60
[INFO 2017-06-29 21:57:56,231 main.py:57] epoch 2500, training loss: 6677.11, average training loss: 6617.57, base loss: 16016.85
[INFO 2017-06-29 21:57:59,105 main.py:57] epoch 2501, training loss: 5922.25, average training loss: 6616.41, base loss: 16016.27
[INFO 2017-06-29 21:58:01,935 main.py:57] epoch 2502, training loss: 6327.42, average training loss: 6615.94, base loss: 16016.13
[INFO 2017-06-29 21:58:04,808 main.py:57] epoch 2503, training loss: 6622.97, average training loss: 6615.51, base loss: 16016.31
[INFO 2017-06-29 21:58:07,651 main.py:57] epoch 2504, training loss: 7054.39, average training loss: 6615.83, base loss: 16017.89
[INFO 2017-06-29 21:58:10,569 main.py:57] epoch 2505, training loss: 6336.08, average training loss: 6615.49, base loss: 16018.39
[INFO 2017-06-29 21:58:13,458 main.py:57] epoch 2506, training loss: 6764.21, average training loss: 6615.57, base loss: 16018.88
[INFO 2017-06-29 21:58:16,302 main.py:57] epoch 2507, training loss: 6401.74, average training loss: 6615.18, base loss: 16018.45
[INFO 2017-06-29 21:58:19,171 main.py:57] epoch 2508, training loss: 6446.63, average training loss: 6615.09, base loss: 16017.91
[INFO 2017-06-29 21:58:22,046 main.py:57] epoch 2509, training loss: 6229.21, average training loss: 6614.33, base loss: 16017.89
[INFO 2017-06-29 21:58:24,874 main.py:57] epoch 2510, training loss: 5910.33, average training loss: 6613.77, base loss: 16017.28
[INFO 2017-06-29 21:58:27,767 main.py:57] epoch 2511, training loss: 6306.91, average training loss: 6613.65, base loss: 16017.26
[INFO 2017-06-29 21:58:30,618 main.py:57] epoch 2512, training loss: 6435.12, average training loss: 6612.98, base loss: 16017.40
[INFO 2017-06-29 21:58:33,508 main.py:57] epoch 2513, training loss: 6826.10, average training loss: 6612.97, base loss: 16017.73
[INFO 2017-06-29 21:58:36,349 main.py:57] epoch 2514, training loss: 6746.91, average training loss: 6612.94, base loss: 16018.64
[INFO 2017-06-29 21:58:39,223 main.py:57] epoch 2515, training loss: 5992.44, average training loss: 6611.71, base loss: 16018.66
[INFO 2017-06-29 21:58:42,054 main.py:57] epoch 2516, training loss: 6151.31, average training loss: 6611.23, base loss: 16017.71
[INFO 2017-06-29 21:58:44,908 main.py:57] epoch 2517, training loss: 6408.78, average training loss: 6611.38, base loss: 16018.07
[INFO 2017-06-29 21:58:47,769 main.py:57] epoch 2518, training loss: 6615.97, average training loss: 6611.40, base loss: 16018.40
[INFO 2017-06-29 21:58:50,615 main.py:57] epoch 2519, training loss: 5914.15, average training loss: 6610.13, base loss: 16017.01
[INFO 2017-06-29 21:58:53,475 main.py:57] epoch 2520, training loss: 6633.59, average training loss: 6610.27, base loss: 16017.47
[INFO 2017-06-29 21:58:56,347 main.py:57] epoch 2521, training loss: 7348.52, average training loss: 6610.86, base loss: 16018.78
[INFO 2017-06-29 21:58:59,208 main.py:57] epoch 2522, training loss: 6720.81, average training loss: 6610.30, base loss: 16018.89
[INFO 2017-06-29 21:59:02,037 main.py:57] epoch 2523, training loss: 6598.33, average training loss: 6610.10, base loss: 16018.35
[INFO 2017-06-29 21:59:04,967 main.py:57] epoch 2524, training loss: 6642.44, average training loss: 6609.28, base loss: 16018.18
[INFO 2017-06-29 21:59:07,834 main.py:57] epoch 2525, training loss: 6685.78, average training loss: 6608.69, base loss: 16018.41
[INFO 2017-06-29 21:59:10,720 main.py:57] epoch 2526, training loss: 6158.51, average training loss: 6608.18, base loss: 16017.47
[INFO 2017-06-29 21:59:13,633 main.py:57] epoch 2527, training loss: 6686.83, average training loss: 6608.02, base loss: 16017.75
[INFO 2017-06-29 21:59:16,506 main.py:57] epoch 2528, training loss: 6191.46, average training loss: 6608.23, base loss: 16018.14
[INFO 2017-06-29 21:59:19,421 main.py:57] epoch 2529, training loss: 6607.98, average training loss: 6607.92, base loss: 16019.11
[INFO 2017-06-29 21:59:22,296 main.py:57] epoch 2530, training loss: 6388.54, average training loss: 6607.86, base loss: 16019.35
[INFO 2017-06-29 21:59:25,166 main.py:57] epoch 2531, training loss: 6848.91, average training loss: 6607.03, base loss: 16020.86
[INFO 2017-06-29 21:59:28,051 main.py:57] epoch 2532, training loss: 6175.34, average training loss: 6606.02, base loss: 16019.98
[INFO 2017-06-29 21:59:30,909 main.py:57] epoch 2533, training loss: 6503.72, average training loss: 6605.43, base loss: 16020.87
[INFO 2017-06-29 21:59:33,789 main.py:57] epoch 2534, training loss: 6413.86, average training loss: 6604.98, base loss: 16019.96
[INFO 2017-06-29 21:59:36,691 main.py:57] epoch 2535, training loss: 6174.05, average training loss: 6604.35, base loss: 16019.01
[INFO 2017-06-29 21:59:39,601 main.py:57] epoch 2536, training loss: 6364.10, average training loss: 6604.09, base loss: 16018.19
[INFO 2017-06-29 21:59:42,507 main.py:57] epoch 2537, training loss: 6082.25, average training loss: 6603.39, base loss: 16016.97
[INFO 2017-06-29 21:59:45,364 main.py:57] epoch 2538, training loss: 5664.54, average training loss: 6601.92, base loss: 16016.20
[INFO 2017-06-29 21:59:48,182 main.py:57] epoch 2539, training loss: 6103.59, average training loss: 6601.27, base loss: 16015.81
[INFO 2017-06-29 21:59:51,063 main.py:57] epoch 2540, training loss: 6624.79, average training loss: 6601.13, base loss: 16017.04
[INFO 2017-06-29 21:59:53,927 main.py:57] epoch 2541, training loss: 6513.19, average training loss: 6600.44, base loss: 16016.63
[INFO 2017-06-29 21:59:56,795 main.py:57] epoch 2542, training loss: 6339.82, average training loss: 6599.77, base loss: 16016.40
[INFO 2017-06-29 21:59:59,660 main.py:57] epoch 2543, training loss: 6347.69, average training loss: 6599.53, base loss: 16016.27
[INFO 2017-06-29 22:00:02,520 main.py:57] epoch 2544, training loss: 6122.09, average training loss: 6599.26, base loss: 16016.39
[INFO 2017-06-29 22:00:05,384 main.py:57] epoch 2545, training loss: 6410.54, average training loss: 6598.89, base loss: 16016.94
[INFO 2017-06-29 22:00:08,256 main.py:57] epoch 2546, training loss: 6457.76, average training loss: 6598.54, base loss: 16017.96
[INFO 2017-06-29 22:00:11,113 main.py:57] epoch 2547, training loss: 6688.54, average training loss: 6598.27, base loss: 16017.67
[INFO 2017-06-29 22:00:13,949 main.py:57] epoch 2548, training loss: 6461.50, average training loss: 6597.47, base loss: 16017.86
[INFO 2017-06-29 22:00:16,841 main.py:57] epoch 2549, training loss: 7178.91, average training loss: 6598.00, base loss: 16019.78
[INFO 2017-06-29 22:00:19,695 main.py:57] epoch 2550, training loss: 6478.13, average training loss: 6597.24, base loss: 16020.78
[INFO 2017-06-29 22:00:22,524 main.py:57] epoch 2551, training loss: 6506.54, average training loss: 6596.77, base loss: 16022.07
[INFO 2017-06-29 22:00:25,377 main.py:57] epoch 2552, training loss: 6829.31, average training loss: 6596.38, base loss: 16022.16
[INFO 2017-06-29 22:00:28,248 main.py:57] epoch 2553, training loss: 6598.59, average training loss: 6596.28, base loss: 16022.66
[INFO 2017-06-29 22:00:31,073 main.py:57] epoch 2554, training loss: 6241.78, average training loss: 6595.23, base loss: 16022.28
[INFO 2017-06-29 22:00:33,934 main.py:57] epoch 2555, training loss: 6492.50, average training loss: 6595.05, base loss: 16022.22
[INFO 2017-06-29 22:00:36,809 main.py:57] epoch 2556, training loss: 6825.56, average training loss: 6595.44, base loss: 16022.84
[INFO 2017-06-29 22:00:39,681 main.py:57] epoch 2557, training loss: 5852.52, average training loss: 6594.78, base loss: 16021.61
[INFO 2017-06-29 22:00:42,566 main.py:57] epoch 2558, training loss: 6551.81, average training loss: 6594.55, base loss: 16022.51
[INFO 2017-06-29 22:00:45,435 main.py:57] epoch 2559, training loss: 6623.21, average training loss: 6594.64, base loss: 16022.12
[INFO 2017-06-29 22:00:48,256 main.py:57] epoch 2560, training loss: 6726.20, average training loss: 6595.02, base loss: 16022.96
[INFO 2017-06-29 22:00:51,142 main.py:57] epoch 2561, training loss: 6678.80, average training loss: 6594.76, base loss: 16023.03
[INFO 2017-06-29 22:00:54,045 main.py:57] epoch 2562, training loss: 6277.30, average training loss: 6594.14, base loss: 16023.24
[INFO 2017-06-29 22:00:56,947 main.py:57] epoch 2563, training loss: 6458.53, average training loss: 6594.01, base loss: 16022.78
[INFO 2017-06-29 22:00:59,797 main.py:57] epoch 2564, training loss: 6559.07, average training loss: 6593.62, base loss: 16023.07
[INFO 2017-06-29 22:01:02,673 main.py:57] epoch 2565, training loss: 6264.53, average training loss: 6593.55, base loss: 16023.04
[INFO 2017-06-29 22:01:05,564 main.py:57] epoch 2566, training loss: 6292.09, average training loss: 6592.85, base loss: 16023.81
[INFO 2017-06-29 22:01:08,456 main.py:57] epoch 2567, training loss: 6264.85, average training loss: 6592.48, base loss: 16023.89
[INFO 2017-06-29 22:01:11,326 main.py:57] epoch 2568, training loss: 6130.73, average training loss: 6592.09, base loss: 16022.33
[INFO 2017-06-29 22:01:14,257 main.py:57] epoch 2569, training loss: 6102.11, average training loss: 6591.65, base loss: 16021.19
[INFO 2017-06-29 22:01:17,170 main.py:57] epoch 2570, training loss: 6783.91, average training loss: 6592.06, base loss: 16021.89
[INFO 2017-06-29 22:01:20,051 main.py:57] epoch 2571, training loss: 6280.34, average training loss: 6591.11, base loss: 16021.05
[INFO 2017-06-29 22:01:22,936 main.py:57] epoch 2572, training loss: 6673.91, average training loss: 6590.94, base loss: 16021.30
[INFO 2017-06-29 22:01:25,839 main.py:57] epoch 2573, training loss: 6005.87, average training loss: 6589.82, base loss: 16020.21
[INFO 2017-06-29 22:01:28,710 main.py:57] epoch 2574, training loss: 6463.69, average training loss: 6589.16, base loss: 16019.48
[INFO 2017-06-29 22:01:31,607 main.py:57] epoch 2575, training loss: 6179.94, average training loss: 6588.01, base loss: 16018.95
[INFO 2017-06-29 22:01:34,470 main.py:57] epoch 2576, training loss: 6682.92, average training loss: 6587.77, base loss: 16018.41
[INFO 2017-06-29 22:01:37,360 main.py:57] epoch 2577, training loss: 6341.41, average training loss: 6587.45, base loss: 16018.08
[INFO 2017-06-29 22:01:40,239 main.py:57] epoch 2578, training loss: 6604.07, average training loss: 6587.37, base loss: 16017.85
[INFO 2017-06-29 22:01:43,165 main.py:57] epoch 2579, training loss: 6628.36, average training loss: 6587.10, base loss: 16018.33
[INFO 2017-06-29 22:01:46,042 main.py:57] epoch 2580, training loss: 6861.14, average training loss: 6587.13, base loss: 16019.38
[INFO 2017-06-29 22:01:48,915 main.py:57] epoch 2581, training loss: 6372.14, average training loss: 6587.02, base loss: 16018.91
[INFO 2017-06-29 22:01:51,763 main.py:57] epoch 2582, training loss: 6534.07, average training loss: 6586.86, base loss: 16019.44
[INFO 2017-06-29 22:01:54,637 main.py:57] epoch 2583, training loss: 6108.87, average training loss: 6585.98, base loss: 16018.39
[INFO 2017-06-29 22:01:57,572 main.py:57] epoch 2584, training loss: 6324.53, average training loss: 6585.96, base loss: 16018.18
[INFO 2017-06-29 22:02:00,481 main.py:57] epoch 2585, training loss: 6264.45, average training loss: 6586.02, base loss: 16017.89
[INFO 2017-06-29 22:02:03,364 main.py:57] epoch 2586, training loss: 6362.50, average training loss: 6585.85, base loss: 16017.96
[INFO 2017-06-29 22:02:06,288 main.py:57] epoch 2587, training loss: 6967.75, average training loss: 6586.10, base loss: 16019.68
[INFO 2017-06-29 22:02:09,154 main.py:57] epoch 2588, training loss: 6952.46, average training loss: 6586.16, base loss: 16020.52
[INFO 2017-06-29 22:02:11,998 main.py:57] epoch 2589, training loss: 6079.48, average training loss: 6584.87, base loss: 16019.56
[INFO 2017-06-29 22:02:14,869 main.py:57] epoch 2590, training loss: 6237.68, average training loss: 6584.09, base loss: 16018.23
[INFO 2017-06-29 22:02:17,747 main.py:57] epoch 2591, training loss: 6491.84, average training loss: 6584.15, base loss: 16017.63
[INFO 2017-06-29 22:02:20,712 main.py:57] epoch 2592, training loss: 6913.16, average training loss: 6584.00, base loss: 16018.73
[INFO 2017-06-29 22:02:23,568 main.py:57] epoch 2593, training loss: 6482.47, average training loss: 6583.24, base loss: 16019.44
[INFO 2017-06-29 22:02:26,454 main.py:57] epoch 2594, training loss: 6997.59, average training loss: 6582.97, base loss: 16021.04
[INFO 2017-06-29 22:02:29,350 main.py:57] epoch 2595, training loss: 5999.71, average training loss: 6581.92, base loss: 16020.06
[INFO 2017-06-29 22:02:32,230 main.py:57] epoch 2596, training loss: 5863.24, average training loss: 6581.31, base loss: 16018.61
[INFO 2017-06-29 22:02:35,085 main.py:57] epoch 2597, training loss: 6653.79, average training loss: 6581.06, base loss: 16018.56
[INFO 2017-06-29 22:02:37,940 main.py:57] epoch 2598, training loss: 6348.92, average training loss: 6580.61, base loss: 16018.18
[INFO 2017-06-29 22:02:40,850 main.py:57] epoch 2599, training loss: 6666.73, average training loss: 6580.52, base loss: 16018.89
[INFO 2017-06-29 22:02:40,850 main.py:59] epoch 2599, testing
[INFO 2017-06-29 22:02:53,180 main.py:104] average testing loss: 6556.83, base loss: 16344.97
[INFO 2017-06-29 22:02:53,180 main.py:105] improve_loss: 9788.14, improve_percent: 0.60
[INFO 2017-06-29 22:02:53,181 main.py:71] current best improved percent: 0.60
[INFO 2017-06-29 22:02:56,011 main.py:57] epoch 2600, training loss: 6306.73, average training loss: 6579.80, base loss: 16019.08
[INFO 2017-06-29 22:02:58,891 main.py:57] epoch 2601, training loss: 6453.31, average training loss: 6579.27, base loss: 16019.18
[INFO 2017-06-29 22:03:01,767 main.py:57] epoch 2602, training loss: 6410.83, average training loss: 6578.13, base loss: 16018.95
[INFO 2017-06-29 22:03:04,606 main.py:57] epoch 2603, training loss: 6372.55, average training loss: 6577.83, base loss: 16018.96
[INFO 2017-06-29 22:03:07,459 main.py:57] epoch 2604, training loss: 6044.20, average training loss: 6577.13, base loss: 16018.01
[INFO 2017-06-29 22:03:10,287 main.py:57] epoch 2605, training loss: 6265.25, average training loss: 6576.76, base loss: 16017.92
[INFO 2017-06-29 22:03:13,171 main.py:57] epoch 2606, training loss: 6212.41, average training loss: 6576.64, base loss: 16016.07
[INFO 2017-06-29 22:03:16,025 main.py:57] epoch 2607, training loss: 6280.22, average training loss: 6576.14, base loss: 16014.72
[INFO 2017-06-29 22:03:18,878 main.py:57] epoch 2608, training loss: 5933.33, average training loss: 6575.67, base loss: 16013.96
[INFO 2017-06-29 22:03:21,774 main.py:57] epoch 2609, training loss: 6121.81, average training loss: 6574.78, base loss: 16013.23
[INFO 2017-06-29 22:03:24,629 main.py:57] epoch 2610, training loss: 6637.37, average training loss: 6574.76, base loss: 16013.47
[INFO 2017-06-29 22:03:27,574 main.py:57] epoch 2611, training loss: 6827.38, average training loss: 6575.33, base loss: 16014.66
[INFO 2017-06-29 22:03:30,502 main.py:57] epoch 2612, training loss: 6509.62, average training loss: 6574.93, base loss: 16013.87
[INFO 2017-06-29 22:03:33,372 main.py:57] epoch 2613, training loss: 6397.37, average training loss: 6575.06, base loss: 16013.13
[INFO 2017-06-29 22:03:36,208 main.py:57] epoch 2614, training loss: 6119.23, average training loss: 6574.35, base loss: 16012.33
[INFO 2017-06-29 22:03:39,091 main.py:57] epoch 2615, training loss: 6749.01, average training loss: 6574.63, base loss: 16013.32
[INFO 2017-06-29 22:03:41,925 main.py:57] epoch 2616, training loss: 6636.34, average training loss: 6575.00, base loss: 16012.80
[INFO 2017-06-29 22:03:44,784 main.py:57] epoch 2617, training loss: 5890.72, average training loss: 6574.13, base loss: 16011.08
[INFO 2017-06-29 22:03:47,658 main.py:57] epoch 2618, training loss: 6439.19, average training loss: 6574.28, base loss: 16011.69
[INFO 2017-06-29 22:03:50,559 main.py:57] epoch 2619, training loss: 6351.79, average training loss: 6574.20, base loss: 16011.97
[INFO 2017-06-29 22:03:53,441 main.py:57] epoch 2620, training loss: 5685.04, average training loss: 6572.66, base loss: 16009.72
[INFO 2017-06-29 22:03:56,335 main.py:57] epoch 2621, training loss: 6245.83, average training loss: 6572.59, base loss: 16008.95
[INFO 2017-06-29 22:03:59,223 main.py:57] epoch 2622, training loss: 6268.15, average training loss: 6572.55, base loss: 16009.54
[INFO 2017-06-29 22:04:02,073 main.py:57] epoch 2623, training loss: 6451.77, average training loss: 6572.40, base loss: 16010.17
[INFO 2017-06-29 22:04:04,906 main.py:57] epoch 2624, training loss: 6314.15, average training loss: 6572.06, base loss: 16010.68
[INFO 2017-06-29 22:04:07,818 main.py:57] epoch 2625, training loss: 6153.54, average training loss: 6571.86, base loss: 16010.29
[INFO 2017-06-29 22:04:10,710 main.py:57] epoch 2626, training loss: 6025.95, average training loss: 6571.19, base loss: 16009.91
[INFO 2017-06-29 22:04:13,573 main.py:57] epoch 2627, training loss: 6117.48, average training loss: 6570.43, base loss: 16009.51
[INFO 2017-06-29 22:04:16,505 main.py:57] epoch 2628, training loss: 6865.55, average training loss: 6570.99, base loss: 16010.89
[INFO 2017-06-29 22:04:19,350 main.py:57] epoch 2629, training loss: 6447.66, average training loss: 6571.31, base loss: 16011.58
[INFO 2017-06-29 22:04:22,238 main.py:57] epoch 2630, training loss: 6630.25, average training loss: 6570.95, base loss: 16011.50
[INFO 2017-06-29 22:04:25,143 main.py:57] epoch 2631, training loss: 6091.69, average training loss: 6570.67, base loss: 16010.65
[INFO 2017-06-29 22:04:28,021 main.py:57] epoch 2632, training loss: 7056.48, average training loss: 6571.13, base loss: 16011.50
[INFO 2017-06-29 22:04:30,869 main.py:57] epoch 2633, training loss: 6545.14, average training loss: 6571.08, base loss: 16011.41
[INFO 2017-06-29 22:04:33,763 main.py:57] epoch 2634, training loss: 6400.74, average training loss: 6571.14, base loss: 16010.67
[INFO 2017-06-29 22:04:36,636 main.py:57] epoch 2635, training loss: 6428.14, average training loss: 6571.01, base loss: 16010.78
[INFO 2017-06-29 22:04:39,519 main.py:57] epoch 2636, training loss: 6473.77, average training loss: 6570.92, base loss: 16011.78
[INFO 2017-06-29 22:04:42,390 main.py:57] epoch 2637, training loss: 6507.43, average training loss: 6570.38, base loss: 16012.53
[INFO 2017-06-29 22:04:45,263 main.py:57] epoch 2638, training loss: 6167.51, average training loss: 6570.01, base loss: 16012.61
[INFO 2017-06-29 22:04:48,116 main.py:57] epoch 2639, training loss: 6552.69, average training loss: 6570.29, base loss: 16013.65
[INFO 2017-06-29 22:04:51,009 main.py:57] epoch 2640, training loss: 7026.71, average training loss: 6569.85, base loss: 16014.85
[INFO 2017-06-29 22:04:53,902 main.py:57] epoch 2641, training loss: 6635.92, average training loss: 6569.64, base loss: 16016.05
[INFO 2017-06-29 22:04:56,783 main.py:57] epoch 2642, training loss: 6631.15, average training loss: 6570.09, base loss: 16016.29
[INFO 2017-06-29 22:04:59,646 main.py:57] epoch 2643, training loss: 6659.80, average training loss: 6569.85, base loss: 16015.30
[INFO 2017-06-29 22:05:02,515 main.py:57] epoch 2644, training loss: 6071.47, average training loss: 6569.73, base loss: 16014.17
[INFO 2017-06-29 22:05:05,427 main.py:57] epoch 2645, training loss: 6288.77, average training loss: 6569.40, base loss: 16012.97
[INFO 2017-06-29 22:05:08,310 main.py:57] epoch 2646, training loss: 6014.08, average training loss: 6568.95, base loss: 16011.39
[INFO 2017-06-29 22:05:11,162 main.py:57] epoch 2647, training loss: 6770.61, average training loss: 6569.75, base loss: 16011.66
[INFO 2017-06-29 22:05:14,016 main.py:57] epoch 2648, training loss: 7114.62, average training loss: 6570.47, base loss: 16013.30
[INFO 2017-06-29 22:05:16,882 main.py:57] epoch 2649, training loss: 6590.54, average training loss: 6569.64, base loss: 16014.24
[INFO 2017-06-29 22:05:19,775 main.py:57] epoch 2650, training loss: 6991.35, average training loss: 6569.98, base loss: 16014.19
[INFO 2017-06-29 22:05:22,636 main.py:57] epoch 2651, training loss: 6847.84, average training loss: 6569.68, base loss: 16014.21
[INFO 2017-06-29 22:05:25,483 main.py:57] epoch 2652, training loss: 6464.79, average training loss: 6569.58, base loss: 16015.27
[INFO 2017-06-29 22:05:28,378 main.py:57] epoch 2653, training loss: 6074.34, average training loss: 6568.79, base loss: 16013.94
[INFO 2017-06-29 22:05:31,230 main.py:57] epoch 2654, training loss: 6261.09, average training loss: 6568.66, base loss: 16013.05
[INFO 2017-06-29 22:05:34,110 main.py:57] epoch 2655, training loss: 6559.72, average training loss: 6568.77, base loss: 16013.29
[INFO 2017-06-29 22:05:36,986 main.py:57] epoch 2656, training loss: 5962.57, average training loss: 6568.04, base loss: 16012.41
[INFO 2017-06-29 22:05:39,851 main.py:57] epoch 2657, training loss: 6504.57, average training loss: 6568.31, base loss: 16012.89
[INFO 2017-06-29 22:05:42,737 main.py:57] epoch 2658, training loss: 6261.05, average training loss: 6567.74, base loss: 16012.25
[INFO 2017-06-29 22:05:45,595 main.py:57] epoch 2659, training loss: 6951.57, average training loss: 6568.00, base loss: 16012.84
[INFO 2017-06-29 22:05:48,492 main.py:57] epoch 2660, training loss: 6415.47, average training loss: 6567.79, base loss: 16013.30
[INFO 2017-06-29 22:05:51,365 main.py:57] epoch 2661, training loss: 6584.27, average training loss: 6568.09, base loss: 16013.15
[INFO 2017-06-29 22:05:54,269 main.py:57] epoch 2662, training loss: 6387.83, average training loss: 6567.02, base loss: 16012.65
[INFO 2017-06-29 22:05:57,183 main.py:57] epoch 2663, training loss: 6861.43, average training loss: 6567.16, base loss: 16013.90
[INFO 2017-06-29 22:06:00,045 main.py:57] epoch 2664, training loss: 6357.94, average training loss: 6566.85, base loss: 16013.13
[INFO 2017-06-29 22:06:02,908 main.py:57] epoch 2665, training loss: 6572.06, average training loss: 6566.87, base loss: 16012.63
[INFO 2017-06-29 22:06:05,804 main.py:57] epoch 2666, training loss: 6605.69, average training loss: 6566.87, base loss: 16012.36
[INFO 2017-06-29 22:06:08,708 main.py:57] epoch 2667, training loss: 5988.04, average training loss: 6565.75, base loss: 16011.53
[INFO 2017-06-29 22:06:11,617 main.py:57] epoch 2668, training loss: 6259.37, average training loss: 6565.13, base loss: 16010.88
[INFO 2017-06-29 22:06:14,533 main.py:57] epoch 2669, training loss: 6837.28, average training loss: 6564.85, base loss: 16012.12
[INFO 2017-06-29 22:06:17,394 main.py:57] epoch 2670, training loss: 6461.30, average training loss: 6564.64, base loss: 16011.07
[INFO 2017-06-29 22:06:20,309 main.py:57] epoch 2671, training loss: 6382.23, average training loss: 6563.77, base loss: 16010.39
[INFO 2017-06-29 22:06:23,174 main.py:57] epoch 2672, training loss: 6438.64, average training loss: 6563.97, base loss: 16011.75
[INFO 2017-06-29 22:06:26,012 main.py:57] epoch 2673, training loss: 6471.19, average training loss: 6563.42, base loss: 16012.69
[INFO 2017-06-29 22:06:28,900 main.py:57] epoch 2674, training loss: 6990.69, average training loss: 6563.85, base loss: 16013.80
[INFO 2017-06-29 22:06:31,791 main.py:57] epoch 2675, training loss: 6781.77, average training loss: 6564.71, base loss: 16014.68
[INFO 2017-06-29 22:06:34,660 main.py:57] epoch 2676, training loss: 6544.08, average training loss: 6564.64, base loss: 16014.56
[INFO 2017-06-29 22:06:37,532 main.py:57] epoch 2677, training loss: 6501.58, average training loss: 6564.07, base loss: 16014.02
[INFO 2017-06-29 22:06:40,385 main.py:57] epoch 2678, training loss: 6671.37, average training loss: 6563.78, base loss: 16015.10
[INFO 2017-06-29 22:06:43,251 main.py:57] epoch 2679, training loss: 6583.02, average training loss: 6562.78, base loss: 16016.29
[INFO 2017-06-29 22:06:46,176 main.py:57] epoch 2680, training loss: 6788.43, average training loss: 6563.12, base loss: 16016.59
[INFO 2017-06-29 22:06:49,067 main.py:57] epoch 2681, training loss: 7169.95, average training loss: 6563.44, base loss: 16017.62
[INFO 2017-06-29 22:06:51,950 main.py:57] epoch 2682, training loss: 6693.72, average training loss: 6562.97, base loss: 16017.03
[INFO 2017-06-29 22:06:54,815 main.py:57] epoch 2683, training loss: 6685.24, average training loss: 6563.69, base loss: 16016.93
[INFO 2017-06-29 22:06:57,648 main.py:57] epoch 2684, training loss: 6406.43, average training loss: 6563.99, base loss: 16016.79
[INFO 2017-06-29 22:07:00,484 main.py:57] epoch 2685, training loss: 6574.90, average training loss: 6564.28, base loss: 16017.08
[INFO 2017-06-29 22:07:03,339 main.py:57] epoch 2686, training loss: 6115.31, average training loss: 6563.63, base loss: 16015.19
[INFO 2017-06-29 22:07:06,213 main.py:57] epoch 2687, training loss: 6120.69, average training loss: 6563.33, base loss: 16014.38
[INFO 2017-06-29 22:07:09,076 main.py:57] epoch 2688, training loss: 6775.34, average training loss: 6562.94, base loss: 16015.39
[INFO 2017-06-29 22:07:11,947 main.py:57] epoch 2689, training loss: 6178.27, average training loss: 6562.67, base loss: 16015.12
[INFO 2017-06-29 22:07:14,767 main.py:57] epoch 2690, training loss: 6755.60, average training loss: 6562.21, base loss: 16015.98
[INFO 2017-06-29 22:07:17,665 main.py:57] epoch 2691, training loss: 6352.92, average training loss: 6561.82, base loss: 16016.93
[INFO 2017-06-29 22:07:20,545 main.py:57] epoch 2692, training loss: 6541.22, average training loss: 6561.27, base loss: 16017.23
[INFO 2017-06-29 22:07:23,423 main.py:57] epoch 2693, training loss: 6214.00, average training loss: 6561.40, base loss: 16017.24
[INFO 2017-06-29 22:07:26,322 main.py:57] epoch 2694, training loss: 6717.19, average training loss: 6562.10, base loss: 16016.93
[INFO 2017-06-29 22:07:29,199 main.py:57] epoch 2695, training loss: 6851.78, average training loss: 6561.95, base loss: 16017.19
[INFO 2017-06-29 22:07:32,072 main.py:57] epoch 2696, training loss: 6118.85, average training loss: 6560.56, base loss: 16016.54
[INFO 2017-06-29 22:07:34,994 main.py:57] epoch 2697, training loss: 6342.01, average training loss: 6560.18, base loss: 16016.52
[INFO 2017-06-29 22:07:37,843 main.py:57] epoch 2698, training loss: 6676.34, average training loss: 6559.94, base loss: 16016.35
[INFO 2017-06-29 22:07:40,691 main.py:57] epoch 2699, training loss: 7162.89, average training loss: 6560.04, base loss: 16016.63
[INFO 2017-06-29 22:07:40,692 main.py:59] epoch 2699, testing
[INFO 2017-06-29 22:07:52,913 main.py:104] average testing loss: 6582.12, base loss: 16264.72
[INFO 2017-06-29 22:07:52,913 main.py:105] improve_loss: 9682.60, improve_percent: 0.60
[INFO 2017-06-29 22:07:52,914 main.py:71] current best improved percent: 0.60
[INFO 2017-06-29 22:07:55,747 main.py:57] epoch 2700, training loss: 7309.23, average training loss: 6560.35, base loss: 16017.43
[INFO 2017-06-29 22:07:58,593 main.py:57] epoch 2701, training loss: 6219.56, average training loss: 6559.74, base loss: 16016.84
[INFO 2017-06-29 22:08:01,479 main.py:57] epoch 2702, training loss: 6729.30, average training loss: 6559.65, base loss: 16017.81
[INFO 2017-06-29 22:08:04,377 main.py:57] epoch 2703, training loss: 6557.87, average training loss: 6560.13, base loss: 16018.51
[INFO 2017-06-29 22:08:07,262 main.py:57] epoch 2704, training loss: 6575.29, average training loss: 6559.57, base loss: 16017.95
[INFO 2017-06-29 22:08:10,137 main.py:57] epoch 2705, training loss: 6436.78, average training loss: 6559.22, base loss: 16017.42
[INFO 2017-06-29 22:08:13,006 main.py:57] epoch 2706, training loss: 6168.24, average training loss: 6558.83, base loss: 16017.10
[INFO 2017-06-29 22:08:15,886 main.py:57] epoch 2707, training loss: 6888.24, average training loss: 6559.12, base loss: 16017.72
[INFO 2017-06-29 22:08:18,780 main.py:57] epoch 2708, training loss: 6161.01, average training loss: 6558.44, base loss: 16016.27
[INFO 2017-06-29 22:08:21,619 main.py:57] epoch 2709, training loss: 7073.58, average training loss: 6559.36, base loss: 16016.82
[INFO 2017-06-29 22:08:24,496 main.py:57] epoch 2710, training loss: 6179.80, average training loss: 6558.62, base loss: 16015.69
[INFO 2017-06-29 22:08:27,375 main.py:57] epoch 2711, training loss: 6454.16, average training loss: 6558.54, base loss: 16015.15
[INFO 2017-06-29 22:08:30,261 main.py:57] epoch 2712, training loss: 6395.45, average training loss: 6558.36, base loss: 16015.12
[INFO 2017-06-29 22:08:33,116 main.py:57] epoch 2713, training loss: 6362.68, average training loss: 6558.26, base loss: 16015.35
[INFO 2017-06-29 22:08:35,986 main.py:57] epoch 2714, training loss: 6000.88, average training loss: 6557.21, base loss: 16013.56
[INFO 2017-06-29 22:08:38,870 main.py:57] epoch 2715, training loss: 6403.48, average training loss: 6557.39, base loss: 16013.27
[INFO 2017-06-29 22:08:41,716 main.py:57] epoch 2716, training loss: 5914.15, average training loss: 6556.68, base loss: 16013.16
[INFO 2017-06-29 22:08:44,610 main.py:57] epoch 2717, training loss: 6731.24, average training loss: 6556.79, base loss: 16013.76
[INFO 2017-06-29 22:08:47,473 main.py:57] epoch 2718, training loss: 6125.21, average training loss: 6556.49, base loss: 16013.04
[INFO 2017-06-29 22:08:50,325 main.py:57] epoch 2719, training loss: 6033.44, average training loss: 6556.02, base loss: 16012.99
[INFO 2017-06-29 22:08:53,192 main.py:57] epoch 2720, training loss: 6685.37, average training loss: 6555.74, base loss: 16013.93
[INFO 2017-06-29 22:08:56,045 main.py:57] epoch 2721, training loss: 5944.00, average training loss: 6554.77, base loss: 16012.30
[INFO 2017-06-29 22:08:58,884 main.py:57] epoch 2722, training loss: 6721.90, average training loss: 6555.44, base loss: 16012.71
[INFO 2017-06-29 22:09:01,732 main.py:57] epoch 2723, training loss: 6842.62, average training loss: 6555.77, base loss: 16012.35
[INFO 2017-06-29 22:09:04,598 main.py:57] epoch 2724, training loss: 6422.39, average training loss: 6555.83, base loss: 16011.97
[INFO 2017-06-29 22:09:07,445 main.py:57] epoch 2725, training loss: 6290.51, average training loss: 6555.15, base loss: 16011.12
[INFO 2017-06-29 22:09:10,331 main.py:57] epoch 2726, training loss: 6172.26, average training loss: 6554.33, base loss: 16010.60
[INFO 2017-06-29 22:09:13,209 main.py:57] epoch 2727, training loss: 6751.58, average training loss: 6554.85, base loss: 16011.49
[INFO 2017-06-29 22:09:16,130 main.py:57] epoch 2728, training loss: 6227.43, average training loss: 6554.74, base loss: 16012.04
[INFO 2017-06-29 22:09:18,987 main.py:57] epoch 2729, training loss: 6015.73, average training loss: 6554.18, base loss: 16011.71
[INFO 2017-06-29 22:09:21,824 main.py:57] epoch 2730, training loss: 6379.28, average training loss: 6554.02, base loss: 16012.09
[INFO 2017-06-29 22:09:24,716 main.py:57] epoch 2731, training loss: 6338.94, average training loss: 6554.03, base loss: 16011.55
[INFO 2017-06-29 22:09:27,577 main.py:57] epoch 2732, training loss: 6471.93, average training loss: 6554.25, base loss: 16010.89
[INFO 2017-06-29 22:09:30,493 main.py:57] epoch 2733, training loss: 6623.43, average training loss: 6554.49, base loss: 16011.42
[INFO 2017-06-29 22:09:33,363 main.py:57] epoch 2734, training loss: 6176.44, average training loss: 6554.24, base loss: 16012.06
[INFO 2017-06-29 22:09:36,241 main.py:57] epoch 2735, training loss: 6243.35, average training loss: 6553.90, base loss: 16012.15
[INFO 2017-06-29 22:09:39,067 main.py:57] epoch 2736, training loss: 6472.10, average training loss: 6553.89, base loss: 16012.47
[INFO 2017-06-29 22:09:41,934 main.py:57] epoch 2737, training loss: 6521.84, average training loss: 6553.75, base loss: 16012.73
[INFO 2017-06-29 22:09:44,802 main.py:57] epoch 2738, training loss: 6134.71, average training loss: 6552.76, base loss: 16011.74
[INFO 2017-06-29 22:09:47,683 main.py:57] epoch 2739, training loss: 6631.90, average training loss: 6552.72, base loss: 16012.43
[INFO 2017-06-29 22:09:50,551 main.py:57] epoch 2740, training loss: 5960.22, average training loss: 6551.84, base loss: 16011.53
[INFO 2017-06-29 22:09:53,423 main.py:57] epoch 2741, training loss: 6812.62, average training loss: 6551.90, base loss: 16011.81
[INFO 2017-06-29 22:09:56,298 main.py:57] epoch 2742, training loss: 6480.62, average training loss: 6552.26, base loss: 16011.81
[INFO 2017-06-29 22:09:59,177 main.py:57] epoch 2743, training loss: 6817.52, average training loss: 6551.93, base loss: 16011.68
[INFO 2017-06-29 22:10:02,062 main.py:57] epoch 2744, training loss: 5956.87, average training loss: 6550.63, base loss: 16011.23
[INFO 2017-06-29 22:10:05,009 main.py:57] epoch 2745, training loss: 6779.94, average training loss: 6550.66, base loss: 16011.59
[INFO 2017-06-29 22:10:07,883 main.py:57] epoch 2746, training loss: 6693.47, average training loss: 6550.93, base loss: 16011.81
[INFO 2017-06-29 22:10:10,797 main.py:57] epoch 2747, training loss: 6125.92, average training loss: 6550.26, base loss: 16010.21
[INFO 2017-06-29 22:10:13,669 main.py:57] epoch 2748, training loss: 6648.10, average training loss: 6550.96, base loss: 16011.04
[INFO 2017-06-29 22:10:16,541 main.py:57] epoch 2749, training loss: 6280.93, average training loss: 6550.44, base loss: 16009.93
[INFO 2017-06-29 22:10:19,386 main.py:57] epoch 2750, training loss: 5828.29, average training loss: 6549.53, base loss: 16009.29
[INFO 2017-06-29 22:10:22,222 main.py:57] epoch 2751, training loss: 6429.06, average training loss: 6549.48, base loss: 16009.59
[INFO 2017-06-29 22:10:25,073 main.py:57] epoch 2752, training loss: 6509.35, average training loss: 6549.64, base loss: 16009.79
[INFO 2017-06-29 22:10:27,972 main.py:57] epoch 2753, training loss: 6506.44, average training loss: 6548.79, base loss: 16009.31
[INFO 2017-06-29 22:10:30,851 main.py:57] epoch 2754, training loss: 7080.38, average training loss: 6548.68, base loss: 16010.44
[INFO 2017-06-29 22:10:33,700 main.py:57] epoch 2755, training loss: 6705.66, average training loss: 6548.58, base loss: 16011.63
[INFO 2017-06-29 22:10:36,624 main.py:57] epoch 2756, training loss: 6532.05, average training loss: 6549.03, base loss: 16011.81
[INFO 2017-06-29 22:10:39,494 main.py:57] epoch 2757, training loss: 6672.60, average training loss: 6549.36, base loss: 16012.77
[INFO 2017-06-29 22:10:42,390 main.py:57] epoch 2758, training loss: 6873.31, average training loss: 6549.64, base loss: 16013.88
[INFO 2017-06-29 22:10:45,252 main.py:57] epoch 2759, training loss: 6862.45, average training loss: 6549.53, base loss: 16014.44
[INFO 2017-06-29 22:10:48,114 main.py:57] epoch 2760, training loss: 6100.03, average training loss: 6549.13, base loss: 16014.34
[INFO 2017-06-29 22:10:51,001 main.py:57] epoch 2761, training loss: 6128.25, average training loss: 6548.35, base loss: 16013.76
[INFO 2017-06-29 22:10:53,881 main.py:57] epoch 2762, training loss: 6366.09, average training loss: 6547.86, base loss: 16013.60
[INFO 2017-06-29 22:10:56,749 main.py:57] epoch 2763, training loss: 6012.61, average training loss: 6547.81, base loss: 16012.38
[INFO 2017-06-29 22:10:59,630 main.py:57] epoch 2764, training loss: 5896.87, average training loss: 6547.49, base loss: 16011.17
[INFO 2017-06-29 22:11:02,564 main.py:57] epoch 2765, training loss: 6551.10, average training loss: 6547.60, base loss: 16011.24
[INFO 2017-06-29 22:11:05,449 main.py:57] epoch 2766, training loss: 6082.65, average training loss: 6547.09, base loss: 16010.64
[INFO 2017-06-29 22:11:08,317 main.py:57] epoch 2767, training loss: 7179.74, average training loss: 6547.72, base loss: 16011.95
[INFO 2017-06-29 22:11:11,183 main.py:57] epoch 2768, training loss: 7358.69, average training loss: 6548.23, base loss: 16013.22
[INFO 2017-06-29 22:11:14,068 main.py:57] epoch 2769, training loss: 6169.24, average training loss: 6547.23, base loss: 16012.73
[INFO 2017-06-29 22:11:16,911 main.py:57] epoch 2770, training loss: 6431.88, average training loss: 6547.37, base loss: 16011.72
[INFO 2017-06-29 22:11:19,757 main.py:57] epoch 2771, training loss: 5944.23, average training loss: 6546.53, base loss: 16011.04
[INFO 2017-06-29 22:11:22,616 main.py:57] epoch 2772, training loss: 6581.02, average training loss: 6546.70, base loss: 16011.07
[INFO 2017-06-29 22:11:25,485 main.py:57] epoch 2773, training loss: 6709.30, average training loss: 6546.35, base loss: 16011.65
[INFO 2017-06-29 22:11:28,353 main.py:57] epoch 2774, training loss: 6103.83, average training loss: 6546.29, base loss: 16011.30
[INFO 2017-06-29 22:11:31,245 main.py:57] epoch 2775, training loss: 5957.55, average training loss: 6545.53, base loss: 16010.53
[INFO 2017-06-29 22:11:34,114 main.py:57] epoch 2776, training loss: 6761.92, average training loss: 6545.73, base loss: 16010.77
[INFO 2017-06-29 22:11:36,968 main.py:57] epoch 2777, training loss: 6133.91, average training loss: 6545.26, base loss: 16011.36
[INFO 2017-06-29 22:11:39,847 main.py:57] epoch 2778, training loss: 6451.42, average training loss: 6544.86, base loss: 16011.80
[INFO 2017-06-29 22:11:42,707 main.py:57] epoch 2779, training loss: 6402.61, average training loss: 6544.55, base loss: 16012.43
[INFO 2017-06-29 22:11:45,594 main.py:57] epoch 2780, training loss: 6022.72, average training loss: 6543.83, base loss: 16011.02
[INFO 2017-06-29 22:11:48,443 main.py:57] epoch 2781, training loss: 6335.05, average training loss: 6542.98, base loss: 16011.11
[INFO 2017-06-29 22:11:51,278 main.py:57] epoch 2782, training loss: 6243.00, average training loss: 6542.56, base loss: 16010.83
[INFO 2017-06-29 22:11:54,143 main.py:57] epoch 2783, training loss: 6828.93, average training loss: 6542.37, base loss: 16011.84
[INFO 2017-06-29 22:11:56,981 main.py:57] epoch 2784, training loss: 6379.99, average training loss: 6542.21, base loss: 16011.89
[INFO 2017-06-29 22:11:59,897 main.py:57] epoch 2785, training loss: 6331.14, average training loss: 6541.57, base loss: 16011.21
[INFO 2017-06-29 22:12:02,780 main.py:57] epoch 2786, training loss: 6035.18, average training loss: 6540.80, base loss: 16010.62
[INFO 2017-06-29 22:12:05,684 main.py:57] epoch 2787, training loss: 6487.01, average training loss: 6541.11, base loss: 16011.01
[INFO 2017-06-29 22:12:08,516 main.py:57] epoch 2788, training loss: 6453.27, average training loss: 6540.90, base loss: 16011.67
[INFO 2017-06-29 22:12:11,358 main.py:57] epoch 2789, training loss: 6973.87, average training loss: 6541.28, base loss: 16013.43
[INFO 2017-06-29 22:12:14,243 main.py:57] epoch 2790, training loss: 6704.31, average training loss: 6541.49, base loss: 16014.10
[INFO 2017-06-29 22:12:17,109 main.py:57] epoch 2791, training loss: 6685.02, average training loss: 6540.90, base loss: 16014.88
[INFO 2017-06-29 22:12:19,994 main.py:57] epoch 2792, training loss: 6809.88, average training loss: 6540.92, base loss: 16015.59
[INFO 2017-06-29 22:12:22,875 main.py:57] epoch 2793, training loss: 6065.21, average training loss: 6540.91, base loss: 16014.16
[INFO 2017-06-29 22:12:25,753 main.py:57] epoch 2794, training loss: 6100.18, average training loss: 6540.21, base loss: 16013.61
[INFO 2017-06-29 22:12:28,626 main.py:57] epoch 2795, training loss: 6571.18, average training loss: 6539.96, base loss: 16014.54
[INFO 2017-06-29 22:12:31,480 main.py:57] epoch 2796, training loss: 5986.82, average training loss: 6538.26, base loss: 16013.84
[INFO 2017-06-29 22:12:34,364 main.py:57] epoch 2797, training loss: 6425.16, average training loss: 6537.58, base loss: 16014.64
[INFO 2017-06-29 22:12:37,243 main.py:57] epoch 2798, training loss: 6482.32, average training loss: 6537.86, base loss: 16015.11
[INFO 2017-06-29 22:12:40,148 main.py:57] epoch 2799, training loss: 6424.16, average training loss: 6537.58, base loss: 16015.19
[INFO 2017-06-29 22:12:40,148 main.py:59] epoch 2799, testing
[INFO 2017-06-29 22:12:52,474 main.py:104] average testing loss: 6268.65, base loss: 15688.95
[INFO 2017-06-29 22:12:52,474 main.py:105] improve_loss: 9420.31, improve_percent: 0.60
[INFO 2017-06-29 22:12:52,476 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 22:12:52,501 main.py:71] current best improved percent: 0.60
[INFO 2017-06-29 22:12:55,393 main.py:57] epoch 2800, training loss: 6223.34, average training loss: 6537.64, base loss: 16015.86
[INFO 2017-06-29 22:12:58,252 main.py:57] epoch 2801, training loss: 7158.24, average training loss: 6537.83, base loss: 16017.39
[INFO 2017-06-29 22:13:01,151 main.py:57] epoch 2802, training loss: 6875.49, average training loss: 6538.33, base loss: 16018.14
[INFO 2017-06-29 22:13:04,000 main.py:57] epoch 2803, training loss: 6868.39, average training loss: 6538.29, base loss: 16019.38
[INFO 2017-06-29 22:13:06,842 main.py:57] epoch 2804, training loss: 6380.14, average training loss: 6538.25, base loss: 16019.80
[INFO 2017-06-29 22:13:09,688 main.py:57] epoch 2805, training loss: 6614.56, average training loss: 6538.09, base loss: 16020.82
[INFO 2017-06-29 22:13:12,523 main.py:57] epoch 2806, training loss: 6897.31, average training loss: 6538.39, base loss: 16021.34
[INFO 2017-06-29 22:13:15,461 main.py:57] epoch 2807, training loss: 6747.68, average training loss: 6538.33, base loss: 16021.87
[INFO 2017-06-29 22:13:18,338 main.py:57] epoch 2808, training loss: 6714.33, average training loss: 6538.69, base loss: 16022.48
[INFO 2017-06-29 22:13:21,200 main.py:57] epoch 2809, training loss: 6976.92, average training loss: 6538.87, base loss: 16023.37
[INFO 2017-06-29 22:13:24,052 main.py:57] epoch 2810, training loss: 6220.57, average training loss: 6538.47, base loss: 16021.25
[INFO 2017-06-29 22:13:26,922 main.py:57] epoch 2811, training loss: 6454.76, average training loss: 6538.06, base loss: 16020.71
[INFO 2017-06-29 22:13:29,755 main.py:57] epoch 2812, training loss: 6290.85, average training loss: 6537.71, base loss: 16020.28
[INFO 2017-06-29 22:13:32,666 main.py:57] epoch 2813, training loss: 6591.65, average training loss: 6537.82, base loss: 16020.02
[INFO 2017-06-29 22:13:35,564 main.py:57] epoch 2814, training loss: 6514.29, average training loss: 6537.92, base loss: 16020.62
[INFO 2017-06-29 22:13:38,507 main.py:57] epoch 2815, training loss: 6326.09, average training loss: 6537.50, base loss: 16020.35
[INFO 2017-06-29 22:13:41,369 main.py:57] epoch 2816, training loss: 6260.64, average training loss: 6537.17, base loss: 16020.79
[INFO 2017-06-29 22:13:44,221 main.py:57] epoch 2817, training loss: 6520.31, average training loss: 6536.85, base loss: 16021.12
[INFO 2017-06-29 22:13:47,080 main.py:57] epoch 2818, training loss: 6171.49, average training loss: 6536.02, base loss: 16020.46
[INFO 2017-06-29 22:13:49,939 main.py:57] epoch 2819, training loss: 7024.50, average training loss: 6536.03, base loss: 16021.41
[INFO 2017-06-29 22:13:52,811 main.py:57] epoch 2820, training loss: 6160.20, average training loss: 6535.54, base loss: 16020.95
[INFO 2017-06-29 22:13:55,699 main.py:57] epoch 2821, training loss: 6855.21, average training loss: 6535.51, base loss: 16020.96
[INFO 2017-06-29 22:13:58,572 main.py:57] epoch 2822, training loss: 6593.40, average training loss: 6535.88, base loss: 16021.78
[INFO 2017-06-29 22:14:01,435 main.py:57] epoch 2823, training loss: 6205.17, average training loss: 6535.97, base loss: 16021.63
[INFO 2017-06-29 22:14:04,355 main.py:57] epoch 2824, training loss: 6692.97, average training loss: 6535.54, base loss: 16022.42
[INFO 2017-06-29 22:14:07,242 main.py:57] epoch 2825, training loss: 6351.99, average training loss: 6535.57, base loss: 16022.19
[INFO 2017-06-29 22:14:10,093 main.py:57] epoch 2826, training loss: 6890.46, average training loss: 6535.34, base loss: 16022.26
[INFO 2017-06-29 22:14:12,974 main.py:57] epoch 2827, training loss: 6286.77, average training loss: 6534.78, base loss: 16021.85
[INFO 2017-06-29 22:14:15,838 main.py:57] epoch 2828, training loss: 5838.24, average training loss: 6533.64, base loss: 16021.37
[INFO 2017-06-29 22:14:18,789 main.py:57] epoch 2829, training loss: 6694.43, average training loss: 6533.95, base loss: 16022.40
[INFO 2017-06-29 22:14:21,644 main.py:57] epoch 2830, training loss: 6281.17, average training loss: 6533.38, base loss: 16022.28
[INFO 2017-06-29 22:14:24,530 main.py:57] epoch 2831, training loss: 6401.74, average training loss: 6533.19, base loss: 16022.19
[INFO 2017-06-29 22:14:27,390 main.py:57] epoch 2832, training loss: 6571.97, average training loss: 6533.98, base loss: 16023.04
[INFO 2017-06-29 22:14:30,251 main.py:57] epoch 2833, training loss: 6373.63, average training loss: 6534.08, base loss: 16022.96
[INFO 2017-06-29 22:14:33,112 main.py:57] epoch 2834, training loss: 6414.39, average training loss: 6534.04, base loss: 16023.66
[INFO 2017-06-29 22:14:36,031 main.py:57] epoch 2835, training loss: 6125.16, average training loss: 6533.47, base loss: 16023.52
[INFO 2017-06-29 22:14:38,925 main.py:57] epoch 2836, training loss: 6733.99, average training loss: 6533.98, base loss: 16024.60
[INFO 2017-06-29 22:14:41,795 main.py:57] epoch 2837, training loss: 6678.65, average training loss: 6533.72, base loss: 16025.49
[INFO 2017-06-29 22:14:44,647 main.py:57] epoch 2838, training loss: 6541.17, average training loss: 6533.56, base loss: 16025.86
[INFO 2017-06-29 22:14:47,519 main.py:57] epoch 2839, training loss: 6568.96, average training loss: 6533.16, base loss: 16027.16
[INFO 2017-06-29 22:14:50,378 main.py:57] epoch 2840, training loss: 6014.11, average training loss: 6531.84, base loss: 16026.68
[INFO 2017-06-29 22:14:53,243 main.py:57] epoch 2841, training loss: 6694.71, average training loss: 6531.86, base loss: 16026.75
[INFO 2017-06-29 22:14:56,149 main.py:57] epoch 2842, training loss: 6156.50, average training loss: 6531.49, base loss: 16026.56
[INFO 2017-06-29 22:14:59,020 main.py:57] epoch 2843, training loss: 6494.83, average training loss: 6531.62, base loss: 16026.78
[INFO 2017-06-29 22:15:01,908 main.py:57] epoch 2844, training loss: 6321.82, average training loss: 6531.86, base loss: 16026.67
[INFO 2017-06-29 22:15:04,752 main.py:57] epoch 2845, training loss: 6614.71, average training loss: 6531.37, base loss: 16027.28
[INFO 2017-06-29 22:15:07,637 main.py:57] epoch 2846, training loss: 6704.89, average training loss: 6531.48, base loss: 16027.97
[INFO 2017-06-29 22:15:10,544 main.py:57] epoch 2847, training loss: 5920.58, average training loss: 6530.84, base loss: 16027.37
[INFO 2017-06-29 22:15:13,374 main.py:57] epoch 2848, training loss: 6618.83, average training loss: 6530.34, base loss: 16028.86
[INFO 2017-06-29 22:15:16,250 main.py:57] epoch 2849, training loss: 6626.78, average training loss: 6529.40, base loss: 16029.41
[INFO 2017-06-29 22:15:19,149 main.py:57] epoch 2850, training loss: 6905.15, average training loss: 6529.51, base loss: 16029.40
[INFO 2017-06-29 22:15:22,018 main.py:57] epoch 2851, training loss: 6404.82, average training loss: 6528.93, base loss: 16028.83
[INFO 2017-06-29 22:15:24,898 main.py:57] epoch 2852, training loss: 6117.38, average training loss: 6528.66, base loss: 16028.67
[INFO 2017-06-29 22:15:27,769 main.py:57] epoch 2853, training loss: 6477.99, average training loss: 6528.07, base loss: 16029.59
[INFO 2017-06-29 22:15:30,647 main.py:57] epoch 2854, training loss: 6234.28, average training loss: 6527.17, base loss: 16029.35
[INFO 2017-06-29 22:15:33,529 main.py:57] epoch 2855, training loss: 6466.41, average training loss: 6526.49, base loss: 16029.25
[INFO 2017-06-29 22:15:36,426 main.py:57] epoch 2856, training loss: 6756.63, average training loss: 6526.52, base loss: 16029.23
[INFO 2017-06-29 22:15:39,300 main.py:57] epoch 2857, training loss: 6131.17, average training loss: 6525.83, base loss: 16028.67
[INFO 2017-06-29 22:15:42,139 main.py:57] epoch 2858, training loss: 6621.15, average training loss: 6525.98, base loss: 16029.70
[INFO 2017-06-29 22:15:45,050 main.py:57] epoch 2859, training loss: 5948.37, average training loss: 6525.57, base loss: 16029.94
[INFO 2017-06-29 22:15:47,881 main.py:57] epoch 2860, training loss: 6027.94, average training loss: 6524.59, base loss: 16028.15
[INFO 2017-06-29 22:15:50,771 main.py:57] epoch 2861, training loss: 6242.42, average training loss: 6524.39, base loss: 16027.41
[INFO 2017-06-29 22:15:53,638 main.py:57] epoch 2862, training loss: 6004.50, average training loss: 6523.88, base loss: 16026.38
[INFO 2017-06-29 22:15:56,511 main.py:57] epoch 2863, training loss: 6442.11, average training loss: 6523.88, base loss: 16026.24
[INFO 2017-06-29 22:15:59,396 main.py:57] epoch 2864, training loss: 6504.22, average training loss: 6523.50, base loss: 16026.15
[INFO 2017-06-29 22:16:02,272 main.py:57] epoch 2865, training loss: 6487.28, average training loss: 6523.54, base loss: 16026.30
[INFO 2017-06-29 22:16:05,139 main.py:57] epoch 2866, training loss: 6238.93, average training loss: 6523.29, base loss: 16025.97
[INFO 2017-06-29 22:16:08,007 main.py:57] epoch 2867, training loss: 6956.04, average training loss: 6523.85, base loss: 16027.46
[INFO 2017-06-29 22:16:10,850 main.py:57] epoch 2868, training loss: 6340.75, average training loss: 6523.58, base loss: 16027.96
[INFO 2017-06-29 22:16:13,765 main.py:57] epoch 2869, training loss: 6657.55, average training loss: 6523.56, base loss: 16028.89
[INFO 2017-06-29 22:16:16,639 main.py:57] epoch 2870, training loss: 6549.39, average training loss: 6523.81, base loss: 16029.21
[INFO 2017-06-29 22:16:19,522 main.py:57] epoch 2871, training loss: 6331.18, average training loss: 6523.62, base loss: 16029.21
[INFO 2017-06-29 22:16:22,365 main.py:57] epoch 2872, training loss: 6460.91, average training loss: 6522.82, base loss: 16029.74
[INFO 2017-06-29 22:16:25,265 main.py:57] epoch 2873, training loss: 5870.87, average training loss: 6522.31, base loss: 16028.54
[INFO 2017-06-29 22:16:28,145 main.py:57] epoch 2874, training loss: 6710.67, average training loss: 6522.15, base loss: 16028.95
[INFO 2017-06-29 22:16:30,997 main.py:57] epoch 2875, training loss: 6360.03, average training loss: 6521.56, base loss: 16027.45
[INFO 2017-06-29 22:16:33,891 main.py:57] epoch 2876, training loss: 6737.09, average training loss: 6521.68, base loss: 16028.03
[INFO 2017-06-29 22:16:36,758 main.py:57] epoch 2877, training loss: 6278.27, average training loss: 6521.19, base loss: 16028.39
[INFO 2017-06-29 22:16:39,615 main.py:57] epoch 2878, training loss: 5810.40, average training loss: 6520.73, base loss: 16027.38
[INFO 2017-06-29 22:16:42,434 main.py:57] epoch 2879, training loss: 6445.36, average training loss: 6520.95, base loss: 16028.08
[INFO 2017-06-29 22:16:45,338 main.py:57] epoch 2880, training loss: 6424.58, average training loss: 6520.65, base loss: 16027.85
[INFO 2017-06-29 22:16:48,181 main.py:57] epoch 2881, training loss: 6291.06, average training loss: 6519.21, base loss: 16027.87
[INFO 2017-06-29 22:16:51,014 main.py:57] epoch 2882, training loss: 5856.36, average training loss: 6518.56, base loss: 16026.85
[INFO 2017-06-29 22:16:53,896 main.py:57] epoch 2883, training loss: 6277.60, average training loss: 6518.33, base loss: 16026.31
[INFO 2017-06-29 22:16:56,757 main.py:57] epoch 2884, training loss: 6504.17, average training loss: 6518.14, base loss: 16026.32
[INFO 2017-06-29 22:16:59,642 main.py:57] epoch 2885, training loss: 6656.13, average training loss: 6518.24, base loss: 16026.42
[INFO 2017-06-29 22:17:02,511 main.py:57] epoch 2886, training loss: 6110.69, average training loss: 6517.27, base loss: 16026.49
[INFO 2017-06-29 22:17:05,401 main.py:57] epoch 2887, training loss: 6080.68, average training loss: 6516.29, base loss: 16025.01
[INFO 2017-06-29 22:17:08,288 main.py:57] epoch 2888, training loss: 6251.56, average training loss: 6516.44, base loss: 16025.23
[INFO 2017-06-29 22:17:11,153 main.py:57] epoch 2889, training loss: 6128.21, average training loss: 6515.24, base loss: 16025.21
[INFO 2017-06-29 22:17:14,014 main.py:57] epoch 2890, training loss: 6561.18, average training loss: 6515.17, base loss: 16024.43
[INFO 2017-06-29 22:17:16,880 main.py:57] epoch 2891, training loss: 6895.07, average training loss: 6515.68, base loss: 16025.54
[INFO 2017-06-29 22:17:19,816 main.py:57] epoch 2892, training loss: 6372.04, average training loss: 6515.77, base loss: 16025.32
[INFO 2017-06-29 22:17:22,670 main.py:57] epoch 2893, training loss: 6355.32, average training loss: 6515.29, base loss: 16026.05
[INFO 2017-06-29 22:17:25,565 main.py:57] epoch 2894, training loss: 6327.61, average training loss: 6514.75, base loss: 16026.05
[INFO 2017-06-29 22:17:28,478 main.py:57] epoch 2895, training loss: 6625.98, average training loss: 6514.68, base loss: 16026.18
[INFO 2017-06-29 22:17:31,353 main.py:57] epoch 2896, training loss: 6389.90, average training loss: 6514.54, base loss: 16026.21
[INFO 2017-06-29 22:17:34,193 main.py:57] epoch 2897, training loss: 6350.68, average training loss: 6513.99, base loss: 16025.83
[INFO 2017-06-29 22:17:37,067 main.py:57] epoch 2898, training loss: 6501.98, average training loss: 6514.06, base loss: 16025.63
[INFO 2017-06-29 22:17:39,928 main.py:57] epoch 2899, training loss: 6473.86, average training loss: 6514.04, base loss: 16026.10
[INFO 2017-06-29 22:17:39,928 main.py:59] epoch 2899, testing
[INFO 2017-06-29 22:17:52,146 main.py:104] average testing loss: 6272.34, base loss: 15721.26
[INFO 2017-06-29 22:17:52,146 main.py:105] improve_loss: 9448.92, improve_percent: 0.60
[INFO 2017-06-29 22:17:52,148 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 22:17:52,173 main.py:71] current best improved percent: 0.60
[INFO 2017-06-29 22:17:55,033 main.py:57] epoch 2900, training loss: 6061.27, average training loss: 6513.80, base loss: 16026.00
[INFO 2017-06-29 22:17:57,896 main.py:57] epoch 2901, training loss: 6755.02, average training loss: 6514.45, base loss: 16026.92
[INFO 2017-06-29 22:18:00,770 main.py:57] epoch 2902, training loss: 6076.81, average training loss: 6514.00, base loss: 16025.98
[INFO 2017-06-29 22:18:03,649 main.py:57] epoch 2903, training loss: 6190.82, average training loss: 6513.95, base loss: 16025.59
[INFO 2017-06-29 22:18:06,552 main.py:57] epoch 2904, training loss: 6923.94, average training loss: 6514.17, base loss: 16027.13
[INFO 2017-06-29 22:18:09,425 main.py:57] epoch 2905, training loss: 6408.86, average training loss: 6514.19, base loss: 16027.72
[INFO 2017-06-29 22:18:12,304 main.py:57] epoch 2906, training loss: 5839.87, average training loss: 6513.67, base loss: 16027.02
[INFO 2017-06-29 22:18:15,182 main.py:57] epoch 2907, training loss: 6974.45, average training loss: 6513.99, base loss: 16028.08
[INFO 2017-06-29 22:18:18,063 main.py:57] epoch 2908, training loss: 7028.79, average training loss: 6513.93, base loss: 16029.46
[INFO 2017-06-29 22:18:20,987 main.py:57] epoch 2909, training loss: 6053.26, average training loss: 6513.64, base loss: 16028.48
[INFO 2017-06-29 22:18:23,839 main.py:57] epoch 2910, training loss: 6965.45, average training loss: 6514.08, base loss: 16028.50
[INFO 2017-06-29 22:18:26,701 main.py:57] epoch 2911, training loss: 6253.25, average training loss: 6513.52, base loss: 16028.39
[INFO 2017-06-29 22:18:29,575 main.py:57] epoch 2912, training loss: 6592.79, average training loss: 6513.62, base loss: 16029.21
[INFO 2017-06-29 22:18:32,474 main.py:57] epoch 2913, training loss: 6178.52, average training loss: 6513.04, base loss: 16029.51
[INFO 2017-06-29 22:18:35,340 main.py:57] epoch 2914, training loss: 6366.04, average training loss: 6512.35, base loss: 16029.66
[INFO 2017-06-29 22:18:38,192 main.py:57] epoch 2915, training loss: 6473.92, average training loss: 6512.44, base loss: 16029.19
[INFO 2017-06-29 22:18:41,077 main.py:57] epoch 2916, training loss: 6271.18, average training loss: 6512.46, base loss: 16029.72
[INFO 2017-06-29 22:18:43,970 main.py:57] epoch 2917, training loss: 6391.93, average training loss: 6512.55, base loss: 16030.36
[INFO 2017-06-29 22:18:46,842 main.py:57] epoch 2918, training loss: 6292.23, average training loss: 6511.61, base loss: 16030.78
[INFO 2017-06-29 22:18:49,745 main.py:57] epoch 2919, training loss: 6471.35, average training loss: 6511.21, base loss: 16030.37
[INFO 2017-06-29 22:18:52,601 main.py:57] epoch 2920, training loss: 6318.45, average training loss: 6511.16, base loss: 16029.81
[INFO 2017-06-29 22:18:55,477 main.py:57] epoch 2921, training loss: 5894.77, average training loss: 6510.19, base loss: 16028.34
[INFO 2017-06-29 22:18:58,374 main.py:57] epoch 2922, training loss: 6498.63, average training loss: 6509.97, base loss: 16027.95
[INFO 2017-06-29 22:19:01,232 main.py:57] epoch 2923, training loss: 6611.28, average training loss: 6510.17, base loss: 16029.04
[INFO 2017-06-29 22:19:04,095 main.py:57] epoch 2924, training loss: 6377.89, average training loss: 6510.50, base loss: 16029.10
[INFO 2017-06-29 22:19:06,982 main.py:57] epoch 2925, training loss: 6246.30, average training loss: 6509.43, base loss: 16029.45
[INFO 2017-06-29 22:19:09,915 main.py:57] epoch 2926, training loss: 6448.84, average training loss: 6508.94, base loss: 16030.62
[INFO 2017-06-29 22:19:12,819 main.py:57] epoch 2927, training loss: 6171.55, average training loss: 6508.37, base loss: 16030.64
[INFO 2017-06-29 22:19:15,706 main.py:57] epoch 2928, training loss: 6014.60, average training loss: 6508.14, base loss: 16030.50
[INFO 2017-06-29 22:19:18,578 main.py:57] epoch 2929, training loss: 6163.52, average training loss: 6507.69, base loss: 16030.20
[INFO 2017-06-29 22:19:21,499 main.py:57] epoch 2930, training loss: 5997.77, average training loss: 6507.00, base loss: 16029.57
[INFO 2017-06-29 22:19:24,354 main.py:57] epoch 2931, training loss: 5969.90, average training loss: 6505.96, base loss: 16029.17
[INFO 2017-06-29 22:19:27,218 main.py:57] epoch 2932, training loss: 6300.37, average training loss: 6505.95, base loss: 16029.33
[INFO 2017-06-29 22:19:30,109 main.py:57] epoch 2933, training loss: 6391.40, average training loss: 6506.14, base loss: 16029.20
[INFO 2017-06-29 22:19:33,010 main.py:57] epoch 2934, training loss: 6436.31, average training loss: 6505.83, base loss: 16029.37
[INFO 2017-06-29 22:19:35,877 main.py:57] epoch 2935, training loss: 6529.41, average training loss: 6505.75, base loss: 16029.88
[INFO 2017-06-29 22:19:38,753 main.py:57] epoch 2936, training loss: 6001.99, average training loss: 6505.09, base loss: 16029.14
[INFO 2017-06-29 22:19:41,592 main.py:57] epoch 2937, training loss: 6774.02, average training loss: 6505.18, base loss: 16030.45
[INFO 2017-06-29 22:19:44,421 main.py:57] epoch 2938, training loss: 6498.29, average training loss: 6505.50, base loss: 16029.72
[INFO 2017-06-29 22:19:47,283 main.py:57] epoch 2939, training loss: 6023.18, average training loss: 6505.08, base loss: 16028.18
[INFO 2017-06-29 22:19:50,191 main.py:57] epoch 2940, training loss: 5876.23, average training loss: 6504.19, base loss: 16027.66
[INFO 2017-06-29 22:19:53,079 main.py:57] epoch 2941, training loss: 6168.70, average training loss: 6503.48, base loss: 16026.96
[INFO 2017-06-29 22:19:55,939 main.py:57] epoch 2942, training loss: 6859.96, average training loss: 6503.22, base loss: 16027.48
[INFO 2017-06-29 22:19:58,851 main.py:57] epoch 2943, training loss: 6357.92, average training loss: 6502.57, base loss: 16027.58
[INFO 2017-06-29 22:20:01,772 main.py:57] epoch 2944, training loss: 6590.81, average training loss: 6502.85, base loss: 16027.99
[INFO 2017-06-29 22:20:04,627 main.py:57] epoch 2945, training loss: 6490.30, average training loss: 6502.61, base loss: 16028.76
[INFO 2017-06-29 22:20:07,503 main.py:57] epoch 2946, training loss: 6394.57, average training loss: 6502.55, base loss: 16028.06
[INFO 2017-06-29 22:20:10,389 main.py:57] epoch 2947, training loss: 6462.64, average training loss: 6501.98, base loss: 16026.98
[INFO 2017-06-29 22:20:13,254 main.py:57] epoch 2948, training loss: 6629.67, average training loss: 6502.28, base loss: 16027.15
[INFO 2017-06-29 22:20:16,137 main.py:57] epoch 2949, training loss: 6157.13, average training loss: 6501.41, base loss: 16027.17
[INFO 2017-06-29 22:20:19,021 main.py:57] epoch 2950, training loss: 6082.94, average training loss: 6500.99, base loss: 16027.84
[INFO 2017-06-29 22:20:21,904 main.py:57] epoch 2951, training loss: 7592.21, average training loss: 6502.03, base loss: 16030.15
[INFO 2017-06-29 22:20:24,756 main.py:57] epoch 2952, training loss: 6262.99, average training loss: 6501.16, base loss: 16030.09
[INFO 2017-06-29 22:20:27,605 main.py:57] epoch 2953, training loss: 5893.83, average training loss: 6500.51, base loss: 16029.15
[INFO 2017-06-29 22:20:30,511 main.py:57] epoch 2954, training loss: 6596.17, average training loss: 6500.92, base loss: 16028.95
[INFO 2017-06-29 22:20:33,418 main.py:57] epoch 2955, training loss: 5973.35, average training loss: 6499.68, base loss: 16028.15
[INFO 2017-06-29 22:20:36,277 main.py:57] epoch 2956, training loss: 6428.29, average training loss: 6499.39, base loss: 16027.64
[INFO 2017-06-29 22:20:39,132 main.py:57] epoch 2957, training loss: 6394.17, average training loss: 6498.24, base loss: 16027.13
[INFO 2017-06-29 22:20:42,013 main.py:57] epoch 2958, training loss: 6716.38, average training loss: 6498.33, base loss: 16027.72
[INFO 2017-06-29 22:20:44,871 main.py:57] epoch 2959, training loss: 6329.94, average training loss: 6498.67, base loss: 16027.50
[INFO 2017-06-29 22:20:47,791 main.py:57] epoch 2960, training loss: 6158.23, average training loss: 6498.92, base loss: 16026.93
[INFO 2017-06-29 22:20:50,673 main.py:57] epoch 2961, training loss: 5985.14, average training loss: 6497.86, base loss: 16025.85
[INFO 2017-06-29 22:20:53,531 main.py:57] epoch 2962, training loss: 6296.81, average training loss: 6497.47, base loss: 16026.18
[INFO 2017-06-29 22:20:56,421 main.py:57] epoch 2963, training loss: 6796.53, average training loss: 6497.57, base loss: 16027.02
[INFO 2017-06-29 22:20:59,281 main.py:57] epoch 2964, training loss: 6317.37, average training loss: 6497.34, base loss: 16026.51
[INFO 2017-06-29 22:21:02,148 main.py:57] epoch 2965, training loss: 6188.53, average training loss: 6496.64, base loss: 16025.58
[INFO 2017-06-29 22:21:05,006 main.py:57] epoch 2966, training loss: 6573.87, average training loss: 6496.72, base loss: 16025.33
[INFO 2017-06-29 22:21:07,887 main.py:57] epoch 2967, training loss: 6386.20, average training loss: 6495.75, base loss: 16025.34
[INFO 2017-06-29 22:21:10,741 main.py:57] epoch 2968, training loss: 6490.06, average training loss: 6496.05, base loss: 16026.14
[INFO 2017-06-29 22:21:13,629 main.py:57] epoch 2969, training loss: 6631.08, average training loss: 6496.51, base loss: 16027.01
[INFO 2017-06-29 22:21:16,547 main.py:57] epoch 2970, training loss: 6075.53, average training loss: 6496.30, base loss: 16026.04
[INFO 2017-06-29 22:21:19,434 main.py:57] epoch 2971, training loss: 6351.22, average training loss: 6496.04, base loss: 16026.07
[INFO 2017-06-29 22:21:22,328 main.py:57] epoch 2972, training loss: 6264.26, average training loss: 6494.88, base loss: 16025.74
[INFO 2017-06-29 22:21:25,238 main.py:57] epoch 2973, training loss: 6676.33, average training loss: 6494.96, base loss: 16026.56
[INFO 2017-06-29 22:21:28,119 main.py:57] epoch 2974, training loss: 6506.87, average training loss: 6494.78, base loss: 16026.35
[INFO 2017-06-29 22:21:30,933 main.py:57] epoch 2975, training loss: 6170.66, average training loss: 6494.34, base loss: 16025.21
[INFO 2017-06-29 22:21:33,786 main.py:57] epoch 2976, training loss: 5954.03, average training loss: 6493.62, base loss: 16023.91
[INFO 2017-06-29 22:21:36,619 main.py:57] epoch 2977, training loss: 6120.85, average training loss: 6493.02, base loss: 16023.30
[INFO 2017-06-29 22:21:39,504 main.py:57] epoch 2978, training loss: 6045.81, average training loss: 6492.23, base loss: 16023.27
[INFO 2017-06-29 22:21:42,416 main.py:57] epoch 2979, training loss: 6275.94, average training loss: 6491.81, base loss: 16023.04
[INFO 2017-06-29 22:21:45,285 main.py:57] epoch 2980, training loss: 6812.54, average training loss: 6492.49, base loss: 16024.74
[INFO 2017-06-29 22:21:48,167 main.py:57] epoch 2981, training loss: 6655.47, average training loss: 6493.03, base loss: 16025.82
[INFO 2017-06-29 22:21:51,031 main.py:57] epoch 2982, training loss: 6289.84, average training loss: 6492.95, base loss: 16025.58
[INFO 2017-06-29 22:21:53,920 main.py:57] epoch 2983, training loss: 6577.17, average training loss: 6493.20, base loss: 16026.27
[INFO 2017-06-29 22:21:56,793 main.py:57] epoch 2984, training loss: 6670.87, average training loss: 6493.36, base loss: 16026.58
[INFO 2017-06-29 22:21:59,680 main.py:57] epoch 2985, training loss: 6574.98, average training loss: 6493.75, base loss: 16025.87
[INFO 2017-06-29 22:22:02,585 main.py:57] epoch 2986, training loss: 6503.15, average training loss: 6493.18, base loss: 16025.46
[INFO 2017-06-29 22:22:05,506 main.py:57] epoch 2987, training loss: 6108.50, average training loss: 6492.54, base loss: 16023.93
[INFO 2017-06-29 22:22:08,351 main.py:57] epoch 2988, training loss: 6012.49, average training loss: 6491.92, base loss: 16023.67
[INFO 2017-06-29 22:22:11,253 main.py:57] epoch 2989, training loss: 6330.72, average training loss: 6491.86, base loss: 16023.92
[INFO 2017-06-29 22:22:14,098 main.py:57] epoch 2990, training loss: 6208.74, average training loss: 6491.90, base loss: 16023.65
[INFO 2017-06-29 22:22:17,022 main.py:57] epoch 2991, training loss: 6666.51, average training loss: 6492.32, base loss: 16024.59
[INFO 2017-06-29 22:22:19,878 main.py:57] epoch 2992, training loss: 6546.94, average training loss: 6491.31, base loss: 16024.37
[INFO 2017-06-29 22:22:22,753 main.py:57] epoch 2993, training loss: 6202.62, average training loss: 6491.03, base loss: 16023.30
[INFO 2017-06-29 22:22:25,621 main.py:57] epoch 2994, training loss: 5830.36, average training loss: 6489.89, base loss: 16022.12
[INFO 2017-06-29 22:22:28,500 main.py:57] epoch 2995, training loss: 6751.22, average training loss: 6490.29, base loss: 16022.57
[INFO 2017-06-29 22:22:31,377 main.py:57] epoch 2996, training loss: 5913.42, average training loss: 6489.36, base loss: 16021.79
[INFO 2017-06-29 22:22:34,295 main.py:57] epoch 2997, training loss: 6661.53, average training loss: 6489.29, base loss: 16021.39
[INFO 2017-06-29 22:22:37,165 main.py:57] epoch 2998, training loss: 5877.48, average training loss: 6488.98, base loss: 16021.35
[INFO 2017-06-29 22:22:40,065 main.py:57] epoch 2999, training loss: 6150.64, average training loss: 6488.30, base loss: 16022.09
[INFO 2017-06-29 22:22:40,066 main.py:59] epoch 2999, testing
[INFO 2017-06-29 22:22:52,533 main.py:104] average testing loss: 6238.98, base loss: 15646.19
[INFO 2017-06-29 22:22:52,534 main.py:105] improve_loss: 9407.22, improve_percent: 0.60
[INFO 2017-06-29 22:22:52,535 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 22:22:52,560 main.py:71] current best improved percent: 0.60
[INFO 2017-06-29 22:22:55,421 main.py:57] epoch 3000, training loss: 6504.50, average training loss: 6488.12, base loss: 16023.08
[INFO 2017-06-29 22:22:58,270 main.py:57] epoch 3001, training loss: 6138.75, average training loss: 6487.65, base loss: 16022.67
[INFO 2017-06-29 22:23:01,092 main.py:57] epoch 3002, training loss: 6651.13, average training loss: 6487.41, base loss: 16024.04
[INFO 2017-06-29 22:23:03,952 main.py:57] epoch 3003, training loss: 6532.72, average training loss: 6487.81, base loss: 16024.90
[INFO 2017-06-29 22:23:06,783 main.py:57] epoch 3004, training loss: 6417.81, average training loss: 6487.00, base loss: 16024.78
[INFO 2017-06-29 22:23:09,640 main.py:57] epoch 3005, training loss: 6561.83, average training loss: 6486.36, base loss: 16024.48
[INFO 2017-06-29 22:23:12,510 main.py:57] epoch 3006, training loss: 6191.64, average training loss: 6485.25, base loss: 16024.04
[INFO 2017-06-29 22:23:15,400 main.py:57] epoch 3007, training loss: 6715.05, average training loss: 6485.47, base loss: 16024.32
[INFO 2017-06-29 22:23:18,265 main.py:57] epoch 3008, training loss: 5779.77, average training loss: 6484.55, base loss: 16022.87
[INFO 2017-06-29 22:23:21,128 main.py:57] epoch 3009, training loss: 6250.76, average training loss: 6484.50, base loss: 16022.29
[INFO 2017-06-29 22:23:24,003 main.py:57] epoch 3010, training loss: 5884.81, average training loss: 6483.90, base loss: 16022.05
[INFO 2017-06-29 22:23:26,857 main.py:57] epoch 3011, training loss: 6239.83, average training loss: 6483.80, base loss: 16022.04
[INFO 2017-06-29 22:23:29,716 main.py:57] epoch 3012, training loss: 6250.50, average training loss: 6482.72, base loss: 16021.67
[INFO 2017-06-29 22:23:32,547 main.py:57] epoch 3013, training loss: 6190.07, average training loss: 6481.79, base loss: 16021.78
[INFO 2017-06-29 22:23:35,368 main.py:57] epoch 3014, training loss: 7141.94, average training loss: 6482.31, base loss: 16023.57
[INFO 2017-06-29 22:23:38,244 main.py:57] epoch 3015, training loss: 6044.53, average training loss: 6481.65, base loss: 16023.26
[INFO 2017-06-29 22:23:41,126 main.py:57] epoch 3016, training loss: 6363.38, average training loss: 6481.46, base loss: 16023.01
[INFO 2017-06-29 22:23:43,948 main.py:57] epoch 3017, training loss: 6468.10, average training loss: 6481.28, base loss: 16023.28
[INFO 2017-06-29 22:23:46,820 main.py:57] epoch 3018, training loss: 6453.84, average training loss: 6481.02, base loss: 16024.46
[INFO 2017-06-29 22:23:49,701 main.py:57] epoch 3019, training loss: 6319.18, average training loss: 6480.61, base loss: 16025.22
[INFO 2017-06-29 22:23:52,551 main.py:57] epoch 3020, training loss: 6422.36, average training loss: 6480.57, base loss: 16025.41
[INFO 2017-06-29 22:23:55,455 main.py:57] epoch 3021, training loss: 6109.89, average training loss: 6480.16, base loss: 16024.92
[INFO 2017-06-29 22:23:58,314 main.py:57] epoch 3022, training loss: 5886.07, average training loss: 6479.35, base loss: 16024.00
[INFO 2017-06-29 22:24:01,174 main.py:57] epoch 3023, training loss: 6151.95, average training loss: 6479.24, base loss: 16023.56
[INFO 2017-06-29 22:24:04,067 main.py:57] epoch 3024, training loss: 6318.84, average training loss: 6479.07, base loss: 16024.15
[INFO 2017-06-29 22:24:06,986 main.py:57] epoch 3025, training loss: 6475.34, average training loss: 6478.79, base loss: 16025.09
[INFO 2017-06-29 22:24:09,925 main.py:57] epoch 3026, training loss: 6657.06, average training loss: 6479.18, base loss: 16025.13
[INFO 2017-06-29 22:24:12,785 main.py:57] epoch 3027, training loss: 6449.13, average training loss: 6479.49, base loss: 16024.86
[INFO 2017-06-29 22:24:15,654 main.py:57] epoch 3028, training loss: 6091.88, average training loss: 6478.83, base loss: 16023.70
[INFO 2017-06-29 22:24:18,510 main.py:57] epoch 3029, training loss: 6146.10, average training loss: 6478.26, base loss: 16021.95
[INFO 2017-06-29 22:24:21,360 main.py:57] epoch 3030, training loss: 6108.55, average training loss: 6478.04, base loss: 16022.07
[INFO 2017-06-29 22:24:24,206 main.py:57] epoch 3031, training loss: 6200.30, average training loss: 6478.15, base loss: 16022.32
[INFO 2017-06-29 22:24:27,089 main.py:57] epoch 3032, training loss: 6514.18, average training loss: 6478.53, base loss: 16022.39
[INFO 2017-06-29 22:24:29,967 main.py:57] epoch 3033, training loss: 6499.23, average training loss: 6477.58, base loss: 16023.26
[INFO 2017-06-29 22:24:32,814 main.py:57] epoch 3034, training loss: 5626.42, average training loss: 6476.52, base loss: 16022.02
[INFO 2017-06-29 22:24:35,649 main.py:57] epoch 3035, training loss: 5998.24, average training loss: 6475.64, base loss: 16022.05
[INFO 2017-06-29 22:24:38,516 main.py:57] epoch 3036, training loss: 6365.67, average training loss: 6475.74, base loss: 16021.85
[INFO 2017-06-29 22:24:41,378 main.py:57] epoch 3037, training loss: 6008.45, average training loss: 6475.37, base loss: 16021.37
[INFO 2017-06-29 22:24:44,257 main.py:57] epoch 3038, training loss: 6369.39, average training loss: 6475.23, base loss: 16022.05
[INFO 2017-06-29 22:24:47,137 main.py:57] epoch 3039, training loss: 6298.44, average training loss: 6474.87, base loss: 16022.01
[INFO 2017-06-29 22:24:49,975 main.py:57] epoch 3040, training loss: 6961.69, average training loss: 6475.00, base loss: 16023.63
[INFO 2017-06-29 22:24:52,870 main.py:57] epoch 3041, training loss: 5894.87, average training loss: 6474.41, base loss: 16023.22
[INFO 2017-06-29 22:24:55,729 main.py:57] epoch 3042, training loss: 6215.07, average training loss: 6474.20, base loss: 16023.33
[INFO 2017-06-29 22:24:58,599 main.py:57] epoch 3043, training loss: 6345.46, average training loss: 6474.16, base loss: 16023.64
[INFO 2017-06-29 22:25:01,504 main.py:57] epoch 3044, training loss: 6488.50, average training loss: 6474.02, base loss: 16023.49
[INFO 2017-06-29 22:25:04,386 main.py:57] epoch 3045, training loss: 6168.26, average training loss: 6474.13, base loss: 16023.42
[INFO 2017-06-29 22:25:07,282 main.py:57] epoch 3046, training loss: 6087.46, average training loss: 6473.21, base loss: 16023.74
[INFO 2017-06-29 22:25:10,181 main.py:57] epoch 3047, training loss: 6537.94, average training loss: 6473.59, base loss: 16025.31
[INFO 2017-06-29 22:25:13,082 main.py:57] epoch 3048, training loss: 6575.79, average training loss: 6473.33, base loss: 16025.27
[INFO 2017-06-29 22:25:15,943 main.py:57] epoch 3049, training loss: 6361.66, average training loss: 6473.30, base loss: 16025.11
[INFO 2017-06-29 22:25:18,868 main.py:57] epoch 3050, training loss: 5928.55, average training loss: 6472.77, base loss: 16023.60
[INFO 2017-06-29 22:25:21,740 main.py:57] epoch 3051, training loss: 6376.91, average training loss: 6472.34, base loss: 16022.59
[INFO 2017-06-29 22:25:24,593 main.py:57] epoch 3052, training loss: 6746.67, average training loss: 6472.47, base loss: 16022.77
[INFO 2017-06-29 22:25:27,477 main.py:57] epoch 3053, training loss: 6329.57, average training loss: 6471.45, base loss: 16021.95
[INFO 2017-06-29 22:25:30,350 main.py:57] epoch 3054, training loss: 6253.75, average training loss: 6471.00, base loss: 16022.05
[INFO 2017-06-29 22:25:33,216 main.py:57] epoch 3055, training loss: 6593.77, average training loss: 6471.14, base loss: 16021.99
[INFO 2017-06-29 22:25:36,098 main.py:57] epoch 3056, training loss: 5937.20, average training loss: 6470.68, base loss: 16021.70
[INFO 2017-06-29 22:25:38,937 main.py:57] epoch 3057, training loss: 6230.09, average training loss: 6470.50, base loss: 16021.39
[INFO 2017-06-29 22:25:41,784 main.py:57] epoch 3058, training loss: 6883.39, average training loss: 6470.15, base loss: 16023.05
[INFO 2017-06-29 22:25:44,610 main.py:57] epoch 3059, training loss: 6587.72, average training loss: 6470.37, base loss: 16024.34
[INFO 2017-06-29 22:25:47,474 main.py:57] epoch 3060, training loss: 6706.73, average training loss: 6470.08, base loss: 16025.06
[INFO 2017-06-29 22:25:50,335 main.py:57] epoch 3061, training loss: 6147.33, average training loss: 6469.03, base loss: 16025.56
[INFO 2017-06-29 22:25:53,241 main.py:57] epoch 3062, training loss: 6503.45, average training loss: 6469.12, base loss: 16025.74
[INFO 2017-06-29 22:25:56,098 main.py:57] epoch 3063, training loss: 6967.91, average training loss: 6469.67, base loss: 16026.44
[INFO 2017-06-29 22:25:58,933 main.py:57] epoch 3064, training loss: 6042.10, average training loss: 6469.05, base loss: 16026.66
[INFO 2017-06-29 22:26:01,817 main.py:57] epoch 3065, training loss: 6679.61, average training loss: 6469.72, base loss: 16027.08
[INFO 2017-06-29 22:26:04,681 main.py:57] epoch 3066, training loss: 6056.62, average training loss: 6469.20, base loss: 16025.97
[INFO 2017-06-29 22:26:07,526 main.py:57] epoch 3067, training loss: 6144.42, average training loss: 6468.12, base loss: 16025.54
[INFO 2017-06-29 22:26:10,429 main.py:57] epoch 3068, training loss: 6172.57, average training loss: 6467.41, base loss: 16024.53
[INFO 2017-06-29 22:26:13,337 main.py:57] epoch 3069, training loss: 6174.81, average training loss: 6466.99, base loss: 16023.41
[INFO 2017-06-29 22:26:16,266 main.py:57] epoch 3070, training loss: 6439.48, average training loss: 6467.18, base loss: 16023.68
[INFO 2017-06-29 22:26:19,121 main.py:57] epoch 3071, training loss: 6712.97, average training loss: 6466.43, base loss: 16024.43
[INFO 2017-06-29 22:26:22,042 main.py:57] epoch 3072, training loss: 6220.63, average training loss: 6465.92, base loss: 16023.60
[INFO 2017-06-29 22:26:24,924 main.py:57] epoch 3073, training loss: 7357.12, average training loss: 6466.74, base loss: 16025.04
[INFO 2017-06-29 22:26:27,801 main.py:57] epoch 3074, training loss: 6469.03, average training loss: 6466.62, base loss: 16025.00
[INFO 2017-06-29 22:26:30,654 main.py:57] epoch 3075, training loss: 6625.10, average training loss: 6466.80, base loss: 16024.87
[INFO 2017-06-29 22:26:33,530 main.py:57] epoch 3076, training loss: 6483.92, average training loss: 6467.11, base loss: 16023.62
[INFO 2017-06-29 22:26:36,421 main.py:57] epoch 3077, training loss: 6416.80, average training loss: 6466.87, base loss: 16022.75
[INFO 2017-06-29 22:26:39,284 main.py:57] epoch 3078, training loss: 5947.02, average training loss: 6465.71, base loss: 16022.05
[INFO 2017-06-29 22:26:42,161 main.py:57] epoch 3079, training loss: 6389.24, average training loss: 6464.69, base loss: 16022.62
[INFO 2017-06-29 22:26:45,047 main.py:57] epoch 3080, training loss: 6809.45, average training loss: 6465.33, base loss: 16023.03
[INFO 2017-06-29 22:26:47,915 main.py:57] epoch 3081, training loss: 6415.61, average training loss: 6464.99, base loss: 16022.45
[INFO 2017-06-29 22:26:50,782 main.py:57] epoch 3082, training loss: 6337.83, average training loss: 6464.78, base loss: 16023.51
[INFO 2017-06-29 22:26:53,684 main.py:57] epoch 3083, training loss: 6148.83, average training loss: 6463.91, base loss: 16024.23
[INFO 2017-06-29 22:26:56,559 main.py:57] epoch 3084, training loss: 6365.44, average training loss: 6463.77, base loss: 16024.45
[INFO 2017-06-29 22:26:59,435 main.py:57] epoch 3085, training loss: 5863.79, average training loss: 6463.42, base loss: 16023.57
[INFO 2017-06-29 22:27:02,297 main.py:57] epoch 3086, training loss: 6313.43, average training loss: 6463.37, base loss: 16023.67
[INFO 2017-06-29 22:27:05,176 main.py:57] epoch 3087, training loss: 6841.91, average training loss: 6463.40, base loss: 16024.63
[INFO 2017-06-29 22:27:08,078 main.py:57] epoch 3088, training loss: 5966.45, average training loss: 6463.18, base loss: 16024.21
[INFO 2017-06-29 22:27:10,965 main.py:57] epoch 3089, training loss: 6100.21, average training loss: 6463.08, base loss: 16023.98
[INFO 2017-06-29 22:27:13,854 main.py:57] epoch 3090, training loss: 6108.87, average training loss: 6462.38, base loss: 16023.02
[INFO 2017-06-29 22:27:16,747 main.py:57] epoch 3091, training loss: 6356.99, average training loss: 6461.80, base loss: 16022.69
[INFO 2017-06-29 22:27:19,598 main.py:57] epoch 3092, training loss: 6135.32, average training loss: 6461.47, base loss: 16022.39
[INFO 2017-06-29 22:27:22,477 main.py:57] epoch 3093, training loss: 5923.50, average training loss: 6460.56, base loss: 16021.89
[INFO 2017-06-29 22:27:25,374 main.py:57] epoch 3094, training loss: 6638.22, average training loss: 6460.72, base loss: 16022.58
[INFO 2017-06-29 22:27:28,266 main.py:57] epoch 3095, training loss: 6633.84, average training loss: 6460.66, base loss: 16023.57
[INFO 2017-06-29 22:27:31,143 main.py:57] epoch 3096, training loss: 7031.42, average training loss: 6461.77, base loss: 16025.27
[INFO 2017-06-29 22:27:34,006 main.py:57] epoch 3097, training loss: 6488.00, average training loss: 6461.28, base loss: 16025.86
[INFO 2017-06-29 22:27:36,840 main.py:57] epoch 3098, training loss: 6409.84, average training loss: 6460.92, base loss: 16026.45
[INFO 2017-06-29 22:27:39,736 main.py:57] epoch 3099, training loss: 6088.48, average training loss: 6460.52, base loss: 16025.96
[INFO 2017-06-29 22:27:39,736 main.py:59] epoch 3099, testing
[INFO 2017-06-29 22:27:52,090 main.py:104] average testing loss: 6422.16, base loss: 16227.61
[INFO 2017-06-29 22:27:52,091 main.py:105] improve_loss: 9805.45, improve_percent: 0.60
[INFO 2017-06-29 22:27:52,092 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 22:27:52,117 main.py:71] current best improved percent: 0.60
[INFO 2017-06-29 22:27:55,011 main.py:57] epoch 3100, training loss: 6738.17, average training loss: 6460.84, base loss: 16026.21
[INFO 2017-06-29 22:27:57,882 main.py:57] epoch 3101, training loss: 6230.02, average training loss: 6460.79, base loss: 16025.27
[INFO 2017-06-29 22:28:00,763 main.py:57] epoch 3102, training loss: 5977.30, average training loss: 6459.91, base loss: 16023.84
[INFO 2017-06-29 22:28:03,637 main.py:57] epoch 3103, training loss: 5906.72, average training loss: 6459.21, base loss: 16022.64
[INFO 2017-06-29 22:28:06,521 main.py:57] epoch 3104, training loss: 6206.50, average training loss: 6458.60, base loss: 16022.76
[INFO 2017-06-29 22:28:09,397 main.py:57] epoch 3105, training loss: 6213.79, average training loss: 6458.98, base loss: 16022.44
[INFO 2017-06-29 22:28:12,265 main.py:57] epoch 3106, training loss: 6475.17, average training loss: 6458.60, base loss: 16022.92
[INFO 2017-06-29 22:28:15,155 main.py:57] epoch 3107, training loss: 5661.23, average training loss: 6457.51, base loss: 16022.46
[INFO 2017-06-29 22:28:18,017 main.py:57] epoch 3108, training loss: 6352.44, average training loss: 6457.44, base loss: 16021.91
[INFO 2017-06-29 22:28:20,857 main.py:57] epoch 3109, training loss: 7078.90, average training loss: 6458.10, base loss: 16022.99
[INFO 2017-06-29 22:28:23,721 main.py:57] epoch 3110, training loss: 6303.55, average training loss: 6457.67, base loss: 16022.72
[INFO 2017-06-29 22:28:26,612 main.py:57] epoch 3111, training loss: 6163.58, average training loss: 6457.39, base loss: 16021.85
[INFO 2017-06-29 22:28:29,481 main.py:57] epoch 3112, training loss: 6515.47, average training loss: 6457.28, base loss: 16021.58
[INFO 2017-06-29 22:28:32,425 main.py:57] epoch 3113, training loss: 6035.08, average training loss: 6456.64, base loss: 16020.09
[INFO 2017-06-29 22:28:35,303 main.py:57] epoch 3114, training loss: 6217.79, average training loss: 6456.90, base loss: 16020.59
[INFO 2017-06-29 22:28:38,160 main.py:57] epoch 3115, training loss: 6439.81, average training loss: 6457.06, base loss: 16021.03
[INFO 2017-06-29 22:28:41,046 main.py:57] epoch 3116, training loss: 6164.59, average training loss: 6456.70, base loss: 16020.65
[INFO 2017-06-29 22:28:43,942 main.py:57] epoch 3117, training loss: 6534.44, average training loss: 6456.63, base loss: 16021.21
[INFO 2017-06-29 22:28:46,818 main.py:57] epoch 3118, training loss: 6449.20, average training loss: 6456.53, base loss: 16021.30
[INFO 2017-06-29 22:28:49,757 main.py:57] epoch 3119, training loss: 6455.34, average training loss: 6456.83, base loss: 16021.78
[INFO 2017-06-29 22:28:52,670 main.py:57] epoch 3120, training loss: 6477.01, average training loss: 6456.50, base loss: 16022.10
[INFO 2017-06-29 22:28:55,566 main.py:57] epoch 3121, training loss: 6336.41, average training loss: 6455.91, base loss: 16021.67
[INFO 2017-06-29 22:28:58,441 main.py:57] epoch 3122, training loss: 5757.72, average training loss: 6455.45, base loss: 16020.45
[INFO 2017-06-29 22:29:01,297 main.py:57] epoch 3123, training loss: 5834.93, average training loss: 6454.99, base loss: 16019.39
[INFO 2017-06-29 22:29:04,170 main.py:57] epoch 3124, training loss: 6404.52, average training loss: 6454.80, base loss: 16019.83
[INFO 2017-06-29 22:29:06,996 main.py:57] epoch 3125, training loss: 6489.07, average training loss: 6454.52, base loss: 16020.86
[INFO 2017-06-29 22:29:09,903 main.py:57] epoch 3126, training loss: 5849.34, average training loss: 6453.23, base loss: 16020.60
[INFO 2017-06-29 22:29:12,804 main.py:57] epoch 3127, training loss: 6713.13, average training loss: 6453.41, base loss: 16020.38
[INFO 2017-06-29 22:29:15,699 main.py:57] epoch 3128, training loss: 6529.09, average training loss: 6453.33, base loss: 16021.43
[INFO 2017-06-29 22:29:18,562 main.py:57] epoch 3129, training loss: 5885.44, average training loss: 6452.75, base loss: 16021.53
[INFO 2017-06-29 22:29:21,430 main.py:57] epoch 3130, training loss: 6342.38, average training loss: 6452.72, base loss: 16021.47
[INFO 2017-06-29 22:29:24,338 main.py:57] epoch 3131, training loss: 6059.00, average training loss: 6452.60, base loss: 16021.43
[INFO 2017-06-29 22:29:27,215 main.py:57] epoch 3132, training loss: 6029.70, average training loss: 6452.54, base loss: 16020.81
[INFO 2017-06-29 22:29:30,062 main.py:57] epoch 3133, training loss: 6123.79, average training loss: 6452.47, base loss: 16020.87
[INFO 2017-06-29 22:29:32,955 main.py:57] epoch 3134, training loss: 5889.04, average training loss: 6451.99, base loss: 16020.18
[INFO 2017-06-29 22:29:35,842 main.py:57] epoch 3135, training loss: 6899.00, average training loss: 6452.63, base loss: 16021.52
[INFO 2017-06-29 22:29:38,731 main.py:57] epoch 3136, training loss: 6578.24, average training loss: 6452.40, base loss: 16022.31
[INFO 2017-06-29 22:29:41,626 main.py:57] epoch 3137, training loss: 6619.83, average training loss: 6452.48, base loss: 16022.98
[INFO 2017-06-29 22:29:44,504 main.py:57] epoch 3138, training loss: 5984.82, average training loss: 6451.87, base loss: 16022.26
[INFO 2017-06-29 22:29:47,361 main.py:57] epoch 3139, training loss: 6077.08, average training loss: 6451.26, base loss: 16021.24
[INFO 2017-06-29 22:29:50,201 main.py:57] epoch 3140, training loss: 6526.18, average training loss: 6451.33, base loss: 16021.40
[INFO 2017-06-29 22:29:53,051 main.py:57] epoch 3141, training loss: 6348.83, average training loss: 6451.32, base loss: 16021.82
[INFO 2017-06-29 22:29:55,898 main.py:57] epoch 3142, training loss: 6414.75, average training loss: 6451.24, base loss: 16021.84
[INFO 2017-06-29 22:29:58,804 main.py:57] epoch 3143, training loss: 6118.26, average training loss: 6451.52, base loss: 16020.59
[INFO 2017-06-29 22:30:01,697 main.py:57] epoch 3144, training loss: 5932.47, average training loss: 6450.42, base loss: 16020.07
[INFO 2017-06-29 22:30:04,585 main.py:57] epoch 3145, training loss: 6524.96, average training loss: 6450.20, base loss: 16020.23
[INFO 2017-06-29 22:30:07,465 main.py:57] epoch 3146, training loss: 6022.36, average training loss: 6449.79, base loss: 16020.52
[INFO 2017-06-29 22:30:10,349 main.py:57] epoch 3147, training loss: 6740.33, average training loss: 6449.71, base loss: 16021.46
[INFO 2017-06-29 22:30:13,231 main.py:57] epoch 3148, training loss: 6222.40, average training loss: 6448.87, base loss: 16020.14
[INFO 2017-06-29 22:30:16,130 main.py:57] epoch 3149, training loss: 6350.90, average training loss: 6448.34, base loss: 16019.98
[INFO 2017-06-29 22:30:18,994 main.py:57] epoch 3150, training loss: 6618.39, average training loss: 6448.26, base loss: 16021.13
[INFO 2017-06-29 22:30:21,833 main.py:57] epoch 3151, training loss: 6738.29, average training loss: 6448.90, base loss: 16022.87
[INFO 2017-06-29 22:30:24,690 main.py:57] epoch 3152, training loss: 6123.53, average training loss: 6448.83, base loss: 16023.12
[INFO 2017-06-29 22:30:27,597 main.py:57] epoch 3153, training loss: 6728.87, average training loss: 6448.88, base loss: 16024.06
[INFO 2017-06-29 22:30:30,478 main.py:57] epoch 3154, training loss: 5766.31, average training loss: 6448.21, base loss: 16023.78
[INFO 2017-06-29 22:30:33,381 main.py:57] epoch 3155, training loss: 6468.44, average training loss: 6447.26, base loss: 16023.94
[INFO 2017-06-29 22:30:36,274 main.py:57] epoch 3156, training loss: 6017.18, average training loss: 6446.79, base loss: 16023.82
[INFO 2017-06-29 22:30:39,162 main.py:57] epoch 3157, training loss: 6892.45, average training loss: 6447.08, base loss: 16025.05
[INFO 2017-06-29 22:30:42,040 main.py:57] epoch 3158, training loss: 6102.79, average training loss: 6446.83, base loss: 16024.28
[INFO 2017-06-29 22:30:44,906 main.py:57] epoch 3159, training loss: 6270.36, average training loss: 6446.55, base loss: 16024.02
[INFO 2017-06-29 22:30:47,758 main.py:57] epoch 3160, training loss: 6819.13, average training loss: 6446.64, base loss: 16024.35
[INFO 2017-06-29 22:30:50,620 main.py:57] epoch 3161, training loss: 6337.60, average training loss: 6446.91, base loss: 16024.05
[INFO 2017-06-29 22:30:53,481 main.py:57] epoch 3162, training loss: 6447.00, average training loss: 6446.24, base loss: 16024.37
[INFO 2017-06-29 22:30:56,370 main.py:57] epoch 3163, training loss: 6288.58, average training loss: 6446.27, base loss: 16024.05
[INFO 2017-06-29 22:30:59,236 main.py:57] epoch 3164, training loss: 6355.89, average training loss: 6445.96, base loss: 16024.65
[INFO 2017-06-29 22:31:02,101 main.py:57] epoch 3165, training loss: 5963.63, average training loss: 6445.57, base loss: 16024.16
[INFO 2017-06-29 22:31:04,963 main.py:57] epoch 3166, training loss: 6223.40, average training loss: 6445.37, base loss: 16023.79
[INFO 2017-06-29 22:31:07,825 main.py:57] epoch 3167, training loss: 6166.96, average training loss: 6445.39, base loss: 16023.54
[INFO 2017-06-29 22:31:10,697 main.py:57] epoch 3168, training loss: 6508.70, average training loss: 6445.49, base loss: 16023.49
[INFO 2017-06-29 22:31:13,559 main.py:57] epoch 3169, training loss: 6550.78, average training loss: 6445.22, base loss: 16022.81
[INFO 2017-06-29 22:31:16,436 main.py:57] epoch 3170, training loss: 6017.15, average training loss: 6444.50, base loss: 16022.74
[INFO 2017-06-29 22:31:19,386 main.py:57] epoch 3171, training loss: 6034.24, average training loss: 6443.60, base loss: 16023.32
[INFO 2017-06-29 22:31:22,286 main.py:57] epoch 3172, training loss: 6607.00, average training loss: 6444.03, base loss: 16023.54
[INFO 2017-06-29 22:31:25,200 main.py:57] epoch 3173, training loss: 6210.14, average training loss: 6444.27, base loss: 16023.83
[INFO 2017-06-29 22:31:28,047 main.py:57] epoch 3174, training loss: 6056.61, average training loss: 6443.87, base loss: 16023.13
[INFO 2017-06-29 22:31:30,977 main.py:57] epoch 3175, training loss: 6421.92, average training loss: 6443.43, base loss: 16022.74
[INFO 2017-06-29 22:31:33,796 main.py:57] epoch 3176, training loss: 6367.77, average training loss: 6443.42, base loss: 16022.77
[INFO 2017-06-29 22:31:36,678 main.py:57] epoch 3177, training loss: 6591.25, average training loss: 6443.24, base loss: 16023.57
[INFO 2017-06-29 22:31:39,588 main.py:57] epoch 3178, training loss: 5869.89, average training loss: 6442.55, base loss: 16022.91
[INFO 2017-06-29 22:31:42,448 main.py:57] epoch 3179, training loss: 6432.69, average training loss: 6442.71, base loss: 16023.01
[INFO 2017-06-29 22:31:45,312 main.py:57] epoch 3180, training loss: 7078.27, average training loss: 6443.36, base loss: 16024.36
[INFO 2017-06-29 22:31:48,158 main.py:57] epoch 3181, training loss: 6142.37, average training loss: 6442.66, base loss: 16024.13
[INFO 2017-06-29 22:31:51,022 main.py:57] epoch 3182, training loss: 5872.21, average training loss: 6442.06, base loss: 16023.93
[INFO 2017-06-29 22:31:53,870 main.py:57] epoch 3183, training loss: 6118.07, average training loss: 6441.65, base loss: 16023.73
[INFO 2017-06-29 22:31:56,731 main.py:57] epoch 3184, training loss: 6089.08, average training loss: 6440.91, base loss: 16024.01
[INFO 2017-06-29 22:31:59,608 main.py:57] epoch 3185, training loss: 5997.62, average training loss: 6439.91, base loss: 16024.12
[INFO 2017-06-29 22:32:02,475 main.py:57] epoch 3186, training loss: 6730.94, average training loss: 6440.32, base loss: 16025.45
[INFO 2017-06-29 22:32:05,347 main.py:57] epoch 3187, training loss: 6414.88, average training loss: 6440.20, base loss: 16026.25
[INFO 2017-06-29 22:32:08,208 main.py:57] epoch 3188, training loss: 6700.92, average training loss: 6440.26, base loss: 16026.86
[INFO 2017-06-29 22:32:11,099 main.py:57] epoch 3189, training loss: 5903.12, average training loss: 6439.61, base loss: 16026.27
[INFO 2017-06-29 22:32:13,945 main.py:57] epoch 3190, training loss: 6133.42, average training loss: 6439.19, base loss: 16025.73
[INFO 2017-06-29 22:32:16,795 main.py:57] epoch 3191, training loss: 6379.90, average training loss: 6438.82, base loss: 16025.75
[INFO 2017-06-29 22:32:19,616 main.py:57] epoch 3192, training loss: 6012.87, average training loss: 6438.30, base loss: 16026.01
[INFO 2017-06-29 22:32:22,499 main.py:57] epoch 3193, training loss: 6497.40, average training loss: 6437.78, base loss: 16027.44
[INFO 2017-06-29 22:32:25,351 main.py:57] epoch 3194, training loss: 6033.83, average training loss: 6437.03, base loss: 16027.08
[INFO 2017-06-29 22:32:28,199 main.py:57] epoch 3195, training loss: 6509.46, average training loss: 6436.61, base loss: 16028.04
[INFO 2017-06-29 22:32:31,067 main.py:57] epoch 3196, training loss: 6241.75, average training loss: 6436.45, base loss: 16027.72
[INFO 2017-06-29 22:32:33,968 main.py:57] epoch 3197, training loss: 6637.72, average training loss: 6436.88, base loss: 16027.90
[INFO 2017-06-29 22:32:36,865 main.py:57] epoch 3198, training loss: 6620.72, average training loss: 6437.08, base loss: 16028.26
[INFO 2017-06-29 22:32:39,719 main.py:57] epoch 3199, training loss: 6224.18, average training loss: 6436.36, base loss: 16027.99
[INFO 2017-06-29 22:32:39,720 main.py:59] epoch 3199, testing
[INFO 2017-06-29 22:32:52,045 main.py:104] average testing loss: 6301.29, base loss: 16132.81
[INFO 2017-06-29 22:32:52,045 main.py:105] improve_loss: 9831.53, improve_percent: 0.61
[INFO 2017-06-29 22:32:52,048 main.py:67] model save to ./model/final.pth
[INFO 2017-06-29 22:32:52,073 main.py:71] current best improved percent: 0.61
[INFO 2017-06-29 22:32:54,908 main.py:57] epoch 3200, training loss: 6481.24, average training loss: 6435.60, base loss: 16028.03
[INFO 2017-06-29 22:32:57,787 main.py:57] epoch 3201, training loss: 6474.02, average training loss: 6435.41, base loss: 16028.61
[INFO 2017-06-29 22:33:00,669 main.py:57] epoch 3202, training loss: 5947.52, average training loss: 6435.04, base loss: 16027.91
[INFO 2017-06-29 22:33:03,530 main.py:57] epoch 3203, training loss: 6642.47, average training loss: 6435.10, base loss: 16028.98
[INFO 2017-06-29 22:33:06,354 main.py:57] epoch 3204, training loss: 6037.01, average training loss: 6434.61, base loss: 16029.07
[INFO 2017-06-29 22:33:09,181 main.py:57] epoch 3205, training loss: 6310.40, average training loss: 6434.49, base loss: 16029.53
[INFO 2017-06-29 22:33:12,048 main.py:57] epoch 3206, training loss: 6905.62, average training loss: 6434.58, base loss: 16030.17
[INFO 2017-06-29 22:33:14,948 main.py:57] epoch 3207, training loss: 6213.75, average training loss: 6433.89, base loss: 16029.28
[INFO 2017-06-29 22:33:17,858 main.py:57] epoch 3208, training loss: 6106.86, average training loss: 6433.55, base loss: 16029.80
[INFO 2017-06-29 22:33:20,697 main.py:57] epoch 3209, training loss: 5938.34, average training loss: 6432.65, base loss: 16029.49
[INFO 2017-06-29 22:33:23,568 main.py:57] epoch 3210, training loss: 5879.53, average training loss: 6432.09, base loss: 16029.48
[INFO 2017-06-29 22:33:26,397 main.py:57] epoch 3211, training loss: 6323.73, average training loss: 6431.90, base loss: 16029.67
[INFO 2017-06-29 22:33:29,312 main.py:57] epoch 3212, training loss: 6377.77, average training loss: 6432.28, base loss: 16029.42
[INFO 2017-06-29 22:33:32,184 main.py:57] epoch 3213, training loss: 6881.02, average training loss: 6433.12, base loss: 16030.05
[INFO 2017-06-29 22:33:35,050 main.py:57] epoch 3214, training loss: 6593.23, average training loss: 6433.42, base loss: 16030.24
[INFO 2017-06-29 22:33:37,946 main.py:57] epoch 3215, training loss: 6183.03, average training loss: 6433.20, base loss: 16030.39
[INFO 2017-06-29 22:33:40,816 main.py:57] epoch 3216, training loss: 6314.12, average training loss: 6432.59, base loss: 16030.90
[INFO 2017-06-29 22:33:43,700 main.py:57] epoch 3217, training loss: 5769.21, average training loss: 6431.31, base loss: 16030.95
[INFO 2017-06-29 22:33:46,587 main.py:57] epoch 3218, training loss: 6411.29, average training loss: 6431.91, base loss: 16031.28
[INFO 2017-06-29 22:33:49,445 main.py:57] epoch 3219, training loss: 5771.56, average training loss: 6431.45, base loss: 16030.70
[INFO 2017-06-29 22:33:52,296 main.py:57] epoch 3220, training loss: 6126.03, average training loss: 6430.99, base loss: 16029.57
[INFO 2017-06-29 22:33:55,134 main.py:57] epoch 3221, training loss: 6361.86, average training loss: 6430.89, base loss: 16029.13
[INFO 2017-06-29 22:33:58,044 main.py:57] epoch 3222, training loss: 6159.42, average training loss: 6430.04, base loss: 16028.94
[INFO 2017-06-29 22:34:00,908 main.py:57] epoch 3223, training loss: 6401.24, average training loss: 6429.88, base loss: 16029.15
[INFO 2017-06-29 22:34:03,763 main.py:57] epoch 3224, training loss: 6347.36, average training loss: 6429.49, base loss: 16029.49
[INFO 2017-06-29 22:34:06,646 main.py:57] epoch 3225, training loss: 6589.96, average training loss: 6429.41, base loss: 16029.91
[INFO 2017-06-29 22:34:09,505 main.py:57] epoch 3226, training loss: 5915.10, average training loss: 6428.93, base loss: 16028.63
[INFO 2017-06-29 22:34:12,357 main.py:57] epoch 3227, training loss: 6973.90, average training loss: 6429.32, base loss: 16029.46
[INFO 2017-06-29 22:34:15,193 main.py:57] epoch 3228, training loss: 6505.16, average training loss: 6429.19, base loss: 16029.71
[INFO 2017-06-29 22:34:18,097 main.py:57] epoch 3229, training loss: 5900.65, average training loss: 6429.05, base loss: 16029.08
[INFO 2017-06-29 22:34:20,935 main.py:57] epoch 3230, training loss: 6353.29, average training loss: 6429.09, base loss: 16029.83
[INFO 2017-06-29 22:34:23,800 main.py:57] epoch 3231, training loss: 6323.35, average training loss: 6429.35, base loss: 16030.28
[INFO 2017-06-29 22:34:26,698 main.py:57] epoch 3232, training loss: 5890.21, average training loss: 6428.75, base loss: 16029.60
